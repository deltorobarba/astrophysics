{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTyMPxPtl588",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "* Regularization is a technique for preventing over-fitting by penalizing a model for having large weights\n",
        "* Weight regularization was borrowed from penalized regression models in statistics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr-q4oEusOUa",
        "colab_type": "text"
      },
      "source": [
        "![Regularization Types](https://raw.githubusercontent.com/deltorobarba/repo/master/5907C4B3-6EC5-40CD-BB51-A5AB75C3DC71.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5O1QrOXswPf",
        "colab_type": "text"
      },
      "source": [
        "Source: ['Getting started with Regression'](https://medium.com/@savannahar68/getting-started-with-regression-a39aca03b75f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlN8ilZEqQj7",
        "colab_type": "text"
      },
      "source": [
        "## L1 Regularization\n",
        "\n",
        "* Sum of the absolute weights. Gives sparse solutions, since it does not take all features\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD), least absolute errors (LAE)\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn’t easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZuDNlbKlWu4",
        "colab_type": "text"
      },
      "source": [
        "## L2 Regularization\n",
        "\n",
        "* Sum of the squared weights. Is the most common type of regularization, also called simply “weight decay,” with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, Tikhonov regularization, Euclidean distance, least squares error (LSE)\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn’t give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge. \n",
        "* **Relationship to Dropout**: Dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8UW9XnleoV",
        "colab_type": "text"
      },
      "source": [
        "## Elastic Net\n",
        "\n",
        "* Linear combination of L1 and L2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJBr4ZHljUM",
        "colab_type": "text"
      },
      "source": [
        "## Lambda Value\n",
        "\n",
        "* Lambda is a regularization hyperparameter\n",
        "* Reasonable values of lambda range between 0 and 0.1\n",
        "* L2 weight regularization with very small regularization hyperparameters such as (e.g. 0.0005 or 5 x 10^−4) may be a good starting point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHu0whFdwoH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc_Iu-7glx75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Choose Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnICMm4Dg1cf",
        "colab_type": "text"
      },
      "source": [
        "The Dense layer takes three regularizers, which all default to None. \n",
        "* **kernel_regularizer**: Regularizer function applied to the kernel weights matrix.\n",
        "* **bias_regularizer**: Regularizer function applied to the bias vector.\n",
        "* **activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb8hpJ37i5wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l2(l=0.0005)\n",
        "bias_regularizer=None\n",
        "activity_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDEv6ZETl129",
        "colab_type": "code",
        "outputId": "8c26326c-c686-4ff3-87ca-7bee2a8e6b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu', \n",
        "                                kernel_regularizer=kernel_regularizer, \n",
        "                                bias_regularizer=bias_regularizer, \n",
        "                                activity_regularizer=activity_regularizer))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6831 - acc: 0.8137 - val_loss: 0.6109 - val_acc: 0.8162\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5375 - acc: 0.8428 - val_loss: 0.5487 - val_acc: 0.8322\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5099 - acc: 0.8490 - val_loss: 0.5410 - val_acc: 0.8361\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4953 - acc: 0.8524 - val_loss: 0.4899 - val_acc: 0.8572\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4853 - acc: 0.8546 - val_loss: 0.5258 - val_acc: 0.8373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f41a9167048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}