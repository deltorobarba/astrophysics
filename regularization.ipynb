{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsP-maXp4TL8",
        "colab_type": "text"
      },
      "source": [
        "## Vectornorm Regularization\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr-q4oEusOUa",
        "colab_type": "text"
      },
      "source": [
        "![Regularization Types](https://raw.githubusercontent.com/deltorobarba/repo/master/vectornorm.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5O1QrOXswPf",
        "colab_type": "text"
      },
      "source": [
        "Source: ['Getting started with Regression'](https://medium.com/@savannahar68/getting-started-with-regression-a39aca03b75f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlN8ilZEqQj7",
        "colab_type": "text"
      },
      "source": [
        "## L1 Regularization\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn’t easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**: if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero. very useful when you want to understand exactly which features are contributing to a decision. if you can ignore the ouliers in your dataset or you need them to be there. use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features  at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZuDNlbKlWu4",
        "colab_type": "text"
      },
      "source": [
        "## L2 Regularization\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply “weight decay,” with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn’t give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge. \n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8UW9XnleoV",
        "colab_type": "text"
      },
      "source": [
        "## Elastic Net\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed λ2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+λ2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGUkhjJ16WtE",
        "colab_type": "text"
      },
      "source": [
        "# Special Topics in Vectornorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJBr4ZHljUM",
        "colab_type": "text"
      },
      "source": [
        "## Lambda Value (λ)\n",
        "\n",
        "* Lambda is a regularization hyperparameter\n",
        "* Reasonable values of lambda range between 0 and 0.1\n",
        "* L2 weight regularization with very small regularization hyperparameters such as (e.g. 0.0005 or 5 x 10^−4) may be a good starting point\n",
        "* Learn more: [Google Course: Regularization for Simplicity: Lambda](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75X3Z2teJNs-",
        "colab_type": "text"
      },
      "source": [
        "## Add Regularization to Cost Function\n",
        "\n",
        "**Theoretical Foundation** \n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWJgeJqB4gy",
        "colab_type": "text"
      },
      "source": [
        "**Add L1 (Lasso) Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYzCd1FUBnjV",
        "colab_type": "text"
      },
      "source": [
        "**Add L2 (Ridge) Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foOVciWvULS",
        "colab_type": "text"
      },
      "source": [
        "## Analytical Equivalence\n",
        "\n",
        "Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "\n",
        "https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inXf6wD14thO",
        "colab_type": "text"
      },
      "source": [
        "# Regularization Objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knt6wCob4xfj",
        "colab_type": "text"
      },
      "source": [
        "## Weight Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvnac8AA41ac",
        "colab_type": "text"
      },
      "source": [
        "* Weight regularization was borrowed from penalized regression models in statistics. Neural networks learn a set of weights that best map inputs to outputs. \n",
        "* A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data. \n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHu0whFdwoH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc_Iu-7glx75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Select Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnICMm4Dg1cf",
        "colab_type": "text"
      },
      "source": [
        "The Dense layer takes three regularizers, which all default to None. \n",
        "* **kernel_regularizer**: Regularizer function applied to the kernel weights matrix.\n",
        "* **bias_regularizer**: Regularizer function applied to the bias vector.\n",
        "* **activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gKj7gjwQtw",
        "colab_type": "text"
      },
      "source": [
        "**Kernel Regularizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRsFyXXiwXYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKn6U3WTwVRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yVhv_57wbN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJSTvHowfzn",
        "colab_type": "text"
      },
      "source": [
        "**Bias Regularizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7QXjYfzwsgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHyBz0fwwvU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NrlNShswyvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDXotzf7wk4o",
        "colab_type": "text"
      },
      "source": [
        "**Activity Regularizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO2jgyv5w30S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXWwwJ5vw8HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFwslg_Bw_9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDEv6ZETl129",
        "colab_type": "code",
        "outputId": "417ede77-ba24-49da-85d4-3f95c16e630f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu', \n",
        "                                kernel_regularizer=kernel_regularizer, \n",
        "                                bias_regularizer=bias_regularizer, \n",
        "                                activity_regularizer=activity_regularizer))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 1.4610 - acc: 0.7735 - val_loss: 0.9306 - val_acc: 0.7923\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.8691 - acc: 0.8017 - val_loss: 0.8504 - val_acc: 0.8149\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.8029 - acc: 0.8120 - val_loss: 0.7497 - val_acc: 0.8215\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7695 - acc: 0.8178 - val_loss: 0.7286 - val_acc: 0.8313\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.7460 - acc: 0.8213 - val_loss: 0.7305 - val_acc: 0.8331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f964d09dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}