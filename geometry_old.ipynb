{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometry_old.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/geometry_old.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjNmcYpFuFTY",
        "colab_type": "text"
      },
      "source": [
        "# **Differential (Information) Geometry - OLD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYViK39mt7Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkjtUvw8nOE",
        "colab_type": "text"
      },
      "source": [
        "## **Cost (Loss) Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabFDgCa9SAr",
        "colab_type": "text"
      },
      "source": [
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n",
        "\n",
        "* In most cases, our parametric model defines a distribution […] and we simply use the principle of maximum likelihood. This means we use the cross-entropy between the training data and the model’s predictions as the cost function.\n",
        "\n",
        "* It is important, therefore, that the function faithfully represent our design goals. If we choose a poor error function and obtain unsatisfactory results, the fault is ours for badly specifying the goal of the search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBgVckIO8r8i",
        "colab_type": "text"
      },
      "source": [
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "* Maximum likelihood seeks to find the optimum values for the parameters by maximizing a likelihood function derived from the training data.\n",
        "\n",
        "* Given input, the model is trying to make predictions that **match the data distribution of the target variable**. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
        "\n",
        "* One way to interpret maximum likelihood estimation is to view it as **minimizing the dissimilarity** between the empirical distribution […] defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. […] **Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions**.\n",
        "\n",
        "* Under appropriate conditions, the maximum likelihood estimator has the **property of consistency** […], meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter.\n",
        "\n",
        "* Under the framework maximum likelihood, the error between two probability distributions is measured using cross-entropy. Under maximum likelihood estimation, we would seek a set of model weights that minimize the difference between the model’s predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
        "\n",
        "When using the framework of maximum likelihood estimation, we will implement a cross-entropy loss function, which often in practice means:\n",
        "* a **cross-entropy** loss function for classification problems and \n",
        "* a **mean squared error** loss function for regression problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4L_PXfl_RMh",
        "colab_type": "text"
      },
      "source": [
        "* Under the framework of maximum likelihood estimation and assuming a **Gaussian distribution for the target variable**, mean squared error can be considered the cross-entropy between the distribution of the model predictions and the distribution of the target variable.\n",
        "\n",
        "* Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer. \n",
        "\n",
        "* Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model. \n",
        "\n",
        "* For example, **mean squared error is the cross-entropy between the empirical distribution and a Gaussian model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXMZQeKBBagQ",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3qVwnOZB5Nb",
        "colab_type": "text"
      },
      "source": [
        "## **Cross-Entropy & Information Theory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrvSwlkmCA4B",
        "colab_type": "text"
      },
      "source": [
        "* Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.\n",
        "\n",
        "* You might recall that information quantifies the number of bits required to encode and transmit an event. Lower probability events have more information, higher probability events have less information.\n",
        "\n",
        "* Entropy is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.\n",
        "\n",
        "* In information theory, we like to describe the “surprise” of an event. Low probability events are more surprising therefore have a larger amount of information. Whereas probability distributions where the events are equally likely are more surprising and have larger entropy.\n",
        "\n",
        "* Skewed Probability Distribution (unsurprising): Low entropy.\n",
        "\n",
        "* Balanced Probability Distribution (surprising): High entropy.\n",
        "\n",
        "* Entropy can be calculated for a random variable with a set of x in X discrete states discrete states and their probability P(x) as follows:\n",
        "\n",
        "```\n",
        "H(X) = – sum x in X P(x) * log(P(x))\n",
        "```\n",
        "Cross-entropy builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsUbtbv7DUv3",
        "colab_type": "text"
      },
      "source": [
        "* The intuition for this definition comes if we consider a target or underlying probability distribution P and an approximation of the target distribution Q, then the cross-entropy of Q from P is the number of additional bits to represent an event using Q instead of P.\n",
        "\n",
        "* The cross-entropy between two probability distributions, such as Q from P, can be stated formally as:\n",
        "\n",
        "H(P, Q)\n",
        "\n",
        "* Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution.\n",
        "\n",
        "* Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "H(P, Q) = – sum x in X P(x) * log(Q(x))\n",
        "```\n",
        "\n",
        "* Where P(x) is the probability of the event x in P, Q(x) is the probability of event x in Q and log is the base-2 logarithm, meaning that the results are in bits. If the base-e or natural logarithm is used instead, the result will have the units called nats.\n",
        "\n",
        "* This calculation is for discrete probability distributions, although a similar calculation can be used for continuous probability distributions using the integral across the events instead of the sum.\n",
        "\n",
        "* The result will be a positive number measured in bits and will be equal to the entropy of the distribution if the two probability distributions are identical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lktzMCgwCdyE",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/what-is-information-entropy/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfg74f5CMAg",
        "colab_type": "text"
      },
      "source": [
        "**Cross-Entropy vs KL Divergence vs Logloss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmqSldRPBk9Z",
        "colab_type": "text"
      },
      "source": [
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3YVyW4Y4h9u",
        "colab_type": "text"
      },
      "source": [
        "## **Metric (Similarity) Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YuJ76Kg4l3M",
        "colab_type": "text"
      },
      "source": [
        "* Triplet loss is probably the most popular loss function of metric learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXAO6AZJla9V",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Similarity_learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szk8f_884n6e",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwyJNSuepvyl",
        "colab_type": "text"
      },
      "source": [
        "## **Distance & Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDaFyrlkAN03",
        "colab_type": "text"
      },
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ≥ 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ≤ d(x, y) + d(y, z)     (subadditivity / triangle inequality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCFr4OMNuoir",
        "colab_type": "text"
      },
      "source": [
        "**Distances**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQsGsly_w8h",
        "colab_type": "text"
      },
      "source": [
        "For continuous data:\n",
        "\n",
        "* Euclidean Distance\n",
        "* Manhattan Distance\n",
        "* Canberra Distance\n",
        "* Bray Curtis Distance\n",
        "* Cosine Distance\n",
        "* Correlation Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiPqyo5E8Uzq",
        "colab_type": "text"
      },
      "source": [
        "**Divergences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYq6ACsE8s7N",
        "colab_type": "text"
      },
      "source": [
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold. \n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback–Leibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMM3R7pCwCHG",
        "colab_type": "text"
      },
      "source": [
        "## **Find the similarity between two probability distributions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eozx-uGEwh_9",
        "colab_type": "text"
      },
      "source": [
        "Using Jensen Shannon Divergence to build a tool to find the distance between probability distributions using Python.\n",
        "\n",
        "I was on a mission to find a good measure of difference between two probability distributions. After doing a lot of research online, taking feedback from my colleagues, and validating various methods, I found one that does a really good job.\n",
        "\n",
        "My problem statement could be solved by calculating the statistical distance between the two probability distributions. To do this, I found out that Jensen Shannon Distance can be used.\n",
        "\n",
        "Jensen-Shannon Divergence (JSD)is a metric derived from another measure of statistical distance called the Kullback-Leiber Divergence(KLD). The reason why I couldn’t use the KLD is that it’s an asymmetrical function. Since there might have been a lot of distance calculations required, it posed a risk.\n",
        "\n",
        "JSD, on the other hand, is a symmetrical function and the square root of JSD gives the Jensen-Shannon Distance. A measure that we can use to find the similarity between the two probability distributions. 0 indicates that the two distributions are the same, and 1 would indicate that they are nowhere similar.\n",
        "\n",
        "Where P & Q are the two probability distribution, M = (P+Q)/2, and D(P ||M) is the KLD between P and M. Similarly D(Q||M) is the KLD between Q and M.\n",
        "Implementation in Python\n",
        "Now that we know the formula, it’s time to implement it. First of all, we need to calculate M and also, the KLD between P&M and Q&M.\n",
        "Scipy is a phenomenal Python Library for scientific computing and it has lots of statistical measures in-built. It turns out that the entropy measure in scipy is implemented using the KLD. Just what we want.\n",
        "\n",
        "(I found it to be quite simple to implement it with python and I got really good results when I tested it with a few distributions.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7Uutxrz9iN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f87e74ca-7d4a-4099-af0f-eb57554c2f84"
      },
      "source": [
        " # Create test data\n",
        "p = np.random.rayleigh(3,3)\n",
        "q = np.random.weibull(3,3)\n",
        "p, q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.84848271, 4.18906706, 5.61567569]),\n",
              " array([1.036487  , 0.9192782 , 0.63485667]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zufnQaip3Xgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9295beb0-3a93-4a6c-a200-880837d402ed"
      },
      "source": [
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
        "# Calculate the entropy of a distribution for given probability values\n",
        "entropy([p, q], base=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.51682914, 0.471327  , 0.32851553])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfittfZK1uDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45ca97b1-13b6-4dd6-b911-8a99baee6e15"
      },
      "source": [
        " m = (p + q) / 2\n",
        "\n",
        "    # compute Jensen Shannon Divergence\n",
        "divergence = (scipy.stats.entropy(p, m))\n",
        "\n",
        "divergence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([inf, inf, inf])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snhT2RwTwKq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create function to compute distance\n",
        "from scipy.stats import entropy\n",
        "def jensen_shannon_distance(p, q):\n",
        "    \"\"\"\n",
        "    method to compute the Jenson-Shannon Distance \n",
        "    between two probability distributions\n",
        "    \"\"\"\n",
        "\n",
        "    # convert the vectors into numpy arrays in case that they aren't\n",
        "    # p = np.array(p)\n",
        "    # q = np.array(q)\n",
        "\n",
        "    # calculate m\n",
        "    m = (p + q) / 2\n",
        "\n",
        "    # compute Jensen Shannon Divergence\n",
        "    divergence = (scipy.stats.entropy(p, m) + scipy.stats.entropy(q, m)) / 2\n",
        "\n",
        "    # compute the Jensen Shannon Distance\n",
        "    distance = np.sqrt(divergence)\n",
        "\n",
        "    return distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOLXS2P0gum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6fcaa25-2fec-4f17-bdb4-adc976bfd30e"
      },
      "source": [
        "print(jensen_shannon_distance(eins,zwei))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[inf inf inf]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R84pGVrPDRV",
        "colab_type": "text"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence"
      ]
    }
  ]
}