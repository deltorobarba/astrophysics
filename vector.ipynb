{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Tk6qnMZUgGQw",
        "yHDbqYQahc-z",
        "WU52z34N2tbD",
        "TPO3O0LTlUVH",
        "IAEtyEp7j7wT",
        "H_qMVeTtrLls",
        "vy1c3rSRLsnY",
        "IbwJ8QbNvEbX",
        "JigccOCnE4NL",
        "HMmIn-HUEykz",
        "t88lRS-5rsqg",
        "EA2aVjJjsGop",
        "XuNsKvz2FWMd",
        "pyPv9Bs5smjo",
        "ZimudeCftX__",
        "3OQ7eTBayshC",
        "oaJIGJNzzymu",
        "Bfkc_-jIFtmU",
        "sZzSya6rSLjr",
        "P9f2lq9s4tJr",
        "67gIZXbG5db3",
        "O3mIl8Xj8Dfs",
        "Y-1lpMAgXzD9",
        "RJ7r2x6ubuF8",
        "PKnoH_tc2luZ",
        "U48kLNEgXDd7",
        "4Bu4c3RgXHan",
        "16P02MVt5VFP",
        "lZucZX5uoQOG",
        "Zppo7qtzowMx",
        "be1delop5C-L",
        "hryEPtLGk31L",
        "rJynYy2j0olt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "# **Moduln and Vector Spaces (Linear Algebra)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U5_6xH7tAQq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk6qnMZUgGQw"
      },
      "source": [
        "#### **Moduln**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRPnBpNgMT4"
      },
      "source": [
        "* Ein Modul ist ein n-dimensionaler Ring.\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativität)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where “scalar multiplication” refers to multiplication by integer scalars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMh1i2RczFjo"
      },
      "source": [
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$ \n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$ \n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8NSS1C9gIni"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD4HgiwtO7x5"
      },
      "source": [
        "**Basis eines Modul**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sneOVEwOPEXn"
      },
      "source": [
        "* Der Begriff der Basis eines Moduls ist im mathematischen Teilgebiet der Algebra eine Verallgemeinerung des Begriffes der Basis eines Vektorraumes. \n",
        "\n",
        "* Wie bei diesen wird eine Basis eines Moduls als linear unabhängiges Erzeugendensystem definiert; im Gegensatz zu Vektorräumen besitzt allerdings nicht jeder Modul eine Basis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKb22bUqO_ae"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Basis_(Modul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3psQpbPHh5"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Basis_(Vektorraum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHDbqYQahc-z"
      },
      "source": [
        "#### **Numerische lineare Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alPE_R99yVQF"
      },
      "source": [
        "https://www.quantamagazine.org/new-algorithm-breaks-speed-limit-for-solving-linear-equations-20210308/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgBeSXhna8t"
      },
      "source": [
        "Die [numerische lineare Algebra](https://de.m.wikipedia.org/wiki/Numerische_lineare_Algebra) ist ein zentrales Teilgebiet der numerischen Mathematik. Sie beschäftigt sich mit der Entwicklung und der Analyse von Rechenverfahren (Algorithmen) für Problemstellungen der linearen Algebra, insbesondere der Lösung von [linearen Gleichungssystemen](https://de.m.wikipedia.org/wiki/Lineares_Gleichungssystem) und [Eigenwertproblemen](https://de.m.wikipedia.org/wiki/Eigenwertproblem). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHrBkfoD4nwD"
      },
      "source": [
        "1. Lösung linearer Gleichungen / Matrizen\n",
        "\n",
        "2. Berechnung von Eigenwerten\n",
        "\n",
        "3. Reduzierung der Grösse von Matrizen (für Recommender zB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGksDOvKkFom"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Zufallsmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU52z34N2tbD"
      },
      "source": [
        "###### <font color=\"blue\">**Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUp1jC-6t1Je"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8kfpHWeyQMO"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Numerische_lineare_Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7eQdTcMB0gp"
      },
      "source": [
        "* **Inverse: benutzt man als [Matrix Solution](https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution) zur Lösung linearer Gleichungssysteme**. man kann hier die Pseudoinverse [Moore-Penrose](https://en.m.wikipedia.org/wiki/Moore–Penrose_inverse) nutzen (QR, Cholesky, Rank decomposition, Singular Value decomposition). 'The pseudoinverse provides a least squares solution to a system of linear equations.' Siehe auch [Linear Least Squares](https://en.m.wikipedia.org/wiki/Linear_least_squares) in bezug auf Moore-Penrose.\n",
        "\n",
        "* **Other methods to solve linear equations**: While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. \n",
        "  \n",
        "  * Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as [pivoting](https://en.m.wikipedia.org/wiki/Pivot_element). \n",
        "  \n",
        "  * Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the [LU decomposition](https://en.m.wikipedia.org/wiki/LU_decomposition) of the matrix A.\n",
        "\n",
        "  * If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the [Cholesky decomposition](https://en.m.wikipedia.org/wiki/Cholesky_decomposition). [Levinson recursion](https://en.m.wikipedia.org/wiki/Levinson_recursion) is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called [sparse matrices](https://en.m.wikipedia.org/wiki/Sparse_matrix)), which appear often in applications.\n",
        "  \n",
        "  * A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of [iterative methods](https://en.m.wikipedia.org/wiki/Iterative_method). For some sparse matrices, the introduction of randomness improves the speed of the iterative methods.\n",
        "  \n",
        "  * There is also a [quantum algorithm for linear systems of equations](https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations).\n",
        "\n",
        "* **Matrix Factorization**: sehr grosse Matrizen kleiner machen (unter möglichen Verlust von Informationen). zum Beispiel uber PCA. sehr häufig genutzt bei Recommender Systems.\n",
        "\n",
        "* **Eigenwertberechnung**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmz1SzUkinQK"
      },
      "source": [
        "https://m.youtube.com/watch?v=e50Bj7jn9IQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-hoOe8hipfW"
      },
      "source": [
        "https://m.youtube.com/watch?v=PFDu9oVAE-g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-O3ZtinirtN"
      },
      "source": [
        "https://m.youtube.com/watch?v=eJWgKvrhDmE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVGuT-N3i2oc"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Time_complexity#Polylogarithmic_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGhOHqtZi4bX"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Ewin_Tang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEAydbXD25-F"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Low-rank_matrix_approximations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPWQVNzQ21BS"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Normalform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_65XrtjC2zJR"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Potenzmethode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX4hU1-JSQFH"
      },
      "source": [
        "**Liste numerischer Verfahren fur lineare Gleichungssysteme**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XXr15dpSTlv"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Liste_numerischer_Verfahren#Lineare_Gleichungssysteme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncUJUElS0kc"
      },
      "source": [
        "* Gaußsches Eliminationsverfahren (bzw. LR-Zerlegung): Ein klassisches direktes Verfahren – für große Matrizen allerdings zu aufwändig.\n",
        "\n",
        "* Cholesky-Zerlegung: Für symmetrische positiv definite Matrizen kann ähnlich wie die LR-Zerlegung eine symmetrische Zerlegung erstellt werden bei halbem Aufwand.\n",
        "\n",
        "* QR-Zerlegung: Ebenfalls ein direktes Verfahren mit mindestens doppelter Laufzeit im Vergleich zum Gauß-Verfahren aber besseren Stabilitätseigenschaften. Umgesetzt mittels Householdertransformationen ist besonders für lineare [Ausgleichsprobleme (Ausgleichsrechnung)](https://de.m.wikipedia.org/wiki/Ausgleichungsrechnung) geeignet.\n",
        "\n",
        "* Splitting-Verfahren: Klassische iterative Verfahren.\n",
        "\n",
        "  * Gauß-Seidel-Verfahren: Wird auch als Einzelschrittverfahren bezeichnet.\n",
        "\n",
        "  * Jacobi-Verfahren: Wird auch als Gesamtschrittverfahren bezeichnet.\n",
        "\n",
        "  * Richardson-Verfahren\n",
        "\n",
        "  * Tschebyschow-Iteration: ein Splitting-Verfahren mit zusätzlicher Beschleunigung\n",
        "\n",
        "  * SOR-Verfahren\n",
        "\n",
        "  * SSOR-Verfahren\n",
        "\n",
        "* Iterative Refinement: Iterative Verbesserung eines direkten Verfahrens, Beziehung zur Grundidee der Krylow-Unterraum-Verfahren\n",
        "\n",
        "* Krylow-Unterraum-Verfahren: Moderne iterative Verfahren, die für große, dünnbesetzte Gleichungssysteme gedacht sind. Wichtiger Spezialfall für symmetrisch positiv definite Probleme ist das Verfahren der konjugierten Gradienten.\n",
        "\n",
        "* Mehrgitterverfahren: Ein modernes Verfahren mit linearer Komplexität speziell für Gleichungssysteme, die von partiellen Differentialgleichungen herrühren.\n",
        "\n",
        "* Vorkonditionierung: Eine Technik, die Kondition einer Matrix in Krylow-Unterraum-Verfahren zu verbessern.\n",
        "\n",
        "* ILU-Zerlegung: Ein wichtiges Vorkonditionierungsverfahren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPO3O0LTlUVH"
      },
      "source": [
        "###### <font color=\"blue\">**Prerequisites: Inverse Matrix, Diagonalmatrix & Jordan Normalform**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h33dxdyuvkWf"
      },
      "source": [
        "**Diagonalmatrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxmwHSpnoRY4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diagonalmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzHv1nEE3zx"
      },
      "source": [
        "**Jordan Normalform**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT-CwWcQGvzs"
      },
      "source": [
        "> Für jede lineare Abbildung eines endlichdimensionalen Vektorraums, deren charakteristisches Polynom vollständig in Linearfaktoren zerfällt, kann eine Vektorraumbasis gewählt werden, so dass die Abbildungsmatrix, die die Abbildung bezüglich dieser Basis beschreibt, [jordansche Normalform](https://de.m.wikipedia.org/wiki/Jordansche_Normalform) hat.\n",
        "\n",
        "* Die jordansche Normalform ist ein einfacher Vertreter der Äquivalenzklasse der zu einer trigonalisierbaren Matrix ähnlichen Matrizen. Die Trigonalisierbarkeit ist gleichbedeutend damit, dass das charakteristische Polynom der Matrix vollständig in Linearfaktoren zerfällt. \n",
        "\n",
        "* Matrizen über einem algebraisch abgeschlossenen Körper sind immer trigonalisierbar und daher immer ähnlich einer jordanschen Normalform.\n",
        "\n",
        "* Genutzt u.a. in der Lösung linearer Differentialgleichungen\n",
        "\n",
        "* Die [Diagonalisierung (Diagonalmatrix)](https://de.m.wikipedia.org/wiki/Diagonalmatrix) ist ein Spezialfall der jordanschen Normalform, während die jordansche Normalform ein Spezialfall der [Weierstraß-Normalform (Frobenius-Normalform)](https://de.m.wikipedia.org/wiki/Frobenius-Normalform) ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOPL22AlE7kJ"
      },
      "source": [
        "Voraussetzungen: \n",
        "\n",
        "* Diagonalmatrix, charakteristisches Polynom, Eigenwerte & Eigenräume, geometrische und algebraische Vielfachheit, Transformationsmatrix, Basiswechsel\n",
        "\n",
        "* Algebraische Vielfachheit: Anzahl gleicher Eigenwerte\n",
        "\n",
        "Herangehensweise:\n",
        "\n",
        "1. Eigenwerte bestimmen\n",
        "\n",
        "2. Eigenräume bestimmen Kern (A - λ * I)\n",
        "\n",
        "3. Hauptvektoren bestimmen\n",
        "\n",
        "4. Transformationsmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-A5mnjE6EH"
      },
      "source": [
        "https://youtu.be/83SgQJekeuk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmX7ZGSoF4n-"
      },
      "source": [
        "https://youtu.be/hPAQdmEPU_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dUwHXhovJSr"
      },
      "source": [
        "**Inverse Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjk8t7RClXmn"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Inverse_Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlHP11vEMa98"
      },
      "source": [
        "**Pseudoinverse (Moore-Penrose Inverse)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf8iDGk5vQpY"
      },
      "source": [
        "* The term generalized inverse is sometimes used as a synonym for pseudoinverse.\n",
        "\n",
        "* A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution (see below under § Applications). Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.\n",
        "\n",
        "* The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers. It can be computed using the singular value decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnODBx_k5vh"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Generalized_inverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aYCDBu3k4LR"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Moore–Penrose_inverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbg2aHY0MfIk"
      },
      "source": [
        "* The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular matrices where the number of rows and columns are not equal.\n",
        "\n",
        "* It is also called the the Moore-Penrose Inverse after two independent discoverers of the method or the Generalized Inverse.\n",
        "\n",
        "* Matrix inversion is not defined for matrices that are not square. When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n",
        "\n",
        "* The pseudoinverse is denoted as A^+, where A is the matrix that is being inverted and + is a superscript. The pseudoinverse is calculated using the singular value decomposition of A:\n",
        "\n",
        "> A^+ = VD^+U^T\n",
        "\n",
        "Where A^+ is the pseudoinverse, D^+ is the pseudoinverse of the diagonal matrix Sigma and U^T is the transpose of U.\n",
        "\n",
        "We can get U and V from the SVD operation.\n",
        "\n",
        "> A = U . Sigma . V^T\n",
        "\n",
        "The D^+ can be calculated by creating a diagonal matrix from Sigma, calculating the reciprocal of each non-zero element in Sigma, and taking the transpose if the original matrix was rectangular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVHOiaDOM_YE"
      },
      "source": [
        "#          s11,   0,   0\n",
        "# Sigma = (  0, s22,   0)\n",
        "#            0,   0, s33"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W5wTVWQNGss"
      },
      "source": [
        "#        1/s11,     0,     0\n",
        "# D^+ = (    0, 1/s22,     0)\n",
        "#            0,     0, 1/s33"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPUAqEstNMMS"
      },
      "source": [
        "The pseudoinverse provides one way of solving the linear regression equation, specifically when there are more rows than there are columns, which is often the case. \n",
        "\n",
        "NumPy provides the function pinv() for calculating the pseudoinverse of a rectangular matrix. The example below defines a 4×2 matrix and calculates the pseudoinverse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-FzJLHNSWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a83b8e52-c022-4bf4-8249-7449196cff07"
      },
      "source": [
        "# Pseudoinverse\n",
        "from numpy import array\n",
        "from numpy.linalg import pinv\n",
        "\n",
        "# define matrix\n",
        "A = array([\n",
        "\t[0.1, 0.2],\n",
        "\t[0.3, 0.4],\n",
        "\t[0.5, 0.6],\n",
        "\t[0.7, 0.8]])\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.2]\n",
            " [0.3 0.4]\n",
            " [0.5 0.6]\n",
            " [0.7 0.8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFzXXFwQNWlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "02766498-1982-4382-de4c-d3d8f70dbbd5"
      },
      "source": [
        "# calculate pseudoinverse\n",
        "B = pinv(A)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.00000000e+01 -5.00000000e+00  1.42385628e-14  5.00000000e+00]\n",
            " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAEtyEp7j7wT"
      },
      "source": [
        "###### <font color=\"blue\">**Klassische Row Reduction (Gaußverfahren, Gauß-Jordan-Algorithmus, Pivotisierung) & Cramer's Rule**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOJ_tst-oau4"
      },
      "source": [
        "For kleine lineare Gleichungssysteme / Matrizen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IQ_Et5mXQ7"
      },
      "source": [
        "**Gaußsches Eliminationsverfahren**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9p-GuskJtM"
      },
      "source": [
        "* Das [gaußsche Eliminationsverfahren](https://de.m.wikipedia.org/wiki/Gaußsches_Eliminationsverfahren) oder einfach Gauß-Verfahren (nach Carl Friedrich Gauß) ist ein Algorithmus aus den mathematischen Teilgebieten der linearen Algebra und der Numerik. \n",
        "\n",
        "* Es ist ein wichtiges Verfahren zum Lösen von linearen Gleichungssystemen und beruht darauf, dass elementare Umformungen zwar das Gleichungssystem ändern, aber die Lösung erhalten. \n",
        "\n",
        "* Dies erlaubt es, jedes eindeutig lösbare Gleichungssystem auf Stufenform zu bringen, an der die Lösung durch sukzessive Elimination der Unbekannten leicht ermittelt oder die Lösungsmenge abgelesen werden kann.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpooYugQofby"
      },
      "source": [
        "**Pivotisierung**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1dNR_F0olVB"
      },
      "source": [
        "* Das [Pivotelement](https://de.m.wikipedia.org/wiki/Pivotelement) (vom Französischen pivot ‚Dreh-/Angelpunkt‘; im Militärjargon bezeichnet Pivot den Flügelmann einer Formation bei einem Wendemanöver) ist dasjenige Element einer Zahlenmenge, das als Erstes von einem Algorithmus (z. B. gaußsche Eliminationsverfahren, Quicksort oder Basisaustauschverfahren) ausgewählt wird, um bestimmte Berechnungen durchzuführen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-rluszgmUOz"
      },
      "source": [
        "**Gauß-Jordan-Algorithmus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ8S8CIEmcVY"
      },
      "source": [
        "* Der [Gauß-Jordan-Algorithmus](https://de.m.wikipedia.org/wiki/Gauß-Jordan-Algorithmus) ist ein Algorithmus aus den mathematischen Teilgebieten der linearen Algebra und Numerik. Mit dem Verfahren lässt sich die Lösung eines linearen Gleichungssystems berechnen. \n",
        "\n",
        "* Es ist eine Erweiterung des gaußschen Eliminationsverfahrens, bei dem in einem zusätzlichen Schritt das Gleichungssystem bzw. dessen erweiterte Koeffizientenmatrix auf die reduzierte Stufenform gebracht wird. \n",
        "\n",
        "* Daraus lässt sich dann die Lösung direkt ablesen. Außerdem kann der Gauß-Jordan-Algorithmus zur Berechnung der Inversen einer Matrix verwendet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaSk-ZB8uHCW"
      },
      "source": [
        "**Cramer's rule**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3Dhyg9uKmD"
      },
      "source": [
        "Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STnZ9j2SuQzU"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/System_of_linear_equations#Cramer's_rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2yydnhyuNXo"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Cramer%27s_rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_qMVeTtrLls"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition): Overview (Linear Equations & Eigenvalues)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTrwO2W3rUmd"
      },
      "source": [
        "In the mathematical discipline of linear algebra, a [matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition) or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1c3rSRLsnY"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Linear Equations: LU Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ms9XQkR0Ts"
      },
      "source": [
        "**Characteristics of an LU Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDrmirVPSXe"
      },
      "source": [
        "The LU decomposition is often used to simplify the **solving of systems of linear equations**, such as **finding the coefficients in a linear regression**, as well as in **calculating the determinant and inverse** of a matrix.\n",
        "\n",
        "* Lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix. \n",
        "\n",
        "* The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian elimination. \n",
        "\n",
        "* Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fK7BDsnHRgk"
      },
      "source": [
        "The **LU decomposition is for square matrices** and decomposes a matrix into L and U components. Let A be a square matrix. An LU factorization refers to the factorization of A, with proper row and/or column orderings or permutations, into two factors – a **lower triangular matrix L** and an **upper triangular matrix U**:\n",
        "\n",
        "> A = L U\n",
        "\n",
        "* The LU decomposition is found using an <u>iterative numerical process</u> and **can fail for those matrices that cannot be decomposed or decomposed easily**.\n",
        "\n",
        "* In the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero. For example, for a 3 × 3 matrix A, its LU decomposition looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0NjEcfOQBFu"
      },
      "source": [
        "> $\\left[\\begin{array}{lll}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
        "u_{11} & u_{12} & u_{13} \\\\\n",
        "0 & u_{22} & u_{23} \\\\\n",
        "0 & 0 & u_{33}\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcKH3CXHSPTV"
      },
      "source": [
        "**Underdeterminism & Unit Triangular Matrix**\n",
        "\n",
        "* Sometimes **equations is [underdetermined](https://en.m.wikipedia.org/wiki/Underdetermined_system)**. In this case any two non-zero elements of L and U matrices are parameters of the solution and can be set arbitrarily to any non-zero value. \n",
        "\n",
        "* Therefore, to find the unique LU decomposition, it is **necessary to put some restriction on L and U matrices**. For example, we can conveniently require the lower triangular matrix L to be a **unit triangular matrix** (i.e. set all the entries of its main diagonal to ones). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wrOC5iYU6jk"
      },
      "source": [
        "**Square matrices**\n",
        "\n",
        "* Any square matrix A admits an LUP factorization. If A is [invertible](https://en.m.wikipedia.org/wiki/Invertible_matrix), then it admits an LU (or LDU) factorization if and only if all its leading principal [minors](https://en.m.wikipedia.org/wiki/Minor_(linear_algebra)) are nonzero. \n",
        "\n",
        "* If A is a singular matrix of rank k, then it admits an LU factorization if the first k leading principal minors are nonzero, although the converse is not true.\n",
        "\n",
        "* If a square, invertible matrix has an LDU (factorization with all diagonal entries of L and U equal to 1), then the factorization is unique. In that case, the LU factorization is also unique if we require that the diagonal of L (or U) consists of ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdqfrMkFVmOO"
      },
      "source": [
        "**Symmetric positive definite matrices**\n",
        "\n",
        "* If A is a symmetric (or [Hermitian](https://en.m.wikipedia.org/wiki/Hermitian_matrix), if A is complex) [positive definite](https://en.m.wikipedia.org/wiki/Definite_symmetric_matrix) matrix, we can arrange matters so that U is the [conjugate transpose](https://en.m.wikipedia.org/wiki/Conjugate_transpose) of L. That is, we can write A as\n",
        "\n",
        "> A = LL*\n",
        "\n",
        "* This decomposition is called the **Cholesky decomposition**. The Cholesky decomposition always exists and is unique — provided the matrix is positive definite. \n",
        "\n",
        "* Furthermore, computing the Cholesky decomposition is more efficient and [numerically more stable](https://en.m.wikipedia.org/wiki/Numerical_stability) than computing some other LU decompositions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoLAEHYLVf7q"
      },
      "source": [
        "**General matrices**\n",
        "\n",
        "* For a (not necessarily invertible) matrix over any field, the exact necessary and sufficient conditions under which it has an LU factorization are known. \n",
        "\n",
        "* The conditions are expressed in terms of the ranks of certain submatrices. The Gaussian elimination algorithm for obtaining LU decomposition has also been extended to this most general case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NScrUC_NTUqh"
      },
      "source": [
        "**Variations of LU Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARKrYJtXUFV1"
      },
      "source": [
        "**LU factorization with partial pivoting (LUP Decomposition)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJYuQ1qSz8n"
      },
      "source": [
        "A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the **LU decomposition with partial pivoting**.\n",
        "\n",
        "> A = P L U\n",
        "\n",
        "The rows of the parent matrix are re-ordered to simplify the decomposition process and the **additional P matrix specifies a way to permute the result or return the result to the original order**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvQWQRRBUYUL"
      },
      "source": [
        "It turns out that a proper permutation in rows (or columns) is sufficient for LU factorization. LU factorization with partial pivoting (LUP) refers often to LU factorization with row permutations only:\n",
        "\n",
        "> PA = LU\n",
        "\n",
        "where L and U are again lower and upper triangular matrices, and P is a [permutation matrix](https://en.m.wikipedia.org/wiki/Permutation_matrix)*, which, when left-multiplied to A, reorders the rows of A. It turns out that all square matrices can be factorized in this form, and the factorization is numerically stable in practice. This makes LUP decomposition a useful technique in practice.\n",
        "\n",
        "*A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column and 0s elsewhere.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHbBijklUI1_"
      },
      "source": [
        "**LU factorization with full pivoting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpDBr0MUNdO"
      },
      "source": [
        "An LU factorization with full pivoting involves both row and column permutations:\n",
        "\n",
        "> PAQ = LU\n",
        "\n",
        "where L, U and P are defined as before, and Q is a permutation matrix that reorders the columns of A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtM3ZbIRQ_ZV"
      },
      "source": [
        "**LDU Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVfgCceCRmCu"
      },
      "source": [
        "An LDU decomposition is a decomposition of the form\n",
        "\n",
        "> A = LDU\n",
        "\n",
        "where D is a diagonal matrix, and L and U are unitriangular matrices, meaning that all the entries on the diagonals of L and U are one.\n",
        "\n",
        "Below we required that A be a square matrix, but these decompositions can all be generalized to rectangular matrices as well. In that case, **L and D are square matrices** both of which have the same number of rows as A, and U has exactly the same dimensions as A. Upper triangular should be interpreted as having only zero entries below the main diagonal, which starts at the upper left corner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgwgHKNRRCpn"
      },
      "source": [
        "![LDU decomposition of a Walsh matrix](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/LDU_decomposition_of_Walsh_16.svg/640px-LDU_decomposition_of_Walsh_16.svg.png)\n",
        "\n",
        "*LDU decomposition of a [Walsh matrix](https://en.m.wikipedia.org/wiki/Walsh_matrix)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TfIjoZITagA"
      },
      "source": [
        "**Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNuYRYpUJUHQ"
      },
      "source": [
        "**Define Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LQILavQHeHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5341ad66-d2c7-4da8-b37a-b578d5ad81b6"
      },
      "source": [
        "# LU decomposition\n",
        "from numpy import array\n",
        "\n",
        "# define a square matrix\n",
        "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxqLQ4E1JVHe"
      },
      "source": [
        "**Decompose**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASjtKDTNHm7c"
      },
      "source": [
        "# LU decomposition\n",
        "from scipy.linalg import lu\n",
        "P, L, U = lu(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppDt7HJZJeP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59871b78-482c-4d17-ce94-ae4941026011"
      },
      "source": [
        "print(P)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzlZ482zJdIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e192dc6c-3b26-42f0-a6ea-bdbde34877fe"
      },
      "source": [
        "print(L)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.        ]\n",
            " [0.14285714 1.         0.        ]\n",
            " [0.57142857 0.5        1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opCE0DFbJcPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8142a81-52e7-406a-bf90-d0daefce7b4f"
      },
      "source": [
        "print(U)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7.         8.         9.        ]\n",
            " [0.         0.85714286 1.71428571]\n",
            " [0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KZgBdoJXJO"
      },
      "source": [
        "**Reconstruct**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcDSYwtgHtLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7947fb18-8f6e-4ff9-bcc5-26196e7845fe"
      },
      "source": [
        "B = P.dot(L).dot(U)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcfF-RJzpu1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9706c0-8a9c-4198-c294-a2cb9ee6db32"
      },
      "source": [
        "# Check for differences between both matrices\n",
        "X = B - A\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwJ8QbNvEbX"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Linear Equations: Rank Factorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isHJ8l3OR46n"
      },
      "source": [
        "* given an m × n matrix A of rank r, a rank decomposition or rank factorization of A is a factorization of A of the form A = C F, where C is an m × r matrix and F is an r × n matrix.\n",
        "\n",
        "* Every finite-dimensional matrix has a rank decomposition\n",
        "\n",
        "* One can also construct a full rank factorization of A by using its singular value decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JigccOCnE4NL"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Linear Equations: Cholesky Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KPidT29-G8L"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Cholesky_decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO44vtYNGQEG"
      },
      "source": [
        "The Cholesky decomposition is for square symmetric matrices where all eigenvalues are greater than zero, so-called [positive definite matrices](https://en.wikipedia.org/wiki/Definite_symmetric_matrix). For our interests in machine learning, we will focus on the Cholesky decomposition for real-valued matrices and ignore the cases when working with complex numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFeymed4KIyz"
      },
      "source": [
        "> A = LL^T\n",
        "\n",
        "Where A is the matrix being decomposed, L is the lower triangular matrix and L^T is the transpose of L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTrHPrDNKArW"
      },
      "source": [
        "The decompose can also be written as the product of the upper triangular matrix, for example:\n",
        "\n",
        "> A = U^T . U\n",
        "\n",
        "* Where U is the upper triangular matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8EITyoKDXA"
      },
      "source": [
        "* The Cholesky decomposition is used for solving linear least squares for linear regression, as well as simulation and optimization methods.\n",
        "\n",
        "* When decomposing symmetric matrices, the Cholesky decomposition is nearly twice as efficient as the LU decomposition and should be preferred in these cases.\n",
        "\n",
        "* While symmetric, positive definite matrices are rather special, they occur quite frequently in some applications, so their special factorization, called Cholesky decomposition, is good to know about. When you can use it, Cholesky decomposition is about a factor of two faster than alternative methods for solving linear equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ2IZ_HMJtd3"
      },
      "source": [
        "**Define Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xcgUf1YHGae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1c772a76-1052-456d-bf61-395929265ac4"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "# define a 3x3 matrix\n",
        "A = array([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 1 1]\n",
            " [1 2 1]\n",
            " [1 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_8K4pHHJpao"
      },
      "source": [
        "**Decompose**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXS0q5LGHHhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2bb8881d-60de-4c4d-8fc5-cadc18f14cff"
      },
      "source": [
        "# Cholesky decomposition\n",
        "from numpy.linalg import cholesky\n",
        "L = cholesky(A)\n",
        "print(L)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.41421356 0.         0.        ]\n",
            " [0.70710678 1.22474487 0.        ]\n",
            " [0.70710678 0.40824829 1.15470054]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDSSrB8sJmZ4"
      },
      "source": [
        "**Reconstruct**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbmN6r5MHMIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2dd37a0f-bb48-4694-c6e5-e3b418931ff8"
      },
      "source": [
        "B = L.dot(L.T)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2. 1. 1.]\n",
            " [1. 2. 1.]\n",
            " [1. 1. 2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMmIn-HUEykz"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Linear Equations: QR Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N597Z0rc93t1"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/QR-Algorithmus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NVXqSfFlBt"
      },
      "source": [
        "* The QR decomposition is for m x n matrices (not limited to square matrices) and decomposes a matrix into Q and R components.\n",
        "\n",
        "> A = Q R\n",
        "\n",
        "* Where A is the matrix that we wish to decompose, Q a matrix with the size m x m, and R is an upper triangle matrix with the size m x n.\n",
        "\n",
        "* **Q is an orthogonal (Q<sup>T</sup> Q = I) or unitary matrix** (Q ∗ Q = I) and **R is an upper triangular matrix**. The QR decomposition is a special case of the [Iwasawa decomposition](https://en.m.wikipedia.org/wiki/Iwasawa_decomposition).\n",
        "\n",
        "* The QR decomposition is found using an iterative numerical method that can fail for those matrices that cannot be decomposed, or decomposed easily.\n",
        "\n",
        "* Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, although is not limited to square matrices.\n",
        "\n",
        "* By default, the qr function returns the Q and R matrices with smaller or ‘reduced’ dimensions that is more economical. We can change this to return the expected sizes of m x m for Q and m x n for R by specifying the mode argument as ‘complete’, although this is not required for most applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOhv-GS8xXZ4"
      },
      "source": [
        "Such a decomposition always exists and can be calculated using various algorithms. The best known are\n",
        "\n",
        "* [Householder transformations](https://de.m.wikipedia.org/wiki/Householdertransformation)\n",
        "* [Givens rotations](https://de.m.wikipedia.org/wiki/Givens-Rotation)\n",
        "* [Gram-Schmidtsch's orthogonalization method](https://de.m.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1JvWMjiJHYY"
      },
      "source": [
        "**Define Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfu-RgIXFxTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5bf6a6ef-59d2-44ba-f418-a2925cdc3400"
      },
      "source": [
        "from numpy import array\n",
        "\n",
        "# define a 3x2 matrix\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VzIv_nJJhw"
      },
      "source": [
        "**Decompose**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1KWNdgFyXz"
      },
      "source": [
        "# QR decomposition\n",
        "from numpy.linalg import qr\n",
        "Q, R = qr(A, 'complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7deKMpRJCsX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2c85ca61-1303-4ce9-ba8e-62a5279d64bc"
      },
      "source": [
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.16903085  0.89708523  0.40824829]\n",
            " [-0.50709255  0.27602622 -0.81649658]\n",
            " [-0.84515425 -0.34503278  0.40824829]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNw-k4UBI-oo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "58fe955f-07c2-4213-c87a-9a2257bd4808"
      },
      "source": [
        "print(R)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-5.91607978 -7.43735744]\n",
            " [ 0.          0.82807867]\n",
            " [ 0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3DercV9JMmm"
      },
      "source": [
        "**Reconstruct**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjR2dwOdF1cS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ca36248c-b4c2-43d2-bd53-9e94e4a5f8a1"
      },
      "source": [
        "B = Q.dot(R)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2.]\n",
            " [3. 4.]\n",
            " [5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t88lRS-5rsqg"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Eigenvalues: Eigendecomposition (spectral decomposition)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfzdfQ14r647"
      },
      "source": [
        "[Summary of Eigendecomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition#Decompositions_based_on_eigenvalues_and_related_concepts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIT2n59wsir"
      },
      "source": [
        "See also article about [Eigenvalue algorithms (Overview & List)](https://en.m.wikipedia.org/wiki/Eigenvalue_algorithm) and [List of Eigenvalue Algorithms](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Eigenvalue_algorithms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooWX-rgS12w"
      },
      "source": [
        "* [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.\n",
        "\n",
        "* Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial. However, this is often impossible for larger matrices, in which case we must use a numerical method. In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative.\n",
        "\n",
        "  * Iterative numerical algorithms for approximating roots of polynomials exist, such as **Newton's method**, but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients.\n",
        "\n",
        "  * A simple and accurate iterative method is the [power iteration method](https://en.m.wikipedia.org/wiki/Power_iteration): a random vector v is chosen and a sequence of unit vectors is computed as $\\frac{\\mathbf{A} \\mathbf{v}}{\\|\\mathbf{A} \\mathbf{v}\\|}, \\frac{\\mathbf{A}^{2} \\mathbf{v}}{\\left\\|\\mathbf{A}^{2} \\mathbf{v}\\right\\|}, \\frac{\\mathbf{A}^{3} \\mathbf{v}}{\\left\\|\\mathbf{A}^{3} \\mathbf{v}\\right\\|}, \\ldots$. This sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). **This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine.**\n",
        "  \n",
        "  * Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration. Alternatively, the important QR algorithm is also based on a subtle transformation of a power method.\n",
        "\n",
        "  * Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equation $\\left(\\mathbf{A}-\\lambda_{i} \\mathbf{I}\\right) \\mathbf{v}_{i, j}=\\mathbf{0}$ using Gaussian elimination or any other method for solving matrix equations.\n",
        "\n",
        "* However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration, for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector). In the QR algorithm for a Hermitian matrix (or any normal matrix), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm. (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure.) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbHJxGY00hK1"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Iterative_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDiA89Z0W9J"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Power_iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c4SFfsI1In0"
      },
      "source": [
        "* The power iteration method is especially suitable for sparse matrices\n",
        "\n",
        "* Some of the more advanced eigenvalue algorithms can be understood as variations of the power iteration. For instance, the inverse iteration method applies power iteration to the matrix $A^{-1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JXbEVje0YnI"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/PageRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA2aVjJjsGop"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Eigenvalues: Schur Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek4RzwAasN8c"
      },
      "source": [
        "Schur decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luGhrlcj9xb1"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Schur-Zerlegung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-LX0vGsQZh"
      },
      "source": [
        "Real Schur decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkITqyuosTTP"
      },
      "source": [
        "QZ decomposition (Generalized Schur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-uIAcCsYBz"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Matrix_decomposition#Schur_decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNsKvz2FWMd"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Eigenvalues: Singular-Value Decomposition (SVD)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Q-0RQL-Mw8"
      },
      "source": [
        "https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBhAyNBEvZP1"
      },
      "source": [
        "singular value decomposition can be used to compute the moore-penrose-inverse (to solvw linear equations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE9_30u0HvKn"
      },
      "source": [
        "* Matrix decomposition, also known as matrix factorization, involves describing a given matrix using its constituent elements.\n",
        "\n",
        "* Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes it more stable than other methods, such as the eigendecomposition. As such, it is often used in a wide array of applications including compressing, denoising, and data reduction.\n",
        "\n",
        "* **The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.**\n",
        "\n",
        "* For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers.\n",
        "\n",
        "> A = U . Sigma . V^T\n",
        "\n",
        "* Where A is the real m x n matrix that we wish to decompose, U is an m x m matrix, Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix, and V^T is the  transpose of an n x n matrix where T is a superscript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Ja69_DICya"
      },
      "source": [
        "* The diagonal values in the Sigma matrix are known as the singular values of the original matrix A. The columns of the U matrix are called the left-singular vectors of A, and the columns of V are called the right-singular vectors of A.\n",
        "\n",
        "* The SVD is calculated via iterative numerical methods. We will not go into the details of these methods. Every rectangular matrix has a singular value decomposition, although the resulting matrices may contain complex numbers and the limitations of floating point arithmetic may cause some matrices to fail to decompose neatly.\n",
        "\n",
        "* The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, the SVD is more generally applicable.\n",
        "\n",
        "* The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.\n",
        "\n",
        "* The SVD can be calculated by calling the svd() function. The function takes a matrix and returns the U, Sigma and V^T elements. The Sigma diagonal matrix is returned as a vector of singular values. The V matrix is returned in a transposed form, e.g. V.T."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kspDVKumIjj0"
      },
      "source": [
        "**Define a Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0UN2E9NIUpX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5e9952aa-35ae-4a25-d3f9-245750abae97"
      },
      "source": [
        "from numpy import array\n",
        "A = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwUy0XTnKjN4"
      },
      "source": [
        "**Decompose**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAOV_EhHIXpN"
      },
      "source": [
        "# Calculate Singular-Value Decomposition\n",
        "from scipy.linalg import svd\n",
        "U, s, VT = svd(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWbLmTykKr3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "13bd262e-ad31-4604-817e-a6c6d1cc7b1c"
      },
      "source": [
        "print(U)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.2298477   0.88346102  0.40824829]\n",
            " [-0.52474482  0.24078249 -0.81649658]\n",
            " [-0.81964194 -0.40189603  0.40824829]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em-H1JnUKqjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b117372-77b1-4761-dd2e-47294ada0c88"
      },
      "source": [
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.52551809 0.51430058]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YCCCYlaKpOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "af5df3cf-6081-4014-f196-16c1e64b01a7"
      },
      "source": [
        "print(VT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.61962948 -0.78489445]\n",
            " [-0.78489445  0.61962948]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDEvqFwaIl7k"
      },
      "source": [
        "**Reconstruct**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLPp9hJELNMv"
      },
      "source": [
        "* The original matrix can be reconstructed from the U, Sigma, and V^T elements.\n",
        "* The U, s, and V elements returned from the svd() cannot be multiplied directly.\n",
        "* The s vector must be converted into a diagonal matrix using the diag() function. By default, this function will create a square matrix that is n x n, relative to our original matrix. This causes a problem as the size of the matrices do not fit the rules of matrix multiplication, where the number of columns in a matrix must match the number of rows in the subsequent matrix.\n",
        "* After creating the square Sigma diagonal matrix, the sizes of the matrices are relative to the original m x n matrix that we are decomposing, as follows:\n",
        "\n",
        "> U (m x m) . Sigma (n x n) . V^T (n x n)\n",
        "\n",
        "* Where, in fact, we require:\n",
        "\n",
        "> U (m x m) . Sigma (m x n) . V^T (n x n)\n",
        "\n",
        "* We can achieve this by creating a new Sigma matrix of all zero values that is m x n (e.g. more rows) and populate the first n x n part of the matrix with the square diagonal matrix calculated via diag()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fWCag1TLzQv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7c546b29-8fcb-4f2b-a11c-0cec844b2245"
      },
      "source": [
        "from numpy import diag\n",
        "from numpy import dot\n",
        "from numpy import zeros\n",
        "\n",
        "# create m x n Sigma matrix\n",
        "Sigma = zeros((A.shape[0], A.shape[1]))\n",
        "\n",
        "# populate Sigma with n x n diagonal matrix\n",
        "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
        "\n",
        "# reconstruct matrix\n",
        "B = U.dot(Sigma.dot(VT))\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2.]\n",
            " [3. 4.]\n",
            " [5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjw_4ya7MAq6"
      },
      "source": [
        "The above complication with the Sigma diagonal only exists with the case where m and n are not equal. The diagonal matrix can be used directly when reconstructing a square matrix, as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VKgvTSqMKxt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8d37dec4-a2f2-4deb-8bbe-3ee89372ad6a"
      },
      "source": [
        "# Reconstruct SVD\n",
        "from numpy import array\n",
        "from numpy import diag\n",
        "from numpy import dot\n",
        "from scipy.linalg import svd\n",
        "\n",
        "# define a matrix\n",
        "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(A)\n",
        "# Singular-value decomposition\n",
        "U, s, VT = svd(A)\n",
        "# create n x n Sigma matrix\n",
        "Sigma = diag(s)\n",
        "# reconstruct matrix\n",
        "B = U.dot(Sigma.dot(VT))\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]\n",
            " [7. 8. 9.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyPv9Bs5smjo"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Eigenvalues: Weitere Verfahren**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19UDRHausu5O"
      },
      "source": [
        "*Numerische Verfahren zur Berechnung von Eigenwerten*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibIFWCAST4PJ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Liste_numerischer_Verfahren#Berechnung_von_Eigenwerten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQNnCKHTxCi"
      },
      "source": [
        "* QR-Algorithmus: Berechnung aller Eigenwerte, allerdings mit hohen Kosten verbunden.\n",
        "\n",
        "* LU-Algorithmus: Auch Treppeniteration genannt, Vorläufer des QR-Verfahrens, aber weniger zuverlässig.\n",
        "\n",
        "* Potenzmethode: Diese erlaubt die Berechnung des betragsgrößten Eigenwertes.\n",
        "\n",
        "* Unterraumiteration: Diese ist eine mehrdimensionale Erweiterung der Potenzmethode und erlaubt die gleichzeitige Berechnung mehrerer der betragsgrößten Eigenwerte.\n",
        "\n",
        "* Inverse Iteration: Diese erlaubt die schnelle Berechnung von Eigenwerten nahe einem Shift.\n",
        "\n",
        "* Rayleigh-Quotienten-Iteration: Eine spezielle sehr schnell konvergierende Variante der Inversen Iteration mit Shift.\n",
        "\n",
        "* Lanczos-Verfahren: Berechnung einiger Eigenwerte von großen dünnbesetzten Matrizen.\n",
        "\n",
        "* Arnoldi-Verfahren: Berechnung einiger Eigenwerte von großen dünnbesetzten Matrizen.\n",
        "\n",
        "* Jacobi-Verfahren: Berechnung aller Eigenwerte und Eigenvektoren von kleinen symmetrischen Matrizen.\n",
        "\n",
        "* Jacobi-Davidson-Verfahren: Berechnung einiger Eigenwerte von großen dünnbesetzten Matrizen.\n",
        "\n",
        "* Folded Spectrum Method (Spektrumsfaltung): Berechnung eines Eigenwertes und des zugehörigen Eigenvektors nahe einem Shift (aus der Mitte des Spektrums)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIzl9WIYrcoQ"
      },
      "source": [
        "**Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBVQc-tOZXGu"
      },
      "source": [
        "* Not to be confused with Matrix factorization of a polynomial.\n",
        "* In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. \n",
        "\n",
        "* There are many different matrix decompositions; each finds use among a particular class of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZimudeCftX__"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for other decompositions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xLi2ykptka5"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Matrix_decomposition#Other_decompositions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQ7eTBayshC"
      },
      "source": [
        "###### <font color=\"blue\">**Factorization (Matrix Decomposition) for Recommender Systems (large matrices)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8k0cwD1_QbC"
      },
      "source": [
        "There are plenty of papers and articles out there talking about the use of matrix factorization for collaborative filtering. We can use Principal Component Analysis (PCA), Probabilistic Matrix Factorization (PMF), SVD, or NMF matrix decomposition techniques, depending on the specific use case. (https://heartbeat.fritz.ai/applications-of-matrix-decompositions-for-machine-learning-f1986d03571a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1lrlUPj_Zap"
      },
      "source": [
        "https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvRk9Ckry2wq"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMxvRtwLzBcs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Collaborative_filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NsYKvLPzOjV"
      },
      "source": [
        "In recent years a number of neural and deep-learning techniques have been proposed. **Some generalize traditional Matrix factorization algorithms via a non-linear neural architecture,** or leverage new model types like Variational Autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AMIWt_tzIno"
      },
      "source": [
        "Neural Collaborative Filtering: https://dl.acm.org/doi/10.1145/3038912.3052569"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaJIGJNzzymu"
      },
      "source": [
        "###### <font color=\"blue\">**Principal component analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa862jnTz4FK"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Principal_component_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfkc_-jIFtmU"
      },
      "source": [
        "###### <font color=\"blue\">**Non-negative matrix factorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsefRffQ-5Er"
      },
      "source": [
        "SVD vs NMF (In SVD, we decompose our matrices with the constraint that resultant matrices are orthogonal. Rather than constraining our factors to be orthogonal, another idea would be to constrain them to be non-negative): https://heartbeat.fritz.ai/applications-of-matrix-decompositions-for-machine-learning-f1986d03571a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4c7lww5T_b8"
      },
      "source": [
        "* [Non-negative matrix factorization](https://en.m.wikipedia.org/wiki/Non-negative_matrix_factorization) (NMF or NNMF), also non-negative matrix approximation is a **group of algorithms** in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. \n",
        "\n",
        "* This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. \n",
        "\n",
        "* Since the problem is not exactly solvable in general, it is commonly approximated numerically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA_5AGX6UJJe"
      },
      "source": [
        "![NNMF](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfuyK3-ZUtCQ"
      },
      "source": [
        "*Illustration of approximate non-negative matrix factorization: the matrix V is represented by the two smaller matrices W and H, which, when multiplied, approximately **reconstruct V**.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZzSya6rSLjr"
      },
      "source": [
        "###### <font color=\"blue\">**Sehr grosse, dünnbesetzte Matrizen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhK7GBvSZrK"
      },
      "source": [
        "* In der [numerischen Mathematik](https://de.m.wikipedia.org/wiki/Numerische_Mathematik) bezeichnet man als [dünnbesetzte oder schwachbesetzte Matrix (englisch sparse matrix)](https://de.m.wikipedia.org/wiki/Dünnbesetzte_Matrix) eine Matrix, bei der so viele Einträge aus Nullen bestehen, dass man nach Möglichkeiten sucht, dies insbesondere hinsichtlich Algorithmen sowie Speicherung auszunutzen.\n",
        "\n",
        "* Die Diskretisierung von partiellen Differentialgleichungen führt meistens auf dünnbesetzte Matrizen, etwa auf Bandmatrizen, ebenfalls die Darstellung von vielen typischen Graphen (bei beschränktem Knotengrad, Planarität o. Ä.) über ihre Adjazenzmatrix. \n",
        "\n",
        "* Zu beachten ist, dass die Inverse einer dünnbesetzten Matrix im Regelfall vollbesetzt ist, ebenso wie die LR-Zerlegung. Eine Ausnahme bilden dabei die Bandmatrizen, bei denen eine solche Zerlegung ebenfalls dünnbesetzt sein kann.\n",
        "\n",
        "* Dünnbesetzte Matrizen haben die Eigenschaft, dass sie effizient abgespeichert werden können, indem man nur Position und Wert der Nicht-Null-Einträge abspeichert. Die Position der Nichtnulleinträge wird auch als Besetzungsstruktur oder Sparsity Pattern bezeichnet. Die Auswertung eines dünnbesetzten Matrix-Vektor-Produkts kann ebenfalls effizient erfolgen, indem die Nullen in der Berechnung des Produkts nicht berücksichtigt werden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2gCNB1USNXq"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Dünnbesetzte_Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0d2DUb7xpAN"
      },
      "source": [
        "[Krylow-Unterraum-Verfahren](https://de.m.wikipedia.org/wiki/Krylow-Unterraum-Verfahren): Moderne iterative Verfahren, die für große, dünnbesetzte Gleichungssysteme gedacht sind. Wichtiger Spezialfall für symmetrisch positiv definite Probleme ist das Verfahren der [konjugierten Gradienten](https://de.m.wikipedia.org/wiki/CG-Verfahren). Krylow-Unterraum-Verfahren sind iterative Verfahren zum Lösen großer, dünnbesetzter linearer Gleichungssysteme, wie sie bei der Diskretisierung von partiellen Differentialgleichungen entstehen, oder von Eigenwertproblemen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9f2lq9s4tJr"
      },
      "source": [
        "#### **Eigenwertproblem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gIZXbG5db3"
      },
      "source": [
        "###### <font color=\"blue\">**Skalar & Determinante**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8I3t063LsOf"
      },
      "source": [
        "*Determinante*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ3mRESjREgW"
      },
      "source": [
        "* **Die Determinante ist das Produkt der Eigenwerte** (und Hauptdiagonalelemente / spur ist die summe der eigenwerte)\n",
        "\n",
        "* nur für quadratische Matrizen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RIOxr9LuLh"
      },
      "source": [
        "**Lineare Abhängigkeit von Vektoren = kein Volumen oder Fläche = Determinante gleich Null = Eigenwert existiert**\n",
        "\n",
        "  * lineare Abhängigkeit: **die Determinante ist null, wenn die beiden Vektoren  v  und  w  linear abhängig sind**. Es kann zB keine Flache zwischen den Vektoren aufgespannt werden, weil sie sich auf derselben Linie befinden. Es existiert dann minestens ein Eigenwert und Rand < n.\n",
        "  \n",
        "  (daher gleich null fur eigenwerte, weil die eigenvektoren orthogonal zueinander sind)\n",
        "\n",
        "  * Rang kleiner n (kein voller Rang - eine Zeile/Spalte nur aus Nullen besteht): Es gilt, dass die Determinante einer Matrix genau dann 0 ist, wenn ihr Rang kleiner n ist. Hat eine Matrix Determinante 0, so hat sie nicht vollen Rang.\n",
        "\n",
        "  * Eine **quadratische Matrix A besitzt einen Kern, wenn ihre Determinante gleich Null ist**. Wäre die Determinante der quadratischen Matrix A ungleich Null, so enthielte der Kern der Matrix nur den Nullvektor.\n",
        "\n",
        "  * zwei Zeilen/Spalten gleich sind\n",
        "\n",
        "  * es existieren Eigenwerte (eine Zeile/Spalte ist eine Linearkombination anderer Zeilen/Spalten)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGmxNJ_WF-_V"
      },
      "source": [
        "**Determinante nicht gleich Null**\n",
        "\n",
        "  * Matrix invertierbar: Die Determinante einer Matrix A ist genau dann ungleich Null, wenn A invertierbar ist (“invertierbar” und “regulär” ist äquivalent)\n",
        "\n",
        "  *  Gleichungssystem ist  eindeutig lösbar (wenn die Determinante der Koeffizientenmatrix ungleich null ist).\n",
        "\n",
        "  * es gibt keine Eigenwerte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkd8yABPSO2q"
      },
      "source": [
        "**Determinanten, Rang & Eigenwert**\n",
        "\n",
        "* Ausführliches zur Rangberechnung gibt es hier (link), kurz zusammengefasst noch einmal, der Rang ist die Anzahl linear unabhängiger Zeilen- oder Spaltenvektoren einer Matrix. Bei einer quadratischen (n×n)-Matrix bedeutet dies, er ist höchstens n. \n",
        "\n",
        "* **Es gilt, dass die Determinante einer Matrix genau dann 0 ist, wenn ihr Rang kleiner n ist.**\n",
        "\n",
        "* Im Zwei- oder Dreidimensionalen haben wir sogar eine anschauliche Begründung aus dem vorigen Abschnitt, **hier berechnet die Determinante die Fläche des aufgespannten Parallelogramms beziehungsweise das Volumen des Parallelpetids**. \n",
        "\n",
        "> **Sind im ℝ2 die zwei Vektoren linear abhängig, so spannen diese eben keine Fläche auf (sondern nur eine Linie).**\n",
        "\n",
        "* Ebenso fehlt bei drei Vektoren umgangssprachlich gesagt, zum Beispiel \"die Höhe\" (in die dritte Dimension), das Volumen des aufgespannten Parallelpetids ist daher Null.\n",
        "\n",
        "* Determinanten und Eigenwerte\n",
        "In der linearen Algebra gibt es den Satz, dass die Determinante einer Matrix A genau das Produkt der Eigenwerte λi beträgt. \n",
        "\n",
        "> Eng verwandt mit der vorigen Aussage ist die folgende: **Hat eine Matrix Determinante 0 so ist mindestens ein Eigenwert 0 und umgekehrt**. Das hat natürlich zur Folge, dass in Kombination mit dem Begriff des Ranges gilt, ist der Rang einer (n×n)-Matrix kleiner n so hat sie mindestens einen Eigenwert gleich 0 und umgekehrt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqltxX_WUgbi"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/eigenwert_determinante.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKMNI6rQUn45"
      },
      "source": [
        "https://www.mathematik.de/algebra/74-erste-hilfe/algebra/matrizen/2427-folgerungen-aus-und-folgerungen-für-die-determinante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LapPO_gURBOX"
      },
      "source": [
        "* zB Flächeninhalt bestimmen: zwei positiv orientierte Vektoren  ( v , w ) erzeugen das Dreieck  0 ,  v ,  w  mit dem Flächeninhalt  F = 1\n",
        "\n",
        "* Eine [Determinante](https://de.m.wikipedia.org/wiki/Determinante) ist eine spezielle Funktion, eine Zahl (ein [Skalar](https://de.m.wikipedia.org/wiki/Skalar_(Mathematik))), die einer **quadratischen Matrix** eine Zahl zuordnet.\n",
        "\n",
        "* Diese Zahl gibt Auskunft über einige Eigenschaften der Matrix:\n",
        "\n",
        "  * Sie gibt an, wie sich das **Volumen** bei der durch die Matrix beschriebenen linearen Abbildung ändert (**Faktor der Flachen- bzw. Volumenanderung**).\n",
        "  \n",
        "  * <u>**det A = 4 bedeutet zB dass die Matrix den Flächeninhalt stets vervierfacht** (angefangen bei der Orthonormal-Basis</u>.\n",
        "  \n",
        "  * Die Determinante einer Matrix ( oder ) gibt an, wie sich das Volumen einer aus Eckpunkten zusammengesetzten Geometrie skaliert, wenn diese durch die Matrix abgebildet wird. Ist die Determinante negativ, so ändert sich zusätzlich die Orientierung der Eckpunkte.\n",
        "\n",
        "  * Mit Hilfe von Determinanten kann man beispielsweise feststellen, ob ein lineares Gleichungssystem eindeutig lösbar ist, und kann die Lösung mit Hilfe der [Cramerschen Regel](https://de.m.wikipedia.org/wiki/Cramersche_Regel) explizit angeben. **Das Gleichungssystem ist genau dann eindeutig lösbar, wenn die Determinante der Koeffizientenmatrix ungleich null ist**. \n",
        "\n",
        "    * Die Cramersche Regel oder Determinantenmethode ist eine mathematische Formel für die Lösung eines linearen Gleichungssystems. Sie ist bei der theoretischen Betrachtung linearer Gleichungssysteme hilfreich. \n",
        "    \n",
        "    * Für die Berechnung einer Lösung ist der Rechenaufwand jedoch in der Regel zu hoch, da dabei verhältnismäßig viele Determinanten auftreten. Deshalb kommen dazu andere [Verfahren aus der numerischen Mathematik](https://de.m.wikipedia.org/wiki/Liste_numerischer_Verfahren#Lineare_Gleichungssysteme) zum Einsatz.\n",
        "  \n",
        "  * Entsprechend ist eine quadratische Matrix mit Einträgen aus einem Körper genau dann **invertierbar**, wenn ihre Determinante ungleich null ist.\n",
        "\n",
        "  * Schreibt man $n$ Vektoren im $\\mathbb {R} ^{n}$ als Spalten einer quadratischen Matrix, so kann die Determinante dieser Matrix gebildet werden. Bilden bei dieser Festlegung die $n$ Vektoren eine Basis, so kann das Vorzeichen der Determinante dazu verwendet werden, die **Orientierung** von euklidischen Räumen zu definieren. Der Absolutbetrag dieser Determinante entspricht zugleich dem **Volumen** des n-Parallelotops (auch Spat genannt), das durch diese Vektoren aufgespannt wird.\n",
        "  \n",
        "  * Eine weitere wichtige Anwendung ist die Berechnung des charakteristischen Polynoms und damit der Eigenwerte der Matrix.\n",
        "\n",
        "  *  Allgemeiner kann man jeder linearen Selbstabbildung [(Endomorphismus) eine Determinante](https://de.m.wikipedia.org/wiki/Determinante#Determinante_eines_Endomorphismus) zuordnen. (Da ähnliche Matrizen die gleiche Determinante haben, kann man die Definition der Determinante von quadratischen Matrizen auf die durch diese Matrizen dargestellten linearen Selbstabbildungen (Endomorphismen) übertragen.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1VS32wOn7V3"
      },
      "source": [
        "*Loesung*\n",
        "\n",
        "* Es gibt geschlossene Formeln zur Berechnung der Determinanten, wie den Laplace’schen Entwicklungssatz oder die [Leibniz-Formel](https://de.m.wikipedia.org/wiki/Determinante#Leibniz-Formel). Diese Formeln sind jedoch eher von theoretischem Wert, da ihr Aufwand bei größeren Matrizen stark ansteigt. \n",
        "\n",
        "* In der Praxis kann man Determinanten am leichtesten berechnen, indem man die Matrix mit Hilfe des [Gauß-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren) in obere oder untere Dreiecksform bringt, die Determinante ist dann einfach das Produkt der [Hauptdiagonalelemente](https://de.m.wikipedia.org/wiki/Hauptdiagonale).\n",
        "\n",
        "  * Will man das Lösen eines quadratischen eindeutig lösbaren Gleichungssystems Ax=b als Computerprogramm umsetzen, bietet es sich an, den Gaußalgorithmus als [LR-Zerlegung (auch LU-Zerlegung oder Dreieckszerlegung genannt)](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren#LR-Zerlegung) zu interpretieren.\n",
        "\n",
        "* Eine Alternative hierzu ist der [Gauß-Jordan-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9F-Jordan-Algorithmus), bei dem nicht nur die unteren Teile eliminiert werden, sondern auch die oberen, so dass eine Diagonalform entsteht, bei der dann der oben genannte zweite Schritt entfällt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfD244DFkJ_U"
      },
      "source": [
        "*Schreibweisen*\n",
        "\n",
        "Übliche Schreibweisen für die Determinante einer quadratischen Matrix $A$ sind $\\operatorname{det}(A), \\operatorname{det} A$ oder $|A|$. Zum Beispiel kann die Determinante einer $2 \\times 2$ -Matrix\n",
        "\n",
        ">$\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "mit der Formel berechnet werden:\n",
        "\n",
        ">$\n",
        "\\operatorname{det} A=\\left|\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AveT-GVHj_rK"
      },
      "source": [
        "*Die 2x2-Determinante **ist gleich dem orientierten Flächeninhalt** des von ihren Spaltenvektoren aufgespannten Parallelogramms*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Area_parallellogram_as_determinant.svg/440px-Area_parallellogram_as_determinant.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GqHv38Lh3z3"
      },
      "source": [
        "*Skalar*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkCe1kdth5cQ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Skalar_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2RMiNq8MzzY"
      },
      "source": [
        "###### <font color=\"blue\">**Eigenwertproblem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmPiMmbg-SBF"
      },
      "source": [
        "**Eigenwertproblem & charakteristisches Polynom (\"Eigen~\" hat immer was mit Stabilität zu tun)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO6cp75wKQuS"
      },
      "source": [
        "Es sei eine quadratische Matrix $A$ gegeben. Die Suche nach einem Vektor $v \\neq 0$ und einer Zahl $\\lambda$\n",
        "sodass folgende Gleichung erfüllt ist, nennt man Eigenwertproblem:\n",
        "\n",
        ">$\n",
        "A \\cdot v=\\lambda \\cdot v\n",
        "$\n",
        "\n",
        "*Man wendet eine Matrix als Transformation an einen Vektor an, und dieser Vektor wird nur verlangert und verkurzt (und bleibt gleich), aber verandert nicht seine Richtung. Z.B. die Hauptachse bei einer Rotation*.\n",
        "\n",
        "* $A$ is the matrix representing some transformation\n",
        "* Den Faktor $\\lambda$  nennt man dann den zugehörigen **Eigenwert** (reell oder komplex), \n",
        "* der Vektor $v$ heißt dann **Eigenvektor** (darf nicht der Nullvektor sein). \n",
        "* Die Menge der Eigenwerte einer Matrix wird als **Spektrum** der Matrix bezeichnet.\n",
        "\n",
        "Matrix-Vector-Multiplication $\n",
        "A \\cdot v$ gives same result as just scaling the Eigenvector $\\lambda \\cdot v$. \n",
        "\n",
        "**Calculating via determinant: The tweaked transformation squishes space into a lower dimension.** (Daher muss rang < n sein)!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbMo9Qo6Hp5f"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/eigenwert_determinante.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xs4wmtVZBYA"
      },
      "source": [
        "**Charakteristika von Eigenwerten**\n",
        "\n",
        "* Die Multiplikation einer Matrix  mit einem Vektor ergibt wieder einen Vektor. **Für quadratische Matrizen gibt es bestimmte Vektoren, die man an die Matrix multiplizieren kann, sodass man den selben Vektor als Ergebnis erhält, nur mit einem Vorfaktor multipliziert**. Einen solchen Vektor nennt man Eigenvektor  und der Vorfaktor heißt Eigenwert einer Matrix.\n",
        "\n",
        "* Ein rotierender Körper ohne äußere Kräfte verbleibt in seiner Bewegung, wenn er um seine **Symmetrieachse** rotiert\n",
        "\n",
        "* Some vectors are just stretched or squeezed out, but ramin on their span. they are called Eigenvectors, and the degree to which each of them got stretched during the transformation (=change of basis!) is called Eigenvalue.\n",
        "\n",
        "* Wenn eine Basis aus Eigenvektoren existiert, so ist die Darstellungsmatrix bezüglich dieser Basis eine **Diagonalmatrix**\n",
        "\n",
        "* im Fall des Trägheitstensors sind die Hauptträgheitsachsen dann eine **Basis aus Eigenvektoren**\n",
        "\n",
        "* Aber nicht zu jeder Matrix existiert eine Basis aus Eigenvektoren\n",
        "\n",
        "* eine solche Basis aus Eigenvektoren stets dann existiert, wenn die Matrix symmetrisch ist\n",
        "\n",
        "* der nullvektor ist kein Eigenvektor, Null darf hingegen ein Eigenwert sein.\n",
        "\n",
        "* Der **Eigenraum** zu einem Eigenwert besteht aus allen Eigenvektoren plus dem Nullvektor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-oqmd-IzDz"
      },
      "source": [
        "Will man Eigenwerte berechnen, so ist es häufig nützlich, wenn man ein paar Eigenschaften darüber kennt. Daher sollen im Folgenden ein paar derer aufgezählt werden. Mit Kenntnis dieser Eigenschaften lassen sich häufig Eigenwerte bestimmen, ohne dabei viel rechnen zu müssen.\n",
        "\n",
        "* Sei $\\lambda$ ein Eigenwert der invertierbaren Matrix $A$ mit dem Eigenvektor $v$. Dann ist auch $\\frac{1}{\\lambda}$ ein Eigenwert der inversen Matrix von A zum Eigenvektor $v$,\n",
        "\n",
        "* Seien $\\lambda_{i}$ die Eigenwerte der Matrix $A \\in \\mathbb{C}^{n \\times n}$. Dann gilt:\n",
        "\n",
        "  1. $\\sum_{i=1}^{n} \\lambda_{i}=\\operatorname{Spur}(A)$ (die Summe aller Eigenwerte ist die Spur)\n",
        "\n",
        "  2. $\\prod_{i=1}^{n} \\lambda_{i}=\\operatorname{det}(A)$ (das Produkt aller Eigenwerte ist die Determinante)\n",
        "\n",
        "* Ist $\\lambda$ ein Eigenwert einer Matrix $A$, so ist er auch ein Eigenwert der transponierten Matrix $A^{T}$ und umgekehrt. Das Spektrum von $A$ stimmt also mit dem Spektrum der Transponierten $A^{T}$ überein.\n",
        "\n",
        "* Jeder Eigenwert einer reellen symmetrischen Matrix ist reell. Im Allgemeinen können aber\n",
        "auch komplexe Eigenwerte durchaus auftreten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4X4yvODwnN"
      },
      "source": [
        "**Praktische Beispiele fur Eigenvektoren** (aus [diesem Video](https://www.youtube.com/watch?app=desktop&v=9P__qyOAPCg))\n",
        "\n",
        "Eigenvektor: Vektoren, die bei einer Transformation mit einer Matrix ihre Richtung nicht ändern. \n",
        "\n",
        "* Z.B. Rotationen um eine Achse = Achse bleibt fest und ist Eigenvektor (bei einfachen Rotationen Eigenwert = 1). \n",
        "\n",
        "* in mechanik: trägheitsmoment gegenüber rotationen. Hauptachsen mit Tensorrechnung. die hauptachsen sind diejenigen richtungen, in die die drehungen stabile verlaufen. drehimpuls und winkelgeschwindigkeit haben gleiche richtung. keine schwankungen der drehachsen, sie sind stabil. (https://www.grund-wissen.de/physik/mechanik/kinematik/kreisfoermige-bewegungen.html)\n",
        "\n",
        "* In der Theorie der elastischen Körper gibt es die Möglichkeit die Deformationen von Körpern aufzuteilen in Scherungen und Schubspannungen und Streckungen, und das fuhrt auch zu einer Hauptachsentransformation.\n",
        "\n",
        "* Oder Hauptachsentransformation (=entlang derer die Eigenvektoren liegen) bei Bildbearbeitung: wenn Bild in eine Richtung verzerrt ist, kann man es entzerren., wenn man Verzerrungsrichtung findet.\n",
        "\n",
        "Wenn man die Transformation auf diese Hauptachsen macht, dann ist die ganze Abbildung nur noch eine Diagonalmatrix (und keine Nebendiagonalelemente mehr). Mit solchen Matrizen kann man sehr einfach rechnen. Nennt man dann Hauptachsentransformation (=Hauptachsen sind Eigenvektorrichtungen). \n",
        "\n",
        "* Stabilitätsprobleme: suche nach Eigenwerten (wirschaftssysteme, wettervorhersage). Zw 2 geschäften wandern kunden hin und her. Will man verhindern dass kunden komplett abwandern in ein geschäft, sucht man eigenwerte. \n",
        "\n",
        "* Physik: kopplung von schwingungen bei 2 objekten. Eigenschwingung des systems wenn bei parallel oder antiparallel schwingen.\n",
        "\n",
        "* Quantenmechanik: hermetische Operatoren. Diese führen zu Eigenwerten und Eigenvektoren. \n",
        "\n",
        "\t* Interessant ist bei solchen Operatoren bzw. bei den linearen Abbildungen bei den symmetrischen Matrizen im Reellen: Eigenvektoren stehen im reellen senkrecht aufeinander. (Bilden eine Orthonormalbasis). \n",
        "\n",
        "\t* Man kann Vektorraum mit diesen Eigenvektoren aufspannen und jeden Vektor als Linearkombination dieser Eigenvektoren darstellen.\n",
        "\n",
        "\t* **Dann ist das Verhalten der Abbildungen quasi reduziert auf das Verhalten der Eigenvektoren mit ihren entsprechenden Eigenwerten, so dass dann jeder Vektor wenn man ihn in die Basis der Eigenvektoren schreibt, relativ einfach transformiert werden kann**, indem man einfach die entsprechenden Eigenvektor-Skalierungen beachtet und später die Summe immer wieder bildet (ist ja alles linear).\n",
        "\n",
        "Spezielle Vektoren der Quantenmechanik:\n",
        "\n",
        "* Hamilton-Operator (Energie-Operator): Die Eigenzustaende dieses Energieoperators sind dann die Energiezustaende, die stabil sind. In der Atomphysik sind das dann die Zustande, die die Elektronen in den Orbitalen beschreiben\n",
        "\n",
        "* Genauso gibt es Eigenvektoren zu den Drehimpuls-Operatoren (und zu anderen Dingen). Hier kommt Eigenwert-Theorie zum blühen. Die entsprechende Orthonormal-Basis ist dann auch die Basis in der man jede beliebigen Zustand des Systems beschreiben kann. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v2hT8WO-ryw"
      },
      "source": [
        "**Beispiele, warum man Eigenvektoren und Eigenwerte in der Praxis braucht:**\n",
        "\n",
        "> eigenvektoren = orthonormalbasis, man kann alle andere vektoren (und damit lösungen) daraus rekonstruieren (ist das so korrekt??)\n",
        "\n",
        "> Zum Beispiel: A complex-valued square matrix $\\mathrm{A}$ is normal (meaning $\\mathrm{A}^{*} \\mathrm{~A}=\\mathrm{AA}^{*}$, where $\\mathrm{A}^{*}$ is the conjugate transpose) if and only if it can be decomposed as $\n",
        "\\mathbf{A}=\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{*}\n",
        "$ where $\\mathrm{U}$ is a unitary matrix (meaning $\\mathrm{U}^{*}=\\mathrm{U}^{-1}$ ) and $\\boldsymbol{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$ is a diagonal matrix. **The columns $\\mathrm{u}_{1}, \\ldots, \\mathrm{u}_{n}$ of $\\mathrm{U}$ form an orthonormal basis and are eigenvectors of $\\mathrm{A}$ with corresponding eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$.**\n",
        "\n",
        "Wie berechnet man am einfachsten, den Zustand des Systems zu einem vorgegebenen Zeitpunkt $t_{k}$ in der ferneren Zukunft; sagen wir $k=1000 ?$\n",
        "\n",
        "* Wir nehmen an, dass der Anfangszustand $\\boldsymbol{v}_{0}$ ein Eigenvektor von $A$ zum Eigenwert $\\lambda$ ist, d. h., es gilt $\\boldsymbol{v}_{1}=A \\boldsymbol{v}_{0}=\\lambda \\boldsymbol{v}_{0}$ \n",
        "\n",
        "* Erklarung: die Transformation (Multiplikation) eines Zustands mit dem Vektors ${v}_{0}$ mit einer Transition-Matrix $A$ in einen neuen Zustand ${v}_{1}$ ist das gleiche wie die Multiplikation des Vektors ${v}_{0}$ mit dem Eigenwert.\n",
        "\n",
        "* Daraus ergibt sich $\\boldsymbol{v}_{2}=A \\boldsymbol{v}_{1}=\\lambda A \\boldsymbol{v}_{0}=\\lambda^{2} \\boldsymbol{v}_{0}$ und allgemein $\n",
        "\\boldsymbol{v}_{k}=A^{k} \\boldsymbol{v}_{0}=\\lambda^{k} \\boldsymbol{v}_{0} \\text { . }\n",
        "$\n",
        "\n",
        "* Wir haben damit eine geschlossene Formel für den Zustand $\\boldsymbol{v}_{k}$, damit können wir also den Zustand des Systems zum Zeitpunkt $t_{k}=1000$ leicht bestimmen, $\n",
        "v_{1000}=\\lambda^{1000} v_{0}\n",
        "$\n",
        "\n",
        "Welches Langzeitverhalten hat das System? Beispielsweise erhebt sich die Frage, ob der komponentenweise zu verstehende Grenzwert $\\lim _{k \\rightarrow \\infty} v_{k}$ existiert, ob die Einträge von $v_{k}$ über alle Grenzen wachsen, ob sie periodisch oszillieren, usw.\n",
        "\n",
        "* Auch die zweite Frage wird dadurch beantwortet: Ist $|\\lambda|<1$, so konvergiert $v_{k}$ komponentenweise gegen $\\mathbf{0}=(0, \\ldots, 0 .$ Ist $|\\lambda|>1$, so wachsen die Komponenten von $v_{k}$ über alle Grenzen, wenn die entsprechende Komponente von $v_{0}$ nicht gerade gleich null ist. \n",
        "\n",
        "* $\\operatorname{Im}$ Fall $\\lambda=1$ ist die Folge stationär: $\\boldsymbol{v}_{k}=\\boldsymbol{v}_{0}$ für alle $k \\in \\mathbb{N}_{0} . \\operatorname{Im}$ Fall $\\lambda=-1$ oszilliert sie:\n",
        "$v_{2 k}=v_{0}$ und $v_{2 k+1}=-v_{0}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQKdBUmBB1w"
      },
      "source": [
        "**Kriterien fur die Existenz von Eigenwerten**\n",
        "\n",
        "(*Wenn eine dieser Aussagen wahr ist, dann alle. Und wenn eine falsch, dann sind alle falsch*)\n",
        "\n",
        "1. $\\operatorname{rg}(B) < n$\n",
        "\n",
        "2. $\\operatorname{det}(B) = 0$ -> dieses Kriterium zu prufen ist am einfachsten und daher am haufigsten!\n",
        "\n",
        "3. $B^{-1}$ existiert nicht\n",
        "\n",
        "4. $B \\vec{X}$ = 0 hat mehr als nur die Losung $\\vec{x}$ = 0\n",
        "\n",
        "5. $\\lambda$ = 0 ist ein Eigenwert von $B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFPn1kkwE2od"
      },
      "source": [
        "**Die Determinante ist das Produkt der Eigenwerte, die Spur die Summe der Eigenwerte** \n",
        "\n",
        "(die Determinante ist null, wenn die beiden Vektoren  v  und  w  linear abhängig sind).\n",
        "\n",
        "Der Zusammenhang zwischen Spur, Determinante und den Eigenwerten einer Matrix: \n",
        "\n",
        "Ist $A \\in \\mathbb{K}^{n \\times n}$ und zerfällt das charakteristische Polynom von $A$ in seine $n$ Linearfaktoren, d. h.\n",
        "\n",
        "* $\n",
        "\\chi_{A}=\\left(\\lambda_{1}-X\\right) \\cdots\\left(\\lambda_{n}-X\\right)\n",
        "$\n",
        "\n",
        "* So gilt: $\n",
        "\\text { Sp } A=\\lambda_{1}+\\cdots+\\lambda_{n}, \\quad \\text { det } A=\\lambda_{1} \\cdots \\lambda_{n}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HteISTfaPgCj"
      },
      "source": [
        "**Herleitung der Lösung des Eigenwertproblems**\n",
        "\n",
        "Wie gelangt man zu dieser Berechnungsvorschrift? Dazu betrachten wir erst einmal das Eigenwertproblem, das es zu lösen gilt:\n",
        "\n",
        "$\n",
        "A \\cdot v=\\lambda \\cdot v\n",
        "$\n",
        "\n",
        "Diese Gleichung lässt sich mithilfe der Einheitsmatrix $E_{n}$ umformulieren:\n",
        "\n",
        "> $\n",
        "\\begin{array}{l}\n",
        "A \\cdot v-\\lambda \\cdot v=0 \\\\\n",
        "\\left(A-\\lambda\\right) \\cdot v=0\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Problem: Man kann eine Matrix A nicht mit einer Zahl verknupfen, daher benutzt man die Einheitsmatrix $E_{n}$**\n",
        "\n",
        "> $(A-\\lambda E_{n}) \\cdot v=0$\n",
        "\n",
        "Gibt es nun eine Zahl $\\lambda$ und einen Vektor $v$, sodass dieser durch Multiplikation mit der Matrix\n",
        "$\\left(A-\\lambda E_{n}\\right)$ auf den Nullvektor abgebildet wird, so ist diese Matrix nicht von vollem Rang und die Multiplikation mit einem Vektor nicht injektiv. Dass die Matrix $\\left(A-\\lambda E_{n}\\right)$ keinen vollen Rang besitzt ist gleichbedeutend damit, dass ihre Determinante Null ist. Wenn es\n",
        "also eine Lösung des Eigenwertproblems gibt, muss gelten:\n",
        "\n",
        "$\n",
        "\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=0\n",
        "$\n",
        "\n",
        "Um das Eigenwertproblem zu lösen, müssen also die Nullstellen des charakteristischen\n",
        "Polynoms\n",
        "\n",
        "$\n",
        "\\chi_{A}(\\lambda)=\\operatorname{det}\\left(A-\\lambda E_{n}\\right)\n",
        "$\n",
        "\n",
        "ermittelt werden, genau wie es der Algorithmus vorschreibt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gewOoWIREvwy"
      },
      "source": [
        "**Berechnung der Eigenwerte, Eigenräume und Eigenvektoren**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbDi3hDN8p4"
      },
      "source": [
        "Es sei die $n \\times n$ -Matrix $A$ vorgegeben und zu dieser wollen wir die Eigenwerte berechnen. Folgende Schritte musst du dabei durchführen. \n",
        "\n",
        "> Zum Beispiel fur die Matrix $A=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUeaBBxAU734"
      },
      "source": [
        "1. Bilde die Matrix $\\left(A-\\lambda E_{n}\\right)$. $E_{n}$ steht für die **Einheitsmatrix**. Du musst also in der Matrix $A$ auf der Diagonalen immer den Wert $\\lambda$ abziehen.\n",
        "\n",
        "> $\\left(A-\\lambda E_{n}\\right)=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)-\\left(\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{array}\\right)=\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67G6O08U4kZ"
      },
      "source": [
        "2. Berechne die **Determinante** dieser Matrix. Diese nennt man das **charakteristische Polynom** $\\chi_{A}(\\lambda)=\\operatorname{det}\\left(A-\\lambda E_{n}\\right)$ der Matrix $A .$ Es ist ein Ausdruck in Abhängigkeit von $\\lambda$.\n",
        "\n",
        "> $\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=\\operatorname{det}\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "$=(2-\\lambda)^{2} \\cdot(3-\\lambda)+2+2-2 \\cdot(2-\\lambda)-2 \\cdot(2-\\lambda)-(3-\\lambda)$\n",
        "$=-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5$ **(=Polynom)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOfq16Q1U2M_"
      },
      "source": [
        "3. Bestimme die Nullstellen des [**charakteristischen Polynoms**](https://de.m.wikipedia.org/wiki/Charakteristisches_Polynom) $\\chi_{A}(\\lambda)=0$. Das sind genau die gesuchten Eigenwerte der Matrix. (**weil dann die Determinante oben Null wird, und das ist was wir wollen**)\n",
        "\n",
        "> Durch Ausprobieren erhalten wir schnell die erste Nullstelle $\\lambda_{1}=1$. Klammern wir dann den Faktor $(\\lambda-1)$ aus, erhalten wir:\n",
        "$\n",
        "-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5=(\\lambda-1) \\cdot\\left(-\\lambda^{2}+6 \\lambda-5\\right)\n",
        "$\n",
        "\n",
        "> Die restlichen Nullstellen sind also Nullstellen des Polynoms $-\\lambda^{2}+6 \\lambda-5 .$ Diese lassen sich mithilfe der [Mitternachtsformel](https://de.m.wikipedia.org/wiki/Quadratische_Gleichung#Lösungsformel_für_die_allgemeine_quadratische_Gleichung_(a-b-c-Formel)) bestimmen:\n",
        "$\n",
        "\\lambda_{2,3}=\\frac{-6 \\pm \\sqrt{36-20}}{-2}=3 \\mp 2\n",
        "$\n",
        "\n",
        "\n",
        "> Somit lauten die drei Eigenwerte der $3 \\times 3$ -Matrix $\\lambda_{1}=\\lambda_{2}=1, \\lambda_{3}=5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O6EyJi3CeCW"
      },
      "source": [
        "Berechnung der Eigenwerte, Eigenräume und Eigenvektoren mit dem charakterischen Polynom, dass man wiederum mit der Determinante erklären kann\n",
        "\n",
        "* Hat eine Matrix A nicht gerade Dreiecks- oder Diagonalgestalt, so bestimmt man im Allgemeinen die Eigenwerte von $A$ systematisch durch Berechnen der Nullstellen des charakteristischen Polynoms $\\chi_{A}$ von $A$.\n",
        "\n",
        "* Eigenwerte sind Nullstellen des charakteristischen Polynoms\n",
        "\n",
        "* Eine Matrix $A \\in \\mathbb{K}^{n \\times n}$ hat genau dann den Eigenwert $\\lambda$, wenn das homogene lineare Gleichungssystem $\\left(A-\\lambda \\mathbf{E}_{n}\\right) v=\\mathbf{0}$ eine Lösung $\\boldsymbol{v} \\neq \\mathbf{0}$ besitzt.\n",
        "\n",
        "* Achtung Das homogene lineare Gleichungssystem $\\left(A-\\lambda \\mathrm{E}_{n}\\right) v=0$ mit $\\lambda \\in \\mathbb{K}$ hat auf jeder Fall die Lösung $v=0$ - unabhängig von $\\lambda$. Wir sind aber gerade an den nichttrivialer Lösungen $v \\neq 0$ interessiert. Wir suchen also die $\\lambda \\in \\mathbb{K}$, für die solche nichttrivialer Lösungen existieren. Hierzu ist die Determinante das Mittel der Wahl.\n",
        "\n",
        "* das homogene lineare Gleichungssystem  ( A - λ E n ) v = 0  genau dann eine Lösung  v ≠ 0, **wenn die Determinante der folgenden Koeffizientenmatrix des homogen linearen Gleichungssystems den Wert  0  hat. Und die Determinante ist null, wenn zwei Vektoren v und w linear abhängig sind**.\n",
        "  \n",
        "> $\n",
        "\\begin{array}{rcccc} \n",
        "& a_{11}-\\lambda & a_{12} & \\cdots & a_{1 n} \\\\\n",
        "\\left|\\boldsymbol{A}-\\lambda \\mathbf{E}_{n}\\right|=\\mid & a_{21} & a_{22}-\\lambda & & \\vdots \\\\\n",
        "& \\vdots & & \\ddots & \\\\\n",
        "& a_{n 1} & \\cdots & & a_{n n}-\\lambda\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7Mz5cMF_gO"
      },
      "source": [
        "Durch sukzessives Entwickeln dieser Determinante nach den ersten\n",
        "Spalten und Zusammenfassen aller Terme mit der gleichen $\\lambda$ -Potenz erhält man daraus eine Gleichung $n$ -ten Grades für $\\lambda$,\n",
        "\n",
        "$\n",
        "\\left(-1 \\lambda^{n}+c_{n-1} \\lambda^{n-1}+\\ldots+c_{1} \\lambda+c_{0}=0\\right.\n",
        "$ mit $c_{0}, c_{1}, \\ldots, c_{n-1} \\in \\mathbb{K}$\n",
        "\n",
        "Das Polynom\n",
        "\n",
        "$\n",
        "\\begin{array}{rlr}\n",
        "\\chi_{A} & := & \\left|A-X \\mathbf{E}_{n}\\right| \\\\\n",
        "& =\\left(-1 X^{n}+c_{n-1} X^{n-1}+\\cdots+c_{1} X+c_{0} \\in \\mathbb{K}[X]\\right.\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "vom Grad $n$ heißt charakteristisches Polynom der Matrix $A \\in \\mathbb{K}^{n \\times n} .$ Es gilt:\n",
        "\n",
        "$\n",
        "\\lambda \\in \\mathbb{K} \\text { ist ein Eigenwert von } A \\Leftrightarrow \\chi_{A}(\\lambda)=0 .\n",
        "$\n",
        "\n",
        "Die Matrix $A$ hat höchstens $n$ Eigenwerte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-5FB4sQ_XI0"
      },
      "source": [
        "**Eigenwerte sind eng mit Diagonalisierbarkeit und Matrizen  verbunden**\n",
        "\n",
        "Kriterium für Diagonalisierbarkeit : \n",
        "\n",
        "Eine Matrix $A \\in \\mathbb{K}^{n \\times n}$ ist genau dann diagonalisierbar, wenn es eine Basis $B$ des $\\mathbb{K}^{n}$ aus Eigenvektoren von $A$ gibt.\n",
        "\n",
        "Ist $B=\\left(b_{1}, \\ldots, b_{n}\\right)$ eine geordnete Basis des $\\mathbb{K}^{n}$ aus Eigenvektoren der Matrix $A$, so ist die Matrix\n",
        "\n",
        "$\n",
        "D=S^{-1} A S\n",
        "$\n",
        "\n",
        "mit $S=\\left(\\left(b_{1}, \\ldots, b_{n}\\right)\\right)$ eine Diagonalmatrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuE-EYRAl5F"
      },
      "source": [
        "Eigenwertproblem einfach erklart: https://www.youtube.com/watch?v=eJWgKvrhDmE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_1v9IbYk03"
      },
      "source": [
        "https://www.youtube.com/watch?v=PFDu9oVAE-g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejJGgzLRZdpG"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/eigenvalue_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAcjHNOhaJRQ"
      },
      "source": [
        "Useful for example to know the axis of rotation (where the Eigenvalue would be 1, since rotation is neither stretching nor squishing).\n",
        "\n",
        "> Rotate $30^{\\circ}$ around $\\left[\\begin{array}{l}2 \\\\ 3 \\\\ 1\\end{array}\\right]$ = $\\left[\\begin{array}{ccc}\\cos (\\theta) \\cos (\\phi) & -\\sin (\\phi) & \\cos (\\theta) \\sin (\\phi) \\\\ \\sin (\\theta) \\cos (\\phi) & \\cos (\\theta) & \\sin (\\theta) \\sin (\\phi) \\\\ -\\sin (\\phi) & 0 & \\cos (\\phi)\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJPAwTAaIWF"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/eigenvalue_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jUCA24x5OuA"
      },
      "source": [
        "*Eigenvektor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWe68ag2M2q1"
      },
      "source": [
        "* Ein [Eigenvektor](https://de.m.wikipedia.org/wiki/Eigenwertproblem) einer Abbildung ist in der linearen Algebra ein vom Nullvektor verschiedener Vektor, dessen Richtung durch die Abbildung nicht verändert wird. \n",
        "\n",
        "* Ein Eigenvektor wird also nur skaliert und man bezeichnet den Skalierungsfaktor als Eigenwert der Abbildung.\n",
        "\n",
        "* Eigenwerte charakterisieren wesentliche Eigenschaften linearer Abbildungen, etwa ob ein entsprechendes lineares Gleichungssystem eindeutig lösbar ist oder nicht. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jjZrNOiNC_V"
      },
      "source": [
        "*Eigenraum*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZKqOG0ANFqz"
      },
      "source": [
        "* [Eigenraum](https://de.m.wikipedia.org/wiki/Eigenraum) ist ein Begriff aus der linearen Algebra. Er bezeichnet die lineare Hülle der Eigenvektoren zu einem bestimmten Eigenwert eines Endomorphismus. Die Eigenvektoren spannen damit einen Untervektorraum auf.\n",
        "\n",
        "* Eine Verallgemeinerung des Eigenraums ist der Hauptraum. Hat ein Eigenwert die algebraische Vielfachheit 1, so sind für diesen Eigenwert Eigenraum und Hauptraum gleich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3mIl8Xj8Dfs"
      },
      "source": [
        "###### <font color=\"blue\">**Spur & Spektrum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtQ3y65LJX43"
      },
      "source": [
        "**Spur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyK6KVjUJKnQ"
      },
      "source": [
        "* Die Spur einer Matrix ist die Summe ihrer Eigenwerte\n",
        "\n",
        "* Die [Spur](https://de.m.wikipedia.org/wiki/Spur_(Mathematik)) (Spurfunktion, Spurabbildung) ist ein Konzept in den mathematischen Teilgebieten der Linearen Algebra sowie der Funktionalanalysis und wird auch in der Theorie der Körper und Körpererweiterungen verwendet.\n",
        "\n",
        "* In der linearen Algebra bezeichnet man als die Spur einer quadratischen $n\\times n$ Matrix A über einem Körper K die Summe der Hauptdiagonalelemente dieser Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVw9YGcYJMoI"
      },
      "source": [
        "$\n",
        "A=\\left(\\begin{array}{cccc}\n",
        "a_{11} & a_{12} & \\cdots & a_{1 n} \\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n 1} & a_{n 2} & \\cdots & a_{n n}\n",
        "\\end{array}\\right)\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vZ_NNVFJOxf"
      },
      "source": [
        "$\n",
        "\\operatorname{Spur}(A)=\\sum_{j=1}^{n} a_{j j}=a_{11}+a_{22}+\\cdots+a_{n n} \\in K\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D-RILUhJQlc"
      },
      "source": [
        "\n",
        "$\n",
        "\\text { Gilt Spur }(A)=0 \\text { , so bezeichnet man die Matrix } A \\text { als spurfrei. }\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeE-pNAcJSkW"
      },
      "source": [
        "Die Spur einer Matrix ist die **Summe ihrer Eigenwerte** (mit algebraischer Vielfachheit). Für diagonalisierbare Matrizen sind algebraische Vielfachheit und geometrische Vielfachheit identisch, so dass die Vielfachheit eines Eigenwertes der Anzahl seiner zugehörigen (linear unabhängigen) Eigenvektoren entspricht.\n",
        "\n",
        "Unter der Spur dürfen Matrizen vertauscht werden.\n",
        "\n",
        "Zwei zueinander ähnliche Matrizen haben die gleiche Spur. Die Spur ist somit invariant unter Basistransformationen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYKq5brzfV_l"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Spektralsatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHunpyf1dvm7"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Spurklasseoperator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P96ciYgRW9yL"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Spectrum_(functional_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fh3J1_XAhl"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Spectral_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IcSwwqWW4X7"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HEUDDUKW7W8"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTyvm_3XXCzx"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Selbstadjungierter_Operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvPf0jxIWz_O"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Eigenmode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S93NclS3W1yh"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Resonanz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-2jM0tXHLW"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Hearing_the_shape_of_a_drum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZKqHZKXI8W"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Dirichlet_eigenvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glNtHGj8poRr"
      },
      "source": [
        "https://youtu.be/Mx75Kiqyaik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu_f_gIaJan7"
      },
      "source": [
        "**Spurklasseoperator (Spur in der Funktionalanalysis)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xRqNvYWJcX0"
      },
      "source": [
        "Das Konzept der Spur in der linearen Algebra kann auch auf unendlichdimensionale Räume ausgedehnt werden. Falls diese existiert. Die Endlichkeit dieser Summe ist abhängig von der Wahl der Orthonormalbasis. Operatoren, für die dies immer der Fall ist (diese sind immer kompakt), also deren Spur über alle Orthonormalbasen endlich ist, werden Spurklasseoperatoren genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D8F4FdJJd-q"
      },
      "source": [
        "In der Quantenmechanik beziehungsweise der Quantenstatistik verallgemeinert man den Begriff der Spur so, dass auch Operatoren erfasst werden, die keine Spurklasseoperatoren sind. Und zwar brauchen diese Operatoren, wie zum Beispiel der grundlegende Hamiltonoperator (Energie-Operator) $\\mathcal {H}$ des Systems, nur selbstadjungiert zu sein. Sie besitzen dann eine [Spektraldarstellung](https://de.m.wikipedia.org/wiki/Spektralsatz) (Der Name leitet sich vom „Spektrum“ der Eigenwerte her).\n",
        "\n",
        "* In der Quantenmechanik hat der Spektralsatz („Entwicklungssatz“) eine zentrale Bedeutung, da messbare physikalische Größen, sogenannte „Observablen“, durch selbstadjungierte Operatoren auf einem Hilbertraum dargestellt werden.\n",
        "\n",
        "* Die möglichen Messwerte einer Observablen entsprechen ihrem Spektrum, welches in Punktspektrum (oder ${ }_{n}$ diskretes Spektrum\") und kontinuierliches Spektrum zerfällt. Die Elemente des Punktspektrums werden auch Eigenwerte genannt. \n",
        "\n",
        "* Für eine diskrete Observable, d. h. eine Observable ohne kontinuierliches Spektrum, ist die Wahrscheinlichkeit, für einen gegebenen quantenmechanischen Zustand $|\\psi\\rangle$ den Messwert $\\lambda_{j}$ zu erhalten, gegeben durch das Betragsquadrat des Skalarproduktes $\\left\\langle\\phi_{j} \\mid \\psi\\right\\rangle$, wobei $\\phi_{j}$ die Eigenfunktion zum Eigenwert $\\lambda_{j}$ ist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEgearjQJjKX"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Spurklasseoperator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaGRQxP8Jgsw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Spektralsatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOjC8w7_JlPT"
      },
      "source": [
        "**Definition Spektrum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6m56NkK3cuG"
      },
      "source": [
        "[Spektraldarstellung](https://de.m.wikipedia.org/wiki/Spektralsatz): Der Name leitet sich vom „Spektrum“ der Eigenwerte her).\n",
        "\n",
        "Das Spektrum eines Operators $T$ ist die Menge aller Elemente $\\lambda$ des Zahlenkörpers (meistens die komplexen Zahlen), für die die Differenz des Operators mit dem $\\lambda$ -fachen der identischen Abbildung\n",
        "\n",
        "> $T-\\lambda$ id\n",
        "\n",
        "nicht beschränkt-invertierbar ist, das heißt, dass es keine Inverse gibt oder diese nicht beschränkt sind.\n",
        "Das Spektrum des Operators wird mit $\\sigma(T)$ bezeichnet und die Elemente des Spektrums heißen Spektralwerte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6ohvyV6XhD"
      },
      "source": [
        "**Beispiel: Matrixen in der linearen Algebra**\n",
        "\n",
        "In der linearen Algebra bilden die nxn-Matrizen mit komplexen Einträgen eine Algebra bezüglich der üblichen Addition und Skalarmultiplikation (komponentenweise) sowie der Matrizenmultiplikation. Die $(n \\times n)$ -Matrizen können daher sowohl als Beispiel für eigentliche Operatoren in ihrer Eigenschaft als lineare Abbildungen des $\\mathbb{C}^{n} \\rightarrow \\mathbb{C}^{n}$ angesehen werden, als auch als Beispiel einer Operatoralgebra, wobei es in diesem Kontext unerheblich ist, welche Operatornorm für die Matrizen gewählt wird. Da alle linearen Abbildungen eines endlichdimensionalen Raumes auf sich automatisch beschränkt sind, kann dieser Begriff in der Definition hier außer Acht gelassen werden.\n",
        "\n",
        "* Eine Matrix $A$ ist invertierbar, wenn es eine Matrix $B$ gibt, so dass $A \\cdot B=B \\cdot A=I$ ([Einheitsmatrix](https://de.wikipedia.org/wiki/Einheitsmatrix)) ist. Dies ist genau dann der Fall, wenn die Determinante nicht verschwindet: det $A \\neq 0$. \n",
        "\n",
        "  * Sei $\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right)\n",
        "$ ist eine $2 \\times 2$ -Matrix.\n",
        "\n",
        "  * Die Determinante $D(A)$ (bzw. $\\operatorname{det}(A)$ oder $\\operatorname{Det}(A)$ ) von $A$ ist gleich $a d-b c$ also:\n",
        "\n",
        "  * $\n",
        "\\operatorname{Det}(A)=\\left|\\begin{array}{cc}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$\n",
        "\n",
        "* **Daher ist eine Zahl $z \\in \\mathbb{C}$ dann ein Spektralwert, wenn $\\operatorname{det}(A-z I)=0$ gilt, also wenn die Determinante verschwindet, was bedeutet, dass keine Invertierbarkeit vorliegt.**\n",
        "\n",
        "* Da dies aber gerade das charakteristische Polynom der Matrix $A$ in $z$ ist, ist $z$ genau dann ein Spektralwert, wenn $z$ ein Eigenwert der Matrix ist. \n",
        "\n",
        "* **In der linearen Algebra bezeichnet das Spektrum einer Matrix daher die Menge der Eigenwerte.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT7QeqIZ7XXu"
      },
      "source": [
        "**Beispiel: Funktionen**\n",
        "\n",
        "Die stetigen Funktionen auf dem Intervall [0,1] mit Werten in den komplexen Zahlen ( C bilden (z. B. mit der Supremumsnorm als Norm, die hier aber nicht von\n",
        "Belang ist) eine Banachalgebra, wobei die Summe zweier Funktionen und das\n",
        "Produkt zweier Funktionen punktweise definiert wird:\n",
        "\n",
        "$\n",
        "(f+g)(x)=f(x)+g(x) \\quad(f \\cdot g)(x)=f(x) \\cdot g(x)\n",
        "$\n",
        "\n",
        "Eine Funktion $f$ heißt dann in dieser Algebra invertierbar, wenn es eine andere Funktion $g$ gibt, so dass $f \\cdot g(=g \\cdot f)=1$ (Einsfunktion) ist, das heißt, wenn es\n",
        "eine Funktion $g$ gibt, deren Werte gerade die Kehrwerte von $f$ sind. Man sieht nun schnell ein, dass **eine Funktion genau dann invertierbar ist, wenn sie nicht den\n",
        "Funktionswert 0 besitzt** und die Inverse in diesem Fall punktweise die inversen\n",
        "Funktionswerte (Kehrwerte) der ursprünglichen Funktion besitzt:\n",
        "\n",
        "> $\n",
        "f^{-1}(x)=(f(x))^{-1}=1 / f(x), \\text { wenn } f(x) \\neq 0 \\text { überall. }\n",
        "$\n",
        "\n",
        "**Eine Zahl $z \\in \\mathbb{C}$ ist also ein Spektralwert, wenn die Funktion $f-z$ nicht invertierbar ist, also den Funktionswert 0 besitzt**. Dies ist natürlich genau dann der Fall, wenn $z$ ein Funktionswert von $f$ ist. **Das Spektrum einer Funktion ist daher\n",
        "genau ihr Bild.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8seR9FL2hAy"
      },
      "source": [
        "**Die Spektraltheorie linearer Operatoren aus der Funktionalanalysis ist eine Verallgemeinerung der [Eigenwerttheorie](https://de.wikipedia.org/wiki/Eigenwertproblem) aus der linearen Algebra.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_uHYcseDMNq"
      },
      "source": [
        "* Das [Spektrum](https://de.wikipedia.org/wiki/Spektrum_(Operatortheorie)) eines linearen Operators ist ein Begriff aus der Funktionalanalysis, einem Teilgebiet der Mathematik. \n",
        "\n",
        "* In der endlichdimensionalen linearen Algebra betrachtet man Endomorphismen, die durch Matrizen dargestellt werden, und ihre Eigenwerte. Die Verallgemeinerung ins Unendlichdimensionale wird in der Funktionalanalysis betrachtet. \n",
        "\n",
        "* **Das Spektrum eines Operators kann man sich als Menge verallgemeinerter Eigenwerte vorstellen. Diese werden Spektralwerte genannt.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av1H5O1y1snw"
      },
      "source": [
        "Sei $A \\in \\mathcal{L}(\\mathcal{H})$. Eine Zahl $\\lambda \\in \\mathbb{C}$ heißt ein **Eigenwert** von $A$, wenn es einen Vektor $x$ $\\in$ $\\mathcal{H} \\backslash\\{0\\}$ gibt mit\n",
        "\n",
        "$\n",
        "A x=\\lambda x\n",
        "$\n",
        "\n",
        "Der Vektor $x$ heißt dann **Eigenvektor** von $A$ zum Eigenwert $\\lambda$. \n",
        "\n",
        "Der Vektorraum $N(A-\\lambda I)$ heißt der Eigenraum von A zum Eigenwert $\\lambda$,\n",
        "\n",
        "> $\n",
        "P_{\\{\\lambda\\}}:=P_{N(A-\\lambda I)}\n",
        "$\n",
        "\n",
        "die zu $\\lambda$ gehörende **Eigenprojektion**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmVA59p82Qqs"
      },
      "source": [
        "https://www.uni-math.gwdg.de/mkohlma/Documents/VL_WS16_17.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldllxRzY3N3H"
      },
      "source": [
        "**Zusammenhang der Spektraltheorie mit der Eigenwerttheorie**\n",
        "\n",
        "* Die Spektraltheorie linearer Operatoren aus der Funktionalanalysis ist eine Verallgemeinerung der Eigenwerttheorie aus der linearen Algebra. \n",
        "\n",
        "* In der linearen Algebra werden Endomorphismen auf endlichdimensionalen Vektorräumen betrachtet. Die Zahlen $\\lambda \\in \\mathbb{C},$ für die die Gleichung\n",
        "\n",
        "> $\n",
        "A x=\\lambda x\n",
        "$\n",
        "\n",
        "Lösungen $x \\neq 0,$ also ungleich dem Nullvektor, hat, werden Eigenwerte genannt, wobei $A$ eine Darstellungsmatrix des gewählten Endomorphismus ist. \n",
        "\n",
        "* Eigenwerte sind also Zahlen $\\lambda,$ für die das Inverse $(A-\\lambda I)^{-1}$ mit der Einheitsmatrix $I$ nicht existiert, das heißt, die Matrix $A-\\lambda I$ nicht bijektiv ist. Das ist im Endlichdimensionalen damit gleichzusetzen, dass der Endomorphismus nicht injektiv und damit auch nicht surjektiv sind. \n",
        "\n",
        "* Betrachtet man jedoch unendlichdimensionale Räume, so ist es notwendig zu unterscheiden, ob der Operator $(A-\\lambda I)$ invertierbar, nicht iniektiv und/oder nicht suriektiv ist. Im unendlichdimensionalen Fall folgt aus der Injektivität eines Endomorphismus nicht automatisch die Suriektivität, wie dies im endlichdimensionalen Fall ist. Im Folgenden wird der Begriff Spektrum in der Funktionalanalysis erlăutert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-1lpMAgXzD9"
      },
      "source": [
        "#### **Vektorraum: Lineare Abbildungen zwischen endlichdimensionalen Vektorräumen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ7r2x6ubuF8"
      },
      "source": [
        "###### **Properties of Vector Spaces & Scalars**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf4Ftp_LWjJm"
      },
      "source": [
        "**Properties of Vector Spaces : Vector Spaces form a commutative group under addition**\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "\\text { Addition: } \\quad \\vec{v}, \\vec{w} \\text { are vectors } \\Longrightarrow \\vec{v}+\\vec{w} \\text { is a vector }\\\\\n",
        "\\text { Commutativity: } \\quad \\vec{v}+\\vec{w}=\\vec{w}+\\vec{v}\\\\\n",
        "\\text { Zero vector: } \\quad 0\\\\\n",
        "\\text { Identity element: } \\quad \\mathbf{0}+\\vec{v}=\\vec{v}+\\mathbf{0}=\\vec{v}\\\\\n",
        "\\text { Inverses: } \\quad \\vec{v}+(-\\vec{v})=(-\\vec{v})+\\vec{v}=0\\\\\n",
        "\\text { Associativity: } \\quad \\vec{v}+(\\vec{w}+\\vec{z})=(\\vec{v}+\\vec{w})+\\vec{z}\n",
        "\\end{array}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpN6m6NCXFUW"
      },
      "source": [
        "**Properties of Scalars**\n",
        "\n",
        "Distributive Properties:\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "c \\cdot(\\vec{v}+\\vec{w})=c \\cdot \\vec{v}+c \\cdot \\vec{w} \\\\\n",
        "(c+d) \\cdot \\vec{v}=c \\cdot \\vec{v}+d \\cdot \\vec{v}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Associative Property:\n",
        "\n",
        "$\n",
        "c \\cdot(d \\cdot \\vec{v})=(c \\times d) \\cdot \\vec{v}\n",
        "$\n",
        "\n",
        "Action of 1: \n",
        "\n",
        "$\\quad 1 \\cdot \\vec{v}=\\vec{v}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_VCqDq0ZlKb"
      },
      "source": [
        "Polynome vom Grad 5 zB sind auch Vektorraume !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNmzS2tf1nqK"
      },
      "source": [
        "**Module Space vs Vector Space**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa9LcQnP1rU5"
      },
      "source": [
        "1. Module = Vector Space with a **ring** of scalars\n",
        "\n",
        "2. Vector Space = Module with a **field** of scalars (commutative under multiplication)\n",
        "\n",
        "Remember: A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)\n",
        "\n",
        "**Vector Space**\n",
        "\n",
        "* Abelian group $V$ of vectors\n",
        "\n",
        "* Field $F$ of \"scalars\"\n",
        "\n",
        "* $f \\cdot v$ is a \"scaled vector\"\n",
        "\n",
        "Distributive properties:\n",
        "$f \\cdot\\left(v_{1}+v_{2}\\right)=f \\cdot v_{1}+f \\cdot v_{2}$\n",
        "$\\left(f_{1}+f_{2}\\right) \\cdot v=f_{1} \\cdot v+f_{2} \\cdot v$\n",
        "\n",
        "Associative property:\n",
        "\n",
        "$\\left(f_{1} \\cdot f_{2}\\right) \\cdot v=f_{1} \\cdot\\left(f_{2} \\cdot v\\right)$\n",
        "$1 \\cdot v=v$\n",
        "\n",
        "**(Left or Right) Module**\n",
        "\n",
        "* Abelian group $M$ of \"elements\"\n",
        "\n",
        "* Ring $R$ of \"scalars\"\n",
        "\n",
        "* $r \\cdot m$ is a \"scaled element\"\n",
        "\n",
        "Distributive properties:\n",
        "\n",
        "$r \\cdot\\left(m_{1}+m_{2}\\right)=r \\cdot m_{1}+r \\cdot m_{2}$\n",
        "о $\\left(r_{1}+r_{2}\\right) \\cdot m=r_{1} \\cdot m+r_{2} \\cdot m$\n",
        "\n",
        "Associative property:\n",
        "\n",
        "$\\left(r_{1} \\cdot r_{2}\\right) \\cdot m=r_{1} \\cdot\\left(r_{2} \\cdot m\\right)$\n",
        "$1 \\cdot m=m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXJcJtv13M2S"
      },
      "source": [
        "**Example of a Module:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mC8z8gh3PYb"
      },
      "source": [
        "Group: $M=\\mathbb{R}^{3}=\\{(x, y, z) \\mid x, y, z \\in \\mathbb{R}\\}$ under $+$\n",
        "\n",
        "Scalar Ring: $R=\\left\\{\\left(\\begin{array}{lll}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33}\\end{array}\\right) \\mid a_{i j} \\in \\mathbb{R}\\right\\}$\n",
        "\n",
        "$R$ is not a field:\n",
        "* Not every matrix is invertible \n",
        "* Not commutative \n",
        "* Does have an identity: $\\left(\\begin{array}{cc}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vz4R8zD2Wle"
      },
      "source": [
        "https://youtu.be/IvukAijXgLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKEgG15rzrh1"
      },
      "source": [
        "https://www.mathe-online.at/lernpfade/lin_alg_glatz/?kapitel=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZz9KeCy6rM"
      },
      "source": [
        "**Lineare Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAL60hn09Lpe"
      },
      "source": [
        "* Konkrete form of vectors don't matter (matrices, vectors, functions, pie transforms etc). All what matters are the 8 axioms which define a vector space:\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/vectorspace_axioms.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=TgKwz5Ikpc8&list=WL&index=33&t=638s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQpZCVuqQuV_"
      },
      "source": [
        "3 most important pillars of linear algebra: https://www.youtube.com/watch?v=Ww_aQqWZhz8&t=194s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXEzBoJBps3s"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Vektorraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ebO3cJXpt8v"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lineare_Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF8ttI2RpvT4"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Lineare_Abbildung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2boqiPmMgYa"
      },
      "source": [
        "Ein Vektorraum u ̈ber einem Körper $\\mathbb K$ ist eine abelsche Gruppe mit der Eigenschaft, dass es zu jedem Element k von $\\mathbb K$ (”Skalar“) eine einstellige Operation (”Skalarmultiplikation“ )\n",
        "\n",
        "Oder anders: \n",
        "* Körper: Plus oder Mal im Körper: zwei Körperelemente, und dann kommt wieder Körperelement heraus\n",
        "* Vektorraum: (Mal heisst nur strecken und stauchen = Skalarmultiplikation: Vektor multipliziert mit Körperelement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOR2paJnqE7"
      },
      "source": [
        "* Ein Vektorraum ist ein n-dimensionaler Körper.\n",
        "\n",
        "* A vector space is an algebraic structure with operations of addition and multiplication by scalars. Gilt zusätzlich multiplikative Inverse und multiplikative Kommuntativität\n",
        "\n",
        "* The scalars are required to be elements of a field, such as the real numbers R. The basic example of a vector space is the set Rn of all vectors with n entries.\n",
        "\n",
        "* Ein Vektorraum oder linearer Raum ist eine algebraische Struktur, die in vielen Teilgebieten der Mathematik verwendet wird. \n",
        "\n",
        "* Vektorräume bilden den zentralen Untersuchungsgegenstand der linearen Algebra. \n",
        "\n",
        "* Die Elemente eines Vektorraums heißen Vektoren. \n",
        "\n",
        "* Sie können addiert oder mit Skalaren (Zahlen) multipliziert werden, das Ergebnis ist wieder ein Vektor desselben Vektorraums.\n",
        "\n",
        "* Die **Skalare, mit denen man einen Vektor multiplizieren kann, stammen aus einem Körper**. Deswegen ist ein Vektorraum immer ein Vektorraum über einem bestimmten Körper. Sehr oft handelt es sich dabei um den Körper \n",
        "$\\mathbb {R}$ der reellen Zahlen oder den Körper \n",
        "$\\mathbb {C}$ der komplexen Zahlen. Man spricht dann von einem reellen Vektorraum bzw. einem komplexen Vektorraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOnSslb8gSWP"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Vektorraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr36ywoZin3h"
      },
      "source": [
        "**K-Algebra (Algebra über einem Körper) & Algebra über einem kommutativen Ring.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8D2MAmd5Oiw"
      },
      "source": [
        "* ist ein Vektorraum über einem Körper K, der um eine mit der Vektorraumstruktur verträgliche Multiplikation erweitert wurde. \n",
        "\n",
        "* Je nach Kontext wird dabei mitunter zusätzlich gefordert, dass die Multiplikation das Assoziativgesetz oder das Kommutativgesetz erfüllt oder dass die Algebra bezüglich der Multiplikation ein Einselement besitzt.\n",
        "\n",
        "* Allgemeiner kann K ein kommutativer Ring sein, dann ist „Vektorraum“ durch „Modul“ zu ersetzen, und man erhält eine [Algebra über einem kommutativen Ring](https://de.m.wikipedia.org/wiki/Algebra_über_einem_kommutativen_Ring)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgKqC3CHiqOz"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Algebra_über_einem_Körper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd1eq64oyrkb"
      },
      "source": [
        "**Verallgemeinerung der K-Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9WU-I5tywp4"
      },
      "source": [
        "* Allgemeiner kann $K$ ein kommutativer Ring sein, dann ist „Vektorraum“ durch „Modul“ zu ersetzen, und man erhält eine [Algebra über einem kommutativen Ring](https://de.m.wikipedia.org/wiki/Algebra_%C3%BCber_einem_kommutativen_Ring)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2HsENfPHt5"
      },
      "source": [
        "**Körpererweiterungen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0dCquzPPxB"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Vektorraum#K%C3%B6rpererweiterungen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95DgzdjZ3PTj"
      },
      "source": [
        "**Assoziative Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtSIIfVH3OI_"
      },
      "source": [
        "* [Assoziative Algebra](https://de.m.wikipedia.org/wiki/Assoziative_Algebra)\n",
        "\n",
        "* Es handelt sich um eine algebraische Struktur, die den Begriff des Vektorraums bzw. des Moduls dahingehend erweitert, dass zusätzlich zur Vektoraddition eine **assoziative Multiplikation** als innere Verknüpfung definiert wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6K1nm6mJdv0"
      },
      "source": [
        "[Numerische Verfahren:](https://de.m.wikipedia.org/wiki/Liste_numerischer_Verfahren) lineare Gleichungssysteme, Eigenwerte etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jUXB20rykQ"
      },
      "source": [
        "* Lineare Abbildung zwischen Vektorräumen\n",
        "\n",
        "* Erfüllt Bedingung der Homogenität und der Additivität (siehe unter Morphismen das Thema \"Vektorraumhomomorphismus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VFhyOwKlduw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lineares_Gleichungssystem#Matrixform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qGUprIALiYY"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lineare_Algebra#Wichtige_Sätze_und_Ergebnisse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ5JeAcWXYNu"
      },
      "source": [
        "https://mathepedia.de/Lineare_Abbildungen.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZcYgLwNCNd"
      },
      "source": [
        "https://de.m.wikibooks.org/wiki/Mathematik:_Lineare_Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BR0eKuy6FCt"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diagonalmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKnoH_tc2luZ"
      },
      "source": [
        "###### **Basis, Orthonormalbasis & Orthogonalprojektion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQgmcQfC5qYe"
      },
      "source": [
        "*Basis*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZNMFPf2koN"
      },
      "source": [
        "* Jeder Vektorraum hat mindestens eine [Basis](https://de.wikipedia.org/wiki/Basis_(Vektorraum))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AucXY1NsAf6R"
      },
      "source": [
        "*Orthonormalbasis*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc1mdTTOAlfK"
      },
      "source": [
        "* **Orthogonalisieren**: zwei (oder mehr) Vektoren orthogonal zueinander bringen\n",
        "\n",
        "* **Orthonormalisieren**: diese auch noch auf Lange 1 zu normalisieren\n",
        "\n",
        "* Ziel: Vereinfachung des Rechnens mit Vektoren\n",
        "\n",
        "* Eine [Orthonormalbasis](https://de.wikipedia.org/wiki/Orthonormalbasis) (ONB) oder ein vollständiges Orthonormalsystem (VONS) in den mathematischen Gebieten lineare Algebra und Funktionalanalysis **ist eine Menge von Vektoren aus einem Vektorraum mit Skalarprodukt (Innenproduktraum), welche auf die Länge eins normiert und zueinander orthogonal sind** (daher Ortho-normal-basis) und deren lineare Hülle dicht im Vektorraum liegt. \n",
        "\n",
        "* Im endlichdimensionalen Fall ist dies eine Basis des Vektorraums. Im unendlichdimensionalen Fall handelt es sich nicht um eine Vektorraumbasis im Sinn der linearen Algebra.\n",
        "\n",
        "* Verzichtet man auf die Bedingung, dass die Vektoren auf die Länge eins normiert sind, so spricht man von einer **Orthogonalbasis**.\n",
        "\n",
        "* Der Begriff der Orthonormalbasis ist sowohl im Fall endlicher Dimension als auch für unendlichdimensionale Räume, insbesondere Hilberträume, von großer Bedeutung.\n",
        "\n",
        "* Siehe auch [Gram-Schmidtsches Orthogonalisierungsverfahren\n",
        "](https://de.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren) und [Video dazu](https://www.youtube.com/watch?v=SLEElig-C9w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRKOu3NaxflF"
      },
      "source": [
        "*Orthogonalprojektion*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8z7Ust4xjck"
      },
      "source": [
        "* Eine [Orthogonalprojektion](https://de.m.wikipedia.org/wiki/Orthogonalprojektion) bzw. orthogonale Projektion oder senkrechte Projektion ist eine Abbildung eines Punkts auf eine Gerade oder eine Ebene, sodass die Verbindungslinie zwischen dem Punkt und seinem Abbild mit dieser Gerade oder Ebene einen rechten Winkel bildet\n",
        "\n",
        "* Das Abbild hat dann von allen Punkten der Gerade oder Ebene den kürzesten Abstand zum Ausgangspunkt. \n",
        "\n",
        "* Eine Orthogonalprojektion ist damit ein Spezialfall einer Parallelprojektion, bei der die Projektionsrichtung gleich der Normalenrichtung der Gerade oder Ebene ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGddk9ZLxwUR"
      },
      "source": [
        "*Orthogonalprojektion eines Punkts P auf eine Ebene E: Der Verbindungsvektor zwischen dem Punkt und seinem Abbild $P'$ bildet mit der Ebene einen rechten Winkel.*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Orthogonal_Projection_qtl3.svg/440px-Orthogonal_Projection_qtl3.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48kLNEgXDd7"
      },
      "source": [
        "###### **Reelle Matrizen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D35xO7FqWkIf"
      },
      "source": [
        "**Singuläre & reguläre Matrix**\n",
        "\n",
        "* Eine [reguläre, invertierbare oder nichtsinguläre Matrix](https://de.m.wikipedia.org/wiki/Reguläre_Matrix) ist in der Mathematik eine quadratische Matrix, die eine Inverse besitzt\n",
        "\n",
        "* Singuläre Matrizen besitzen keine inverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyf5VgyKW_kM"
      },
      "source": [
        "**Transponierte Matrix**\n",
        "\n",
        "* bijektive, selbstinverse Abbildung einer reellen Matrix\n",
        "\n",
        "* Die [transponierte Matrix](https://de.m.wikipedia.org/wiki/Transponierte_Matrix), gespiegelte Matrix oder gestürzte Matrix ist in der Mathematik diejenige Matrix, die durch Vertauschen der Rollen von Zeilen und Spalten einer gegebenen Matrix entsteht.\n",
        "\n",
        "* die transponierte Matrix entsteht also durch Spiegelung der Ausgangsmatrix an ihrer Hauptdiagonale. \n",
        "\n",
        "* Unterschied zu symmetrischen Matrizen: diese sind quadratisch, transponierte Matrizen koennen auch zB 2x3 Matrizen sein, und symmetrische haben die gleiche Werte in der quadratischen Matrix auf den gegenuberliegenden Seiten (wahrend transponierte Matrizen auch quadratisch sein koennen, aber jeweils andere Werte bei der Speigelung)\n",
        "\n",
        "  * **Eine symmetrische Matrix ist eine quadratische Matrix, die gleich ihrer Transponierten ist**\n",
        "\n",
        "  * **Eine orthogonale Matrix ist eine quadratische Matrix, deren Transponierte gleich ihrer Inversen ist**\n",
        "\n",
        "* Viele Kenngrößen von Matrizen, wie Spur, Rang, Determinante und Eigenwerte, bleiben unter Transponierung erhalten. (zB der Rang und die Spur der transponierten Matrix gleich dem der Ausgangsmatrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_B9P7BhtCks"
      },
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LUFzmxp3P0"
      },
      "source": [
        "**Symmetrische Matrix**\n",
        "\n",
        "* sehr wichtig in Naturwissenschaften\n",
        "\n",
        "* [Symmetrische Matrizen](https://de.m.wikipedia.org/wiki/Symmetrische_Matrix) ist eine quadratische Matrix, deren **Einträge spiegelsymmetrisch bezüglich der Hauptdiagonale sind**. \n",
        "\n",
        "* Eine symmetrische Matrix stimmt demnach mit ihrer transponierten Matrix überein (ändern sich nicht durch Transponieren)\n",
        "\n",
        "* symmetrische Matrizen muessen nicht unbedingt invertierbar sein (So ist etwa jede Nullmatrix symmetrisch, aber keinesfalls invertierbar.)\n",
        "\n",
        "* Reelle symmetrische Matrizen weisen eine Reihe weiterer besonderer Eigenschaften auf:\n",
        "\n",
        "  * So ist eine reelle symmetrische Matrix **stets selbstadjungiert** (mit sich selbst  transponiert + conjugiert (=alle Vorzeichen umgekehrt)), sie besitzt nur reelle Eigenwerte und sie ist **stets orthogonal diagonalisierbar**. \n",
        "  \n",
        "  * Für komplexe symmetrische Matrizen gelten diese Eigenschaften im Allgemeinen nicht; das entsprechende Gegenstück sind dort **hermitesche Matrizen**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI49jyDlsCl3"
      },
      "source": [
        "*Symmetriemuster einer symmetrischen (5×5)-Matrix:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Matrix_symmetry_qtl1.svg/220px-Matrix_symmetry_qtl1.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9lzUq1qXNSQ"
      },
      "source": [
        "**Inverse Matrix (Invertierbarkeit)**\n",
        "\n",
        "* Die [inverse Matrix](https://de.m.wikipedia.org/wiki/Inverse_Matrix), Kehrmatrix oder kurz Inverse einer quadratischen Matrix ist eine ebenfalls **quadratische Matrix**, die **mit der Ausgangsmatrix multipliziert die Einheitsmatrix ergibt**. \n",
        "\n",
        "  * Aber: nicht jede quadratische Matrix besitzt eine Inverse; \n",
        "\n",
        "  * Allgemeiner ist jede Matrix, die eine Nullzeile enthält, nicht invertierbar.\n",
        "\n",
        "  * Allgemeiner sind Matrizen mit zwei gleichen Zeilen niemals invertierbar.\n",
        "\n",
        "* die invertierbaren Matrizen werden **reguläre Matrizen** genannt. Matrizen, die nicht invertierbar sind, heißen **singulär** .\n",
        "\n",
        "* Ist eine Matrix invertierbar, gibt es eine Lösung\n",
        "\n",
        "  * So ist etwa  A genau dann invertierbar, wenn  det⁡ A ≠ 0  gilt (Determinante)\n",
        "\n",
        "  * Matrix ist invertierbar (=regulär), wenn [Rang](https://de.m.wikipedia.org/wiki/Rang_(Mathematik)) = n (Dimension der Matrix ohne Nullzeile ist)\n",
        "\n",
        "  * Bestimmung der Inversen einer invertierbaren Matrix: Algorithmus von Gauß und Jordan zur Lösung von Gleichungssystemen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19DuDbQ90GlY"
      },
      "source": [
        "Die Inverse der reellen $(2 \\times 2)$ -Matrix\n",
        "\n",
        "$\n",
        "A=\\left(\\begin{array}{ll}\n",
        "2 & 1 \\\\\n",
        "6 & 4\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "ist\n",
        "\n",
        "$\n",
        "A^{-1}=\\left(\\begin{array}{cc}\n",
        "2 & -0,5 \\\\\n",
        "-3 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "denn es gilt\n",
        "\n",
        "$\n",
        "A \\cdot A^{-1}=\\left(\\begin{array}{ll}\n",
        "2 & 1 \\\\\n",
        "6 & 4\n",
        "\\end{array}\\right) \\cdot\\left(\\begin{array}{cc}\n",
        "2 & -0,5 \\\\\n",
        "-3 & 1\n",
        "\\end{array}\\right)=\\left(\\begin{array}{cc}\n",
        "4-3 & -1+1 \\\\\n",
        "12-12 & -3+4\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ctFO4Lp4_Y"
      },
      "source": [
        "**Orthogonale Matrix**\n",
        "\n",
        "* sehr wichtig in Naturwissenschaften\n",
        "\n",
        "* Eine [orthogonale Matrix](https://de.m.wikipedia.org/wiki/Orthogonale_Matrix) ist eine quadratische, reelle Matrix, deren Zeilen- und Spaltenvektoren orthonormal bezüglich des Standardskalarprodukts sind. **Damit ist die Inverse einer orthogonalen Matrix gleichzeitig ihre Transponierte**.\n",
        "\n",
        "> Eine reelle quadratische Matrix $Q \\in \\mathbb{R}^{n \\times n}$ heißt orthogonal, wenn das Produkt mit ihrer transponierten Matrix $Q^{T}$ die Einheitsmatrix $I$ ergibt, also $Q^{T} \\cdot Q=I$\n",
        "\n",
        "* sind invertierbar, symmetrische nicht unbedingt (So ist etwa jede Nullmatrix symmetrisch, aber keinesfalls invertierbar.)\n",
        "\n",
        "* stellen Kongruenzabbildungen im euklidischen Raum, also Drehungen, Spiegelungen und Kombinationen daraus, dar.\n",
        "\n",
        "  * [Drehmatrizen](https://de.m.wikipedia.org/wiki/Drehmatrix), also Matrizen, die eine Drehung um den Koordinatenursprung in der euklidischen Ebene beschreiben, sind orthogonal. \n",
        "\n",
        "  * [Spiegelungsmatrizen](https://de.m.wikipedia.org/wiki/Spiegelungsmatrix), also Matrizen, die eine (senkrechte) Spiegelung an einer Ursprungsgerade in der euklidischen Ebene beschreiben, sind orthogonal.\n",
        "\n",
        "  * [Permutationsmatrizen](https://de.m.wikipedia.org/wiki/Permutationsmatrix), also Matrizen, bei denen genau ein Eintrag pro Zeile und Spalte gleich eins ist und alle anderen Einträge null sind, sind orthogonal. \n",
        "\n",
        "* werden beispielsweise bei der numerischen Lösung linearer Gleichungssysteme oder Eigenwertprobleme eingesetzt\n",
        "\n",
        "* Der analoge Begriff bei komplexen Matrizen ist die unitäre Matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu2GqSJp1IvH"
      },
      "source": [
        "*Durch Multiplikation mit einer orthogonalen Matrix Q können Vektoren gedreht (links) oder gespiegelt (rechts) werden. Die Länge der Vektoren und der Winkel zwischen den Vektoren bleiben dabei erhalten*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Orthogonal_transformation_qtl1.svg/240px-Orthogonal_transformation_qtl1.svg.png)\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Orthogonal_transformation_qtl2.svg/240px-Orthogonal_transformation_qtl2.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMV87xM2TG0"
      },
      "source": [
        "Die Matrix\n",
        "\n",
        "$\n",
        "Q=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "ist orthogonal, denn es gilt\n",
        "\n",
        "$\n",
        "Q^{T} Q=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right) \\cdot\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 \\cdot 0+1 \\cdot 1 & 0 \\cdot 1+1 \\cdot 0 \\\\\n",
        "1 \\cdot 0+0 \\cdot 1 & 1 \\cdot 1+0 \\cdot 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I \\text { . }\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8TSW4f2lbv"
      },
      "source": [
        "Auch die Matrix\n",
        "\n",
        "$\n",
        "Q=\\frac{1}{5}\\left(\\begin{array}{cc}\n",
        "3 & 4 \\\\\n",
        "-4 & 3\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "ist orthogonal, denn es gilt\n",
        "\n",
        "$\n",
        "Q^{T} Q=\\frac{1}{5}\\left(\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "4 & 3\n",
        "\\end{array}\\right) \\cdot \\frac{1}{5}\\left(\\begin{array}{cc}\n",
        "3 & 4 \\\\\n",
        "-4 & 3\n",
        "\\end{array}\\right)=\\frac{1}{25}\\left(\\begin{array}{cc}\n",
        "9+16 & 12-12 \\\\\n",
        "12-12 & 16+9\n",
        "\\end{array}\\right)=\\frac{1}{25}\\left(\\begin{array}{cc}\n",
        "25 & 0 \\\\\n",
        "0 & 25\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_g3slk7tOuT"
      },
      "source": [
        "**Selbstadjungierte Matrix**\n",
        "\n",
        "* Eine [selbstadjungierte Matrix](https://de.m.wikipedia.org/wiki/Selbstadjungierte_Matrix) ist eine spezielle Art von **quadratischen Matrizen.**\n",
        "\n",
        "* Sind die Koeffizienten reell, so ist sie gerade eine **symmetrische Matrix**\n",
        "\n",
        "  * Eine reelle Matrix ist genau dann selbstadjungiert, wenn sie symmetrisch ist, also wenn $A=A^{T}$\n",
        "\n",
        "* Sind die Koeffizienten komplex, so ist sie eine **hermitesche Matrix.**\n",
        "\n",
        "  * eine komplexe Matrix genau dann selbstadjungiert, wenn sie hermitesch ist, also wenn $A=A^{*}$\n",
        "\n",
        "  * Die [Pauli-Matrizen](https://de.m.wikipedia.org/wiki/Pauli-Matrizen) sind selbstadjungiert (zur Beschreibung des Spins, Basis des 4-dimensionalen komplexen Vektorraums aller komplexen & hermetischen 2×2-Matrizen)\n",
        "\n",
        "* Jede selbstadjungierte Matrix ist auch normal, das heißt, es gilt $A^{{*}}\\cdot A=A\\cdot A^{{*}}$. Die Umkehrung gilt im Allgemeinen nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMNh-cF9ut-j"
      },
      "source": [
        "**Normale Matrix**\n",
        "\n",
        "* eine [normale Matrix](https://de.m.wikipedia.org/wiki/Normale_Matrix) ist eine Matrix, die mit ihrer adjungierten Matrix kommutiert\n",
        "\n",
        "  * eine komplexe Matrix $A\\in {\\mathbb  {C}}^{{n\\times n}}$ ist normal, wenn $A^{{*}}\\cdot A=A\\cdot A^{{*}}$\n",
        "\n",
        "  * eine reelle Matrix $B\\in {\\mathbb  {R}}^{{n\\times n}}$ ist normal, wenn $B^{{T}}\\cdot B=B\\cdot B^{{T}}$\n",
        "\n",
        "* Der [Spektralsatz](https://de.m.wikipedia.org/wiki/Spektralsatz) besagt, dass eine Matrix A genau dann normal ist, wenn es eine unitäre Matrix U gibt (orthogonal in komplex), so dass $A=UDU^{{{\\rm {*}}}}$, wobei D eine Diagonalmatrix ist. \n",
        "\n",
        "* Normale Matrizen haben also die Eigenschaft, dass sie unitär diagonalisierbar sind. Es existiert daher eine Orthonormalbasis aus Eigenvektoren von A. Die Hauptdiagonalelemente von D sind genau die Eigenwerte von A. \n",
        "\n",
        "* Insbesondere sind jede reelle symmetrische Matrix und jede komplexe hermitesche Matrix normal. Zudem ist jede unitäre Matrix normal.\n",
        "\n",
        "* [**Normaler Operator**](https://de.m.wikipedia.org/wiki/Normaler_Operator): Verallgemeinerung der normalen Matrix. Ein \"normaler Operator\" ist basisunabhängig definiert und im unendlichdimensionalen Raum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bu4c3RgXHan"
      },
      "source": [
        "###### **Komplexwertige Matrizen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfFwNW2kqZqi"
      },
      "source": [
        "**Conjugierte Matrizen**\n",
        "\n",
        "* Vertauschen der Vorzeichen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4xVuXrtNMP1"
      },
      "source": [
        "**Adjungierte Matrix** \n",
        "\n",
        "* = transponiert + conjugiert\n",
        "\n",
        "* komplexwertige Matrix, die transponiert + (komplex) conjugiert ist (Vorzeichen umgekehrt)\n",
        "\n",
        "* Eine adjungierte Matrix muss nicht quadratisch sein (kann also zB auch 2x3 Matrix sein) und auch nicht symmetrisch sein\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "B=\\left(\\begin{array}{ll}\n",
        "0 & i \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "\\\\\n",
        "B^{T}=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "i & 0\n",
        "\\end{array}\\right) \\\\\n",
        "B^{*} &=\\left(\\begin{array}{cc}\n",
        "0 & 0 \\\\\n",
        "-i & 0\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo3VtMb_QQYS"
      },
      "source": [
        "**Hermetisch**\n",
        "\n",
        "* selbstadjunktiert = symmetrisch\n",
        "\n",
        "* also wenn man die adjunktierte von A* bildet, kommt wieder A raus: A* = A\n",
        "\n",
        "* Spiegelung an der Diagonalen. Matrix muss quadratisch sein\n",
        "\n",
        "* im reellen ist es die Symmetrie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJQeE2vR-iD"
      },
      "source": [
        "**Normal**\n",
        "\n",
        "* A* A = A A*\n",
        "\n",
        "* Einheitsmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y8k2HKIQpCC"
      },
      "source": [
        "**Unitär**\n",
        "\n",
        "* Orthogonalität\n",
        "\n",
        "* Eine Matrix die invertierbar ist, ist durch die adjunktierte Matrix gegeben: A<sup>-1</sup> = A*\n",
        "\n",
        "* eine unitäre Matrix ändert nichts an den Längen / Abständen / Winkeln von Vektoren zB bei Rotationen im Raum\n",
        "\n",
        "* Verallgemeinerung von orthogonalen Matrizen in der komplexen Welt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZGRjIQPxQaI"
      },
      "source": [
        "**Polarzerlegung**\n",
        "\n",
        "* Die [Polarzerlegung](https://de.m.wikipedia.org/wiki/Polarzerlegung) ist eine spezielle Zerlegung in ein Produkt von Matrizen mit reellen oder komplexen Einträgen, und in Verallgemeinerung von linearen Operatoren auf einem Hilbert-Raum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndztinshThYu"
      },
      "source": [
        "https://youtu.be/uf-rooMdDME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16P02MVt5VFP"
      },
      "source": [
        "###### **Linear, Bilinear & Dual**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnYfeI8nbrHp"
      },
      "source": [
        "*Lineare Abbildung*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwC8xIKejbd5"
      },
      "source": [
        "https://www.youtube.com/watch?v=KK_fHodz-lQ&t=932s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVblE4wdcZeW"
      },
      "source": [
        "* Eine [lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung) ist ein Vektorraum-Homomorphismus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0DuPx0wb024"
      },
      "source": [
        "> $\\varphi: V \\longrightarrow \\omega$\n",
        "\n",
        "Eigenschaften:\n",
        "\n",
        "> $\\varphi\\left(v_{1}+v_{2}\\right)=\\varphi\\left(v_{c}\\right)+\\varphi\\left(v_{2}\\right)$\n",
        "\n",
        "> $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5An2CAZtGBO"
      },
      "source": [
        "* Eine lineare Abbildung $f\\colon V\\to V$ (also ein Endomorphismus) eines endlichdimensionalen Vektorraumes $V$ ist bereits invertierbar, wenn sie injektiv oder surjektiv ist. \n",
        "\n",
        "* Dies ist wiederum genau dann der Fall, wenn ihre Determinante ungleich null ist. \n",
        "\n",
        "* Hieraus folgt, dass die Eigenwerte eines Endomorphismus genau die Nullstellen seines charakteristischen Polynoms sind. \n",
        "\n",
        "* Eine weitere wichtige Aussage über das charakteristische Polynom ist der Satz von Cayley-Hamilton."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9HKc4DefUMp"
      },
      "source": [
        "*Linearform (= Funktional im endlichdimensionale Vektorraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEQxy1PMfdz3"
      },
      "source": [
        "* Eine [Linearform](https://de.wikipedia.org/wiki/Linearform) ist ein Objekt aus dem mathematischen Teilgebiet der linearen Algebra. Es handelt sich dabei um eine lineare Abbildung von einem Vektorraum in den zugrundeliegenden Körper.\n",
        "\n",
        "* Es sei $K$ ein Körper und $V$ ein $K$ -Vektorraum. Eine Abbildung $f: V \\rightarrow K$ heißt Linearform, wenn für alle Vektoren $x, y \\in V$ und Skalare $\\alpha \\in K$ gilt:\n",
        "\n",
        "  1. $f(x+y)=f(x)+f(y)$ (**Additivität**);\n",
        "\n",
        "  2. $f(\\alpha x)=\\alpha f(x)$ (**Homogenität**).\n",
        "\n",
        "* Im Kontext der Funktionalanalysis, das heißt im Falle eines topologischen $\\mathbb {R}$ - oder $\\mathbb {C}$-Vektorraums, sind die betrachteten Linearformen meistens stetige lineare Funktionale.\n",
        "\n",
        "* Linearform als Tensor: Eine Linearform $f$ ist ein kovarianter Tensor erster Stufe; man nennt sie deshalb manchmal auch [1-Form (Pfaffsche Form)](https://de.wikipedia.org/wiki/Pfaffsche_Form). 1-Formen bilden die Grundlage für die Einführung von Differentialformen. **Pfaffsche Formen sind die natürlichen Integranden für Wegintegrale**.\n",
        "\n",
        "* \n",
        "Eine Abbildung, die linear oder semilinear in mehr als einem Argument ist, ist eine [Sesquilinearform](https://de.wikipedia.org/wiki/Sesquilinearform), eine [Bilinearform](https://de.wikipedia.org/wiki/Bilinearform), oder allgemein eine [Multilinearform](https://de.wikipedia.org/wiki/Multilinearform) (zB kann man Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht. Dafur hat man Bilinearform! Eine bilinearform ordnet praktisch zwei Vektoren ein Zahl zu (zB Winkel).\n",
        "\n",
        "* 'Linear form' is a more modern and abstract concept of 'functional'\n",
        "\n",
        "* Im Kontext der Funktionalanalysis, das heißt im Falle eines topologischen \n",
        "$\\mathbb {R}$ - oder $\\mathbb {C}$-Vektorraums, sind die betrachteten Linearformen meistens [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale).\n",
        "\n",
        "* **Übergang zu Dualraum**: Die Menge aller Linearformen (= stetigen, linearen Abbildungen) über einem gegebenen Vektorraum $V$ bildet dessen Dualraum $V^{*}$ und damit selbst wieder in natürlicher Weise einen $K$ -Vektorraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOy1jeecu7Vy"
      },
      "source": [
        "*Linearkombination*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZVoEQOZu9xJ"
      },
      "source": [
        "* Unter einer [Linearkombination](https://de.m.wikipedia.org/wiki/Linearkombination) versteht man in der linearen Algebra einen Vektor, der sich durch gegebene Vektoren unter Verwendung der Vektoraddition und der skalaren Multiplikation ausdrücken lässt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am14siVaZVod"
      },
      "source": [
        "*Bilineare Abbildung*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PBVIB0tZaE8"
      },
      "source": [
        "* Bilinear = gemischtes Assoziativgesetz & Distributivgesetz\n",
        "\n",
        "* [Bilineare Abbildungen](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung) verallgemeinern die verschiedensten Begriffe von Produkten (im Sinne einer Multiplikation). \n",
        "\n",
        "* Die Bilinearität entspricht dem Distributivgesetz bei der normalen Multiplikation:\n",
        "\n",
        ">$\n",
        "a \\cdot(b+c)=a \\cdot b+a \\cdot c\n",
        "$\n",
        "\n",
        "* Beispiel: \n",
        "\n",
        "  * Sämtliche gemeinhin übliche Produkte sind bilineare Abbildungen: die Multiplikation in einem Körper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), \n",
        "  \n",
        "  * aber auch das Vektor- oder Kreuzprodukt, \n",
        "  \n",
        "  * und das Skalarprodukt auf einem reellen Vektorraum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-xG4GXrbJBc"
      },
      "source": [
        "*Bilinearformen*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgRnyYJxi8Nf"
      },
      "source": [
        "https://www.youtube.com/watch?v=TjAFH6hWg1I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ_Br1SeMgEs"
      },
      "source": [
        "* man kann Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht. Dafur hat man Bilinearform\n",
        "\n",
        "* **Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen (Winkel sind wichtiger Anwendungsfall dafur).** \n",
        "\n",
        "* **Jedes Skalarprodukt ist wiederum eine spezielle Bilinearform** (es gelten noch weitere Eigenschaften: symmetrisch <v,w> = <w,v>, und positiv definit). Genauso Integral.\n",
        "\n",
        "* Input sind zwei Vektoren (zB in R2 und in R3), aber Ziel ist nicht Vektorraum (wie bei linearen Abbildungen), sondern ein Koerper (zB reelle Zahlen R allgemein). Eine bilinearform ordnet praktisch zwei Vektoren ein Zahl zu (zB Winkel)\n",
        "\n",
        "* Bei diesen ist der Wertebereich $G$ mit dem Skalarkörper $K$ der Vektorräume $E$ und $F$ identisch.\n",
        "\n",
        "> $\n",
        "f: E \\times F \\rightarrow K\n",
        "$\n",
        "\n",
        "> $\\langle\\cdot, \\cdot \\rangle\\ : V \\times W \\longrightarrow K$\n",
        "\n",
        "$\\langle\\cdot, \\cdot \\rangle\\$ \n",
        "\n",
        "fur zwei Vektoren ist wie \n",
        "\n",
        "$\\varphi$\n",
        "\n",
        "bei einem Vektor.\n",
        "\n",
        "* Bilinearformen sind für die analytische Geometrie und [Dualitätstheorie](https://de.m.wikipedia.org/wiki/Dualität_(Mathematik)) wichtig.\n",
        "\n",
        "* Als [Bilinearform](https://de.m.wikipedia.org/wiki/Bilinearform#) bezeichnet man in der linearen Algebra eine Funktion, welche zwei Vektoren einen Skalarwert zuordnet und die linear in ihren beiden Argumenten ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn_GUomJiA7X"
      },
      "source": [
        "[Bilinearform](https://de.wikipedia.org/wiki/Bilinearform): cross product of two vectors, normal and tangent, see [Frenet–Serret_formulas](https://en.m.wikipedia.org/wiki/Frenet–Serret_formulas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pcDMUh0W28v"
      },
      "source": [
        "*Projektion*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9FiTDIFU438"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Projektion_(Lineare_Algebra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnXE8yB5jgSI"
      },
      "source": [
        "*Dualraum*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_3nXUY3jiow"
      },
      "source": [
        "https://www.youtube.com/watch?v=2vvjrBbcTZU&t=480s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZucZX5uoQOG"
      },
      "source": [
        "###### **Faktorraum & Untervektorraum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0xvRQ6p4xC"
      },
      "source": [
        "* Jeder Vektorraum hat mindestens eine Basis. Je zwei Basen eines Vektorraumes haben gleich viele Elemente; nur deshalb ist es sinnvoll, von der Dimension eines Vektorraumes zu sprechen. \n",
        "\n",
        "* Für Summen und Durchschnitte von [Untervektorräumen](https://de.wikipedia.org/wiki/Untervektorraum) gilt die Dimensionsformel und für die Dimensionen von [Faktorräumen](https://de.wikipedia.org/wiki/Faktorraum) die Formel $\\dim V/U=\\dim V-\\dim U$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsY5Ba9E2P1"
      },
      "source": [
        "*Lineare Hülle (Span)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AUkiDlBE4CB"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Lineare_Hülle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zppo7qtzowMx"
      },
      "source": [
        "###### **Homomorphiesatz & Rangsatz**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpn4OpISqDQ2"
      },
      "source": [
        "* Jede lineare Abbildung $f\\colon V\\to W$ ist durch die Angabe der Bilder einer Basis von $V$ eindeutig festgelegt. \n",
        "\n",
        "* Für lineare Abbildungen gelten der [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) und der [Rangsatz](https://de.wikipedia.org/wiki/Rangsatz). \n",
        "\n",
        "* Lineare Abbildungen können bezüglich fest gewählter Basen durch Matrizen dargestellt werden. Dabei entspricht der Hintereinanderausführung von linearen Abbildungen die Multiplikation ihrer Darstellungsmatrizen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km4QJ7cPpMd8"
      },
      "source": [
        "Eine weitreichende Verallgemeinerung des Rangsatzes ist die Aussage, dass die alternierende Summe der Dimensionen der einzelnen Komponenten eines [Kettenkomplexes](https://de.wikipedia.org/wiki/Kettenkomplex) gleich der alternierenden Summe der Dimensionen seiner Homologiegruppen ist. Siehe dazu die [Euler-Charakteristik eines Kettenkomplexes](https://de.wikipedia.org/wiki/Kettenkomplex#Euler-Charakteristik)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1delop5C-L"
      },
      "source": [
        "###### **Produkte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP-7CmCaNZNN"
      },
      "source": [
        "*Skalarprodukt (Dot Product)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL06JjQ0NcpH"
      },
      "source": [
        "* Das [Skalarprodukt](https://de.m.wikipedia.org/wiki/Skalarprodukt) (auch inneres Produkt oder Punktprodukt) ist eine mathematische Verknüpfung, die zwei Vektoren eine Zahl (Skalar) zuordnet.\n",
        "\n",
        "* Es ist Gegenstand der analytischen Geometrie und der linearen Algebra. Historisch wurde es zuerst im euklidischen Raum eingeführt. \n",
        "\n",
        "* Geometrisch berechnet man das Skalarprodukt zweier Vektoren $\\vec {a}$ und $\\vec {b}$ nach der Formel\n",
        "\n",
        ">$\n",
        "\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\triangleleft(\\vec{a}, \\vec{b})\n",
        "$\n",
        "\n",
        "* Dabei bezeichnen $|\\vec{a}|$ und $|\\vec{b}|$ jeweils die Längen (Beträge) der Vektoren. Mit $\\cos \\triangleleft(\\vec{a}, \\vec{b})=\\cos \\varphi$ wird der Kosinus des von den beiden Vektoren eingeschlossenen Winkels $\\varphi$ bezeichnet. Das Skalarprodukt zweier Vektoren gegebener Länge ist damit null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nre71ny4X6yj"
      },
      "source": [
        "In einem **kartesischen Koordinatensystem** berechnet sich das Skalarprodukt zweier Vektoren $\\vec{a}=\\left(a_{1}, a_{2}, a_{3}\\right)$ und $\\vec{b}=\\left(b_{1}, b_{2}, b_{3}\\right)$ als\n",
        "\n",
        ">$\n",
        "\\vec{a} \\cdot \\vec{b}=a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3}\n",
        "$\n",
        "\n",
        "Kennt man die kartesischen Koordinaten der Vektoren, so kann man mit dieser Formel das Skalarprodukt und daraufhin mit der Formel aus dem vorhergehenden Absatz den Winkel $\\varphi=\\alpha(\\vec{a}, \\vec{b})$ zwischen den beiden Vektoren ausrechnen, indem diese nach $\\varphi$ aufgelöst wird:\n",
        "\n",
        ">$\n",
        "\\varphi=\\arccos \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6h98WeW78L"
      },
      "source": [
        "*Das Skalarprodukt zweier Vektoren im euklidischen Anschauungsraum hängt von der Länge der Vektoren und dem eingeschlossenen Winkel ab.*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Dot-product-1.svg/440px-Dot-product-1.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sHjrweRYsim"
      },
      "source": [
        "*In allen drei Beispielen gilt $|\\vec{a}|=5$ und $|\\vec{b}|=3 .$ Die Skalarprodukte ergeben sich mithilfe der speziellen Kosinuswerte $\\cos 0^{\\circ}=1, \\cos 60^{\\circ}=\\frac{1}{2}$ und $\\cos 90^{\\circ}=0$ :*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twRZsFPBYgSd"
      },
      "source": [
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/skalarprodukt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaHcxvFjZCa0"
      },
      "source": [
        "Algebraische Eigenschaften des Skalarprodukt:\n",
        "\n",
        "1. Es ist symmetrisch (Kommutativgesetz):\n",
        "$\\vec{a} \\cdot \\vec{b}=\\vec{b} \\cdot \\vec{a}$ für alle Vektoren $\\vec{a}$ und $\\vec{b}$\n",
        "\n",
        "2. Es ist homogen in jedem Argument (gemischtes Assoziativgesetz):\n",
        "$(r \\vec{a}) \\cdot \\vec{b}=r(\\vec{a} \\cdot \\vec{b})=\\vec{a} \\cdot(r \\vec{b})$ für alle Vektoren $\\vec{a}$ und $\\vec{b}$ und alle Skalare $r \\in \\mathbb{R}$\n",
        "\n",
        "3. Es ist additiv in jedem Argument (Distributivgesetz):\n",
        "$\\vec{a} \\cdot(\\vec{b}+\\vec{c})=\\vec{a} \\cdot \\vec{b}+\\vec{a} \\cdot \\vec{c}$ und\n",
        "$(\\vec{a}+\\vec{b}) \\cdot \\vec{c}=\\vec{a} \\cdot \\vec{c}+\\vec{b} \\cdot \\vec{c}$ für alle Vektoren $\\vec{a}, \\vec{b}$ und $\\vec{c}$.\n",
        "\n",
        "Die Eigenschaften 2 und 3 fasst man auch zusammen zu: Das Skalarprodukt ist [bilinear](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stqsZWzKN3Pm"
      },
      "source": [
        "*Kreuzprodukt*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz_aQ1w7N7F5"
      },
      "source": [
        "* Verknüpfung von zwei Vektoren, deren Ergebnis wieder ein Vektor ist\n",
        "\n",
        "* Das [Kreuzprodukt](https://de.m.wikipedia.org/wiki/Kreuzprodukt), auch Vektorprodukt, vektorielles Produkt oder äußeres Produkt, ist eine Verknüpfung im dreidimensionalen euklidischen Vektorraum, die zwei Vektoren wieder einen Vektor zuordnet. Um es von anderen Produkten, insbesondere vom Skalarprodukt, zu unterscheiden, wird es im deutsch- und englischsprachigen Raum mit einem Malkreuz \n",
        "×\n",
        "\\times  als Multiplikationszeichen geschrieben"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2eTKpsqnW8l"
      },
      "source": [
        "*Spatprodukt*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-notr2nZTh"
      },
      "source": [
        "* Das [Spatprodukt](https://de.m.wikipedia.org/wiki/Spatprodukt), auch gemischtes Produkt genannt, ist das Skalarprodukt aus dem Kreuzprodukt zweier Vektoren und einem dritten Vektor. \n",
        "\n",
        "* Siehe auch [Parallelepiped (Spat)](https://de.m.wikipedia.org/wiki/Parallelepiped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOasbveIONjn"
      },
      "source": [
        "*Direktes Produkt*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMAHs_i5ORVZ"
      },
      "source": [
        "* ein [direktes Produkt](https://de.m.wikipedia.org/wiki/Direktes_Produkt) ist eine mathematische Struktur, die mit Hilfe des kartesischen Produkts aus vorhandenen mathematischen Strukturen gebildet wird. \n",
        "\n",
        "* Wichtige Beispiele sind das direkte Produkt von Gruppen, Ringen und anderen algebraischen Strukturen, sowie direkte Produkte von nichtalgebraischen Strukturen wie topologischen Räumen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hryEPtLGk31L"
      },
      "source": [
        "###### **Beispiele fur Vektorraume**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWYh4UmExlqY"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Vektorraum#Vektorr%C3%A4ume_mit_zus%C3%A4tzlicher_Struktur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbnBDi0xO2VA"
      },
      "source": [
        "**Funktionenraum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDqWrO6O55i"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Funktionenraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUY4T_QBO9Vj"
      },
      "source": [
        "**Folgenraum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSO-Q01APAt4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Folgenraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV02xQ1_PDzD"
      },
      "source": [
        "**Polynomräume**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRVrAArWPNOg"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Vektorraum#Polynomr%C3%A4ume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJynYy2j0olt"
      },
      "source": [
        "#### **Vektorraum: Lineare Abbildungen zwischen unendlichdimensionalen Vektorräumen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU118uUD00pd"
      },
      "source": [
        "* Insbesondere in der Funktionalanalysis betrachtet man [lineare Abbildungen zwischen unendlichdimensionalen Vektorräumen](https://de.m.wikipedia.org/wiki/Lineare_Abbildung#Lineare_Abbildungen_zwischen_unendlichdimensionalen_Vektorr%C3%A4umen). \n",
        "\n",
        "* In diesem Kontext nennt man die linearen Abbildungen meist [lineare Operatoren](https://de.m.wikipedia.org/wiki/Linearer_Operator). \n",
        "\n",
        "* Die betrachteten Vektorräume tragen meist noch die zusätzliche Struktur eines [normierten](https://de.m.wikipedia.org/wiki/Normierter_Raum) vollständigen Vektorraums. Solche Vektorräume heißen [Banachräume](https://de.m.wikipedia.org/wiki/Banachraum). \n",
        "\n",
        "* Im Gegensatz zum endlichdimensionalen Fall reicht es nicht, lineare Operatoren nur auf einer Basis zu untersuchen. Nach dem baireschen Kategoriensatz hat nämlich eine Basis eines unendlichdimensionalen Banachraums überabzählbar viele Elemente und die Existenz einer solchen Basis lässt sich nicht konstruktiv begründen, das heißt nur unter Verwendung des [Auswahlaxioms](https://de.m.wikipedia.org/wiki/Auswahlaxiom) (Zermelo-Fraenkel-Mengenlehre). \n",
        "\n",
        "* Man verwendet daher einen anderen Basisbegriff, etwa [Orthonormalbasen](https://de.m.wikipedia.org/wiki/Orthonormalbasis) oder allgemeiner [Schauderbasen](https://de.m.wikipedia.org/wiki/Schauderbasis). \n",
        "\n",
        "  * In der Funktionalanalysis wird eine Folge $\\left(b_{n}\\right)_{n \\in \\mathbb{N}}$ eines Banachraums als Schauderbasis bezeichnet, falls jeder Vektor bezüglich ihr eine eindeutige Darstellung als (unendliche) Linearkombination hat. \n",
        "  \n",
        "  * Sie ist zu unterscheiden von der [Hamelbasis](https://de.m.wikipedia.org/wiki/Basis_(Vektorraum)), von der verlangt wird, dass sich jeder Vektor als endliche Linearkombination der Basiselemente darstellen lässt.--\n",
        "\n",
        "* Damit können gewisse Operatoren wie zum Beispiel [Hilbert-Schmidt-Operatoren](https://de.m.wikipedia.org/wiki/Hilbert-Schmidt-Operator) mithilfe „unendlich großer Matrizen“ dargestellt werden, wobei dann auch unendliche Linearkombinationen zugelassen werden müssen.\n",
        "\n"
      ]
    }
  ]
}