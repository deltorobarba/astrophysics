{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "overfitting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om7NCel838Tz",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmeQ8b1-FLsh",
        "colab_type": "text"
      },
      "source": [
        "* A fundamental problem in machine learning is the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations. Overfitting is when you have a complicated model that gives worse predictions than a simpler model.\n",
        "* Regularization is a technique for preventing a model from overfitting (e.g. preventing over-fitting by penalizing a model for having large weights)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWI1wMkeFQtj",
        "colab_type": "text"
      },
      "source": [
        "## Regularization techniques against overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufh9iks53VvB",
        "colab_type": "text"
      },
      "source": [
        "* Add more data,\n",
        "* Vectornorm (L1, L2, Elastic Net) $^{1}$\n",
        "* Dropout, \n",
        "* Jitter (add noise),\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data), \n",
        "* Ensemble models, \n",
        "* Batch size (Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process) $^{2}$ \n",
        "* early stopping (this is not a formal regularization method, but can effectively limit overfitting). \n",
        "\n",
        "$^{1}$ *Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.*\n",
        "\n",
        "$^{2}$\n",
        "*Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPm2I47AE6Ed",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting: Variance-Bias-Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyEGpc3eoqwn",
        "colab_type": "text"
      },
      "source": [
        "Generally, we refer to this model as having a large variance and a small bias. That is, the model is sensitive to the specific examples, the statistical noise, in the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yIStEYAooIO",
        "colab_type": "text"
      },
      "source": [
        "![Bias Variance Tradeoff](https://raw.githubusercontent.com/deltorobarba/repo/master/bias-and-variance.png)\n",
        "\n",
        "Source: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff\n",
        "](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4vFqdUuFxO9",
        "colab_type": "text"
      },
      "source": [
        "## Benefits of regularization from a mathematical optimization point of view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQ3KBtJrVAb",
        "colab_type": "text"
      },
      "source": [
        "* Machine learning is an optimization problem, where we try to minimize a cost function to find optimal values for our model's parameter. Some machine learning models, like neural networks, have non-convex cost functions. Stationary points in these cost functions are problematic because numerical optimization schemes (like gradient descent) can easily get stuck, leading to poor results.\n",
        "* Regularization can be used as a way of ‚convexifying‘ a non-convex cost function. The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLuLUadhFDWG",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting or Overtraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIaDoXZgokpS",
        "colab_type": "text"
      },
      "source": [
        "* [Mehmet Suzen](https://www.linkedin.com/in/mehmetsuzen/): Regularisation does not prevent overfitting or even reduces. Regularisation originally developed for reducing ill-conditioning in inverse-problems. Regularisation, along with early-stopping, cross-validation and drop out, reduces and provides a reliable measure for generalisation error. Overfitting, on the other hand, is about the 'fit' , i.e., the model complexity. In deep nets, model complexity correlates with the full architecture and the activation functions. This is called 'overtraining' ([IEEE](https://ieeexplore.ieee.org/document/623200)).\n",
        "* Answer: if let it be, the learning process \"will tend to learn more and more complex functions as the number of iterations increases\". A model represented by a more complex function, thus having poor generalization, is an overfitting model. From the statement above, such a model can be prevented by stopping the learning early (among other techniques). Regularization is a process of applying those techniques. "
      ]
    }
  ]
}