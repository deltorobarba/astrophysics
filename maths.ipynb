{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y-1lpMAgXzD9",
        "04vVZZU1Q4rZ",
        "s8I3t063LsOf",
        "30ncah90297J",
        "YfOrrAOFvF6m",
        "-wUJznaApPv0",
        "yIoPrdtMpRd_",
        "xemut5KHpjBk",
        "cLeG1PswpSaL",
        "919bw_THpTYb",
        "h0hcBLukpUcp",
        "0p0KFlAypVZ9",
        "IbwJ8QbNvEbX",
        "0ChC-FyHZ41O",
        "PREcYcRRPEQ4",
        "0-Lu47J5MYMj",
        "_OexAhQsM_ex",
        "gS-QUpcFmXh1",
        "iqHduf9WDZMP",
        "jNQs-MXfBmVV",
        "Q-zfzmbO4ZOP",
        "Nj_F9wX47fTp",
        "ZiAK4jblJhPd",
        "Wkacn855ct_Q",
        "WqR3v9EaJrvV",
        "JYooOdCtJ76C",
        "MMxCx0fZKDii",
        "LUxoBU9TKYpU",
        "vD3xBKSIK3Uj",
        "q09fihf25FPw",
        "oEn5oZesJVdK",
        "HrtIDW9V0p96",
        "YWqVgPOwPCIJ",
        "Ym5Ax5nCOtuS",
        "QBuL0NbxO6cP",
        "M5NxmpI85zgL",
        "ur1A1K3aOl3W",
        "SN4YJmrE6HF3",
        "fVQl5cbp7iqO",
        "PsodM6QW8wsN",
        "zTjh6Dkf9Imc",
        "icJDI9EQ9gc0",
        "VkuVX0zf959A",
        "Eg2UlbUr-YCc",
        "GCJMH1Oj-vBg",
        "YgcZdTDc_TCw",
        "V44oHA2a_1hG",
        "VcEin6DXApdH",
        "457rg_0-BFtj",
        "vq-kSppvBbU7",
        "F4gfVMumBdd4",
        "pBNDxf7EBkj-",
        "pAHNvHgpB0r-",
        "r-rllUswCV7X",
        "8h7LKwE9CYwF",
        "ihDto5NgCay8",
        "BJaouQGICdBE",
        "QHq7RwnYCe8j",
        "UDNzWru2eKpK",
        "uOwaDiWZDTxx",
        "Nror5UBfDJ3o",
        "emoYR7MX-Q2h",
        "gmN2IqZ4FDsw",
        "nrkLBUH1hb36",
        "qYW5u_0Oi7l5",
        "0JfDpLK4KBqT",
        "4SpYIJ602L38",
        "SZkKviQcpLi5",
        "F1l9_Erf5T5S",
        "6quxRLo3U9Qf",
        "32OR42wtUS3y",
        "_Syw4dL96u31",
        "Gdr6F1zuU4MG",
        "9AR9PIXysIa4",
        "eloiR80j3LKg",
        "sprbeM9PikBv",
        "KYzlWwuA8hXl",
        "puX57AODxaUG",
        "SqYBnfUAD1yS",
        "w5zER_lMlN46",
        "DfJ1ATjNLmjN",
        "TH8b8Mbvy_Fe",
        "aGvecdZZd-Kk",
        "9Aolz28MRggQ",
        "FLeX-skuQC_8",
        "2T3n7jhWXtHv",
        "rc8jFaF5d0R1",
        "Ntt3OPDuYpRi",
        "YUy8UbeIYqx1",
        "ybqAMWlqJu7y",
        "3HEOBISTrpoC",
        "L-vCM9-DX-j2",
        "UGSV9UrCvfQh",
        "x9e3prN3rXj-",
        "b2zTmFCuVE-u",
        "u0Zw_t0Ptro-",
        "eDHpbm7feTwJ",
        "Vcm5d0r2fDD0",
        "zLvbjOH0rbDs",
        "6Nb2j8QPdg3L",
        "DKh7A9IGTuCF",
        "_3o5ynDavqaH",
        "5BSzjyZFFBdO",
        "dolnoE19IyYG",
        "Gy3Md6zcdZRL",
        "4_WdsJ1dILCs",
        "tJjGPdHbO379",
        "_JMvUMNWZHG5",
        "3iZYeNImKkLo",
        "VQdL1yhzPGzI",
        "Rs9afq_oyhKi",
        "9F9ROmFYzbtx",
        "sOKlhEom4-yx",
        "Pf3GY2W7nkkL",
        "WS0hoJa5wico",
        "xDFYb8uW7h9k",
        "-TPsQ4p0KZ2L",
        "CQu9zwVlu7sn",
        "w3UozhzpvHs2",
        "L5JSmpl71sGx",
        "nbxMBXJa5GRY",
        "0USsGJE7Tfz3",
        "PhQKZZ-1M-GE",
        "il2F19KA_-h1",
        "TJ0lhhj3e3Mi",
        "R40nIcrRpM0F",
        "521_dSZMrpGC",
        "LxkcVzsECAEO",
        "xcWGR3zwT-Fo",
        "kJlRjb0F8ULh",
        "SOpIknui8ULm",
        "YUvl0oGH8ULh",
        "0SjFey3T8ULx",
        "if8_02pL8ULy",
        "ZECPGokG8UL3",
        "bbSSWxK78UL5",
        "PkPdLE5H8UL-",
        "hu1qRXiA8UL-",
        "XiU5eKGE8UMH",
        "Kg26k2KQ8UMK",
        "dnkSoEwu8UMN",
        "YQYPe_Rt8UMO",
        "F0hPCoGn8UMR",
        "Q9cYkVi78UMV",
        "lh3x3_1K_e1m",
        "b8EVGNJmEfDo",
        "Qa3-DIbUt0Ri",
        "sxfXe3_nNrjv",
        "8yH2OFqH-37c",
        "f4lHqJyMa24i",
        "2KRiQv6BoMmA",
        "O8cFhBpbIFFd",
        "7bWObSh9Mh0N",
        "OeO_i6Z9-rLD",
        "xDUD9L1G7HIj",
        "fq5mO57kWnjQ",
        "OkGNbw_qosx7",
        "pHwO7VW3o75X",
        "4JijVDdYpeTw",
        "RTdYXBryoZqW",
        "fhdD4JnqZHKf",
        "hIwXOz85zxW_",
        "_DcbvY2o-JyH",
        "m066sar7fCUi",
        "U3xyzZzNOEhv",
        "82Lb_gBqUzi8",
        "y1flpzoQFTVa",
        "8lpmflh50O1X",
        "8chCbs1o_Iud",
        "8idRoUCpsoKR",
        "vUbYRNP_zXe4",
        "m0K1gPOoznzE",
        "LVOw3me6qbku",
        "ECcHaTG2nKiM",
        "P4KDiI20GSS8",
        "JGhitrlOQEHM",
        "T4A4TeH7SewU",
        "ErN__bggDh9m",
        "Vc8T8hpoiWFR",
        "IoZiS2oORjId",
        "4S4z_HeLRbtc",
        "2Aoj2PpxvDQV",
        "wc37qhDWywkD",
        "_j3LHUI5y6Co",
        "qCTiyeYsy0Y0",
        "fk2nIlNKjMY4",
        "F4uWOy1kiJIi",
        "h8VPlSUFiaHF",
        "sCrxJgguiyCW",
        "F1FnAXf0DeCY",
        "DjYXE5LZhFXn",
        "UvgTnPqOhs3q",
        "_cMWr-Beh3wr",
        "G1TQJG1ufr-D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/maths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Mathematics ü¶ã**"
      ],
      "metadata": {
        "id": "2VU6BC9nYCGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1000.png)"
      ],
      "metadata": {
        "id": "EChUkdiQsKjJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-1lpMAgXzD9"
      },
      "source": [
        "### <font color=\"blue\">**Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Linear Algebra*"
      ],
      "metadata": {
        "id": "04vVZZU1Q4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Linear algebra for Quantum Mechanics](https://www.youtube.com/watch?v=FF05fXg03A0)"
      ],
      "metadata": {
        "id": "ihAL0jVFhV5o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8I3t063LsOf"
      },
      "source": [
        "###### *Eigenwerte*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQKdBUmBB1w"
      },
      "source": [
        "**Kriterien fur die Existenz von Eigenwerten**\n",
        "\n",
        "(*Wenn eine dieser Aussagen wahr ist, dann alle. Und wenn eine falsch, dann sind alle falsch*)\n",
        "\n",
        "1. $\\operatorname{rg}(B) < n$\n",
        "\n",
        "2. $\\operatorname{det}(B) = 0$ -> dieses Kriterium zu prufen ist am einfachsten und daher am haufigsten!\n",
        "\n",
        "3. $B^{-1}$ existiert nicht (nicht invertierbar)\n",
        "\n",
        "4. $B \\vec{X}$ = 0 hat mehr als nur die Losung $\\vec{x}$ = 0\n",
        "\n",
        "5. $\\lambda$ = 0 ist ein Eigenwert von $B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RIOxr9LuLh"
      },
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1192.png)\n",
        "\n",
        "*Calculating Eigenvalues via determinant: The tweaked transformation squishes space into a lower dimension (Daher muss rang < n sein)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rayleigh-Quotient & Satz von Courant-Fischer*"
      ],
      "metadata": {
        "id": "30ncah90297J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), auch Rayleigh-Koeffizient genannt, ist ein Objekt aus der linearen Algebra.\n",
        "\n",
        "Der Rayleigh-Quotient wird insbesondere zur numerischen Berechnung von Eigenwerten einer quadratischen Matrix $A$ verwendet.\n",
        "\n",
        "Sei $A \\in {\\mathbb K}^{n \\times n}$ eine reelle symmetrische oder komplexe hermitesche Matrix und $x\\in {\\mathbb {K} }^{n}$ mit $x\\neq 0$ ein Vektor, dann ist der Rayleigh-Quotient von $A$ zum Vektor $x$ definiert durch:\n",
        "\n",
        "> $R_{A}(x)={\\frac  {x^{*}Ax}{x^{*}x}}$\n",
        "\n",
        "Der Rayleigh-Quotient hat eine enge Beziehung zu den Eigenwerten von $A$. Ist $v$ ein Eigenvektor der Matrix $A$ und $\\lambda$  der zugeh√∂rige Eigenwert, dann gilt:\n",
        "\n",
        "> $R_{A}(v)={\\frac  {v^{*}Av}{v^{*}v}}={\\frac  {v^{*}\\lambda v}{v^{*}v}}=\\lambda$\n",
        "\n",
        "Durch den Rayleigh-Quotienten wird also jeder Eigenvektor von $A$ auf den dazugeh√∂rigen Eigenwert $\\lambda$ abgebildet. Diese Eigenschaft wird unter anderem in der numerischen Berechnung von Eigenwerten benutzt.\n",
        "\n",
        "Insbesondere gilt f√ºr eine symmetrische oder hermitesche Matrix $A$ mit dem kleinsten Eigenwert $\\lambda _{{{\\rm {min}}}}$ und dem gr√∂√üten Eigenwert $\\lambda _{{{\\rm {max}}}}$ nach dem [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) (Der Satz von Courant-Fischer stellt die Eigenwerte einer symmetrischen oder hermiteschen Matrix als minimale beziehungsweise maximale Rayleigh-Quotienten):\n",
        "\n",
        "> $\\lambda _{{{\\rm {min}}}}\\leq R_{A}(x)\\leq \\lambda _{{{\\rm {max}}}}$\n",
        "\n",
        "Die Berechnung des kleinsten bzw. gr√∂√üten Eigenwerts ist damit √§quivalent zum Auffinden des Minimums bzw. Maximums des Rayleigh-Quotienten. Das l√§sst sich unter geeigneten Voraussetzungen auch noch auf den unendlichdimensionalen Fall verallgemeinern und ist als Rayleigh-Ritz-Prinzip bekannt.\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem Ellipsoid (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "q7aw0uQL3AnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kernel*"
      ],
      "metadata": {
        "id": "YfOrrAOFvF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel = Nullspace**\n",
        "\n",
        "The [kernel of this linear map (linear algebra)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) is the set of solutions to the equation Ax = 0, where 0 is understood as the zero vector."
      ],
      "metadata": {
        "id": "nHdImLnovH1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In algebra, the [kernel (algebra)](https://en.m.wikipedia.org/wiki/Kernel_(algebra)) of a homomorphism (function that preserves the structure) is generally the inverse image of 0.\n",
        "\n",
        "* The kernel of a homomorphism is reduced to 0 (or 1) if and only if the homomorphism is injective, that is if the inverse image of every element consists of a single element (jedes element im ziel hat nur ein element im ursprung, es kann aber mehr elemente im ziel ohne ein element im ursprung geben).\n",
        "\n",
        "* This means that the kernel can be viewed as a measure of the degree to which the homomorphism fails to be injective.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/320px-KerIm_2015Joz_L2.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "hLbt81t-PVPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "der Kern einer linearen Abbildung f genau dann nichttrivial ist, wenn eine linear unabh√§ngige Menge (bzw. genauer Familie) S existiert sodass f(S) linear abh√§ngig ist. Durch √úbergang zu den Negationen erhalten wir dann die √§quivalente Aussage:\n",
        "\n",
        "- kerf = {0}\n",
        "- ...genau dann, wenn gilt...\n",
        "- f√ºr alle linear unabh√§ngigen S ist auch f(S) linear unabh√§ngig.\n",
        "\n",
        "https://www.youtube.com/watch?v=GNf3StvaiFA&list=WL&index=9"
      ],
      "metadata": {
        "id": "UbcK-n4ZJ5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Diagonalisierbarkeit*"
      ],
      "metadata": {
        "id": "-wUJznaApPv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-5FB4sQ_XI0"
      },
      "source": [
        "**[Diagonalisierbarkeit](https://en.m.wikipedia.org/wiki/Diagonalizable_matrix) von Matrizen**:\n",
        "\n",
        "* **Eigenwerte liegen auf der Diagonalen**. Dabei: Spur = Summer aller Eigenwerte. Determinante = Produkt aller Eigenwerte = 0.\n",
        "\n",
        "* Square matrix $A$ into **invertible matrix** $P$ and **diagonal matrix** $D$ such that  ${\\displaystyle P^{-1}AP=D}$. Approach: [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix).\n",
        "\n",
        "> $P^{-1} A P=\\left[\\begin{array}{cccc}\n",
        "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right]$\n",
        "\n",
        "* Ein rotierender K√∂rper ohne √§u√üere Kr√§fte verbleibt in seiner Bewegung, wenn er um seine Symmetrieachse rotiert. Wenn eine Basis aus Eigenvektoren existiert, so ist die Darstellungsmatrix bez√ºglich dieser Basis eine Diagonalmatrix\n",
        "\n",
        "* *The diagonalization of a symmetric matrix can be interpreted as a rotation of the axes to align them with the eigenvectors:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/4/4e/Diagonalization_as_rotation.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Determinant (Permanent, Immanant)*"
      ],
      "metadata": {
        "id": "yIoPrdtMpRd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Determinante](https://de.m.wikipedia.org/wiki/Determinante)**:\n",
        "\n",
        "* [Determinante](https://de.m.wikipedia.org/wiki/Determinante) (nur fur quadratischen Matrix - bei $n \\cdot m$ Matrizen nutzt man zB SVD)\n",
        "\n",
        "  * gibt an, wie sich Fl√§che bzw. Volumen durch linearen Abbildung √§ndert. det(A) = 4 $\\rightarrow$ Matrix vervierfacht Fl√§cheninhalt.\n",
        "\n",
        "  * gibt an, ob lineares Gleichungssystem l√∂sbar ist (L√∂sung dann mit der Cramerschen Regel)\n",
        "\n",
        "  * Determinante ist Produkt aller Eigenwerte: $\\prod_{i=1}^{n} \\lambda_{i}=\\operatorname{det}(A) = 0$\n",
        "\n",
        "* Berechnung: $\\operatorname{det}(A)$: $\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right)\n",
        "$ $\\rightarrow$ $\n",
        "\\operatorname{det} A=\\left|\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$. *Die 2x2-Determinante **ist gleich dem orientierten Fl√§cheninhalt** des von ihren Spaltenvektoren aufgespannten Parallelogramms:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Area_parallellogram_as_determinant.svg/220px-Area_parallellogram_as_determinant.svg.png)\n",
        "\n",
        "* [Entwicklungssatz](https://de.m.wikipedia.org/wiki/Determinante#Laplacescher_Entwicklungssatz) oder  [Leibniz-Formel](https://de.m.wikipedia.org/wiki/Determinante#Leibniz-Formel) (geschlossene Form, theoretisch). [Gau√ü-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren) in Dreiecksform - Determinante ist Produkt der [Hauptdiagonale](https://de.m.wikipedia.org/wiki/Hauptdiagonale). Computeralgorithmus: [LU-Zerlegung](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren#LR-Zerlegung).\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) (=has the eigenvalues as roots, and it has the determinant and the trace / sum o fEigenvalues of the matrix among its coefficients):\n",
        "\n",
        "> <font color=\"red\">$f(\\lambda) = \\operatorname{det}(A) = 0$</font> $\\quad (= \\prod_{i=1}^{n} \\lambda_{i})$\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, <font color=\"red\">**the solutions of $f(\\lambda) = 0$** (determinant is equal to zero, Eigenvalues are \"coefficients in characteristic polynomial\" (it's trace to be precise))</font> are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "J1Ir4QyrBark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Permanent_(mathematics)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Immanant"
      ],
      "metadata": {
        "id": "U3D3fAGop0ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Characteristics polynomial (Fundamental theorem of algebra)*"
      ],
      "metadata": {
        "id": "xemut5KHpjBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Characteristics polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) = has the Eigenvalues as [roots](https://en.m.wikipedia.org/wiki/Zero_of_a_function) (zero of a function)\n",
        "\n",
        "Root (Zero of a function): The function f attains the value of 0 at x, or equivalently, x is the solution to the equation f(x) = 0. A root of a polynomial is a zero of the corresponding polynomial function. The [fundamental theorem of algebra](https://en.m.wikipedia.org/wiki/Fundamental_theorem_of_algebra) shows that any non-zero polynomial has a number of roots at most equal to its degree.\n",
        "\n",
        "For example, the polynomial f of degree two, defined by $f(x)=x^{2}-5x+6$ has the two roots (or zeros) that are $\\lambda_1=2$ and $\\lambda_2=3$:\n",
        "\n",
        "> ${\\displaystyle f(2)=2^{2}-5\\times 2+6=0{\\text{ and }}f(3)=3^{2}-5\\times 3+6=0.}$\n",
        "\n",
        "<font color=\"red\">Zero of a function is important because every equation in the unknown x may be rewritten as $f(x)=0$ by regrouping all the terms in the left-hand side. Hence the study of zeros of functions is exactly the same as the study of solutions of equations.</font>\n",
        "\n",
        "Siehe [Nullstellen](https://de.m.wikipedia.org/wiki/Nullstelle) sind bei einer Funktion diejenigen Werte der Ausgangsmenge (des Definitionsbereichs D), bei denen das im Rahmen der Abbildung zugeordnete Element der Zielmenge (des Wertebereichs W) die Null ist (${\\displaystyle 0\\in W}$). Nullstellen von Polynomfunktionen werden auch als Wurzeln bezeichnet.\n",
        "\n",
        "[Kern](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) is L√∂sungsmenge der [homogenen linearen Gleichung](https://de.m.wikipedia.org/wiki/Lineare_Gleichung) f(x)=0 und wird hier auch Nullraum genannt (denjenigen Vektoren in V, die auf den Nullvektor in W abgebildet werden)\n",
        "\n",
        "Example: A graph of the function $\\cos (x)$ for $x$ in $[-2 \\pi, 2 \\pi]$, with zeros at $-\\frac{3 \\pi}{2},-\\frac{\\pi}{2}, \\frac{\\pi}{2}$, and $\\frac{3 \\pi}{2}$, marked in red.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/X-intercepts.svg/480px-X-intercepts.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "0jYqsd3QptJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Invertible Matrix*"
      ],
      "metadata": {
        "id": "cLeG1PswpSaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Invertible Matrix**\n",
        "\n",
        "* ($B^{-1}$ existiert nicht (Matrix nicht invertierbar) $\\rightarrow$ dann existieren Eigenwerte)\n",
        "\n",
        "* [Invertible matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix) is used in methods to solve systems of linear equations and **help to get Eigenvalues**\n",
        "\n",
        "> Apply Eigendecomposition to matrix (or other method) $\\rightarrow$ Get matrix inverse (with Eigenvectors and a diagonal with Eigenvalues) $\\rightarrow$ Solve systems of linear equations\n",
        "\n",
        "* You can also use pseudo-inverse [Moore-Penrose](https://en.m.wikipedia.org/wiki/Moore‚ÄìPenrose_inverse) with [matrix solution](https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution) to solve systems of linear equations or in [curve fitting](https://de.wikipedia.org/wiki/Ausgleichungsrechnung) (like regression or ML). Methods: QR, Cholesky, Rank decomposition, SVD. 'The pseudoinverse provides a [Linear Least Squares](https://en.m.wikipedia.org/wiki/Linear_least_squares) solution to a system of linear equations.'\n",
        "\n",
        "  * Exkurs: [Linear least squares (LLS)](https://en.wikipedia.org/wiki/Linear_least_squares) for solving systems of linear equations: is the least squares approximation of linear functions to data (linear regression). **Numerical methods include inverting the matrix of the normal equations and orthogonal decomposition methods**.\n",
        "\n",
        "  * **Overdetermined case**: A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution. See under [Applications](https://en.m.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Applications)\n",
        "\n",
        "  * **Underdetermined case**: Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. [Underdetermined system](https://en.m.wikipedia.org/wiki/Underdetermined_system): there are **fewer equations than unknowns**. Has an infinite number of solutions, if any. In optimization problems that are subject to linear equality constraints, only one of the solutions is relevant, namely the one giving the **highest or lowest value of an objective function**. The use of the **(Moore Pensore) pseudoinverse is to find the minimum (Euclidean) norm solution** to a system of linear equations with multiple solutions. Can be computed using the singular value decomposition. *Example: The solution set for two equations in three variables is, in general, a line ([Source](https://en.m.wikipedia.org/wiki/System_of_linear_equations#General_behavior)):*\n",
        "\n",
        "  ![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Intersecting_Planes_2.svg/240px-Intersecting_Planes_2.svg.png)\n",
        "\n",
        "* [Methods to compute inverse of matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion): Gaussian elimination, Newton's method, Cayley‚ÄìHamilton method, Eigendecomposition, Cholesky decomposition, Analytic solution (Cramer's rule), Blockwise inversion.\n",
        "\n",
        "* Example: If matrix A can be [eigendecomposed](https://en.m.wikipedia.org/wiki/Invertible_matrix), and if none of its eigenvalues are zero, then A is invertible and its inverse is given by\n",
        "\n",
        ">${\\displaystyle \\mathbf {A} ^{-1}=\\mathbf {Q} \\mathbf {\\Lambda } ^{-1}\\mathbf {Q} ^{-1}}$\n",
        "\n",
        "* where $\\mathbf {Q}$  is the square ($N√óN$) matrix whose i-th column is the **eigenvector** $q_{i}$ of $\\mathbf {A}$\n",
        "* ${\\displaystyle \\mathbf {\\Lambda } }$ is the diagonal matrix whose diagonal elements are the corresponding **eigenvalues** ${\\displaystyle \\Lambda _{ii}=\\lambda _{i}}$."
      ],
      "metadata": {
        "id": "49Uc9N-uV6_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rank*"
      ],
      "metadata": {
        "id": "919bw_THpTYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a matrix**\n",
        "\n",
        "\n",
        "The rank of a matrix is the number of linearly independent vectors (columns).\n",
        "\n",
        "So **low rank is a low number** of linearly independent vectors. A low rank matrix can be this:\n",
        "\n",
        "1\n",
        "\n",
        "2\n",
        "\n",
        "3\n",
        "\n",
        "\n",
        "* a low rank matrix (whether approximation or not) is simply a matrix for which the number of linearly independent row or columns is much smaller than the actual number of rows or columns.\n",
        "\n",
        "* Viewed as a linear transformation, the span of its range is small or the span of its null space is large.\n",
        "\n",
        "* Quantum computers give exponential speedups for high rank matrices"
      ],
      "metadata": {
        "id": "t9uYK8aS8akn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Sparse Matrix vs Low Rank Matrix**\n",
        "\n",
        "Sparse matrices and low-rank matrices are two very different types of objects. The matrix\n",
        "\n",
        "$\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4\\end{array}\\right]$\n",
        "\n",
        "\n",
        "is sparse (meaning it has a lot of zero entries) but not low-rank (as a matter of fact it‚Äôs full rank). On the other hand, the matrix\n",
        "\n",
        "$\\left[\\begin{array}{cccc}1 & 2 & 3 & 4 \\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 9 & 12 \\\\ 4 & 8 & 12 & 16\\end{array}\\right]$\n",
        "\n",
        "is low-rank but not sparse!\n",
        "\n",
        "There's however a connection to be made between the two concepts: **a low-rank matrix has a sparse set of singular values**.\n",
        "\n",
        "Take the following singular value decomposition for a general matrix $\\mathrm{X}$\n",
        "\n",
        "> $\n",
        "\\mathrm{X}=\\mathrm{USV}^T\n",
        "$\n",
        "\n",
        "Then the number of non-zero entries in (the diagonal of) $S$ is precisely equal to the rank of $\\mathbf{X}$. Thus, a sparse $\\mathbf{S}$ leads to a low-rank $\\mathbf{X}$ and vice-versa.\n"
      ],
      "metadata": {
        "id": "4eqnSMcl83mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Algebraic and geometric multiplicity of eigenvalues*"
      ],
      "metadata": {
        "id": "h0hcBLukpUcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic and geometric multiplicity of eigenvalues**\n",
        "\n",
        "> Two or more distinct eigenvalues = algebraic multiplicity\n",
        "\n",
        "> By how many linearly independent vectors is the Eigenspace of $\\lambda_1$ (which is an Eigenvalue with multiplicity 1 or more) formed? = geometric multiplicity\n",
        "\n",
        "> Geometric multiplicity is max equal or less than its algebraic multiplicity\n",
        "\n",
        "> When the geometric multiplicity of a repeated eigenvalue is **strictly less** than its algebraic multiplicity, then that eigenvalue is said to be **defective**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multiplicity_(mathematics)#Multiplicity_of_a_root_of_a_polynomial\n",
        "\n",
        "*algebraic multiplicity*\n",
        "\n",
        "The **algebraic multiplicity** of an eigenvalue is the number of times it appears as a root of the characteristic polynomial (i.e., the polynomial whose roots are the eigenvalues of a matrix).\n",
        "\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its characteristic polynomial is:\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, the solutions of $f(\\lambda) = 0$  are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Thus, $A$ has one repeated eigenvalue whose algebraic multiplicity is\n",
        "\n",
        ">$\n",
        "\\mu\\left(\\lambda_1\\right)=\\mu\\left(\\lambda_2\\right)=2\n",
        "$\n",
        "\n",
        "*geometric multiplicity*\n",
        "\n",
        "The **geometric multiplicity** of an eigenvalue is the dimension of the linear space of its associated eigenvectors (i.e., its eigenspace).\n",
        "\n",
        "If the Eigenspace of $\\lambda_1$ is generated only by a single vector, it has dimension 1. As a consequence, the geometric multiplicity of $\\lambda_1$ is 1, less than its algebraic multiplicity, which is equal to 2.\n",
        "\n",
        "See complete example in [this pdf document](https://raw.githubusercontent.com/deltorobarba/repo/master/multiplicity.pdf)."
      ],
      "metadata": {
        "id": "VR06jupDflnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Condition Numbers*"
      ],
      "metadata": {
        "id": "0p0KFlAypVZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Condition Numbers**\n",
        "\n",
        "https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/"
      ],
      "metadata": {
        "id": "0JY7J0768m4Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwJ8QbNvEbX"
      },
      "source": [
        "###### *Get Eigenvalues: **Algorithms** (Power & Inverse Iteration, Charakteristisches Polynom) & **Matrix Decomposition** (Eigendecomposition/Spectrum, Schur, SVD)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenvalue algorithm](https://en.m.wikipedia.org/wiki/Eigenvalue_algorithm) - matrices are diagonalized numerically using computer software. See [List of Eigenvalue algorithms](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Eigenvalue_algorithms)."
      ],
      "metadata": {
        "id": "cjzaJcIsQrtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbDi3hDN8p4"
      },
      "source": [
        "**Small Matrices: Ermittlung der Eigenwerte der Matrix $A$ mit Determinante und charakteristischem Polynom**\n",
        "\n",
        "In practice, eigenvalues of large matrices are not computed using the characteristic polynomial [Source](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Numerical_computations)\n",
        "\n",
        "> $A=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)$\n",
        "\n",
        "**Step 1: Bilde mit der Einheitsmatrix $E_{n}$ die Matrix $\\left(A-\\lambda E_{n}\\right)$**\n",
        "\n",
        "> $\\left(A-\\lambda E_{n}\\right)=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)-\\left(\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{array}\\right)=\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "**Step 2: Berechne die Determinante von $\\operatorname{det}\\left(A-\\lambda E_{n}\\right) = \\chi_{A}(\\lambda)$ $\\rightarrow$ [charakteristisches Polynom](https://de.m.wikipedia.org/wiki/Charakteristisches_Polynom)**\n",
        "\n",
        "> $\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=\\operatorname{det}\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "$=(2-\\lambda)^{2} \\cdot(3-\\lambda)+2+2-2 \\cdot(2-\\lambda)-2 \\cdot(2-\\lambda)-(3-\\lambda)$\n",
        "$=-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5$ $\\quad$ (= Polynom)\n",
        "\n",
        "**Step 3: Bestimme die Nullstellen des charakteristischen Polynoms, weil das sind die Eigenwerte der Matrix $A$ (und Determinante wird Null)**\n",
        "\n",
        "> $(A-\\lambda E_{n}) \\cdot v=0$\n",
        "\n",
        "* Durch Ausprobieren: erste Nullstelle $\\lambda_{1}=1$.\n",
        "\n",
        "* Klammern wir dann den Faktor $(\\lambda-1)$ aus, erhalten wir:\n",
        "$-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5=(\\lambda-1) \\cdot\\left(-\\lambda^{2}+6 \\lambda-5\\right)\n",
        "$.\n",
        "\n",
        "* Anwendung der [Mitternachtsformel](https://de.m.wikipedia.org/wiki/Quadratische_Gleichung#L√∂sungsformel_f√ºr_die_allgemeine_quadratische_Gleichung_(a-b-c-Formel)): $\n",
        "\\lambda_{2,3}=\\frac{-6 \\pm \\sqrt{36-20}}{-2}=3 \\mp 2$\n",
        "\n",
        "* Somit lauten die drei Eigenwerte der Matrix $\\lambda_{1}=\\lambda_{2}=1$ sowie $\\lambda_{3}=5$\n",
        "\n",
        "*Gibt es eine Zahl $\\lambda$ und einen Vektor $v$, sodass dieser durch Multiplikation mit der Matrix $\\left(A-\\lambda E_{n}\\right)$ auf den Nullvektor abgebildet wird, so ist diese Matrix nicht von vollem Rang. Das bedeutet, dass ihre Determinante Null ist. Vektoren wie $v$ und $w$ sind linear abh√§ngig.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooWX-rgS12w"
      },
      "source": [
        "**Eigendecomposition (spectral decomposition)**\n",
        "\n",
        "* [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or sometimes spectral decomposition, see [spectral theorem](https://en.m.wikipedia.org/wiki/Spectral_theorem), is the factorization of a matrix into a canonical form (Normalform) - the matrix is represented in terms of its eigenvalues and eigenvectors. Matrix must be diagonalizable. See also [Summary of Eigendecomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition#Decompositions_based_on_eigenvalues_and_related_concepts).\n",
        "\n",
        "> ${\\displaystyle A=VDV^{-1}}A=VDV^{{-1}}$\n",
        "\n",
        "* $D$ = eigenvalues of $A$ (diagonal)\n",
        "* $V$ = eigenvectors of $A$\n",
        "\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur decomposition**\n",
        "\n",
        "* [Schur decomposition](https://en.m.wikipedia.org/wiki/Schur_decomposition) is a [matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition).\n",
        "\n",
        "* Take complex square matrix and **get an upper triangular matrix whose diagonal elements are the eigenvalues** of the original matrix."
      ],
      "metadata": {
        "id": "wqlosIz9egUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singular Value Decomposition**\n",
        "\n",
        "* [Singular Value Decomposition](https://de.m.wikipedia.org/wiki/Singul√§rwertzerlegung) is used in calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.\n",
        "\n",
        "* SVD can also be used in least squares linear regression, image compression, and denoising data. Siehe auch [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm)\n"
      ],
      "metadata": {
        "id": "lCrTpBOyevh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Decomposition** (Gaussian Elimination, Cholesky, QR, LU)*"
      ],
      "metadata": {
        "id": "0ChC-FyHZ41O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical: [Gaussian Elimination](https://en.m.wikipedia.org/wiki/Gaussian_elimination) (row reduction). See [List of numerical algorithms to solve systems of linear equation](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Solving_systems_of_linear_equations)"
      ],
      "metadata": {
        "id": "KzcuDJOLhCGd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8EITyoKDXA"
      },
      "source": [
        "**Cholesky Decomposition** (Hermitian / squared)\n",
        "\n",
        "* **alternative to Eigendecomposition** to get matrix inverse for solving linear equations\n",
        "\n",
        "* [Cholesky decomposition](https://en.m.wikipedia.org/wiki/Cholesky_decomposition) of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,\n",
        "\n",
        "* useful for efficient numerical solutions, e.g., Monte Carlo simulations (solving linear least squares for linear regression, as well as simulation and optimization methods)\n",
        "\n",
        "* When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.\n",
        "\n",
        "> $A = LL^T$\n",
        "\n",
        "* $L$ is the lower triangular matrix and $L^T$ is the transpose of L. Decompose as the product of upper triangular matrix $U$ is also possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NVXqSfFlBt"
      },
      "source": [
        "**QR Decomposition**\n",
        "\n",
        "* We can use QR decomposition to [find the determinant of a square matrix](https://en.m.wikipedia.org/wiki/QR_decomposition#Connection_to_a_determinant_or_a_product_of_eigenvalues)\n",
        "\n",
        "* The [QR decomposition](https://en.m.wikipedia.org/wiki/QR_decomposition) is for m x n matrices (not limited to square matrices) and decomposes a matrix into $Q$ (orthogonal $Q^T Q = I$ or unitary $Q \\cdot Q = I$, size m x m) and $R$ (upper triangle matrix with the size m x n) components.\n",
        "\n",
        "> $A = Q R$\n",
        "\n",
        "* Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, **although is <u>not</u> limited to square matrices**.\n",
        "\n",
        "* There are several methods for actually computing the QR decomposition, such as by [Householder transformations](https://de.m.wikipedia.org/wiki/Householdertransformation), [Givens rotations](https://de.m.wikipedia.org/wiki/Givens-Rotation) and  [Gram-Schmidtsch's orthogonalization method](https://de.m.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDrmirVPSXe"
      },
      "source": [
        "**LU Decomposition**\n",
        "\n",
        "* The [LU decomposition](https://en.m.wikipedia.org/wiki/LU_decomposition) is often used to simplify the **solving of systems of linear equations**, such as **finding the coefficients in a linear regression**, as well as in **calculating the determinant and inverse** of a matrix.\n",
        "\n",
        "* Lower‚Äìupper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix.\n",
        "\n",
        "* The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian elimination.\n",
        "\n",
        "* Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix.\n",
        "\n",
        "The **LU decomposition is for square matrices** and decomposes a matrix into L and U components. Let A be a square matrix. An LU factorization refers to the factorization of A, with proper row and/or column orderings or permutations, into two factors ‚Äì a **lower triangular matrix L** and an **upper triangular matrix U**:\n",
        "\n",
        "> A = L U\n",
        "\n",
        "* The LU decomposition is found using an <u>iterative numerical process</u> and **can fail for those matrices that cannot be decomposed or decomposed easily**.\n",
        "\n",
        "* In the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero. For example, for a 3 √ó 3 matrix A, its LU decomposition looks like this:\n",
        "\n",
        "> $\\left[\\begin{array}{lll}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
        "u_{11} & u_{12} & u_{13} \\\\\n",
        "0 & u_{22} & u_{23} \\\\\n",
        "0 & 0 & u_{33}\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Type** (Squared & Non-Squared)*"
      ],
      "metadata": {
        "id": "PREcYcRRPEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Solving Systems of Linear Equations: <u>Squared Matrix</u> (for HHL) - Eigendecomposition to get Pseudo-inverse**\n",
        "\n",
        "> https://towardsdatascience.com/from-eigendecomposition-to-determinant-fundamental-mathematics-for-machine-learning-with-1b6b449a82c6\n",
        "\n",
        "\n",
        "<font color=\"red\">**If A is squared is a matrix (and has [full rank](https://de.m.wikipedia.org/wiki/Rang_(Mathematik))) in a linear system of equations: Getting the matrix inverse via Eigendecomposition (pseudoinverse Moore-Penrose provides a least squares solution to a system of linear equations.')**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "then you can take the [inverse](https://de.m.wikipedia.org/wiki/Inverse_Matrix) if A to solve for x:\n",
        "\n",
        "> $x = A^{-1} b$\n",
        "\n",
        "<font color=\"red\">*This part is important for HHL:*\n",
        "\n",
        "* $\\hat{A} = \\hat{A}^{\\dagger}$ $\\quad$ - Hermitian operators are [Self-adjoint operators](https://en.m.wikipedia.org/wiki/Self-adjoint_operator)\n",
        "\n",
        "* Adjungierte Matrix = transponiert + complex konjugiert (Vorzeichen umgekehrt). Hermetian: selbstadjunktiert = symmetrisch\n",
        "\n",
        "* Following from this, in bra-ket notation: $\n",
        "\\left\\langle\\phi_{i}|\\hat{A}| \\phi_{j}\\right\\rangle=\\left\\langle\\phi_{j}|\\hat{A}| \\phi_{i}\\right\\rangle^{*}\n",
        "$\n",
        "\n",
        "\n",
        "<font color=\"red\">*Since $A$ is Hermitian, it has a spectral decomposition : $\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$*\n",
        "\n",
        "<font color=\"red\">*You need the Eigendecomposition (spectral decomposition) to get the inverse of a matrix (=here unitary and hence normal)*\n",
        "\n",
        "Getting the [Matrix inverse via eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition): If a matrix $\\mathbf{A}$ can be eigendecomposed and if none of its eigenvalues are zero, then $\\mathbf{A}$ is invertible and its inverse is given by\n",
        "\n",
        ">$\n",
        "\\mathbf{A}^{-1}=\\mathbf{Q}^{-1} \\mathbf{\\Lambda}^{-1} \\mathbf{Q}\n",
        "$\n",
        "\n",
        "If $\\mathbf{A}$ is a symmetric matrix, since $\\mathbf{Q}$ is formed from the eigenvectors of $\\mathbf{A}, \\mathbf{Q}$ is guaranteed to be an orthogonal matrix, therefore $\\mathbf{Q}^{-1}=\\mathbf{Q}^{\\mathrm{T}}$. Furthermore, because $\\mathbf{\\Lambda}$ is a diagonal matrix, its inverse is easy to calculate:\n",
        "\n",
        ">$\n",
        "\\left[\\Lambda^{-1}\\right]_{i i}=\\frac{1}{\\lambda_{i}}\n",
        "$\n",
        "\n",
        "**Example - the matrix:**\n",
        "\n",
        "$A=\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]$\n",
        "\n",
        "has the eigenvectors:\n",
        "\n",
        "$\\mathbf{u}_{1}=\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{1}=4$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\mathbf{u}_{2}=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{2}=-1$\n",
        "\n",
        "We can verify (as illustrated in Figure 1) that only the length of $\\mathbf{u}_{1}$ and $\\mathbf{u}_{2}$ is changed when one of these two vectors is multiplied by the matrix $\\mathbf{A}$ :\n",
        "\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=4\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 8\\end{array}\\right]$\n",
        "\n",
        "and\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=-1\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{r}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "For most applications we normalize the eigenvectors (i.e. transform them such that their length is equal to one):\n",
        "\n",
        "$\n",
        "\\mathbf{u}^{\\top} \\mathbf{u}=1 \\text {. }\n",
        "$\n",
        "\n",
        "For the previous example we obtain:\n",
        "\n",
        "$\n",
        "\\mathbf{u}_{1}=\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Exkurs: wie man einen Vektor normiert:\n",
        "\n",
        "1) Betrag von $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ist gleich $\\sqrt{3^{2}+2^{2}}=3.6055$ Vektor normieren, also mit 1/Betrag malnehmen:\n",
        "\n",
        "2) $\n",
        "\\frac{1}{3.6055} \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right)= 0.27735 \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right) =\\left(\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\n",
        "We can check that:\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "3.3284 \\\\\n",
        "2.2188\n",
        "\\end{array}\\right]=4\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "and\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
        ".7071 \\\\\n",
        "-.7071\n",
        "\\end{array}\\right]=-1\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5z-Ez-u4RWhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Solving Systems of Linear Equations: <u>Non-Squared Matrix</u> - Singular value decomposition (SVD) to get Pseudo-inverse**\n",
        "\n",
        "<font color=\"red\">**If a matrix A is not squared: singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.**\n",
        "\n",
        "> $A x = b$ $\\quad$ ($A$ is not regular)\n",
        "\n",
        "You multiply both sides by the $A^{T}$, which is the transpose of A:\n",
        "\n",
        "> $A^{T} A x = A^{T} b$\n",
        "\n",
        "Then move $A^{T} A$ on right side (by taking their inverse). This is not the original x anymore, but the [least squares](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate#Lineare_Modellfunktion) $\\hat{x}$\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "And that term <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> is know as (Moore‚ÄìPenrose) pseudo-inverse <font color=\"blue\">$A^{+}$</font>:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$A^{+}$</font> $b$\n",
        "\n",
        "And a pseudo-inverse is nothing else than our least-squares solutions. See more details under [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)\n",
        "\n",
        "*In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.*\n",
        "\n",
        "* non squared matrix\n",
        "\n",
        "* to solve you need the inverse, but normal eigendecomposition doesn work\n",
        "\n",
        "> you need to compute mooore penrose pseudo-inverse, and here you need to apply singular value decomposition to get eigendecomposition\n",
        "\n",
        "* SVD is a type of [Matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition), specicially one based on eigenvalue concepts. and SVD is a full rank decomposition (besides broader [Rank factorization methods](https://en.m.wikipedia.org/wiki/Rank_factorization))\n",
        "\n",
        "* matrix decompositions in general are used to take a large matrix apart (i.e. factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U) so the new matrices require fewer additions and multiplications to solve, compared with the original system $A\\mathbf {x} =\\mathbf {b}$\n",
        "\n",
        "<font color=\"red\">**Example:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right] x=\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "* We can immediately see that matrix A is of rank 2 because the first two rows are multiples of each other (2 and -2 and -2 and 2), just the last row numbers are not multiples of each other (5 and 3).\n",
        "\n",
        "* Now we can apply the pseudo-inverse to find the least-squares solution:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "> $\\hat{x}=$ <font color=\"blue\">$\\left(\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]$</font>$\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "> $\\hat{x}=$$\\left[\\begin{array}{l}-4 \\\\ -2\\end{array}\\right]$\n",
        "\n",
        "\n",
        "*Inserting the least-squares $\\hat{x}$ into the original equation:*\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\left[\\begin{array}{l}-4 \\\\ -3\\end{array}\\right]$ = $\\left[\\begin{array}{c}2 \\cdot(-4)+(-2) \\cdot(-3) \\\\ (-2)\\cdot (-4)+2 \\cdot (-3) \\\\ 5\\cdot (-4)+3 \\cdot (-3)\\end{array}\\right]$ = $\\left[\\begin{array}{c}-2 \\\\ 2 \\\\ -29\\end{array}\\right]$ $\\approx$ $\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$"
      ],
      "metadata": {
        "id": "g9VGZpsMZff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum-inspired Algorithms*"
      ],
      "metadata": {
        "id": "0-Lu47J5MYMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.hpcwire.com/2022/11/03/quantum-annealing-pioneer-d-wave-introduces-expanded-hybrid-solver/"
      ],
      "metadata": {
        "id": "c6wJklgHkzBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewin Tang: [A quantum-inspired classical algorithm for recommendation systems](https://arxiv.org/abs/1807.04271)\n",
        "\n",
        "https://www.youtube.com/watch?v=P2Bucnq_ap0"
      ],
      "metadata": {
        "id": "dCqBW6oqOTUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1905.10415 with https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf\n",
        "\n",
        "* There are two ways to compute the average of many values. A first method is to add all of them together and divide the result by the total number of values. An alternative approach is to select a few values at random, then compute their average using the first method. Of course, in the second case we get only an approximation of the true average, but in many cases that is good enough and can be done in significantly less time.\n",
        "\n",
        "* **This is the strategy of quantum-inspired algorithms: the coefficients of the solution vector are inner products between vectors, which can be expressed as the average of a collection of numbers that depend on the entries of the vectors**. Instead of directly computing this average (which takes linear time) we select a few of the values at random and compute their average instead.\n",
        "\n",
        "* The question is, how many values do we need to sample to get a good approximation? It turns out that **the number of samples grows with the precision of the approximation, the rank, and the condition number of the input matrix.**\n",
        "\n",
        "* In practice, this estimation method is only faster than a direct calculation of the coefficients if (i) precision, rank, and condition number are small, and (ii) the input matrix has a colossal dimension.\n",
        "\n",
        "* (..)\n",
        "\n",
        "* In practice, it pays off to be slightly more sophisticated: we keep each number with likelihood proportional to its value, such that large numbers are kept with high probability and small numbers with low probability. This is the basis of a technique called [rejection sampling](https://en.m.wikipedia.org/wiki/Rejection_sampling), **which is employed in quantum-inspired algorithms to sample from the solution vectors**. Once the approximate singular value decomposition has been obtained and the coefficients have been estimated, it is possible to query any entry of the solution vector in sublinear time.\n",
        "\n",
        "https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n",
        "\n",
        "https://cstheory.stackexchange.com/questions/42338/list-of-quantum-inspired-algorithms\n",
        "\n",
        "https://github.com/XanaduAI/quantum-inspired-algorithms"
      ],
      "metadata": {
        "id": "-wa6Ap4QMXJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulated annealing (inspired) vs quantum annealing (with quantum tunneling):\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/quantum/optimization-overview-introduction\n"
      ],
      "metadata": {
        "id": "4E5NYRMXNrHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Group Theory*"
      ],
      "metadata": {
        "id": "_OexAhQsM_ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Group_theory"
      ],
      "metadata": {
        "id": "nCPfqdkENfK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Groups*"
      ],
      "metadata": {
        "id": "gS-QUpcFmXh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kongruenzklassen, √Ñquivalenzklassen & Restklassen (Modulorechnung)**\n",
        "\n",
        "* **Kongruenz**: [Kongruenz (Zahlentheorie)](https://de.m.wikipedia.org/wiki/Kongruenz_(Zahlentheorie))) Die Kongruenz zwischen zwei ganzen Zahlen ist in Bezug auf einen Teiler definiert. **Der Teiler hei√üt in diesem Zusammenhang Modul.**\n",
        "  * Gegeben sei ein Modul $m \\in \\mathbb{N}$. Zwei ganze Zahlen $a$ und $b$ hei√üen kongruent modulo $m$, **wenn die Division von $a$ und $b$ durch $m$ den gleichen Rest $r$ l√§sst**.\n",
        "  * Kongruenz ist eine Art Erweiterung der Modulorechnung: **Modulorechnung**: 17 mod 3 = 2.\n",
        "  * **Kongruenz: 11 $\\equiv$ 17 mod 3 (weil bei beiden der Rest 2 ist - beide sind in der der gleichen Restklasse)**\n",
        "  * Man kann alternativ zur [**Restklassenermittlung**](https://de.m.wikipedia.org/wiki/Restklasse) auch sagen, dass zwei Zahlen kongruent sind (modulo der nat√ºrlichen Zahl n), wenn ihre Differenz durch n teilbar ist. Hier: 17 - 11 = 6, ist teilbar durch 3.\n",
        "\n",
        "* [**√Ñquivalenz und √Ñquivalenzklassen** (Congruence Classes)](https://de.m.wikipedia.org/wiki/√Ñquivalenzrelation). √Ñquivalenz: Objekte, die sich in einem bestimmten Zusammenhang gleichen, als gleichwertig bzw. √§quivalent angesehen. The result of the modulo operation is an equivalence class (√Ñquivalenzklassen). Any member of the class may be chosen as representative.\n",
        "\n",
        "  * Von besonderem Interesse sind jedoch solche √Ñquivalenzrelationen $\\equiv$ , deren Quotientenabbildung $\\mathrm{q}_{\\mathrm{z}}: A \\rightarrow A / \\equiv, a \\mapsto[a]_{\\equiv}$ **mit der Struktur auf $A$ vertr√§glich bzw. ein Homomorphismus ist**, weil dann die von $\\mathrm{q}_{=}$ erzeugte Struktur auf der [Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) $A / \\equiv$ von der gleichen Art ist wie die von $A$. **Eine solche √Ñquivalenzrelation $\\equiv$ nennt man eine Kongruenzrelation auf der strukturierten Menge $A$.** <font color=\"blue\">Die Quotientengruppe G/N ist homomorph zur Gruppe G.</font>\n",
        "\n",
        "* **[Restklassen](https://de.wikipedia.org/wiki/Restklasse)** sind die √Ñquivalenzklassen in der Kongruenzrelation. Eine Zahl a modulo einer Zahl m die Menge aller Zahlen, die bei Division durch m denselben Rest lassen wie a. See also [Modulo Operation](https://en.m.wikipedia.org/wiki/Modulo_operation) (Restklassenrechnung).\n",
        "\n",
        "* In der Gruppentheorie werden √Ñquivalenzklassen als [Nebenklassen (Cosets)](https://de.m.wikipedia.org/wiki/Gruppentheorie#Nebenklassen) bezeichnet."
      ],
      "metadata": {
        "id": "0a8DDz9Vbn1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfs10URbe-RG"
      },
      "source": [
        "**Subgroups, Normal Subgroups & Cosets**\n",
        "\n",
        "* **Subgroups (Unterguppen):**\n",
        "\n",
        "  * Gegeben sei $\\mathbb{Z}$ die Gruppe der ganzen Zahlen mit der Addition als Gruppenoperation. Man kann fur diese Gruppe verschiedene (unendlich viele) **[Untergruppen](https://de.m.wikipedia.org/wiki/Untergruppe) (Subgroups)** bilden: 2$\\mathbb{Z}$, 3$\\mathbb{Z}$, 4$\\mathbb{Z}$, 5$\\mathbb{Z}$, 6$\\mathbb{Z}$, 7$\\mathbb{Z}$.\n",
        "\n",
        "  * Beispiel: Die ganzen Zahlen $\\mathbb {Z} $ sind bez√ºglich der Addition eine Untergruppe der rationalen Zahlen $\\mathbb {Q} $.\n",
        "\n",
        "  * Two standard subgroups / every group has at least 2 subgroups: the identity element {e} and the entire group G. These are technically **normal subgroups (Normalteiler)**.\n",
        "\n",
        "  * If a group has no other normal subgroups then these two, than it's called a **simple group**. A simple group does not have any factor (quotient) groups, but they are the building blocks of other groups.\n",
        "\n",
        "* **Normal Subgroups (Normalteiler)**:\n",
        "\n",
        "  * <font color=\"blue\">**Die √Ñquivalenzklasse mit dem Rest Null ist der Normalteiler**. Normalteiler sind spezielle Untergruppen, und ihre Bedeutung liegt vor allem darin, **dass sie genau die Kerne von Gruppenhomomorphismen sind** (=L√∂sungsmenge ist Null, also Null als Rest).</font> Normal Subgroups heissen auch \"invariant or self-conjugate subgroups\".\n",
        "\n",
        "  * Normal subgroups determine what kinds of homomorphisms are possible from a group $G$ to other groups $f : G -> H$.\n",
        "\n",
        "  * Trivial examples: Standard subgroups identity element {e} and the entire group G.\n",
        "\n",
        "  * Die Gruppe $\\mathbb{Z}$ ist **abelsch** (kommutativ) und somit ist jede Untergruppe auch ein **[Normalteiler](https://de.wikipedia.org/wiki/Normalteiler) bzw. [Normal Subgroup](https://en.wikipedia.org/wiki/Normal_subgroup)**.\n",
        "\n",
        "* **Nebenklassen (Cosets)**:\n",
        "\n",
        "  * Nebenklassen werden benutzt, um den [Satz von Lagrange](https://de.m.wikipedia.org/wiki/Satz_von_Lagrange) zu beweisen, um die Begriffe [Normal Subgroup (Normalteiler)](https://de.m.wikipedia.org/wiki/Normalteiler) und [Quotient Group (Faktorgruppe)](https://de.m.wikipedia.org/wiki/Faktorgruppe) zu erkl√§ren und um Gruppenoperationen zu studieren.\n",
        "\n",
        "  * In contrast to Subgroups, the Coset (Nebenklasse) are not closed under addition, have no inverse and don't contain the identity element\n",
        "\n",
        "  * Cosets are there to define how many (finite) possible subgroups exist in a group. There are **two types of cosets**: left cosets and right cosets.\n",
        "\n",
        "  * A subgroup $H$ of a group $G$ may be used to decompose the underlying set of $G$ into disjoint, equal-size subsets called [cosets](https://en.m.wikipedia.org/wiki/Coset)\n",
        "\n",
        "* <font color=\"blue\">**Example: Use Normal Subgroup 5$\\mathbb{Z}$ von $\\mathbb{Z}$ (integers mod 5) to divide a Group into Cosets. We get 5 sets of remainders (the congruence classes) which are the [Quotient Group (Faktorgruppe)](https://en.m.wikipedia.org/wiki/Quotient_group), a group with 5 elements: $\\mathbb{Z}$ mod 5 = {$\\overline{0}$, $\\overline{1}$, $\\overline{2}$, $\\overline{3}$, $\\overline{4}$}.**</font>\n",
        "\n",
        "    * 5$\\mathbb{Z}$ + Rest 0 = $\\overline{0} : \\{\\ldots-10,-5,0,5,10 \\ldots\\}$ **(Normal) Subgroup**, which is technically also a Coset 0+ 5 $\\mathbb{Z}$\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 1 = $\\overline{1} : \\{\\ldots-9,-4,1,6,11 \\ldots\\}$ **Coset 1+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 2 = $\\overline{2} : \\{\\ldots-8,-3,2,7,12 \\ldots\\}$ **Coset 2+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 3 = $\\overline{3} : \\{\\ldots-7,-2,3,8,13 \\ldots\\}$ **Coset 3+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 4 = $\\overline{4} : \\{\\ldots-6,-1,4,9,14 \\ldots\\}$ **Coset 4+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "* **Quotient Group (Faktorgruppe)**:\n",
        "\n",
        "  * Die Faktorgruppe oder Quotientengruppe wird mit $G / N$ bezeichnet und ist die Menge der Nebenklassen (Cosets). Aus einer Gruppe $G$ und jedem ihrer Normalteiler $N$ l√§sst sich eine Faktorgruppe $G/N$ bilden.\n",
        "\n",
        "    * Die Quotientengruppe unterteilt eine Menge in √Ñquivalenzklassen bzw. eine Gruppe in Restklassen / Nebenklassen. Diese Menge der Restklassen (√Ñquivalenzklassen) heisst **[Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) bzw. Faktormenge**.\n",
        "\n",
        "  * Diese [Quotientengruppen](https://de.wikipedia.org/wiki/Faktorgruppe) sind homomorphe Bilder von G, **und jedes homomorphe Bild von G ist zu einer solchen Quotientengruppe G/N isomorph**. See fundamental theorem on homomorphisms of groups as \"**Every homomorphic image of a group is isomorphic to a quotient group**\". [Source](https://en.m.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms)\n",
        "\n",
        "  * For finite groups you can find a chain of normal subgroups called a \"composition series\" which acts as a kind of 'prime factorization' of the group (1 ‚óÉ N1, ‚óÉ N2 ‚óÉ ... ‚óÉ Nr ‚óÉ G ). Normal subgroups can also be used to study fields (K√∂rper), i.e. in Galois theory (Field extension K / F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generators of a group*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0942.jpg)\n",
        "\n",
        "> **Each group SU(3) x SU(2) x U(1) leads to a symmetry resulting in a conservation law**"
      ],
      "metadata": {
        "id": "Pp3Z0FI7ipEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVfPVtLVGzQ"
      },
      "source": [
        "**Zusammenfassung algebraischer Strukturen mit Operationen**\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ is a Group\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ is a Ring\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ $√∑$ $\\quad$ is a Field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvvUdIJw3cu"
      },
      "source": [
        "A group is a set $G$ with an operation $*$ such that\n",
        "\n",
        "1. **Closure** [Abgeschlossenheit](https://de.m.wikipedia.org/wiki/Abgeschlossenheit_(algebraische_Struktur)): If $x$ and $y$ are in $G$ then $x * y$ is in $\\mathrm{G}$\n",
        "  * until here it is a [Magma (Groupoid)](https://de.m.wikipedia.org/wiki/Magma_(Mathematik))\n",
        "  * Beispiele (Magmen, die keine Halbgruppen sind):\n",
        "    * $(\\mathbb{Z},-):$ die ganzen Zahlen mit der Subtraktion\n",
        "    * (R $\\backslash\\{0\\}, /)$ : die reellen Zahlen ungleich 0 mit der Division\n",
        "    * Die nat√ºrlichen Zahlen mit der Exponentiation, also mit der Verkn√ºpfung $a * b=a^{b}$\n",
        "    * Die reellen Zahlen mit der Bildung des arithmetischen Mittels als Verkn√ºpfung\n",
        "\n",
        "2. **Associativity** [Assoziativit√§t](https://de.m.wikipedia.org/wiki/Assoziativgesetz): For all $x, y, z$ in $G$ $\\text { we have }(x * y) * z=x *(y * z)$\n",
        "  * until here it is a [Halbgruppe](https://de.m.wikipedia.org/wiki/Halbgruppe)\n",
        "  * Beispiel: Die Menge $\\mathbb  N$ $_0$ = {0, 1, 2 ..} der nat√ºrlichen Zahlen bildet mit der gew√∂hnlichen Addition eine kommutative und k√ºrzbare Halbgruppe ($\\mathbb  N$ $_0$,+), die keine Gruppe ist. Da hier die negativen Zahlen fehlen, also die ‚ÄûH√§lfte‚Äú der abelschen Gruppe ($\\mathbb Z,+$) der ganzen Zahlen, lag der Name Halbgruppe f√ºr diese mathematische Struktur nahe.\n",
        "\n",
        "3. **Identity Element** [Neutrales Element](https://en.m.wikipedia.org/wiki/Identity_element): There is an element $e$ in $G$ such that $e * x=x * e=x$ for all $x$ in $G$\n",
        "  * until here it is a [Monoid](https://de.m.wikipedia.org/wiki/Monoid)\n",
        "  * Beispiel: die nat√ºrlichen Zahlen mit der Addition und der Zahl 0 als neutralem Element.\n",
        "\n",
        "4. **Inverse Elements** [Inverse Elemente](https://de.m.wikipedia.org/wiki/Inverses_Element): For each element $x$ in $G,$ there is an element $x^{-1}$ such that $x * x^{-1}=x^{-1} * x=e$\n",
        "  * until here it is a [Group](https://de.m.wikipedia.org/wiki/Gruppe_(Mathematik))\n",
        "  * Beispiel: die Menge der ganzen Zahlen zusammen mit der Addition\n",
        "  * Ringe, K√∂rper (Field), Moduln und Vektorr√§ume sind Gruppen mit zus√§tzlichen Strukturen und Eigenschaften\n",
        "\n",
        "5. **Kommutative** [Kommutativit√§t](https://de.m.wikipedia.org/wiki/Kommutativgesetz): F√ºr alle $a, b \\in G$ gilt: $a * b=b * a$\n",
        "  * until here it is an [Abelian Group](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe)\n",
        "  * Beispiel:\n",
        "    * $(\\mathbb {Z} ,+)$ ist die wichtigste abelsche Gruppe. Dabei ist Z die Menge der ganzen Zahlen und + die gew√∂hnliche Addition.\n",
        "    * $(\\mathbb {Q}^{\\cdot} , \\cdot)$ ist eine abelsche Gruppe. Dabei ist $\\mathbb {Q}^{\\cdot}$ die Menge der rationalen Zahlen ohne die 0 und ‚ãÖ ist die gew√∂hnliche Multiplikation. Die Null muss hierbei ausgeschlossen werden, da sie kein inverses Element besitzt: ‚Äû1/0‚Äú ist nicht definiert\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "    * Die Menge der Drehungen in einer Ebene um einen Punkt bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Drehungen.\n",
        "    * Die Menge der Drehstreckungen in einer Ebene bilden eine abelsche Gruppe.\n",
        "    * Die Menge der endlichen Dezimalzahlen sind bez√ºglich der Multiplikation keine abelsche Gruppe. Zum Beispiel hat die Zahl 3 kein Inverses bez√ºglich der Multiplikation. $\\displaystyle {\\frac {1}{3}}$ l√§sst sich nicht als endlicher Dezimalbruch schreiben. Bez√ºglich der normalen Addition bilden die endlichen Dezimalbr√ºche eine abelsche Gruppe.\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "  * [Free Abelian Group](https://de.m.wikipedia.org/wiki/Freie_abelsche_Gruppe): eine abelsche Gruppe, die als $\\mathbb {Z}$-Modul eine Basis hat. Ist G eine freie abelsche Gruppe ist, so wird man eine Basis w√§hlen und alle Elemente als Linearkombinationen von Elementen dieser Basis ausdr√ºcken. Allerdings sollte man betonen, dass es meist keine ausgezeichnete Basis geben wird.\n",
        "\n",
        "6. **Distributive law** [Distributivgesetz](https://de.m.wikipedia.org/wiki/Distributivgesetz): a(b + c) = ab + ac.\n",
        "  * Gilt zus√§tzlich Distributivgesetz gilt f√ºr Ring (da man hier zwei Operationen ben√∂tigt). Gruppen haben immer nur eine Operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/groups.png)"
      ],
      "metadata": {
        "id": "y12kc1i6O7GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Group Theory**\n",
        "\n",
        "* Solutions to polynomial equations, like find 2 roots of a quadratic equation. there is also q cubic formula and a quartic for degree 4 polynomial. Find formula to solve degree 5 polynomial - group theory showed it doesnt exist. Has to do with permutation group S5.\n",
        "\n",
        "* Connection to Physics - Noether's theorem: Conservation law - symmetry. Momentum - translation in space. Energy - translation in time.\n",
        "\n",
        "> [Researchers Use Group Theory to Speed Up Algorithms ‚Äî Introduction to Groups](https://www.youtube.com/watch?v=KufsL2VgELo&list=WL&index=3&t=1579s)"
      ],
      "metadata": {
        "id": "wwGvCcMgGFMM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCaLVo_nP2aQ"
      },
      "source": [
        "**Trivial Group (Zero Group)**\n",
        "\n",
        "Die [triviale Gruppe](https://de.m.wikipedia.org/wiki/Triviale_Gruppe) ist in der Gruppentheorie eine Gruppe, deren Tr√§germenge genau ein Element enth√§lt. Die triviale Gruppe ist bis auf Isomorphie eindeutig bestimmt. **Jede Gruppe enth√§lt die triviale Gruppe als Untergruppe**.\n",
        "\n",
        "Die triviale Gruppe $(\\{e\\}, *)$ ist eine Gruppe, die aus der einelementigen Menge $\\{e\\}$ besteht und versehen ist mit der einzig m√∂glichen Gruppenoperation\n",
        "\n",
        "$\n",
        "e * e=e\n",
        "$\n",
        "\n",
        "Das Element $e$ ist damit das **neutrale Element** der Gruppe.\n",
        "\n",
        "Alle trivialen Gruppen sind zueinander isomorph. Beispiele f√ºr triviale Gruppen sind:\n",
        "\n",
        "* die zyklische Gruppe $C_{1}$ vom Grad 1\n",
        "\n",
        "* die alternierende Gruppe $A_{2}$ vom Grad 2\n",
        "\n",
        "* die symmetrische Gruppe $S_{1}$ einer einelementigen Menge\n",
        "\n",
        "*Eigenschaften trivialer Gruppen*:\n",
        "\n",
        "* Da die Gruppenoperation $\\ast$ kommutativ ist, ist die triviale Gruppe eine abelsche Gruppe.\n",
        "\n",
        "* Die einzige Untergruppe der trivialen Gruppe ist die triviale Gruppe selbst.\n",
        "\n",
        "* Die triviale Gruppe wird von der leeren Menge erzeugt:\n",
        "$\\{e\\}=\\langle \\emptyset \\rangle$ . Hierbei ergibt das leere Produkt nach √ºblicher Konvention das neutrale Element.\n",
        "\n",
        "* Jede Gruppe enth√§lt die triviale Gruppe und sich selbst als (triviale) Normalteiler. **Die triviale Gruppe wird daher meistens nicht als einfache Gruppe angesehen**, die aus genau 2 Normalteilern besteht).\n",
        "\n",
        "* In der Kategorie der Gruppen Grp fungiert die triviale Gruppe als Nullobjekt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVFg6eAKyF0"
      },
      "source": [
        "**Simple Group**\n",
        "\n",
        "* [Simple Groups](https://en.wikipedia.org/wiki/Simple_group) bzw. [einfache Gruppen](https://de.wikipedia.org/wiki/Einfache_Gruppe_(Mathematik)) **are the fundamental building blocks of finite groups** (just like Prime numbers are fundamental building blocks in number theory)\n",
        "\n",
        "* Just as you can factor integers into prime numbers, you can break apart some groups into a direct product of simpler groups.\n",
        "\n",
        "*  **a simple group is a <u>nontrivial</u> group whose only normal subgroups are the trivial group and the group itself.**\n",
        "\n",
        "* **Jede Gruppe hat sich selbst und die nur das neutrale Element enthaltende Menge als Normalteiler.**\n",
        "\n",
        "Damit stellt sich die Frage, welche Gruppen keine weitere Normalteiler besitzen. Bei diesen handelt es sich per definitionem gerade um die einfachen Gruppen.\n",
        "\n",
        "  * Eine Gruppe $G$ heisst einfach, falls sie als Normalteiler nur $G$ und $\\{e\\}$ mit dem neutralen Element $e$ hat.\n",
        "  * Au√üerdem wird zusƒÉtzlich $G \\neq\\{e\\}$ gefordert, wonach man knapper sagen kann:\n",
        "  * **Eine Gruppe hei√üt einfach, wenn sie genau zwei Normalteiler besitzt.**\n",
        "\n",
        "* A group that is not simple can be broken into two smaller groups, namely a nontrivial [normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup) and the corresponding quotient group. This process can be repeated, and for finite groups one eventually arrives at uniquely determined simple groups, by the [Jordan‚ÄìH√∂lder theorem (Composition series)](https://en.wikipedia.org/wiki/Composition_series).\n",
        "\n",
        "* The complete classification of finite simple groups, completed in 2004, is a major milestone in the history of mathematics.\n",
        "\n",
        "**Seit 1982 sind die endlichen einfachen Gruppen vollst√§ndig klassifiziert, die Liste besteht aus**\n",
        "\n",
        "* den zyklischen Gruppen von Primzahlordnung,\n",
        "\n",
        "* den alternierenden Gruppen $A_{n}$ mit $n\\geq 5$,\n",
        "\n",
        "* den Gruppen vom Lie-Typ (16 jeweils unendliche Serien)\n",
        "\n",
        "* 26 sporadischen Gruppen (Es handelt sich um die endlichen einfachen Gruppen, die sich nicht in eine der (18) systematischen Familien mit unendlich vielen Mitgliedern (von endlichen einfachen Gruppen) einordnen lassen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSRFOXm2QgM"
      },
      "source": [
        "**Finite Groups**\n",
        "\n",
        "* Eine Gruppe ($G$,*) hei√üt [endliche Gruppe](https://de.m.wikipedia.org/wiki/Endliche_Gruppe), wenn $G$ eine endliche Menge ist, also eine endliche Anzahl von Elementen hat.\n",
        "\n",
        "* Die Annahme der Endlichkeit erm√∂glicht ein vereinfachtes Axiomensystem\n",
        "\n",
        "Ein Paar $(G, *)$ mit einer endlichen Menge $G$ und einer inneren zweistelligen Verkn√ºpfung $*: G \\times G \\rightarrow G$ hei√üt Gruppe, wenn folgende Axiome erf√ºllt sind:\n",
        "\n",
        "* Assoziativit√§t: F√ºr alle Gruppenelemente $a, b, c$ gilt $(a * b) * c=a *(b * c)$,\n",
        "\n",
        "* [K√ºrzungsregel](https://de.m.wikipedia.org/wiki/K√ºrzbarkeit): Aus $a * x=a * x^{\\prime}$ oder $x * a=x^{\\prime} * a$ folgt $x=x^{\\prime}$\n",
        "\n",
        "Aus der K√ºrzungsregel folgt, dass die Links- und Rechtsmultiplikationen $x \\mapsto a * x$ und $x \\mapsto x * a$\n",
        "injektiv sind, woraus wegen der Endlichkeit auch die Surjektivit√§t folgt. Daher gibt es ein $x$ mit\n",
        "$a * x=a,$ was zur Existenz des neutralen Elementes $e$ f√ºhrt, und dann ein $x$ mit $a * x=e$, was\n",
        "die Existenz der inversen Elemente zeigt.\n",
        "\n",
        "* The [List of small groups](https://en.wikipedia.org/wiki/List_of_small_groups) contains finite groups of small [order](https://en.wikipedia.org/wiki/Order_(group_theory)) [up to](https://en.wikipedia.org/wiki/Up_to) [group isomorphism](https://en.wikipedia.org/wiki/Group_isomorphism).\n",
        "\n",
        "* Die folgende Liste enth√§lt eine Auswahl [endlicher Gruppen kleiner Ordnung](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen).\n",
        "\n",
        "  * Diese Liste kann benutzt werden, um herauszufinden, zu welchen bekannten endlichen Gruppen eine Gruppe G isomorph ist.\n",
        "\n",
        "  * Als erstes bestimmt man die Ordnung von G und vergleicht sie mit den unten aufgelisteten Gruppen gleicher Ordnung.\n",
        "\n",
        "  * Ist bekannt, ob G abelsch (kommutativ) ist, so kann man einige Gruppen ausschlie√üen. Anschlie√üend vergleicht man die Ordnung einzelner Elemente von G mit den Elementen der aufgelisteten Gruppen, wodurch man G bis auf Isomorphie eindeutig bestimmen kann.\n",
        "\n",
        "In der nachfolgenden Liste werden folgende Bezeichnungen verwendet:\n",
        "\n",
        "- $\\mathbb{Z}_{n}$ ist die zyklische Gruppe der Ordnung $n$ (die auch als $C_{n}$ oder $\\mathbb{Z} / n \\mathbb{Z}$ geschrieben wird).\n",
        "\n",
        "- $D_{n}$ ist die Diedergruppe der Ordnung $2 n$.\n",
        "\n",
        "- $S_{n}$ ist die symmetrische Gruppe vom Grad $n$, mit $n !$ Permutationen von $n$ Elementen.\n",
        "\n",
        "- $A_{n}$ ist die alternierende Gruppe vom Grad $n$, mit $n ! / 2$ Permutationen von $n$ Elementen f√ºr $n \\geq 2$.\n",
        "\n",
        "- Dic $_{n}$ ist die dizyklische Gruppe der Ordnung $4 n$.\n",
        "\n",
        "- $V_{4}$ ist die [Klein'sche Vierergruppe](https://de.m.wikipedia.org/wiki/Kleinsche_Vierergruppe) der Ordnung $4 .$\n",
        "\n",
        "- $Q_{4 n}$ ist die Quaternionengruppe der Ordnung $4 n$ fur $n \\geq 2$.\n",
        "\n",
        "[Liste aller Gruppen bis Ordnung 20](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen#Liste_aller_Gruppen_bis_Ordnung_20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN_XWVc0s5p"
      },
      "source": [
        "**Finite Simple Groups**\n",
        "\n",
        "* [Endliche einfache Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe) gelten in der Gruppentheorie als die Bausteine der [endlichen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_Gruppe).\n",
        "\n",
        "* Die endlichen einfachen Gruppen spielen f√ºr die endlichen Gruppen eine √§hnliche Rolle wie die Primzahlen f√ºr die nat√ºrlichen Zahlen: Jede endliche Gruppe l√§sst sich in ihre einfachen Gruppen ‚Äûzerteilen‚Äú (f√ºr die Art der Eindeutigkeit siehe den Satz von Jordan-H√∂lder).\n",
        "\n",
        "* Die Rekonstruktion einer endlichen Gruppe aus diesen ihren ‚ÄûFaktoren‚Äú ist aber nicht eindeutig.\n",
        "\n",
        "* Es gibt jedoch keine ‚Äûnoch einfacheren Gruppen‚Äú, aus denen sich die endlichen einfachen Gruppen konstruieren lassen.\n",
        "\n",
        "Obwohl die endlichen einfachen Gruppen seit 1982 als vollst√§ndig klassifiziert galten, schlossen Mathematiker um Aschbacher die Klassifikation erst im Jahre 2002 mit einem 1200 Seiten langen Beweis ab:\n",
        "\n",
        "* Fast alle dieser Gruppen lassen sich einer von 18 Familien endlicher einfacher Gruppen zuordnen.\n",
        "\n",
        "* Es existieren 26 Ausnahmen. Diese Gruppen werden als **sporadische Gruppen** bezeichnet (Zu den sporadischen Gruppen z√§hlen die Conway-Gruppe, das Babymonster und die [**Monstergruppe**](\n",
        "https://de.m.wikipedia.org/wiki/Monstergruppe) (mit fast 1054 Elementen die gr√∂√üte sporadische Gruppe).\n",
        "\n",
        "* Die [sporadischen Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe) sind 26 spezielle Gruppen in der Gruppentheorie. Es handelt sich um die [endlichen einfachen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe), die sich nicht in eine der [(18) systematischen Familien mit unendlich vielen Mitgliedern](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Familien_endlicher_einfacher_Gruppen) (von endlichen einfachen Gruppen) einordnen lassen.\n",
        "\n",
        "> https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/\n",
        "\n",
        "\n",
        "*Klassifikation der endlichen einfachen Gruppe*\n",
        "\n",
        "Die endlichen einfachen Gruppen [lassen sich einteilen in](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Klassifikation) bzw [Classification of finite simple groups](https://en.m.wikipedia.org/wiki/Classification_of_finite_simple_groups), Every finite simple group is isomorphic to one of the following groups:\n",
        "\n",
        "* a member of one of three infinite classes of such, namely:\n",
        "\n",
        "  * (1) [zyklische Gruppen](https://de.m.wikipedia.org/wiki/Zyklische_Gruppe) von Primzahlordnung,\n",
        "\n",
        "  * (1) [alternierende Gruppen](https://de.m.wikipedia.org/wiki/Alternierende_Gruppe) $A_{n}$ mit $n>4$,\n",
        "\n",
        "  * (16) [Gruppen vom Lie-Typ](https://de.m.wikipedia.org/wiki/Gruppe_vom_Lie-Typ) √ºber einem [endlichen K√∂rper](https://de.m.wikipedia.org/wiki/Endlicher_K√∂rper) (16 jeweils unendliche Familien),\n",
        "\n",
        "* (26) one of 26 groups called the \"sporadic groups\" / [26 sporadische Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe).\n",
        "\n",
        "* (1) the [Tits group](https://en.m.wikipedia.org/wiki/Tits_group) (which is sometimes considered a 27th sporadic group)\n",
        "\n",
        "\n",
        "[2004: Classification of Quasithin group](https://en.m.wikipedia.org/wiki/Quasithin_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Structures (Moduln, Ring, Field)*"
      ],
      "metadata": {
        "id": "KHUTVWQOmCm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/%C3%9Cbersicht_K%C3%B6rper.svg/775px-%C3%9Cbersicht_K%C3%B6rper.svg.png)"
      ],
      "metadata": {
        "id": "MdAk53GrsPJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Modul](\n",
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)) ist ein n-dimensionaler Ring.**\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativit√§t)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where ‚Äúscalar multiplication‚Äù refers to multiplication by integer scalars.\n",
        "\n",
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$\n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$\n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ],
      "metadata": {
        "id": "xPGuj4OPrOPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Ring](https://en.m.wikipedia.org/wiki/Ring_theory) ist eine Menge R mit <u>zwei inneren bin√§ren Verkn√ºpfungen</u> ‚Äû+‚Äú und ‚Äû‚àô‚Äú, sodass gilt:**\n",
        "\n",
        "1. **Addition: (R, +) ist eine abelsche Gruppe**\n",
        "\n",
        "* Addition is associative and commutative;\n",
        "\n",
        "* There is an additive identity, zero;\n",
        "\n",
        "* Every element has an additive inverse;\n",
        "\n",
        "2. **Multiplikation: (R, ‚àô) ist eine Halbgruppe**, das bedeutet:\n",
        "\n",
        "* Halbgruppe in der Multiplikation im Ring: **nur nur die Assoziativit√§t, aber keine Inverse, neutrales element oder kommutativit√§t**)\n",
        "\n",
        "> **Das bedeutet: -> Sowohl Ringe als auch K√∂rper verlangen, dass bzgl. der Addition eine kommutative Gruppe vorliegt (abelsch!). Bei der Multiplikation erfolgt der √úbergang vom Ring zum K√∂rper durch die Versch√§rfung der Forderungen**\n",
        "\n",
        "* Unlike a field, a ring is not required to have multiplicative inverses, and the multiplication is not required to be commutative.\n",
        "\n",
        "> **A good example of a ring is the set of all n√ón matrices under the operations of matrix addition and matrix multiplication.** [Matrix-Multiplication is non-commutative!](https://en.m.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity)\n",
        "\n",
        "$\\mathbf{A B} \\neq \\mathbf{B A}$\n",
        "For example\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "but\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "> **The integers Z also form a ring under the operations of addition and multiplication.**\n",
        "\n",
        "3. **Die Distributivgesetze a*(b+c)=a*b+a*c und (a+b)*c = a*c+b*c sind f√ºr alle a,b,c Œµ $R$ erf√ºllt.**\n",
        "\n",
        "4. **Das neutrale Element 0 von (R, +) hei√üt Nullelement von R.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Ring_(Algebra)\n",
        "\n",
        "**Ein Ring hei√üt kommutativ**, falls er bez√ºglich der Multiplikation kommutativ ist (Ein Ring hei√üt kommutativ, falls er bez√ºglich der Multiplikation kommutativ ist, ansonsten spricht man von einem nicht-kommutativen Ring.)\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* 2√ó2 Real matrices.\n",
        "\n",
        "* Das wichtigste Beispiel eines Ringes sind die Integers / ist die Menge (Ùè∞Å$\\mathbb Z$,+,‚àô) der ganzen Zahlen mit der √ºblichen Addition und Multiplikation. Es handelt sich dabei um einen nullteilerfreien kommutativen Ring mit Einselement, also einen Integrit√§tsring.\n",
        "\n",
        "* the Integers modulo some Natural number greater than one;\n",
        "\n",
        "* Ebenso bildet ($\\mathbb Q$,+,‚àô) der rationalen Zahlen mit der √ºblichen Addition und Multiplikation einen Ring. Da in diesem Fall nicht nur ($\\mathbb Q$,+), sondern auch ($\\mathbb Q$ \\ {0},‚àô) eine abelsche Gruppe bildet, liegt sogar ein K√∂rper vor; es handelt sich dabei um den Quotientenk√∂rper des Integrit√§tsringes (Ùè∞Å$\\mathbb Z$,+,‚àô).\n",
        "\n",
        "* Kein Ring ist die Menge ($\\mathbb N$Ùè∞Ä,+,‚àô) der nat√ºrlichen Zahlen mit der √ºblichen Addition und Multiplikation, da die Addition √ºber den nat√ºrlichen Zahlen nicht invertierbar ist.\n",
        "\n",
        "https://www.quora.com/What-are-the-differences-between-rings-and-fields"
      ],
      "metadata": {
        "id": "usbC2QwNr7Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [K√∂rper (Field)](https://de.m.wikipedia.org/wiki/K%C3%B6rper_(Algebra)) ist eine spezielle Form von Ring**:\n",
        "\n",
        "> Man nennt die Elemente im K√∂rper nicht Vektoren, sondern Skalare. √úber dem Skalark√∂rper betrachtet man einen Vektorraum und dessen Elemente hei√üen Vektoren.\n",
        "\n",
        "* **A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)**\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, hei√üt ein K√∂rper, wenn in ihm jedes von Null verschiedene Element multiplikativ invertierbar ist.\n",
        "Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring K, in dem die Einheitengruppe K* gleich K \\ {0}, also maximal gro√ü, ist.\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, ist ein K√∂rper, wenn in ihm jedes von Null verschiedene Element ein Inverses bez√ºglich der Multiplikation besitzt. Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring $K$, in dem die Einheitengruppe $K^{*}$ gleich $K \\backslash\\{0\\}$ ist.\n",
        "\n",
        "\n",
        "Ein Tripel (K,+,‚Ä¢), bestehend aus einer Menge K und zwei bin√§ren Verkn√ºpfungen ‚Äû+‚Äú und ‚Äû‚Ä¢‚Äú (die √ºblicherweise Addition und Multiplikation genannt werden), ist genau dann ein K√∂rper, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "* $(K,+)$ ist eine abelsche Gruppe (mit Neutralelement 0)\n",
        "\n",
        "* $(K \\backslash\\{0\\}, ‚Ä¢)$ ist eine abelsche Gruppe (mit Neutralelement 1)\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ und $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ (Distributivgesetz)\n",
        "\n",
        "Additive Eigenschaften:\n",
        "\n",
        "* $a+(b+c)=(a+b)+c$ (Assoziativgesetz)\n",
        "\n",
        "* $a+b=b+a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $0 \\in K$ mit $0+a=a$ (neutrales Element)\n",
        "\n",
        "* Zu jedem $a \\in K$ existiert das additive Inverse $(-a)$ mit $(-a)+a=0$\n",
        "\n",
        "Multiplikative Eigenschaften:\n",
        "\n",
        "* $\\cdot a \\cdot(b \\cdot c)=(a \\cdot b) \\cdot c$ (Assoziativgesetz)\n",
        "\n",
        "* $a \\cdot b=b \\cdot a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $1 \\in K$ mit $1 \\cdot a=a$ (neutrales Element), und es ist $1 \\neq 0$.\n",
        "\n",
        "* Zu jedem $a \\in K \\backslash\\{0\\}$ existiert das multiplikative Inverse $a^{-1}$ mit $a^{-1} \\cdot a=1$\n",
        "\n",
        "Zusammenspiel von additiver und multiplikativer Struktur:\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ (Links-Distributivgesetz)\n",
        "\n",
        "* Das Rechts-Distributivgesetz $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ folgt dann aus den √ºbrigen Eigenschaften:\n",
        "$(a+b) \\cdot c=c \\cdot(a+b)=c \\cdot a+c \\cdot b=a \\cdot c+b \\cdot c$\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* The most familiar form of algebra is the elementary algebra that you learned in high school, namely the algebra of the real numbers. From an abstract point of view, this is the algebra of fields.\n",
        "\n",
        "* Note that the axioms for a field are precisely the axioms for algebra on the real numbers. As a result, the real numbers R form a field under the usual operations of addition and multiplication. However, the real numbers are not the only possible field. Indeed, you are already familiar with a few other examples:\n",
        "\n",
        "* set of rational numbers under addition and multiplication. The rational numbers Q form a field under the usual operations of addition and multiplication. In particular, we can add or multiply two elements of Q to obtain another element of Q, and these operations obey all of the axioms listed above.\n",
        "\n",
        "* The complex numbers C form a field under the commonly defined operations of addition and multiplication. Complex numbers do obey all of the listed axioms for a field, which is why elementary algebra works as usual for complex numbers.\n",
        "\n",
        "* The Integers modulo a Prime number.\n",
        "\n",
        "*An example of a set of numbers that is **not a field** is the set of integers. It is an \"integral domain.\" It is not a field because it lacks multiplicative inverses. Without multiplicative inverses, division may be impossible.*\n",
        "\n",
        "* Both are algebraic objects with a notion of addition and multiplication, **but the multiplication in a field is more specialized**: it is necessarily commutative and every nonzero element has a multiplicative inverse.\n"
      ],
      "metadata": {
        "id": "j07gGLGUtEU6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxw9DSyjiah"
      },
      "source": [
        "**Siehe auch**\n",
        "\n",
        "* [Algebraische Struktur](https://de.m.wikipedia.org/wiki/Algebraische_Struktur)\n",
        "\n",
        "* [Outline_of_algebraic_structures](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures)\n",
        "\n",
        "* [Mathematische Strukturen](https://de.m.wikipedia.org/wiki/Mathematische_Struktur) (neben topologischen Strukturen, geometrischen Strukturen und Zahlbereichen)\n",
        "\n",
        "* [Geometrische Gruppentheorie](https://de.wikipedia.org/wiki/Geometrische_Gruppentheorie) (Gruppenoperationen auf Graphen und metrischen R√§umen, letztlich werden die Gruppen selbst zu solchen geometrischen Objekten)\n",
        "\n",
        "* [Verkn√ºpfungen](https://de.m.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik))\n",
        "\n",
        "* [Gruppenoperation](https://de.wikipedia.org/wiki/Gruppenoperation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Icosahedral Symmetry](https://youtu.be/B-WI8JZR140)"
      ],
      "metadata": {
        "id": "ibeXZKTToqNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Homomorphism*"
      ],
      "metadata": {
        "id": "iqHduf9WDZMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Homomorphiesatz (Isomorphie on quotient group)\n",
        "\n",
        "Video: [First Isomorphism Theorem for Groups](https://www.youtube.com/watch?v=JiS43Twomsk&list=WL&index=10)"
      ],
      "metadata": {
        "id": "M8aSZ3c541kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Persistent Homology](https://youtu.be/ktKCzMmDXDk)\n",
        "\n",
        "Video: [Chapter 6: Homomorphism and (first) isomorphism theorem | Essence of Group Theory](https://youtu.be/2kmIHyD8zTk)\n",
        "\n",
        "Kern is a normal subgroup (normalteiler) von H, weil Rest is 0. alles andere sind cosets (mit jeweils rest 1,2,3, etc nachdem was der normalteiler ist)"
      ],
      "metadata": {
        "id": "Z8eBrD-9qS_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Monad_(linear_algebra)"
      ],
      "metadata": {
        "id": "8QS1Q_s8JBFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cantorsparadise.com/an-intro-to-topology-9e0478313b63"
      ],
      "metadata": {
        "id": "6hBdWGAqr4lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence (simplicial homology). Ist das die normal subgroup (normal subgroup)??\n",
        "\n",
        "Der Kern von f ist stets ein Normalteiler von G und das Bild von f ist eine Untergruppe von H. Nach dem Homomorphiesatz ist die Faktorgruppe G/Kern(f) isomorph zu Bild (f)."
      ],
      "metadata": {
        "id": "R4CVKRmNV-E5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35B6lF4hR7zq"
      },
      "source": [
        "**Homomorphiesatz (Fundamental Theorem on Homomorphism)**\n",
        "\n",
        "* [Homomorphiesatz](https://de.m.wikipedia.org/wiki/Homomorphiesatz)\n",
        "\n",
        "* https://youtu.be/QA9rrDMlaHc (Homomorphiesatz mit Hasen und Jaegern)\n",
        "\n",
        "* https://youtu.be/390eRzVSC2k (Homomorphie mit Modulo und kommutativen Diagramm)\n",
        "\n",
        "**Homomorphiesatz (allgemein)**:\n",
        "\n",
        "* Aus einer Abbildung $f$ zwischen zwei Gruppen $G$ und $H$, die weder injektiv noch surjektiv ist, wollen wir eine bijektive Abbildung herleiten.\n",
        "\n",
        "* Schritt 1: Das macht man, indem man zuerst auf der rechten Seite alle Elemente ausschliesst, die nicht Teil der Zielmenge sind, und sich nur auf die 'getroffenen' Elemente fokussiert (The image of $f$ is hierbei a subgroup of $H$.)\n",
        "\n",
        "* Schritt 2: Jetzt hat man nur noch das Problem auf der linken Seite, dass mehrere Startelemente in $G$ auf ein und dasselbe Zielelement in $H$ verweisen. Man betrachtet die Startelemente dann einfach als identisch. Das macht man in dem man $G$ umwandelt in $G$ / Kern ($f$) (man bildet die Faktorgruppe, und spricht aus: modulo Kern von f. Der Quotientenvektorraum von $G$ nach kern von $f$.\n",
        "  * Das geht, weil $f$ ein Homomorphismus ist\n",
        "  * Das bedeutet, wenn zwei Elemente a und b im Start auf ein Element im Ziel verweisen, dann unterscheiden sie sich um ein Kernelement. Heisst, a minus b ist ein Element, das auf 0 geschickt wird, und damit ein Kernelement.\n",
        "  * Und wenn ich jetzt modulo des kerns rechne, dann tue ich so, als ob es die Differenz nicht gibt. Weil es bedeutet a =b. Der Quotientenraum ist ein kunstlich geschaffener Raum, wo a und b identisch gemacht wurden (kongruent).\n",
        "  * Remember aus Modulorechnung: Zwei Zahlen sind kongruent (modulo des Moduls m), wenn ihre Differenz durch m teilbar ist. Hier ist das Modulo der Kern. Also Rest muss 0 sein.\n",
        "  * Modulo n (Reste berechnen, hierbei 0): die Differenz zweier Elemente ist teilbar durch n. Die beiden Elemente sind dann kongruent (identisch).\n",
        "  * $G$ modulo Kern $f$ ist isomorph (identisch =bijektiv und homomorph)) zu Bild $f$, das in $H$ liegt.\n",
        "\n",
        "* **Der Kern von $f$ ist stets ein Normalteiler von $G$ und das Bild von $f$ ist eine Untergruppe von $H$. Nach dem Homomorphiesatz ist die Faktorgruppe $G / \\operatorname{Kern}(f)$ [isomorph (bijektiv)](https://de.m.wikipedia.org/wiki/Isomorphismus) zu Bild $(f)$.**\n",
        "\n",
        "**Bedingungen:**\n",
        "\n",
        "* Let $G$ and $H$ be two groups.\n",
        "* and let $f$ : $G \\rightarrow H$ be a [group homomorphism](https://de.wikipedia.org/wiki/Gruppenhomomorphismus).\n",
        "* and let $K$ be a normal subgroup (Normalteiler) in $G$ and $\\varphi$ the natural surjective homomorphism $G \\rightarrow G / K$ (where $G / K$ is a quotient group). Diese Faktorgruppen sind homomorphe Bilder von G und **jedes homomorphe Bild von G ist zu einer solchen Faktorgruppe G/K isomorph**.\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/homomorphy.jpg)\n",
        "\n",
        "**Then:**\n",
        "\n",
        "1. **Dann ist der Kern von $f$ ein Normalteiler von $G$.**\n",
        "  * Normalteiler sind die [Kerne](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) von Gruppenhomomorphismen, weshalb dann klar ist, dass umgekehrt der Kern von $f$ ein Normalteiler von $G$ ist.\n",
        "\n",
        "  * If $K$ is a **subset** of ker $(f)$ then there exists a unique homomorphism $h: G / K \\rightarrow H$ such that $f=h$ $\\varphi$. In other words, the natural projection $\\varphi$ is universal among homomorphisms on $G$ that map $K$ to the identity element.\n",
        "\n",
        "2. **und daher kann die Faktorgruppe $G /$ ker $f$ gebildet werden.**\n",
        "\n",
        "3. **Nach dem [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) ist diese Faktorgruppe $G /$ ker $f$ isomorph zum Bild von $f$, das eine Untergruppe von $H$ ist.**\n",
        "  * The image of $f$ is isomorphic to the quotient group $G /$ ker ($f$). And in particular, if $f$ is surjective then $H$ is isomorphic to $G$ / ker $(f)$. [Source](https://en.m.wikipedia.org/wiki/Isomorphism_theorems#First_Isomorphism_Theorem_4)\n",
        "  * The image of $f$ is hierbei a subgroup of $H$.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Group_homomorphism_ver.2.svg/500px-Group_homomorphism_ver.2.svg.png)\n",
        "\n",
        "*Image of a group homomorphism (h) from G (left) to H (right).*\n",
        "\n",
        "*The smaller oval inside H is the image of h. N is the kernel of h and aN is a coset of N.* [Source](https://en.m.wikipedia.org/wiki/Group_homomorphism)\n",
        "\n",
        "See also: https://mathepedia.de/Kern_und_Bild_Homomorphismus.html\n",
        "\n",
        "*Diagram of the fundamental theorem on homomorphisms where f is a homomorphism, N is a normal subgroup of G and e is the identity element of G.*\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Diagram_of_the_fundamental_theorem_on_homomorphisms.svg/440px-Diagram_of_the_fundamental_theorem_on_homomorphisms.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Category Theory*"
      ],
      "metadata": {
        "id": "jNQs-MXfBmVV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eLPSGAeDW0j"
      },
      "source": [
        "[Category](https://en.m.wikipedia.org/wiki/Category_(mathematics)) and [Category theory](https://en.m.wikipedia.org/wiki/Category_theory)\n",
        "\n",
        "* Eine Kategorie besteht aus Objekten und Morphismen. Man m√∂chte jedem Objekt in einer Kategorie ein Objekt in der anderen Kategorie zuordnen, und das gleiche mit den Morphismen zwischen den Objekten.\n",
        "\n",
        "* [Objects](https://ncatlab.org/nlab/show/object)\n",
        "\n",
        "* [Product und Coproduct](https://de.m.wikipedia.org/wiki/Produkt_und_Koprodukt)\n",
        "\n",
        "* [Anfangsobjekt (initiales), Endobjekt (terminales, finales) und Nullobjekt](https://de.m.wikipedia.org/wiki/Anfangsobjekt,_Endobjekt_und_Nullobjekt)\n",
        "\n",
        "* [Functor](\n",
        "https://ncatlab.org/nlab/show/functor): A homomorphism between categories is a functor. Zuordnung zwischen zwei Kategorien\n",
        "\n",
        "  * Funktoren werden auch Diagramme genannt (mitunter nur in bestimmten Kontexten), da sie eine formale Abstraktion [kommutativer Diagramme](https://de.m.wikipedia.org/wiki/Kommutatives_Diagramm) darstellen.\n",
        "\n",
        "  * functors must preserve [identity morphisms](https://en.m.wikipedia.org/wiki/Morphism#Definition) and [composition of morphisms](https://en.m.wikipedia.org/wiki/Function_composition)\n",
        "\n",
        "* [Natural Transformations](https://en.m.wikipedia.org/wiki/Natural_transformation) are Maps between functors (and functors are maps between categories)\n",
        "\n",
        "* [Duality](https://en.m.wikipedia.org/wiki/Dual_(category_theory)) is a correspondence between the properties of a category C and the dual properties of the opposite category C<sup>op</sup>.\n",
        "\n",
        "* Topos: Category theorists have proposed [topos theory](https://en.m.wikipedia.org/wiki/Topos) as an alternative to traditional [axiomatic set theory](https://de.m.wikipedia.org/wiki/Axiomatische_Mengenlehre). Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory.\n",
        "\n",
        "> [Outline of category theory](https://en.m.wikipedia.org/wiki/Outline_of_category_theory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlwPUfSMCiu"
      },
      "source": [
        "**Category Theory and Higher Category Theory**\n",
        "\n",
        "0) A category is a collection of objects and morphisms between those objects that satisfy some rules.\n",
        "\n",
        "1) A functor is a morphism in the category of categories.\n",
        "\n",
        "2) A natural transformation is a morphism in the category of functors.\n",
        "\n",
        "But they all stop right there. What about:\n",
        "\n",
        "3) the morphisms in the category of natural transformations?\n",
        "\n",
        "4) Or the \"morphisms in the category of the morphisms in the category of natural transformations\"\n",
        "\n",
        "*Definition*\n",
        "\n",
        "* [higher category theory](https://en.m.wikipedia.org/wiki/Higher_category_theory) bzw. [higher category theory](https://ncatlab.org/nlab/show/higher+category+theory) is the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities.\n",
        "\n",
        "* Higher category theory is often applied in algebraic topology (especially in homotopy theory), where one studies algebraic invariants of spaces, such as their fundamental weak ‚àû-groupoid.\n",
        "\n",
        "* *From 2-category to Higher Order Category*: The concept of 2-category generalizes further in higher category theory to n-categories, which have k-morphisms for all\n",
        "k\n",
        "‚â§\n",
        "n\n",
        ". The morphisms can be composed along the objects, while the 2-morphisms can be composed in two different directions: along objects ‚Äì called horizontal composition ‚Äì and along morphisms ‚Äì called vertical composition. The composition of morphisms is allowed to be associative only up to coherent associator 2-morphisms.\n",
        "\n",
        "* See also: [infinity-category](https://ncatlab.org/nlab/show/infinity-category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsmc7LDx_K-"
      },
      "source": [
        "*Morphismen (Linear Maps)*\n",
        "\n",
        "Ein [Morphismus](https://de.m.wikipedia.org/wiki/Morphismus) ist eine Funktion in Kategorientheorie. Man schreibt: $f\\colon X\\to Y$. Image Source: [Morphismen](https://youtu.be/0wKsFNLR15g)\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "* [Automorphismus](https://de.m.wikipedia.org/wiki/Automorphismus): bijektiv, Definitionsmenge = Zielmenge\n",
        "\n",
        "* [Isomorphismus](https://de.m.wikipedia.org/wiki/Isomorphismus): bijektiv, Definitionsmenge ‚â† Zielmenge\n",
        "\n",
        "* [Endomorphismus](https://de.m.wikipedia.org/wiki/Endomorphismus): Definitionsmenge = Zielmenge. aber Bildmenge umfasst nicht den ganzen Vektorraum (Ziele < Uspr√ºnge)\n",
        "\n",
        "* [Monomorphismus](https://de.m.wikipedia.org/wiki/Monomorphismus): injektiv (jedes Element des Usprungs hat ein exklusives Element)\n",
        "\n",
        "* [Epimorphismus](https://de.m.wikipedia.org/wiki/Epimorphismus): surjektiv (jedes Element im Ziel ist mind 1 mal getroffen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNkozbcHgYo"
      },
      "source": [
        "**Deep Dive: Homomorphismus**\n",
        "\n",
        "> Eine lineare Abbildung ist ein (Homo-)Morphismus zwischen Vektorr√§umen.\n",
        "\n",
        "*Unterschied zwischen Isomorphismus und Homomorphismus:*\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/isomorphismus.JPG)\n",
        "\n",
        "Ein [Vektorraum-Homomorphismus](https://de.m.wikipedia.org/wiki/Homomorphismus) (\"Lineare Abbildung\") ist eine Abbildung $\\varphi: V \\rightarrow W$ zwischen $K$ -Vektorr√§umen $V$ und $W$ (gemeinsamer Grundk√∂rper $K$) mit den folgenden Eigenschaften:\n",
        "\n",
        "\n",
        "1. **Additivit√§t**: $\\varphi(v+u)=\\varphi(v)+\\varphi(u)$ f√ºr alle $u$ und $v \\in V$\n",
        "\n",
        "2. **Homogenit√§t**: $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v) \\quad$ f√ºr alle $\\lambda \\in K$ und $v \\in V$ (Skalarmultiplikation)\n",
        "\n",
        "* Beispiel: Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt f√ºr die Multiplikation mit einem Skalar aus dem Grundk√∂rper.\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung unendlichdimensionaler Vektorr√§ume, die eine Topologie tragen, spricht man meist von linearen Operatoren statt von [linearen Abbildungen](https://de.m.wikipedia.org/wiki/Lineare_Abbildung).\n",
        "\n",
        "\n",
        "![xyz](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Reflection_of_a_triangle_about_the_y_axis.svg/320px-Reflection_of_a_triangle_about_the_y_axis.svg.png)\n",
        "\n",
        "*Achsenspiegelung als Beispiel einer linearen Abbildung*\n",
        "\n",
        "**Beispiele f√ºr Vektorraumhomomorphismus**\n",
        "\n",
        "* F√ºr $V=W=\\mathbb{R}$ hat jede lineare Abbildung die Gestalt $f(x)=m x$ mit $m \\in \\mathbb{R}$\n",
        "\n",
        "* Es sei $V=\\mathbb{R}^{n}$ und $W=\\mathbb{R}^{m}$. Dann wird f√ºr jede $m \\times n$ -Matrix $A$ mit Hilfe der Matrizenmultiplikation eine lineare Abbildung $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ durch\n",
        "\n",
        "$f(x)=A x=\\left(\\begin{array}{ccc}a_{11} & \\cdots & a_{1 n} \\\\ \\vdots & & \\vdots \\\\ a_{m 1} & \\cdots & a_{m n}\\end{array}\\right)\\left(\\begin{array}{c}x_{1} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right)$\n",
        "definiert.\n",
        "\n",
        "Jede lineare Abbildung von $\\mathbb{R}^{n}$ nach $\\mathbb{R}^{m}$ kann so dargestellt werden.\n",
        "\n",
        "Siehe auch: [Gruppenhomomorphismus](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Morphismen:**\n",
        "\n",
        "F√ºr manche Kategorien gibt es besondere Bezeichnungen f√ºr Morphismen.\n",
        "\n",
        "* Ein [Hom√∂omorphismus](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist ein Isomorphismus zwischen topologischen R√§umen. Sind (beispielsweise) die [Fundamentalgruppen](https://de.m.wikipedia.org/wiki/Fundamentalgruppe) zweier R√§ume isomorph, so sind die R√§ume hom√∂omorph.\n",
        "\n",
        "* Ein [Diffeomorphismus](https://de.m.wikipedia.org/wiki/Diffeomorphismus) ist ein Isomorphismus zwischen differenzierbaren Mannigfaltigkeiten.\n",
        "\n",
        "* Eine [Isometrie](https://de.m.wikipedia.org/wiki/Isometrie) ist ein Isomorphismus in der Kategorie der metrischen R√§umen mit den nichtexpansiven stetigen Abbildungen."
      ],
      "metadata": {
        "id": "p_vf2S1FOc2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Lie Algebra*"
      ],
      "metadata": {
        "id": "Q-zfzmbO4ZOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lie Groups Based Machine Learning\n",
        "- Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems\n",
        "https://lnkd.in/e-r-ChbX\n",
        "- Structure preserving deep learning\n",
        "https://lnkd.in/gPghU6x\n",
        "- Lie Group Cohomology and (Multi)Symplectic Integrators: New Geometric Tools for Lie Group Machine Learning Based on Souriau Geometric Statistical Mechanics\n",
        "https://lnkd.in/dCPaWBU\n",
        "\n",
        "more information at SEE GSI'23:\n",
        "https://gsi2023.org/\n"
      ],
      "metadata": {
        "id": "WQFsTGz04fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From: [QHack 2022: Marco Cerezo ‚ÄîBarren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)"
      ],
      "metadata": {
        "id": "imfDlpfHFb7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Theory**\n",
        "\n",
        "> ‚ÄúThe essential phenomenon of Lie theory is that one may associate in a natural way to a Lie group $\\mathcal{G}$ its Lie algebra $\\mathfrak{g}$. The Lie algebra $\\mathfrak{g}$ is first of all a vector space and secondly is endowed with a bilinear nonassociative product called the Lie bracket [...]. **Amazingly, the group $\\mathcal{G}$ is almost completely determined by $\\mathfrak{g}$ and its Lie bracket**. Thus for many purposes <font color=\"blue\">**one can replace $\\mathcal{G}$ with $\\mathfrak{g}$. Since $\\mathcal{G}$ is a complicated nonlinear object and $\\mathfrak{g}$ is just a vector space, it is usually vastly simpler to work with $\\mathfrak{g}$**</font>. [...] This is one source of the power of Lie theory.\" Stillwell: ‚Äúthe miracle of Lie theory‚Äù. (*https://arxiv.org/pdf/1812.01537.pdf*)\n",
        "\n",
        "* Article about [Lie-Gruppe](https://de.m.wikipedia.org/wiki/Lie-Gruppe)\n",
        "\n",
        "* See also [Lie algebra representation](https://en.m.wikipedia.org/wiki/Lie_algebra_representation) and [representation of a Lie group](https://en.m.wikipedia.org/wiki/Representation_of_a_Lie_group) (a linear action of a Lie group on a vector space). Representations play an important role in the study of continuous symmetry.\n",
        "\n",
        "* See this very good paper: [A micro Lie theory for state estimation in robotics](https://arxiv.org/abs/1812.01537)\n",
        "\n",
        "* Lie theory for the roboticist: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "* https://github.com/artivis/manif/blob/devel/paper/Lie_theory_cheat_sheet.pdf"
      ],
      "metadata": {
        "id": "NoSEZYMD4ijj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Group**\n",
        "\n",
        "* A [Lie group](https://en.wikipedia.org/wiki/Lie_group) is a group that is also a **differentiable manifold**. A manifold is a space that locally resembles Euclidean space, whereas groups define the abstract, generic concept of **multiplication and the taking of inverses (division)**. Combining these two ideas, one obtains a continuous group where points can be multiplied together, and their inverse can be taken.\n",
        "\n",
        "* Lie groups provide a natural model for the concept of **continuous symmetry**, a celebrated example of which is the rotational symmetry in three dimensions (given by the special orthogonal group ${\\text{SO}}(3)$).\n",
        "\n",
        "* Lie groups were first found by studying matrix subgroups\n",
        "$G$ contained in ${\\text{GL}}_{n}(\\mathbb {R} )$ or ${\\text{GL}}_{n}(\\mathbb {C} )$, the groups of $n\\times n$ invertible matrices over $\\mathbb {R}$  or $\\mathbb {C}$ (now called the classical groups).\n",
        "\n",
        "* Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations."
      ],
      "metadata": {
        "id": "1a43uSa04kxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of Lie groups**\n",
        "\n",
        "> [Table of some common Lie groups and their associated Lie algebras](https://en.m.wikipedia.org/wiki/Table_of_Lie_groups)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_01.png)\n",
        "\n",
        "* **Der Einheitskreis in der komplexen Zahlenebene**, d. h. die Menge $S^{1}=\\{z \\in \\mathbb{C}:|z|=1\\}$ der komplexen Zahlen vom Betrag 1, ist eine Untergruppe von $\\left(\\mathbb{C}^{*}, \\cdot\\right)$, die sogenannte **Kreisgruppe**: Das Produkt zweier Zahlen vom Betrag 1 hat wieder Betrag 1, ebenso das Inverse. Auch hier hat man eine $_{n}$ mit der Differentialrechnung vertr√§gliche Gruppenstruktur\", d. h. eine Lie-Gruppe.\n",
        "\n",
        "* **Die Menge $\\mathbb{C}^{*}=\\mathbb{C} \\backslash\\{0\\}$ der komplexen Zahlen ungleich 0 bildet mit der gew√∂hnlichen Multiplikation eine Gruppe $\\left(\\mathbb{C}^{*}, \\cdot\\right)$**. Die Multiplikation ist eine differenzierbare Abbildung $m: \\mathbb{C}^{*} \\times \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ definiert durch $m(x, y)=x y_{i}$ auch die durch $i(z)=z^{-1}=\\frac{1}{z}$ definierte Inversion $i: \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ ist differenzierbar. Die Gruppenstruktur der komplexen Ebene (bzgl. Multiplikation) ist also mit der Differentialrechnung vertr√§glich.\n",
        "\n",
        "* [Beispiele fur Lie-Gruppen](https://de.wikipedia.org/wiki/Lie-Gruppe#Beispiele) sind:  allgemeine lineare Gruppe,  Orthogonale Gruppe,  Unit√§re Gruppe & Spezielle Unit√§re Gruppe,  Affine Gruppe, [Poincar√©-Gruppe](https://en.m.wikipedia.org/wiki/Poincar√©_group), Galilei-Gruppe\n",
        "\n",
        "* **Simple Lie Groups**: a simple Lie group is a connected non-abelian Lie group G which does not have nontrivial connected normal subgroups. The list of simple Lie groups can be used to read off the list of simple Lie algebras and [Riemannian symmetric spaces](https://en.wikipedia.org/wiki/Symmetric_space). See also [Classification of semisimple Lie algebras](https://en.wikipedia.org/wiki/Dynkin_diagram#Classification_of_semisimple_Lie_algebras)\n"
      ],
      "metadata": {
        "id": "V9P7SfTo4m6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Algebra**\n",
        "\n",
        "* a [Lie algebra](https://en.m.wikipedia.org/wiki/Lie_algebra) **is a vector space $g$ together with an operation called the Lie bracket**, an alternating bilinear map $\\mathfrak{g} \\times \\mathfrak{g} \\rightarrow \\mathfrak{g},(x, y) \\mapsto[x, y]$, that satisfies the Jacobi identity.\n",
        "\n",
        "* The vector space $\\mathfrak{g}$ together with this operation is a non-associative algebra, meaning that the Lie bracket is not necessarily associative.\n",
        "\n",
        "> <font color=\"blue\">**Any Lie group gives rise to a Lie algebra, which is its tangent space at the identity.**\n",
        "\n",
        "* In physics, Lie groups appear as symmetry groups of physical systems, and their Lie algebras (tangent vectors near the identity) may be thought of as infinitesimal symmetry motions. Thus Lie algebras and their representations are used extensively in physics, notably in quantum mechanics and particle physics.\n",
        "\n",
        "* [Lie Algebra](https://de.m.wikipedia.org/wiki/Lie-Algebra) ist eine algebraische Struktur, die mit einer Lie-Klammer versehen ist, d. h. es existiert eine antisymmetrische Verkn√ºpfung, die die Jacobi-Identit√§t erf√ºllt.\n",
        "\n",
        "* Lie-Algebren werden haupts√§chlich zum Studium geometrischer Objekte wie Lie-Gruppen und differenzierbarer Mannigfaltigkeiten eingesetzt.\n",
        "\n",
        "* **Simple Lie Algebra**: a [simple Lie algebra](https://en.wikipedia.org/wiki/Simple_Lie_algebra) is a Lie algebra that is nonabelian and contains no nonzero proper ideals. The classification of real simple Lie algebras is one of major achievements of Wilhelm Killing and √âlie Cartan. A direct sum of simple Lie algebras is called a semisimple Lie algebra. A simple Lie group is a connected Lie group whose Lie algebra is simple.\n",
        "\n",
        "* **Semisimple Lie algebra**: a [Lie algebra is semisimple](https://en.wikipedia.org/wiki/Semisimple_Lie_algebra) if it is a direct sum of simple Lie algebras (non-abelian Lie algebras without any non-zero proper ideals)."
      ],
      "metadata": {
        "id": "xcVYeFvX4o4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie-Algebra der Lie-Gruppe**\n",
        "\n",
        "* Die Vektorfelder auf einer glatten Mannigfaltigkeit $M$ bilden mit der Lie-Klammer eine unendlich-dimensionale Lie-Algebra. Die zu einer Lie-Gruppe $G$ geh√∂rende Lie-Algebra $\\mathfrak{g}$ besteht aus dem Unterraum der [links-invarianten](https://de.m.wikipedia.org/wiki/Translationsinvarianz) Vektorfelder auf $G$.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum $T_{e} G$ am neutralen Element $e$ von $G$. Insbesondere gilt also $\\operatorname{dim} G=\\operatorname{dim} \\mathfrak{g}$. Bez√ºglich der LieKlammer $[\\cdot, \\cdot]$ ist der Vektorraum $\\mathfrak{g}$ abgeschlossen.\n",
        "\n",
        "* **Somit ist der Tangentialraum einer Lie-Gruppe $G$ am neutralen Element eine Lie-Algebra. Diese Lie-Algebra nennt man die Lie-Algebra der Lie-Gruppe $G$.**\n",
        "\n",
        "* Zu jeder Lie-Gruppe $G$ mit Lie-Algebra $\\mathfrak{g}$ gibt es eine **Exponentialabbildung exp (exponential map)**: $\\mathfrak{g} \\rightarrow G$. Diese Exponentialabbildung kann man definieren durch $\\exp (A)=\\Phi_{1}(e)$, wobei $\\Phi_{t}$ der Fluss des links-invarianten Vektorfelds $A$ und $e \\in G$ das neutrale Element ist. Falls $G$ eine abgeschlossene Untergruppe der $\\mathrm{GL}(n, \\mathbb{R})$ oder $\\mathrm{GL}(n, \\mathbb{C})$ ist, so ist die so definierte Exponentialabbildung identisch mit der Matrixexponentialfunktion.\n",
        "\n",
        "\n",
        "* Jedes Skalarprodukt auf $T_{e} G=\\mathfrak{g}$ definiert eine $G$ -links-invariante Riemannsche Metrik auf $G$. Im Spezialfall, dass diese Metrik zus√§tzlich auch\n",
        "rechtsinvariant ist, stimmt die Exponentialabbildung der Riemannschen\n",
        "Mannigfaltigkeit $G$ am Punkt $e$ mit der Lie-Gruppen-Exponentialabbildung\n",
        "√ºberein.\n",
        "\n",
        "* Mit der Lie-Gruppe $\\mathrm{SO}(n)$ ist eine Lie-Algebra $\\mathfrak{s o}(n)$ verkn√ºpft, ein Vektorraum mit einem bilinearen alternierenden Produkt (Lie-Klammer), wobei der Vektorraum bez√ºglich der Lie- Klammer abgeschlossen ist.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum am neutralen Element der $\\mathrm{SO}(n)$ (neutrales Element ist die Einheitsmatrix), sodass insbesondere $\\operatorname{dim}_\\mathfrak{s o}(n)=\\operatorname{dim} \\mathrm{SO}(n)$ gilt.\n",
        "\n",
        "* Die Lie-Algebra besteht aus allen schiefsymmetrischen $n \\times n$\n",
        "-Matrizen und ihre Basis sind die sog. Erzeugenden Generators).\n",
        "\n",
        "* Die Exponentialabbildung verkn√ºpft die Lie-Algebra mit der Lie-Gruppe:\n",
        "\n",
        "> $\\exp : \\mathfrak{s o}(n) \\rightarrow \\mathrm{SO}(n), J \\mapsto \\sum_{k=0}^{\\infty} \\frac{1}{k !} J^{k}$\n",
        "\n",
        "Exponentiation in quantum: $\\mathbb{G} = e^\\mathfrak{g}$"
      ],
      "metadata": {
        "id": "YiJEo1Y54q9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why using Lie Algebra?**\n",
        "\n",
        "* For example $\\mathcal{G}$ is all the operations on the surface of a ball (nonlinear): Lie Groups are Continuous Transformation Groups, like a 3D rotation vector on a curved surface. Very complicated to work with. (Lie Algebra SO(3)): $\n",
        "\\boldsymbol{W}^{\\wedge}=[\\boldsymbol{\\omega}]_{\\times}=\\left[\\begin{array}{ccc}\n",
        "0 & -\\omega_{z} & \\omega_{y} \\\\\n",
        "\\omega_{z} & 0 & -\\omega_{x} \\\\\n",
        "-\\omega_{y} & \\omega_{x} & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "* Meanwhile Lie Algebra $\\mathfrak{g}$ is at the origin of the tangent plane which is a linear vector space: Cartesian R3: $\n",
        "\\omega=\\left(\\omega_{x}, \\omega_{y}, \\omega_{z}\\right)\n",
        "$\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algebra\"** $\\rightarrow$ Exponential map translates between both\n",
        "\n",
        "**Lie Groups** were know as \"Continuous Transformation Groups\". **= a group that is also a smooth (differential) manifold**\n",
        "\n",
        "  * **a smooth manifold whose elements satisfy the group axioms**\n",
        "\n",
        "  * (so no singularities or breaks where differentiation or integration wouldn't work anymore)\n",
        "\n",
        "  * each point on a manifold represents one element of the Lie Group (i.e. 3d rotation matrix on the manifold)\n",
        "\n",
        "  * corresponding to it in the cartesian tangent space you can find a 3d rotation vector\n",
        "\n",
        "**Lie Algebra** is the in origin point on manifold (https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s)\n",
        "\n",
        "**Exponential Map** from (cartesian) tangent space to manifold (i.e 3 D surface):\n",
        "\n",
        "  * from tangent space a to manifold ('**exponential of a**'), like the exponential of a rotation vector (on tangent space) is the 3d rotation matrix (on manifold)\n",
        "\n",
        "  * and back: point on manifold x and its '**logarithm of x**' back on the tangent space\n",
        "\n",
        "  * it's an exact operation, and no approximation\n",
        "\n",
        "  * explanation and proof here: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_07.png)"
      ],
      "metadata": {
        "id": "9tPBGJIU4s4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie algebra is the origin point (= identity) on the tangent space!**\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algreba\"** **each time you take a derivative on the manifold, you are out of the manifold**, but you can stay on the tangent space for doing this operation (because there they are well defined)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_08.png)\n",
        "\n",
        "*https://arxiv.org/pdf/1812.01537.pdf*"
      ],
      "metadata": {
        "id": "YmuDaXN94zEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **antipodal point** to the origin: from origin point go half a turn around a 3d ball, you and up on the antipodal point, and there are many ways to go (any vector with length pie œÄ)\n",
        "\n",
        "* The tangent space will cover the manifold multiple times !!\n",
        "\n",
        "  * \"**First cover of the manifold by the tangent space**\": all points on the tangent space will end up in the antipodal point when applying any vector with length pie œÄ)\n",
        "\n",
        "  * there you can see relationship between Lie group and tangent space on manifold\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_02.png)"
      ],
      "metadata": {
        "id": "wiDgFHe2449f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following image: $R^{m}$ and $T_{E} M$ are the same vector space but with different representations\n",
        "\n",
        "> **Lie Algebra $T_{E} M \\sim R_{m} \\text { (Cartesian Tangent Space) and } w \\sim w^{\\wedge}$**\n",
        "\n",
        "* This means that $T_{E} M$ is isomorph to $R_{m}$\n",
        "\n",
        "* Since you can always go to an $R_{m}$ space, any Lie Group will have an cartesian $R_{m}$ tangent space, which is a vector\n",
        "\n",
        "* you can alweays go from one to the other give the isomorphism\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_03.png)"
      ],
      "metadata": {
        "id": "cwmJvdVO47i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SO(3): Lie group of rotation matrices in 3D**\n",
        "\n",
        "* you can write [w]<sub>x</sub> as a linear combination of 3 base matrices $E$<sub>x, y, z</sub>, which facilitates the calculation / it's easier than working directly with [w]<sub>x</sub>\n",
        "\n",
        "* and since $T_{E} M \\sim R_{m}$ sowie $w \\sim w^{\\wedge}$ (Isomorphism), it's also an allowed (exact) operation\n",
        "\n",
        "* Let's write a tangent vector as a regular cartesian vector with 3 coordinates\n",
        "\n",
        "> $[\\omega]_{\\times}=\\omega_{x} \\mathbf{E}_{x}+\\omega_{y} \\mathbf{E}_{y}+\\omega_{z} \\mathbf{E}_{z}$\n",
        "\n",
        "* The matrix product $[\\omega]_{\\times}$ is equivalent to cross product of vectors $\\omega =\\omega_{x} \\mathbf +\\omega_{y} \\mathbf +\\omega_{z}$, isomorph, two ways of representing the same elemtn of the tangent space\n",
        "\n",
        "* looking at the previous slide: cartesian R<sup>m</sup> is easier to work with than Lie Algebra T<sub>E</sub>M\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_04.png)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_05.png)\n",
        "\n",
        "**An elementary example is the space of three dimensional vectors $\\mathfrak{g}=\\mathbb{R}^{3}$** with the bracket operation defined by the cross product $[x, y]=x \\times y$. This is skew-symmetric since $x \\times y=-y \\times x$, and instead of associativity it satisfies the Jacobi identity:\n",
        "\n",
        "> $\n",
        "x \\times(y \\times z)=(x \\times y) \\times z+y \\times(x \\times z)\n",
        "$\n",
        "\n",
        "* **This is the Lie algebra of the Lie group of rotations of space**, and each vector $v \\in \\mathbb{R}^{3}$ may be pictured as an infinitesimal rotation around the axis $v$, with velocity equal to the magnitude of $v$.\n",
        "\n",
        "* The Lie bracket is a measure of the noncommutativity between two rotations: since a rotation commutes with itself, we have the alternating property $[x, x]=x \\times x=0$."
      ],
      "metadata": {
        "id": "ds8k2eji4-9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Multilinear Algebra (Exterior / Grassmann)*"
      ],
      "metadata": {
        "id": "Nj_F9wX47fTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Multivector"
      ],
      "metadata": {
        "id": "XkCXzKHAmnV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Algebra*"
      ],
      "metadata": {
        "id": "ZiAK4jblJhPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear algebra**\n",
        "\n",
        "* [Multilinear algebra](https://en.m.wikipedia.org/wiki/Multilinear_algebra) extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, **multilinear algebra builds on the concepts of**\n",
        "\n",
        "  * [Multivectors (p-vectors)](https://en.m.wikipedia.org/wiki/Multivector)\n",
        "\n",
        "  * [Exterior algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra) bzw. [Grassmann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra).\n",
        "\n",
        "* Fundamental objects of study in multilinear algebra are\n",
        "\n",
        "  * [Multilinear maps](https://en.m.wikipedia.org/wiki/Multilinear_map) (Multilineare_Abbildung: Abbildung, die f√ºr jedes ihrer Argumente linear ist)\n",
        "\n",
        "  * [Multilinear forms](https://en.m.wikipedia.org/wiki/Multilinear_form)\n",
        "\n",
        "* Abbildung von Modul in einen Ring (also Verallgemeinerung der K-Algebra von Vektorraum in den Korper, zB bei Integration oder Differential)\n",
        "\n",
        "  * Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "  * Jede lineare Abbildung ist eine 1-lineare Abbildung.\n",
        "\n",
        "  * Jede bilineare Abbildung ist eine 2-lineare Abbildung. (S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "    * Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen. Bei diesen ist der Wertebereich G mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch.)\n",
        "\n",
        "  * Sparprodukt ist eine 3-lineare Abbildung\n",
        "\n",
        "*  Multilinearform: wie linear- oder bilinearform, nur mehr argumente.\n"
      ],
      "metadata": {
        "id": "lKfW4ry1JlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exterior (Grassmann) Algebra*"
      ],
      "metadata": {
        "id": "Wkacn855ct_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gra√ümann-Algebra (Exterior Algebra)**\n",
        "\n",
        "Die Gra√ümann-Algebra $\\Lambda V$ eines reellen Vektorraumes $V$ ist die Clifford-Algebra $Cl(V,0)$ mit der trivialen quadratischen Form $Q=0$.\n",
        "\n",
        "Diese Beziehung ist unter anderem f√ºr die Quantisierung supersymmetrischer Feldtheorien wichtig.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Clifford-Algebra#Gra√ümann-Algebra\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) bzw. [Exterior Algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra): Algebra der Differentialformen.\n",
        "\n",
        "* Exterior Algebra eines Vektorraums V ist eine assoziative, schiefsymmetrisch-graduierte Algebra mit Einselement.\n",
        "\n",
        "* The exterior algebra provides an algebraic setting in which to answer geometric questions.\n",
        "\n",
        "* **Exterior algebra, geometric algebra, and clifford algebra are linear algebras** [Quora](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* Sie ist ‚Äì je nach Definition ‚Äì **Unteralgebra oder eine Faktoralgebra einer antisymmetrisierten** [**Tensoralgebra**](https://de.m.wikipedia.org/wiki/Tensoralgebra) von V und wird durch $\\Lambda V$ dargestellt.\n",
        "\n",
        "* Die Multiplikation wird als **√§u√üeres Produkt, Keilprodukt, Dachprodukt oder Wedgeprodukt** bezeichnet. Ein Spezialfall dieses Produkts ist mit dem Kreuzprodukt verwandt.\n",
        "\n",
        "* Anwendung: linearen Algebra (Theorie der Determinanten), Differentialgeometrie (Algebra der Differentialformen)"
      ],
      "metadata": {
        "id": "PaJipKZ_JnRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Vectors ($k$-blades)*"
      ],
      "metadata": {
        "id": "WqR3v9EaJrvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exterior Product (Wedge product u $\\wedge$ v) in Exterior Algebra (Gra√ümann-Algebra)**\n",
        "\n",
        "* **The exterior product (also called the Wedge product u $\\wedge$ v) used to construct bivectors and multivectors (is multilinear)**\n",
        "\n",
        "* the [exterior product or wedge product of vectors](https://en.m.wikipedia.org/wiki/Exterior_algebra) is an algebraic construction used in geometry to study areas, volumes, and their higher-dimensional analogues.\n",
        "\n",
        "> **The exterior product (also called the wedge product) used to construct bivectors and multivectors (is multilinear)**: The wedge product of two vectors is another object commonly called a bivector, and these lie in their own vector space. [Quora](https://www.quora.com/Is-the-wedge-product-the-same-thing-as-the-general-outer-product-If-not-why-is-the-wedge-symbol-used-for-both)\n",
        "\n",
        "* The exterior product of two vectors $u$ and $v$, denoted by $u\\wedge v$, is called a [bivector](https://en.m.wikipedia.org/wiki/Bivector) and lives in a space called the exterior square, a vector space that is distinct from the original space of vectors.\n",
        "\n",
        "* The exterior product, commonly called the wedge product, acts on tangent vectors and is an important operation in differential geometry that generalizes the cross product of 3-vectors.\n",
        "\n",
        "* See also: https://towardsdatascience.com/exterior-product-ecd5836c28ab\n",
        "\n",
        "* Add-on Differentiation:\n",
        "\n",
        "  * The **exterior product** is related to the tensor product in that the exterior product of two forms (a form is a skew-symmetric tensor of type (0,ùëù)) is just the antisymmetrization of the tensor product.\n",
        "\n",
        "  * The **cross product** is a speciality of the three-dimensional space; here the space of 2-forms has the same dimension as the space of 1-forms; indeed, given a metric, the hodge star maps between them. Since the metric also allows to associate vectors and 1-forms, you can define the cross product of v and ùë§ by the following procedure: Determine the 1-forms corresponding to ùë£ and ùë§, calculate their exterior product (which is a 2-form), apply the Hodge star to the result (which, given that we are in three dimensions, again results in a 1-form), and finally determine the vector corresponding to that 1-form.\n",
        "\n",
        "  * [Cross product as an external product](https://en.m.wikipedia.org/wiki/Cross_product#Cross_product_as_an_external_product)\n",
        "\n",
        "  * https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "9yW5RW7IJtlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$k$-blade ($k$-vector)**\n",
        "\n",
        "* a [$k$-blade](https://en.m.wikipedia.org/wiki/Blade_(geometry)) or a simple $k$-vector is a generalization of the concept of scalars and vectors to include simple bivectors, trivectors, etc.\n",
        "\n",
        "* Specifically, a $k$ blade is a $k$-vector that can be expressed as the exterior product (informally wedge product) of 1-vectors, and is of grade $k$. In detail:\n",
        "\n",
        "  * A 0-blade is a scalar.\n",
        "\n",
        "  * A 1-blade is a vector. Every vector is simple.\n",
        "\n",
        "  * A 2-blade is a simple bivector. Sums of 2-blades are also bivectors, but not always simple. A 2-blade may be expressed as the wedge product of two vectors $a$ and $b$ : $a \\wedge b$.\n",
        "\n",
        "  * A 3-blade is a simple trivector, that is, it may be expressed as the wedge product of three vectors $a, b$, and $c$ : $a \\wedge b \\wedge c \\text {. }$\n",
        "\n",
        "  * In a vector space of dimension $n$, a blade of grade $n-1$ is called a [pseudovector](https://en.m.wikipedia.org/wiki/Pseudovector) or an [antivector](https://en.m.wikipedia.org/wiki/Antivector)"
      ],
      "metadata": {
        "id": "ffxYeT3eJvhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Antisymmetry**\n",
        "\n",
        "> $\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2}=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}$\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} &=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1} \\wedge \\mathbf{e}_{3} \\\\\n",
        "&=\\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} \\wedge \\mathbf{e}_{1} \\\\\n",
        "&=-\\mathbf{e}_{3} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra"
      ],
      "metadata": {
        "id": "2ouXP-bDJxWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivector (Clifford number)**\n",
        "\n",
        "* In multilinear algebra, a [multivector](https://en.m.wikipedia.org/wiki/Multivector), sometimes called **Clifford number**,  is an element of the exterior algebra $\\Lambda(V)$ of a vector space $V$.\n",
        "\n",
        "* The exterior product $\\wedge$ (Wedge product) used to construct [bivectors](https://en.m.wikipedia.org/wiki/Bivector) and [multivectors](https://en.m.wikipedia.org/wiki/Multivector) (is multilinear)\n",
        "\n",
        "* This algebra is graded, associative and alternating, and consists of linear combinations of simple $k$ -vectors, also known as decomposable $k$ -vectors  or [$k$-blades](https://en.m.wikipedia.org/wiki/Blade_(geometry)), of the form\n",
        "\n",
        "> $v_{1} \\wedge \\cdots \\wedge v_{k}$\n",
        "\n",
        "where $v_{1}, \\ldots, v_{k}$ are in $V$.\n",
        "\n",
        "* A $k$ -vector is such a linear combination that is homogeneous of degree $k$ (all terms are\n",
        "$k$ -blades for the same $k$ ). Depending on the authors, a \"multivector\" may be either a $k-$\n",
        "vector or any element of the exterior algebra (any linear combination of $k$ -blades with potentially differing values of $k$ ).\n",
        "\n",
        "* In differential geometry, a $k$ -vector is a vector in the exterior algebra of the tangent\n",
        "vector space; that is, it is an antisymmetric tensor obtained by taking linear\n",
        "combinations of the exterior product of $k$ tangent vectors, for some integer $k \\geq 0 .$\n",
        "\n",
        "* A differential $k$ -form is a $k$ -vector in the exterior algebra of the dual of the tangent space,\n",
        "which is also the dual of the exterior algebra of the tangent space.\n",
        "\n",
        "> **For $k=0,1,2$ and $3, k$ -vectors are often called respectively scalars, vectors, bivectors and trivectors; they are respectively dual to 0 -forms, 1 -forms, 2 -forms and 3 forms.**"
      ],
      "metadata": {
        "id": "JbnIY2DPJzO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation for the exterior product of n 1-forms (Œµ, Œ∑, œâ) to obtain an n-form (\"mesh\" of coordinate surfaces, here planes), for n = 1, 2, 3. The \"circulations\" show orientation*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0938.png)"
      ],
      "metadata": {
        "id": "yyX6pjvrJ1J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation of grade n elements in a real exterior algebra for n = 0 (signed point), 1 (directed line segment, or vector), 2 (oriented plane element), 3 (oriented volume). The exterior product of n vectors can be visualized as any n-dimensional shape (e.g. n-parallelotope, n-ellipsoid); with magnitude (hypervolume), and orientation defined by that of its (n ‚àí 1)-dimensional boundary and on which side the interior is.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0939.png)"
      ],
      "metadata": {
        "id": "vcYQLc-WJ3H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parallel plane segments with the same orientation and area corresponding to the same bivector a ‚àß b:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Wedge_product.JPG/471px-Wedge_product.JPG)"
      ],
      "metadata": {
        "id": "7-GwR_RyJ40H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Maps (Operators)*"
      ],
      "metadata": {
        "id": "JYooOdCtJ76C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "XG6PAvtPN-V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Eine [lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)* from vector space(s) to a vector space\n",
        "\n",
        "> $f\\colon V\\to V$\n",
        "\n",
        "* [Lineare Abbildung = Linear Maps](https://de.wikipedia.org/wiki/Lineare_Abbildung): Eine lineare Abbildung (auch lineare Transformation oder Vektorraumhomomorphismus $\\varphi: V \\longrightarrow \\omega$ genannt) ist in der linearen Algebra ein wichtiger Typ von **Abbildung zwischen zwei Vektorr√§umen √ºber demselben K√∂rper**.\n",
        "\n",
        "* Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt f√ºr die Multiplikation mit einem Skalar aus dem Grundk√∂rper.\n",
        "\n",
        "Algebraische Eigenschaften:\n",
        "\n",
        "> $\\varphi\\left(v_{1}+v_{2}\\right)=\\varphi\\left(v_{c}\\right)+\\varphi\\left(v_{2}\\right)$\n",
        "\n",
        "> $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v)$\n",
        "\n",
        "* Beispiel: matrix multiplication\n",
        "\n",
        "* Eine lineare Abbildung $f\\colon V\\to V$ (also ein Endomorphismus) eines endlichdimensionalen Vektorraumes $V$ ist bereits invertierbar, wenn sie injektiv oder surjektiv ist.\n",
        "\n",
        "* Dies ist wiederum genau dann der Fall, wenn ihre Determinante ungleich null ist.\n",
        "\n",
        "* Hieraus folgt, dass die Eigenwerte eines Endomorphismus genau die Nullstellen seines charakteristischen Polynoms sind.\n",
        "\n",
        "* Eine weitere wichtige Aussage √ºber das charakteristische Polynom ist der Satz von Cayley-Hamilton.\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$. <font color=\"red\">Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)</font>\n",
        "\n",
        "* [LINEARE ABBILDUNG / Homomorphismus einfach erkl√§rt](https://www.youtube.com/watch?v=KK_fHodz-lQ)\n",
        "\n",
        "* Lineare Abbildungen als Tensoren:\n",
        "\n",
        "  * **Case 1: Linear Maps for Transformations of vectors within one basis (with linear maps)**: rank 1 tensor (1,0)-tensor, 1 axe\n",
        "\n",
        "  * **Case 2: Linear Maps for Transformations of vectors across two basis (with linear maps + forward/backward)**: Linear Transformation as a (1,1)-Tensor - rank 2 tensor, which usually is represented as a matrix (e.g. 2 axes). For example changing the linear map $L$ in one basis (i.e. Euclidean basis) and rotate basis where we define a new linear map $\\widetilde{L}$. We're not just multiplying by the inverse transform (contravariant), nor just the forward transform (covariant), we're doing both, which hints that this is a (1,1)-tensor!\n",
        "\n",
        "Bildung des Vektorraums $L(V,W)$:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/4/49/Vektorraum_linearer_Abbildungen.svg)"
      ],
      "metadata": {
        "id": "XR5ccHQcMM3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Besondere lineare Abbildungen:**](https://de.wikipedia.org/wiki/Lineare_Abbildung#Besondere_lineare_Abbildungen)\n",
        "\n",
        "* **Monomorphismus**\n",
        "Ein Monomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die injektiv ist. Dies trifft genau dann zu, wenn die Spaltenvektoren der Darstellungsmatrix linear unabh√§ngig sind.\n",
        "\n",
        "* **Epimorphismus**\n",
        "Ein Epimorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die surjektiv ist. Das ist genau dann der Fall, wenn der Rang der Darstellungsmatrix gleich der Dimension von $W$ ist.\n",
        "\n",
        "\n",
        "* **Isomorphismus**\n",
        "Ein Isomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die bijektiv ist. Das ist genau der Fall, wenn die Darstellungsmatrix regul√§r ist. Die beiden R√§ume $V$ und $W$ bezeichnet man dann als isomorph.\n",
        "\n",
        "* **Endomorphismus**\n",
        "Ein Endomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung, bei der die R√§ume $V$ und $W$ gleich sind: $f: V \\rightarrow V$. Die Darstellungsmatrix dieser Abbildung ist eine quadratische Matrix.\n",
        "\n",
        "* **Automorphismus**\n",
        "Ein Automorphismus zwischen Vektorr√§umen ist eine bijektive lineare Abbildung, bei der die R√§ume $V$ und $W$ gleich sind. Er ist also sowohl ein Isomorphismus als auch ein Endomorphismus. Die Darstellungsmatrix dieser\n",
        "Abbildung ist eine regul√§re Matrix."
      ],
      "metadata": {
        "id": "xEceSoEzMS6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lineare Operatoren (unendlichdimensional)**\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung [unendlichdimensionaler Vektorr√§ume](https://de.m.wikipedia.org/wiki/Lineare_Abbildung#Lineare_Abbildungen_zwischen_unendlichdimensionalen_Vektorr√§umen), die eine Topologie tragen, spricht man meist von [linearen Operatoren](https://de.wikipedia.org/wiki/Linearer_Operator) statt von linearen Abbildungen. Formal gesehen sind die Begriffe gleichbedeutend.\n",
        "\n",
        "* Die Bedeutung linearer Operatoren besteht darin, dass sie die lineare Struktur des unterliegenden Raumes respektieren, d. h., sie sind Homomorphismen zwischen Vektorr√§umen [Source](https://de.wikipedia.org/wiki/Linearer_Operator)\n",
        "\n",
        "* Der Begriff linearer Operator wurde in der Funktionalanalysis (einem Teilgebiet der Mathematik) eingef√ºhrt und ist **synonym zum Begriff der linearen Abbildung**. Eine lineare Abbildung ist eine **strukturerhaltende Abbildung zwischen Vektorr√§umen √ºber einem gemeinsamen K√∂rper**.\n",
        "\n",
        "* Werden Vektorr√§ume √ºber dem K√∂rper der reellen oder komplexen Zahlen betrachtet und sind diese mit einer Topologie versehen (lokalkonvexe R√§ume, normierte R√§ume, Banachr√§ume), so spricht man vorzugsweise von linearen Operatoren.\n",
        "\n",
        "* **Anwendungen linearer Operatoren sind:**\n",
        "\n",
        "  * Die Beschreibung von Koordinatentransformationen im dreidimensionalen Euklidischen Raum (**Spiegelung, Drehung, Streckung**) und der [Lorentztransformation](https://de.wikipedia.org/wiki/Lorentz-Transformation) in der vierdimensionalen Raumzeit durch Matrizen.\n",
        "\n",
        "  * **Die Entwicklung von L√∂sungstheorien f√ºr Differential- und Integralgleichungen, siehe Sobolew-Raum und Distribution.**\n",
        "\n",
        "  * Die Darstellung von Observablen in der Quantenmechanik und die Beschreibung der Dynamik eines quantenmechanischen Systems durch seinen Hamilton-Operator H in der Schr√∂dingergleichung."
      ],
      "metadata": {
        "id": "xMl0YGFkMUnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Operatoralgebra*\n",
        "\n",
        "* Welche Homomorphismen von einer Banachalgebra in eine Operatoralgebra existieren, wird in der Darstellungstheorie untersucht. Ein besonderes Interesse gilt dabei Darstellungen auf Hilbertr√§umen, das hei√üt Homomorphismen in die Operatoralgebra √ºber einem Hilbertraum, was zu den Begriffen Von-Neumann-Algebra und C*-Algebra f√ºhrt.\n",
        "\n",
        "* Siehe also [Chemtext](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_%28Levitus%29/11%3A_Operators/11.02%3A_Operator_Algebra)\n",
        "\n",
        "* [Operatoralgebren](https://de.m.wikipedia.org/wiki/Operatoralgebra) sind  **Verallgemeinerungen der [Matrizenalgebren](http://www-hm.ma.tum.de/ws0607/ei1/folien/Folie_11_RechnenMitMatrizen.pdf)** der linearen Algebra\n",
        "\n",
        "* Sind $E, F, G$ normierte R√§ume und $A: E \\rightarrow F$ und $B: F \\rightarrow G$ stetige, lineare Operatoren, so ist auch deren Komposition ein stetiger, linearer Operator $B \\circ A: E \\rightarrow G,$ und f√ºr die Operatornormen gilt $\\|B \\circ A\\| \\leq\\|B\\| \\cdot\\|A\\| .$\n",
        "\n",
        "* Daher wird der Raum $L(E)$ der stetigen, linearen Operatoren von $E$ in sich mit der Komposition als Multiplikation zu einer normierten Algebra, die bei vollst√§ndiaem $E$ sogar eine Banachalgebra ist.\n",
        "\n",
        "* Diese Algebren und ihre Unteralgebren nennt man Operatoralgebren, wobei der Fall, dass $E$ ein Hilbertraum ist, besonders intensiv untersucht wird."
      ],
      "metadata": {
        "id": "z1znuc7zMWf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Beschr√§nkter Operator*](https://de.wikipedia.org/wiki/Beschr√§nkter_Operator)\n",
        "\n",
        "* Im Gegensatz zu endlichdimensionalen R√§umen, wo lineare Operatoren stets beschr√§nkt sind, tauchen bei unendlichdimensionalen R√§umen auch unbeschr√§nkte lineare Operatoren auf.\n",
        "\n",
        "* Ein linearer Operator ist genau dann beschr√§nkt, wenn er stetig ist, also eine der\n",
        "folgenden √§quivalenten Bedingungen erf√ºlit:\n",
        "\n",
        "  * falls $x_{n} \\rightarrow x,$ so gilt $T x_{n} \\rightarrow T x$ in der von der jeweiligen Norm induzierten\n",
        "Metrik,\n",
        "\n",
        "  * f√ºr alle $x_{0} \\in X$ und alle $\\epsilon>0$ gibt es ein $\\delta>0$ mit\n",
        "$\n",
        "\\left\\|x-x_{0}\\right\\|<\\delta \\Rightarrow\\left\\|T x-T x_{0}\\right\\|<\\epsilon\n",
        "$\n",
        "\n",
        "  * Urbilder offener Mengen sind offen.\n",
        "\n",
        "* Beschr√§nkte lineare Operatoren werden deshalb oft als **stetige lineare Operatoren** bezeichnet. Wenn die Linearit√§t vorausgesetzt wird, spricht man h√§ufig auch nur von stetigen Operatoren oder beschr√§nkten Operatoren. **Ist der Bildraum der Skalarenk√∂rper, sagt man Funktional statt Operator.**"
      ],
      "metadata": {
        "id": "GdGpj-kGMYfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Unbeschr√§nkte lineare Operatoren auf Hilbertr√§umen*:\n",
        "\n",
        "* l√§sst man oft auch Operatoren zu, deren Definitionsbereich (Dom√§ne) lediglich ein Unterraum des betrachteten Raumes ist, so l√§sst man als Definitionsbereich auch einen Pr√§hilbertraum als Teilraum eines Hilbertraums zu, pr√§ziser spricht man dann von dicht definierten unbeschr√§nkten linearen Operatoren (s. u.). Der Operator wird als **partielle Abbildung** aufgefasst.\n",
        "\n",
        "* **Differential- und Multiplikationsoperatoren sind i. A. unbeschr√§nkt.** (Viele Anwendungsbeispiele werden durch unbeschr√§nkte Operatoren beschrieben (z.B. Differentialoperatoren). W√§hrend beschr√§nkte Operatoren auf dem ganzen Hilbertraum H definiert sind, werden unbeschr√§nkte Operatoren immer auf einem Definitionsbereich D(T ) ‚äÇ H betrachtet.)\n",
        "\n",
        "* Die Darstellung von Observablen der Quantenmechanik erfordert unbeschr√§nkte lineare Operatoren, da die den Observablen zugeordneten Operatoren i. A. unbeschr√§nkt sind.\n",
        "\n",
        "* Der [Laplace-Operator](https://de.wikipedia.org/wiki/Laplace-Operator) = [linearen Differentialoperator](https://de.m.wikipedia.org/wiki/Differentialoperator#Linearer_Differentialoperator) $\\Delta \\colon D(\\Delta )\\to L^{2}(\\mathbb{R} ^{n})$ ist ein unbeschr√§nkter Operator."
      ],
      "metadata": {
        "id": "JEZDMINdMaCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Selbstadjungierte & symmetrische Operatoren*\n",
        "\n",
        "* Sei $H$ ein Hilbertraum. Ein linearer Operator $T: D(T) \\rightarrow H$ hei√üt symmetrisch bzw. selbstadjungiert, falls\n",
        "\n",
        "> $\\langle T y, x\\rangle=\\langle y, T x\\rangle$\n",
        "\n",
        "* Der [selbstadjungierte Operator](https://de.wikipedia.org/wiki/Selbstadjungierter_Operator) ist eine Verallgemeinerung der selbstadjungierten Matrix. Siehe auch [Adjungierter_Operator](https://de.m.wikipedia.org/wiki/Adjungierter_Operator)\n",
        "\n",
        "* ein [symmetrischer Operator](https://de.wikipedia.org/wiki/Symmetrischer_Operator) ist ein **linearer Operator** und wird in der Funktionalanalysis im Kontext **unbeschr√§nkter Operatoren** betrachtet\n",
        "\n",
        "* ein **beschr√§nkter symmetrischer Operator ist ein selbstadjungierter Operator**.\n",
        "\n",
        "* **Unterschiede:**\n",
        "\n",
        "  * Symmetrische Operatoren koennen auch nicht-reelle Eigenwerte haben (im Gegensatz zu den selbstadjungierten Operatoren)\n",
        "\n",
        "  * Bei symmetrischen Operatoren wird nicht gefordert, dass der Operator $T$ dicht definiert sein muss (im Gegensatz zum selbstadjungierten Operator)\n",
        "\n",
        "  * Ist $T$ dicht definiert (und damit der adjungierte Operator wohl definiert), so ist $T$ genau dann symmetrisch, wenn $T\\subseteq T^{*}$ gilt.\n",
        "\n",
        "  * Nur fur selbstadjungierten Operatoren kann eine Spektralzerlegung gezeigt werden\n",
        "\n",
        "  * F√ºr beschr√§nkte Operatoren fallen die Begriffe selbstadjungiert und symmetrisch zusammen (daher sind symmetrische, nicht selbstadjungierte Operatoren immer unbeschr√§nkt)"
      ],
      "metadata": {
        "id": "0eCn9if2MbwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **In mathematics, a bilinear operator is a generalized \"multiplication\" which satisfies the distributive law.**\n",
        "\n",
        "[*Bilinearen Abbildungen*](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung) *verallgemeinern die verschiedensten Begriffe von Produkten (im Sinne einer Multiplikation). Die Bilinearit√§t entspricht dem Distributivgesetz $a \\cdot (b + c) = a \\cdot b + a \\cdot c$ bei der normalen Multiplikation.*\n",
        "\n",
        "* Jede bilineare Abbildung (Map) ist eine 2-lineare Abbildung. (S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "* **Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen**. Bei diesen ist der Wertebereich G mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch.)\n",
        "\n",
        "For a formal definition, given three vector spaces V, W and X over the same base field F, a bilinear operator is a function\n",
        "\n",
        "$B : V √ó W ‚Üí X$\n",
        "\n",
        "> [*A bilinear map*](https://en.m.wikipedia.org/wiki/Bilinear_map) *is a function combining elements of two vector spaces to yield an element of a third vector space, and is linear in each of its arguments. Matrix multiplication is an example as visible in $B : V √ó W ‚Üí X$. In contrast: linear map war eine matrix als map von vector zu vector, das hier ist eine matrix als map zw matrix und matrix!*\n",
        "\n",
        "such that for any w in W the map\n",
        "\n",
        "$v \\mapsto B(v, w)$\n",
        "\n",
        "is a linear operator from V to X, and for any v in V the map\n",
        "\n",
        "$w \\mapsto B(v, w)$\n",
        "\n",
        "is a linear operator from W to X. In other words, if we hold the first entry of the bilinear operator fixed, while letting the second entry vary, the result is a linear operator, and similarly if we hold the second entry fixed.\n",
        "\n",
        "* If V = W and we have B(v,w)=B(w,v) for all v,w in V, then we say that B is symmetric.\n",
        "\n",
        "> **The case where X is a field F for $B : V √ó W ‚Üí X$, and we have a bilinear form, is particularly useful (see for example scalar product, inner product and quadratic form).**\n",
        "\n",
        "* The definition works without any changes if instead of vector spaces we use modules over a commutative ring R. It also can be easily generalized to n-ary functions, where the proper term is multilinear.\n",
        "\n",
        "* For the case of a non-commutative base ring R and a right module MR and a left module RN, we can define a bilinear operator B : M √ó N ‚Üí T, where T is a commutative group, such that for any n in N, m |-> B(m, n) is a group homomorphism, and for any m in M, n |-> B(m, n) is a group homomorphism, and which also satisfies\n",
        "\n",
        "$B(mr, n) = B(m, rn)$\n",
        "\n",
        "for all m in M, n in N and r in R.\n",
        "\n",
        "https://academickids.com/encyclopedia/index.php/Bilinear_operator\n",
        "\n",
        "* Bilinear = gemischtes Assoziativgesetz & Distributivgesetz\n",
        "\n",
        "* [Bilineare Abbildungen](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung) verallgemeinern die verschiedensten Begriffe von Produkten (im Sinne einer Multiplikation).\n",
        "\n",
        "* Die Bilinearit√§t entspricht dem Distributivgesetz bei der normalen Multiplikation:\n",
        "\n",
        ">$\n",
        "a \\cdot(b+c)=a \\cdot b+a \\cdot c\n",
        "$\n",
        "\n",
        "* Beispiel:\n",
        "\n",
        "  * S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen),\n",
        "\n",
        "  * aber auch das Vektor- oder Kreuzprodukt,\n",
        "\n",
        "  * und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n"
      ],
      "metadata": {
        "id": "JOtdUCczMd0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear Map**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multilinear_map\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Multilineare_Abbildung\n",
        "\n",
        "> Multilinear Map - from Tensors to Tensor Spaces (Product Spaces & Vector Spaces ‚äó)\n",
        "\n",
        "> Riemann Curvature Tensor is a multilinear map. Wie findet man heraus, ob eine Surface flach oder gekruemmt ist?\n",
        "\n",
        "**Multilinear Maps: So the strongest kind of linearity we could reasonably impose is that $m$ is linear in each coordinate when all else is fixed**.\n",
        "\n",
        "https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Matrix_multiplication\n"
      ],
      "metadata": {
        "id": "LhNgU-EEMp8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product Spaces $\\otimes$**\n",
        "\n",
        "[Eigenchris: Tensors for Beginners 15: Tensor Product Spaces](https://www.youtube.com/watch?v=M-OLmxuLdbU&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=18)\n",
        "\n",
        "Im mathematischen Teilgebiet der linearen Algebra und verwandter Gebiete wird durch die [multilineare Abbildung](https://de.m.wikipedia.org/wiki/Multilineare_Abbildung) der Begriff der linearen Abbildung verallgemeinert. Ein wichtiges Beispiel einer multilinearen Abbildung ist die Determinante.\n",
        "\n",
        "In linear algebra, a [multilinear map](https://en.m.wikipedia.org/wiki/Multilinear_map) is a function of several variables that is linear separately in each variable. More precisely, a multilinear map is a function\n",
        "\n",
        "> $\n",
        "f: V_{1} \\times \\cdots \\times V_{n} \\rightarrow W\n",
        "$\n",
        "\n",
        "where $V_{1}, \\ldots, V_{n}$ and $W$ are vector spaces (or modules over a commutative\n",
        "ring), with the following property: for each $i$, if all of the variables but $v_{i}$ are held constant, then $f\\left(v_{1}, \\ldots v_{i}, \\ldots v_{n}\\right)$ is a linear function of $v_{i} .$\n",
        "\n",
        "A multilinear map of one variable is a linear map, and of two variables is a bilinear map. More generally, a multilinear map of k variables is called a k-linear map. **If the codomain of a multilinear map is the field of scalars, it is called a multilinear form**. Multilinear maps and multilinear forms are fundamental objects of study in multilinear algebra.\n",
        "\n",
        "\n",
        "For multilinear maps used in cryptography, see [Cryptographic multilinear map](https://en.m.wikipedia.org/wiki/Cryptographic_multilinear_map)."
      ],
      "metadata": {
        "id": "33s1HgP6M5rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product Rules**\n",
        "\n",
        "**Tensor Product Operation Rules - which gives us a (Tensor) Vector Space**\n",
        "\n",
        "Tensor product: Combining tensors that follow these scaling and adding rules:\n",
        "\n",
        "> $n(\\vec{v} \\alpha)=(n \\vec{v}) \\alpha=\\vec{v}(n \\alpha)$ (*Note the scaling rule: scale only with one side, not both at the same time!*)\n",
        "\n",
        "> $\\vec{v} \\alpha+\\vec{v} \\beta=\\vec{v}(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\alpha+\\vec{w} \\alpha=(\\vec{v}+\\vec{w}) \\alpha$\n",
        "\n",
        "And written in proper tensor notation:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_63.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_64.png)\n"
      ],
      "metadata": {
        "id": "mxTsLRy3NHv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tensor Products as Multilinear Maps*\n",
        "\n",
        "**What do all these functions have in common?**\n",
        "\n",
        "* If you make all inputs constant except one, we can scale the input before or scale the output after and we get the same result\n",
        "\n",
        "* And also if we replace these inout vector components with a sum of two sets of vector components I can just distribute these out and get this sum here (red line under equation in image), so basically I can add the inputs or I can add the outputs.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_73.png)\n",
        "\n",
        "> **Multilinear Map: A function that is linear when all inputs except one are held constant** (they are linear in each input variable)\n",
        "\n",
        "* when we scale the input variable by $n$ (and all other are held constant) that's the same as scaling the ouput of the function by $n$\n",
        "\n",
        "* when we hold all input constant except one, and we do a sum in the input slot, that's the same thing as doing the sum of the outputs (in image below)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_71.png)\n",
        "\n",
        "And all tensors are **multilinear maps** which means they are functions that take some number of inputs and they are linear in each input variable while all other input variables are held constant (kind of ceteris paribus in economics!).\n",
        "\n",
        "**Tensor Product $\\otimes$**\n",
        "\n",
        "* The outer product of tensors is also referred to as their [tensor product](https://en.m.wikipedia.org/wiki/Tensor_product), and can be used to define the tensor algebra.  https://en.m.wikipedia.org/wiki/Outer_product\n",
        "\n",
        "> The tensor product can be considered as a generalization and abstraction of the outer product. https://en.m.wikipedia.org/wiki/Tensor_product\n",
        "\n",
        "* a collection of tensor products attached to another space is called a [tensor bundle](https://en.m.wikipedia.org/wiki/Tensor_field)\n",
        "\n",
        "* Siehe auch: https://www.math3ma.com/blog/the-tensor-product-demystified"
      ],
      "metadata": {
        "id": "-OjmCCbxNnzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Forms (Functionals)*"
      ],
      "metadata": {
        "id": "MMxCx0fZKDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Form (Lineare Algebra) - 1-Form**:\n",
        "\n",
        "* they take one vector as input to output a number (scalar):\n",
        "\n",
        "> $\\mathcal{B}: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * A [linear form](https://de.m.wikipedia.org/wiki/Linearform) is a function that takes one or more vectors as input and outputs a number (= lineare Abbildung von einem Vektorraum in den zugrundeliegenden K√∂rper)\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * Algebraische Regeln: Es sei $K$ ein K√∂rper und $V$ ein $K$ -Vektorraum. Eine Abbildung $f: V \\rightarrow K$ hei√üt Linearform, wenn f√ºr alle Vektoren $x, y \\in V$ und Skalare $\\alpha \\in K$ gilt:\n",
        "\n",
        "    * $f(x+y)=f(x)+f(y)$ (**Additivit√§t**);\n",
        "\n",
        "    * $f(\\alpha x)=\\alpha f(x)$ (**Homogenit√§t**).\n",
        "\n",
        "  * Linear Form = 1-Form = Functional (incl Distribution) = Covector = Dualvector = Differentialformen\n",
        "\n",
        "> **A column vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "x \\\\\n",
        "y\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>vector</u>.** i.e. Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state'\n",
        "\n",
        "> **A row vector $\\begin{equation}\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>covector (linear form, functional)</u>.** i.e. Bra $[b_{0} \\quad b_{1}]$\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti‚Äìisomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra‚Äìket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© = 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "THMYOFHeKF__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector (Tensor Algebra)**: in der Physik verwendet man gerne die Sprache der Tensoralgebra:\n",
        "\n",
        "  * dann hei√üen die Elemente von $V$ **kontravariante Vektoren** und die von $V^{*}$ heissen **kovariante Vektoren oder auch [Kovektoren](https://en.m.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors)**\n",
        "\n",
        "  * Covectors are functions from vectors to real numbers. They take one vector and output a scalar: $\\alpha: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * This scalar can be a integrand (Wegintegral) or Differential, Temperature, Speed etc.(meanwhile a metric tensor is two-form because it takes 2 vectors to output one scalar, which is length or angle).\n",
        "\n",
        "  * They follow the linearity properties: $\\alpha(\\vec{v}+\\vec{w})=\\alpha(\\vec{v})+\\alpha(\\vec{w})$ and $\\alpha(n \\vec{v})=n \\alpha(\\vec{v})$\n",
        "\n",
        "  * Eine Linearform $f$ ist ein kovarianter Tensor erster Stufe; man nennt sie deshalb manchmal auch [1-Form (Pfaffsche Form)](https://de.m.wikipedia.org/wiki/Pfaffsche_Form). <font color=\"red\">Linear forms are (0,1) tensors</font> (so they transform using 1 covariant rule when we change coordinate systems)\n",
        "\n",
        "  * Die Abbildung $V \\times V^{*} \\rightarrow K,(x, f) \\mapsto\\langle x, f\\rangle:=f(x)$ ist eine nicht ausgeartete Bilinearform und hei√üt duale Paarung.\n",
        "\n",
        "  * All covectors can be written as the linear combination of the dual basis vectors!\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 4: What are Covectors?](https://www.youtube.com/watch?v=LNoQ_Q5JQMY)"
      ],
      "metadata": {
        "id": "4HJ0C217KH3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funktional (Functional Analysis)**:\n",
        "\n",
        "* Im Kontext der Funktionalanalysis, im topologischen $\\mathbb {R}$ - oder $\\mathbb {C}$-Vektorraums, sind die betrachteten Linearformen meistens [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale). Aber: 'Linear form' is a more modern and abstract concept of 'functional'\n",
        "\n",
        "* Sei $V$ ein $\\mathbb{K}$ -Vektorraum mit $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\} .$ Ein Funktional $T$ ist eine Abbildung $T: V \\rightarrow \\mathbb{K} .$\n",
        "\n",
        "* **Function:**\n",
        "$\\mathbb{R} \\rightarrow f(x) \\rightarrow \\mathbb{R}$. **Functional:**\n",
        "$f(x) \\rightarrow J[f(x)] \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* A typical example of a linear functional is integration [Source](https://en.m.wikipedia.org/wiki/Linear_form)\n",
        "\n",
        "* In Funktionalanalyse ist der untersuchte Vektorraum $V$ zumeist ein Funktionenraum, wo diesen durch Funktionale Skalare zugeordnet werden. Beispiel: [Lebesgue-Integral](https://de.m.wikipedia.org/wiki/Lebesgue-Integral).\n",
        "\n",
        "* Eine Funktion bildet Elemente eines K√∂rpers auf andere ab, zum Beispiel. $f: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$. Ein Funktional hingegen ist ein Operator, der Elemente eines Vektorraumes $V$ auf den [Skalark√∂rper](https://de.wikipedia.org/wiki/Skalar_(Mathematik)) $K$ abbildet, √ºber dem $V$ modelliert ist. Diese Elemente k√∂nnen zum Beispiel selber Funktionen sein (wenn der Vektorraum ein Funktionenraum ist)\n",
        "\n",
        "* Siehe auch [Funktionaldeterminante](https://de.wikipedia.org/wiki/Funktionaldeterminante) oder Jacobi-Determinante fur Koordinatentransformationen zB von kartesisches zu Polarkoordinaten in der mehrdimensionalen Integralrechnung, also der **Berechnung von Oberfl√§chen- und Volumenintegralen**  an.\n",
        "\n",
        "* Siehe auch [Functional integration](https://en.m.wikipedia.org/wiki/Functional_integration): Richard Feynman used functional integrals as the central idea in his sum over the histories formulation of quantum mechanics. This usage implies an integral taken over some function space."
      ],
      "metadata": {
        "id": "AdSJOOjuKJwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dualraum (Lineare Algebra)**:\n",
        "\n",
        "* https://www.quora.com/What-is-a-dual-vector-and-what-does-it-do\n",
        "\n",
        "* Die Menge aller Linearformen (= stetigen, linearen Abbildungen) √ºber einem gegebenen Vektorraum $V$ bildet dessen [Dualraum](https://de.wikipedia.org/wiki/Dualraum) $V^{*}$ und damit selbst wieder in nat√ºrlicher Weise einen $K$ -Vektorraum.\n",
        "\n",
        "* Zu einem Vektorraum $V$ √ºber einem K√∂rper $K$ bezeichnet $V^{*}$ den zu $V$ geh√∂rigen [Dualraum](https://de.m.wikipedia.org/wiki/Dualraum), das hei√üt die Menge aller linearen Abbildungen von $V$ nach $K$.\n",
        "\n",
        "* Die Menge aller Funktionale ist wiederum in nat√ºrlicher Form ein Vektorraum uber dem gleichen K√∂rper $\\mathbb{K}$, indem man f√ºr zwei Funktionale $f$ und $g$ √ºber $V$ die Addition und Skalarmultiplikation punktweise definiert, $d .$ h. $\n",
        "(f+g)(x):=f(x)+g(x) \\quad(\\lambda f)(x):=\\lambda(f(x)), x \\in V\n",
        "$. Der Vektorraum der linearen Funktionale auf dem Vektorraum $V$ wird der algebraische **Dualraum** genannt und oft mit $V^{*}$ bezeichnet.\n",
        "\n",
        "* Kovektoren als Linearformen von [Normalenvektoren](https://de.wikipedia.org/wiki/Normalenvektor) (das Ergebnis ist eine Zahl), ist sowas wie ein Zeilenvektor. Zeilenvektor (Kovektor, Linearform) c * Spaltenvektor (Normalenvektor) a = eine Zahl, zB 2\n",
        "\n",
        "* Aus Basis fur den Vektorraum V {e<sub>1</sub>, e<sub>2</sub>} kann man nun √ºberlegen, wie man daraus eine Basis fur den Dualraum V* bekommt {e<sup>1</sup>, e<sup>2</sup>}. Die Dualbasis soll jetzt einen Originalvektor aus V nehmen und eine reelle Zahl als Ergebnis liefern. Also zB fur Vektor im Originalvektorraum mit den Koordinaten von der Basis: a = 1,3 e<sub>1</sub> + 1,2 e<sub>2</sub> , dann sind 1,3 = e<sup>1</sup> und 1,2 = e<sup>2</sup>\n",
        "\n",
        "* Dualraum: https://youtu.be/2vvjrBbcTZU, [Video: What are Dual vectors](https://www.youtube.com/watch?v=T04Yq1D20AM)\n",
        "\n",
        "* [Video: An introduction to vectors and dual vectors](https://youtu.be/2MC4xMhscjQ)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_17.png)"
      ],
      "metadata": {
        "id": "Q3I9O2SVKLjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution (Partial Differential Equations) - Special Functional**:\n",
        "\n",
        "* Eine [Distribution](https://en.m.wikipedia.org/wiki/Distribution_(mathematics)) (english) bzw. [Distribution (deutsch)](https://de.m.wikipedia.org/wiki/Distribution_(Mathematik)) bzw. [mathepedia](https://mathepedia.de/Distributionen.html) bezeichnet eine **besondere Art eines Funktionals** (eine Verallgemeinerung)\n",
        "\n",
        "* But: **it allows only linear operations. Distributions cannot be multiplied** (except for very special cases). For example it is not meaningful to square the Dirac delta function.\n",
        "\n",
        "* Es gibt partielle Differentialgleichungen, die keine hinreichend oft differenzierbare oder gar keine klassischen L√∂sungen haben, aber L√∂sungen im distributionellen Sinn. Beispiel: 'Knicke' in Differentialgleichungen (zB Ableitung der [Heaviside Funktion](https://de.wikipedia.org/wiki/Heaviside-Funktion))\n",
        "\n",
        "* Nutzung der [Delta Funktion (Delta Distribution)](https://de.wikipedia.org/wiki/Delta-Distribution). Delta Funktion ist Null ausserhalb der kritischen Stelle Null. Aber: Delta ist keine Funktion im ublichen Sinn. Delta ist Null fast uberall (bez. des Lebesgue Masses). Das heisst jegliches Integral ist Null. Loesung: Delta wird als 'Distribution' definiert (oder auch: 'verallgemeinerte Funktion'). Funktionen fasst man jetzt als eine Dichte auf (zB Massedichte auf einem eindimensionalen Stab). Delta-Funktion ist dann eine singulare Dichte (Punktmasse). https://www.youtube.com/watch?v=c5WYrQK_7ls. **Delta-Distribution** ([Dirac-Funktion](https://de.m.wikipedia.org/wiki/Delta-Distribution)) ist eine spezielle irregul√§re Distribution mit kompaktem Tr√§ger. Sie hat in der Mathematik und Physik grundlegende Bedeutung. Ihr √ºbliches Formelsymbol ist Œ¥ (kleines Delta).\n",
        "\n",
        "* [Testfunktionen](https://de.m.wikipedia.org/wiki/Testfunktion):\n",
        "Formulier problem in variationell (sobolove r√§ume), und dann Eigenschaften von Testfunktionen ausnutzen. Als Testfunktionen bezeichnet man gewisse Typen von Funktionen, die in der Distributionentheorie eine wesentliche Rolle spielen. √úblicherweise **fasst man Testfunktionen eines bestimmten Typs zu einem Vektorraum** zusammen. **Die zugeh√∂rigen Distributionen sind dann lineare Funktionale auf diesen Vektorr√§umen**. Ihr Name r√ºhrt daher, dass man die Distributionen (im Sinne linearer Abbildungen) auf die Testfunktionen anwendet und dadurch testet. In der mathematischen Literatur werden h√§ufig der Raum der glatten Funktionen mit kompaktem Tr√§ger oder der [Schwarz Raum](https://de.m.wikipedia.org/wiki/Schwartz-Raum) als Testfunktionenraum bezeichnet.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3sfnX5WeKNY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bilinear Forms (=2-Forms)** takes two vectors as input to output a number (scalar): $k=2$,  $f:V\\times V\\to K$\n",
        "\n",
        "> $\\mathcal{B}: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* Algebraic rules - linearity properties:\n",
        "\n",
        "  * $a \\mathcal{B}(\\vec{v}, \\vec{w})=\\mathcal{B}(a \\vec{v}, \\vec{w})=\\mathcal{B}(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}+\\vec{u}, \\vec{w})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}, \\vec{w}+\\vec{t})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{v}, \\vec{t})$\n",
        "\n",
        "* [Bilinear Forms](https://en.m.wikipedia.org/wiki/Bilinear_form) sind ein Spezialfall der bilinearen Abbildungen (Wertebereich G ist mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch). Winkel sind wichtiger Anwendungsfall dafur: man kann Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht.\n",
        "\n",
        "  * A familiar and important example of a (symmetric) bilinear form is the standard [inner product (dot product)](https://en.m.wikipedia.org/wiki/Dot_product) of vectors. Jedes Skalarprodukt ist wiederum eine spezielle Bilinearform (es gelten noch weitere Eigenschaften: symmetrisch <v,w> = <w,v>, und positiv definit). Genauso Integral.\n",
        "\n",
        "  * [Bilinearform](https://de.wikipedia.org/wiki/Bilinearform), Bilinearform: cross product of two vectors, normal and tangent, see [Frenet‚ÄìSerret_formulas](https://en.m.wikipedia.org/wiki/Frenet‚ÄìSerret_formulas).\n",
        "\n",
        "* Die [Sesquilinearform](https://en.m.wikipedia.org/wiki/Sesquilinear_form) ist eine Generalizations der Bilinear Form auf den Koerper der komplexen Zahlen\n",
        "\n",
        "* Der [Metric Tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) ist ein Spezialfall einer Bilinear Form. The metric tensor has 2 additional properties that other bilinear forms might not have: components are symmetric, output must be positive:\n",
        "\n",
        "  * Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  * Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Just like the metric tensor <font color=\"red\">bilinear forms are (0,2) tensors</font> (so they transform using 2 covariant rules when we change coordinate systems):\n",
        "\n",
        "  * $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "  * $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 10: Bilinear Forms](https://www.youtube.com/watch?v=jLiBCaBEB3o&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14) und das Video [Bilinearform einfach erkl√§rt :) | Math Intuition](https://www.youtube.com/watch?v=TjAFH6hWg1I)"
      ],
      "metadata": {
        "id": "J9EJ2VfeKO--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between linear and bilinear forms**\n",
        "\n",
        "* If look at the bilinear form rules and focus only on the rules that involve the first input and pretend the second input $\\vec{w}$ is fixed, $\\mathcal{B}$ looks very much like a covector or a linear form (scale input linearly and addition is linear):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_33.png)\n",
        "\n",
        "* Alternatively if we focus only on the rules that involve the second input and pretend the first input is fixed $\\vec{v}$ also looks like a covector or linear form:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_34.png)\n",
        "\n",
        "**So if we say that $\\mathcal{B}$ is a bilinear form, it's because it's a form where each individual input is linear while the other input is held constant!**"
      ],
      "metadata": {
        "id": "PleWkdx8KQx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) is a function that takes vectors as inputs and outputs a number:**\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) on a vector space $V$ over a field $K$ is a map\n",
        "$f: V^{k} \\rightarrow K$\n",
        "that is separately $K$ -linear in each of its $k$ arguments.\n",
        "\n",
        "* A multilinear $k$ -form on $V$ over $\\mathbf{R}$ is called a (**covariant**) **$k$ -tensor**, and the vector space is usually denoted $\\mathcal{T}^{k}(V)$ or $\\mathcal{L}^{k}(V) \\cdot$\n",
        "\n",
        "* <font color=\"red\">Multilinear forms are (0,k) tensors</font> (so they transform using k covariant rules when we change coordinate systems) (??)\n",
        "\n",
        "* Kovariante Tensoren (Covectors) sind Multilinearformen [Source](https://de.m.wikipedia.org/wiki/Multilinearform)\n",
        "\n",
        "* Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "* https://unapologetic.wordpress.com/2009/10/22/multilinear-functionals/\n",
        "\n",
        "* Understanding the definition of tensors as multilinear maps: https://math.stackexchange.com/questions/2138459/understanding-the-definition-of-tensors-as-multilinear-maps\n"
      ],
      "metadata": {
        "id": "5uK_H84HKSuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Differential Operator*"
      ],
      "metadata": {
        "id": "LUxoBU9TKYpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Differential operator](https://en.m.wikipedia.org/wiki/Differential_operator) is an operator defined as a function of the differentiation operator. Consider differentiation as an abstract operation that **accepts a function and returns another function**.\n",
        "\n",
        "*Differential- & Nabla-Operator*\n",
        "\n",
        "Der [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, **um kontextabh√§ngig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren**. Es ist zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei Differentialoperatoren [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) (Anwendung im [Gradientenverfahren](https://de.wikipedia.org/wiki/Gradientenverfahren) in der Numerik), [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes) oder [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes)\n",
        "\n",
        "https://www.youtube.com/watch?v=YW-bUVIOpB0&t=51s\n",
        "\n",
        "* **Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "  * [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "* **Diese drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei r√§umlicher Drehung des urspr√ºnglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden. Diese Eigenschaft folgt aus den **koordinatenunabh√§ngigen** Definitionen.\n",
        "\n",
        "https://www.youtube.com/watch?v=rB83DpBJQsE\n",
        "\n",
        "*Integraloperator*\n",
        "\n",
        "* **Integral**: Der [**Volterraoperator**](https://de.wikipedia.org/wiki/Integraloperator#Volterraoperator) $\\int_{0}^{t}$ zur Bildung des [bestimmten Integrals](https://de.wikipedia.org/wiki/Integralrechnung) (ist ein Beispiel fur einen [Integraloperator](https://de.wikipedia.org/wiki/Integraloperator). Operatoren wie diese, die einer Funktion eine Zahl zuordnen, nennt man [Funktional](https://de.wikipedia.org/wiki/Funktional).\n",
        "\n",
        "  * Die Fourier-Transformation ${\\mathcal {F}}$ ist z.B. ein **linearer Operator** (siehe unten).\n",
        "\n",
        "* Ein [linearer Integraloperator](https://de.wikipedia.org/wiki/Integraloperator) ist ein mathematisches Objekt aus der Funktionalanalysis. Dieses Objekt ist ein linearer Operator, der mit einer bestimmten Integralschreibweise mit einem Integralkern dargestellt werden kann."
      ],
      "metadata": {
        "id": "Y0ofnEZLKsn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Differential Form*"
      ],
      "metadata": {
        "id": "vD3xBKSIK3Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**√Ñu√üere Ableitung** (exterior derivative)\n",
        "\n",
        "* Die [√§u√üere Ableitung](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) ist ein Operator, der einer k-Differentialform eine $(k+1)$-Differentialform zuordnet.\n",
        "\n",
        "* Betrachtet man sie auf der Menge der $0$-Differentialformen, also auf der Menge der glatten Funktionen, so entspricht die √§u√üere Ableitung der √ºblichen Ableitung f√ºr Funktionen.\n",
        "\n",
        "* Die √§u√üere Ableitung $\\mathrm{d} \\omega$ einer $k$ -Form $\\omega$ wird induktiv mithilfe der [Lie-Ableitung](https://de.m.wikipedia.org/wiki/Lie-Ableitung) (=die Ableitung eines Vektorfeldes oder allgemeiner eines Tensorfeldes entlang eines Vektorfeldes. ) und der Cartan-Formel\n",
        "\n",
        "> $\n",
        "\\mathcal{L}_{X}=i_{X} \\circ \\mathrm{d}+\\mathrm{d} \\circ i_{X}\n",
        "$\n",
        "\n",
        "* definiert; dabei ist $X$ ein Vektorfeld, $\\mathcal{L}_{X}$ die Lie-Ableitung und $i_{X}$ die Einsetzung von $X$."
      ],
      "metadata": {
        "id": "hwqLWAHcK7af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "uDFxcwyecPgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connection between Differential Forms and Differential Operators (grad, div, curl):**\n",
        "\n",
        "  * By taking the [exterior derivative](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) of 0-forms, 1-forms and 2-forms we can express the three important operators of vector calculus - grad, curl and div - in the language of differential forms. The gradient of a sclara field f(x,y,z) is given by:\n",
        "\n",
        "  * $\\operatorname{grad} f=\\nabla f=\\frac{\\partial f}{\\partial x} \\hat{e}_{x}+\\frac{\\partial f}{\\partial y} \\hat{e}_{y}+\\frac{\\partial f}{\\partial z} \\hat{\\mathrm{e}}_{z}$\n",
        "\n",
        "  * Using the correspondance:\n",
        "\n",
        "  * $d x \\Leftrightarrow \\hat{\\mathbf{e}}_{x}, d y \\Leftrightarrow \\hat{\\mathbf{e}}_{y}, d z \\Leftrightarrow \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "  * this gradient vector field can be associated with the exterior derivative of the 0-form f(x,y,z), which is a 1-form\n",
        "\n",
        "  * $d f=\\frac{\\partial f}{\\partial x} d x+\\frac{\\partial f}{\\partial y} d y+\\frac{\\partial f}{\\partial z} d z$\n",
        "\n",
        "  * Grad operator: $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$"
      ],
      "metadata": {
        "id": "QmtG8d3bK9Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form (Differential Geometry)**:\n",
        "\n",
        "* **[Differential forms](https://en.m.wikipedia.org/wiki/Differential_form) are linear or, more generally, multilinear alternating functions of tangent vectors**. Eine Differentialform ordnet einem Punkt einer Mannigfaltigkeit eine **alternierende Multilinearform** auf dem zugeh√∂rigen Tangentialraum zu. Differentialformen sind dabei das bekannteste Beispiel von Schnitten.\n",
        "\n",
        "* The differential forms form an [alternating algebra](https://en.m.wikipedia.org/wiki/Alternating_algebra). This implies that:\n",
        "\n",
        "  > $dy\\wedge dx=-dx\\wedge dy$\n",
        "\n",
        "  > $dx\\wedge dx=0.$\n",
        "\n",
        "* This alternating property reflects the orientation of the domain of integration.\n",
        "\n",
        "* Differential forms provide a unified approach to define integrands over curves, surfaces, solids, and higher-dimensional manifolds. ([Differentialformen](https://de.m.wikipedia.org/wiki/Differentialform) erlauben eine koordinatenunabh√§ngige Integration auf allgemeinen orientierten differenzierbaren Mannigfaltigkeiten.)\n",
        "\n",
        "* **Differential forms are a special class of tensors where you can find derivatives even in the absence of connections or metrics**. You can think of differential forms as a generalization of single variable calculus that works on manifolds, as well as curves, solids, and surfaces.\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) ist die Algebra der Differentialformen\n",
        "\n",
        "* Simple Example of a Differential Form: in the expression $\\int_{0}^{1}$ <font color=\"blue\">$ x^{2} d x$</font> the term $x^{2}$ is the **integrand** and <font color=\"blue\">$ x^{2} d x$</font> is the **differential form** [(Source)](https://www.calculushowto.com/differential-operator/). So for identification, differential forms can be generally recognised as things containing differentials such as $d x, d y$ and $d z$.\n",
        "\n",
        "  * [Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes) (Vektoranalysis): sehr grundlegenden Satz √ºber die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "  * [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) (sowie Koordinatentransformationen und Funktionaldeterminante in der Vektoranalysis): Aus mathematischer Sicht ist eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird Riemann'sche Volumenform genannt (**Hodge-Stern-Operator in der Differentialgeometrie**).\n",
        "\n",
        "* Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind. Covectors (functional): Differential Forms = Covector Fields. Video Eigenchris: [Differential Forms and Covectors](https://youtu.be/XGL-vpk-8dU)\n"
      ],
      "metadata": {
        "id": "dl0fYX1-K_B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)"
      ],
      "metadata": {
        "id": "HyzwIvWTLA31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Types of Differential Forms where the $\\wedge$ symbol denotes a type of multiplication called the wedge product (exterior product)**:\n",
        "\n",
        "* **0-form = Function**:\n",
        "\n",
        "  * is a special case and is simply a smooth function. A 0-forms eats a point and returns a number\n",
        "\n",
        "  * [Glatte Funktionen](https://de.m.wikipedia.org/wiki/Glatte_Funktion) sind 0-Formen. Is a skalar from a function.\n",
        "\n",
        "* **1-form = Linear Form** (Pfaffsche Formen). Eats a vector and returns a number. Is a Covector.\n",
        "\n",
        "  * [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind 1-Formen (= zB Wegintegral). [1-Formen](https://en.m.wikipedia.org/wiki/One-form) bilden die Grundlage f√ºr die Einf√ºhrung von Differentialformen.\n",
        "\n",
        "  * [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind die nat√ºrlichen Integranden f√ºr Wegintegrale. Kovektorfeld oder kurz 1-Form ein Objekt, das in gewisser Weise dual zu einem Vektorfeld ist.\n",
        "\n",
        "  * Es ist eine Differentialform vom Grad 1. A one-form on a differentiable manifold is a smooth section of the cotangent bundle (= differential form).\n",
        "\n",
        "  * One Form and Line Integral: the expression $f(x) d x$ from one-variable calculus is an example of a [$l-$ form](https://en.m.wikipedia.org/wiki/One-form) (Pfaff'sche Form), and can be integrated over an oriented interval $[a, b]$ in the domain of $f: \\int_{a}^{b} f(x) d x$. See Video: [Line Integrals in Differential Forms](https://youtu.be/dhZgGIYzPUU).\n",
        "\n",
        "  * [Video: How to visualise a one-form](https://www.youtube.com/watch?v=dxz9JZPewu8): family of surfaces that it pierces. Integrating a one-form, we are just counting the number of surfaces that the line actually passes through\n",
        "\n",
        "  * Each term contains one differential: $\\begin{array}{a}\n",
        "\\omega=2 x d x+3 y d y-d z, \\\\\n",
        "\\omega=x d y, \\\\\n",
        "\\omega=d x .\n",
        "\\end{array}$\n",
        "\n",
        "* **2-form = Bilinear Form (Surface Integral)**: eats two vectors and returns a number:\n",
        "\n",
        "  * $\\omega=8 d y \\wedge d z$\n",
        "\n",
        "  * Similarly, the expression $f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z$ is a 2 -form that has a [surface integral](https://en.m.wikipedia.org/wiki/Surface_integral) over an oriented surface $S$ :\n",
        "\n",
        "  * $\\int_{S}(f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z) .$\n",
        "\n",
        "  * The symbol $\\wedge$ denotes the exterior product, sometimes called the wedge product, of two differential forms.\n",
        "\n",
        "* **3 -form = Trilinear Form (Volume Integral)**: eats three vectors and returns a number:\n",
        "\n",
        "  * $\\omega=-7 z d x \\wedge d y \\wedge d z$\n",
        "\n",
        "  * Likewise, a $3-$ form $f(x, y, z) d x \\wedge d y \\wedge d z$ represents a [volume element](https://en.m.wikipedia.org/wiki/Volume_element) that can be integrated over an oriented region of space.\n",
        "\n",
        "* **$k$-form = Multilinear Form**: eats $k$-vectors and returns a number: $\\omega= .. $\n",
        "\n",
        "  * In general, a $k$-form is an object that may be integrated over a $k$-dimensional oriented manifold, and is homogeneous of degree $k$ in the coordinate differentials.\n",
        "\n",
        "  * On an $n$-dimensional manifold, the top-dimensional form $(n$-form $)$ is called a [volume form](https://en.m.wikipedia.org/wiki/Volume_form)."
      ],
      "metadata": {
        "id": "gtnhb-2MLClh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Siehe auch [Level Sets](https://en.wikipedia.org/wiki/Level_set) bzw. Niveaumenge** (= die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist.)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)"
      ],
      "metadata": {
        "id": "NwHEgDTLLEiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)"
      ],
      "metadata": {
        "id": "BjDe-v7oLGGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier mit Niveaumenge (level sets):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)"
      ],
      "metadata": {
        "id": "TCjQDbgwLKl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: If we think of x as a scalar field, it would look like this: it‚Äôs a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [**Level Set (Niveaumenge)**](https://de.wikipedia.org/wiki/Niveaumenge). Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)"
      ],
      "metadata": {
        "id": "CptmeMH_LVXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)"
      ],
      "metadata": {
        "id": "unJXl_UfLXfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing One-Forms:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_87.png)"
      ],
      "metadata": {
        "id": "vAo_tYsKLaHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_88.png)"
      ],
      "metadata": {
        "id": "08oxaMctLcFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition der Differentialform (Tangent Spaces)**\n",
        "\n",
        "Es sei $U$\n",
        "- eine offene Teilmenge des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Untermannigfaltigkeit des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Mannigfaltigkeit.\n",
        "\n",
        "In jedem dieser F√§lle gibt es:\n",
        "- den Begriff der differenzierbaren Funktion auf $U$; der Raum der beliebig oft differenzierbaren Funktionen auf $U$ werde mit $C^{\\infty}(U)$ bezeichnet;\n",
        "- den Begriff des Tangentialraums $\\mathrm{T}_{p} U$ an $U$ in einem Punkt $p \\in U$;\n",
        "- den Begriff der Richtungsableitung $\\frac{\\partial f}{\\partial X}$ f√ºr einen Tangentialvektor $X \\in \\mathrm{T}_{p} U$ und eine differenzierbare Funktion $f$;\n",
        "- den Begriff des differenzierbaren Vektorfeldes auf $U$; der Raum der Vektorfelder auf $U$ sei mit $\\Gamma(\\mathrm{T} U)$ bezeichnet.\n",
        "- Der Dualraum des Tangentialraums $\\mathrm{T}_{p} U$ wird als Kotangentialraum $\\mathrm{T}_{p}^{*} U$\n",
        "bezeichnet.\n",
        "\n",
        "**Definition: Eine Differentialform vom Grad $k$ auf $U$ oder kurz $k$ -Form $\\omega$ ist ein glatter Schnitt in der $k$ -ten √§u√üeren Potenz des Kotangentialb√ºndels von $U$**.\n",
        "\n",
        "* In symbolischer Schreibweise bedeutet dies $\\omega \\in \\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$, wobei $T^{*} U$ das Kotangentialb√ºndel von $U, \\Lambda^{k}\\left(T^{*} U\\right)$ die $k$ -te √§u√üere Potenz von $T^{*} U$ und $\\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$ somit die Menge der glatten Schnitte von $\\Lambda^{k}\\left(T^{*} U\\right)$ bezeichnet.\n",
        "\n",
        "* Dies bedeutet, **dass jedem Punkt $p \\in U$ eine alternierende Multilinearform $\\omega_{p}$ auf dem Tangentialraum $T_{p} U$ zugeordnet wird**; und zwar so, dass f√ºr $k$ glatte Vektorfelder $X_{1}, \\ldots, X_{k}$ die folgende Funktion glatt, also beliebig oft differenzierbar, ist:\n",
        "\n",
        "> $p \\mapsto \\omega_{p}\\left(\\left(X_{1}\\right)_{p}, \\ldots,\\left(X_{k}\\right)_{p}\\right) \\in \\mathbb{R}$\n",
        "\n",
        "* **Alternativ dazu kann man eine $k$ -Form $\\omega$ als eine alternierende, glatte multilineare Abbildung $\\omega:(\\Gamma T U)^{k} \\rightarrow C^{\\infty}(U)$ auffassen**.\n",
        "\n",
        "* Das bedeutet: $\\omega$ ordnet $k$ Vektorfeldern $X_{1}, \\ldots, X_{k}$ eine Funktion $\\omega\\left(X_{1}, \\ldots, X_{k}\\right)$ zu, sodass\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}+X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)=\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}, \\ldots, X_{k}\\right)+\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)$\n",
        "\n",
        "* $\\omega\\left(X_{1}, \\ldots, f \\cdot X_{i}, \\ldots, X_{k}\\right)=f \\cdot \\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$ f√ºr $f \\in C^{\\infty}(U), 1 \\leq i \\leq k$\n",
        "\n",
        "* und\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{j}, \\ldots, X_{k}\\right)=-\\omega\\left(X_{1}, \\ldots, X_{j}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$\n",
        "gilt.\n",
        "\n",
        "* Alternative unter R√ºckgriff auf Tensorfelder: **Eine $k$ -Form ist ein alternierendes, kovariantes Tensorfeld der Stufe $k$.**"
      ],
      "metadata": {
        "id": "-gGtBekDLeBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raum der Differentialformen**\n",
        "\n",
        "* Die Menge der $k$ -Formen auf $U$ bildet einen Vektorraum und wird mit $\\Omega^{k}(U)$ bezeichnet. Weiterhin setzt man\n",
        "\n",
        "> $\n",
        "\\Omega(U)=\\bigoplus_{k=1}^{\\infty} \\Omega^{k}(U) .\n",
        "$\n",
        "\n",
        "* F√ºr endlichdimensionale Mannigfaltigkeiten ist diese Summe endlich, da f√ºr $k>\\operatorname{dim} U$ der Vektorraum $\\Omega^{k}(U)$ der Nullvektorraum ist. Die Menge $\\Omega(U)$ ist eine Algebra mit dem √§u√üeren Produkt als Multiplikation und somit auch wieder ein Vektorraum. Aus topologischer Sicht ist dieser Raum auch eine [Garbe](https://de.m.wikipedia.org/wiki/Garbe_(Mathematik)).\n",
        "\n",
        "* Man kann $\\omega_{p}$ als Element der √§u√üeren Potenz $\\Lambda^{k}\\left(T_{p}^{*} U\\right)$ auffassen; infolgedessen definiert das √§u√üere Produkt (d. h. das Produkt $\\wedge$ in der √§u√üeren Algebra) Abbildungen\n",
        "\n",
        "> $\n",
        "\\Omega^{k}(U) \\times \\Omega^{\\ell}(U) \\rightarrow \\Omega^{k+\\ell}(U), \\quad(\\omega, \\eta) \\mapsto \\omega \\wedge \\eta\n",
        "$\n",
        "\n",
        "* wobei $\\omega \\wedge \\eta$ durch\n",
        "\n",
        "> $\n",
        "(\\omega \\wedge \\eta)_{p}=\\omega_{p} \\wedge \\eta_{p}\n",
        "$\n",
        "\n",
        "* punktweise definiert ist. Dieses Produkt ist graduiert-kommutativ, es gilt\n",
        "\n",
        "> $\n",
        "\\omega \\wedge \\eta=(-1)^{\\operatorname{deg} \\omega \\cdot \\operatorname{deg} \\eta} \\cdot \\eta \\wedge \\omega ;\n",
        "$\n",
        "\n",
        "* dabei bezeichnet $\\operatorname{deg} \\omega$ den Grad von $\\omega, d$. h.: Ist $\\omega$ eine $k$ -Form, so ist $\\operatorname{deg} \\omega=k$. Demnach ist das Produkt zweier Formen ungeraden Grades antikommutativ und in allen anderen Kombinationen kommutativ."
      ],
      "metadata": {
        "id": "S70lQDJOLgKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form <u>Field</u> (on the manifold)**\n",
        "\n",
        "* A vector field on a manifold gives a vector (actually, a tangent vector) at every point. So, for example, say a point $P$ on $M$, a region of $\\mathbb{R}^{3}$, has coordinates $x=3, y=5, z=1$. At $P$ the vector field\n",
        "\n",
        "> $\\mathbf{v}=7 x \\hat{\\mathbf{e}}_{x}+4 y \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "* gives the (tangent) vector\n",
        "\n",
        "> $\\begin{array}{c}\\mathbf{v}=(7 \\times 3) \\hat{\\mathbf{e}}_{x}+(4 \\times 5) \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z} \\\\\\mathbf{v}=21 \\hat{\\mathbf{e}}_{x}+20 \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z}\\end{array}$\n",
        "\n",
        "* Similarly, a differential form on a manifold can be thought of as a differential form field, that gives a particular differential form at every point. For example, at the same point $P(3,5,1)$ on $M$ the above 1-form field, $\\omega=2 x d x+3 y d y-d z$, gives the particular 1-form\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\omega=(2 \\times 3) d x+(3 \\times 5) d y-d z \\\\\n",
        "\\omega=6 d x+15 d y-d z\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "* If the tangent vector and 1-form are associated with the same point, they can act on each other to give a number.\n",
        "\n",
        "* Following widely accepted practice, we won't use the term 'field', as in 'differential form field'. Instead, **we'll let 'differential form' refer to both a differential form at a point and a differential form field on the manifold.**\n",
        "\n",
        "* (A hint of what is to come: just as the vectors $\\hat{\\mathbf{e}}_{x}, \\hat{\\mathbf{e}}_{y}$ and $\\hat{\\mathbf{e}}_{z}$ form a set of basis vectors for $\\mathbb{R}^{3}$, the differentials $d x, d y$ and $d z$ form a set of basis 1 -forms for $\\mathbb{R}^{3}$.)"
      ],
      "metadata": {
        "id": "2TNAlnDuLh7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Geometric and Clifford Algebra*"
      ],
      "metadata": {
        "id": "q09fihf25FPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Noncommutative Geometry*"
      ],
      "metadata": {
        "id": "oEn5oZesJVdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Noncommutative_geometry\n",
        "\n",
        "Noncommutative Algebra: Weyl algebra, Clifford algebra, Superalgebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Noncommutative_ring"
      ],
      "metadata": {
        "id": "iUaQzHb9JY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences between commutative and noncommutative algebra**\n",
        "\n",
        "Because noncommutative rings of scientific interest are more complicated than commutative rings, their structure, properties and behavior are less well understood. A great deal of work has been done successfully generalizing some results from commutative rings to noncommutative rings. A major difference between rings which are and are not commutative is the necessity to separately consider [right ideals and left ideals](https://en.m.wikipedia.org/wiki/Ideal_(ring_theory)#Definitions_and_motivation). It is common for noncommutative ring theorists to enforce a condition on one of these types of ideals while not requiring it to hold for the opposite side. For commutative rings, the left‚Äìright distinction does not exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "pwIYqOy9JnTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Clifford Algebra*"
      ],
      "metadata": {
        "id": "HrtIDW9V0p96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Hierarchy of Clifford Algebra $C‚Ñì_{p,q}$](https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras)**\n",
        "\n",
        "*Clifford algebra over the reals is also called Geometric algebra*\n",
        "\n",
        "Algebra $\\rightarrow$ [Signature](https://en.m.wikipedia.org/wiki/Metric_signature) $\\rightarrow$ Equation\n",
        "\n",
        "${Cl}_{4,2} (\\mathbb {R})$ - [Conformal geometric algebra (CGA)](https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra) $\\rightarrow$ +,+,+,+,-,-, $\\rightarrow$ [Twistor](https://en.m.wikipedia.org/wiki/Twistor_space) $\\rightarrow$ [twistor theory](https://en.m.wikipedia.org/wiki/Twistor_theory)\n",
        "\n",
        "\n",
        "\n",
        "${Cl}_{1,3} (\\mathbb {R})$ - [Spacetime algebra\n",
        "](https://en.m.wikipedia.org/wiki/Spacetime_algebra) and ${Cl}_{1,3} (\\mathbb {C})$ - [Dirac algebra\n",
        "](https://en.m.wikipedia.org/wiki/Dirac_algebra) $\\rightarrow$ +,-,-,-, $\\rightarrow$ [Dirac equation](https://en.m.wikipedia.org/wiki/Dirac_equation) $\\rightarrow$ [relativistic spin-1/2](https://en.m.wikipedia.org/wiki/Relativistic_wave_equations#Spin_1.2F2) $\\rightarrow$ [Gamma matrices](https://en.m.wikipedia.org/wiki/Gamma_matrices)\n",
        "\n",
        "${Cl}_{3,0} (\\mathbb {R})$ - [Algebra of physical space (Pauli algebra)\n",
        "](https://en.m.wikipedia.org/wiki/Algebra_of_physical_space) $\\rightarrow$ +,+,+ $\\rightarrow$ [Pauli equation](https://en.m.wikipedia.org/wiki/Pauli_equation) $\\rightarrow$ [spin-1/2](https://en.m.wikipedia.org/wiki/Spin-1/2) $\\rightarrow$ [Pauli matrices](https://en.m.wikipedia.org/wiki/Pauli_matrices)\n",
        "\n",
        "${Cl}_{0,3} (\\mathbb {R})$ - [i.e. Quaternions](https://en.m.wikipedia.org/wiki/Clifford_algebra#Quaternions)\n",
        "\n",
        "\n",
        "${Cl}_{0,1} (\\mathbb {R})$ - [Clifford_algebra#Real_numbers](https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers) $\\rightarrow$ - $\\rightarrow$ [Schr√∂dinger equation](https://en.m.wikipedia.org/wiki/Schr%C3%B6dinger_equation) $\\rightarrow$ spin-0\n",
        "\n",
        "> *See also: For a complete classification of these algebras see [Classification of Clifford algebras](https://en.m.wikipedia.org/wiki/Classification_of_Clifford_algebras).*\n"
      ],
      "metadata": {
        "id": "77his5x9yBCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Clifford Algebra $Cl_{p,q} (\\mathbb{C})$ (Incl. Weyl Algebra)*\n",
        "\n",
        "Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)\n",
        "\n",
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "https://clifford.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_gates\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/representations-and-spinors/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Majorana_equation\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/pauli-and-dirac-matrices/\n"
      ],
      "metadata": {
        "id": "ginSvag_5QfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Properties**\n",
        "\n",
        "* In Geometric Algebra the **basis vectors for the space are typically real valued vectors**. Complex valued vectors have uses in GA (i.e. frequency domain representation of vectors in electrodynamics), but the underlying basis for the vector space is still real valued (i.e. span{ùêû1,ùêû2,ùêû3}.\n",
        "\n",
        "* Clifford algebras provide a further generalization, **allowing those basis vectors to reside in a complex vector space**, with suitable modifications of the vector product rules.\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra\n",
        "\n",
        "* Clifford Algebra focuses on abstract mathematical and algebraic properties\n",
        "\n",
        "* Geometric Algebra focuses on geometric and physical applications\n",
        "\n",
        "> A [Clifford algebra](https://en.m.wikipedia.org/wiki/Clifford_algebra) is an algebra generated by a vector space with a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form), and is a unital [associative algebra](https://en.m.wikipedia.org/wiki/Associative_algebra) (not commutative, but anti-commutative!).\n",
        "\n",
        "* The best known example of a quadratic form is the square of the amount of a vector: $|\\vec{v}|^{2}=x^{2}+y^{2}+z^{2}+\\ldots$ (like used in quantum mechanics on Bloch sphere)\n",
        "\n",
        "* As K-algebras, they generalize the real numbers, complex numbers, quaternions and several other hypercomplex number systems.\n",
        "\n",
        "> **The theory of Clifford algebras is intimately connected with the theory of [quadratic forms](https://en.m.wikipedia.org/wiki/Quadratic_form) and [orthogonal transformations](https://en.m.wikipedia.org/wiki/Orthogonal_group) (Orthogonal group, like SO(2), SO(3) and SO(4)).**\n",
        "\n",
        "* Clifford algebras have important applications in a variety of fields including geometry, theoretical physics and digital image processing.\n",
        "\n",
        "A Clifford algebra is a unital associative algebra that contains and is generated by a vector space $V$ over a field $K_{1}$ where $V$ is equipped with a quadratic form $Q: V \\rightarrow K$. The Clifford algebra $\\mathrm{Cl}(V, Q)$ is the \"freest\" algebra generated by $V$ subject to the condition\n",
        "\n",
        "> $v^{2}=Q(v) 1$ for all $v \\in V$\n",
        "\n",
        "where the product on the left is that of the algebra, and the 1 is its [multiplicative identity](https://en.m.wikipedia.org/wiki/Identity_element#Definitions) (neutral element).\n",
        "\n",
        "See also: https://en.m.wikipedia.org/wiki/Clifford_algebra#Universal_property_and_construction"
      ],
      "metadata": {
        "id": "f7RjlHat54zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Extensions & Generalizations*"
      ],
      "metadata": {
        "id": "FMIv268H0nv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extensions & Generalizations**\n",
        "\n",
        "More precisely, **Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra**, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "Weyl algebras and Clifford algebras admit a further structure of a [*-algebra](https://en.m.wikipedia.org/wiki/*-algebra), and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "Source: https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "[Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra), a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra) by a [symplectic form](https://en.m.wikipedia.org/wiki/Symplectic_vector_space)\n",
        "\n",
        "Clifford algebra, a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the exterior algebra by a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form).\n",
        "\n",
        "The Weyl algebra is also referred to as the symplectic Clifford algebra. Weyl algebras represent the same structure for symplectic bilinear forms that Clifford algebras represent for non-degenerate symmetric bilinear forms.\n",
        "\n",
        "*Clifford Algebra & Free Algebra*\n",
        "\n",
        "\n",
        "* for Clifford Algebra: The idea of being the \"freest\" or \"most general\" algebra subject to this identity can be formally expressed through the notion of a [universal property](https://en.m.wikipedia.org/wiki/Universal_property) (from category theory).\n",
        "\n",
        "* in the area of abstract algebra known as ring theory, a [free algebra](https://en.m.wikipedia.org/wiki/Free_algebra) is the noncommutative analogue of a polynomial ring since its elements may be described as \"polynomials\" with non-commuting variables. Likewise, the polynomial ring may be regarded as a free commutative algebra.\n",
        "\n",
        "*Clifford Algebra & Exterior Algebra*\n",
        "\n",
        "* Clifford Algebra as a quantization of the exterior algebra\n",
        "\n",
        "* Clifford algebras are closely related to exterior algebras. Indeed, if Q = 0 then the Clifford algebra Cl(V, Q) is just the exterior algebra ‚ãÄ(V). For nonzero Q there exists a canonical linear isomorphism between ‚ãÄ(V) and Cl(V, Q) whenever the ground field K does not have characteristic two.\n",
        "\n",
        "* **Clifford multiplication together with the distinguished subspace is strictly richer than the exterior product since it makes use of the extra information provided by Q.**\n",
        "\n",
        "* The Clifford algebra is a [filtered algebra](https://en.m.wikipedia.org/wiki/Filtered_algebra), the [associated graded algebra](https://en.m.wikipedia.org/wiki/Associated_graded_ring) is the exterior algebra.\n",
        "\n",
        "* More precisely, Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, **in the same way that the Weyl algebra is a quantization of the symmetric algebra.**\n",
        "\n",
        "* Weyl algebras and Clifford algebras admit a further structure of a *-algebra, and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Clifford_algebra#Relation_to_the_exterior_algebra\n",
        "\n",
        "*Clifford Algebra & Geometric Algebra*\n",
        "\n",
        "* Geometric Algebra is a special form of the more general Clifford algebra\n",
        "\n",
        "* Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.\n",
        "\n",
        "*Other notes*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "...the geometric algebra for this quadratic space is the Clifford algebra... [Source](https://en.m.wikipedia.org/wiki/Geometric_algebra)\n",
        "\n",
        "Clifford algebras were born of a synthesis of inner product spaces and Grassmann's exterior algebras, both of which have geometric applications.\n",
        "\n",
        "A Clifford algebra is constructed from an inner product space (ùëâ,ùëÑ) by generating an associative algebra (whose product is a descendant of the tensor product in the tensor algebra for ùëâ). These are compatible in a sense made clear in the Wiki.\n",
        "\n",
        "https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "P3d4_VAH6tFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Applications*"
      ],
      "metadata": {
        "id": "Ig6IVIFPww88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*Applications*\n",
        "\n",
        "* One of the principal applications of the exterior algebra is in differential geometry where it is used to define the bundle of differential forms on a smooth manifold. In the case of a (pseudo-)Riemannian manifold, the tangent spaces come equipped with a natural quadratic form induced by the metric. Thus, one can define a Clifford bundle in analogy with the exterior bundle. This has a number of important applications in Riemannian geometry. Perhaps more importantly is the link to a spin manifold, its associated spinor bundle and spinc manifolds.\n",
        "\n",
        "* More: https://en.m.wikipedia.org/wiki/Clifford_algebra#Applications\n",
        "\n",
        "*Examples of Clifford Algebras $C‚Ñì_{p,q}$*\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras\n",
        "\n",
        "Different focusea created based on use cases / application\n",
        "\n",
        "Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "See also hierrchy of clifford algebra: https://en.m.wikipedia.org/wiki/Basil_Hiley#\n",
        "\n"
      ],
      "metadata": {
        "id": "FHJ0usg67LAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Geometric Algebra*"
      ],
      "metadata": {
        "id": "YWqVgPOwPCIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=60z_hpEAtD8&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=19)\n",
        "\n",
        "Video: [Addendum to A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=0bOiy0HVMqA&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)\n",
        "\n",
        "Video: [From Zero to Geo Introduction (Geometric Algebra Series)](https://www.youtube.com/watch?v=2hBWCCAiCzQ&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)"
      ],
      "metadata": {
        "id": "P5GPkVzEPEgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometric Algebra $Cl_{p,q} (\\mathbb{R})$ or $G(M^n)$ (Clifford algebra over reals)\n",
        "\n",
        "> In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does [Source](https://en.m.wikipedia.org/wiki/Vector_calculus)\n",
        "\n",
        "> Geometric Algebra is the Clifford Algebra over the field of real numbers.\n",
        "\n",
        "[**Geometric Product**](https://en.m.wikipedia.org/wiki/Geometric_algebra) *Sum of inner product (dot) and outer (not exterior??) product (wedge). Is the geometric product of any two vectors a and b as the sum of a symmetric product and an antisymmetric product:*\n",
        "\n",
        "> $a b=\\frac{1}{2}(a b+b a)+\\frac{1}{2}(a b-b a) =a \\cdot b+a \\wedge b$\n",
        "\n",
        "> $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n"
      ],
      "metadata": {
        "id": "cHOe2QmAPN5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Geometric Product: Sum of inner product (dot) and outer product (wedge)**: $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n",
        "* Magnitude of u $\\wedge$ v (=bivector) is the area of a parallelogram - similar to cross product, but not restricted to R3 like [cross product](https://en.m.wikipedia.org/wiki/Cross_product) (including magnitude and orientation)\n",
        "\n",
        "* Geometric algebra (GA) is an extension or completion of vector algebra (VA)\n",
        "\n",
        "* **Geometric Algebra: special form of the more general Clifford algebra**\n",
        "\n",
        "* the [geometric algebra](https://en.m.wikipedia.org/wiki/Geometric_algebra) (GA) of a vector space with a quadratic form (usually the Euclidean metric or the Lorentz metric) is an algebra over a field, the Clifford algebra of a vector space with a quadratic form with its multiplication operation called the geometric product.\n",
        "\n",
        "> **The algebra elements are called multivectors, which contains both the scalars $F$ and the vector space $V$.**\n",
        "\n",
        "* Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "* Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis and differential geometry, **e.g. by using the Clifford algebra instead of differential forms**.\n",
        "\n",
        "* Geometric algebra has been advocated as the **preferred mathematical framework for physics**. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity. GA has also found use as a computational tool in computer graphics and robotics.\n"
      ],
      "metadata": {
        "id": "tE1L0QYwPPsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "AuG47_giPSx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences to other algebras**\n",
        "\n",
        "* **Geometric Algebra vs Clifford algebra**: [Source](https://www.quora.com/How-are-geometric-algebra-and-Clifford-algebra-different)\n",
        "\n",
        "  * **Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.**\n",
        "\n",
        "  * A Clifford algebra is a unital associative algebra that contains and is generated by a vector space V over a field K, where V is equipped with a quadratic form Q.\n",
        "\n",
        "  * A Geometric algebra is a Clifford algebra of a vector space over the field of real numbers endowed with a quadratic form.\n",
        "\n",
        "  * **So a Geometric Algebra is a special form of the more general Clifford algebra**. In particular it is a CA over the reals. Additionally rather than being just an abstract object it has specific geometric meaning. For instance the 3+1 dimensional spacetime algebra is a Geometric algebra.\n",
        "\n",
        "* **Geometric Algebra vs Tensor Algebra**: Every element of a geometric algebra can be identified with a tensor, but not every tensor can be identified with an element of a geometric algebra [Source](\n",
        "https://math.stackexchange.com/questions/725350/is-geometric-algebra-isomorphic-to-tensor-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Exterior Algebra**: Exterior algebra defines an antisymmetric wedge product. In an exterior algebra, one can add k-forms to other k-forms, but would not add forms of different rank. This restriction is relaxed in geometric algebra (GA). [Source](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Vector Algebra**: [Source](\n",
        "https://en.m.wikipedia.org/wiki/Comparison_of_vector_algebra_and_geometric_algebra):\n",
        "\n",
        "  * Geometric algebra is an extension of vector algebra, providing additional algebraic structures on vector spaces, with geometric interpretations. Vector algebra uses all dimensions and signatures, as does geometric algebra, notably 3+1 spacetime as well as 2 dimensions.\n",
        "\n",
        "  * For example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. **The torque or curl is then a normal vector field in this 3rd dimension**.\n",
        "\n",
        "  * By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product."
      ],
      "metadata": {
        "id": "HkxCUeosPWmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: The cross product in relation to the exterior product. In red are the orthogonal unit vector, and the \"parallel\" unit bivector.*\n",
        "\n",
        "* $\\mathbf u \\times \\mathbf v$ is perpendicular to the plane containing $\\mathbf {u}$ and $\\mathbf {v}$\n",
        "\n",
        "* $\\mathbf u \\wedge \\mathbf v$ is an oriented representation of the same plane.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Exterior_calc_cross_product.svg/260px-Exterior_calc_cross_product.svg.png)"
      ],
      "metadata": {
        "id": "GLlRNs2BPZpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "9tCNN2HDPcAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "example: **Conformal geometric algebra $C‚Ñì_{4,2}$ - Twistor equation**"
      ],
      "metadata": {
        "id": "nt89GGCOPeSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Paravector"
      ],
      "metadata": {
        "id": "85mklZLQPgRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versor"
      ],
      "metadata": {
        "id": "GHAYUzVUPiWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{4,2}(\\mathbb{R})$ - Conformal geometric algebra*"
      ],
      "metadata": {
        "id": "Ym5Ax5nCOtuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra"
      ],
      "metadata": {
        "id": "g1zDxtzDOvrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{C})$ - Dirac Algebra*"
      ],
      "metadata": {
        "id": "QBuL0NbxO6cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Dirac_algebra"
      ],
      "metadata": {
        "id": "js5NF8RU528q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{R})$ - Spacetime Algebra*"
      ],
      "metadata": {
        "id": "M5NxmpI85zgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Spacetime Algebra](https://www.youtube.com/watch?v=e7aIVSVc8cI&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=21)\n",
        "\n",
        "> Spacetime Algebra: Geometric Algebra + Special Relativity\n",
        "\n",
        "**Spacetime Algebra $C‚Ñì_{1,3}$ - Dirac equation**\n",
        "\n",
        "> Spacetime algebra concerns the Clifford algebra Cl1,3(R) of the four-dimensional [Minkowski spacetime](https://en.m.wikipedia.org/wiki/Minkowski_space)\n",
        "\n",
        "* In mathematical physics, [spacetime algebra (STA)](https://en.m.wikipedia.org/wiki/Spacetime_algebra) is a name for the Clifford algebra Cl1,3(R), or equivalently the geometric algebra G(M4).\n",
        "\n",
        "* According to David Hestenes, spacetime algebra can be particularly closely associated with the geometry of special relativity and relativistic spacetime.\n",
        "\n",
        "* It is a vector space that allows not only vectors, but also bivectors (directed quantities associated with particular planes, such as areas, or rotations) or blades (quantities associated with particular hyper-volumes) to be combined, as well as rotated, reflected, or [Lorentz boosted](https://en.m.wikipedia.org/wiki/Lorentz_transformation#boost).\n",
        "\n",
        "* It is also the natural parent algebra of spinors in special relativity."
      ],
      "metadata": {
        "id": "T8yu7WqUO26x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{3,0}(\\mathbb{R})$ - Algebra of Physical Space and Pauli Algebra*"
      ],
      "metadata": {
        "id": "ur1A1K3aOl3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebra of physical space $C‚Ñì_{3,0}$ - Pauli equation**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebra_of_physical_space\n",
        "\n",
        "used in special relativity, classical electrodynamics and relativistic quantum mechanics (Dirac equation in quantum field theory)\n",
        "\n",
        "Whereas the mathematicians do not give special attention to the case  ùëõ=2, the physicists, dealing with four-dimensional space-time, have every reason to do so, and it turns out to be most rewarding to develop procedures and proofs for the special case rather than refer to the general mathematical theorems. The technique for such a program has been developed some years ago [Source](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)"
      ],
      "metadata": {
        "id": "C_DZg7k4OpnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli Algebra $A_2$**\n",
        "\n",
        "See article [The Pauli Algebra - Mathematics LibreTexts](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)\n",
        "\n",
        "$\\mathcal{A}_{2}$ is called the Pauli algebra. The basis matrices are\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\sigma_{0}=I=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{1}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{2}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{3}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "The [Pauli matrices](https://de.m.wikipedia.org/wiki/Pauli-Matrizen) are the following four 2 √ó 2 matrices:\n",
        "\n",
        "> $\\sigma_{0}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right), \\sigma_{1}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right), \\sigma_{2}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right), \\sigma_{3}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)$\n",
        "\n",
        "\n",
        "* The three Pauli matrices satisfy the well known multiplication rules\n",
        "\n",
        "* All of the basis matrices are Hermitian, or self-adjoint\n",
        "\n",
        "**Pauli Matrices (Pauli Operators)**\n",
        "\n",
        "The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse), meaning that the square of a Pauli matrix is the identity matrix.\n",
        "\n",
        ">$\n",
        "I^{2}=X^{2}=Y^{2}=Z^{2}=-i X Y Z=I\n",
        "$\n",
        "\n",
        "The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example $Z X=i Y=-X Z$.\n",
        "\n",
        "*Anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. **Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments**. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation, such as addition.*\n",
        "\n",
        "Single spin one half particle, focus on spin degrees of freedom:\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad 1 \\downarrow=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$\n",
        "\n",
        "**X, Y and Z axis on Bloch sphere:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_077.PNG)\n",
        "\n",
        "Source: https://www.researchgate.net/figure/The-Bloch-sphere-representation-of-a-qubit-The-basis-states-are-located-at-the-north_fig2_284259345\n",
        "\n",
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Bloch_Sphere.svg/423px-Bloch_Sphere.svg.png)\n",
        "\n",
        "*Bloch sphere*"
      ],
      "metadata": {
        "id": "Vi6h1_4V3hsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{0,0}, C‚Ñì_{0,1}, C‚Ñì_{0,2}(\\mathbb{R})$*"
      ],
      "metadata": {
        "id": "SN4YJmrE6HF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few low-dimensional cases are:\n",
        "\n",
        "* Cl0,0(R) is naturally **isomorphic to R** since there are no nonzero vectors.\n",
        "* Cl0,1(R) is a two-dimensional algebra generated by e1 that squares to ‚àí1, and is algebra-isomorphic to C, the field of **complex numbers**.\n",
        "* Cl0,2(R) is a four-dimensional algebra spanned by {1, e1, e2, e1e2}. The latter three elements all square to ‚àí1 and anticommute, and so the algebra is isomorphic to the **quaternions** H.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers\n"
      ],
      "metadata": {
        "id": "nywgP0rp6VF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Tensor Algebra*"
      ],
      "metadata": {
        "id": "fVQl5cbp7iqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Analysis*"
      ],
      "metadata": {
        "id": "PsodM6QW8wsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Motivation:*** *In our subject of differential geometry, where you talk about manifolds, **one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning**. They are allowed to **undergo transformation**. And in order to handle this kind of situation, an important tool is the so-called **tensor analysis, or Ricci calculus**, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.*\n",
        "\n",
        "* Die [Tensoranalysis](https://de.wikipedia.org/wiki/Tensoranalysis) ist ein Teilgebiet der Differentialgeometrie beziehungsweise der Differentialtopologie.\n",
        "\n",
        "* In der Tensoranalysis wird **das Verhalten von geometrischen Differentialoperatoren auf Tensorfeldern untersucht**.\n",
        "\n",
        "> <font color=\"blue\">**Tensor Analysis ist eine Verallgemeinerung der Vektoranalysis**</font>\n",
        "\n",
        "* Zum Beispiel kann der Differentialoperator Rotation in diesem Kontext auf n Dimensionen verallgemeinert werden.\n",
        "\n",
        "* Zentrale Objekte der Tensoranalysis sind Tensorfelder. Es wird untersucht, wie Differentialoperatoren auf diesen Feldern wirken.\n",
        "\n",
        "* Vektoren und Matrizen, sofern sie geometrische oder physikalische Groessen reprasentieren, koennen unter dem begriff eines tensors subsumiert werden\n",
        "\n",
        "* **Tensorrechnung in 2 teilen:**\n",
        "\n",
        "  * Anschauungsraum fur ingenieure (**kartesische tensoren**).\n",
        "\n",
        "  * Fur **schiefwinklige (also affine) oder krummlinige Koordinaten** sind begriffe wie kovariant und kontravariant wichtig (zur arbeit mit metriken, deren skalarproduktauf einer nicht positiv definiten bilinearform beruht)."
      ],
      "metadata": {
        "id": "rx-E5d4r8zxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoralgebra**\n",
        "\n",
        "> **The [tensor algebra](https://en.m.wikipedia.org/wiki/Tensor_algebra) is important because many other algebras arise as quotient algebras of T(V)**. These include the [exterior algebra (Grassmann algebra of Differential Forms)](https://en.m.wikipedia.org/wiki/Exterior_algebra), the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra), the [Clifford algebras](https://en.m.wikipedia.org/wiki/Clifford_algebra), the [Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra) and [universal enveloping algebras](https://en.m.wikipedia.org/wiki/Universal_enveloping_algebra).\n",
        "\n",
        "* Die [Tensoralgebra](https://de.m.wikipedia.org/wiki/Tensoralgebra) ist ein mathematischer Begriff, der in vielen Bereichen der Mathematik wie der linearen Algebra, der Algebra, der Differentialgeometrie sowie in der Physik verwendet wird.\n",
        "\n",
        "* Sie fasst \"alle Tensoren\" √ºber einem Vektorraum in der Struktur einer graduierten Algebra zusammen.\n",
        "\n",
        "* Es sei $V$ ein Vektorraum √ºber einem K√∂rper $K$ oder allgemeiner ein Modul √ºber einem kommutativen Ring mit Einselement. Dann ist die Tensoralgebra (als Vektorraum) definiert durch die direkte Summe aller Tensorprodukte des Raums mit sich selbst.\n",
        "\n",
        ">$\n",
        "\\mathrm{T}(V):=\\bigoplus_{n \\geq 0} V^{\\otimes n}=K \\oplus V \\oplus(V \\otimes V) \\oplus(V \\otimes V \\otimes V) \\oplus \\ldots\n",
        "$\n",
        "\n",
        "* Mit der Multiplikation, die auf den homogenen Bestandteilen durch das Tensorprodukt gegeben ist, wird $\\mathrm{T}(V)$ zu einer $\\mathbb{N}_{0}$ -graduierten, unit√§ren, assoziativen Algebra.\n",
        "\n",
        "* Quotientenr√§ume der Tensoralgebra: Durch Herausteilen eines bestimmten Ideals kann man aus der Tensoralgebra beispielsweise die symmetrische Algebra, die √§u√üere Algebra oder die **Clifford-Algebra** gewinnen. Diese Algebren sind in der Differentialgeometrie von Bedeutung.\n",
        "\n"
      ],
      "metadata": {
        "id": "MERE084982Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Example for Tensors: Einstein Field Equations in General Relativity:**\n",
        "\n",
        "> $\n",
        "R_{\\mu \\nu}-\\frac{1}{2} R g_{\\mu \\nu}+\\Lambda {g_{\\mu v}}=\\frac{8 \\pi G}{c^{4}} T_{\\mu \\nu}\n",
        "$\n",
        "\n",
        "4x4 rank 2 metric tensor: ${g_{\\mu v}}$ (measure lengths and angles in the curved geometry of spacetime)\n",
        "\n",
        "* Gravitation als geometrische Eigenschaft der gekr√ºmmten vierdimensionalen Raumzeit.\n",
        "\n",
        "* Zur Beschreibung der **Raumzeit und ihrer Kr√ºmmung bedient man sich der Differentialgeometrie**, die die Euklidische Geometrie des uns vertrauten ‚Äûflachen‚Äú dreidimensionalen Raumes der klassischen Mechanik umfasst und erweitert.\n",
        "\n",
        "* Die Differentialgeometrie verwendet zur Beschreibung gekr√ºmmter R√§ume, wie der Raumzeit der ART, sogenannte **Mannigfaltigkeiten**. Wichtige **Eigenschaften werden mit sogenannten Tensoren beschrieben**, die Abbildungen auf der Mannigfaltigkeit darstellen.\n",
        "\n",
        "* Die gekr√ºmmte Raumzeit wird als [Lorentz-Mannigfaltigkeit](https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit) (Pseudo-riemannsche Mannigfaltigkeit) beschrieben.\n",
        "\n",
        "* Eine besondere Bedeutung kommt dem [**metrischen Tensor**](https://de.wikipedia.org/wiki/Metrischer_Tensor) zu. Wenn man in den metrischen Tensor zwei Vektorfelder einsetzt, erh√§lt man f√ºr jeden Punkt der Raumzeit eine reelle Zahl.\n",
        "\n",
        "  * In dieser Hinsicht kann man den metrischen Tensor als ein verallgemeinertes, punktabh√§ngiges Skalarprodukt f√ºr Vektoren der Raumzeit verstehen.\n",
        "\n",
        "  * Mit seiner Hilfe werden Abstand und Winkel definiert und er wird daher kurz als Metrik bezeichnet.\n",
        "\n",
        "* Ebenso bedeutend ist der [riemannsche Kr√ºmmungstensor](https://de.wikipedia.org/wiki/Riemannscher_Kr√ºmmungstensor) zur Beschreibung der Kr√ºmmung der Mannigfaltigkeit,\n",
        "\n",
        "  * der eine Kombination von ersten und zweiten Ableitungen des metrischen Tensors darstellt.\n",
        "\n",
        "  * Wenn ein beliebiger Tensor in irgendeinem Koordinatensystem in einem Punkt nicht null ist, kann man √ºberhaupt kein Koordinatensystem finden, sodass er in diesem Punkt null wird. Dies gilt dementsprechend auch f√ºr den Kr√ºmmungstensor.\n",
        "\n",
        "  * Umgekehrt ist der Kr√ºmmungstensor in allen Koordinatensystemen null, wenn er in einem Koordinatensystem null ist. Man wird also in jedem Koordinatensystem bez√ºglich der Frage, ob eine Mannigfaltigkeit an einem bestimmten Punkt gekr√ºmmt ist oder nicht, zum gleichen Ergebnis gelangen.\n",
        "\n",
        "* Die ma√ügebliche Gr√∂√üe zur Beschreibung von Energie und Impuls der Materie ist der [Energie-Impuls-Tensor](https://de.wikipedia.org/wiki/Energie-Impuls-Tensor). Dieser Tensor bestimmt die Kr√ºmmungseigenschaften der Raumzeit. Siehe auch [Vierertensor](https://de.wikipedia.org/wiki/Vierertensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "1xA1Fe1B83_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Example for Tensors: Quantum Mechanics & Quantum Computing**\n",
        "\n",
        "Quantum Superposition:\n",
        "\n",
        "> $\\vec{\\psi}=a \\vec{v}+b \\vec{w}$\n",
        "\n",
        "* linear combination, physical quantum states are just vectors, using linear combinations to give more complicated states\n",
        "\n",
        "\n",
        "Quantum Entanglement\n",
        "\n",
        "> $\\vec{\\psi} \\otimes \\vec{\\phi}$\n",
        "\n",
        "* two states are 'entangled' together means state these state vectors have been combined together using the 'tensor product' (circle X)\n",
        "\n",
        "* Takes the geometrical space where the first system lives and the second system and combines them together to create a more complicated geometrical space, and that's where the entangled system lives"
      ],
      "metadata": {
        "id": "thzMwgWj86A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu k√∂nnen, ist eine Basis n√∂tig. Im n-dimensionalen Raum besteht diese aus n linear unabh√§ngigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur f√ºr lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) m√∂glich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabh√§ngigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): F√ºr echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabh√§ngigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekr√ºmmt sind, sind die Koordinatenachsen nicht r√§umlich fest, wie es f√ºr kartesische Koordinaten gilt. Dies f√ºhrt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abh√§ngt ‚Äì im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenfl√§chen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgew√§hlten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "oZN7V4P688A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Tangent Space & Kotangentialraum**\n",
        "\n",
        "* **Tangential and Normal Components (Koordinaten)**: Given a vector at a point on a curve (just a vector, not a tangential vector!), that vector can be decomposed uniquely as a sum of two vectors,\n",
        "\n",
        "  * $\\mathbf{v}_{\\|}$ : one tangent to the curve, called the **tangential component** of the vector. $\\mathbf{v}_{\\perp}$  : another one perpendicular to the curve, called the **normal component** of the vector. Zusatzlich gibt es noch: The binormal unit vector B is defined as the cross product of T and N ([Source](https://en.m.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas))\n",
        "\n",
        "  * More formally, let $S$ be a surface, and $x$ be a point on the surface. Let $\\mathbf{v}$\n",
        "be a vector at $x$. Then one can write uniquely $\\mathbf{v}$ as a sum: $\\mathbf{v}=\\mathbf{v}_{\\|}+\\mathbf{v}_{\\perp}$. Similarly a vector at a point on a surface can be broken down the same way.\n",
        "\n",
        "  * More generally, given a submanifold $N$ of a manifold $M,$ and\n",
        "a vector in the tangent space to $M$ at a point of $N$, it can be\n",
        "decomposed into the component tangent to $N$ and the\n",
        "component normal to $N$.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Surface_normal_tangent.svg/248px-Surface_normal_tangent.svg.png)\n",
        "\n",
        "* **Calculate the components**: To calculate the tangential and normal components, consider a [unit normal](https://en.wikipedia.org/wiki/Normal_(geometry)) to the surface, that is, a [unit vector](https://en.wikipedia.org/wiki/Unit_vector) $\\hat{n}$ perpendicular to $S$ at $x$.\n",
        "\n",
        "  * Then, normal component: $\\mathbf{v}_{\\perp}=(\\mathbf{v} \\cdot \\hat{n}) \\hat{n}$ and thus tangential component: $\\mathbf{v}_{\\|}=\\mathbf{V}-\\mathbf{V}_{\\perp}$ where \".\" denotes the [dot product](https://en.wikipedia.org/wiki/Dot_product).\n",
        "\n",
        "  * Another formula for the tangential component is $\n",
        "\\mathbf{v}_{\\|}=-\\hat{n} \\times(\\hat{n} \\times \\mathbf{v})$ where \" $\\times$ \" denotes the [cross product](https://en.wikipedia.org/wiki/Cross_product).\n",
        "\n",
        "  * Note that these formulas do not depend on the particular unit normal $\\hat{n}$ used (there exist two unit normals to any surface at a given point, pointing in opposite directions, so one of the unit normals is the negative of the other one).\n",
        "\n",
        "* Siehe Artikel: [Tangential and normal components](https://en.wikipedia.org/wiki/Tangential_and_normal_components)\n",
        "\n",
        "* **Tangentialvektor**: Sei $\\gamma:(-\\varepsilon, \\varepsilon) \\rightarrow M$ eine differenzierbare Kurve mit $\\gamma(0)=x$ und dem **Kurvenparameter $t$** (siehe 'Parameterdarstellungen' oben), dann ist: $v=\\frac{d \\gamma}{d t}(0) \\in T_{x} M$ ein [Tangentialvektor](https://en.wikipedia.org/wiki/Tangent_vector). Die Tangentialvektoren in einem Punkt $x \\in M$ spannen einen Vektorraum auf, den Tangentialraum $T_{x} M$. Siehe auch [Tangentialb√ºndel](https://de.wikipedia.org/wiki/Tangentialb√ºndel).\n",
        "\n",
        "* **Tangent Space (Tangentialraum)** Ein [Tangentialraum](https://de.wikipedia.org/wiki/Tangentialraum) ist $T_{x} M$ ein Vektorraum, der eine differenzierbare Mannigfaltigkeit $M$ am Punkt $x$ linear approximiert.\n",
        "\n",
        "* **Normal (Vector) Space**: The normal vector space or normal space of a manifold at point P is the **set of vectors which are orthogonal to the tangent space at P**. Normal vectors are of special interest in the case of smooth curves and smooth surfaces.\n",
        "\n",
        "* **Kotangentialraum**: Der [Kotangentialraum](https://de.wikipedia.org/wiki/Kotangentialraum) ist der Dualraum des entsprechenden Tangentialraums. Er ist ein Vektorraum, der einem Punkt einer differenzierbaren Mannigfaltigkeit $M$ zugeordnet wird.\n",
        "\n",
        "  * Sei $M$ eine differenzierbare Mannigfaltigkeit und $T_{p} M$ ihr Tangentialraum am Punkt $p \\in M$. Dann ist der Kotangentialraum definiert als der Dualraum von $T_{p} M$.\n",
        "\n",
        "  * **Das hei√üt, der Kotangentialraum besteht aus allen Linearformen auf dem Tangentialraum $T_{p} M$**.\n",
        "\n",
        "  * In differential geometry, **one can attach to every point $x$ of a smooth (or differentiable) manifold, $\\mathcal{M},$ a vector space called the cotangent space at $x .$** Typically, the cotangent space, $T_{x}^{*} \\mathcal{M}$ is defined as the dual space of the tangent space at $x, T_{x} \\mathcal{M},$ although there are more direct definitions. The elements of the cotangent space are **called cotangent vectors or tangent covectors**.\n",
        "\n",
        "  * Let $\\mathcal{M}$ be a smooth manifold and let $x$ be a point in $\\mathcal{M}$. Let $T_{x} \\mathcal{M}$ be\n",
        "the tangent space at $x$. Then the cotangent space at $x$ is defined as the dual space of $T_{x} \\mathcal{M}$ : $\n",
        "T_{x}^{*} \\mathcal{M}=\\left(T_{x} \\mathcal{M}\\right)^{*}\n",
        "$\n",
        "\n",
        "  * Concretely, **elements of the cotangent space are linear functionals on $T_{x} \\mathcal{M}$**. That is, **every element $\\alpha \\in T_{x}^{*} \\mathcal{M}$ is a linear map** $\n",
        "\\alpha: T_{x} \\mathcal{M} \\rightarrow F\n",
        "$\n",
        "\n",
        "  * where $F$ is the underlying field of the vector space being considered, for example, the field of real numbers. **The elements of $T_{x}^{*} \\mathcal{M}$ are called cotangent vectors.**\n",
        "\n",
        "*Die [Hauptkr√ºmmungen](https://de.wikipedia.org/wiki/Hauptkr%C3%BCmmung) sind [Eigenwerte](https://de.wikipedia.org/wiki/Eigenwertproblem) der [Weingartenabbildung](https://de.wikipedia.org/wiki/Weingartenabbildung)*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/e/eb/Minimal_surface_curvature_planes-en.svg)"
      ],
      "metadata": {
        "id": "E84nx70p9Db6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Koordinatentransformation (Geometric transformation) & Basiswechsel**\n",
        "\n",
        "* [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation), wenn sich ein Problem in einem anderen Koordinatensystem leichter l√∂sen l√§sst, z. B. bei der Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten.\n",
        "\n",
        "* [Liste von Transformationen in der Mathematik](https://de.m.wikipedia.org/wiki/Liste_von_Transformationen_in_der_Mathematik) und [Intro: Koordinatentransformation](http://walter.bislins.ch/physik/index.asp?page=Koordinatentransformation). Beispiel: [Drehung](https://de.wikipedia.org/wiki/Drehung) (Rotation), [Skalierung](https://de.wikipedia.org/wiki/Skalar_(Mathematik)), [Scherung](https://de.wikipedia.org/wiki/Scherung_(Geometrie)) & [Verschiebung](https://de.wikipedia.org/wiki/Parallelverschiebung) (Translation). [Affine Transformationen](https://de.wikipedia.org/wiki/Affine_Abbildung) aus linearen Transformation und Translation. Translation ist ein Spezialfall einer affinen Transformation, bei der A die Einheitsmatrix ist.\n",
        "\n",
        "* In der Regel verwendet man spezielle Transformationen, bei denen diese Funktionen gewissen Einschr√§nkungen ‚Äì z. B. Differenzierbarkeit, Linearit√§t oder Formtreue ‚Äì unterliegen.\n",
        "\n",
        "* Bei **lineare Transformationen** ([Lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)) sind die neuen Koordinaten lineare Funktionen der urspr√ºnglichen. [Matrixmultiplikation](https://de.wikipedia.org/wiki/Matrizenmultiplikation) des alten Koordinatenvektors $\\vec{x} = (x_1, \\dots, x_n)$ mit der Matrix $A \\rightarrow {\\vec {x}}'=A{\\vec {x}}$.\n",
        "\n",
        "    * $x_{1}^{\\prime}=a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n}$\n",
        "    * $x_{2}^{\\prime}=a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n}$\n",
        "    * $\\cdots$\n",
        "    * $x_{n}^{\\prime}=a_{n 1} x_{1}+a_{n 2} x_{2}+\\cdots+a_{n n} x_{n} .$\n",
        "\n",
        "* **Ein Basiswechsel ist ein Spezialfall einer Koordinatentransformation**: [**Basiswechsel im Vektorraum (Transformationsmatrix)**](https://de.wikipedia.org/wiki/Basiswechsel_(Vektorraum)) (lineare Algebra) ist der √úbergang zwischen zwei verschiedenen Basen eines endlichdimensionalen Vektorraums √ºber einem K√∂rper $K$. Beispiele:\n",
        "\n",
        "  * [Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten](https://de.m.wikipedia.org/wiki/Koordinatentransformation#Beispiele)\n",
        "\n",
        "  * [Transformation von Differential-Operatoren](https://de.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Differentialen) (Jacobi-Matrix)\n",
        "\n",
        "  * [Transformation von Vektorfeldern](https://de.m.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Vektorfeldern_und_-Operatoren)\n",
        "\n",
        "  * [Lorentz-Transformationen](https://de.wikipedia.org/wiki/Lorentz-Transformation) zur Beschreibungen von Ph√§nomenen in verschiedenen Bezugssystemen (Spezielle Relativit√§tstheorie). Verbinden in vierdimensionaler Raumzeit die Zeit- und Ortskoordinaten, mit denen verschiedene Beobachter angeben, wann und wo Ereignisse stattfinden. Lorentz-Transformationen erhalten Abst√§nde in nichteuklidischer Raumzeit ([Minkowskiraum](https://de.wikipedia.org/wiki/Minkowski-Raum)), Winkel aber nicht, da der Minkowskiraum kein normierter Raum ist.\n",
        "\n",
        "  * [Galilei-Transformation](https://de.wikipedia.org/wiki/Galilei-Transformation): √Ñquivalent zu Lorentz-Transformationen im dreidimensionalen euklidischen Raum. Anwendbar, wenn sich Bezugssysteme durch geradlinig-gleichf√∂rmige Bewegung, Drehung und/oder eine Verschiebung in Raum oder Zeit unterscheiden. Alle Beobachtungen von Strecken, Winkeln und Zeitdifferenzen stimmen in beiden Bezugssystemen √ºberein; alle beobachteten Geschwindigkeiten unterscheiden sich um die konstante Relativgeschwindigkeit der beiden Bezugssysteme.\n",
        "\n",
        "  * [Eichtransformation](https://de.wikipedia.org/wiki/Eichtransformation) ver√§ndert die Eichfelder einer physikalischen Theorie (z. B. die elektromagnetischen Potentiale oder die potentielle Energie) dergestalt, dass die physikalisch wirksamen Felder (z. B. das elektromagnetische Feld oder ein Kraftfeld) und damit alle beobachtbaren Abl√§ufe dabei die gleichen bleiben, z. B. die Verschiebung des Nullpunkts der potentiellen Energie, die Wahl des Referenzpotentials bei der Messung elektrischer Spannungen, ein konstanter Phasenfaktor an der komplexen Wellenfunktion der Quantenmechanik.\n",
        "\n",
        "* [Youtube Video 1](https://www.youtube.com/watch?v=CR7e7Zc0QLg) und [Youtube Video 2](https://www.youtube.com/watch?v=FFVauAY_FMI)\n",
        "\n",
        "**Eigenchris Video series:**\n",
        "\n",
        "* [Eigenchris: Tensors For Beginners (-1): Motivation](https://www.youtube.com/watch?v=8ptMTLzV4-I&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1: Forward and Backward Transformations](https://www.youtube.com/watch?v=sdCmW5N1LW4&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=3)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1.5: Correction on Forward + Backward Transforms](https://www.youtube.com/watch?v=ipRrCPvftTk&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=4)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 2: Vector definition](https://www.youtube.com/watch?v=uPbBDToXjBw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=5)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 3: Vector Transformation Rules](https://www.youtube.com/watch?v=A1h_eucHFW4&t=177s)\n",
        "\n"
      ],
      "metadata": {
        "id": "k7tUmz4d9FUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ as Kronecker Product, of Tensors and of Tensor Spaces*"
      ],
      "metadata": {
        "id": "zTjh6Dkf9Imc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product** (generalization of the outer product)\n",
        "\n",
        "* the [tensor product V ‚äó W](https://en.m.wikipedia.org/wiki/Tensor_product) of two vector spaces V and W (over the same field) is a vector space which can be thought of as the space of all tensors that can be built from vectors from its constituent spaces using an additional operation which can be **considered as a generalization and abstraction of the outer product**.\n",
        "\n",
        "**Tensor Product: The 2 different tensor product use cases**\n",
        "\n",
        "* the \"little\" tensor product which combines individual tensors\n",
        "\n",
        "* the \"big\" tensor product which combines entire tensor vector spaces\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_62.png)\n",
        "\n",
        "* the Kronecker product, sometimes denoted by ‚äó,[1] is an operation on two matrices of arbitrary size resulting in a block matrix.\n",
        "* It is a generalization of the outer product (which is denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis.\n",
        "* The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n",
        "* The Kronecker product is a special case of the tensor product, so it is bilinear and associative"
      ],
      "metadata": {
        "id": "JRvzAr6v9RWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Tensor Product $\\otimes$ vs Kronecker Product $\\otimes$**\n",
        "\n",
        "* They are technically different things, but highly related to each other.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 13: Tensor Product vs Kronecker Product](https://www.youtube.com/watch?v=qp_zg_TD0qE&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=16)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_40.png)\n",
        "\n",
        "* **Here are examples on how the tensor products works:**\n",
        "\n",
        "* Basis for Vector Space $V$\n",
        "\n",
        "> $\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}} \\in V$\n",
        "\n",
        "Basis for Dual Space $V*$\n",
        "\n",
        "> $\\epsilon^{1}, \\epsilon^{2} \\in V^{*}$\n",
        "\n",
        "* And the covectors are linear functions that are defined by these rules here, where the covector $\\epsilon^{i}$ is acting on the vector $\\overrightarrow{e_{j}}$ gives the Kronecker delta $\\delta_{j}^{i}$ as the result (=we get 1 if i and j are the same and 0 if not).\n",
        "\n",
        "> $\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{j}^{i}=\\left\\{\\begin{array}{l}1, i=j \\\\ 0, i \\neq j\\end{array}\\right.$\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)\n",
        "\n",
        "**To demonstrate that this is a linear map, we can pass in an input vector $\\overrightarrow{v}$**\n",
        "\n",
        "To get the output we can just we pass $v$ to the covector $\\epsilon$ first\n",
        "\n",
        "> $\\left(\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}\\right)(\\vec{v})$\n",
        "\n",
        "Pass v to the covector:\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}(\\vec{v})\\right)$\n",
        "\n",
        "Then we expand v as a linear combination of the basis vectors\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "The we bring the components outside since covectors are linear functions (we can scale before or after):\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "And the covector acting on a vector becomes a Kronecker delta:\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "Then Kronecker index cancellation rule we can remove the $k$ indexes and get j:\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "So this is a function that takes a vector input and produces a vector output\n",
        "\n",
        "\n",
        "**Here are examples on how the Kronecker products works:**\n",
        "\n",
        "* Here you get a row of columns when you take the first array on the left and distribute every element to the second array on the right:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_46.png)\n",
        "\n",
        ".. and multiplying this with another column vector we get the same thing (here you get a column of rows of columns)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_47.png)\n",
        "\n",
        "\n",
        "**Summary**\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "* these coefficient are the entries of a matrix (on the tensor product side)\n",
        "\n",
        "* on the Kronecker delta side you can see this matrix !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_48.png)\n",
        "\n",
        "* Tensor product and Kronecker product are doing basically the same kind of thing\n",
        "\n",
        "* It‚Äôs just the tensor product is combining the abstract vector and the abstract covector in the land of algebraic symbols\n",
        "\n",
        "* And the Kronecker product is combining the the vector array and the covector array  in the land of arrays\n",
        "\n",
        "* But the components that we get from the tensor are just the components of the matrix that we get from Kronecker product\n",
        "\n",
        "* So they are sort of the same operation they just do the work in different contexts\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_49.png)"
      ],
      "metadata": {
        "id": "kz-n4Xvc9TFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 1*\n",
        "\n",
        "We can see a lot of similarities between:\n",
        "* the Kronecker product for matrices and\n",
        "* the tensor product for abstract Hilbert spaces.\n",
        "\n",
        "* If a vector can be expressed as the tensor product of two vectors, we call it a product vector, i.e., $|\\Psi\\rangle$ **is a product vector** if $|\\Psi\\rangle=|\\phi \\otimes \\chi\\rangle$ for some $|\\phi\\rangle$ and $|\\chi\\rangle$. Any nonproduct vector is called entangled. The entangled states of a composite system occupy a distinguished position in the interpretation of quantum mechanics.\n",
        "\n",
        "* We would like to define a <font color=\"blue\">**tensor product for abstract Hilbert spaces**</font>.\n",
        "\n",
        "* The main need for this comes from the necessity of <font color=\"blue\">**describing composite quantum systems**</font>.\n",
        "\n",
        "* If we have two different systems $\\mathrm{A}$ and $\\mathrm{B}$, then **we have separate Hilbert spaces**, say $\\mathcal{H}_{A}$ and $\\mathcal{H}_{B}$, for describing the quantum states of these systems.\n",
        "\n",
        "* Basically a vector $|\\alpha\\rangle_{A}$ in $\\mathcal{H}_{A}$ gives a state of $\\mathrm{A}$ and a vector $|\\beta\\rangle_{B}$ in $\\mathcal{H}_{B}$ gives a state of $\\mathrm{B}$.\n",
        "\n",
        "* In a similar way, we should <font color=\"blue\">**have an entirely different Hilbert space** $\\mathcal{H}_{A B}$ for describing the states of the composite system AB.</font>\n",
        "\n",
        "* In particular, we would like to have a vector in $\\mathcal{H}_{A B}$ that describes the state of $\\mathrm{AB}$ such that $\\mathrm{A}$ is in state $\\mid \\alpha \\rangle_{A}$ and $\\mathrm{B}$ is in state $|\\beta\\rangle_{B}$. We will denote this state as $|\\alpha\\rangle_{A} \\otimes|\\beta\\rangle_{B}$.\n",
        "\n",
        "* We will also want to have this product $\\otimes$ be such that superpositions of states in $\\mathrm{A}$ or in B could also be equivalently described as superpositions of states in $\\mathrm{AB} ;$ hence **distributivity**.\n",
        "\n",
        "http://www.physics.metu.edu.tr/~sturgut/p455/qm-math4.pdf"
      ],
      "metadata": {
        "id": "L_0hUsHx9Wtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 2*\n",
        "\n",
        "* **The two notions represent operations on different objects: Kronecker product on matrices; tensor product on linear maps between vector spaces**.\n",
        "\n",
        "* But there is a connection: Given two matrices, we can think of them as representing linear maps between vector spaces equipped with a chosen basis.\n",
        "\n",
        "* The Kronecker product of the two matrices then represents the tensor product of the two linear maps.\n",
        "\n",
        "* (This claim makes sense because the tensor product of two vector spaces with distinguished bases comes with a distinguish basis.)\n",
        "\n",
        "https://math.stackexchange.com/questions/203947/tensor-product-and-kronecker-product"
      ],
      "metadata": {
        "id": "o08kUOZC9Ym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 3*\n",
        "\n",
        "Sometimes the Kronecker product is also called direct product\n",
        "or tensor product.\n",
        "\n",
        "https://www.worldscientific.com/doi/pdf/10.1142/9789811202520_0002"
      ],
      "metadata": {
        "id": "SdLDqnuT9aid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 4*\n",
        "\n",
        "* |u> <v| is a way of writing the <font color=\"blue\">**tensor product of a vector and a dual vector**</font> (ie, an element of the Hilbert space and an element of its dual, which is usually casually identified with the Hilbert space using the inner product).\n",
        "\n",
        "* This is a linear operator on the Hilbert space, sending |w > to <v|w>|u>.\n",
        "\n",
        "* In general, the tensor product of a vector space and its dual is the space of (finite rank) linear operators on the vector space.\n",
        "\n",
        "* On the other hand, |u>|v> is an element of the tensor product of the vector space with itself, usually used in physics for describing a composite of two identical systems. Again, since there is an isomorphism between the vector space and its dual, there is one between the space of composite states and the space of linear operators. This is interesting, but I've never seen this put to good use.\n",
        "\n",
        "* Finally, **the Kronecker product is just a particular representation of the tensor product, convenient for dealing with tensor products of linear operators**.\n",
        "\n",
        "Source https://www.physicsforums.com/threads/difference-between-the-outer-product.236244/"
      ],
      "metadata": {
        "id": "Xim2BtEb9cXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 5*\n",
        "\n",
        "* the Kronecker product and tensor product essentially have the same mathematical \"actions\" (an expanded matrix with dimensions equal to the product of the two matrices), but the tensor product explicitly applies only to matrices representing linear maps.\n",
        "\n",
        "* Every matrix represents a linear map.\n",
        "\n",
        "* Linear maps are in some sense more general than matrices. They are also different (types of) objects, even though matrices can be used to represent _some_ linear maps. The matrix representation of a particular linear map depends on the choice of basis of the domain and target space, for example, whereas the linear map itself is invariant under such choices. On the other hand a matrix can also represent a bilinear form, not just a linear map. Consider reading Axler's _Linear Algebra Done Right_ for a good explanation of some of such subtleties.\n",
        "\n",
        "https://math.stackexchange.com//questions/203947"
      ],
      "metadata": {
        "id": "iGV-6fNH9eGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covariance (Covector) and Contravariance (Vector): Tensor Notation for Vectors & Covectors in Coordinate Systems*"
      ],
      "metadata": {
        "id": "icJDI9EQ9gc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of a Tensor**\n",
        "\n",
        "> **Tensor = an object that is invariant under a change of coordinates, and... has components that change in a special, predictable way under a change of coordinates**\n",
        "\n",
        "* the direction and the lengths of an object are invariant (=vector)\n",
        "\n",
        "> **The vector components are NOT invariant. They change  depending on the coordinate system they use.**\n",
        "\n",
        "* if you know how to switch between coordinate system, you should be able to know how to switch between vector components (forward and backward)\n",
        "\n",
        "> **Tensor: A collection of vectors and convectors combined together using the tensor product.** (abstract definition)\n",
        "\n",
        "> **Tensors as partial derivatives and gradients that transform with the Jacobian matrix.**\n",
        "\n",
        "* Tensoren sind Gr√∂ssen, mit deren Hilfe man Skalare, Vektoren und weitere Gr√∂ssen analoger Struktur in ein einheitliches Schema zur Beschreibung mathematischer und physikalischer Zusammenh√§nge einordnen kann.\n",
        "\n",
        "* Sie sind definiert durch ihre Transformationseigenschaften gegen√ºber orthogonalen Transformationen wie z.B. Drehungen. Es geht darum, was √§ndert sich, was √§ndert sich nicht, wenn man das Bezugssystem √§ndert?\n",
        "\n",
        "* Die Besonderheit von Tensorgleichungen ist, dass sie transformations-invariant sind. Wenn es gelingt, einen physikalischen Sachverhalt in Tensorschreibweise zu formulieren, so kann man wegen der speziellen Art, wie Tensoren transformieren, sicher sein, dass die Gleichungen in jedem beliebigen Koordinatensystem gelten.\n",
        "\n",
        "* **Rang (oder Stufe) eines Tensors und Indizes**: Tensoren haben **Indizes**. Die Anzahl der Indizes gibt den Rang oder die Stufe des Tensors an. Die Indizes laufen entsprechend der **Dimension** $D$ des Raumes √ºber $1 . . D$. Bei der 4-dimensionalen Raumzeit beginnt die Nummerierung bei $0,$ wobei der Index 0 die zeitliche Komponente betrifft. Beim Arbeiten mit den Tensoren muss die **Reihenfolge der Indices** immer klar sein. Das Element $t_{12}$ eines Tensors ist in der Regel vom Element $t_{21}$ verschieden.\n",
        "\n",
        "  * Der einfachste Tensor ist ein Tensor mit Rang 0 . Dabei handelt es sich einfach um einen **Skalar**. Ein Skalar hat eigentlich keine Komponenten, sondern ist nur ein einzelner Wert und ben√∂tigt somit keinen Index; daher der Rang $0 .$\n",
        "\n",
        "  * Ein Tensor mit nur einem Index nennt man auch **Vektor**. Man sagt, der Vektor ist ein Tensor mit dem Rang 1. Der Index hat so viele Werte, wie die Dimension des Vektors. Bei einem 3 -dimensionalen Vektor hat der Index also 3 Werte (z.B: 1,2,3$),$ weil der Vektor 3 Komponenten hat: $\\vec{V}=\\left(V^{1}, V^{2}, V^{3}\\right)$.\n",
        "\n",
        "  * Ein Tensor vom Rang 2 hat 2 Indizes und stellt eine quadratische **Matrix** dar usw.\n",
        "\n",
        "  * Jede Tensor-Komponente kann eine Funktion oder eine Zahl sein. In der AR (allgemeinen Relativitatstheorie) sind die Komponenten in der Regel Funktionen der Raumzeit.\n",
        "\n",
        "* **Dimension eines Tensors**: Ein Vektor oder Tensor ist ein Objekt, welches Komponenten hat. Die Anzahl der Komponenten eines Vektors enstpricht der Dimension D des Raumes. Ein 3-dimensionaler Vektor besteht somit aus 3 Komponenten. Ein 3-dimensionaler Tensor der Stufe 2 besteht aus 3x3 Komponenten usw.\n",
        "\n",
        "> **Typ eines Tensors**: Wenn **Rang, Dimension und Komponenten-Arten** (Anzahl Indizes oben und unten) von Tensoren √ºbereinstimmen, dann sagt man, die Tensoren sind vom selben Typ. Dies muss bei der Tensor-Arithmetik beachtet werden.\n",
        "\n",
        "* **Tensor-Arithmetik**: Weil Skalare und Vektoren Subklassen von Tensoren sind ist zu erwarten, dass Tensoren denselben bekannten Rechenregeln f√ºr Addition, Subtraktion, Multiplikation und Division folgen.  Dies stimmt meistens, jedoch mit einigen √Ñnderungen und Einschr√§nkungen. Tensoren zeigen zudem neue Eigenschaften, die es bei Skalaren und Vektoren nicht gibt. [Source](http://walter.bislins.ch/physik/index.asp?page=Tensor%2DArithmetik)\n",
        "\n",
        "* **Tensor vs. Matrix**: A tensor of rank n is a mathematical object that has n indices and m$^n$ components. **Matrix is a tensor rank 2, because you need 2 basis vectors. Tensors rank 3 are the boxes with several stacked matrices**.\n",
        "\n",
        "  * Matrix is just and array of numbers, meanwhile a tensor (like stress tensor) obeys specific transformation rules and has a rules physical meaning. We can use a matrix to represent a tensor, but a tensor has a deeper physical significance. Matrix: I can just write down the matrix and its elements, but don‚Äôt have to add anything else\n",
        "\n",
        "  * Tensor: I need to specify the coordinate system, the components, and the basis vectors that each of those components correspond to. A tensor requires more detailed specification than a matrix. And it has these transformation properties where it‚Äôs invariant under a change of coordinate system, it has physical significance.\n",
        "\n",
        "* See video: [Introduction to Tensors](https://www.youtube.com/watch?v=uaQeXi4E7gA&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=1&t=187s)"
      ],
      "metadata": {
        "id": "icw4h44B9uTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoren und Koordinatensysteme**\n",
        "\n",
        "* In conventional math syntax we make **use of covariant indexes when dealing with Cartesian coordinate systems** $(x_{1},x_{2},x_{3})$ frequently without realizing this is a limited use of tensor syntax as covariant indexed components.\n",
        "\n",
        "* [Curvilinear coordinate systems](https://en.wikipedia.org/wiki/Curvilinear_coordinates), such as **cylindrical or spherical coordinates**, are often used in physical and geometric problems. Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and **covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another**.\n",
        "\n",
        "* In der Anwendung steht das Tensorsymbol immer f√ºr eine bestimmte Bedeutung. Zum Beispiel den Ort eines Teilchens. Der Ort kann in verschiedenen Koordinatensystemen gemessen werden. Entsprechend haben die Komponenten des Tensors in jedem Koordinatensystem andere Werte. Doch der Tensor bleibt derselbe (zB L√§nge eines Vektors), egal in welchem Koordinatensystem er gemessen wird.\n",
        "\n",
        "* **Gradient Vector**: Tensor calculus provides a generalization to the gradient vector formula from standard calculus **that works in all coordinate systems**: $\\nabla F=\\nabla_{i} F \\vec{Z}^{i}$ where: $\\nabla_{i} F=\\frac{\\partial F}{\\partial Z^{i}}$. In contrast, for standard calculus, the gradient vector formula is dependent on the coordinate system in use (example: Cartesian gradient vector formula vs. the polar gradient vector formula vs. the spherical gradient vector formula, etc.). In standard calculus, each coordinate system has its own specific formula, unlike **tensor calculus that has only one gradient formula that is equivalent for all coordinate systems**. This is made possible by an understanding of the metric tensor that tensor calculus makes use of."
      ],
      "metadata": {
        "id": "XavAxkWY9wMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covariance and Contravariance**\n",
        "\n",
        "> In multilinear algebra and tensor analysis, [covariance and contravariance](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors) describe how the quantitative description of certain geometric or physical entities changes with a [change of basis](https://en.wikipedia.org/wiki/Change_of_basis).\n",
        "\n",
        "* **The reason of having covariant or contravariant tensors is because you want to represent the same thing in a different coordinate system. Such a new representation is achieved by a transformation using a set of partial derivatives. In tensor analysis, a good transformation is one that leaves invariant the quantity you are interested in.**\n",
        "\n",
        "* Man unterscheidet in der [Tensor notation](https://en.wikipedia.org/wiki/Tensor_calculus#Syntax) bei Tensoren zwei Arten von Komponenten (*Jeder Tensor kann sowohl in kontravarianten, als auch in kovarianten Komponenten dargestellt werden*):\n",
        "\n",
        "  * **contravariant** upper component index: $v^{i}$\n",
        "\n",
        "  * **covariant** lower component index: $v_{i}$\n",
        "\n",
        "* [**Mixed Tensors**](https://en.wikipedia.org/wiki/Mixed_tensor) with both covariant and contravariant (for Tensors rang 2 or higher): $T^{m}{ }_{n}$. Consider related tensors: $T_{\\alpha \\beta \\gamma}, T_{\\alpha \\beta}^{\\gamma}, T_{\\alpha}{ }^{\\beta}{ }_{\\gamma}, T_{\\alpha}{ }^{\\beta \\gamma}, T_{\\beta \\gamma}^{\\alpha}, T^{\\alpha}{ }_{\\beta}^{\\gamma}, T^{\\alpha \\beta}{ }_{\\gamma}, T^{\\alpha \\beta \\gamma}$. The first one is covariant, the last one contravariant, and the remaining ones mixed.\n",
        "\n",
        "  * A mixed tensor of type or valence $\\left(\\begin{array}{l}M \\\\ N\\end{array}\\right),$ also written \"type $(M, N)$\" which has $M$ contravariant indices and $N$ covariant indices can be defined as a linear function which maps an $(M+N)$ -tuple of $M$ one-forms and $N$ vectors to a scalar.\n",
        "\n",
        "  * a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the [tangent bundle](https://en.wikipedia.org/wiki/Tangent_bundle) as well as the [cotangent bundle](https://en.wikipedia.org/wiki/Cotangent_bundle)\n",
        "\n",
        "* **Pushforwards (covariant) & Pullbacks (contravariant)**:\n",
        "\n",
        "  * Vectors with upper-index objects have [pushforwards](https://de.m.wikipedia.org/wiki/Pushforward), which are covariant.\n",
        "\n",
        "  * Covectors with lower-index objects have [pullbacks](https://de.m.wikipedia.org/wiki/R√ºcktransport), which are contravariant.\n",
        "\n",
        "  * In category theory covariance and contravariance are properties of [functors](https://en.m.wikipedia.org/wiki/Functor). Sometimes, contravariant functors are called [\"cofunctors\"](https://en.m.wikipedia.org/wiki/Functor).\n",
        "\n",
        "* Der Wechsel von einem Tensor in kontravarianter Darstellung zu einem Tensor in kovarianter Darstellung, wird [**\"Index ziehen\" oder Index Manipulation**](http://walter.bislins.ch/physik/index.asp?page=Index%2DManipulation+per+Metrik%2DTensor) genannt. Die Umrechnung geht durch Multiplikation mit dem sog. **Metrischen Tensor**.\n",
        "\n",
        "  * A given **contravariant index of a tensor can be lowered using the [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor)** $g_{\\mu v}$ and a given covariant index can be raised using the inverse metric tensor $g^{\\mu v}$. Thus, $g_{\\mu v}$ could be called the index lowering operator and $g^{\\mu v}$ the index raising operator.\n",
        "\n",
        "* On a manifold, a tensor field will typically have multiple, upper and lower indices, where [Einstein notation](https://en.m.wikipedia.org/wiki/Einstein_notation) is widely used. * Tensors notation allows a vector $(\\vec{V})$ to be decomposed into an [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation) representing the [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) of a [basis vector](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) $\\left(\\vec{Z}_{i}\\right.$ or $\\left.\\vec{Z}^{i}\\right)$ with a component vector $\\left(V_{i}\\right.$ or $\\left.V^{i}\\right)$: $\\vec{V}=V^{i} \\vec{Z}_{i}=V_{i} \\vec{Z}^{i}$.\n",
        "\n",
        "* *Exkurs: In einem engeren Wortsinn bezeichnet kovariant in der mathematischen Physik Gr√∂√üen, **die wie Differentialformen transformieren**. Diese kovarianten Gr√∂√üen $P$ bilden einen Vektorraum $\\mathcal{V}$, auf dem eine Gruppe von linearen Transformationen wirkt. Die Menge der linearen Abbildungen der kovarianten Gr√∂√üen in die reellen Zahlen $Q: P \\mapsto Q(P) \\in \\mathbb{R}, \\quad Q(a P+b \\tilde{P})=a Q(P)+b Q(\\tilde{P})$ bildet den zu $\\mathcal{V}$ dualen Vektorraum $\\mathcal{V}^{*}$. Schreiben wir die transformierten, kovarianten Gr√∂√üen $P^{\\prime}$ mit einer Matrix $\\Lambda$ als $P^{\\prime}=\\Lambda P$ dann definiert $Q^{\\prime}\\left(P^{\\prime}\\right)=Q(P)$ das kontravariante oder kontragrediente Transformationsgesetz des Dualraumes $Q^{\\prime}=\\Lambda^{-1 \\mathrm{~T}} Q$. Wegen $\\left(\\Lambda_{2}\\right)^{-1 \\mathrm{~T}}\\left(\\Lambda_{1}\\right)^{-1 \\mathrm{~T}}=\\left(\\Lambda_{2} \\Lambda_{1}\\right)^{-1 \\mathrm{~T}}$ gen√ºgt die kontravariante Transformation derselben Gruppenverkn√ºpfung wie die kovariante Transformation.*\n",
        "\n",
        "* Quellen: [Ko- und kontravariante Darstellung](https://www.math.tugraz.at/~ganster/lv_vektoranalysis_ss_10/20_ko-_und_kontravariante_darstellung.pdf), [Kovarianz und Kontravarianz von Vektoren](http://walter.bislins.ch/physik/index.asp?page=Kovarianz+und+Kontravarianz+von+Vektoren), [Kovariante und Kontravariante Komponenten](http://walter.bislins.ch/physik/index.asp?page=Kovariante+und+Kontravariante+Komponenten). Siehe auch Video: [Tensorrechnung, 3.1 : Was bedeutet Kovariant und kontravariant?](https://www.youtube.com/watch?v=PNRoBOzije8) und [Kovarianz, Kontravarianz, Tensoren](https://av.tib.eu/media/19916)\n"
      ],
      "metadata": {
        "id": "g1CsjpEU90rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein beliebiger Vektor kann als Linearkombination entweder der kovarianten Basisvektoren oder der kontravarianten Basisvektoren dargestellt werden** (die Transformationen invers zueinander).\n",
        "\n",
        "* **Kovariante Basisvektoren** $\\vec{b}_{u}$ sind kombiniert mit **kontravariante Koordinaten** $a_{u_{i}}$. Heisst auch kontravarianter Vektor (besser: kontravarianter Koordinatenvektor) und sind Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "* **Kontravariante Basisvektoren** $\\vec{b}_{u_{i}}^{*}$ kombiniert mit **kovarianten Koordinaten** $a_{u_{i}}^{*}$. Heisst auch kovarianter Vektor bzw. Covector (Kovektoren als Linearformen von Normalenvektoren) und sind normal zu den Koordinatenfl√§chen.\n",
        "\n",
        "* **Vektordarstellungen**: $\\vec{a}=\\sum_{i=1}^{n} a_{u_{i}} \\vec{b}_{u_{i}}=\\sum_{i=1}^{n} a_{u_{i}}^{*} \\vec{b}_{u_{i}}^{*}$\n",
        "\n",
        "* Wenn $\\vec{g}_{i} \\cdot \\vec{g}^{j}=\\delta_{i}^{j}$ (**Kronecker Delta / Skalarprodukt ist Null**) dann heissen $\\left\\{\\overrightarrow{g_{i}}\\right\\}$ und $\\left\\{\\overrightarrow{g^{j}}\\right\\}$ **reziprok** zueinander. Die beiden Klassen von Basisvektoren sind dual bzw. reziprok zueinander (siehe [duale Basis](https://de.wikipedia.org/wiki/Duale_Basis)). Diese beiden Basen bezeichnet man als **holonome Basen**.\n",
        "\n",
        "* Diese kreuzweise Paarung (kontra-ko bzw. ko-kontra) sorgt daf√ºr, dass der Vektor $\\vec{a}$ unter Koordinatentransformation invariant ist (zB Geschwindigkeit eines Teilchens), da die Transformationen von Koordinaten und Basisvektoren invers zueinander sind und sich gegenseitig aufheben."
      ],
      "metadata": {
        "id": "51tAOvER93PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Vectors): Basiswechsel und Vector-(Coordinate)-Transformation (Contravariant)*"
      ],
      "metadata": {
        "id": "VkuVX0zf959A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vektorkoordinaten verhalten sich kontravariant**\n",
        "\n",
        "* **Problem**: For example, we consider the transformation from one coordinate system $x^{1}, \\ldots, x^{n}$ to another $x^{\\prime}, \\ldots, x^{\\prime n}$\n",
        "$x^{i}=f^{i}\\left(x^{\\prime}, x^{\\prime 2}, \\ldots, x^{\\prime n}\\right)$ where $f^{i}$ are certain functions.\n",
        "Take a look at a couple of specific quantities. How do we transform coordinates? The answer is \"**coordinate differentials**\":\n",
        "\n",
        "> $d x^{i}=\\frac{\\partial x^{i}}{\\partial x^{\\prime k}} d x^{\\prime k}$\n",
        "\n",
        "* **Darstellung**: $c^{1}$ (upper index, hochgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Vektorkoordinaten verhalten sich kontravariant. (wahrscheinlich: kovariante Basisvektoren (= \"kontravarianten Vektor\", weil Koordinaten kontravariant): Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da, wie oben beschrieben, die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "  * Every quantity which under a transformation of (vector) coordinates, transforms like the coordinate differentials is called a contravariant tensor.\n",
        "\n",
        "  * **Vectors exhibit a behavior of changing scale inversely to changes in scale to the reference axes are called contravariant** (see second example below). As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).\n",
        "\n",
        "* **Examples**: (of contravariant vectors / vectors with contravariant components)\n",
        "\n",
        "  * position of an object relative to an observer, or any derivative of position with respect to time: velocity, acceleration, jerk, displacement, momentum, force.\n",
        "\n",
        "  * For instance, by changing scale from meters to centimeters (that is, **dividing the scale of the reference axes by 100**), the components of a measured velocity vector are **multiplied by 100**.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * In physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem.\n",
        "\n",
        "  * A contravariant vector or tangent vector (often abbreviated simply as vector, such as a direction vector or velocity vector) has components that contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be contravariant.\n",
        "\n",
        "  * If the **reference axes** were rotated in one direction, the **component representation** of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way. In Einstein notation, contravariant components are denoted with upper indices as in $\\mathbf{v}=v^{i} \\mathbf{e}_{i}$ (note: implicit summation over index \"i\")\n",
        "\n",
        "https://math.stackexchange.com/questions/8170/intuitive-way-to-understand-covariance-and-contravariance-in-tensor-algebra\n",
        "\n",
        "*Kontravariantes Transformationsverhalten*\n",
        "\n",
        "* Grundfrage: Wie werden Normalenvektoren in den alten Basisvektoren zu Normalenvektoren in neuen Basivektoren?\n",
        "\n",
        "* Die normalen Vektoren gehen gegen die Basis, d.h. kontravariant\n",
        "\n",
        "* Gegeben einen Normalenvektor a aus dem Vektorraum V, und diesen zerlegen in die Basisvektoren in alter Basis und dann transformieren in neue Basis:\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_03.png)\n",
        "\n",
        "Nun Kovektor angewandt auf Summe von Zahlen mal Vektoren (Vektoren sind kontravariant):\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_04.png)\n",
        "\n",
        "> **Man nennt a einen \"kontravarianten\" Vektor, aber eigentlich muss man sagen, dass seine Komponenten kontravariant transformieren. Die eigentlichen Vektoren aus dem Originalvektorraum sind kontravariant.**\n",
        "\n",
        "\n",
        "* and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates\n",
        "\n",
        "* The inverse of a covariant transformation is a **contravariant transformation**.\n",
        "\n",
        "  * Whenever a vector should be invariant under a change of basis, that is to say **it should represent the same geometrical or physical object <u>having the same magnitude and direction as before</u>, its components must transform according to the contravariant rule**.\n",
        "\n",
        "  * Conventionally, indices identifying the components of a vector are placed as upper indices and so are all indices of entities that transform in the same way.\n",
        "\n",
        "* The sum over pairwise matching indices of a product with the same lower and upper indices are invariant under a transformation.\n",
        "\n",
        "* A vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector $\\mathbf{v}$ is given, say, in components $V^{\\prime}$ on a chosen basis $\\mathbf{e}_{i}$. On another basis, say $\\mathbf{e}^{\\prime}{ }_{j}$, the same vector $\\mathbf{v}$ has different components $v^{\\prime j}$ and\n",
        "\n",
        "> $\n",
        "\\mathbf{v}=\\sum_{i} v^{i} \\mathbf{e}_{i}=\\sum_{j} v^{\\prime j} \\mathbf{e}_{j}^{\\prime}\n",
        "$\n",
        "\n",
        "* As a vector, $\\mathbf{v}$ should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its \"real world\" direction and magnitude should appear the same regardless of the basis vectors.\n",
        "\n",
        "* If we perform a change of basis by transforming the vectors $\\mathbf{e}_{i}$ into the basis vectors $\\mathbf{e}_{j}$, we must also ensure that the components $v^{i}$ transform into the new components $v$ to compensate.\n",
        "\n",
        "* The needed transformation of $\\mathbf{v}$ is called the contravariant\n",
        "transformation rule."
      ],
      "metadata": {
        "id": "4bkcyHaG-NV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation: Forward and Backward Transform between Basis Vectors in 2 dimensions**\n",
        "\n",
        "*Forward: build the new basis vectors (not any vector!) from the old basis vectors*:\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "\\text { Old Basis: }\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\\\\n",
        "\\text { New Basis: }\\left\\{\\widetilde{e_{1}}, \\widetilde{e_{2}}\\right\\}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_08.png)\n",
        "\n",
        "* Forward transformation (create new basis vectors):\n",
        "\n",
        "$\n",
        "\\begin{array}{ll}\n",
        "\\widetilde{e_{1}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Which gives us for the image above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}=-1 / 2 \\overrightarrow{e_{1}}+1 / 4 \\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Result is in the forward matrix (Notice how the coefficient from the first equation end up in the first column (2 and 1), and how the coefficients from the second equation end up in the second column)\n",
        "\n",
        "$\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\overrightarrow{e_{1}}=1 / 4 \\widetilde{e_{1}}+(-1) \\widetilde{\\overrightarrow{e_{2}}} \\\\\n",
        "\\overrightarrow{e_{2}}=1 / 2 \\widetilde{e_{1}}+2 \\widetilde{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Which gives as the backward matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in 2 dimensions?**\n",
        "\n",
        "* How do forward and backward matrices relate to each other?\n",
        "\n",
        "* They are inverses, and matrix multiplication (dot product) leads to the identity matrix:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "F B &=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "(2*1/4)+(-1/2*-1) & (2*1/2)+(-1/2*2) \\\\\n",
        "(1*1/4)+(1/4*-1) & (1*1/2)+(1/4*2)\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "> This means: $\n",
        "B=F^{-1}\n",
        "$\n",
        "\n",
        "*Reminder: Matrix Multiplication Rules:*\n",
        "\n",
        "$\\begin{aligned}\n",
        "&=\\left[\\begin{array}{cc}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "e & f \\\\\n",
        "g & h\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "ae+bg & af+bh \\\\\n",
        "ce+dg & cf+dh\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Summary of two transformation matrices between old basis and new basis:**\n",
        "\n",
        "> Forward Matrix: $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> Backward Matrix: $\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_09.png)"
      ],
      "metadata": {
        "id": "3Xgqkmin-PcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and Backward Transform to n dimensions (Generalization)**\n",
        "\n",
        "First construct the new basis vectors from the old ones with right coefficients:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=F_{11} \\overrightarrow{e_{1}}+F_{21} \\overrightarrow{e_{2}}+\\cdots+F_{n 1} \\overrightarrow{e_{n}} \\\\\n",
        "\\widetilde{e_{2}}={F_{12} \\overrightarrow{e_{1}}+F_{22} \\overrightarrow{e_{2}}+\\cdots+F_{n 2} \\overrightarrow{e_{n}}}\\\\\n",
        "{\\ldots} \\\\\n",
        "\\widetilde{e_{n}}=F_{1 n} \\overrightarrow{e_{1}}+F_{2 n} \\overrightarrow{e_{2}}+\\cdots+F_{n n} \\overrightarrow{e_{n}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $F$ (notice again **how it's transposed to the coefficients above**!)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "F_{11} & F_{12} & \\ldots & F_{1 n} \\\\\n",
        "F_{21} & F_{22} & \\ldots & F_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "F_{n 1} & F_{n 2} & \\ldots & F_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "**How to facilitate it without writing all these equations above?**\n",
        "\n",
        "Let‚Äôs take an example: in the following equation taken from the table above (not the coefficient matrix, because there the values are transposed!):\n",
        "\n",
        "$F_{12}$ tells us how much of $\\overrightarrow{e_{1}}$ is in $\\widetilde{e_{2}}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{e_{2}}=F_{12} \\overrightarrow{e_{1}}\n",
        "\\end{equation}$\n",
        "\n",
        "**So what I want is $\\sum F_{k j}$ that tells us how much of $\\overrightarrow{e_{k}}$ makes up $\\widetilde{{e}_{j}}$**:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Exkurs: later we will use the Einstein notation and drop the summation sign to make it easier:**\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        " Now we do the same for the backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=B_{11} \\widetilde{e_{1}}+B_{21} \\widetilde{e_{2}}+\\cdots+B_{n 1} \\widetilde{e_{n}} \\\\\n",
        "\\overrightarrow{e_{2}} &=B_{1 2} \\widetilde{e_{1}}+B_{22} \\widetilde{e_{2}}+\\cdots+B_{n 2} \\widetilde{e_{n}} \\\\\n",
        "{\\ldots} \\\\\n",
        "\\overrightarrow{e_{n}} & =B_{1 n} \\widetilde{e_{1}}+B_{2 n} \\widetilde{e_{2}}+\\cdots+B_{n n} \\widetilde{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $B$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "B_{11} & B_{12} & \\ldots & B_{1 n} \\\\\n",
        "B_{21} & B_{22} & \\ldots & B_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "B_{n 1} & B_{n 2} & \\ldots & B_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "Summarize the backward transformation as well:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Now both together forward and backward transformation:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "* The index that we do the summation over (here: k = 1 and j = 1 under the summation sign) is the first letter of the forward or backward transform and **corresponds to the index of the basis vector that we‚Äôre transforming**.\n",
        "\n",
        "* The second index of the transform (j and i at F, B and e) **corresponds to the output basis vector**\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in n dimensions?**\n",
        "\n",
        "How are they related to each other? (once again proving)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Replacing with following at the last term above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\stackrel{\\sim}{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "And you get:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging summation signs:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k}\\left(\\sum_{j} F_{k j} B_{j i}\\right) \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "So now we build the old basis vectors $\\overrightarrow{e_{i}}$ using the summation of the old basis vectors $\\overrightarrow{e_{k}}$\n",
        "\n",
        "Now the following middle part should be an identity matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ = $I$\n",
        "\n",
        "so that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Which means then that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=\\overrightarrow{e_{1}} \\\\\n",
        "\\overrightarrow{e_{2}} &=\\overrightarrow{e_{2}} \\\\\n",
        "& \\vdots \\\\\n",
        "\\overrightarrow{e_{n}} &=\\overrightarrow{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "In order to achieve that we want for this part the following conditions:\n",
        "\n",
        "for this $\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ we want:\n",
        "\n",
        "* it is 1 when i = k\n",
        "\n",
        "* it is 0 when i ‚â† k\n",
        "\n",
        "because that will give us the identity matrix (because 1 is where row is equal to the columns, so i = k, and all else 0):\n",
        "\n",
        "$\\begin{equation}\n",
        "I_{n}=\\left[\\begin{array}{ccccc}\n",
        "1 & 0 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 0 & 1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\cdots & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "So for this operation we have the Kronecker Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\sum_{j} F_{k j} B_{j i}=\\delta_{i k}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=k \\\\\n",
        "0 \\text { if } i \\neq k\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n"
      ],
      "metadata": {
        "id": "7HbfIgCM-RXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Transformation Rules (Contravariant)**\n",
        "\n",
        "* **How can you transform from one basis to the other using the vector components (and not the basis vectors)?**\n",
        "\n",
        "* Now one could apply the forward matrix to the vector components of the old basis $\\overrightarrow{e_{i}}$ and should get the vector components of the new basis $\\widetilde{e_{i}}$ with\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{\\overrightarrow{e_{i}} (old vector components)} \\quad\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]_{\\widetilde{e_{i}} (new  vector components)}\n",
        "\\end{equation}$\n",
        "\n",
        "* We do Forward Transform in Basis Vectors and Backward Transform in Vector Components to get from old to new vector basis!\n",
        "\n",
        "* Achtung: In this case (=for vector components) you need to apply the backward transformation to get from the old components to the new components!.\n",
        "**The reason is: for basis vectors, forward brings us from old to new, and backward from new to old.** But with vector components it‚Äôs the opposite.\n",
        "This will be important for understanding covariance & contravariance!\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "  * So, we take the vector components of the old basis: $\\begin{equation}\n",
        "B\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "  * Then we multiply it with our backward transformation matrix: $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "0.25 & 0.5 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "  * Making a few transformations: $\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "{\\left[\\begin{array}{l}\n",
        "0.25(1)+0.5(1.5) \\\\\n",
        "(-1)(1)+2(1.5)\n",
        "\\end{array}\\right]} \\\\\n",
        "{\\left[\\begin{array}{c}\n",
        "0.25+0.75 \\\\\n",
        "-1+3\n",
        "\\end{array}\\right]}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "  * And the result are the correct vector components of the new basis: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> **Be aware of difference between changes with basis vectors (forward from old to new, backward from new to old) and vector components (forward from new to old, backward from old to new):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_10.png)\n",
        "\n",
        "**Why does this make perfect sense?**\n",
        "\n",
        "* Look at this example: the vector components on the left (original) are [1, 1],\n",
        "\n",
        "* and the size of the new basis vectors $\\widetilde{e_{1}}$ is just double of the old one: $\\begin{equation}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}\n",
        "\\end{equation}$ as well as $\\begin{equation}\n",
        "\\widetilde{e_{2}}=2 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$.\n",
        "\n",
        "* This corresponds to a forward transformation matrix for the basis vector $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ to get the new basis vector\n",
        "\n",
        "* the vector components of the vector $\\vec{v}$ are then in the new coordinate system half: [0.5, 0.5], which makes sense.\n",
        "\n",
        "* **This is because (as said in the beginning) the <u>length and the direction of a vector should never change</u> (it's invariant!), but the vector component can change and in this case they change opposite to the change of the basis vectors to keep the vectors stable as they were in both bcoordinate systems!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_11.png)\n",
        "\n",
        "> **Learning: Vector components behave the opposite way that basis vectors do !**\n",
        "\n",
        "* Similar, **if you rotate the basis vector clockwise, the vector components of the vector $\\vec{v}$ rotate anti-clockwise**.\n",
        "\n",
        "* But remember: $\\vec{v}$ has not moved, just its components changed opposite to the basis vector components!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_12.png)"
      ],
      "metadata": {
        "id": "hPW8cblf-TWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So we have two ways of writing a vector $\\vec{v}$:**\n",
        "\n",
        "  * **a linear combination of the old basis vectors with the coefficient being the old components,**\n",
        "\n",
        "  * **or we can write it as a linear combination of the new basis vectors with the coefficient being the new components**.\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=v_{1} \\overrightarrow{e_{1}}+v_{2} \\overrightarrow{e_{2}}+\\cdots+v_{n} \\overrightarrow{e_{n}}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "as well as:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\widetilde{v_{1}} \\widetilde{{e_{1}}}+\\widetilde{v_{2}} \\widetilde{e_{2}}+\\cdots+\\widetilde{v_{n}} \\widetilde{e_{n}}=\\sum_{j=1}^{n} \\widetilde{v_{j}} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "So both if these summations are equal to $\\vec{v}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "Let‚Äôs bring out the forward and backward transformations at this point.\n",
        "\n",
        "* Forward: $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{i=1}^{n} F_{i j} \\overrightarrow{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward: $\\begin{equation}\n",
        "\\overrightarrow{e_{j}}=\\sum_{i=1}^{n} B_{i j} \\stackrel{\\sim}{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "If we take the old basis and the old components, what we can do is we can use the backward transformation B to replace the old basis vectors\n",
        "\n",
        "> $\\overrightarrow{e_{j}}$ with the new basis vectors from B: $\\begin{equation}\n",
        "=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{j=1}^{n} v_{j}\\left(\\sum_{i=1}^{n} B_{i j} \\widetilde{{e}_{i}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging the summation we get this:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right) \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Now we have $\\vec{v}$ written as a summation of the new basis vectors**.\n",
        "\n",
        "Nut now, let‚Äôs have a look at the middle part of the equation above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "If we compare that to our summation in the step before when we wrote $\\vec{v}$ as a summation of new basis vectors:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "the coefficients have to be the new components $\\widetilde{v}_{i}$. And indeed $\\widetilde{v}_{i}$ equals:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{v}_{i}=\\sum_{j=1}^{n} B_{i j} v_{j}\n",
        "\\end{equation}$\n",
        "\n",
        "we actually used the backward transformation! So, **this is the proof that to move from the old components to the new components we actually used the backwards transformation!**\n",
        "\n",
        "**Summary**: We know how basis vectors transform and how vector components transform (opposite direction).\n",
        "\n",
        "* Now because vector components behave contrary to the basis vectors we say that vector components are contra variant!\n",
        "\n",
        "* And we see later that vectors are contra variant tensors!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_13.png)\n",
        "\n",
        "**This means also we make a small but important change:**\n",
        "\n",
        "Here you can see we wrote the vector $\\vec{v}$ as a linear combination of the old and the new basis:\n",
        "\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v_{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "But since the vector components $v_{i}$ and $\\widetilde{v}_{i}$ behave contra variant we put the index up $v^{i}$ and $\\widetilde{v}^{i}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v^{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}^{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "t8PMvuxn-Vdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Linear Forms): Basiswechsel und Covector-(Coordinate)-Transformation (Covariant)*"
      ],
      "metadata": {
        "id": "Eg2UlbUr-YCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Rules:**\n",
        "\n",
        "* Vector Space $V$ to get from old to new: **Forward Transform in Basis Vectors (covariant) and Backward Transform in Vector Components (contravariant)**\n",
        "\n",
        "* Vector Space $V*$ to get from old to new: **Backward Transform in Basis Vectors (contravariant) and Forward Transform in Vector Components (covariant)**\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "\\overrightarrow{e_{i}} \\text{Old Basis V with Vectors} \\quad \\vec{v} &  \\widetilde{e_{i}} \\text{New Basis V with Vectors} \\quad \\widetilde{v}\\\\\n",
        "\\overrightarrow{\\epsilon_{i}} \\text{Old Dual Basis V* with Covectors} \\quad \\vec{\\alpha} & \\widetilde{\\epsilon_{i}} \\text{New Dual Basis V* with Covectors} \\quad \\widetilde{\\alpha}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "s9ikXvt0-iN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* covectors / linear forms, like scalars (for example differentials), change with basis (=covariant)\n",
        "\n",
        "* change of coordinate system does not change scalar (like temperature), hence linear forms =scalars) bechave covariant with basis (meanwhile direction in vectors change contravariant to keep the original position of a vector)\n",
        "\n",
        "* **All covectors can be written as the linear combination of the dual basis vectors!**\n",
        "\n",
        "* **Covector components can be obtained by counting how many covector lines that the basis vector pierces**\n",
        "\n",
        "* **Covector components transform in the opposite way that vector components do**\n",
        "\n",
        "* Videos:\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 5: Covector Components ](https://www.youtube.com/watch?v=rG2q77qunSw&t=10s)\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 6: Covector Transformation Rules](https://www.youtube.com/watch?v=d5da-mcVJ20&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=9)\n",
        "\n",
        "\n",
        "**Fun facts**\n",
        "\n",
        "* Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!** We need Epsilon $\\epsilon^{n}$ as dual basis.\n",
        "\n",
        "* **Basis Vector in $V$ - Upper-left**: we started with the transformation rules for basis vectors = covariant: from old to new with the forward transform.\n",
        "\n",
        "* **Vector Components in $V$ - Bottom-left**: The transformation rules for vector components are the opposite compared to the basis vectors (from old to new with the backward transform). So they are contra variant.\n",
        "\n",
        "* **Basis Covector in $V*$ - Upper-right (Dual Space)**: Transformation rules for basis covectors are also opposite compared to basis vectors. So basis covectors also transform by the contra variant rule.\n",
        "\n",
        "\n",
        "* **Covector Components in $V*$ - Bottom-right (Dual Space)**: Covector components transform in the same way that basis vectors do = This means that covectors transform covariantly. (and contravariant to vectors components)\n",
        "\n",
        "> **Wie man sieht: man konstruiert (neue) basisvektoren aus (alten) basisvektoren und (neue) vektorkomponenten aus (alten) vektorkomponenten**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_18.png)"
      ],
      "metadata": {
        "id": "WATiQxEx-kZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does it mean when we say the covector has components?**\n",
        "\n",
        "* Look at this example: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}} \\text { = } 2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$\n",
        "\n",
        "* So when we write a **column** vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* What we mean is: This vector is given by the **linear combination of** $2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}$\n",
        "\n",
        "> **It tells you how much of each basis vector is needed to make the vector**\n",
        "\n",
        "**So what exactly are we measuring with when we write [2 1] ?** - **2 of what and 1 of what?**\n",
        "\n",
        "* Remember: Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!**\n",
        "\n",
        "\n",
        "**Epsilon $\\epsilon^{n}$ as dual basis**\n",
        "\n",
        "Take the basis $\\begin{equation}\n",
        "\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\text { for } V \\text { . }\n",
        "\\end{equation}$\n",
        "\n",
        "We introduce two special covectors: $\\epsilon^{1},\\epsilon^{2}: V \\rightarrow \\mathbb{R}$, which are both functions from vectors to numbers (notice how the labels 1 and 2 are now above instead of below).\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0 \\\\\n",
        "\\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)=0 & \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=1\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "(Hint: kovariant basis from $V$ with kontravariant basis from $V*$ equals the identity)\n",
        "\n",
        "this means we can use the Kronecker-Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{i j}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=j \\\\\n",
        "0 \\text { if } i \\neq j\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n",
        "\n",
        "**So what does this epsilon $\\epsilon$ mean?**\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{1}(\\vec{v})=\\epsilon^{1}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "We can rewrite this like this (this addition and scaling can be brought outside the function since it's a linear combination):\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} \\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "From the previous part we know that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "So we can rewrite the equation above as:\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} * 1 +v^{2} * 0\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{1} = \\epsilon^{1}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "And the same goes for $\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})=\\epsilon^{2}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)=v^{1} \\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=v^{2}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{2} = \\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "> The epsilons are projecting out vector components\n",
        "\n",
        "*Directions of epsilon basis vectors*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_15.png)\n",
        "\n",
        "See image below: Write a general covector alpha $\\alpha$ (which can be any covector of our choice) as a linear combination of the epsilon covectors.\n",
        "\n",
        "**This means that epsilon covectors form a basis for the set of all covectors!**\n",
        "\n",
        "> **And for that reason we call these epsilons the dual basis, because they are basis for the dual space $V*$.**\n",
        "\n",
        "* **We can't just flip column vectors on their side to get the row vectors! This works only in an orthonormal basis**.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_16.png)"
      ],
      "metadata": {
        "id": "1Fkz_0Ra-mfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Die Koeffizienten der Linearformen verhalten sich kovariant**\n",
        "\n",
        "* **Problem**: How do we transform some scalar $\\Phi$ ? The answer is \"**derivatives of a scalar**\":\n",
        "\n",
        "> $\\frac{\\partial \\Phi}{\\partial x^{i}}=\\frac{\\partial \\Phi}{\\partial x^{\\prime k}} \\frac{\\partial x^{\\prime k}}{\\partial x^{i}}$\n",
        "\n",
        "* **Darstellung**: $x_{1}$ (lower index, tiefgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Koeffizienten der Linearformen (=zB Skalare) verhalten sich kovariant. (wahrscheinlich: kontravariante Basisvektoren (= \"kovarianten Vektor\", weil Koordinaten kovariant): Normal zu den Koordinatenfl√§chen)\n",
        "\n",
        "  * [Skalar](https://de.m.wikipedia.org/wiki/Skalar_(Mathematik)): In der Physik werden Skalare verwendet zur Beschreibung physikalischer Gr√∂√üen, die **richtungsunabh√§ngig** sind. Beispiele f√ºr skalare physikalische Gr√∂√üen sind die **Masse eines K√∂rpers, seine Temperatur, seine Energie und auch seine Entfernung von einem anderen K√∂rper** (als Betrag der Differenz der Ortsvektoren). Anders gesagt: Eine skalare physikalische Gr√∂√üe √§ndert sich bei √Ñnderungen der Lage oder Orientierung nicht. Wird hingegen f√ºr die vollst√§ndige Beschreibung der Gr√∂√üe eine Richtung ben√∂tigt, wie bei der Kraft oder der Geschwindigkeit, so wird ein Vektor verwendet, bei Abh√§ngigkeit von mehreren Richtungen ein Tensor (genauer: Tensor 2. oder noch h√∂herer Stufe).\n",
        "\n",
        "  * Every quantity which under a coordinate transformation, transforms like the derivatives of a scalar is called a covariant tensor.\n",
        "\n",
        "  * Covectors (also called **dual vectors**) typically have units of the inverse of distance or the inverse of distance with other units. The components of covectors **change in the same way as changes to scale of the reference axes and consequently are called covariant**.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * Koeffizienten der [Linearformen](https://de.wikipedia.org/wiki/Linearform) (=zB Skalare)\n",
        "\n",
        "  * An example of a covector is the gradient, which has units of a spatial derivative, or distance<sup>‚àí1</sup>.\n",
        "\n",
        "  * Examples of covariant vectors generally appear when taking a gradient of a function.\n",
        "\n",
        "* **Beispiele fur Kovarianz [in der Physik](https://de.wikipedia.org/wiki/Kovarianz_(Physik))**\n",
        "\n",
        "  * Unter Galilei-Transformationen transformieren sich die Beschleunigung und die Kraft in den newtonschen Bewegungsgleichungen im gleichen Sinne wie die Ortsvektoren. Daher sind die Newtonschen Bewegungsgleichungen und damit die klassische Mechanik kovariant bzgl. der Gruppe der Galilei-Transformationen.\n",
        "\n",
        "  * Im gleichen Sinne sind die Einstein-Gleichungen der Gravitation in der allgemeinen Relativit√§tstheorie kovariant unter beliebigen (nichtlinearen glatten) Koordinatentransformationen.\n",
        "\n",
        "  * Ebenso ist die Dirac-Gleichung der Quantenelektrodynamik kovariant unter der Gruppe der linearen Lorentz-Transformationen.\n",
        "\n",
        "  * Die linke Seite der Klein-Gordon-Gleichung f√ºr ein Skalarfeld √§ndert sich unter Lorentz-Transformationen nicht, sie ist spezieller invariant oder skalar.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * A covariant vector or cotangent vector (often abbreviated as covector) has components that co-vary with a change of basis.\n",
        "\n",
        "  * That is, the **components must be transformed by the same matrix as the change of basis matrix**. The components of covectors (as opposed to those of vectors) are said to be covariant.  In Einstein notation, covariant components are denoted with lower indices as in $\\mathbf{e}_{i}(\\mathbf{v})=v_{i}$\n",
        "\n",
        "  * A covariant vector has **components (=coordinates) that change oppositely to the coordinates or, equivalently,\n",
        "transform like the reference axes**.\n",
        "\n",
        "  * For instance, the components of the gradient vector of a function $\\nabla f=\\frac{\\partial f}{\\partial x^{1}} \\widehat{x}^{1}+\\frac{\\partial f}{\\partial x^{2}} \\widehat{x}^{2}+\\frac{\\partial f}{\\partial x^{3}} \\widehat{x}^{3}$\n",
        "transform like the reference axes themselves.\n",
        "\n",
        "  * See also [Covariant Transformation](https://en.wikipedia.org/wiki/Covariant_transformation)\n",
        "\n",
        "\n",
        "Die Komponenten eines Kovektors, eines Vektors aus dem Dualraum, transformieren kovariant.\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_05.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "amwdLqv5-oX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The derivative of a function transforms covariantly!** (and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates)\n",
        "\n",
        "* In physics, a [covariant transformation](https://en.wikipedia.org/wiki/Covariant_transformation) is a rule that specifies how certain entities, such as vectors or tensors, change under a change of basis.\n",
        "\n",
        "* The transformation that describes the new basis vectors as a linear combination of the old basis vectors is defined as a covariant transformation.\n",
        "\n",
        "* Conventionally, indices identifying the basis vectors are placed as lower indices and so are all entities that transform in the same way.\n",
        "\n",
        "*Covariant Derivative*\n",
        "\n",
        "* the [covariant derivative](https://en.wikipedia.org/wiki/Covariant_derivative) is a way of specifying **a derivative along tangent vectors of a manifold**.\n",
        "\n",
        "* The name is motivated by the importance of changes of coordinate in physics: the covariant derivative transforms covariantly under a general coordinate transformation, that is, linearly via the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the transformation.\n",
        "\n",
        "* The covariant derivative is a generalization of the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative) / [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) from vector calculus (ps: Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale R√§ume ist das [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential).)\n",
        "\n",
        "* The covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule,\n",
        "\n",
        "  * $\\nabla_{\\mathbf{u}} \\mathbf{v},$ which takes as its inputs:\n",
        "\n",
        "  * (1) a vector, $\\mathbf{u},$ defined at a point $P$, and\n",
        "\n",
        "  * (2) a vector field, $\\mathbf{v}$, defined in a neighborhood of $P$\n",
        "\n",
        "  * The output is the vector $\\nabla_{\\mathbf{u}} \\mathbf{v}(P)$, also at the point $P$.\n",
        "\n",
        "*Covariant derivative and covariant transformation*\n",
        "\n",
        "* The primary difference from the usual directional derivative is that $\\nabla_{\\mathrm{u}} \\mathrm{v}$ must, in a certain precise sense, **be independent of the manner in which it is expressed in a coordinate system**.\n",
        "\n",
        "* A vector may be described as a list of numbers in terms of a basis, **but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis**.\n",
        "\n",
        "* This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation.\n",
        "\n",
        "> **The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: <u>the covariant derivative must change by a covariant transformation</u>.**\n",
        "\n",
        "*In the Euclidean Space*\n",
        "\n",
        "* In the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points. In such a system one translates one of the vectors to the origin of the other, keeping it parallel.\n",
        "\n",
        "* **With a Cartesian (fixed orthonormal) coordinate system \"keeping it parallel\" amounts to keeping the components constant**.\n",
        "\n",
        "* Euclidean space provides the simplest example: a covariant derivative which is obtained by taking the ordinary directional derivative of the components in the direction of the displacement vector between the two nearby points.\n",
        "\n",
        "*In other (more general) Spaces*\n",
        "\n",
        "* In the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in **polar coordinates** in a two dimensional Euclidean plane, **then it contains extra terms that describe how the coordinate grid itself \"rotates\"**.\n",
        "\n",
        "* **In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves**, etc.\n",
        "\n",
        "* **In this case \"keeping it parallel\" does NOT amount to keeping components constant under translation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "u2amkyDK-qT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/covariant_transformation.png)\n",
        "\n",
        "* In the shown example, a vector $\\mathbf{v}=\\sum_{i \\in\\{x, y\\}} v^{i} \\mathbf{e}_{i}=\\sum_{j \\in\\{r, \\phi\\}} v^{\\prime j} \\mathbf{e}_{j}^{\\prime} .$ is described by two different coordinate\n",
        "systems:\n",
        "  * a rectangular coordinate\n",
        "system (the black grid),\n",
        "  * and a radial\n",
        "coordinate system (the red grid).\n",
        "\n",
        "* Basis vectors have been chosen for both\n",
        "coordinate systems: $\\mathbf{e}_{x}$ and $\\mathbf{e}_{\\mathbf{y}}$ for the rectangular coordinate system, and $\\mathbf{e}_{\\mathrm{r}}$\n",
        "and $\\mathbf{e}_{\\phi}$ for the radial coordinate system.\n",
        "\n",
        "* The radial basis vectors $\\mathbf{e}_{\\mathrm{r}}$ and $\\mathbf{e}_{\\phi}$ appear\n",
        "rotated anticlockwise with respect to the\n",
        "rectangular basis vectors $\\mathbf{e}_{\\mathrm{x}}$ and $\\mathbf{e}_{\\mathrm{y}}$.\n",
        "\n",
        "> **The covariant transformation, performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors**.\n",
        "\n",
        "* The coordinates of $v$ must be transformed into the new coordinate system, **but the vector $v$ itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates**.\n",
        "\n",
        "* The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view $v$ from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors $\\mathbf{e}_{\\mathbf{r}}$ and $\\mathbf{e}_{\\Phi}$. compared to how it appeared relative to the rectangular basis vectors $\\mathbf{e}_{x}$ and $\\mathbf{e}_{y}$.\n",
        "\n",
        "* **Thus, the needed contravariant transformation to $v$ in this example is a clockwise rotation.**\n",
        "\n",
        "Example\n",
        "\n",
        "* Consider the example of moving along a curve $\\gamma(t)$ in the Euclidean plane. In polar coordinates, $\\gamma$ may be written in terms of its radial and angular coordinates by $\\gamma(t)=(r(t), \\theta(t))$.\n",
        "\n",
        "* A vector at a particular time $t$ (for instance, the acceleration of the curve) is expressed in terms of $\\left(\\mathbf{e}_{r}, \\mathbf{e}_{\\theta}\\right),$ where $\\mathbf{e}_{r}$ and $\\mathbf{e}_{\\theta}$ are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and [tangential components](https://en.wikipedia.org/wiki/Tangential_and_normal_components).\n",
        "\n",
        "* At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the [Christoffel symbols](https://en.wikipedia.org/wiki/Christoffel_symbols)) serve to express this change.\n",
        "\n",
        "*Another Example*\n",
        "\n",
        "* In a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.\n",
        "\n",
        "* A vector e on a globe on the equator at point Q is directed to the north. Suppose we parallel transport the vector first along the equator until at point P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q.\n",
        "\n",
        "* **Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector**; instead, it has another orientation. **This would not happen in Euclidean space and is caused by the curvature of the surface of the globe**. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature."
      ],
      "metadata": {
        "id": "mpNqLFw7-sbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Bilinear Forms) $\\mathcal{B}_{i i} \\epsilon^{i} \\epsilon^{j} \\rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$*"
      ],
      "metadata": {
        "id": "GCJMH1Oj-vBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor product $\\otimes$ for bilinear maps: Bilinear forms are linear combinations of covector-covector-pairs** (including the metric tensor)\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$\n",
        "\n",
        "* Generalization: Bilinear Forms as Covector-Covector-Pairs\n",
        "\n",
        "* Why not vector-vector-pairs or so? - Bilinear forms take two vector inputs and since covectors take one vector each, a pair of covectors would take two vector inputs.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 12: Bilinear Forms are Covector-Covector pairs](https://www.youtube.com/watch?v=uDRzJIaN2qw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=15)\n",
        "\n",
        "**Advantages of the tensor product for bilinear maps:**\n",
        "\n",
        "* We can write bilinear forms as linear combinations of covector covector pairs and this:\n",
        "\n",
        "  * immediately gives us the transformation rules $\\begin{aligned} \\widetilde{\\epsilon}^{i} &=B_{j}^{i} \\epsilon^{j} & \\widetilde{\\mathcal{B}_{i j}} &=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l} \\\\ \\epsilon^{i} &=F_{j}^{i} \\widetilde{\\epsilon}^{j} & \\mathcal{B}_{i j} &=B_{i}^{k} B_{j}^{l} \\widetilde{B_{k l}} \\end{aligned}$\n",
        "\n",
        "  * the component multiplication formula: $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\Rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$\n",
        "\n",
        "  * and the correct array shape\n",
        "\n",
        "* **Combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors (and not a row and a column vwector like for the Kronecker delta) associated with the covectors**\n",
        "\n",
        "* Meanwhile the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_43.png)\n"
      ],
      "metadata": {
        "id": "svrOb-ni_IT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "ZdLtfLao_KKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 1: write out any bilinear form as a linear combination of covector-covector pairs**\n",
        "\n",
        "*Look at our classic transformation rules*:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{i j}=B_{i}^{k} B_{j}^{l} \\widetilde{\\mathcal{B}_{k l}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{e_{j}}=F_{j}^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "> $\\overrightarrow{e_{j}}=B_{j}^{i} \\widetilde{e_{i}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{\\epsilon}^{i}=B_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\epsilon^{i}=F_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "If we can assume we can write out any bilinear form as a linear combination of covector-covector pairs **to get the transformation rule for these components** we just transform the basis covectors individually:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l} \\epsilon^{k} \\epsilon^{l}$\n",
        "\n",
        "Basis covectors are contra variant, so to build old from the new we use the forward transform $F$\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l}\\left(F_{i}^{k} \\widetilde{\\epsilon}^{i}\\right)\\left(F_{j}^{l} \\widetilde{\\epsilon}^{j}\\right)$\n",
        "\n",
        "and putting these in front gives us this:\n",
        "\n",
        "> $\\mathcal{B}=\\left(F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}\\right) \\widetilde{\\epsilon}^{i}\\widetilde{\\epsilon}^{j}$\n",
        "\n",
        "which as we can see is the correct one:\n",
        "\n",
        "> $\\widetilde{B_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$"
      ],
      "metadata": {
        "id": "-1G-vCul_L_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 2: We can also get the correct component multiplication formula when a bilinear form acts on two vector inputs:**\n",
        "\n",
        "Given:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "> $\\vec{w}=w^{l} \\overrightarrow{e_{l}}$\n",
        "\n",
        "**Proof:** replace components with equations above:\n",
        "\n",
        "> $s=\\mathcal{B}(\\vec{v}, \\vec{w})$\n",
        "\n",
        "Replace the bilinear form and the vectors with their linear combination expansions in some basis\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}, w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Now we pass each of these vector inputs to their corresponding covectors\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i}\\left(v^{k} \\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "\n",
        "$v$ and $w$ come out in front:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\epsilon^{i}\\left(\\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(\\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Replace ($\\overrightarrow{e_{k}}$) and ($\\overrightarrow{e_{l}}$) with Kronecker deltas\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\delta_{k}^{i} \\delta_{l}^{j}$\n",
        "\n",
        "Finally using the index cancellation rules for $k$ and $l$ we get the correct component multiplication formula ends up giving us a single number as a result:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{i} w^{j}$\n"
      ],
      "metadata": {
        "id": "GaERJjMl_Nua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 3**\n",
        "\n",
        "* Remember with linear maps, the new way of writing with the tensor product makes a lot more tensors (with a row of rows): Even though we had two vector inputs we needed to write one as a column and one as a row  flipped on its side to make the multiplication work correctly. Which is awkward, because vectors should always be written as columns and not as rows!\n",
        "\n",
        "* When we write the bilinear forms as a row of rows the matrix multiplication formula makes a lot more sense. We can write out both vectors as columns\n",
        "\n",
        "* **Above is the old way (row vector & column vector both to describe vectors, and the transition matrix in the middle) and on the bottom is the new better way with tensor products $\\otimes$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_42.png)"
      ],
      "metadata": {
        "id": "94JzCC8X_QsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Metric Tensor) $g(\\vec{\\color{blue}v}, \\vec{\\color{blue}w}) \\mapsto \\color{blue}{v^{i}} \\color{blue}{w^{j}} g_{i j}$ for Index Manipulation (contravariant $Z^{ij}$ and covariant $Z_{ij}$)*"
      ],
      "metadata": {
        "id": "YgcZdTDc_TCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor**\n",
        "\n",
        "* In differential geometry, one definition of a [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) is a type of function which takes as input a pair of tangent vectors $v$ and $w$ at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar $g(v, w)$ in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space.\n",
        "\n",
        "* Define length of and angle between tangent vectors when basis changes (same way like a dot product). Use Cases: How to measure the length of a vector when we change the vector (with a linear map) as well as when the basis changes?\n",
        "\n",
        "* Through integration, the metric tensor allows one to define and **compute the length of curves on the manifold** [Source](http://walter.bislins.ch/physik/index.asp?page=Metrik%2DTensor)\n",
        "\n",
        "**Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$. In a given basis we compute the output of a metric tensor using this formula**:\n",
        "\n",
        "> $g(\\vec{\\color{red}v}, \\vec{\\color{blue}w}) \\mapsto \\color{red}{v^{i}} \\color{blue}{w^{j}} g_{i j}$\n",
        "\n",
        "And this is the formula for computing the output of a metric tensor when it acts on two vectors\n",
        "\n",
        "> $\\left[\\begin{array}{ll}\\color{red}{v^{1}} & \\color{red}{v^{2}}\\end{array}\\right]\\left[\\begin{array}{ll}g_{11} & g_{12} \\\\ g_{21} & g_{22}\\end{array}\\right]\\left[\\begin{array}{l}\\color{blue}{w^{1}} \\\\ \\color{blue}{w^{2}}\\end{array}\\right]$\n",
        "\n",
        "**The algebraic properties of metric tensors are**:\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 9: The Metric Tensor](https://www.youtube.com/watch?v=C76lWSOTqnc&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=12)"
      ],
      "metadata": {
        "id": "_mR6eUfX_WoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)"
      ],
      "metadata": {
        "id": "L2D_3hgX_oLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). **Metric tensors are (0,2) tensors, because it transforms using tow covariant rules**:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "KPlaTJsa_qJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensors for measuring length or angles**\n",
        "\n",
        "* Is a tensor whose components in a given vector basis are given by the **dot products of the basis vectors**:\n",
        "\n",
        "  * <font color=\"blue\">$g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}$</font>\n",
        "\n",
        "* Since the dot products don't care about the order of the input $g_{i j}=g_{j i}$, which means the **tensor is symmetric** along the diagonal line (spur).\n",
        "\n",
        "  * $g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}=\\overrightarrow{e_{j}} \\cdot \\overrightarrow{e_{i}}=g_{j i}$\n",
        "\n",
        "* when we want to get an **angle**, we put both vectors in:\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{v}=\\|\\vec{v}\\|^{2}=$ <font color=\"blue\">$v^{i} v^{j} g_{i j}$</font>\n",
        "\n",
        "  * $\\vec{w} \\cdot \\vec{w}=\\|\\vec{w}\\|^{2}=$ <font color=\"blue\">$w^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* when we want to get a **lengths** we out the same vector twice in\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{w}=\\|\\vec{v}\\|\\|\\vec{w}\\| \\cos \\theta=$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* **In a given basis the Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$**:\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}) \\mapsto$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5NQ7BCXo_r5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric tensor $g$ and inverse metric tensor $\\mathfrak{g}$ for lowering & raising indexes (Contraction)**\n",
        "\n",
        "* In mathematics and mathematical physics, [raising and lowering indices](https://en.m.wikipedia.org/wiki/Raising_and_lowering_indices) are operations on tensors which change their type. Raising and lowering indices are a form of [index manipulation (Ricci calculus)](https://en.m.wikipedia.org/wiki/Ricci_calculus) in tensor expressions.\n",
        "\n",
        "* The metric tensor represents a matrix with scalar elements $\\left(g_{i j}\\right.$ or $\\mathfrak{g}^{i j}$ ) and is a tensor object which is used to raise or lower the index on another tensor object by an operation called [Tensor Contraction](https://en.m.wikipedia.org/wiki/Tensor_contraction), thus **allowing a covariant tensor to be converted to a contravariant tensor, and vice versa**.\n",
        "\n",
        "  * In multilinear algebra, a [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) is an operation on a tensor that arises from the natural pairing of a finite-dimensional vector space and its dual.\n",
        "\n",
        "  * In components, it is expressed as a sum of products of scalar components of the tensor(s) caused by applying the summation convention to a pair of dummy indices that are bound to each other in an expression.\n",
        "\n",
        "  * The contraction of a single mixed tensor occurs when a pair of literal indices (one a subscript, the other a superscript) of the tensor are set equal to each other and summed over. In the Einstein notation this summation is built into the notation. **The result is another tensor with order reduced by 2.**\n",
        "\n",
        "  * Es ist eine Verallgemeinerung der Spur einer linearen Abbildung auf Tensoren, die mindestens einfach kovariant und einfach kontravariant sind.\n",
        "\n",
        "* **For an orthonormal [Cartesian coordinate system](https://en.m.wikipedia.org/wiki/Cartesian_coordinate_system), the metric tensor is just the [kronecker delta](https://en.m.wikipedia.org/wiki/Kronecker_delta) $\\delta_{i j}$ or $\\delta^{i j},$ which is just a tensor equivalent of the identity matrix, and $\\delta_{i j}=\\delta^{i j}=\\delta_{j}^{i}$**. For measuring length of a vector: Pythagoras theorem is only valid in orthonormal bases!\n",
        "\n",
        "* Normally a metric tensor as a function from a pair of vectors to scalars $g: V x V \\rightarrow \\mathbb{R}$. But it can also be a function from a vector in $V$ to a covector in $V^{*}$ $g: V  \\rightarrow V^{*}$ using the metric tensor. Reverse direction: from a covector to its vector partner.\n",
        "\n",
        "* From the metric tensor we know its components have 2 lower indices, so it‚Äôs a member of $V^{*}$ tensor $V^{*}$ bzw. $g \\in V^{*} \\otimes V^{*}$. Now we introduce what's called the inverse metric tensor $\\mathfrak{g} \\in V \\otimes V$. The combination between both in a summation gives you the Kronecker delta: $\\mathfrak{g}^{k i} g_{i j}=\\delta_{j}^{k}$. This is how we define the inverse metric tensor.\n",
        "\n",
        "* The **ordinary metric tensor $g_{i j}$ is covariant** with $g_{i j}=\\vec{g}_{i} \\cdot \\vec{g}_{j}$. It **lowers indexes** and its components are covariant: $T_{i}=g_{i j} T^{j}$\n",
        "\n",
        "* The **inverse metric tensor $\\mathfrak{g}^{k i}$ is contravariant** with $\\mathfrak{g}^{i j}=\\vec{\\mathfrak{g}}^{i} \\cdot \\vec{\\mathfrak{g}}^{j}$. It **raises the indexes** and its components are contravariant (go in the other direction): $T^{i}=\\mathfrak{g}^{i j} T_{j}$.\n",
        "\n",
        "\n",
        "* Both metric tensors are related by the identity: $g_{i k} \\, \\mathfrak{g}^{j k}=\\delta_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_82.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "87h0yKnx_try"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sharp Operator $\\sharp$ and Flat Operator $\\flat$ to generalize raising and lowering indexes on the component of any tensor**\n",
        "\n",
        "* The ordinary metric tensor lowers indexes meanwhile the inverse metric tensor raises indexes.\n",
        "\n",
        "* But these raising and lowering operations don‚Äôt just apply to vector and covector components. We can also raise and lower the indexes on the components of other tensors\n",
        "\n",
        "* $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$ wird zu $\\rightarrow$ $\\flat \\vec{v}=v_{i} \\epsilon^{i}$. The flat operator (on the left side) lowers the indexes (on the right side) from $v^{i}$ to $v_{i}$. **Another way to think of it: the $\\flat$ operator is transforming a vector arrow into a covector stack** (like flattening the pointy arrow into a flat stack)\n",
        "\n",
        "* The covector alpha has components with downstairs indexes: $\\alpha=\\alpha_{i} \\epsilon^{i}$ and the vector alpha sharp has components with upstairs indexes $\\sharp \\alpha=\\alpha^{i} \\overrightarrow{e_{i}}$. The sharp operator is basically raising the index. Also the sharp operator is basically turning a flat covector stack into a sharp pointy arrow vector.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 16: Raising/Lowering Indexes (with motivation, sharp + flat operators)](https://www.youtube.com/watch?v=_z9R7OMpxhY)\n",
        "\n",
        "* See article [Sharp & Flat Operator - Isomorphismus](https://de.wikipedia.org/wiki/√Ñu√üere_Ableitung#Be-_und_Kreuz-_(Flat-_und_Sharp-)_Isomorphismus)\n",
        "\n",
        "* Take tensor $Q$ which is member of vector space $Q \\in V \\otimes V^{*} \\otimes V^{*}$ and has components $Q_{j k}^{I}$:  $Q=Q_{j k}^{i} \\vec{e}_{i} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "* If we multiply it but the inverse metric tensor and sum over $j$ like this $Q^{i}{ }_{j k} \\mathfrak{g}^{j x}$ we can raise the index upward: $=Q^{i x}{ }_{k}$ and we get this new tensor $Q^{\\prime}=Q_{k}^{i x} \\overrightarrow{e_{i} {e}_{x}} \\epsilon^{k}$\n",
        "\n",
        "* This is a member of the new vector space $Q^{\\prime} \\in V \\otimes \\color{red}{V} \\otimes V^{*}$ - we raised the middle index $Q \\in V \\otimes \\color{red}{V^{*}} \\otimes V^{*}$\n",
        "\n",
        "* All these vector spaces can be traveled between using the ordinary metric tensor to lower indexes (blue arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_84.png)\n",
        "\n",
        "* Or we can use the inverse metric tensor to raise indexes (red arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_85.png)"
      ],
      "metadata": {
        "id": "spzpC64M_vis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor as Special Bilinear Form**: Metric Tensor = \"First Fundamental Form\". The metric tensor is a special bilinear form. Generalisation via tensor product: Bilinear Forms as Covector-Covector-Pairs.\n",
        "\n",
        "* So what is the difference between metric tensor and bilinear form? The metric tensor is a bilinear form, but it's a very specific example of a bilinear form. The metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* **Videos**: https://youtu.be/Hf-BxbtCg_A and https://youtu.be/Dn0ZZRVuJcU"
      ],
      "metadata": {
        "id": "oyUSwoxm_xXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "g4uy9SPO_zMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector-Pairs (Linear Maps) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$ to map vector to vector within one basis*"
      ],
      "metadata": {
        "id": "V44oHA2a_1hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear maps**\n",
        "\n",
        "1. maps vectors to vectors $L: V \\mapsto W$, or $L: V \\mapsto V$\n",
        "\n",
        "2. Linearity (addition & scaling)\n",
        "\n",
        "* Both covectors and linear maps are functions. The only difference is that covectors output is a scalar, and linear maps output vectors.\n",
        "\n",
        "> **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* Linear maps are linear combinations of vector-covector-pairs $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ with the tensor product $\\overrightarrow{e_{i}} \\epsilon^{j}$ bzw. $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* **Vector changes, basis not: When we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**.\n",
        "\n",
        "  * While the output vector might be different than the input vector, we are still measuring the output vector using the same basis\n",
        "\n",
        "  * With forward and backward transform we modify the vector components when we change from one basis to another, or from one dual basis to another.\n",
        "\n",
        "  * With linear maps now we modify the vector components when we move a vector around within a given basis!\n",
        "\n",
        "* **Coordinate representations of linear maps end up being matrices!**. Column vectors are coordinate representation of vectors. Row vectors are coordinate representation of covectors.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 7: Linear Maps](https://www.youtube.com/watch?v=dtvM-CzNe50&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=10)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 11: Linear maps are Vector-Covector Pairs](https://www.youtube.com/watch?v=YK2zVcWpROA&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14)\n",
        "\n",
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). Metric tensors are (0,2) tensors, because it transforms using tow covariant rules:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "8erR4QCL_9Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_19.png)"
      ],
      "metadata": {
        "id": "kjVQbCBy__67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Case / Fun fact about how matrices transform vectors:**\n",
        "\n",
        "When you use the column vector (1,0) as an input vector, you get the first column of the matrix as the output (3, -1):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "-1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "When you use the column vector (0,1) as an input vector, you get the second column of the matrix as the output (-4, 2):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "-4 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "The column vectors 1, 0 and 0,1 are like copies of the basis vectors e1 and e2. Because: **Linear maps transform input vectors. But linear maps don't transform the basis!**\n",
        "\n",
        "**So when we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**. While the output vector might be different than the input vector, we are still measuring the output vector using the same basis."
      ],
      "metadata": {
        "id": "xEM1uOD7ACQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_20.png)"
      ],
      "metadata": {
        "id": "IFqMZilJAEFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maps - Case 1: Transformations of vectors within one basis (Linear Maps as Tensor Product $\\otimes$ (Vector-Covector-Pairs)**"
      ],
      "metadata": {
        "id": "pg7lwrp9AJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pure & Impure Matrices & how Vector-Covectors-Pairs help**\n",
        "\n",
        "* When we multiply a row vector and a column vector in this order we get a scalar: $\\begin{aligned} &\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right] \\end{aligned}$ $= (2)(3)+(1)(-4) = 6-4 =2$\n",
        "\n",
        "* But if we reverse the order we get a matrix (which is basically a linear map):  $\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right]\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]$ = $\\left[\\begin{array}{cc}6 & 3 \\\\ -8 & -4\\end{array}\\right]$\n",
        "\n",
        "* **Can we do this the other way around (get the row and column vector from the matrix)?** (Answer: not always so easy)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_36.png)\n",
        "\n",
        "* Here is the proof why it doesn't work for this matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_37.png)\n",
        "\n",
        "* There are some matrices that can be broken up into column vectors and row vectors, and others not. We call them pure and impure matrices!\n",
        "\n",
        "* Pure matrices are boring when they are used as linear maps, because all the output vectors exist along the same direction.\n",
        "\n",
        "* The reason behind is that the columns of the matrices are all scalable multiples of each other (as shown below). Output vectors point all to the same direction.\n",
        "\n",
        "* The set of transformations a pure matrix can do as linear map is really limited\n",
        "\n",
        "* Impure matrices as linear maps are more interesting because they can send basis vectors into different directions.\n",
        "\n",
        "* **Question: Since impure matrices are more interesting, how can we construct impure matrices using column vector, row vector and products**?\n",
        "\n",
        "* **Solution: We define four special vector-covector-pairs using the old e basis $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$  and the old epsilon dual basis $\\left\\{\\epsilon^{1}, \\epsilon^{2}\\right\\}$** (Hint: this will be the transition from 'classic' linear maps to vector-covector-tensor products with another way of writing them!)\n",
        "\n",
        "* For example the 1 on top left can be written using the column and row vector via $\\overrightarrow{e_{1}} \\epsilon_{1}$:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{1}$\n",
        "\n",
        "And we can do the same for all other 3 matrix components:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{1}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{2}$\n",
        "\n",
        "And you‚Äôll notice that these 4 matrices when taking in linear combination, so when we scale each matrix by a different amount and then add them together, we get any general 2x2 matrix $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$ that we like, just by picking the right scaling numbers a, b, c and d:\n",
        "\n",
        "> $a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "So this set of 4 products forms a basis for all matrices that are linear maps from the vector space $V \\mapsto V$ to itself\n",
        "\n",
        "> $\\left\\{\\overrightarrow{e_{1}} \\epsilon^{1}, \\overrightarrow{e_{1}} \\epsilon^{2}, \\overrightarrow{e_{2}} \\epsilon^{1}, \\overrightarrow{e_{2}} \\epsilon^{2}\\right\\}$ is a basis for $V \\rightarrow V$\n",
        "\n",
        "So any general linear map $L$ can be written as this linear combination if we pick the coefficients right:\n",
        "\n",
        "> $L=a \\overrightarrow{e_{1}} \\epsilon^{1}+b \\overrightarrow{e_{1}} \\epsilon^{2}+c \\overrightarrow{e_{2}} \\epsilon^{1}+d \\overrightarrow{e_{2}} \\epsilon^{2}$\n",
        "\n",
        "And we can summarize this using the Einstein notation, any linear map $L$ can be written using these components\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "> **This is a vector and a covector written together - which is a linear map**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_38.png)\n"
      ],
      "metadata": {
        "id": "adsf_EarAL1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Formula for matrix multiplication**\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]\\left[\\begin{array}{l} \\color{red}x \\\\ \\color{blue}y\\end{array}\\right]=\\left[\\begin{array}{l}a \\color{red}x+b \\color{blue}y \\\\ c \\color{red}x+d \\color{blue}y\\end{array}\\right]$\n",
        "\n",
        "This originates in the following abstract algebraic definition:\n",
        "\n",
        "$L: V \\rightarrow W$\n",
        "\n",
        "$L(\\vec{v}+\\vec{w})=L(\\vec{v})+L(\\vec{w})$\n",
        "\n",
        "$L(n \\vec{v})=n L(\\vec{v})$\n",
        "\n",
        "**How to turn the $\\vec{v}$ coefficients into the $\\vec{w}$ coefficients?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_21.png)\n",
        "\n",
        "\n",
        "**Proof: Derive the matrix multiplication formulas just from the abstract linearity properties of linear maps**\n",
        "\n",
        "If we have a linear map $L$ that transforms a vector v into another vector w (where w can be written as a linear combination of basis vectors)\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=w^{1} \\overrightarrow{e_{1}}+w^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "and we know how $L$ transforms basis vectors (via the L coefficients)\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{1}}\\right)=L_{1}^{1} \\overrightarrow{e_{1}}+L_{1}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{2}}\\right)=L_{2}^{1} \\overrightarrow{e_{1}}+L_{2}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "this means we can transform the v components into  the w components using the formulas below\n",
        "\n",
        "$w^{1}=L_{1}^{1} v^{1}+L_{2}^{1} v^{2}$\n",
        "\n",
        "$w^{2}=L_{1}^{2} v^{1}+L_{2}^{2} v^{2}$.\n",
        "\n",
        "And if we repeat this argument for any number of dimensions, so if we have a linear map $L$ in n-dimensions\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=\\sum_{i=1}^{n} w^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "We would get all the $L$ coefficients from this formula below\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{i}}\\right)=\\sum_{j=1}^{n} L_{i}^{j} \\overrightarrow{e_{j}}$\n",
        "\n",
        "we can transform the v components into the w components\n",
        "\n",
        "$w^{i}=\\sum_{j=1}^{n} L_{j}^{i} v^{j}$\n",
        "\n",
        "This is the standard matrix multiplication formula for multiplying matrices and vectors together.\n"
      ],
      "metadata": {
        "id": "whZ3s5JiAh4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit of seeing linear maps as vector-covector-pairs - which are tensor products $\\otimes$**\n",
        "\n",
        "*When we have a linear map acting on a vector, we can get the correct matrix vector component multiplication formula*\n",
        "\n",
        "If you think of some linear map $L$ as a linear combination of these basis linear maps\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "and we have also a vector $\\vec{v}$ which is a linear combination of these basis vectors\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "What would we get in $\\vec{w}$ with $L$ acts on the input $\\vec{v}$?\n",
        "\n",
        "> $\\vec{w}=L(\\vec{v})$\n",
        "\n",
        "To find out we substitute the components $L$ and $\\vec{v}$ accordingly:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "The $\\epsilon^{j}$ dual basis vector is now acting on the input vector! (and that's what covectors do, they act on vectors).\n",
        "\n",
        "So we use the linearity of $\\epsilon^{j}$ to take out the scaling coefficient $v^{k}$ and put it out in front:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "This leaves us with $\\epsilon^{j}$ acting on $\\left(\\overrightarrow{e_{k}}\\right)$. And by definition this is just a Kronecker delta:\n",
        "\n",
        ">  $\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$ = $\\delta_{k}^{j}$\n",
        "\n",
        "So we can replace this in the equation:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "And by the kronecker delta cancellation rule, we can cancel out the $k$'s and replace it at $v$ with $j$:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}} \\delta^{j}$\n",
        "\n",
        "And this is our output vector written as a linear combination of the $e$ basis vectors:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "And we get these coefficients:\n",
        "\n",
        "> $L_{j}^{i} v^{j}$\n",
        "\n",
        "from the standard matrix multiplication rule:\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\vec{w}=L(\\vec{v}) \\\\\n",
        "w^{i}=L_{j}^{i} v^{j}\n",
        "\\end{array}$\n",
        "\n",
        "So from our equation above:\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "**we can see that this product pair is really a linear map**:\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "It took an input vector, transformed it, and gave us an output vector.\n",
        "\n",
        "> **Proof: vector-covector-pairs are linear maps - which are tensor products $\\otimes$  !**\n",
        "\n",
        "* These are pure matrices, so you get the boring linear maps! To get the more interesting linear maps (the impure matrices!), we need to combine a bunch of pure linear maps together in linear combination, which helps us to get more interesting impure linear maps like $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] = a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]$\n",
        "\n",
        "* **And finally this vector-covector-pair is actually a tensor product**\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$ normally written like this: $\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_39.png)"
      ],
      "metadata": {
        "id": "bv6NmiXzAOBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two ways of multiplying a row vector with a column vector: classic way (left) and tensor product (right - much better)**\n",
        "\n",
        "\n",
        "* The $\\otimes$ operator tells us to take the array on the left and distribute it to each of the components inside the array on the right.\n",
        "\n",
        "* This means take the column vector on the left and distribute a copy to each element inside the second array. And you get a row of columns (bottom right), which is basically like a matrix.\n",
        "\n",
        "* **This means that linear maps are rows of columns!**\n",
        "\n",
        "* Picture below: On the left is the old way (row vector & column vector both to describe vectors) and on the right side is the new better way with tensor products $\\otimes$\n",
        "\n",
        "* the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "* (meanwhile combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors associated with the covectors)\n",
        "\n",
        "* **The tensor product for linear maps gave a great change of perspective**. This idea of distributing arrays into each other will turn out to be very useful later on!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_41.png)"
      ],
      "metadata": {
        "id": "bsHsugHiAQoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector Pairs (Linear Maps) + Forward and Backward Transforms (Vectors) $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$ to map vector to vector across two bases*"
      ],
      "metadata": {
        "id": "VcEin6DXApdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **We need transformation rules for linear maps when we are moving a vector AND going from one basis to another**, because linear maps are only working within one basis (move vectors around within one basis), we need to find the coefficients of the linear map in the new basis when we change basis.\n",
        "\n",
        "* Task: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "**How do we do this?**\n",
        "\n",
        "If you take vector components $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ from the vector $\\vec{v}$ in the original basis $\\overrightarrow{e_{i}}$\n",
        "\n",
        "1. **apply the linear map** $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$, you get the new vector components $L(\\vec{v})$ $=\\left[\\begin{array}{c}1 / 2 \\\\ 2\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ that is still **in the same original basis** $\\overrightarrow{e_{i}}$.\n",
        "\n",
        "2. then **apply the backward transform** with the matrix $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$, you get the new vector components for $\\vec{v}$ $\\left[\\begin{array}{l}3/4 \\\\ 1\\end{array}\\right]_{\\widetilde{e_{i}}}$ **in the new basis** ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "**We use the backward transform from old to new, since vector components behave contravariant !!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_22.png)"
      ],
      "metadata": {
        "id": "nrdE_XbCA5ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you get the new vector components $L(\\vec{v})$ in the new basis ${\\widetilde{e_{i}}}$?**\n",
        "\n",
        "* Answer: We need to find the coefficients of the linear map ${\\widetilde{L_{j}^{q}}}$ in the new basis ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "* Again: how do you get the new vector components $L(\\vec{v})$ (the transformed vector with the linear map $L$) in the new basis ${\\widetilde{e_{i}}}$, and not only the components of $\\vec{v}$ (the original vector) in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* Said differently: what are the components of the output vector in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* we cannot apply the linear map $L$ $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$ since it's only valid in the original basis\n",
        "\n",
        "* **We need to find a new matrix ${\\widetilde{L}}$ in the new basis ${\\widetilde{e_{i}}}$ that tells us how to build output vectors using the ${\\widetilde{e_{1}}}$ and ${\\widetilde{e_{2}}}$ basis vectors.**\n",
        "\n",
        "* This means we need to find the ${\\widetilde{L_{j}^{q}}}$ coefficients:\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{1}}}\\right)=\\widetilde{L_{1}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{1}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{2}}}\\right)=\\widetilde{L_{2}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{2}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "**How to get the components of the output vector in the new basis ${\\widetilde{e_{i}}}$:**\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\color{red}{\\widetilde{e_{i}}}\\right)$\n",
        "\n",
        "Let's first use the forward transform $\\widetilde{e_{i}}=\\sum_{j=1}^{n} F_{i}^{j} \\overrightarrow{e_{j}}$ to rewrite the new basis vectors in terms of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\sum_{j=1}^{n} F_{i}^{j} \\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use the linearity of $L$ to take the scale and sum coefficients outside the function:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use this definition $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)=\\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$ to write the output $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$ as linear combination of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} \\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Then we re-arrange the sums a bit:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Now we write the old basis in terms of the new basis vectors with $\\color{blue}{\\overrightarrow{e_{k}}}=\\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{\\overrightarrow{e_{l}}}}$ using the bckward transform:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "We re-arrange the sums again:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "* So on the left we have a linear combination of ${\\widetilde{e}}$ basis vectors with the summation index $q$.\n",
        "\n",
        "* So on the right we have a linear combination of ${\\widetilde{e}}$ basis vectors again but with the summation index $l$.\n",
        "\n",
        "* So we have a linear combination of ${\\widetilde{e}}$ basis vectors on both sides.\n",
        "\n",
        "But with different summation indexes. But the choice doesn't really matter. So we change all the $q$ with $l$:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\widetilde{L_{i}^{l}} \\color{red}{\\widetilde{e_{l}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "Now we see that the $\\widetilde{L_{i}^{l}}$ coefficients on the left side are equal to this part $\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} B_{k}^{l}$ in the middle of the right equation:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\color{pink}{\\widetilde{L_{i}^{l}}} \\widetilde{e_{l}}=\\sum_{l=1}^{n} \\color{pink}{\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}} \\widetilde{e_{l}}$\n",
        "\n",
        "**This is saying that to transform the matrix coordinates from the old basis to the new basis we multiply the old matrix $L_{j}^{k}$ by the backward transform $B$ on the left and by the forward transform $F$ on the right:**\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "**This is what we just did (explanation below):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)"
      ],
      "metadata": {
        "id": "l4kEzo39A7c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUMMARY: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)**\n",
        "\n",
        "* So, matrices, or linear maps, transform with both the forward transform and the backward transform. Here is why:\n",
        "\n",
        "* You have two ways to get the new basis vector component: you can use $\\widetilde{L}$ matrix n the bottom to go from left to right directly $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$\n",
        "\n",
        "* But it's the same thing as going the other way around:\n",
        "\n",
        "  1. To transform the new vector components into the old vector components you use the forward transform\n",
        "\n",
        "  2. And here to transform the components of the input vector into components of the output vector in the old basis, we just use the matrix $L$\n",
        "\n",
        "  3. And finally to get from the old vector components to the new vector components for the output vector we use the backward transform\n",
        "\n",
        "  4. Now we have the components of the new basis vector $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$ with which you can describe any vector in the new vector space.\n",
        "\n",
        "* So the idea of transforming matrix components using both the forward and backward transformations makes sense.\n",
        "\n",
        "**Let's check this on our example from above:**\n",
        "\n",
        "This is the equation again to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis):\n",
        "\n",
        "> $\\widetilde{L}_{i}^{l}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "* Backward Transform $B$: $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$\n",
        "\n",
        "* Linear Map $L$ in old basis ${\\overrightarrow{e_{j}}}$: $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\overrightarrow{e_{j}}}$\n",
        "\n",
        "* Forward Transform $F$: $\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "Now let's place them all in the equation to get the linear map in the new basis:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 8 & 1 \\\\ -1 / 2 & 4\\end{array}\\right]\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "We get this matrix the:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}5 / 4 & 3 / 16 \\\\ 3 & 5 / 4\\end{array}\\right]$\n",
        "\n",
        "And this matrix above tells us how to write the outputs of the linear map as linear combination of the new basis:\n",
        "\n",
        "> $L\\left(\\widetilde{e_{1}}\\right)=5 / 4 \\widetilde{e_{1}}+3 \\widetilde{e_{2}}$\n",
        "\n",
        "> $L\\left(\\widetilde{e_{2}}\\right)=3 / 16 \\widetilde{e_{1}}+5 / 4 \\widetilde{e_{2}}$\n",
        "\n",
        "**Results are correct, the new matrix / linear map in the new basis is converting the vector properly from its original position to the new position (measuring both at the new basis vectors):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_24.png)"
      ],
      "metadata": {
        "id": "qozTbjNIA9Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward & Backward Transform of Linear Maps between different basis**\n",
        "\n",
        "* A tensor is an object that is invariant under a change of coordinates, and **has components that change in a special, predictable way under a change of coordinates**. The way we transform them is by a applying a series of forward and backward transforms!\n",
        "\n",
        "**Forward Transform (going from old $T$ to new $\\widetilde{T}$)**\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **backward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **forward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Backward Transformation (going from new $\\widetilde{T}$ to old $T$)**\n",
        "\n",
        "> $T_{r s t \\ldots}^{i j k \\ldots}=\\left(F_{\\color{red}a}^{i} F_{\\color{red}b}^{j} F_{\\color{red}c}^{k} \\cdots\\right) \\widetilde{T_{\\color{blue}{x y z} \\ldots}^{\\color{red}{a b c} \\ldots}}\\left(B_{r}^{\\color{blue}x} B_{s}^{\\color{blue}y} B_{t}^{\\color{blue}z} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **forward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **backward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Illustration from the linear maps part that helps to understand the $FTB$ = $\\widetilde{T}$ (oben) bzw. $FLB$ = $\\widetilde{L}$ (unten) transform:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$ (forward transform)\n",
        "\n",
        "**How many contravariant and covariant rules we need to follow during a transformation?** - Core Question when it gets more complicated with more basis & dual basis !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_32.png)"
      ],
      "metadata": {
        "id": "5cOVS7o9BBX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "_AUtbuZDBDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Tensors (Any Vector-Covector combination) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$*"
      ],
      "metadata": {
        "id": "457rg_0-BFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Maps vs Bilinear Forms**\n",
        "\n",
        "* **Bilinear Forms: Linear combinations of covector-covector-pairs** $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$ (including metric tensor)\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 14: Tensors are general vector/covector combinations](https://www.youtube.com/watch?v=9R4vhqvE_jw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=17)\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)"
      ],
      "metadata": {
        "id": "m-CmxPR5BUhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take two new tensor**\n",
        "\n",
        "This is a (2,0) tensor\n",
        "\n",
        "> $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "This is a (1,2) tensor\n",
        "\n",
        "> $Q=Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "**Now we can ask the same questions:**\n",
        "\n",
        "1. What are the coordinate transform rules?\n",
        "2. What is the multiplication formula for $Q$ ($D$)?\n",
        "3. What are the array shapes?\n",
        "\n",
        "**1. How to $D$ and $Q$ under a change of basis?**\n",
        "\n",
        "* Transforming tensor components is not hard, long as you have the transformation rules for basis vectors and covectors (plug backward transform in here for example twice on the left side)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_50.png)\n",
        "\n",
        "**2. What is the formula for $Q$ acting on the input $D$ = $Q(D)$**\n",
        "\n",
        "* This is tricky because there is no one way to do that.\n",
        "\n",
        "* **The challenge is now that: As we make these tensors bigger and bigger with more and more covariant and contravariant parts, we end up with more and more ways to do the summations, and more and more ways to compute functions.**\n",
        "\n",
        "* Writing $D$ = $Q(D)$ is ambuguous as it doesn't tell us exactly what to do\n",
        "\n",
        "* so we need to write it out in the Einstein notation like on the left side, for example $Q_{j k}^{i} D^{j k}$\n",
        "\n",
        "* Some examples are:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_51.png)\n",
        "\n",
        "**3. What is the array shape?**\n",
        "\n",
        "* It's like the Kronecker product between two column vectors: like this part $\\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$  in this here: $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "* We can do this for the first example from above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_52.png)\n",
        "\n",
        "* For the other example it would look like this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_53.png)\n",
        "\n",
        "* One could use the 3d visualisation, but it's better in the matrix array way. Because looking at this you can see it‚Äôs a (1,2) tensor with one column aspect and two row aspects\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_54.png)"
      ],
      "metadata": {
        "id": "rXmd5aWCBWWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Array Multiplication**\n",
        "\n",
        "* Easy for smaller tensors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_55.png)\n",
        "\n",
        "* That's not so easier for larger tensors that have high type numbers, because there are several possible multiplication rules (as discussed earlier):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_56.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_57.png)\n",
        "\n",
        "* For much more complex tensors, it's the easiest to just stick with the Einstein notation and also not do the array representation.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_58.png)\n",
        "\n",
        "* So with high type tensors the abstract notation and the array notation have their limitations. When trying to express tensor multiplication formulas it‚Äôs usually easier just to stick with the Einstein component notation. For this reason a lot of sources just write tensors like this and leave out basis vector and covectors completely:\n",
        "\n",
        "> $Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ $\\mapsto$ $Q_{j k}^{i}$\n",
        "\n",
        "> $D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$ $\\mapsto$ $D=D^{a b}$\n",
        "\n",
        "* **But it's important to remember that tensor components always come from a choice of basis, and the same tensor can have different components if we choose to represent it in a different basis.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_59.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_60.png)"
      ],
      "metadata": {
        "id": "mGPcQ4POBYNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector Spaces (Hilbert Spaces)*"
      ],
      "metadata": {
        "id": "vq-kSppvBbU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "These vectors are element of the vector space ${V}$\n",
        "\n",
        "> $\\vec{v}, \\vec{w}, \\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}$ $\\in {V}$\n",
        "\n",
        "These covectors are element of the dual vector space ${V^{*}}$\n",
        "\n",
        "> $\\alpha, \\beta, \\epsilon^{1}, \\epsilon^{2}$ $\\in {V^{*}}$\n",
        "\n",
        "We can make following vector-covector-pairs:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "And we can add them and scale with them with the tensor product rules which forms a vector space:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "So we know that these must be vectors in a vectors space:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "But in which vector space do they live in? In this one:\n",
        "\n",
        "> $\\in V \\otimes V^{*}$\n",
        "\n",
        "**Now this is a new use case of $\\otimes$, because so far we used it for combining vectors or tensors. Here we use it to combine entire vector spaces.** (more about it below in a short separat chapter)"
      ],
      "metadata": {
        "id": "gr-jRSA3I5Ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V \\otimes V^{*}$?** (1,1)-tensors\n",
        "\n",
        "\n",
        "> **(1,1)-Tensors**: $L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\in V \\otimes V^{*}$\n",
        "\n",
        "Remember: vectors have an upstairs index (because they transform contravariant):\n",
        "\n",
        "> $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "and covectors habe a downstairs index (because they transform covariant):\n",
        "\n",
        "> $\\alpha=\\alpha_{i} \\epsilon^{i}$\n",
        "\n",
        "\n",
        "\n",
        "So if we do a summation with vector components like this over $j$, we end up with vector components as the output (because we have one upstairs $i$ index that's left):\n",
        "\n",
        "> $L_{j}^{i} v^{j}=w^{i}$\n",
        "\n",
        "In this case $L$ ist acting as a map from $V$ to $V$, or essentially a linear map.\n",
        "\n",
        "> $V \\mapsto V$\n",
        "\n",
        "> **This is a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a summation with covector components like this with sum over $i$, and our outout would have $j$ as the dowstairs index:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i}=\\beta_{j}$\n",
        "\n",
        "We would end up with covector components as the output. So covector in, covector out:\n",
        "\n",
        "> $V^{*} \\mapsto V^{*}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "Also we can provide $L$ with both vector components and covector components and we do two summations over $i$ and $j$, and that would give us a scalar as the output since there are no indices left to sum over.\n",
        "\n",
        "> $L_{j}^{i} v^{j} \\alpha_{i}=s$\n",
        "\n",
        "So in this case $L$ can be viewed as a function from a pair of vectors and covectors to scalars.\n",
        "\n",
        "> $V \\times V^{*} \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "And finally we can do the same thing but reverse the order of the inputs:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i} v^{j}=s$\n",
        "\n",
        "And in this case $L$ is a function from a covector-vector-pair to scalar:\n",
        "\n",
        "> $V^{*} \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "4bn8DJW0I9YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V^{*} \\otimes V^{*}$?** (0,2)-tensors\n",
        "\n",
        "**Now what if we use the tensor product to combine two covectors together?**\n",
        "\n",
        "Those covectors life in $V^{*}$:\n",
        "\n",
        "> $\\alpha, \\beta, \\gamma, \\delta, \\epsilon^{1}, \\epsilon^{2}$ $\\in V^{*}$\n",
        "\n",
        "So when we have covector-covector-pairs like following it turns out that they all life in the vector space **$V^{*} \\otimes V^{*}$**\n",
        "\n",
        "> $\\alpha \\beta, \\alpha \\gamma, \\delta \\beta, \\epsilon^{1} \\epsilon^{2}, \\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$ **$\\in V^{*} \\otimes V^{*}$**\n",
        "\n",
        "**So elements of the $V^{*} \\otimes V^{*}$are (0,2)-tensors**\n",
        "\n",
        "> $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\in V^{*} \\otimes V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "(Covector-covector-pairs and their linear combinations)\n",
        "\n",
        "If we take these $B$ components and do two summations with two sets of vector components (via $i$ and $j$), then we end up with a scalar:\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i} w^{j}=s$\n",
        "\n",
        "And this of course is a bilinear form (which takes a pair of vectors and outputs a scalar):\n",
        "\n",
        "> $V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a single summation over $i$ with a set of vector components and we‚Äôd be left with the index $j$ downstairs. So the output would be a set of covector components\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i}=\\alpha_{j}$\n",
        "\n",
        "So in this case $B$ is a map from vectors to covectors:\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "Also we could choose to do summation with vector components over the $j$ index and then end up with covector components $i$\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{j}=\\beta_{i}$\n",
        "\n",
        "This would be another map from $V$ to $V^{*}$, but it would be a different map than the previous one, because we‚Äôre doing the summation differently.\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "AZ-VNTnoI_JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create even larger vector spaces from basic building blocks $V$ and $V^{*}$**\n",
        "\n",
        "**So we can create larger and larger vector spaces out of the two basic building blocks $V$ and $V^{*}$**:\n",
        "\n",
        "* starting with two vector spaces $V$ and $V^{*}$ (basic building blocks)\n",
        "\n",
        "  * they contain tensors $v_{i}$ and $\\alpha_{j}$\n",
        "\n",
        "  * with vector components with upstairs index and covector components with downstairs index\n",
        "\n",
        "* we can combine these 2 vector spaces into new vector spaces using the tensor product $V \\otimes V$, $V \\otimes V^{*}$, $V^{*} \\otimes V$ and $V^{*} \\otimes V^{*}$\n",
        "\n",
        "  * and these vectors spaces have following vector components: $\\mathcal{A}^{i j}$, $L_{j}^{I}$, $L_{j}{ }^{i}$ and $\\mathcal{B}_{i j}$\n",
        "\n",
        "  * Indexes from $V$ go upstairs and index from $V^{*}$ go downstairs\n",
        "\n",
        "* And we can continue to make larger and larger vector spaces using the tensor product like $V \\otimes V \\otimes V$, $V^{*} \\otimes V \\otimes V$ etc\n",
        "\n",
        "  * And all these vector spaces contain tensors like $T^{i j k}$, $T_{i}^{j k}$, $T_{j}^{i k}$ etc.\n",
        "\n",
        "  * with components that have different combinations of upstairs and downstairs indexes depending on whether they are constructed using $V$ or $V^{*}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_65.png)"
      ],
      "metadata": {
        "id": "npgLhzSYJA3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So if we have some new tensor $T$ from a vector space we‚Äôve never seen before, we can easily get the correct component indexes just by looking at the vector spaces.**\n",
        "\n",
        "> $T \\in V^{*} \\otimes V \\otimes V^{*} \\otimes V^{*}$\n",
        "\n",
        "Looking at these vector spaces we see that the basis would be made up of a covector, a vector, a covector and another covector in combination\n",
        "\n",
        "> $T={\\epsilon}^{i}  \\overrightarrow{e_{j}} \\epsilon^{k} {\\epsilon}^{l}$\n",
        "\n",
        "And we get the components just by **placing the indexes in the opposite position that we see in the basis**, so that all the summations work out properly.\n",
        "\n",
        "**And now we can ask: How can this tensor $T$ act on other tensors?**\n",
        "\n",
        "> $T_{i}^{j}{ }_{k l}$\n",
        "\n",
        "examples of other tensors to be acted on:\n",
        "\n",
        "> $u^{c} \\quad \\begin{array}{cc}D^{f g} & \\beta_{s} \\\\ Q_{u v}^{t} & L_{y}^{x} & w^{b} & U^{m n o}\\end{array}$\n",
        "\n",
        "**Well, we can basically do any summations we like as long as the upstairs indexes are matched with downstairs indexes and downstairs indexes are matched with upstairs indexes.**\n",
        "\n",
        "We could do something like this with four summations and you can see that all the indexes are positioned properly:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensors_69.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_68.png)"
      ],
      "metadata": {
        "id": "HEU05oWyJCrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is how the result for each would look like** [see explanation](https://youtu.be/M-OLmxuLdbU?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&t=683)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_70.png)\n",
        "\n",
        "The tensors we get from these tensor product form new vector spaces:\n",
        "\n",
        "> $V \\otimes V$\n",
        "\n",
        "> $V \\otimes V^{*}$\n",
        "\n",
        "> $V^{*} \\otimes V$\n",
        "\n",
        "> $V^{*} \\otimes V^{*}$\n"
      ],
      "metadata": {
        "id": "LU6hlHpSJEt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Tensor Products $\\otimes$ in Quantum Mechanics*#"
      ],
      "metadata": {
        "id": "F4gfVMumBdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two systems being described as a joint system:**\n",
        "\n",
        "1. the structure of the two systems is pre- served:\n",
        "\n",
        "2. a measurement on one of the systems does not disturb the other one;\n",
        "\n",
        "3. maximal information obtained on both systems separately gives maximal information on the joint system.\n",
        "\n",
        "With these conditions we show, within the framework of the propositional system formalism, that\n",
        "\n",
        "* if the systems are classical the joint system is described by the cartesian product of the corresponding phase spaces, and\n",
        "\n",
        "* if the systems are quantal the joint system is described by the tensor product of the corresponding Hilbert spaces.\n",
        "\n",
        "[Source: Paper Aerts](https://raw.githubusercontent.com/deltorobarba/papers/master/aerts.pdf)\n",
        "\n",
        "**One should deal with at least two operator products:**\n",
        "\n",
        "* one is given by the composition of two operators defined on the same vector space = product yields an operator de!ned on the common vector space of the factor operators\n",
        "\n",
        "* the other is the direct or tensor product of operators = leads to an operator de!ned on a different vector space: the direct or tensor product of the vector spaces of the factor operators\n",
        "\n",
        "The Kronecker product and some of its physical applications (Francisco M Fern√°ndez)"
      ],
      "metadata": {
        "id": "fZWP-A5kPUTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider two distinguishable particles:\n",
        "\n",
        "* Particle 1: its quantum mechanics is described by a complex vector space V. It has associated operators T1, T2, ..\n",
        "\n",
        "* Particle 2: its quantum mechanics is described by a complex vector space W. It has associated operators S1, S2, ..\n",
        "\n",
        "*This list of operators for each particle may include some or many of the operators you are already familiar with: position, momentum, spin, Hamiltonians, projectors, etc.*\n",
        "\n",
        "**Once we have two particles, the two of them together form our system.**\n",
        "\n",
        "* We are after the description of quantum states of this two-particle system.\n",
        "\n",
        "* On first thought, we may think that any state of this system should be described by giving the state v ‚àà V of the first particle and the state w ‚àà W of the second particle.\n",
        "\n",
        "* This information could be represented by the ordered list (v, w) where the first item is the state of the first particle and the second item the state of the second particle. This is a state of the two-particle system, but it is far from being the general state of the two-particle system. It misses remarkable new possibilities, as we shall soon see.\n",
        "\n",
        "We thus introduce a new notation. Instead of representing the state of the two-particle system with particle one in $v$ and particle two in $w$ as $(v, w)$, **we will represent it as $v \\otimes w$**.\n",
        "\n",
        "* <font color=\"blue\">This element $v \\otimes w$ will be viewed as a vector in a new vector space $V \\otimes W$ that will carry the description of the quantum states of the system of two particles.</font>\n",
        "\n",
        "* <font color=\"blue\">This $\\otimes$ operation is called the \"tensor product.\" In this case we have two vector spaces over $\\mathbb{C}$ **and the tensor product $V \\otimes W$ is a new complex vector space**:</font>\n",
        "\n",
        "> $v \\otimes w \\in V \\otimes W \\quad$ when $\\quad v \\in V, w \\in W$\n",
        "\n",
        "$\\operatorname{In} v \\otimes w$ there is no multiplication to be carried out, we are just placing one vector to the left of $\\otimes$ and another to the right of $\\otimes$.\n",
        "\n",
        "We have only described some elements of $V \\otimes W$, not quite given its definition yet. We now explain two physically motivated rules that define the tensor product completely.\n"
      ],
      "metadata": {
        "id": "3aRUE2W-PW9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Rule 1</u>: If the vector representing the state of the first particle is scaled by a complex number this is equivalent to scaling the state of the two particles. The same for the second particle. So we declare:**</font>\n",
        "\n",
        "> <font color=\"blue\">$(a v) \\otimes w=v \\otimes(a w)=a(v \\otimes w), \\quad a \\in \\mathbb{C}$\n",
        "\n",
        "* like in tensor algebra the linear map bzw. bilinear map, where a factor is attached only to one of two, not both\n",
        "\n",
        "<font color=\"blue\">**<u>Rule 2</u>: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition. We thus demand distributive properties for the tensor product:**</font>\n",
        "\n",
        "> <font color=\"blue\">$\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "\n",
        "> <font color=\"blue\">$v \\otimes\\left(w_{1}+w_{2}\\right)=v \\otimes w_{1}+v \\otimes w_{2}$\n",
        "\n",
        "Example in Quantum Phase Estimation: We first perform a Hadamard gate on the first qubit to get the state and then distribute the superposition (omitted the normalization factor of 1/‚àö2 for clarity):\n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "\n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "\n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "> The tensor product $V \\otimes W$ is thus defined to be the vector space whose elements are **(complex) linear combinations** of elements of the form $v \\otimes w$, with $v \\in V, w \\in W$, with the above rules for manipulation. The tensor product $V \\otimes W$ is the complex vector space of states of the two-particle system!"
      ],
      "metadata": {
        "id": "2hHr5XVJPZLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment: Let $v_{1}, v_{2} \\in V$ and $w_{1}, w_{2} \\in W$. A vector in $V \\otimes W$ constructed by superposition is**\n",
        "\n",
        "> $\n",
        "\\alpha_{1}\\left(v_{1} \\otimes w_{1}\\right)+\\alpha_{2}\\left(v_{2} \\otimes w_{2}\\right) \\in V \\otimes W\n",
        "$\n",
        "\n",
        "<font color=\"blue\">This shows clearly that a general state of the two-particle system cannot be described by stating the state of the first particle and the state of the second particle</font>. The above superpositions give rise to entangled states. An entangled state of the two particles is one that, roughly, **cannot be disentangled into separate states of each of the particles**.\n",
        "\n",
        "*Explanation 1*\n",
        "\n",
        "* **The \"Kronecker product\", better known as the tensor product**, is the natural notion of a product for spaces of states, when these are considered properly:\n",
        "\n",
        "* **A space of states is not a Hilbert space $\\mathcal{H}$, but the projective Hilbert space $\\mathbb{P} \\mathcal{H}$ associated to it**. This is the statement that quantum states are rays in a Hilbert space.\n",
        "\n",
        "* <font color=\"blue\">Now, **why does the physical notion of combining the spaces of states of individual systems into a space of states of the combined system correspond to taking the tensor product?**</font>\n",
        "\n",
        "  * The reason is that <font color=\"blue\">**we want every action of an operator (which are linear maps) on the individual states to define an action on the combined state**</font>\n",
        "\n",
        "  * and the tensor product is exactly that, since, <font color=\"red\">for every pair of linear maps $T_{i}: \\mathcal{H}_{i} \\rightarrow \\mathcal{H}$ (which is a bilinear map $\\left(T_{1}, T_{2}\\right): \\mathcal{H}_{1} \\times \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$ ) there is a unique linear map $T_{1} \\otimes T_{2}: \\mathcal{H}_{1} \\otimes \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$</font>\n",
        "\n",
        "* Alternatively, <font color=\"blue\">**concentrating more on the projective nature of the spaces of states, we observe that $|\\psi\\rangle$ and $a|\\psi\\rangle$ are the same state for any $a \\in \\mathbb{C}$.**</font> Therefore, denoting the sought-for physical product by $\\otimes$ (i.e. not assuming it is the tensor product), we must demand that\n",
        "\n",
        "><font color=\"red\">$\n",
        "|\\psi\\rangle \\otimes|\\phi\\rangle=(a|\\psi\\rangle) \\otimes|\\phi\\rangle=a(|\\psi\\rangle \\otimes|\\phi\\rangle)\n",
        "$</font>\n",
        "\n",
        "* since the states produced by $|\\psi\\rangle$ and $a|\\psi\\rangle$ must yield the same state, i.e. map onto the same projective state. **This obviously fails for the cartesian product, since the pair $(a|\\psi\\rangle,|\\phi\\rangle)$ <u>is not a multiple</u> of the $\\operatorname{pair}(|\\psi\\rangle,|\\phi\\rangle)$, but it is true for the tensor product**.\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/148378/importance-of-kronecker-product-in-quantum-computation\n",
        "\n",
        "*Explanation 2*\n",
        "\n",
        "* In der Mathematik k√∂nnen mit Hilfe des Tensorprodukts (Kronecker-Produkts) von PauliMatrizen (mit Einheitsmatrix) die Darstellungen der h√∂heren Clifford-Algebren √ºber den reellen Zahlen aufgebaut werden.\n",
        "\n",
        "* Pauli-Matrizen k√∂nnen zur Darstellung von Hamilton-Operatoren und zur N√§herung der Exponentialfunktion solcher Operatoren verwendet werden. Sind $\\sigma_{0}, \\sigma_{1}, \\sigma_{2}, \\sigma_{3}$ die vier Pauli-Matrizen, so kann man mit Hilfe des Kronecker-Produkt h√∂herdimensionale Matrizen erzeugen.\n",
        "\n",
        "> $\n",
        "p:=\\sigma_{\\mu_{1}} \\otimes \\sigma_{\\mu_{2}} \\otimes \\ldots \\otimes \\sigma_{\\mu_{n}} \\quad ; \\quad \\mu_{1}, \\mu_{2}, \\ldots, \\mu_{n} \\in\\{0,1,2,3\\} \\quad ; \\quad n \\in \\mathbb{N}\n",
        "$\n",
        "\n",
        "* **Das Kronecker-Produkt von Pauli-Matrizen tritt bei der Beschreibung von Spin-1/2-Systemen auf, die aus mehreren Teilsystemen aufgebaut sind**.\n",
        "\n",
        "* Der Zusammenhang ist dadurch gegeben, dass **das Tensorprodukt zweier Operatoren in der zugeh√∂rigen Matrixdarstellung durch das Kronecker-Produkt der Matrizen gegeben ist** (siehe [Kronecker-Produkt#Zusammenhang mit Tensorprodukten](https://de.wikipedia.org/wiki/Kronecker-Produkt#Zusammenhang_mit_Tensorprodukten)).\n",
        "\n",
        "\n",
        "https://de.wikipedia.org/wiki/Pauli-Matrizen#Kronecker-Produkt_von_Pauli-Matrizen\n",
        "\n",
        "*Explanation 3*\n",
        "\n",
        "* The kronecker product in group theory is widely used, especially with [Wigner D-function](https://de.wikipedia.org/wiki/Wignersche_D-Matrix)\n",
        "\n",
        "* The main purpose of its use in physics is to get the higher dimensional vector space. For example, in atomic physics, when we want to calculate the eigenvalues and eigenvectors of a system of spins 1/2 or spin Hamiltonian.\n",
        "\n",
        "* We analytically or with the help of computer diagonalize spin Hamiltonian and find eigenvectors and eigenvalues with the kronecker product:\n",
        "\n",
        "  * By appling the kronecker product between differnt spins matrices e.g., two matrices (dimensions 2 √ó 2) of spins 1/2, we will get the matrix of dimensions (4 √ó 4).\n",
        "\n",
        "  * This method is very compact, which means we can use the computer to get the eigenvectors and eigenvalues of matrix after applying kronecker product for higher number of spins e.g., for the system of spins 1/2.\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104019\n",
        "\n",
        "*Explanation 4*\n",
        "\n",
        "In quantum theory the analog of a Cartesian product of classical phase spaces is a tensor product of Hilbert spaces.\n",
        "\n",
        "https://quantum.phys.cmu.edu/CQT/chaps/cqt06.pdf\n",
        "\n",
        "*Explanation 5*\n",
        "\n",
        "How do you describe the combined state of two qubits? **Remember that each qubit is a vector space, so they can't just be multiplied**. Instead, you use a tensor product, which is a related operation that creates a new vector space from individual vector spaces, and is represented by the $\\otimes$ symbol.\n",
        "\n",
        "For example, the tensor product of two qubit states $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ and $\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]$ is calculated\n",
        "\n",
        "> $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right] \\otimes\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]=\\left[\\begin{array}{l}a\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right] \\\\ b\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$\n",
        "\n",
        "The result is a four-dimensional matrix, with each element representing a probability.\n",
        "\n",
        "For example, $a c$ is the probability of the two qubits collapsing to 0 and $0, a d$ is the probability of 0 and 1, and so on.\n",
        "\n",
        "Just as a single qubit state $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ must meet the requirement that $|a|^{2}+|b|^{2}=1$ in order to represent a quantum state,\n",
        "\n",
        "a two-qubit state $\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$ must meet the requirement that $|a c|^{2}+|a d|^{2}+|b c|^{2}+|b d|^{2}=1$\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/quantum/overview-algebra-for-quantum-computing\n",
        "\n",
        "*Explanation 6*\n",
        "\n",
        "If we have a set, denoted by 'a', of possible outcomes from one event and a set of outcomes for another event, denoted by 'b', then the possible outcomes for the union event is always the tensor product a‚äób.\n",
        "\n",
        "https://www.quora.com/What-is-a-tensor-product-in-quantum-mechanics\n",
        "\n",
        "*Explanation 7*\n",
        "\n",
        "At some point in the history of quantum mechanics, it was accepted that a single particle is described by a wavefunction which is a function of the position of the particle r, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        ")\n",
        ".\n",
        "\n",
        "At some (possibly later) point it was also accepted that two particles are described by a wavefunction which is a function of the positions of each one of the particles, r1 and r2, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        "1\n",
        ",\n",
        "r\n",
        "2\n",
        ")\n",
        ".\n",
        "\n",
        "In other words, the Hilbert space describing the two-particle system is the tensor product of the Hilbert spaces describing the system of each particle.\n",
        "\n",
        "https://physics.stackexchange.com/questions/53039/when-and-how-did-the-idea-of-the-tensor-product-originate-in-the-history-quantum\n",
        "\n",
        "*Explanation 8*\n",
        "\n",
        "Tensor Products are used to describe systems consisting of multiple subsystems. Each subsystem is described by a vector in a vector space (Hilbert space). For example, let us have two systems I and $/ /$ with their corresponding Hilbert spaces $H_{1}$ and $H_{11}$. Thus, using the bra-ket notation, the vectors $\\left|\\Psi_{1}\\right\\rangle$ and $\\mid \\Psi_{I I}$ ) describe the states of system I and $\\|$ with the state of the total system given by the tensor product $\\left|\\psi_{i}\\right\\rangle \\otimes\\left|\\psi_{1 I}\\right\\rangle$.\n",
        "\n",
        "https://www.quantiki.org/wiki/tensor-product\n",
        "\n",
        "*Explanation 9*\n",
        "\n",
        "The Hilbert space of a composite system is the Hilbert space tensor product of the state spaces associated with the component systems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "Others:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_mechanics\n",
        "\n",
        "http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html\n",
        "\n",
        "https://docs.microsoft.com/de-de/azure/quantum/overview-algebra-for-quantum-computing"
      ],
      "metadata": {
        "id": "QWKNU3sBPbDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary: Tensor Transformations & Einstein Notation*"
      ],
      "metadata": {
        "id": "pBNDxf7EBkj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "JIDiucDuBmiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Tensor Transformation**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_overview.png)"
      ],
      "metadata": {
        "id": "yZth_WAwBoY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)"
      ],
      "metadata": {
        "id": "g_PC1EsqBqK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "UxjiWAuDBsC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vectors and covectors (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)"
      ],
      "metadata": {
        "id": "u4TZq3cvBtyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vector fields and covector fields / differential forms (not single vectors!) (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)"
      ],
      "metadata": {
        "id": "0umDhwJfBvmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einstein Notation**\n",
        "\n",
        "* $\\sum_{i=1}^{3} a_{i} x_{i}=a_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}$  is in this case the same as saying: $a_{i} x_{i}$\n",
        "\n",
        "* See video: [4 rules of Einstein notation](https://www.youtube.com/watch?v=CLrTj7D2fLM&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=3)\n",
        "\n",
        "* This derivation to transform matrix components is pretty complex and can be written in a much easier way:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_25.png)\n",
        "\n",
        "If we have the same index on and top and on the bottom we end up summing over that index letter and can drop the summation sign, in this case: $\\color{red}{\\sum_{k=1}^{n}}$:\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "**Because when we see an index repeated on the top and bottom we know that there is a summation that's going to happen.**\n",
        "\n",
        "Another examples:\n",
        "\n",
        "> $\\color{red}{\\sum_{q=1}^{n}} \\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "can be rewritten by dropping the summation sign because ${\\color{red}{q}}$ (to which the sum sign refers to) is on the top and bottom:\n",
        "\n",
        "> $\\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "And as for the total formula:\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}= \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=  B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_26.png)\n",
        "\n",
        "* According to the [Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation), when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set \\{1,2,3\\} ,\n",
        "\n",
        "> $\n",
        "y=\\sum_{i=1}^{3} c_{i} x^{i}=c_{1} x^{1}+c_{2} x^{2}+c_{3} x^{3}\n",
        "$\n",
        "\n",
        "is simplified by the convention to:\n",
        "\n",
        ">$\n",
        "y=c_{i} x^{i}\n",
        "$\n",
        "\n",
        "* The upper indices are not exponents but are indices of coordinates, coefficients or basis vectors.\n",
        "\n",
        "In [general relativity](https://en.wikipedia.org/wiki/General_relativity), a common convention is that\n",
        "\n",
        "* the Greek alphabet is used for space and time components, where indices take on values 0,1,2 ,\n",
        "or 3 (frequently used letters are $\\mu, v, \\ldots),$\n",
        "\n",
        "* the Latin alphabet is used for spatial components only, where indices take on values $1,2,$ or 3 (frequently used letters are $i, j, \\ldots)$\n",
        "\n",
        "*Die [Penrosesche graphische Notation](https://de.m.wikipedia.org/wiki/Penrosesche_graphische_Notation) ist eine alternative Schreibweise f√ºr die Darstellung von Tensoren!*"
      ],
      "metadata": {
        "id": "OCv-TJ6LBxzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Field*"
      ],
      "metadata": {
        "id": "pAHNvHgpB0r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorfeld**\n",
        "\n",
        "* Tensorfelder sind Funktionen, **die jedem Punkt einen Tensor zuordnen** (Tensor meint in diesem Fall ein rein algebraisches Objekt)\n",
        "\n",
        "* Tensorfelder werden auf ihre analytischen Eigenschaften untersucht (zB differenziert). Man erh√§lt durch Differenzieren eines Tensorfeldes wieder ein Tensorfeld. Tensorfelder sind besondere glatte Abbildungen, die in Tensorb√ºndel hinein abbilden (siehe unten).\n",
        "\n",
        "* Sei $M$ eine differenzierbare Mannigfaltigkeit. Ein [Tensorfeld](https://de.wikipedia.org/wiki/Tensorfeld) vom Typ (r,s) ist ein glatter [Schnitt](https://de.m.wikipedia.org/wiki/Schnitt_(Faserb√ºndel)) im Tensorb√ºndel $T_{s}^{r}(M)$.\n",
        "\n",
        "  * Ein Tensorfeld ist also ein glattes Feld $M \\rightarrow T_{s}^{r}(M),$ welches jedem Punkt der Mannigfaltigkeit einen (r,s)-Tensor zuordnet.\n",
        "\n",
        "  * Die Menge der Tensorfelder wird oft mit $\\Gamma^{\\infty}\\left(T_{s}^{r}(M)\\right)$ bezeichnet."
      ],
      "metadata": {
        "id": "TP8jXL78B271"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensordichte**\n",
        "\n",
        "* [Tensordichte](https://de.wikipedia.org/wiki/Tensordichte) ist die Quantit√§tsgr√∂√üe eines Tensorfeldes (Generalisierung)\n",
        "\n",
        "* die Tensordichte ist eine **Verallgemeinerung der Tensorfelder** in der Tensoranalysis\n",
        "\n",
        "* wurde eingef√ºhrt, um den ‚ÄûUnterschied zwischen Quantit√§t und Intensit√§t, soweit er physikalische Bedeutung hat‚Äú, zu erfassen: ‚Äûdie Tensoren sind die Intensit√§ts-, die Tensordichten die Quantit√§tsgr√∂√üen‚Äú.\n",
        "\n",
        "* eine **Tensordichte** ordnet einem Koordinatensystem ein Tensorfeld derart zu, dass es bei einem Koordinatenwechsel mit dem Absolutbetrag der Funktionaldeterminante multipliziert wird. Eine Tensordichte der Stufe null ist demnach eine skalare Dichte, deren Integral gem√§√ü dem Transformationssatz eine Invariante liefert."
      ],
      "metadata": {
        "id": "WCB2oE9SB4xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field*"
      ],
      "metadata": {
        "id": "r-rllUswCV7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis of Vector Fields**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Tensorfeld\n",
        "\n",
        "> Basis Vectors = Partial Derivatives (Jacobian)\n",
        "\n",
        "> **From Single Vectors to Vector Fields (Transformations)**\n",
        "\n",
        "Energy Momentum Tensor: https://youtu.be/ii7rffG0EwU\n",
        "\n",
        "A (basis) vector can be re-interpreted as a partial derivative (see image below):\n",
        "\n",
        "> $\\overrightarrow{e_{x}} \\equiv \\frac{\\partial \\vec{R}}{\\partial x}$\n",
        "\n",
        "https://www.youtube.com/watch?v=rr5qEb_kT6c&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=3\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_112.png)\n",
        "\n",
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=OMCguyCnTQk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=4\n",
        "\n",
        "* Now if you want to get the coefficients when your old basis in Cartesian and your new basis is Polar coordinates, then you have another forward map at every point in the polar coordinate system\n",
        "\n",
        "* if you now think of the basis vectors as partial derivatives, it makes things much easier. Use mutlivariable chain rules:\n",
        "\n",
        "  * the first old basis vector $\\overrightarrow{e_{x}}$ can be thought of as the partrial derivative in the x direction: $\\frac{\\partial \\vec{R}}{\\partial x}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * the second old basis vector $\\overrightarrow{e_{y}}$ can be thought of as the partrial derivative in the y direction: $\\frac{\\partial \\vec{R}}{\\partial y}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * same goes for the new basis vector (polar coordinates) as partial derivaties into r and $\\theta$ directions\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_90.png)\n",
        "\n",
        "The underlined part are the coefficients to move from one basis (cartesian) to another (polar) (Achtung: not normalised):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_91.png)\n",
        "\n",
        "The forward matrix = the Jacobian matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_92.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_95.png)\n",
        "\n",
        "And you can do the same the other way around to get the Backward transform, which is the inverse Jacobian:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_93.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_94.png)\n",
        "\n",
        "It total it looks likes this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_96.png)\n",
        "\n",
        "And we can store the (forward & backward) coefficients in matrix\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_97.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tYEF_rwU4Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Field Components**\n",
        "\n",
        "> Vectors Components = Derivatives in Vector Fields (vector fields of tangent vectors along curves)\n",
        "\n",
        "> Task: Figure out vector components in a new basis, **but instead of one vector, we consider vector fields**.\n",
        "\n",
        "We consider vectors along a curve (=tangent vectors)\n",
        "\n",
        "https://www.youtube.com/watch?v=9yOb9gHnLUk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=5\n",
        "\n",
        "\n",
        "The cases are both the same:\n",
        "\n",
        "* on we have a single vector in a new basis constructed from the basis vectors in the old basis and its components\n",
        "\n",
        "* on the bottom we have a whole vector field constructed from the old basis (using the chain rule!) and the vectir components ar derivates!\n",
        "\n",
        "* Framed in red is the vector we want to expand, in blue framed the basis vectors and in green framed the vector components\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_98.png)\n",
        "\n",
        "Example: in the cartesian coordinate system the basis vectors are every where the same. Just components are everywhere different. But it‚Äôs easy using the multivariable chain rule when you take the vector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_99.png)\n",
        "\n",
        "Other example iof tangents on a circle in a cartesian coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_100.png)\n",
        "\n",
        "\n",
        "Do these component make sense? Let's check with an example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_101.png)\n",
        "\n",
        "Also works in the polar coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_103.png)\n",
        "\n",
        "Checking if it's true, and it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMCdUWicVfKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contravariance of Vector Field Components**\n",
        "\n",
        "https://www.youtube.com/watch?v=zKuyaQ4JRs8&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=6&t=154s\n",
        "\n",
        "* Vector components are contravariant\n",
        "\n",
        "* We can follow the same reasoning for vector fields of tangent vectors along curves\n",
        "\n",
        "In this example, the polar components have two be equal to the Cartesian components multiplied by the backward transform. So we get this transformation formula for the components of the tangent vectors (the box on the bottom right):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "Partial derivative basis vectors transform one way and the vector component derivatives transform the other way, the vector components are contravariant. (Dervative coefficients are opposites) But don't memorize them! Simply use multivariable chain rules for the four formulas on the bottom and you get the transformation (chain rules over cartesian frames in purple, chain rule sover polar coordinates framed in green)::\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_105.png)\n",
        "\n",
        "Remember when we used for forward and backward transforms with single vectors the following formula:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_107.png)\n",
        "\n",
        "**This is similar for Vector Fields: basis vectors transform one way, and the vector components transform the other wa, using the Jacobian $J$ and the Jacobian inverse $J^-1$ as the forward and backward transforms:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_106.png)\n",
        "\n",
        "..and this is for the specific example we had for the circular curve with radius 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_108.png)\n",
        "\n",
        "**One important last thing: we remove the position vector $R$ and leave the <u>derivative operators = (basis) vector</u>, and they will be considered basis vectors from now on:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_109.png)\n",
        "\n",
        "And this makes sense: The partial derivative with respect to x points in the x direction (and so on):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_110.png)\n",
        "\n",
        "This is useful becasue position vectors $R$ rely on an origin, and as we will see later, on manifolds on curved surfaces we cannot rely on the existnce of an origin point:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_111.png)"
      ],
      "metadata": {
        "id": "J152JTPfWWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covector Field (1-form-Differential Form)*"
      ],
      "metadata": {
        "id": "8h7LKwE9CYwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector Field (= 1-form-Differential Form)**\n",
        "\n",
        "> <font color=\"blue\">**Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind**\n",
        "\n",
        "> **Covectors: Differential Forms = Covector Fields**\n",
        "\n",
        "Das Differential eines Skalarfeldes (differentialform) ist ein Covector field, weil differentiale von skalaren covectoren sind??\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_119.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8&t=201s\n",
        "\n",
        "Re-interprete $d$ from $df$ as being an operator that takes a scalar field and outputs a covector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_120.png)\n",
        "\n",
        "Example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)\n",
        "\n",
        "The way to get the covector fields is by tracing out the level sets of the scalar function: **Skalarfeld links mit Temperaturen, Kovektorfeld rechts mit den Konturen, wo √ºberall die gleichen Konturen existieren (√Ñquivalenzen)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)\n",
        "\n",
        "Another example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_124.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_122.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)\n",
        "\n",
        "Example: If we think of x as a scalar field, it would look like this: it‚Äôs a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [Level Set (Niveaumenge)](https://de.wikipedia.org/wiki/Niveaumenge): **die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist**\n",
        "\n",
        "Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)\n",
        "\n",
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)\n",
        "\n",
        "**How to calculate now? - How do covector fields like $df$ act on $\\vec{v}$ to give us output values?**\n",
        "\n",
        "> Count the number of tangent lines to the curve at p (where the vector originates at point p)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_130.png)\n"
      ],
      "metadata": {
        "id": "mjpXmcIJSASJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But what's the geometrical meaning of $df$ ($\\vec{v}$)?**\n",
        "\n",
        "On a map we can draw out curves of constant elevation (which is the same thing as level sets). We can think of this level set drawing of a mountain as a covector field associated with the mountain.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_131.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_132.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_133.png)\n",
        "\n",
        "> $d f(\\vec{v})$ tells us the rate of change of $f$ when moving at velocity $\\vec{v}$. **$d f(\\vec{v})$ is the directional derivative of $f$ in direction $\\vec{v}$**.\n",
        "\n",
        "**Covector Field Components: The following are the basis covectors**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_134.png)\n",
        "\n",
        "So just as we can expand individual covectors into linear combinations of dual basis vectors (and of course we get different components depending on which basis we use (all on top), we can also expand differential forms - also callee covector fields - into linear combinations of other covector fields where we get different components depending o which basis we use (on the bottom).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_135.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=r_20yXBdhJk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=9\n",
        "\n"
      ],
      "metadata": {
        "id": "05_SpyG3SEPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation Rules of Differential Forms (Covector Fields)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)\n",
        "\n",
        "Same logic applies for covector fields: if we for example want to build the Cartesian basis covector fields our of the polar basis covector fields (from new to old), we use the following coefficients, which are the entries of the Jacobian matrix (=forward transform, because covector basis components act contravariant).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_137.png)\n",
        "\n",
        "**Summary of all transformations (between 2 basis of covector fields, for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=4doR1XCXzKU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=10"
      ],
      "metadata": {
        "id": "FffzqkWvSGIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration with Differential Forms**\n",
        "\n",
        "* With the following interpretation of differential forms we can create the integration and it doesn't depend on coordinate systms at all\n",
        "\n",
        "* Also we **just need start point and end point** and count the number of pierced covector components instead of computing the integral at each point on the line\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_147.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_149.png)\n",
        "\n",
        "Since the covector fields and the paths are the same in both following cases, the result of the integral which is negative 4, is also the same in both cases:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_148.png)\n",
        "\n",
        "> **Covector fields are invariant of the choice of coordinates. Covector field components depend on the choice if coordinate.*And that's why when we change the variable in an integral we still get the same answer.**\n",
        "\n",
        "https://www.youtube.com/watch?v=kyzSofggsqg&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=11\n",
        "\n",
        "https://www.youtube.com/watch?v=PzrGGbX-_54&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=12"
      ],
      "metadata": {
        "id": "rAtqpzXZSH87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field vs Covector Field*"
      ],
      "metadata": {
        "id": "ihDto5NgCay8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gradient ‚àá vs ùëë operator (exterior derivative/differential)\n",
        "\n",
        "https://www.youtube.com/watch?v=nJpONHO_X5o&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=15\n",
        "\n",
        "https://www.youtube.com/watch?v=Do5vzLJRWRE&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=16\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_139.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_zLHtahCwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Extrinsic (Exterior) Geometry vs Intrinsic Geometry*"
      ],
      "metadata": {
        "id": "BJaouQGICdBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=VHkL5HpL0HY&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=7\n",
        "\n",
        "* Let's take a 2D surface like earth, and someone drives along it\n",
        "\n",
        "* we want to get the velocity of the car. We need position vectors, where we need an origin point (center of earth), then we get 2 vectors, take their limit to compute velocity\n",
        "\n",
        "* There are 2 issues with this:\n",
        "\n",
        "  1. Firstly the origin point that we've chosen doesn't live o the earth's 2D surface. We picked an origin point that was exterior to the surface.\n",
        "\n",
        "  2. Secondly the target velocity actually leaves the surface that we are studying and goes off into the outside space - **Remember here Lie algreba and Lie group**: the tangent is outside the 2D surface and the tangent point is the Lie algebra. All on the 2D surface however is part of the Lie group.\n",
        "\n",
        "* So here, both the origin point and the velocity vector are both defined using the 3 dimensional space, even though we are only studying 2 dimensional spherical surface\n",
        "\n",
        "* **This is why it's all called exterior geometrty and exterior product** - studying something outside\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_113.png)"
      ],
      "metadata": {
        "id": "zH_Dj6_LEh_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to find the distance between 2 points on a map of earth, it's called \"intrinsiv geometry\" when we are not allowed to leave the surface (and hence cannot draw a straight line, but need to find a geodesic on the flat map).\n",
        "\n",
        "And that gets even more complicated in General Relativity:\n",
        "\n",
        "1. 4D spacetime is curved space, so we cannot draw a straight line int he curved space. So that means we cant draw poisiton vectors.\n",
        "\n",
        "2. And we cannot pick an origin outside the 4D spacetime, because that would mean picking a point outside of the universe\n",
        "\n",
        "That's why its called intrinsic geometry: you need to stay inside! **But how do we study velocity if we are not allowed to use position vectors?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_114.png)"
      ],
      "metadata": {
        "id": "Y8Mr4AgTEjud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution: we cannot draw straight lines, but we can draw curved paths on a 2D on a map for exmaple**.\n",
        "\n",
        "> **So we can't use normal position vectors to talk about directions on a surface, but we can use derivative operators to talk about different directions:**\n",
        "\n",
        "* If have have some path that's traveling around in our curved space $(x(\\lambda), y(\\lambda))$, we can still consider the direction that the path is pointing in using the derivative with respect to the curve parameter $\\frac{d}{d \\lambda}$\n",
        "\n",
        "* And we can break this direction up into its x and y components using the multivariable chain rules that we have for derivative operators $\\frac{d}{d \\lambda}=\\frac{d x}{d \\lambda} \\frac{\\partial}{\\partial x}+\\frac{d y}{d \\lambda} \\frac{\\partial}{\\partial y}$\n",
        "\n",
        "* But Achtung: you shouldn't think of this direction vector as actually connecting the two points on the earth (origin and destination of yellow arrow). The derivative just gives the general direction that the curve is traveling in at a given point.\n",
        "\n",
        "* So derivatives with us a way to talk about directions on a curved surface in a way that is complete intrinsic to the surface and doesn't require an outside space in any way.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_115.png)"
      ],
      "metadata": {
        "id": "cabaMULeElaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same approach in 4 D space: we can't draw straight lines, but we can still draw curved paths. And that means we can take derivatives with respect to the paths parameters to get a sense of different directions.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_116.png)"
      ],
      "metadata": {
        "id": "PFhrRHMWEnRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> So the old notation uses actual vectors from the **vector spaces R2 and R3** to define directions. But the new notation use the **vector space of derivative operators**.\n",
        "\n",
        "> **The vector space of derivative operators is formally known the \"Tangent Vector Space\" and is denoted: $T_{p}M$**, which is the vector space of derivatives at some point p on a surface $M$. And keep in mind that the vectors on the left and the vectors on the right are in fact from different vector spaces!!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_117.png)\n",
        "\n",
        "And they do form a vector space because we can scale and add them linearly! So these partial derivatives are vectors in the tangent space $T_{p}M$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_118.png)"
      ],
      "metadata": {
        "id": "dvn3ZKheEpKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Metric Tensor Field*"
      ],
      "metadata": {
        "id": "QHq7RwnYCe8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Metric Tensor to compute arc length of a curve in Flat & Curved Space**\n",
        "\n",
        "**In Flat Space**\n",
        "\n",
        "So in summary the equation for calculating the arc length of a curve is an integral that depends on the magnitude of the curves tangent vectors $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|$ in this:\n",
        "\n",
        "> $\\operatorname{arclength}=\\int\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\| d \\lambda$\n",
        "\n",
        "And to calculate the squared magnitude of the tangent vectors we need to use the dot product\n",
        "\n",
        "> $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|^{2}=\\frac{d \\vec{R}}{d \\lambda} \\cdot \\frac{d \\vec{R}}{d \\lambda}$\n",
        "\n",
        "And we end up with this equation in Cartesian coordinates:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$\n",
        "\n",
        "And this equation in polar coordinates:\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p^{j}}\\right)$\n",
        "\n",
        "And the basis vector dot products $\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$ and $\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p j}\\right)$ give us the components of the metric tensor:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda} g_{i j}$\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda} \\widetilde{g_{i j}}$\n",
        "\n",
        "\n",
        "**So the key to getting the arc length of a curve is the tangent vector magnitude. And the key to getting the tangent vector magnitude is the metric tensor components $g_{i j}$ and $\\widetilde{g_{i j}}$**.\n",
        "\n",
        "And remember: the metric tensor is a (0,2) tensor because its components obey 2 covariant transformation laws:\n",
        "\n",
        "> $\\begin{aligned} \\widetilde{g_{i j}} &=\\frac{\\partial c^{k}}{\\partial p^{i}} \\frac{\\partial c^{l}}{\\partial p^{j}} g_{k l} \\\\ g_{k l} &=\\frac{\\partial p^{i}}{\\partial c^{k}} \\frac{\\partial p^{j}}{\\partial c^{l}} \\widetilde{g_{i j}} \\end{aligned}$\n",
        "\n",
        "In flat space there is only 1 metric tensor with which you can calculate the arc length of any curve as long as we can do this integral.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_150.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=BbQmTmSzUCI&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=13\n"
      ],
      "metadata": {
        "id": "L3N2cBc6FoEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Curved Space**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_142.png)\n",
        "\n",
        "In curved space: Vector field: vector changes from point to point. Covector fields have covectors that change from point to point. And now a metric tensor field involves a different metric tensor being placed everywhere in space that change from point to point.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_151.png)\n",
        "\n",
        "Every curved space has its own metric tensor field that gives you the rules for measuring distance on it:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_152.png)\n",
        "\n",
        "Metric tensors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_143.png)\n",
        "\n",
        "Simplify them:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_144.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_145.png)\n",
        "\n",
        "For the intrinsic view (left side only) we can remove $R$ vectors completely and treat derivative operators themselves as the vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_146.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=SmjbpIgVKFs&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=14"
      ],
      "metadata": {
        "id": "yS56QI6iGRym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDNzWru2eKpK"
      },
      "source": [
        "##### <font color=\"blue\">*Homological Algebra*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Homological_algebra"
      ],
      "metadata": {
        "id": "4-rErV2smYu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Simplicial Homology*"
      ],
      "metadata": {
        "id": "uOwaDiWZDTxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantum-journal.org/views/qv-2023-01-26-70/"
      ],
      "metadata": {
        "id": "pRRqjqbIK3yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1416.png)\n"
      ],
      "metadata": {
        "id": "0l5UtGJTKpJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1417.png)\n"
      ],
      "metadata": {
        "id": "U13kdlWrKrds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1418.png)\n"
      ],
      "metadata": {
        "id": "wVu6RaMlKsPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1419.png)\n"
      ],
      "metadata": {
        "id": "ud88megeKtzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1420.png)\n"
      ],
      "metadata": {
        "id": "_hlWriNLKu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1421.png)\n"
      ],
      "metadata": {
        "id": "2b-HFVA1KwWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1422.png)\n"
      ],
      "metadata": {
        "id": "9m7jXEzgKxIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1423.png)\n"
      ],
      "metadata": {
        "id": "_mR6uRUtKx_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Free abelian groups: https://youtu.be/xPeeFp_Hd3A\n",
        "\n",
        "simplicial complexes: https://youtu.be/Uq4dTjHfLpI\n",
        "\n",
        "computing homology groups: https://youtu.be/YNBi4Ix3cY0\n",
        "\n",
        "computing more homology groups: https://youtu.be/l7QWg0UzBRA\n",
        "\n",
        "betti numbers: https://youtu.be/NgrIPPqYKjQ\n",
        "\n",
        "persistent homology: https://youtu.be/ktKCzMmDXDk\n",
        "\n",
        "first isomorphism theorem: https://youtu.be/2kmIHyD8zTk\n",
        "\n",
        "isomorphic graphs: https://youtu.be/EwV4Puk2coU\n",
        "\n",
        "cohomoly and forms: https://youtu.be/2ptFnIj71SM\n",
        "\n",
        "differential forms: https://youtu.be/CYz_s82JnY8\n",
        "\n",
        "covectors and one-forms: https://youtu.be/ziD8ewQjaf4"
      ],
      "metadata": {
        "id": "gNk02DYZ_HXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Homology explained via Graph and Group Theory**\n",
        "\n",
        "* Find the nullspace of the matrix, then we can find all the cycles (min 29)\n",
        "\n",
        "* The number of vectors in the spanning set of the null space of the reduced matrix representation would be the number of generating cycles\n",
        "\n",
        "* A [free abelian group](https://en.m.wikipedia.org/wiki/Free_abelian_group) is an abelian group (commutative) with a basis.\n",
        "\n",
        "Source: [An introduction to homology | Algebraic Topology | NJ Wildberger](https://www.youtube.com/watch?v=ShWdSNJeuOg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1391.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1392.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1394.png)\n",
        "\n",
        "Now adding a higher dimensional disc:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1395.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1396.png)\n",
        "\n",
        "Continue: https://www.youtube.com/watch?v=2wn10l9qbJI&list=PL6763F57A61FE6FE8&index=36&t=1362s\n",
        "\n",
        "Playlist: https://www.youtube.com/playlist?list=PL6763F57A61FE6FE8"
      ],
      "metadata": {
        "id": "QdlLIADIS-QI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeX5czO1VjRi"
      },
      "source": [
        "**Simplicial Homology (Application of Quotient Groups)**\n",
        "\n",
        "> <font color=\"blue\">**Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence**\n",
        "\n",
        "[Simplicial Homology: On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://m0nads.wordpress.com/tag/persistent-homology/)\n",
        "\n",
        "*Since Bn is a subgroup of Zn, we may form the quotient group Hn = Zn/Bn -> so Modulo (=Restwerte) ist dann die Dimension der L√∂cher (=invarianten)*\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_07.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_10.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_08.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_09.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_06.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.intmath.com/differential-equations/3-integrable-combinations.php"
      ],
      "metadata": {
        "id": "xWw52dxv_93x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple examples of homology:\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Graph_homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology"
      ],
      "metadata": {
        "id": "iP7S89gV1zb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In biology, [homology](https://en.m.wikipedia.org/wiki/Homology_(biology)) is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles **are all derived from the same ancestral tetrapod structure**.\n",
        "\n",
        "https://de.wiktionary.org/wiki/homolog\n",
        "\n",
        "Biologie: **von einer gemeinsamen Urform ableitbar**.\n",
        "\n",
        "Worttrennung: ho¬∑mo¬∑log, keine Steigerung\n",
        "\n",
        "Beispiel: Der Fl√ºgelbug des Vogels ist unserem Handgelenk homolog.\n",
        "\n",
        "https://de.wiktionary.org/wiki/Homologie\n",
        "\n",
        "Biologie: √úbereinstimmung von biologischen Strukturen (Organe, Makromolek√ºle et cetera) oder Verhaltensanteilen hinsichtlich ihrer Entwicklungsgeschichte und nicht in Hinsicht auf ihre Funktion"
      ],
      "metadata": {
        "id": "XvGbLiHsv5na"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FhQOJufFMx0"
      },
      "source": [
        "**Homology**\n",
        "\n",
        "* [Homology](https://en.m.wikipedia.org/wiki/Homology_(mathematics)) is a general way of **associating a sequence of algebraic objects, such as abelian groups (or modules), to other mathematical objects such as topological spaces.**\n",
        "\n",
        "* **Homology Groups**: Homology groups were originally defined in algebraic topology. Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry. See [Construction of homology groups](https://en.m.wikipedia.org/wiki/Homology_(mathematics)#Construction_of_homology_groups)\n",
        "\n",
        "* In homology we want to Linearize the equivalence relation! Homology and cohomology are linear theories (easier to compute, and methods of linear algebra applicable) (though in the process you loose a bit information\n",
        "\n",
        "\n",
        "* Homology counts components, holds, voids etc.\n",
        "\n",
        "* In topology we define something called homology for simplicial complexes.\n",
        "\n",
        "* **Homology of a simplicial complex is computable via linear algebra**.\n",
        "\n",
        "* Die Homologie ist ein mathematischer Ansatz, die Existenz von L√∂chern zu formalisieren.\n",
        "\n",
        "* Gewisse ‚Äûsehr feine‚Äú L√∂cher sind f√ºr die Homologie unsichtbar; hier kann u. U. auf die schwerer zu bestimmenden Homotopiegruppen zur√ºckgegriffen werden.\n",
        "\n",
        "1. define set of vertices V0 (=vector space V0 with basis given by the set of vertices) and set of edges V1\n",
        "\n",
        "2. linearize equivalence relation: instead of looking at ‚Äûtail of edge is equivalent to head of edge‚Äú, we consider the difference between these two\n",
        "\n",
        "  * **e $\\mapsto$ h(e) - t(e)** is the linear combination of two vertices, and hence an element of e zero! e wird abgebildet auf dem Element (h(e) - t(e))\n",
        "  * **h(e) $\\equiv$ t(e) mod (im d)** das heisst: h(e) ist identisch zu t(e) modulo dem Bild von d\n",
        "\n",
        "* The linear map d : V1 -> V0 sends an edge e to (h(e) - t(e))\n",
        "* e is an edge which is a basis element inside V1\n",
        "* So the image of d (difference) is just the supspace of V0 generated by these elements\n",
        "* Generate [equivalence relation](https://en.m.wikipedia.org/wiki/Equivalence_relation) (Homomorphiesatz?): impose reflexivity, symmetry and transivity.\n",
        "* So: to get the image of d you look at the subspace generated by e |‚Äî> h(e) - t(e)\n",
        "    * Closure under addition more or less corresponds to transitivity,\n",
        "    * closure under negation corresponds to symmetry, and\n",
        "    * closure under zero corresponds to reflexivity.\n",
        "    * So this precisely linearizes the equivalence relation from before.\n",
        "* Now we have one linear map. But we need two linear maps which composes 0 to get homology or cohomology. There are two ways you can get a composite to get 0 very easily.\n",
        "    * Either start from zero and map to V1 and then go to V0, or\n",
        "    * start at V1 and use d to get to V0 and then go to 0 with the zero map there.\n",
        "* Modular the image of this map which is zero\n",
        "\n",
        "* https://ncatlab.org/nlab/show/homology\n",
        "\n",
        "**Homological Algebra**\n",
        "\n",
        "* [Homological algebra](https://en.m.wikipedia.org/wiki/Homological_algebra) is the study of homological functors and the intricate algebraic structures that they entail; its development was closely intertwined with the emergence of category theory. A central concept is that of chain complexes, which can be studied through both their homology and cohomology.\n",
        "\n",
        "*A diagram used in the snake lemma, a basic result in homological algebra:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Snake_lemma_origin.svg/375px-Snake_lemma_origin.svg.png)\n",
        "\n",
        "\n",
        "* Why using Homological Algebra: **translate a problem of interest into a sequence of ‚Äûhigher‚Äú (=homological algebra) linear algebra problems**\n",
        "\n",
        "* homological algebra is the **study of homological functors** and the intricate algebraic structures that they entail.\n",
        "\n",
        "* One quite useful and ubiquitous concept in mathematics is that of **chain complexes**, which can be studied through both their homology and cohomology.\n",
        "\n",
        "* Homological algebra affords the means to **extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces**, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.\n",
        "\n",
        "**Simplicial Homology**\n",
        "\n",
        "* Die [Simpliziale Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie) ist in der Algebraischen Topologie, einem Teilgebiet der Mathematik, eine Methode, die einem beliebigen Simplizialkomplex **eine Folge [abelscher Gruppen](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe) zuordnet**.\n",
        "\n",
        "* Anschaulich gesprochen z√§hlt sie die L√∂cher unterschiedlicher Dimension des zugrunde liegenden Raumes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph homology**\n",
        "\n",
        "In algebraic topology and graph theory, [graph homology](https://en.m.wikipedia.org/wiki/Graph_homology) describes the homology groups of a graph, where the graph is considered as a topological space. It formalizes the idea of the number of \"holes\" in the graph. It is a special case of a simplicial homology, as a graph is a special case of a simplicial complex. Since a finite graph is a 1-complex (i.e., its 'faces' are the vertices - which are 0-dimensional, and the edges - which are 1-dimensional), the only non-trivial homology groups are the 0-th group and the 1-th group.\n",
        "\n",
        "The 1st homology group\n",
        "\n",
        "Similarly, $\\partial _{1}$ maps any cycle in C1 to the zero element of C0. In other words, the set of cycles in C1 **generates the null space (the kernel)** of $\\partial _{1}$.\n",
        "\n",
        "https://www.youtube.com/watch?v=ShWdSNJeuOg&t=240s"
      ],
      "metadata": {
        "id": "nKnPM0x-6BG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Graph Theory**\n",
        "\n",
        "In mathematics, [topological graph theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory) is a branch of graph theory. It studies the embedding of graphs in surfaces, spatial embeddings of graphs, and graphs as topological spaces.[1] It also studies immersions of graphs.\n",
        "\n",
        "To an undirected graph we may associate an abstract simplicial complex C with a single-element set per vertex and a two-element set per edge.[2] The geometric realization |C| of the complex consists of a copy of the unit interval [0,1] per edge, with the endpoints of these intervals glued together at vertices. In this view, embeddings of graphs into a surface or as subdivisions of other graphs are both instances of topological embedding, homeomorphism of graphs is just the specialization of topological homeomorphism, the notion of a connected graph coincides with topological connectedness, and a connected graph is a tree if and only if its fundamental group is trivial.\n",
        "\n",
        "Other simplicial complexes associated with graphs include the **Whitney complex or clique complex**, with a set per clique of the graph, and the matching complex, with a set per matching of the graph (equivalently, the clique complex of the complement of the line graph). The matching complex of a complete bipartite graph is called a chessboard complex, as it can be also described as the complex of sets of nonattacking rooks on a chessboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvWc8p5p4Qlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph (topology)**\n",
        "\n",
        "In topology, a branch of mathematics, a [graph is a topological space](https://en.m.wikipedia.org/wiki/Graph_(topology)) which arises from a usual graph\n",
        "G\n",
        "=\n",
        "(\n",
        "E\n",
        ",\n",
        "V\n",
        ")\n",
        "{\\displaystyle G=(E,V)} by replacing vertices by points and each edge\n",
        "e\n",
        "=\n",
        "x\n",
        "y\n",
        "‚àà\n",
        "E\n",
        "{\\displaystyle e=xy\\in E} by a copy of the unit interval\n",
        "I\n",
        "=\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "{\\displaystyle I=[0,1]}, where\n",
        "0\n",
        "{\\displaystyle 0} is identified with the point associated to\n",
        "x\n",
        "x and\n",
        "1\n",
        "1 with the point associated to\n",
        "y\n",
        "y. That is, as topological spaces, graphs are exactly the simplicial 1-complexes and also exactly the one-dimensional **CW complexes**."
      ],
      "metadata": {
        "id": "40IB_N2Q6Mk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **path** is a walk with no repeated vertices\n",
        "\n",
        "A closed path is a **cycle**."
      ],
      "metadata": {
        "id": "kSH33jQVs3bM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLoLSdtySVs"
      },
      "source": [
        "**Cycles (ker), boundaries (img) and chains**\n",
        "\n",
        "* Cycles (and its Kern): A cycle is a closed submanifold,\n",
        "\n",
        "* boundary is a cycle which is also the boundary of a submanifold.\n",
        "\n",
        "* Boundary operator on [chains](https://en.m.wikipedia.org/wiki/Chain_(algebraic_topology)) (and its image): The boundary of a chain is the linear combination of boundaries of the simplices in the chain. The boundary of a k-chain is a (k‚àí1)-chain. Note that the boundary of a simplex is not a simplex, but a chain with coefficients 1 or ‚àí1 ‚Äì thus chains are the closure of simplices under the boundary operator.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Chainline.svg/320px-Chainline.svg.png)\n",
        "\n",
        "* *The boundary of a polygonal curve is a linear combination of its nodes; in this case, some linear combination of A1 through A6. Assuming the segments all are oriented left-to-right (in increasing order from Ak to Ak+1), the boundary is A6 ‚àí A1.*\n",
        "\n",
        "* If the 1 -chain $c=t_{1}+t_{2}+t_{3}$ is a path from point $v_{1}$ to point $v_{4},$ where $t_{1}=\\left[v_{1}, v_{2}\\right], t_{2}=\\left[v_{2}, v_{3}\\right]$ and $t_{3}=\\left[v_{3}, v_{4}\\right]$ are its\n",
        "constituent 1 -simplices, then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\partial_{1} c &=\\partial_{1}\\left(t_{1}+t_{2}+t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(t_{1}\\right)+\\partial_{1}\\left(t_{2}\\right)+\\partial_{1}\\left(t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(\\left[v_{1}, v_{2}\\right]\\right)+\\partial_{1}\\left(\\left[v_{2}, v_{3}\\right]\\right)+\\partial_{1}\\left(\\left[v_{3}, v_{4}\\right]\\right) \\\\\n",
        "&=\\left(\\left[v_{2}\\right]-\\left[v_{1}\\right]\\right)+\\left(\\left[v_{3}\\right]-\\left[v_{2}\\right]\\right)+\\left(\\left[v_{4}\\right]-\\left[v_{3}\\right]\\right) \\\\\n",
        "&=\\left[v_{4}\\right]-\\left[v_{1}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Closed_polygonal_line.svg/320px-Closed_polygonal_line.svg.png)\n",
        "\n",
        "* *A closed polygonal curve, assuming consistent orientation, has null boundary. (deswegen f√ºhrt der boundary operator Œ¥1 alle Werte immer in Null, zumindest bei geschlossenen objekten)*\n",
        "\n",
        "* Example 2: The boundary of the triangle is a formal sum of its edges with signs arranged to make the traversal of the boundary counterclockwise.\n",
        "\n",
        "   * Cycle: **A chain is called a cycle when its boundary is zero**.\n",
        "\n",
        "   * Boundary: A chain that is the boundary of another chain is called a boundary.\n",
        "\n",
        "   * **Boundaries are cycles**, so chains form a chain complex, whose homology groups (cycles modulo boundaries) are called simplicial homology groups.\n",
        "\n",
        "* Example 3: A 0-cycle is a linear combination of points such that the sum of all the coefficients is 0. Thus, the 0-homology group measures the number of path connected components of the space.\n",
        "\n",
        "* Example 4: The plane punctured at the origin has nontrivial 1-homology group **since the unit circle is a cycle, but not a boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazhAwJAFmL-"
      },
      "source": [
        "**Chain Complex**\n",
        "\n",
        "A chain complex\n",
        "(\n",
        "A\n",
        "‚àô\n",
        ",\n",
        "d\n",
        "‚àô\n",
        ")\n",
        "(A_{\\bullet },d_{\\bullet }) is a sequence of abelian groups or modules ..., A0, A1, A2, A3, A4, ... connected by homomorphisms (called boundary operators or differentials)\n",
        "\n",
        "> **[Chain Complex](https://en.m.wikipedia.org/wiki/Chain_complex) is a sequence of homomorphism of abelian groups**\n",
        "\n",
        "A chain complex $V$. is a sequence $\\left\\{V_{n}\\right\\}_{n \\in \\mathbb{Z}}$ of abelian groups or modules (for instance yector spaces) or similar equipped with linear maps $\\left\\{d_{n}: V_{n+1} \\rightarrow V_{n}\\right\\}$ such that $d^{2}=0,$ i.e. the composite of two consecutive such maps is the zero morphism $d_{n} \\circ d_{n+1}=0$\n",
        "\n",
        "* https://ncatlab.org/nlab/show/chain+complex\n",
        "\n",
        "* https://ncatlab.org/nlab/show/zero+morphism\n",
        "\n",
        "* https://m0nads.wordpress.com/tag/persistent-homology/\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Simplicial_homology_-_exactness_of_boundary_maps.svg/384px-Simplicial_homology_-_exactness_of_boundary_maps.svg.png)\n",
        "\n",
        "* The boundary of a boundary of a 2-simplex (left) and the boundary of a 1-chain (right) are taken.\n",
        "\n",
        "* Both are 0, being sums in which both the positive and negative of a 0-simplex occur once. The boundary of a boundary is always 0.\n",
        "\n",
        "* A nontrivial cycle is something that closes up like the boundary of a simplex, in that its boundary sums to 0, but which isn't actually the boundary of a simplex or chain.\n",
        "\n",
        "* Because trivial 1-cycles are equivalent to 0 in H1, the 1-cycle at right-middle is homologous to its sum with the boundary of the 2-simplex at left.\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_04.png)\n",
        "\n",
        "\n",
        "[Vergleich mit paper von Krishna 2017]\n",
        "\n",
        "5 kongruent 11 und 17 etc. modulo 3, weil (3 * 1) + 2 = 5, und (3 * 3) + 2 = 11. Reste m√ºssen identisch sein.\n",
        "* **Zp (Kern)**: 5, 11, 17 etc. (alle Objekte aus einer Filtration zB in Cp, die kongruent zueinander sind, **weil deren Differenz ein ganzzahliges Vielfaches von Bp (modulo) ist)**.\n",
        "* **Hp (Hom)**: 2 (= das Loch)\n",
        "* **Bp (Img)**: 3 (modulo)\n",
        "- drei Basiselemente: 0, 1 und 2 (?)\n",
        "\n",
        "Before:\n",
        "\n",
        "5 kongruent 11 modulo 3, weil 3 * 1 + 2 = 5, und 3 * 3 + 2 + 11, Reste m√ºssen identisch sein, sowie 17 etc.\n",
        "\n",
        "- drei Basiselemente: 0, 1 und 2\n",
        "- 3 ist wie Bp, also Image (element des vektorraums, und ist linear, weil zB multiplizier mit Skala bleibt im vektorraum)\n",
        "- Rest ware Hp, also 2 (das Loch)\n",
        "- 5 und 11 und 17 sind Zp (Kern)\n",
        "- Berate alle Objekte aus der Filtration\n",
        "- Ein element aus Zp was plus ein Element aus Bp\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDP2ORM8EGQ"
      },
      "source": [
        "**Exact Sequences**\n",
        "\n",
        "An [exact sequence](https://en.m.wikipedia.org/wiki/Exact_sequence) is a sequence of morphisms between objects (for example, groups, rings, modules, and, more generally, objects of an abelian category) such that the image of one morphism equals the kernel of the next.\n",
        "\n",
        "In the context of group theory, a sequence\n",
        "\n",
        "> $G_{0} \\stackrel{f_{1}}{\\longrightarrow} G_{1} \\stackrel{f_{2}}{\\longrightarrow} G_{2} \\stackrel{f_{3}}{\\longrightarrow} \\cdots \\stackrel{f_{n}}{\\longrightarrow} G_{n}$\n",
        "\n",
        "of groups and group homomorphisms is called exact if the image of each homomorphism is equal to the kernel of the next:\n",
        "\n",
        "> $\\operatorname{im}\\left(f_{k}\\right)=\\operatorname{ker}\\left(f_{k+1}\\right)$\n",
        "\n",
        "The sequence of groups and homomorphisms may be either finite or infinite.\n",
        "\n",
        "> **Every exact sequence is a [chain complex](https://en.m.wikipedia.org/wiki/Chain_complex)**\n",
        "\n",
        "**Vergleich mit exact sequence: im‚Å°( f k ) = ker( f k + 1 ) bzw. Zp = Bp mit Hp = 0**\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Illustration_of_an_Exact_Sequence_of_Groups.svg/640px-Illustration_of_an_Exact_Sequence_of_Groups.svg.png)\n",
        "\n",
        "![ccc](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/640px-KerIm_2015Joz_L2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti Numbers (Betti Sequence)**\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Topologische_Invariante\n",
        "\n",
        "* The formal definition of homology uses the language of group theory. (The equivalence class of loops surrounding a hole have a group structure.) Persistent homology examines these homological features from a multiscale perspective.\n",
        "\n",
        "* Persistent homology is a powerful tool to compute, study and encode efficiently multiscale topological features of nested families of simplicial complexes and topological spaces.\n",
        "\n",
        "* It does not only provide efficient algorithms to compute the Betti numbers of each complex in the considered families, as required for homology inference in the previous section, but also encodes the evolution of the homology groups of the nested complexes across the scales.\n",
        "Im Bereich der algebraischen Topologie sind die Homologien beziehungsweise die **Homologiegruppen Invarianten eines topologischen Raums**.\n",
        "\n",
        "* **Simplicial homology groups and Betti numbers are topological invariants**: if K,K‚Ä≤ are two simplicial complexes whose geometric realizations are homotopy equivalent, then their homology groups are isomorphic and their Betti numbers are the same.\n",
        "\n",
        "* Persistent Homology, a recent breakthrough idea, extends Homology theory to work across a range of parameterized Simplicial Complexes, like the one arising from a point cloud, instead of just a single, isolated complex.\n",
        "It looks for topological invariants across various scales of a topological manifold.\n",
        "\n",
        "* ‚ÄûBut there is a problem. Betti numbers are computationally demanding to calculate, ‚Äúquickly overwhelming even the most powerful classical computers, even for not-so-large data sets,‚Äù\n",
        "\n",
        "* https://www.technologyreview.com/s/610138/a-small-scale-demonstration-shows-how-quantum-computing-could-revolutionize-data-analysis/\n",
        "\n",
        "* In algebraic topology, the Betti numbers are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* **The nth Betti number represents the rank of the nth homology group, denoted Hn, which tells us the maximum amount of cuts that must be made before separating a surface into two pieces or 0-cycles, 1-cycles, etc.[1] These numbers are used today in fields such as simplicial homology, computer science, digital images, etc**.\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Betti_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Homology_(mathematics)\n",
        "\n",
        "* Betti numbers is a compact method to present this Homology Groups by investigating the properties of the topological spaces.\n",
        "\n",
        "* It distinguishes topological spaces according to the connectivity of n-dimensional simplicial complexes. The nth Betti number represents the rank of the nth homology group, denoted as Hn.\n",
        "\n",
        "* Informally, the nth Betti number refers to the number of n-dimensional holes on a topological surface.\n",
        "\n",
        "* Figure 9 shows that the first three Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2- dimensional simplicial complexes: Œ≤0 is the number of connected components Œ≤1 is the number of holes(one-dimensional) and Œ≤2 is the number of two-dimensional \"voids\".\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_437.png)\n",
        "\n",
        "Die Bettizahlen geben an, wie viele k-dimensionale nicht zusammenh√§ngende Fl√§chen der entsprechende topologische Raum hat. Die ersten drei Bettizahlen besagen anschaulich also:\n",
        "\n",
        "* b0 ist die Anzahl der Wegzusammenhangskomponenten* (connected components)\n",
        "\n",
        "* b1 ist die Anzahl der ‚Äûzweidimensionalen L√∂cher‚Äú.\n",
        "\n",
        "* b2 ist die Anzahl der dreidimensionalen Hohlr√§ume.\n",
        "\n",
        "Der unten abgebildete Torus (gemeint ist Oberfl√§che) besteht aus einer Zusammenhangskomponente, hat zwei ‚Äûzweidimensionale L√∂cher‚Äú, zum einen das in der Mitte, zum andern das im Inneren des Torus, und hat einen dreidimensionalen Hohlraum. Die Bettizahlen des Torus sind daher 1, 2, 1, die weiteren Bettizahlen sind 0.\n",
        "\n",
        "Ist der zu betrachtende topologische Raum jedoch keine orientierbare kompakte Mannigfaltigkeit, so versagt diese Anschauung allerdings schon.\n"
      ],
      "metadata": {
        "id": "8v0mv3YqBQGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti number & Rank of a group**\n",
        "\n",
        "* In algebraic topology, the [Betti numbers](https://en.m.wikipedia.org/wiki/Betti_number) are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* The nth Betti number represents the [rank (of a group)](https://en.m.wikipedia.org/wiki/Rank_of_a_group) of the nth [homology group](https://en.m.wikipedia.org/wiki/Homology_(mathematics)), denoted Hn, **which tells us the maximum number of cuts that can be made before separating a surface into two pieces or 0-cycles, 1-cycles**, etc.\n",
        "\n",
        "* The first few Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2-dimensional simplicial complexes:\n",
        "\n",
        "  * b0 is the number of (connected) components;\n",
        "  * b1 is the number of one-dimensional or \"circular\" holes;\n",
        "  * b2 is the number of two-dimensional \"voids\" or \"cavities\".\n",
        "\n",
        "* for example, a torus has one connected surface component so b0 = 1, two \"circular\" holes (one equatorial and one meridional) so b1 = 2, and a single cavity enclosed within the surface so b2 = 1.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/5/54/Torus_cycles.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvR1-z919rUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oP2QkFIRXLX"
      },
      "source": [
        "**Homology Group & Betti Number**\n",
        "\n",
        "Let $\\sigma=\\left(v_{0}, \\ldots, v_{k}\\right)$ be an oriented $k$ -simplex, viewed as a basis element of $C_{k}$. The boundary operator\n",
        "\n",
        "$\n",
        "\\partial_{k}: C_{k} \\rightarrow C_{k-1}\n",
        "$\n",
        "\n",
        "is the homomorphism defined by:\n",
        "\n",
        "$\n",
        "\\partial_{k}(\\sigma)=\\sum_{i=0}^{k}(-1)^{i}\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "where the oriented simplex\n",
        "\n",
        "$\n",
        "\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "is the $I^{\\text {th }}$ face of $\\sigma,$ obtained by deleting its $i^{\\text {th }}$ vertex.\n",
        "In $C_{k},$ elements of the subgroup\n",
        "\n",
        "$\n",
        "Z_{k}:=\\operatorname{ker} \\partial_{k}\n",
        "$\n",
        "\n",
        "are referred to as cycles, and the subgroup\n",
        "\n",
        "$\n",
        "B_{k}:=\\operatorname{im} \\partial_{k+1}\n",
        "$\n",
        "\n",
        "is said to consist of boundaries.\n",
        "\n",
        "The $k^{\\text {th }}$ homology group $H_{k}$ of $S$ is defined to be the [quotient abelian group](https://en.m.wikipedia.org/wiki/Quotient_group)\n",
        "\n",
        "$\n",
        "H_{k}(S)=Z_{k} / B_{k}\n",
        "$\n",
        "\n",
        "* It follows that the **homology group $H_{k}(S)$ is nonzero exactly when there are $k-$\n",
        "cycles on $S$ which are not boundaries**. In a sense, this means that there are $k$ -\n",
        "dimensional holes in the complex.\n",
        "\n",
        "* For example, consider the complex $S$ obtained by gluing two triangles (with no interior) along one edge, shown in the image. The edges of each triangle can be oriented so as to form a cycle. These two cycles are by construction not boundaries (since every 2 -chain is zero).\n",
        "\n",
        "* One can compute that the homology group $\\mathrm{H}_{1}(\\mathrm{S})$ is isomorphic to $\\mathrm{Z}^{2}$, with a basis given by the two cycles mentioned. This makes precise the informal idea that $S$ has two \"1-\n",
        "dimensional holes\".\n",
        "\n",
        "* Holes can be of different dimensions. The rank of the $k$ th homology group, the\n",
        "number\n",
        "\n",
        "$\n",
        "\\beta_{k}=\\operatorname{rank}\\left(H_{k}(S)\\right)\n",
        "$\n",
        "\n",
        "**is called the $k$ th Betti number of $S$. It gives a measure of the number of $k$ - dimensional holes in $S$.**\n",
        "\n",
        "**A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is \"not there\".**\n",
        "\n",
        "* An homology class Hn (which represents a hole) is an [equivalence class](https://en.m.wikipedia.org/wiki/Equivalence_class) of\n",
        "    * [cycles](https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles) Ker(Œ¥) modulo boundaries Im(Œ¥) bzw.\n",
        "    * h(e) $\\equiv$ t(e) mod (im d)\n",
        "\n",
        "* Ker(Œ¥) kann beschrieben werden: h(e) $\\equiv$ t(e) bzw. e $\\mapsto$ h(e) - t(e) or: ‚àÇn‚àí1 ‚ó¶ ‚àÇn = 0\n",
        "\n",
        "* This is the linear combination of two vertices, and hence an element of e zero\n",
        "\n",
        "* Get the image of d bzw. Im(Œ¥) when you look at the subspace generated by e $\\mapsto$ h(e) - t(e)\n",
        "\n",
        "*  Im(Œ¥): boundaries?\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles\n",
        "\n",
        "https://ncatlab.org/nlab/show/boundary+of+a+simplex\n",
        "\n",
        "https://ncatlab.org/nlab/show/chain+map\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental group**\n",
        "\n",
        "In the mathematical field of algebraic topology, the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) of a topological space is the group of the equivalence classes under homotopy of the loops contained in the space. It records information about the basic shape, or holes, of the topological space.\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))\n",
        "\n",
        "If $X$ is a graph and ${\\displaystyle T\\subseteq X}$ a maximal tree, then the fundamental group $\\pi _{1}(X)$ equals the free group generated by elements ${\\displaystyle (f_{\\alpha })_{\\alpha \\in A}}$, where the ${\\displaystyle \\{f_{\\alpha }\\}}$ correspond bijectively to the edges of ${\\displaystyle X\\setminus T}$; in fact, $X$ is homotopy equivalent to a wedge sum of circles."
      ],
      "metadata": {
        "id": "7WtsJdDE9oVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *TDA*"
      ],
      "metadata": {
        "id": "Nror5UBfDJ3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAh4XJMHLDp9"
      },
      "source": [
        "**Persistent Barcode, Diagram & Landscape**\n",
        "\n",
        "* What is the ideal size of d? - Consider all distances d between d1 min (to connect two balls) and d2 max size (all are connected). Each hole appears at a particular value of d and disappears at anotjer value of d. We can represent the persistence of this hole as a pair (d1, d2).\n",
        "* Out of this distance we get a bar. Several holes result in a barcode. Short bars represent noise. Long bars are features.\n",
        "* Persistent barcodes are stable with respect to pertubations if data (Edelsbrunner 2007).\n",
        "* Barcode is computable via linear algebra. Runtime is O (n3), it‚Äòs cubic, where n is the number of simplices (Carlsson 2005).\n",
        "* A barcode is a visualization of an algebraic structure.\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_06.jpg)\n",
        "\n",
        "> **Homology of a simplicial complex is computable via linear algebra**\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_07.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pjdD_O2saB"
      },
      "source": [
        "**Topological Data Analysis + Machine Learning**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/tda_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Geometry**"
      ],
      "metadata": {
        "id": "emoYR7MX-Q2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Spaces*"
      ],
      "metadata": {
        "id": "gmN2IqZ4FDsw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkLBUH1hb36"
      },
      "source": [
        "###### *Spaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqI_Wvvlu5y7"
      },
      "source": [
        "**What constitutes a 'space'?**\n",
        "\n",
        "* **A space is a [set](https://en.m.wikipedia.org/wiki/Set_(mathematics)) - (sometimes called a [universe - Grundmenge](https://en.m.wikipedia.org/wiki/Universe_(mathematics))) with some added structure.**\n",
        "\n",
        "* A space consists of selected **mathematical objects that are treated as points**, and selected **relationships between these points** (nature of the points can vary widely: for example, the **points can be elements of a set, functions on another space, or subspaces of another space.**)\n",
        "\n",
        "\n",
        "* **[Taxonomy of Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces)**: While each type of space has its own definition, the general idea of \"space\" evades formalization (modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, but it does not define the notion of \"space\" itself)\n",
        "\n",
        "Quelle: [Einordnung in die Hierarchie mathematischer Strukturen](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Einordnung_in_die_Hierarchie_mathematischer_Strukturen) sowie [Topologische R√§ume](https://de.m.wikipedia.org/wiki/Topologischer_Raum#Beispiele)\n",
        "\n",
        "Topologischer Raum |  | dazugeh√∂rige Struktur\n",
        "--- | --- | ---\n",
        "[Euklidischer Raum](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) | hat | Skalarprodukt\n",
        "[Normierter Raum](https://de.m.wikipedia.org/wiki/Normierter_Raum) | hat | Norm\n",
        "[Metrischer Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) | hat | Metrik\n",
        "[Uniformer Raum](https://de.m.wikipedia.org/wiki/Uniformer_Raum) | hat | Uniforme Struktur\n",
        "[Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum) | hat | Topologie\n",
        "\n",
        "![xxx](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Beziehungen_zwischen_mathematischen_R√§umen.svg/220px-Beziehungen_zwischen_mathematischen_R√§umen.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ostHcxEvNZ5r"
      },
      "source": [
        "![Normed Vector Space](https://upload.wikimedia.org/wikipedia/commons/7/74/Mathematical_Spaces.png)\n",
        "\n",
        "Quelle: [Mathematical Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYW5u_0Oi7l5"
      },
      "source": [
        "###### *Inner Product Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3TsP_aDsFR"
      },
      "source": [
        "**Inner Product**\n",
        "\n",
        "> * **Inner product $\\rightarrow$ measures distances, lengths, angles**\n",
        "\n",
        "* Das innere Produkt (Skalarprodukt / Produktpunkt called when applied to functions - the alternate name of inner product in linear algebra is 'dot product') stellt quasi eine Geometrie im Vektorraum her, wir k√∂nnen dadurch definieren, welche Vektoren orthogonal, und welche parallel zueinander sind.\n",
        "\n",
        "* **Das Skalarprodukt ben√∂tigt man**,\n",
        "  * um die Lange von Vektoren zu berechnen,\n",
        "  * den Winkel zwischen Vektoren zu berechnen (uber Cosinus von Alpha) und\n",
        "  * ob zwei Vektoren senkrecht zueinander stehen.\n",
        "\n",
        "* Das **Dot Product / Scalar Product / [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt)** (auch inneres Produkt oder Punktprodukt) ist eine [mathematische Verkn√ºpfung](https://de.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik)), die zwei Vektoren eine Zahl (Skalar) zuordnet.\n",
        "\n",
        "* **Scalar vs Scalar Product**: A [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics)) is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector. The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically, the determinant can be viewed as the volume scaling factor of the linear transformation described by the matrix.\n",
        "\n",
        "* Geometrisch berechnet man das Skalarprodukt zweier Vektoren $\\vec{a}$ und $\\vec{b}$ nach der Formel:\n",
        "\n",
        "> $\n",
        "\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})\n",
        "$\n",
        "\n",
        "* Null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben.\n",
        "\n",
        "* **A common special case of the inner product is the scalar product or dot product, is written with a centered dot a ‚ãÖ b.**\n",
        "\n",
        "* In [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) the inner product is the dot product, also known as the scalar product. (They generalize Euclidean spaces to vector spaces of any (possibly infinite) dimension.)\n",
        "\n",
        "* Ist das Skalarprodukt von zwei Vektoren $\n",
        "\\vec{a} \\cdot \\vec{b}= 0$, dann folgt daraus, dass diese orthogonal zueinander stehen.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * A simple example is the real numbers $\\mathbb{R}$ with the standard multiplication as the inner product $\\langle x, y\\rangle:=x y$\n",
        "\n",
        "  * **Inner Product of Functions** says how similar two functions are (how much they align with each other). **Inner product of function are used a lot in Fourier Transform**\n",
        "\n",
        "    * i.e. if they are orthogonal, then zero. if they are very similar, then they have a large inner product\n",
        "\n",
        "    > $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "    * You can also take samples from both functions and calculate the inner product between both. Up to infinity, you get at the integral like written above (Riemann approximation of the continuuos integral above):\n",
        "\n",
        "    > $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcOpkYQjFKx"
      },
      "source": [
        "**Inner Product Space**\n",
        "\n",
        "* Der **[Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space)** (Pr√§hilbertraum bzw. Skalarprodukt) ist ein **Vektorraum**, auf dem ein **inneres Produkt definiert ist**.\n",
        "\n",
        "* An [inner product space](https://en.m.wikipedia.org/wiki/Inner_product_space) is a normed space, **where the norm of a vector is the square root of the inner product of the vector by itself**: $\\sqrt{\\vec{x} \\cdot \\vec{x}} = \\sqrt{{x}^{2}}$\n",
        "\n",
        "* [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) generalize **Euclidean spaces (in which the inner product is the dot product, also known as the scalar product**) to vector spaces of any (possibly infinite) dimension.\n",
        "\n",
        "An **inner product space** is a vector space $V$ over the field $\\mathbb{F}$ together with a map\n",
        "\n",
        "$\n",
        "\\langle\\cdot, \\cdot\\rangle: V \\times V \\rightarrow \\mathbb{F}\n",
        "$\n",
        "\n",
        "called an inner product that satisfies the following conditions $(1),(2),$ and $(3)$ for all vectors $x, y, z \\in V$ and all scalars $a \\in \\mathbb{F}:$ [see here](https://en.m.wikipedia.org/wiki/Inner_product_space)\n",
        "\n",
        "* In linear algebra, **an inner product space is a vector space with an additional structure called an inner product**. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Geometric interpretation of the angle between two vectors defined using an inner product.\n",
        "\n",
        "> **Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors**. They also provide the means of defining orthogonality between vectors (**zero inner product**).\n",
        "\n",
        "> An inner product **naturally induces an associated norm**, (|x| and |y| are the norms of x and y, in the picture) thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Normed Vector Space*"
      ],
      "metadata": {
        "id": "0JfDpLK4KBqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "\n",
        "classical probability theory: 1-Norm: ‚àë pi = ‚àë | p | = || p || 1 = 1\n",
        "\n",
        "quantum probabiliyt theory: 2-Norm: || | œà > || 2 = 1\n",
        "\n",
        "**Reeller Vektor**\n",
        "\n",
        "Die 1-, 2-, 3- und $\\infty$-Normen des reellen Vektors $x=(3,-2,6)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3|^3+|-2|^3+|6|^3}=\\sqrt[3]{251} \\approx 6,308 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Komplexer Vektor**\n",
        "\n",
        "Die 1-, 2-, 3-und $\\infty$-Normen des komplexen Vektors $x=(3-4 i,-2 i)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3-4 i|^3+|-2 i|^3}=\\sqrt[3]{5^3+2^3}=\\sqrt[3]{133} \\approx 5,104 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "1Ui-wrEg177C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhAD7kvd3uuL"
      },
      "source": [
        "> **Eine Norm gibt die <u>Gr√∂√üe / L√§nge (Betrag) eines Elements</u> in einem Vektorraum an**\n",
        "\n",
        "* Eine Metrik gibt hingegen die Distanz zwischen <u>zwei Vektoren (Punkten)</u> an. A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ (but Instead of distance between points, a norm gives us the length of a vector, as measured from the origin)\n",
        "\n",
        "* Eine **[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) ist eine Abbildung (Funktion)** $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$, welche einem Element von einem reellen oder komplexen Vektorraum eine **nicht-negative reelle Zahl** $\\mathbb{R}^{\\geq \\ 0 }$ zuordnet und folgende Eigenschaften besitzt (f√ºr alle $x, y$ aus dem $\\mathbb{K}$ Vektorraum und alle $\\lambda$ aus $\\mathbb{K}$):\n",
        "\n",
        "1. **[Definitheit](https://de.m.wikipedia.org/wiki/Definitheit)**:\n",
        "  * It is **nonnegative**, that is for every vector x, one has ‚Äñx‚Äñ ‚â• 0.\n",
        "  * It is **positive on nonzero vectors**, that is, ‚Äñx‚Äñ = 0 ‚ü∫ x = 0.\n",
        "\n",
        "2. **[Absolute Homogenit√§t](https://de.m.wikipedia.org/wiki/Homogene_Funktion)**: For every vector x, and every **scalar Œ±**, one has ‚Äñ Œ± x ‚Äñ = | Œ± | ‚Äñ x ‚Äñ.\n",
        "\n",
        "3. **[Subadditivit√§t, Dreiecksungleichung](https://de.m.wikipedia.org/wiki/Additive_Funktion#Sub-_und_Superadditivit√§t)**: for every vectors x and y, one has ‚Äñ x+y ‚Äñ ‚â§ ‚Äñ x ‚Äñ + ‚Äñ y ‚Äñ.\n",
        "\n",
        "* From Inner Products to Norms: Eine Norm kann (muss aber nicht) von einem [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (eine sogenannte ['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall is **the norm of a vector the square root of the inner product of the vector by itself**. A complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "\n",
        "* **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. If such a space is complete, we call it a [Banach space](https://de.m.wikipedia.org/wiki/Banachraum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygj95-xb7Eu"
      },
      "source": [
        "[**1. Normen auf endlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "\n",
        "* [**Zahlnorm**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): ein Beispiel ist die [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion).\n",
        "\n",
        "  * ist **induziert vom Standardskalarprodukt** (erf√ºllt die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t) zweier reeller bzw. komplexen Zahlen. Die Betragsnorm ist. der Betrag einer reellen Zahl $z \\in \\mathbb{R}$:\n",
        "  > $\n",
        "\\|z\\|=|z|=\\sqrt{z^{2}}=\\left\\{\\begin{array}{cl}\n",
        "z & \\text { f√ºr } z \\geq 0 \\\\\n",
        "-z & \\text { f√ºr } z < 0\n",
        "\\end {array}\\right.$\n",
        "\n",
        "* [**Matrixnorm**](https://de.m.wikipedia.org/wiki/Matrixnorm): Siehe [Norm -> Matrixnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Matrixnormen) sowie [https://de.m.wikipedia.org/wiki/Matrixnorm](Matrixnorm)\n",
        "  * [Nat√ºrliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): Eine nat√ºrliche Matrixnorm entspricht anschaulich dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor entsteht\n",
        "  * [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm):  Anschaulich entspricht die Spektralnorm damit dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor der L√§nge Eins entsteht = Die Spektralnorm einer Matrix entspricht ihrem maximalen Singul√§rwert, also der Wurzel des gr√∂√üten Eigenwerts des Produkts der adjungierten (transponierten) Matrix mit dieser Matrix. Spektralnorm die von der euklidischen Norm abgeleitete nat√ºrliche Matrixnorm. Von: [Singul√§rwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "\n",
        "* [**Vektornormen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen): ein Beispiel sind die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm)\n",
        "\n",
        "  * Die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) sind eine Klasse von [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen), die f√ºr reelle Zahlen $p \\geq 1$ definiert sind.\n",
        "\n",
        "  * Die $p$ -Norm eines Vektors $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{K}^{n}$ mit $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{C}$ ist f√ºr reelles\n",
        "$1 \\leq p<\\infty$ definiert durch:\n",
        "\n",
        "  > **$\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$**\n",
        "\n",
        "  * p-Normen **erf√ºllen die Minkowski-Ungleichung sowie die H√∂lder-Ungleichung**. For all p ‚â• 1, the p-norms erf√ºllen die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t.\n",
        "\n",
        "  * Die wichtigsten Vektornormen sind die [**Summennorm**](https://de.m.wikipedia.org/wiki/Summennorm) , Euklidische Norm und [**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**2. Normen auf unendlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorr√§umen)\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**[Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum)**\n",
        "\n",
        "* **The sequence space (Folgenraum) is a special case of the function space (Funktionenraum): $\\ell_{\\infty}=L_{\\infty}(\\mathbb{N})$ where the natural numbers are equipped with the counting measure.**\n",
        "\n",
        "* Die $\\ell^{p}$ -Normen sind die Verallgemeinerung der $p$ -Normen auf Folgenr√§ume, wobei lediglich die endliche Summe durch eine unendliche ersetzt wird. Die $\\ell^{p}$ -Norm einer in $p$ -ter Potenz betragsweise summierbaren Folge ist f√ºr reelles $1 \\leq p<\\infty$ dann definiert als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Versehen mit diesen Normen werden die $\\ell$ - R√§ume jeweils zu vollst√§ndigen normierten R√§umen. ${ }^{[6]}$ F√ºr den Grenzwert $p \\rightarrow \\infty$ ergibt sich der Raum der beschr√§nkten Folgen $\\ell^{\\infty}$ mit der Supremumsnorm.\n",
        "\n",
        "**[Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum**\n",
        "\n",
        "* Siehe auch L-p-Raum: https://de.m.wikipedia.org/wiki/Lp-Raum\n",
        "\n",
        "* Die $\\mathcal{L}^{p}$ -Normen einer in $p$ -ter Potenz **Lebesgue-integrierbaren Funktion** mit $1<p<\\infty$ sind in Analogie zu den $\\ell^{p}$ -Normen definiert als\n",
        "\n",
        "> $\n",
        "\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}\n",
        "$\n",
        "\n",
        "* **wobei die Summe durch ein Integral ersetzt wurde**. Ebenso wie bei der wesentlichen Supremumsnorm sind diese Narmen zun√§chst nur Halbnormen, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Ma√ü Null II von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man wieder die Menge der √Ñauivalenzklassen unnn Funktionen $[f] \\in L^{p}(\\Omega)$, die fast √ºberall gleich sind, und definiert auf diesen $L^{p}$ -R√§umen die $L^{p}$ -Normen durch\n",
        "$\\|[f]\\|_{L P(\\Omega)}=\\|f\\|_{\\mathcal{L}^{p}(\\Omega)}$\n"
      ],
      "metadata": {
        "id": "EC3JVQRj2H01"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27wE3ZTM_154"
      },
      "source": [
        "**Beispiele fur Normed Vector Spaces**\n",
        "\n",
        "* **A [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) or normed space <u>is a vector space</u> over the real or complex numbers, on which a norm is defined**.\n",
        "\n",
        "  * Ist $V$ ein [Vektorraum](https://de.m.wikipedia.org/wiki/Vektorraum) √ºber dem K√∂rper $\\mathbb{K}$ der reellen oder der komplexen Zahlen und $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$ eine Norm auf $V,$ dann nennt man das Paar $(V,\\|\\cdot\\|)$ einen **normierten Vektorraum**.\n",
        "\n",
        "  * **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. (If such a space is complete, we call it a Banach space.)\n",
        "\n",
        "  * **Be careful**: A [vector space](https://en.m.wikipedia.org/wiki/Vector_space) is an [algebraic structure](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures), meanwhile a [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) is a type of [abstract (topological) space](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces) (a normed vector space is a vector space over the real or complex numbers, on which a norm is defined). (See also [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space)\n",
        "\n",
        "* **Banach Space**: **$\\mathbb{R}$<sup>n</sup> together with the p-norm is a [Banach space](https://de.wikipedia.org/wiki/Banachraum) = ein vollst√§ndiger normierter Vektorraum**. This Banach space is the Lp-space over Rn. Viele **Folgenr√§ume $\\ell$** oder **Funktionenr√§ume $L$** sind unendlichdimensionale Banachr√§ume. Function Spaces $L$ are a type of infinite vector space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces**, whose vectors are functions.\n",
        "\n",
        "* **Hilbert Space**: Ein Banachraum, dessen Norm durch ein Skalarprodukt induziert ist, hei√üt **[Hilbertraum](https://de.wikipedia.org/wiki/Hilbertraum)**. (z.B. p2-Norm Euklidische Norm, aber nicht p1-Summennorm). L√§sst man die Bedingung der Vollst√§ndigkeit fallen, spricht man von einem Pr√§hilbertraum).\n",
        "  * Die Struktur eines Hilbertraums ist eindeutig festgelegt durch seine Hilbertraumdimension. Diese kann eine beliebige Kardinalzahl sein. Ist die Dimension endlich und betrachtet man als K√∂rper die reellen Zahlen, so handelt es sich um einen euklidischen Raum.\n",
        "  * Hilbertr√§ume tragen durch ihr Skalarprodukt eine topologische Struktur. Dadurch sind hier im Gegensatz zu allgemeinen Vektorr√§umen Grenzwertprozesse m√∂glich.\n",
        "\n",
        "* **Hardy-Space**: Untersucht man statt der messbaren Funktionen nur die holomorphen beziehungsweise die harmonischen Funktionen auf Integrierbarkeit, so werden die entsprechenden $L^{p}$-R√§ume [Hardy-R√§ume](https://de.m.wikipedia.org/wiki/Hardy-Raum) genannt, [L_p Space-Hardy](https://de.m.wikipedia.org/wiki/Lp-Raum#Hardy-R%C3%A4ume)\n",
        "\n",
        "* **F-Space**: The **space Lp for 0 < p < 1 is an [F-space](https://en.m.wikipedia.org/wiki/F-space)**: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ‚â• 1. Some authors use the term [Fr√©chet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space) rather than F-space, but usually the term \"Fr√©chet space\" is reserved for locally convex F-spaces.\n",
        "\n",
        "* **Sobolev-Raum**: Ein [Sobolev-Raum](https://de.wikipedia.org/wiki/Sobolev-Raum), ist ein Funktionenraum von schwach differenzierbaren Funktionen, der zugleich ein Banachraum ist. Der Sobolev-Raum ist der Raum derjenigen reellwertigen Funktionen $u \\in L^{p}(\\Omega),$ deren gemischte partielle schwache Ableitungen bis zur Ordnung $k$ im Lebesgue-Raum $L^{p}(\\Omega)$ liegen.\n",
        "\n",
        "  * Aus der Variationsrechnung: Diese minimiert Funktionale √ºber Funktionen - Sobolev-R√§ume bilden dabei die Grundlage der L√∂sungstheorie partieller Differentialgleichungen.\n",
        "\n",
        "  * A Sobolev space is a vector space of functions equipped with a norm that is a **combination of Lp-norms of the function together with its derivatives up to a given order**.\n",
        "\n",
        "  * The derivatives are understood in a suitable weak sense to make the space complete, i.e. a Banach space.\n",
        "\n",
        "  * **A Sobolev space is a space of functions**\n",
        "\n",
        "    * **possessing sufficiently many derivatives** for some application domain, such as partial differential equations,\n",
        "\n",
        "    * and **equipped with a norm** that measures both the size and regularity of a function.\n",
        "\n",
        "    * Their importance comes from the fact that **weak solutions of some important partial differential equations exist in appropriate Sobolev spaces**, even when there are no strong solutions in spaces of continuous functions with the derivatives understood in the classical sense.\n",
        "\n",
        "  * Partielle Differentialgleichungen betrachtet man meistens auf Sobolew-R√§umen. In diesen R√§umen werden Funktionen, die bis auf Nullmengen √ºbereinstimmen, als gleich angesehen. Da der Rand eines Gebietes √ºblicherweise eine Nullmenge ist, ist der Begriff der Randbedingung problematisch. L√∂sungen f√ºr dieses Problem sind sobolewsche Einbettungss√§tze oder ‚Äì allgemeiner ‚Äì [**Spuroperatoren**](https://de.wikipedia.org/wiki/Sobolev-Raum#Spuroperator).\n",
        "\n",
        "* **$L^p$ (Lebesgue) Space** are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. **They are sometimes called Lebesgue spaces**. Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * [L<sup>p</sup>-Raum](https://de.m.wikipedia.org/wiki/Lp-Raum) sind spezielle R√§ume, die aus allen p-fach integrierbaren Funktionen bestehen. Das $p$ in der Bezeichnung ist ein reeller Parameter: F√ºr jede Zahl $0 < p \\leq \\infty$ ist ein $L^{p}$ -Raum definiert. Die Konvergenz in diesen R√§umen wird als Konvergenz im $p$ -ten Mittel bezeichnet.\n",
        "\n",
        "  * diese R√§ume werden √ºber das Lebesgue-Integral definiert\n",
        "\n",
        "  * Im Fall Banachraum-wertiger Funktionen bezeichnet man sie auch als Bochner-Lebesgue-R√§ume.\n",
        "\n",
        "  * [**Konvergenz_im_p-ten_Mittel**](https://de.m.wikipedia.org/wiki/Konvergenz_im_p-ten_Mittel)\n",
        "\n",
        "  * ${\\mathcal {L}}^{p}$ mit Halbnorm und ${\\mathcal {L}}^{p}$ mit Norm\n",
        "\n",
        "  * Hilbertraum ${\\mathcal {L}}^{2}$\n",
        "\n",
        "  * Der **normierte Vektorraum** $L^{p}$ ist [vollst√§ndig](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum) und damit ein [Banachraum](https://de.m.wikipedia.org/wiki/Banachraum), die Norm $\\|\\cdot\\|_{L} p$ wird **$L^{p}$ Norm** genannt.\n",
        "\n",
        "  * Auch wenn man von sogenannten $L^{p}$ -Funktionen spricht, handelt es sich dabei um die gesamte √Ñquivalenzklasse einer klassischen Funktion. Allerdings liegen im Falle des Lebesgue-Ma√ües auf dem $\\mathbb{R}^{n}$ zwei verschiedene stetige Funktionen nie in der gleichen √Ñquivalenzklasse, so dass der $L^{p}$-Begriff eine nat√ºrliche Erweiterung des Begriffs stetiger Funktionen darstellt.\n",
        "\n",
        "  * The [**Lp spaces**](https://de.m.wikipedia.org/wiki/Lp-Raum) are [function spaces](https://en.m.wikipedia.org/wiki/Function_space) defined using a natural **generalization of the p-norm for finite-dimensional vector spaces**. They are sometimes called **Lebesgue spaces**.\n",
        "\n",
        "  * A normed vector space is automatically a metric space, by defining the metric in terms of the norm in the natural way. But a metric space may have no algebraic (vector) structure ‚Äî i.e., it may not be a vector space ‚Äî so the concept of a **metric space is a generalization of the concept of a normed vector space**.\n",
        "\n",
        "  * Lp spaces form an important class of [Banach spaces](https://en.m.wikipedia.org/wiki/Banach_space) in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as [solutions to variational problems](https://en.m.wikipedia.org/wiki/Central_tendency#Solutions_to_variational_problems)\n",
        "\n",
        "  * An Lp space may be defined as a space of measurable functions for which the p-th power of the absolute value is Lebesgue integrable, where functions which agree almost everywhere are identified.\n",
        "\n",
        "  * More generally, let 1 ‚â§ p < ‚àû and (S, Œ£, Œº) be a [measure space](https://en.m.wikipedia.org/wiki/Measure_space). Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, that\n",
        "\n",
        "  * $\\|f\\|_{p} \\equiv\\left(\\int_{S}|f|^{p} \\mathrm{d} \\mu\\right)^{1 / p}<\\infty$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SpYIJ602L38"
      },
      "source": [
        "###### *Metric Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gcLfqnPX33L"
      },
      "source": [
        "> **Metric $\\rightarrow$ measures distances**\n",
        "\n",
        "* *Difference Metric to Norm: Instead of distance between points, a norm gives us the length of a vector, as measured from the origin*\n",
        "\n",
        "* **Eine Metrik definiert Abst√§nde zwischen Elementen des Vektorraumes.**\n",
        "\n",
        "* Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n",
        "\n",
        "* Eine Metrik (auch Abstandsfunktion) ist eine Funktion, die je zwei Elementen des Raums einen nicht negativen reellen Wert zuordnet, der als Abstand der beiden Elemente voneinander aufgefasst werden kann.\n",
        "\n",
        "* Sei M eine Menge. **<u>Eine Metrik ist eine Abbildung</u>** d: $M \\times M \\rightarrow \\mathbb{R}$ auf $M \\times M$ wenn folgende drei Axiome erf√ºllt sind:\n",
        "\n",
        "1. Beide zusammen bilde Positive Definitheit (**positive definiteness**):\n",
        "  * $d(x, y) \\geq 0$ (**non-negativity**) sowie\n",
        "  * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) f√ºr alle $x, y \\in M$.\n",
        "\n",
        "2. $d(x, y)=d(y, x)$ (**symmetry**) Symmetrie\n",
        "$d(x, y)=d(y, x) \\forall x, y \\in M$\n",
        "\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "\n",
        "**Metriken geben einem Raum eine globale und eine lokale mathematische Struktur**:\n",
        "  * Die globale Struktur kommt in **geometrischen Eigenschaften wie der Kongruenz** von Figuren zum Ausdruck.\n",
        "  * Die lokale metrische Struktur, also die Definition kleiner Abst√§nde, erm√∂glicht unter bestimmten zus√§tzlichen Voraussetzungen die Einf√ºhrung von **Differentialoperationen**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdrlGVRmlmx"
      },
      "source": [
        "**Aus Normen erzeugte Metriken**\n",
        "\n",
        "* **A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ.**\n",
        "\n",
        "* Jede Norm auf einem Vektorraum induziert durch die Festlegung $d(x, y) \\equiv\\|x-y\\|$ eine Metrik. Somit ist jeder normierte Vektorraum (und erst recht jeder Innenproduktraum, Banachraum oder Hilbertraum) ein metrischer Raum.\n",
        "\n",
        "* **Aber Achtung**: nicht jede Metrik ist durch eine Norm induziert! Jede Norm induziert eine Metrik, aber nicht umgekehrt.\n",
        "\n",
        "* **Eine Metrik, die aus einer $p$ -Norm abgeleitet ist, hei√üt auch [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances)**. It is a metric in a normed vector space. p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold (which is possible, but then it's not a metric anymore). Wichtige Spezialf√§lle sind:\n",
        "  * [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "  * [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$\n",
        "  * [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "\n",
        "* Als eine [**Fr√©chet-Metrik**](https://de.m.wikipedia.org/wiki/Fr√©chet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. **Sie stellt eine Verbindung zwischen Metrik und Norm her.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUnISc3anLSR"
      },
      "source": [
        "**Nicht aus Normen erzeugte Metriken**\n",
        "\n",
        "* Auf jeder Menge l√§sst sich eine triviale Metrik, die sogenannte gleichm√§√üig diskrete Metrik (die sogar eine Ultrametrik ist) definieren: $d(x, y)=\\left\\{\\begin{array}{ll}0 & \\text { f√ºr } x=y \\\\ 1 & \\text { f√ºr } x \\neq y\\end{array}\\right.$\n",
        "\n",
        "* Im Allgemeinen **nicht durch eine Norm induziert ist die riemannsche Metrik**, die aus einer differenzierbaren Mannigfaltigkeit eine [riemannsche Mannigfaltigkeit](https://en.m.wikipedia.org/wiki/Riemannian_manifold) macht. (zB  Die k√ºrzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geod√§ten) sind nicht zwingend Geradenst√ºcke, sondern k√∂nnen gekr√ºmmte Kurven sein. **Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch gr√∂√üer (z. B. Kugel) oder kleiner (hyperbolische R√§ume) als 180¬∞ sein**.\n",
        "\n",
        "* Die [franz√∂sische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Franz√∂sische_Eisenbahnmetrik).\n",
        "\n",
        "* Die [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man k√∂nnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zur√ºck.\n",
        "\n",
        "* Der [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) ist eine Metrik auf dem Coderaum, die die Unterschiedlichkeit von (gleich langen) Zeichenketten angibt. Die [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) kann als Erweiterung des Hamming-Abstands angesehen werden. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro‚ÄìWinkler distance](https://en.m.wikipedia.org/wiki/Jaro‚ÄìWinkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "\n",
        "* Mehr Beispiele von nicht aus Normen erzeugten Metriken [hier](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-8s7XOx_Mxn"
      },
      "source": [
        "**Metric Space**\n",
        "\n",
        "* Unter einem [metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space) versteht man in der Mathematik eine Menge, auf der eine Metrik definiert ist.\n",
        "\n",
        "  * **Das Paar $(M, d)$ nennt man einen <u>[metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space)</u>**.\n",
        "\n",
        "  * Beispiel fur einen metrischen Raum: Die Menge der reellen Zahlen $\\mathbb{R}$ mit der Abstandsmetrik $d(x, y):=|x-y|$ bilden einen metrischen Raum.\n",
        "\n",
        "* Jeder metrische Raum ist ein [Hausdorff-Raum](https://de.m.wikipedia.org/wiki/Hausdorff-Raum).\n",
        "\n",
        "\n",
        "* **[Isometrie](https://de.m.wikipedia.org/wiki/Isometrie)**\n",
        "\n",
        "  * Isomorphismen zwischen metrischen R√§umen hei√üen Isometrien. Ein metrischer Raum hei√üt vollst√§ndig, falls alle Cauchy-Folgen konvergieren. Eine Isometrie ist eine Abbildung, die zwei metrische R√§ume aufeinander abbildet und dabei die Metrik ‚Äì also die Abst√§nde zwischen je zwei Punkten ‚Äì erh√§lt.\n",
        "\n",
        "  * Sind zwei metrische R√§ume $\\left(M_{1}, d_{1}\\right),\\left(M_{2}, d_{2}\\right)$ gegeben, und ist $f: M_{1} \\rightarrow M_{2}$ eine Abbildung mit der Eigenschaft\n",
        "\n",
        "  * $d_{2}(f(x), f(y))=d_{1}(x, y)$ f√ºr alle $x, y \\in M_{1}$\n",
        "\n",
        "  * dann hei√üt $f$ Isometrie von $M_{1}$ nach $M_{2}$. Eine solche Abbildung ist stets injektiv.\n",
        "\n",
        "  * Ist $f$ sogar bijektiv, dann hei√üt $f$ **isometrischer Isomorphismus**, und die R√§ume $M_{1}$ und $M_{2}$ hei√üen is isometrische Einbettung von $M_{1}$ in $M_{2}$. [Isometrische Isomorphie](https://de.m.wikipedia.org/wiki/Isometrische_Isomorphie) beschreibt in der Funktionalanalysis einen Zusammenhang zwischen zwei unterschiedlichen R√§umen, die geometrisch identisch sind.\n",
        "\n",
        "* [**Vollst√§ndiger Raum**](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum)\n",
        "\n",
        "  * Ein vollst√§ndiger Raum ist in der Analysis ein metrischer Raum, in dem jede Cauchy-Folge von Elementen des Raums auf eine Zahl (element) innerhalb desselben raumes konvergiert.\n",
        "\n",
        "  * ZB alle rationalen Zahlen sollten nach einsetzung in die cauchy-folge mit einer bestimmten metrik (zB betragsmetrik) auch wieder als ziel in rationalen zahlen muenden. tun sie das nicht, ist es ein unvollst√§ndiger Raum.\n",
        "\n",
        "  * Andererseits ist der Raum der rationalen Zahlen mit der Betragsmetrik nicht vollst√§ndig, weil etwa die Zahl 2‚Äì‚àö nicht rational ist, es jedoch Cauchy-Folgen rationaler Zahlen gibt, die bei Einbettung der rationalen Zahlen in die reellen Zahlen gegen 2‚Äì‚àö und somit gegen keine rationale Zahl konvergieren. Es ist aber stets m√∂glich, die L√∂cher auszuf√ºllen, also einen unvollst√§ndigen metrischen Raum zu vervollst√§ndigen. Im Fall der rationalen Zahlen erh√§lt man dadurch den Raum der reellen Zahlen.\n",
        "\n",
        "  * Die Menge Q der rationalen Zahlen ist mit der Betragsmetrik\n",
        "\n",
        "  * $\n",
        "d(x, y)=|x-y|\n",
        "$\n",
        "\n",
        "  * nicht vollst√§ndig, denn die Folge rationaler Zahlen $x_{1}=1, x_{n+1}=\\frac{x_{n}}{2}+\\frac{1}{x_{n}}$ ist eine Cauchy-Folge, deren Grenzwert (siehe Heron-Verfahren) die irrationale Zahl $\\sqrt{2}$ ist, die nicht in $\\mathbb{Q}$ liegt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exkurs: Divergence, Distance and Metric*"
      ],
      "metadata": {
        "id": "SZkKviQcpLi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ‚â• 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ‚â§ d(x, y) + d(y, z)     (subadditivity / triangle inequality).\n",
        "\n",
        "**Begriffsabgrenzungen**\n",
        "\n",
        "* **Divergence** fullfills property of positive definiteness (1 + 2)\n",
        "\n",
        "* **Distance** fullfills property of positive definiteness and symmetrie (1 + 2+ 3)\n",
        "\n",
        "* **Metric** fullfills property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3 + 4). H√§ufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion) bezeichnet. Metric Space: Together with the set, a metric makes up a metric space."
      ],
      "metadata": {
        "id": "EmfRw-AgpSZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergences in Machine Learning**\n",
        "\n",
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold.\n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback‚ÄìLeibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ],
      "metadata": {
        "id": "oryOJsv-pW2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Meaning of 'no symmetry' in divergences**\n",
        "\n",
        "*\n",
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)\n",
        "\n",
        "* The Kullback-Leibler divergence is not symmetric. Roughly speaking, it's because you should think of the two arguments of the KL divergence as different kinds of things: the first argument is empirical data, and the second argument is a model you're comparing the data to.\n",
        "\n",
        "* Take a bunch of independent random variables $X_{1}, \\ldots, X_{n}$ whose possible values lie in a finite set.\n",
        "\n",
        "* Say these variables are identically distributed, with $\\operatorname{Pr}\\left(X_{i}=x\\right)=p_{x}$. Let $F_{n, x}$ be the number of variables whose values are equal to $x$. The list $F_{n}$ is a random variable, often called the \"empirical frequency distribution\" of the $X_{i} .$ What does $F_{n}$ look like when $n$ is very large?\n",
        "\n",
        "* More specifically, let's try to estimate the probabilities of the possible values of $F_{n} .$ since the set of possible values is different for different $n$, take a sequence of frequency distributions $f_{1}, f_{2}, f_{3}, \\ldots$ approaching a fixed frequency distribution $f$. It turns out $^{* *}$ that\n",
        "\n",
        "> $\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\ln \\operatorname{Pr}\\left(F_{n}=f_{n}\\right)=-\\mathrm{KL}(f, p)$\n",
        "\n",
        "* In other words, the Kullback-Leibler divergence of $f$ from $p$ lets you estimate the probability of getting an empirical frequency distribution close to $f$ from a large number of independent random variables with distribution $p$.\n"
      ],
      "metadata": {
        "id": "vxh1T80ovldM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1l9_Erf5T5S"
      },
      "source": [
        "###### *Topological Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy5f4vGA-Llx"
      },
      "source": [
        "**General topology (point-set topology)**\n",
        "\n",
        "> Topology $\\rightarrow$ Lagebeziehungen wie ‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú werden verallgemeinert\n",
        "\n",
        "The fundamental concepts in point-set topology are continuity, compactness, and connectedness:\n",
        "\n",
        "* **Continuous functions**, intuitively, take nearby points to nearby points.\n",
        "\n",
        "* **Compact sets** are those that can be covered by finitely many sets of arbitrarily small size.\n",
        "\n",
        "* **Connected sets** are sets that cannot be divided into two pieces that are far apart.\n",
        "\n",
        "**Once a choice of open sets is made, the properties of continuity, connectedness, and compactness, which use notions of nearness, can be defined using these open sets.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Trennungsaxiom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNzMCPIBjYm"
      },
      "source": [
        "**Topological Vector Space**\n",
        "\n",
        "> **Man nennt $T$ eine Topologie auf $X$, und das Paar ($X$,$T$) einen topologischen Raum.**\n",
        "\n",
        "* a topological space ([Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum)) may be defined as a set of points, along with a set of **neighbourhoods** for each point, satisfying a set of **axioms** relating points and neighbourhoods.\n",
        "\n",
        "* The definition of a topological space **relies only upon set theory** and is the most general notion of a mathematical space that allows for the definition of concepts such as **[continuity](https://en.m.wikipedia.org/wiki/Continuous_function#Continuous_functions_between_topological_spaces), [connectedness](https://en.m.wikipedia.org/wiki/Connected_space), and [convergence](https://en.m.wikipedia.org/wiki/Limit_of_a_sequence)**.\n",
        "\n",
        "* **Other spaces, such as manifolds and metric spaces, are specializations of topological spaces with extra structures or constraints.**\n",
        "\n",
        "* Topological spaces are **studied in Point-Set Topology** (General Topology)\n",
        "\n",
        "* ein topologischer Raum ist ein elementarer Gegenstand der Topologie\n",
        "\n",
        "* Durch die Einf√ºhrung einer topologischen Struktur auf einer Menge lassen sich intuitive Lagebeziehungen wie **‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú** aus dem [Anschauungsraum (Euklidischer Raum)](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) auf sehr viele und sehr allgemeine Strukturen √ºbertragen und mit pr√§ziser Bedeutung versehen.\n",
        "\n",
        "* Ein **[topologischer Vektorraum](https://de.m.wikipedia.org/wiki/Topologischer_Vektorraum)** / [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space) ist ein Vektorraum, auf dem neben seiner algebraischen auch noch eine damit vertr√§gliche topologische Struktur definiert ist.\n",
        "\n",
        "* Sei $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. Ein $\\mathbb{K}$ -Vektorraum $E$, der zugleich topologischer Raum ist, hei√üt topologischer Vektorraum, wenn folgende Vertr√§glichkeitsaxiome gelten:\n",
        "  * Die Vektoraddition $E \\times E \\rightarrow E$ ist stetig,\n",
        "  * Die Skalarmultiplikation $\\mathbb{K} \\times E \\rightarrow E$ ist stetig.\n",
        "\n",
        "* **Beispiele:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erkl√§rt, dass wir eine Menge $\\Omega$ C $\\mathbb{R}$ offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen l√§sst.\n",
        "\n",
        "* **Separation Axioms**: Topologische R√§ume k√∂nnen [klassifiziert werden nach Kolmogorov](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms).\n",
        "\n",
        "*What are some examples of topological spaces which are not a metric space?*\n",
        "\n",
        "A set with a single element  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  only has one topology, the discrete one (which in this case is also the indiscrete one‚Ä¶) So that‚Äôs not helpful.\n",
        "\n",
        "A set with two elements, however, is more interesting.  ùëã={‚àô,‚àò}\n",
        "X\n",
        "=\n",
        "{\n",
        "‚àô\n",
        ",\n",
        "‚àò\n",
        "}\n",
        "  can be topologized by having all subsets open (discrete), only the empty set and the whole space open (indiscrete), and also the topology in which  ‚àÖ,{‚àò}\n",
        "‚àÖ\n",
        ",\n",
        "{\n",
        "‚àò\n",
        "}\n",
        "  and  ùëã\n",
        "X\n",
        " are open (and the symmetric one with  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  open).\n",
        "\n",
        "A metric on a two-element set forces us to declare that there‚Äôs a certain distance between the two points, rendering both singleton sets open (and closed). Indeed, a finite metric space has a discrete topology. But we just found three topologies on  ùëã\n",
        "X\n",
        "  which are not the discrete one, giving us three examples of a non-metrizable topological space.\n",
        "\n",
        "Note that the indiscrete topology on  ùëã\n",
        "X\n",
        "  can be regarded as coming from the pseudo-metric where the distance between the points is zero. However, the other two topologies, where one singleton set is open and the other is not, cannot be interpreted in this way. A metric is by definition symmetric, and these two topologies are not.\n",
        "\n",
        "*Source: [Quora](https://www.quora.com/What-are-some-examples-of-topological-spaces-which-are-not-a-metric-space)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mx_opXBBr-r"
      },
      "source": [
        "**Frechet-R√§ume**\n",
        "\n",
        "* [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Verallgemeinerungen des Banachraums und topologische Vektorraum mit speziellen Eigenschaften. Die Hauptvertreter von Fr√©chet-R√§umen sind Vektorr√§ume von glatten Funktionen. Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten.\n",
        "\n",
        "* Es handelt sich um einen **topologischen Vektorraum** mit speziellen Eigenschaften, die ihn als **Verallgemeinerung des Banachraums** charakterisieren.\n",
        "\n",
        "* Die Hauptvertreter von [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Vektorr√§ume von [glatten Funktionen](https://de.wikipedia.org/wiki/Glatte_Funktion).\n",
        "  * Eine glatte Funktion ist eine mathematische Funktion, die unendlich oft differenzierbar (insbesondere stetig) ist.\n",
        "\n",
        "* Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUGnGb0RB0L-"
      },
      "source": [
        "**Uniform Spaces**\n",
        "\n",
        "* **[Uniforme R√§ume](https://de.m.wikipedia.org/wiki/Uniformer_Raum) erlauben es zwar nicht Abst√§nde einzuf√ºhren**, aber trotzdem Begriffe wie gleichm√§√üige Stetigkeit, Cauchy-Folgen, Vollst√§ndigkeit und Vervollst√§ndigung zu definieren. Jeder uniforme Raum ist auch ein topologischer Raum.\n",
        "\n",
        "* **Jeder topologische Vektorraum (egal ob metrisierbar oder nicht) ist auch ein uniformer Raum**. Allgemeiner ist jede kommutative topologische Gruppe ein uniformer Raum. Eine nichtkommutative topologische Gruppe tr√§gt jedoch zwei uniforme Strukturen, eine links-invariante und eine rechts-invariante. Topologische Vektorr√§ume sind in endlichen Dimensionen vollst√§ndig, in unendlichen Dimensionen im Allgemeinen aber nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_1$-Norm und $L^1$-Metrik*"
      ],
      "metadata": {
        "id": "6quxRLo3U9Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) im endlichdimensionalen Raum** (p=1, Lasso, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|$\n",
        "\n",
        "* zB Summennorm des reellen Vektors $x=(3,-2,6) \\in \\mathbb{R}^{3}$ ist $\\|x\\|_{1}=|3|+|-2|+|6|=11$\n",
        "\n",
        "* Von Summennorm abgeleitete Metrik ist [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik). Siehe auch [Taxicab_geometry](https://en.m.wikipedia.org/wiki/Taxicab_geometry).\n",
        "\n",
        "* Die Summennorm ist im Gegensatz zur euklidischen Norm (2-Norm) nicht von einem Skalarprodukt induziert.\n",
        "\n",
        "* Die Einheitssph√§re der reellen Summennorm ist ein Kreuzpolytop mit minimalem Volumen √ºber alle p-Normen. **Daher ergibt die Summennorm f√ºr einen gegebenen Vektor den gr√∂√üten Wert aller p-Normen**. (zB 3 + (-2) + 6 = 11 in Summennorm, aber = 7 in euklidischer Norm fur p=2)\n",
        "\n",
        "* Techniques which use an L1 penalty, like [LASSO](https://en.m.wikipedia.org/wiki/Lasso_(statistics)), encourage solutions where many parameters are zero\n",
        "\n",
        "\n",
        "*Summennorm in zwei Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Vector-1-Norm_qtl1.svg/316px-Vector-1-Norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "LdVhJHBAUSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summennorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "$\\ell^{1}-$ Norm (**Folgenraum**)\n",
        "\n",
        "* Die $\\ell^{1}$ -Norm ist die Verallgemeinerung der Summennorm auf den Folgenraum $\\ell^{1}$ der **betragsweise summierbaren Folgen** $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{N} .$\n",
        "\n",
        "* Hierbei wird lediglich **die endliche Summe durch eine unendliche ersetzt** und die $\\ell^{\\text {t }}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{1}}=\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|$\n",
        "\n",
        "$L^{1}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* Weiter kann die Summennorm auf den Funktionenraum $L^{1}(\\Omega)$ der auf einer Menge $\\Omega$ betragsweise integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{1}$ Norm einer **betragsweise Lebesgue-integrierbaren Funktion** $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}=\\int_{\\Omega}|f(x)| d x$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{1}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden.\n",
        "\n",
        "* Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{1}(\\Omega)$, die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{1}$ -Raum die $L^{1}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{1}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}$"
      ],
      "metadata": {
        "id": "MIi62WcSUgP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 - Manhattan Distance (Lasso)**\n",
        "\n",
        "* The Manhattan norm gives rise to the [Manhattan distance](https://de.m.wikipedia.org/wiki/Manhattan-Metrik), where the distance between any two points, or vectors, is the sum of the differences between corresponding coordinates.\n",
        "\n",
        "* **Die Manhattan-Metrik ist die von der Summennorm (1-Norm) eines Vektorraums erzeugte Metrik.**\n",
        "\n",
        "* ***Aber: Die Summennorm ist nicht von einem Skalarprodukt induziert.***\n",
        "\n",
        "* Die Manhattan-Metrik (auch Manhattan-Distanz, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz d zwischen zwei Punkten a und b als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird:\n",
        "\n",
        "> $d(a, b)=\\sum_{i}\\left|a_{i}-b_{i}\\right|$"
      ],
      "metadata": {
        "id": "n9vaUs0sXyOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_2$-Norm und $L^2$-Metrik*"
      ],
      "metadata": {
        "id": "32OR42wtUS3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidische Norm](https://de.m.wikipedia.org/wiki/Euklidische_Norm) im endlichdimensionalen Raum** (p=2, Ridge, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{2}= \\left(x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}\\right)^{1 / 2} = \\sqrt{\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}}$\n",
        "\n",
        "* Ziel: berechnen die L√§nge (Betrag) eines Vektors in der euklidischen Ebene.  The length of a vector $x = (x_1, x_2, ..., x_n)$ in the $n$-dimensional real vector space $\\mathbb{R}^n$ is usually given by the Euclidean norm $||x||_{2}$.\n",
        "\n",
        "* Die euklidische Norm ist eine von einem Skalarprodukt induzierte Norm (im Gegensatz zur p1 Summennorm)\n",
        "\n",
        "* Beispiel: Vektor ${\\vec {v}}$ mit Komponenten $x$, $y$ und $z$ in drei Dimensionen durch ${\\vec {v}}=(x,y,z)$ wird die L√§nge berechnet durch:\n",
        "\n",
        "> $|{\\vec {v}}|={\\sqrt {x^{2}+y^{2}+z^{2}}}$\n",
        "\n",
        "* Die Euclidean Norm besitzt als eine von einem Skalarprodukt [induzierte Norm](https://de.m.wikipedia.org/wiki/Skalarproduktnorm) **neben den [drei Normaxiomen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Definition) eine Reihe weiterer Eigenschaften**:\n",
        "\n",
        "  * die G√ºltigkeit der [Cauchy-Schwarz-Ungleichung](https://de.m.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung)\n",
        "\n",
        "  * der [Parallelogrammgleichung](https://de.m.wikipedia.org/wiki/Parallelogrammgleichung)\n",
        "\n",
        "  * sowie eine Invarianz unter unit√§ren Transformationen (Die euklidische Norm √§ndert sich also unter unit√§ren Transformationen nicht. F√ºr reelle Vektoren sind solche Transformationen beispielsweise Drehungen des Vektors um den Nullpunkt. Diese Eigenschaft wird zum Beispiel bei der numerischen L√∂sung linearer Ausgleichsprobleme √ºber die **Methode der kleinsten Quadrate mittels QR-Zerlegungen genutzt**.)\n",
        "\n",
        "* F√ºr orthogonale Vektoren erf√ºllt die euklidische Norm selbst eine allgemeinere Form des Satzes des Pythagoras.\n",
        "\n",
        "* Sieht man eine Matrix mit reellen oder komplexen Eintr√§gen als entsprechend langen Vektor an, so kann die euklidische Norm auch f√ºr Matrizen definiert werden und hei√üt dann [**Frobeniusnorm**](https://de.m.wikipedia.org/wiki/Frobeniusnorm). Die euklidische Norm kann auch auf unendlichdimensionale Vektorr√§ume √ºber den reellen oder komplexen Zahlen verallgemeinert werden und hat dann zum Teil eigene Namen.\n",
        "\n",
        "*Euklidische Norm in zwei reellen Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Vector-2-Norm_qtl1.svg/316px-Vector-2-Norm_qtl1.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "p1rB9cfOJadt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euklidische Norm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "* Als [Folgennorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum): Die $\\ell^{2}-$ Norm im Folgenraum ist die Verallgemeinerung der euklidischen Norm auf den [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum) $\\ell^{2}$ der quadratisch summierbaren Folgen $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{\\mathrm{N}} .$ Hierbei wird lediglich die endliche Summe durch eine unendliche ersetzt und die $\\ell^{2}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{2}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{2}\\right)^{1 / 2}$\n",
        "\n",
        "* Die ‚Ñì-p -R√§ume sind ein Spezialfall der allgemeineren Lp-R√§ume, wenn man das Z√§hlma√ü auf dem Raum N betrachtet.\n",
        "\n",
        "* Als [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im [Funktionenraum](https://de.m.wikipedia.org/wiki/Funktionenraum) $L^{2}(\\Omega)$-Norm der auf einer Menge $\\Omega$ quadratisch integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{2}$ Norm einer quadratisch Lebesgue-integrierbaren Funktion $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{2} d x\\right)^{1 / 2}$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{2}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{2}(\\Omega),$ die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{2}$ -Raum die $L^{2}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{2}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}$\n",
        "\n",
        "* Der Raum $L^{2}(\\Omega)$ ist der [Hilbertraum fur L2](https://de.m.wikipedia.org/wiki/Lp-Raum#Der_Hilbertraum_L2) mit dem Skalarprodukt zweier Funktionen\n",
        "\n",
        "> $\\langle f, g\\rangle_{L_{2}(\\Omega)}=\\int_{\\Omega} \\overline{f(x)} \\cdot g(x) d x$"
      ],
      "metadata": {
        "id": "mXr0GKzWTwNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euclidian Distance](https://de.m.wikipedia.org/wiki/Euklidischer_Abstand) (Metrik) induziert aus der Euklidischen Norm**  (L2, Ridge)\n",
        "\n",
        "> $d_{2}:(x, y) \\mapsto\\|x-y\\|_{2}=\\sqrt{d_{\\mathrm{SSD}}}=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}$\n",
        "\n",
        "* Ziel: berechnen den Abstand zwischen zwei Vektoren in der euklidischen Ebene\n",
        "\n",
        "* Special case of the [Minkowski distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) with p=2\n",
        "\n",
        "* In Statistik siehe auch [Tikhonov Regularization (Ridge)](https://en.m.wikipedia.org/wiki/Tikhonov_regularization). Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small."
      ],
      "metadata": {
        "id": "3eQ4165xTtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidian (Metric) Space](https://en.m.wikipedia.org/wiki/Euclidean_space)**\n",
        "\n",
        "* Together with the [Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance) the Euclidean space is a metric space (x element R, d). http://theanalysisofdata.com/probability/B_4.html\n"
      ],
      "metadata": {
        "id": "kbrT-D7PTrew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Syw4dL96u31"
      },
      "source": [
        "###### *Special: Euclidean $p_{‚àû}$-Norm und $L^{‚àû}$-Metrik*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm) (p $\\rightarrow \\infty$): $\\rightarrow$\n",
        "\n",
        "> $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Sie ist ein Spezialfall der [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm).\n",
        "\n",
        "* Anschaulich gesprochen ist **der aus der Maximumsnorm abgeleitete Abstand immer dann relevant, wenn man sich in einem mehrdimensionalen Raum in alle Dimensionen gleichzeitig und unabh√§ngig voneinander gleich schnell bewegen kann**. (zB Rochade beim Schach)\n",
        "\n",
        "* Allgemeiner kann die Maximumsnorm benutzt werden, um zu bestimmen, wie schnell man sich in einem zwei- oder dreidimensionalen Raum bewegen kann, wenn angenommen wird, dass die Bewegungen in x-, y- (und z-)Richtung unabh√§ngig, gleichzeitig und mit gleicher Geschwindigkeit erfolgen.\n",
        "\n",
        "* Noch allgemeiner kann man ein System betrachten, dessen Zustand durch n unabh√§ngige Parameter bestimmt wird. An allen Parametern k√∂nnen gleichzeitig und ohne gegenseitige Beeinflussung √Ñnderungen vorgenommen werden. **Dann ‚Äûmisst‚Äú die Maximumsnorm in Rn die Zeit, die man ben√∂tigt, um das System von einem Zustand in einen anderen zu √ºberf√ºhren**. Voraussetzung hierf√ºr ist allerdings, dass man die Parameter so normiert hat, dass gleiche Abst√§nde zwischen den Werten auch gleichen √Ñnderungszeiten entsprechen. Andernfalls m√ºsste man eine gewichtete Version der Maximumsnorm verwenden, die die unterschiedlichen √Ñnderungsgeschwindigkeiten der Parameter ber√ºcksichtigt.\n",
        "\n",
        "* F√ºr einen Vektor $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{R}^{n}$ nennt man $\\|x\\|_{\\max }:=\\max \\left(\\left|x_{1}\\right|, \\ldots,\\left|x_{n}\\right|\\right)$ $\\rightarrow$ $\\|x\\|_{\\infty}=\\max _{i=1, \\ldots, n}\\left|x_{i}\\right|$ die Maximumsnorm von x.\n",
        "\n",
        "*√Ñquivalenz der euklidischen Norm (blau) und der Maximumsnorm (rot) in zwei Dimensionen:* [Source](https://de.m.wikipedia.org/wiki/√Ñquivalente_Normen)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Equiv_2-norm_max-norm_qtl1.svg/240px-Equiv_2-norm_max-norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "rpLxgxJgWGVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximumsnorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "* Im Gegensatz zur Maximumsnorm wird die Supremumsnorm $\\|f\\|_{\\text {sup }}:=\\sup _{t \\in X}|f(t)|$ nicht f√ºr stetige, sondern f√ºr beschr√§nkte Funktionen $f$ definiert.\n",
        "\n",
        "* In diesem Fall ist es nicht notwendig, dass $X$ kompakt ist; $X$ kann eine beliebige Menge sein.\n",
        "\n",
        "* **Da stetige Funktionen auf kompakten R√§umen beschr√§nkt sind, ist die Maximumsnorm ein Spezialfall der Supremumsnorm**.\n",
        "\n",
        "* Die Supremumsnorm (auch Unendlich-Norm genannt) ist in der Mathematik eine Norm auf dem Funktionenraum der beschr√§nkten Funktionen. Im einfachsten Fall einer reell- oder komplexwertigen beschr√§nkten Funktion ist die Supremumsnorm das Supremum der Betr√§ge der Funktionswerte. Allgemeiner betrachtet man Funktionen, deren Zielmenge ein normierter Raum ist, und die Supremumsnorm ist dann das Supremum der Normen der Funktionswerte.\n",
        "\n",
        "* **F√ºr stetige Funktionen auf einer kompakten Menge ist die Maximumsnorm ein wichtiger Spezialfall der Supremumsnorm.**\n",
        "\n",
        "*Die Supremumsnorm der reellen Arkustangens-Funktion ist œÄ/2. Auch wenn die Funktion diesen Wert betragsm√§√üig nirgendwo annimmt, so bildet er dennoch die kleinste obere Schranke.*\n",
        "\n",
        "![alternativer Text](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Graf_arctg.svg/260px-Graf_arctg.svg.png)\n",
        "\n",
        "\n",
        "Supremumsnorm vs Maximumsnorm:\n",
        "\n",
        "* So ist etwa die **Supremumsnorm** der linearen Funktion $f(x)=x$ in diesem Intervall gleich $1 .$ Die Funktion nimmt diesen Wert zwar innerhalb des Intervalls nicht an, kommt inm jedoch beliebig nahe.\n",
        "\n",
        "* W√§hlt man stattdessen das abgeschlossene Einheitsintervall $M=[0,1]$, dann wird der Wert 1 angenommen und die Supremumsnorm entspricht der **Maximumsnorm**.\n",
        "\n",
        "\n",
        "$L^{‚àû}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* L‚àû is a **function space** (Funktionenraum). Its elements are the essentially bounded measurable functions. More precisely, L‚àû is defined based on an underlying measure space, (S, Œ£, Œº). Start with the set of all measurable functions from S to R which are essentially bounded, i.e. bounded up to a set of measure zero. Two such functions are identified if they are equal almost everywhere. Denote the resulting set by L‚àû(S, Œº).\n",
        "\n",
        "\n",
        "* [Normen auf Operatoren](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_Operatoren)\n",
        "\n",
        "$\\ell^{‚àû}$ -Norm (**Folgenraum**)\n",
        "\n",
        "* The vector space ‚Ñì‚àû is a **sequence space** (Folgenraum) whose elements are the bounded sequences. The vector space operations, addition and scalar multiplication, are applied coordinate by coordinate.\n",
        "\n",
        "* $\\ell^{\\infty},$ the (real or complex) vector space of bounded sequences with the **[supremum norm](https://de.m.wikipedia.org/wiki/Supremumsnorm)**, and $L^{\\infty}=L^{\\infty}(X, \\Sigma, \\mu)$, the vector space of essentially bounded measurable functions with the **[essential supremum norm](https://de.m.wikipedia.org/wiki/Wesentliches_Supremum)**, are two closely related Banach spaces.\n",
        "\n",
        "* In fact the former is a special case of the latter. As a Banach space they are the continuous dual of the Banach spaces $\\ell_{1}$ of absolutely summable sequences, and $L^{1}=L^{1}(X, \\Sigma, \\mu)$ of absolutely integrable measurable functions (if the measure space fulfills the conditions of being localizable and therefore\n",
        "semifinite).\n",
        "\n",
        "* Pointwise multiplication gives them the structure of a Banach algebra, and in fact they are the standard examples of abelian Von Neumann algebras."
      ],
      "metadata": {
        "id": "jC0WO_F2VJou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L ‚àû - Chebyshev Distance**\n",
        "\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance) (or Tchebychev distance), maximum metric, or L‚àû metric is a metric defined on a vector space **where the distance between two vectors is the greatest of their differences** along any coordinate dimension.\n",
        "\n",
        "* The maximum norm gives rise to the **Chebyshev distance** or chessboard distance, the minimal number of moves a chess king would take to travel from x to y. The Chebyshev distance is the L‚àû-norm of the difference, a special case of the Minkowski distance where p goes to infinity. It is also known as Chessboard distance.\n",
        "\n",
        "> $d_{\\infty}:(x, y) \\mapsto\\|x-y\\|_{\\infty}=\\lim _{p \\rightarrow \\infty}\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{p}\\right)^{\\frac{1}{p}}=\\max _{i}\\left|x_{i}-y_{i}\\right|$"
      ],
      "metadata": {
        "id": "reDInvHxXs-r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdr6F1zuU4MG"
      },
      "source": [
        "##### <font color=\"blue\">*Variationsanalyse*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AR9PIXysIa4"
      },
      "source": [
        "###### *Variational Principle*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Mathematisch gesehen ist die Wirkung ein Funktional. W√§hrend Funktionen bestimmten Zahlen andere Zahlen zuordnen, ordnen Funktionale bestimmten Funktionen Zahlen zu.\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "7qhLvSkEBgaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXIblPI6_-oP"
      },
      "source": [
        "**Variationsrechnung**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Variationsrechnung\n",
        "\n",
        "* calculus og variation, khan academy https://youtube.com/playlist?list=PLdgVBOaXkb9CD8igcUr9Fmn5WXLpE8ZE_\n",
        "\n",
        "* **Find [stationary points](https://internal.ncl.ac.uk/ask/numeracy-maths-statistics/core-mathematics/calculus/stationary-points.html) (=derivative is zero, local minima or maxima) of a functional, like an integral I[f] (=here for example the path lenghts, or time spent travelling) is minimal between two points a and b.**\n",
        "\n",
        "  * A stationary point of a function $f(x)$ is a point where the derivative of $f(x)$ is equal to 0 .\n",
        "  * These points are called \"stationary\" because at these points the function is neither increasing nor decreasing.\n",
        "  * Graphically, this corresponds to points on the graph of $f(x)$ where the tangent to the curve is a horizontal line.\n",
        " * The stationary points of a function $y=f(x)$ are the solutions to $\n",
        "\\frac{d y}{d x}=0 $. This repeats in mathematical notation the definition given above: \"points where the gradient of the function is zero\".\n",
        "\n",
        "* **The integral is a functional (=function of functions), its stationary point is a fix point / minima of a functional (not function). Solve (usually differential) equations for stationary function f(x) (via calculus of variations)**\n",
        "\n",
        "* from regular calculus to calculus of variations: find stationary functions, not only stationary points, a function becomes a functional.\n",
        "\n",
        "* **Typical problem in variational calculus: find minimal path between points A and B, not necessarily a linear one (in physics for examples check Brachistochrone !)**.\n",
        "\n",
        "* Also consider that velocity depending on position changes the minimum paths or time to travel (later in vector analysis relevant for Kurvenintegral)\n",
        "\n",
        "In general, Calculus of variations seeks to find y = f(x) such that this integral:\n",
        "\n",
        "> $I[f]=\\int_{x_{1}}^{x_{2}} F\\left(x, y, \\frac{d y}{d x}\\right) d x$\n",
        "\n",
        "is stationary (ps: $\\frac{d y}{d x}$ = $y'$)\n",
        "\n",
        "1. Die [Variationsrechnung](https://de.wikipedia.org/wiki/Variationsrechnung) ist eine **Erweiterung der Funktionalanalysis und beschaeftigt sich mit <u>nichtlinearen Funktionalen</u>** (in der Funktionalanalysis sind es linear Funktionale)\n",
        "\n",
        "2. The [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations) is a field that **uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals**: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. <u>**Functions that maximize or minimize functionals may be found using the Euler‚ÄìLagrange equation of the calculus of variations.**</u>\n",
        "\n",
        "* In calculus of variations we are **NOT concerned with finding fix points of functions (like local maxima in a function), but rather fix points of functionals.**\n",
        "\n",
        "\n",
        "* dann f√ºhrt eine Variation der Wirkung: https://de.m.wikipedia.org/wiki/Feldtheorie_(Physik)#Formalismus\n",
        "\n",
        "* Beispiel: https://de.wikipedia.org/wiki/Fluiddynamik\n",
        "\n",
        "* Martin: formulier problem in variationelle formulierung (dann bist du in sobolove r√§ume), und dann Eigenschaften von Testfunktionen ausnutzen\n",
        "\n",
        "* **Variation der Elemente**: die [Variation der Elemente](https://de.wikipedia.org/wiki/Variation_der_Elemente) ist eine im 19. Jahrhundert entwickelte Methode zur genauen Bahnbestimmung von Himmelsk√∂rpern. Sie dient bis heute zur Modellierung von [Bahnst√∂rungen](https://de.wikipedia.org/wiki/Bahnst√∂rung).\n",
        "\n",
        "* **History of variational principles in physics**:\n",
        "https://en.m.wikipedia.org/wiki/History_of_variational_principles_in_physics\n",
        "\n",
        "* [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential) ist eine **Verallgemeinerung des gew√∂hnlichen Differentiationsbegriffes** dar, indem es die Richtungsableitung auch in unendlichdimensionalen R√§umen definiert.\n",
        "\n",
        "* Variational method in quantum mechanics: In quantum mechanics, the [variational method](https://en.m.wikipedia.org/wiki/Variational_method_(quantum_mechanics)) is one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states. This allows calculating approximate wavefunctions such as molecular orbitals. The basis for this method is the variational principle.\n",
        "\n",
        "Die Variationsrechnung besch√§ftigt sich mit der Minimierung bzw. Maximierung von Funktionalen, die als Integral dargestellt werden k√∂nnen. Man k√∂nnte sie daher als ‚Äûnat√ºrliche‚Äú Methode zur L√∂sung physikalischer Probleme bezeichnen, da die Physik ja bekanntlich von Extremalprinzipen regiert wird (k√ºrzeste Bahn, kleinste Wirkung, Gesamtenergie, Hamilton-Funktion). Die ‚ÄûVariation‚Äú dieser Integralbeziehung bez√ºglich einer abh√§ngigen Gr√∂√üe (in der Physik z.B. der Bahnkurve im Zustandsraum) f√ºhrt auf eine Differentialgleichung, deren L√∂sung diesen Integralausdruck minimiert respektive maximiert.\n",
        "\n",
        "https://link.springer.com/chapter/10.1007/978-3-642-83621-3_7\n",
        "\n",
        "Der Name Variationsrechnung bezieht sich dabei auf die Technik der Variation der Argumente. Wesentliches Ziel der Variationsrechnung ist das Finden von Extrema (haufig unter Nebenbedingungen) f√ºr ein gegebenes Funktional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqXlKua2Mj8"
      },
      "source": [
        "**Variational Principle**\n",
        "\n",
        "* a [variational principle](https://en.wikipedia.org/wiki/Variational_principle) is one that enables a problem to be solved using calculus of variations, which concerns finding such functions which optimize the values of quantities that depend upon those functions.\n",
        "\n",
        "* For example, the problem of determining the shape of a hanging chain suspended at both ends‚Äîa catenary‚Äîcan be solved using variational calculus, and in this case, the variational principle is the following: The solution is a function that minimizes the gravitational potential energy of the chain.\n",
        "\n",
        "* Any physical law which can be expressed as a variational principle describes a **self-adjoint operator.** These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/1/10/Total_variation.gif)\n",
        "\n",
        "*As the green ball travels on the graph of the given function, the length of the path travelled by that ball's projection on the y-axis, shown as a red ball, is the total variation of the function.*\n",
        "\n",
        "* the [total variation](https://en.wikipedia.org/wiki/Total_variation) identifies several slightly different concepts, related to the (local or global) structure of the codomain of a function or a measure. For a real-valued continuous function f, defined on an interval [a, b] ‚äÇ ‚Ñù, its total variation on the interval of definition is a measure of the one-dimensional [arclength](https://en.wikipedia.org/wiki/Arc_length) of the curve with parametric equation x ‚Ü¶ f(x), for x ‚àà [a, b].\n",
        "\n",
        "* In der Variationsrechnung und der Theorie der stochastischen Prozesse ist die [Variation](https://de.wikipedia.org/wiki/Variation_(Mathematik)) (auch totale Variation genannt) einer Funktion **ein Ma√ü f√ºr das lokale Schwingungsverhalten der Funktion**.\n",
        "\n",
        "* Bei den stochastischen Prozessen ist die Variation von besonderer Bedeutung, da sie die Klasse der zeitstetigen Prozesse in zwei fundamental verschiedene Unterklassen unterteilt: jene mit endlicher und solche mit unendlicher Variation.\n",
        "\n",
        "Die [erste Variation](https://de.wikipedia.org/wiki/Erste_Variation) ist eine verallgemeinerte Richtungsableitung eines Funktionals. Ihre Eigenschaften sind in der angewandten Mathematik und der theoretischen Physik relevant. Die erste Variation spielt eine zentrale Rolle in der Variationsrechnung und wird in der analytischen Mechanik genutzt. Ein verwandtes Konzept ist die Funktionalableitung.\n",
        "\n",
        "In der Analysis ist eine Funktion von [beschr√§nkter Variation](https://de.wikipedia.org/wiki/Beschr√§nkte_Variation) (beschr√§nkter Schwankung), wenn ihre totale Variation (totale Schwankung) endlich ist, sie also in gewisser Weise nicht beliebig stark oszilliert. Diese Begriffe h√§ngen eng mit der Stetigkeit und der Integrierbarkeit von Funktionen zusammen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anwendungsgebiete**\n",
        "\n",
        "* Die Variationsrechnung ist die mathematische Grundlage aller physikalischen Extremalprinzipien und deshalb besonders in der theoretischen Physik wichtig, so etwa\n",
        "\n",
        "  * im Lagrange-Formalismus der klassischen Mechanik\n",
        "\n",
        "  * bzw. der Bahnbestimmung, in der Quantenmechanik in Anwendung des Prinzips der kleinsten Wirkung\n",
        "\n",
        "  * und in der statistischen Physik im Rahmen der Dichtefunktionaltheorie.\n",
        "\n",
        "  * In der Mathematik wurde die Variationsrechnung beispielsweise bei der riemannschen Behandlung des Dirichlet-Prinzips f√ºr harmonische Funktionen verwendet.\n",
        "\n",
        "  * Auch in der Steuerungs- und Regelungstheorie findet die Variationsrechnung Anwendung, wenn es um die Bestimmung von Optimalreglern geht.\n",
        "\n",
        "* Ein typisches Anwendungsbeispiel ist das Brachistochronenproblem: Auf welcher Kurve in einem Schwerefeld von einem Punkt A zu einem Punkt B, der unterhalb, aber nicht direkt unter A liegt, ben√∂tigt ein Objekt die geringste Zeit zum Durchlaufen der Kurve? Von allen Kurven zwischen A und B minimiert eine den Ausdruck, der die Zeit des Durchlaufens der Kurve beschreibt. Dieser Ausdruck ist ein Integral, das die unbekannte, gesuchte Funktion, die die Kurve von A nach B beschreibt, und deren Ableitungen enth√§lt."
      ],
      "metadata": {
        "id": "A4hJCXFKFMa9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkRUt1ihCd_"
      },
      "source": [
        "**Fundamentallemma der Variationsrechnung**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Fundamentallemma_der_Variationsrechnung\n",
        "\n",
        "**Fundamentalsatz der Variationsrechnung**\n",
        "\n",
        "* Fundamental Theorem of the Calculus of Variations - [Fundamentalsatz der Variationsrechnung](https://de.wikipedia.org/wiki/Fundamentalsatz_der_Variationsrechnung)\n",
        "\n",
        "* eng verwandt mit dem [weierstra√üschen Satz vom Minimum](https://de.wikipedia.org/wiki/Satz_vom_Minimum_und_Maximum)\n",
        "\n",
        "* Er behandelt die in der Variationsrechnung zentrale Frage, unter welchen Bedingungen reellwertige Funktionale ein Minimum annehmen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eloiR80j3LKg"
      },
      "source": [
        "###### *Brachistochrone & Tautochronie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNCf_416SQR"
      },
      "source": [
        "[**Brachistochrone Curve**](https://en.wikipedia.org/wiki/Brachistochrone_curve) (in rot): Der K√∂rper gleitet auf einer solchen Bahn schneller zum Ziel als auf jeder anderen Bahn, beispielsweise auf einer geradlinigen, obwohl diese k√ºrzer ist.\n",
        "\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/6/63/Brachistochrone.gif)\n",
        "\n",
        "* Brachistochrone: Path between 2 points $A$ and $B$ which minimizes the time taken by a particle falling from $A$ to $B$ under the influence of gravity.\n",
        "\n",
        "* Time = distance / speed. Goal: Mix of minimize distance and maximize speed\n",
        "\n",
        "* Johann I Bernoulli hat sich mit dem **Problem des schnellsten Falles** besch√§ftigt. Im Jahre 1696 fand er schlie√ülich die L√∂sung in der **Brachistochrone**. Heute sieht man dies oft als die **Geburtsstunde der Variationsrechnung**.\n",
        "\n",
        "* Video: [The Brachistochrone Problem and Solution | Calculus of Variations](https://www.youtube.com/watch?v=zYOAUG8PxyM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvO3MMS6JFg"
      },
      "source": [
        "**Tautochronie** der Brachistochrone ‚Äì von jedem Startpunkt auf der Kurve erreichen die Kugeln das ‚ÄûZiel‚Äú gleichzeitig.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/b/bd/Tautochrone_curve.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Bewegungsgleichungen (Newton)*"
      ],
      "metadata": {
        "id": "sprbeM9PikBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hintergrund: Bewegungsgleichungen**\n",
        "\n",
        "* Unter einer [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt.\n",
        "\n",
        "* In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung (=Beschleunigung / Acceleration)\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "> **Es gibt drei Ans√§tze f√ºr Bewegungsgleichungen: Newtonian Mechanics, Lagrangian and Hamiltonian. F√ºr letztere beiden gilt the Principle of Least Action.**"
      ],
      "metadata": {
        "id": "hXh4vP1EJIiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newtonsche Gesetze**\n",
        "\n",
        "Die [Newtonschen Gesetze](https://de.m.wikipedia.org/wiki/Newtonsche_Gesetze) (Fundamentalkonzept der klassischen Mechanik, Extremalprinzip des Wirkungsfunktionals) gelten als die Grundlage der klassischen Mechanik, auf der alle weiteren Modelle basieren. Zentrales Konzept dieser Formulierung ist die Einf√ºhrung von Kr√§ften, die eine Beschleunigung $\\ddot{\\vec{x}}$ einer Masse $m$ hervorrufen. Die Bewegungsgleichung dieser Masse wird bestimmt durch die √úberlagerung der Kr√§fte $\\vec{F}_{i}$, die auf die Masse wirken\n",
        "\n",
        "> $\n",
        "m \\ddot{\\vec{x}}=\\sum_{i=1}^{N} \\vec{F}_{i}\n",
        "$\n",
        "\n",
        "1. Ein kr√§ftefreier K√∂rper bleibt in Ruhe oder bewegt sich geradlinig mit konstanter Geschwindigkeit (siehe [Tr√§gheit](https://de.m.wikipedia.org/wiki/Tr√§gheit#Bedeutung_f√ºr_wichtige_Prinzipien_der_Mechanik))\n",
        "\n",
        "2. Kraft gleich Masse mal Beschleunigung. $(\\vec{F}=m \\cdot \\vec{a})$ $\\rightarrow$ Equation of Motion (2. Newtonsches Gesetz)\n",
        "\n",
        "3. Kraft gleich Gegenkraft: Eine Kraft von K√∂rper A auf K√∂rper B geht immer mit einer gleich gro√üen, aber entgegen gerichteten Kraft von K√∂rper B auf K√∂rper A einher.\n",
        "\n",
        ">$\n",
        "\\vec{F}_{A \\rightarrow B}=-\\vec{F}_{B \\rightarrow A}\n",
        "$\n",
        "\n",
        "**Bewegungsgleichungen**\n",
        "\n",
        "* eine [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) ist eine Gleichung, die die Entwicklung eines mechanischen Systems bei √§u√üeren Einfl√ºssen beschreibt\n",
        "\n",
        "* Unter einer Bewegungsgleichung versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt. In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung.\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "* L√∂sung: Die L√∂sung der Bewegungsgleichung ist die [Trajektorie](https://de.m.wikipedia.org/wiki/Trajektorie_(Physik)), auf der sich das System bewegt. Sie ist, abgesehen von einigen einfachen F√§llen (siehe Beispiele unten), meist nicht in analytisch geschlossener Form darstellbar und muss √ºber [numerische Methoden](https://de.m.wikipedia.org/wiki/Numerische_Mathematik) gewonnen werden. Dies ist z. B. zur Ermittlung der Trajektorien dreier Himmelsk√∂rper, die sich gegenseitig gravitativ anziehen, erforderlich (siehe [Dreik√∂rperproblem](https://de.m.wikipedia.org/wiki/Dreik%C3%B6rperproblem)). Zur L√∂sung eines N-Teilchensystems l√§sst sich die [discrete element method](https://de.m.wikipedia.org/wiki/Discrete_element_method) anwenden. In einfachen F√§llen wird die geschlossene L√∂sung als ‚ÄûBahngleichung‚Äú bezeichnet.\n",
        "\n",
        "**Newtonsche Axiome**\n",
        "\n",
        "Prinzipen: Zum Aufstellen von Bewegungsgleichungen in der klassischen Physik wird verwendet:\n",
        "\n",
        "* das 2. Newtonsche Gesetz,\n",
        "* der Lagrange-Formalismus oder\n",
        "* der Hamilton-Formalismus\n",
        "\n",
        "Darauf basierend ergibt sich die Bewegungsgleichung der Quantenmechanik, die Schr√∂dingergleichung.\n",
        "\n",
        "**In der Technischen Mechanik werden verwendet**:\n",
        "\n",
        "* das Prinzip der virtuellen Arbeit (D‚ÄôAlembertsches Prinzip)\n",
        "* das Prinzip der virtuellen Leistung (Prinzip von Jourdain)\n",
        "* das Prinzip des kleinsten Zwanges.\n",
        "\n"
      ],
      "metadata": {
        "id": "mi3ntZBhiiWf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYzlWwuA8hXl"
      },
      "source": [
        "###### *Principle of Least / Stationary Action (Lagrange-Formalismus)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange-Formalismus**\n",
        "\n",
        "Der [Lagrange-Formalismus](https://de.m.wikipedia.org/wiki/Lagrange-Formalismus) beschreibt die Gesetze der klassischen Mechanik durch die Lagrange-Funktion $L$, die f√ºr Systeme mit einem generalisierten Potential und holonomen [Zwangsbedingungen](https://de.m.wikipedia.org/wiki/Zwangsbedingung) als Differenz aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben ist:\n",
        "\n",
        ">$\n",
        "L=T-V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der Euler-Lagrange-Gleichungen, die die Ableitungen nach der Zeit $t$, den Geschwindigkeiten $\\dot{q}_{i}$ und den [generalisierten Koordinaten](https://de.m.wikipedia.org/wiki/Generalisierte_Koordinate) $q_{i}$ miteinander in Verbindung setzt:\n",
        "\n",
        ">$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial L}{\\partial \\dot{q}_{i}}=\\frac{\\partial L}{\\partial q_{i}}\n",
        "$\n",
        "\n",
        "> Action $S$ = Kinetic Energy - Potential Energy = $\\int (T - V) dt$ = $\\int (\\frac{1}{2} mv^2 - mgh) dt$\n",
        "\n",
        "$L$ is the Lagrangian of the particle: $L=E_{u}-U$.\n",
        "\n",
        "*Need to solve Lagrange equations to make $S$ stationary:*\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{1}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{1}}\\right)$\n",
        "\n",
        ">$\\frac{\\partial L}{\\partial q_{2}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{2}}\\right)$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{3}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{3}}\\right) \\quad \\begin{array}{c}\\\\ \\end{array}$\n",
        "\n",
        "We can use a general coordinate system:\n",
        "$\\\\ {\\left[q_{1}(t), q_{2}(t), q_{3}(t)\\right]}$\n",
        "\n",
        "*Lagrange-Gleichungen erster Art*\n",
        "\n",
        "* Mit den Lagrange-Gleichungen erster Art lassen sich die Zwangskr√§fte berechnen.\n",
        "\n",
        "* Wenn man annimmt, dass sich die √§u√üeren Kr√§fte aus einem Potential ableiten lassen, kann man die Bewegungsgleichung schreiben (Lagrange-Gleichung 1. Art):\n",
        "\n",
        "*Lagrange-Gleichungen zweiter Art*\n",
        "\n",
        "* Die Lagrange-Gleichungen zweiter Art ergeben sich als sogenannte Euler-Lagrange-Gleichungen eines Variationsproblems und liefern die Bewegungsgleichungen, wenn die Lagrange-Funktion gegeben ist.\n",
        "\n",
        "* Sie folgen aus der Variation des mit der Lagrange-Funktion gebildeten Wirkungsintegrals im Hamiltonschen Prinzip.\n",
        "\n",
        "Eingef√ºhrte Formulierung der klassischen Mechanik, in der die Dynamik eines Systems durch **eine einzige skalare Funktion, die Lagrange-Funktion**, beschrieben wird. Der Formalismus ist (im Gegensatz zu der newtonschen Mechanik, die a priori nur in Inertialsystemen gilt) auch in beschleunigten Bezugssystemen g√ºltig. Der Lagrange-Formalismus ist invariant gegen Koordinatentransformationen.\n",
        "\n",
        "Der [Langrange Formalismus](https://de.wikipedia.org/wiki/Lagrange-Formalismus) ist eine m√∂gliche (von vielen!) Formulierung der klassischen Mechanik. Hier wird die Dynamik eines Systems durch die Langrange-Funktion L(${\\boldsymbol{q}}$, $\\dot{\\boldsymbol{q}}$, $t$) beschrieben:\n",
        "\n",
        "* ${\\boldsymbol{q}}$ = (q1, q2, ..., qw) - allgemeine Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ = (d / dt) q ... ($\\dot{\\boldsymbol{q}}$1, $\\dot{\\boldsymbol{q}}$2.. $\\dot{\\boldsymbol{q}}$n) - Vektor der Wirkung (allgemeine Geschwindigkeiten)\n",
        "* $t$ - Zeit (Die explizite Zeitabh√§ngige ber√ºcksichtigt externe, zeitabh√§ngige Faktoren (z.B. Magnet-Felder etc). Wird auf Null gesetzt, denn bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "> $L_{x}(q, \\dot{q}, t)$ wird zu: $L_{x}(q, \\dot{q})$ = $T$<sub>kin</sub> - $V$<sub>pot</sub>"
      ],
      "metadata": {
        "id": "BR8DkOZUtAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Principle of Least Action or Stationary Action*\n",
        "\n",
        "> Action  ùëÜ  = Kinetic Energy - Potential Energy\n",
        "\n",
        "* Principle of Least Action = better: [Stationary Action](https://en.m.wikipedia.org/wiki/Stationary-action_principle) = Wirkung\n",
        "\n",
        "https://youtu.be/dPxhTiiq-1A\n",
        "\n",
        "[The Principle of Stationary Action](https://www.youtube.com/watch?v=M05ixbSOY80): If a particle/system $P$ travels from one point to another in the time interval $\\left[t_{1}, t_{2}\\right]$, the path the particle traverses is such that this function:\n",
        "\n",
        "> $\n",
        "S=\\int_{t_{1}}^{t_{2}} \\mathcal{L} \\text { dt}$\n",
        "\n",
        "is stationary.\n",
        "\n",
        "* Total energy = kinetic energy + potential energy.\n",
        "\n",
        "> **Kinetic - potential energy = Lagrangian $L$**\n",
        "\n",
        "> has no physical meaning !! It's a ver useful mathematical tool\n",
        "\n",
        "* Kinetic energy depends on velocity of a particle (detonated with a dot over x,y,z), potential energy depends on position of a particle x,y,z. Hence the Lagrangian of a particle depends on all of the positions and all of their time derivatives.\n",
        "\n",
        "* **Use and solve the three Lagrangian equations in order to determine the equation of motion of a particle (and Lagrange equations are equivalent to Newton's second law)**\n",
        "\n",
        "* And they can easily applied to other coordinate systems than cartesian (i.e cylindric, or spherical)\n",
        "\n",
        "* Also: **Lagrange equations are very similar to Euler-Lagrange Equations!**\n",
        "\n",
        "* Because the three Lagrange equations very strongly resemble the Euler-Lagrange-equation, there must be some functional that's being made stationary by Lagrange equations\n",
        "\n",
        "* **$S$ is the action integral (= a functional!)**\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_01.png)\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_02.png)"
      ],
      "metadata": {
        "id": "rfHQAhk-EqQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Euler-Lagrange Equation*"
      ],
      "metadata": {
        "id": "puX57AODxaUG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_F2rBw5bk4"
      },
      "source": [
        "**Euler-Lagrange Equation**\n",
        "\n",
        "*Why using Euler-Lagrange Equation?*\n",
        "\n",
        "1) lagrangian mechanies gives equation of motion without considering forces at all, only energy\n",
        "\n",
        "2) This is more convenient for complicated systems with multiple forces to be considered\n",
        "\n",
        "3) Great for dealing with multiple coordinate\n",
        "\n",
        "*How to get the Euler-Lagrange Equation?*\n",
        "\n",
        "* **Step 1: Lagrangian equation**: L = T (kinetic energy) - V (potential energy)\n",
        "\n",
        "  * T = $\\frac{1}{2}m \\dot x ^2 $\n",
        "\n",
        "    * with $\\dot x$ short for: $\\frac{dx}{dt}$\n",
        "\n",
        "  * V = $\\frac{1}{2}k x ^2 $\n",
        "\n",
        "  * so: L = $\\frac{1}{2}m \\dot x ^2 $ - $\\frac{1}{2}k x ^2 $ -> This is our Lagrangian for a specific system !\n",
        "\n",
        "* **Step 2: Now take Euler‚ÄìLagrange Equation**: $\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot q}\\right)=\\frac{\\partial L}{\\partial q}$\n",
        "\n",
        "  * we see it contains the Lagrangian\n",
        "\n",
        "  * it's consistent with Newtonian classical mechanics F=ma etc\n",
        "\n",
        "  * we can plug in our Lagrangian for a specific system (replace L with formula above). we get the equation of motion!\n",
        "\n",
        "* **Step 3: Equation of Motion**: $m \\ddot x = -kx$\n",
        "\n",
        "  * mass m of an object multipliplied by acceleration $\\ddot x$ (which is the second derivative of x, the whole left side is same as newton's: f=m*a)\n",
        "\n",
        "  * we are stating something about the forces acting on the system\n",
        "\n",
        "*More about Euler‚ÄìLagrange equation*\n",
        "\n",
        "* Langrangian = Kinetic Energy - Potential Engergy\n",
        "\n",
        "* then insert it into the **Euler-Langrange Equation**:\n",
        "\n",
        "> $\\frac{d}{d t} \\frac{\\partial L}{\\partial \\dot{\\theta}}=\\frac{\\partial L}{\\partial \\theta}$\n",
        "\n",
        "* the Euler-Langrange Equation is the condition of the action $S$ to be minimized, where action is integral of Lagrangian\n",
        "\n",
        "* Means: of all the possible paths a particle could follow, the actual path it chooses is the one that minimizes (or actually \"extremizes\") the action = **principle of least action**\n",
        "\n",
        "* with Lagrangian we don't need any vectors anymore like in Newtonian, we can sue whatever coordinates. Also it makes it easier to deal with constraints and understands symmetries\n",
        "\n",
        "* F = ma in the Euler Lagrange Equation gives us a single second-order differential equation\n",
        "\n",
        "The Euler‚ÄìLagrange equation is an equation satisfied by a function q of a real argument t, which is a stationary point of the functional:\n",
        "\n",
        "> $S(\\boldsymbol{q})=\\int_{a}^{b} L(t, \\boldsymbol{q}(t), \\dot{\\boldsymbol{q}}(t)) \\mathrm{d} t$\n",
        "\n",
        "* ${\\boldsymbol{q}}$ - Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ - Vektor der Wirkung\n",
        "* t - Zeit (wird auf Null gesetzt, den bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "The Euler‚ÄìLagrange equation, then, is given by\n",
        "\n",
        "> $L_{x}(t, q(t), \\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d} t} L_{v}(t, q(t), \\dot{q}(t))=0$\n",
        "\n",
        "* partial derivative of one dimension, then second dimension and then time\n",
        "\n",
        "* the [Euler equation](https://en.m.wikipedia.org/wiki/Euler‚ÄìLagrange_equation) is a **second-order partial differential** equation whose **solutions are the functions for which a given functional is stationary**.\n",
        "\n",
        "* Because **a differentiable functional is stationary at its local extrema**, the Euler‚ÄìLagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing or maximizing it.\n",
        "\n",
        "* This is analogous to [Fermat's theorem](https://en.m.wikipedia.org/wiki/Fermat%27s_theorem_(stationary_points)) in calculus, stating that at any point where a differentiable function attains a local extremum its derivative is zero.\n",
        "\n",
        "* In Lagrangian mechanics, according to [Hamilton's principle](https://en.m.wikipedia.org/wiki/Hamilton%27s_principle) of stationary action, the evolution of a physical system is described by the solutions to the Euler equation for the action of the system. In this context Euler equations are usually called Lagrange equations. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations.\n",
        "\n",
        "  * Hamilton's principle is William Rowan Hamilton's formulation of the [principle of stationary action](https://en.m.wikipedia.org/wiki/Principle_of_least_action) (also called: 'Principle of least action'). It states that the **dynamics of a physical system are determined by a variational problem for a functional based on a single function**, the Lagrangian, which may contain all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hamiltonian Function*"
      ],
      "metadata": {
        "id": "SqYBnfUAD1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamilton'sches Prinzip (Wirkungsfunktional)*\n",
        "\n",
        "> Das [Hamilton'sche Prinzip](https://de.wikipedia.org/wiki/Hamiltonsches_Prinzip) zeichnet tatsachlich durchlaufene Bahnen dadurch aus, **dass bei ihnen die Wirkung (=Funktional) S[q] (verglichen mit anderen Bahnen) ein Minimum annimmt (minimal variation ??)**.\n",
        "\n",
        "* Das Hamiltonsche Prinzip der Theoretischen Mechanik ist ein **Extremalprinzip**. Physikalische Felder und Teilchen nehmen danach f√ºr eine bestimmte Gr√∂√üe einen extremalen (d. h. gr√∂√üten oder kleinsten) Wert an. Diese Bewertung nennt man Wirkung (=Action), mathematisch ist die Wirkung ein Funktional, daher auch die Bezeichnung **Wirkungsfunktional**.\n",
        "\n",
        "* Die Wirkung erweist sich in vielen F√§llen nicht als minimal, sondern nur als **‚Äûstation√§r‚Äú** (d. h. extremal). Deshalb wird das Prinzip von manchen Lehrbuchautoren auch das Prinzip der **station√§ren Wirkung** genannt. Manche Autoren nennen das Hamiltonsche Prinzip auch **'Prinzip der kleinsten Wirkung'**, was jedoch ‚Äì wie oben ausgef√ºhrt ‚Äì nicht pr√§zise ist.\n",
        "\n",
        "* Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation:\n",
        "\n",
        "> $\\frac{\\delta \\mathcal{S}}{\\delta \\mathbf{q}(t)}=0$\n",
        "\n",
        "* That is, the system takes a path in configuration space for which the action is stationary, with fixed boundary conditions at the beginning and the end of the path.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Least_action_principle.svg/500px-Least_action_principle.svg.png)\n",
        "\n",
        "*As the system evolves, q traces a path through configuration space (only some are shown). The path taken by the system (red) has a stationary action (Œ¥S = 0) under small changes in the configuration of the system (Œ¥q).*"
      ],
      "metadata": {
        "id": "aH2WDPHcw04D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamiltonsche Mechanik & Hamilton-Funktion*\n",
        "\n",
        "> Die [**Hamilton-Funktion**](https://de.m.wikipedia.org/wiki/Hamilton-Funktion) eines Systems von Teilchen ist (,wenn keine rheonomen (d. h. zeitabh√§ngigen) Zwangsbedingungen vorliegen), **die Gesamtenergie als Funktion der Orte und Impulse der Teilchen** und gegebenenfalls der Zeit.\n",
        "\n",
        "* Total Energy = Kinetic Energy + Potential Energy\n",
        "\n",
        "* rewrite everything in terms of momentum, and we get the **Hamiltonian**:\n",
        "\n",
        "> $H=\\frac{p^{2}}{2 m l^{2}}-m g l \\cos \\theta$\n",
        "\n",
        "* the Hamiltonian gives us a pair of first-order differential equations for theta and pi\n",
        "\n",
        "* the generalized version is Momentum * Velocity - L (Lagrangian):\n",
        "\n",
        "> $H=P \\dot{\\theta}-L$\n",
        "\n",
        "* we get a new geometric perspective by connecting it with something known as \"flow phase space\": P (momentum) and Theta create a new vector space (pairs of them) called the \"phase space\", where I can see what the particle will do in the future. Energy is constant = particle travels along a line of constant energy\n",
        "\n",
        "Die [Hamiltonsche Mechanik](https://de.m.wikipedia.org/wiki/Hamiltonsche_Mechanik) ist die am st√§rksten verallgemeinerte Formulierung der klassischen Mechanik und Ausgangspunkt der Entwicklung neuerer Theorien und Modelle, wie der Quantenmechanik. Zentrale Gleichung dieser Formulierung ist die Hamilton-Funktion H. Sie ist folgenderma√üen definiert:\n",
        "\n",
        "> $\n",
        "H=\\sum_{i} \\dot{q}_{i} p_{i}-L(\\vec{q}, \\dot{\\vec{q}}, t)\n",
        "$\n",
        "\n",
        "Dabei sind $\\dot{q}_{i}$ die generalisierten Geschwindigkeiten und $p_{i}$ die generalisierten Impulse.\n",
        "\n",
        "> Die hamiltonschen Bewegungsgleichungen folgen aus dem hamiltonschen Prinzip der station√§ren Wirkung.\n",
        "\n",
        "Ist die potentielle Energie unabh√§ngig von der Geschwindigkeit und h√§ngen die TransformationsGleichungen, die die generalisierten Koordinaten definieren, nicht von der Zeit ab, ist die Hamilton-Funktion in der klassischen Mechanik durch die Summe aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben:\n",
        "\n",
        "> $\n",
        "H=T+V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der kanonischen Gleichungen:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\dot{q}_{i} &=\\frac{\\partial H}{\\partial p_{i}} \\\\\n",
        "\\dot{p}_{i} &=-\\frac{\\partial H}{\\partial q_{i}}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Mit dem Hamilton-Jacobi-Formalismus existiert eine modifizierte Form dieser Beschreibung, die die Hamilton-Funktion mit der Wirkung verkn√ºpft."
      ],
      "metadata": {
        "id": "ntdvyhHFuhn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operators in Classical Mechanics**\n",
        "\n",
        "[Operators in classical mechanics](https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics): In classical mechanics, the movement of a particle (or system of particles) is completely determined by the **Lagrangian** $L(q, \\dot{q}, t)$ or equivalently the **Hamiltonian** $H(q, p, t)$, a function of the generalized coordinates $q$, generalized velocities $\\dot{q}=\\mathrm{d} q / \\mathrm{d} t$ and its conjugate momenta:\n",
        "\n",
        ">$\n",
        "p=\\frac{\\partial L}{\\partial \\dot{q}}\n",
        "$\n",
        "\n",
        "If either $L$ or $H$ is independent of a generalized coordinate $q$, meaning the $L$ and $H$ do not change when $q$ is changed, which in turn means the dynamics of the particle are still the same even when q changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate $q$ is a symmetry). **Operators in classical mechanics are related to these symmetries.**\n",
        "\n",
        "More technically, when $H$ is invariant under the action of a certain group of transformations $G$ :\n",
        "\n",
        ">$\n",
        "S \\in G, H(S(q, p))=H(q, p)\n",
        "$\n",
        "\n",
        "the elements of $G$ are physical operators, which map physical states among themselves.\n",
        "\n",
        "**Connections to Quantum Mechanics**\n",
        "\n",
        "* Functions on phase phase in classical mechanics turn into operators in the quantum space of states in quantum mechanics\n",
        "\n",
        "* And if you know the state of a quantum system at time t0, the Schr√∂dinger equation says they at a later time t will be $|\\psi \\rangle$ $\\rightarrow$ $e^{-\\frac{i}{\\hbar} H t}|\\psi\\rangle$ acting on the state,\n",
        "\n",
        "* where $H$ is the operator version of the classical Hamiltonian function\n",
        "\n",
        "**Feynman Path Integral Formulation**\n",
        "\n",
        "> *Principle of Least Action in Quantum Mechanics (Path Integral Formulation)*\n",
        "\n",
        "The [path integral formulation](https://en.m.wikipedia.org/wiki/Path_integral_formulation) is a description in quantum mechanics that **generalizes the action principle of classical mechanics**. It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or functional integral, over an infinity of quantum-mechanically possible trajectories to compute a quantum amplitude.\n",
        "\n",
        "* Richard Feynman zeigte in den 1940ern, dass sich das Hamiltonsche Prinzip in der Quantenfeldtheorie gerade dadurch ergibt, dass alle m√∂glichen Pfade (auch die nicht zielgerichteten) zul√§ssig sind und aufintegriert werden. Dabei √ºberlagern sich Pfade mit extremaler Wirkung konstruktiv und davon abweichende destruktiv, so dass die Natur schlie√ülich zielgerichtet erscheint.\n",
        "\n",
        "* Principle of least action is equivalent to newtonian mechanics (one can derive the on from the other). And both is for large scale objects, **meanwhile the principle of least action is the large scale approximation of the feynman path integral on quantum objects!** source at Min 7:53 here: https://www.youtube.com/watch?v=dPxhTiiq-1A\n",
        "\n",
        "**Bewegungsgleichung der Allgemeinen Relativit√§tstheorie**\n",
        "\n",
        "Die [Bewegung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) eines K√∂rpers wird durch die Geod√§tengleichung der gekr√ºmmten Raumzeit beschrieben, sofern nur gravitative Kr√§fte auf ihn einwirken. Dann bewegt sich der K√∂rper entlang einer Geod√§ten der Raumzeit. Die Geod√§tengleichung lautet\n",
        "\n",
        "> $\n",
        "\\ddot{x}^{\\mu}+\\Gamma_{\\lambda \\nu}^{\\mu} \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=\\ddot{x}^{\\mu}+\\frac{g^{\\mu \\rho}}{2}\\left(\\partial_{\\lambda} g_{\\nu \\rho}+\\partial_{\\nu} g_{\\lambda \\rho}-\\partial_{\\rho} g_{\\lambda \\nu}\\right) \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=0\n",
        "$\n",
        "\n",
        "wobei $\\Gamma_{\\lambda \\nu}^{\\mu}$ ein Christoffelsymbol 2. Art ist, welches die Abh√§ngigkeit des metrischen Tensors vom Raumzeitpunkt (Ereignis), d. h. der Kr√ºmmung der Raumzeit, charakterisiert."
      ],
      "metadata": {
        "id": "anQlQ5k0TzaJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zER_lMlN46"
      },
      "source": [
        "###### *Special: Minimal Surfaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**minimal surface**\n",
        "\n",
        "A [minimal surface](https://en.m.wikipedia.org/wiki/Minimal_surface) is a surface that locally minimizes its area.\n",
        "\n",
        "* Intro Video: https://youtu.be/_t-3lCZXlPM\n",
        "\n",
        "* Siehe auch: https://www.chemie-schule.de/KnowHow/Oberfl√§chenspannung\n",
        "\n",
        "* This is equivalent to having **zero mean curvature = second derivative is always zero at any point**\n",
        "\n",
        "* **Minimal surfaces represent the lowest energy state!** A flat plane is minimalist surface area\n",
        "\n",
        "* Erwin Schr√∂dinger used Minimal Surfaces equations in 1926 to describe the quantum state of real phsysical systems\n",
        "\n",
        "* the apparent horizon of a back whole can be are always minimal surfaces\n",
        "\n",
        "* non trivial minimal surface: [Katenoid](https://en.m.wikipedia.org/wiki/Catenoid) and a [Helicoid (Wendelfl√§che)](https://de.m.wikipedia.org/wiki/Wendelfl√§che)\n",
        "\n",
        "* For a given constraint there may also exist several minimal surfaces with different areas (for example, see [minimal surface of revolution](https://en.m.wikipedia.org/wiki/Minimal_surface_of_revolution)): the standard definitions only relate to a local optimum, not a global optimum.\n",
        "\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Catenoid.svg/600px-Catenoid.svg.png)\n",
        "\n",
        "\n",
        "Minimal surface theory originates with Lagrange who in 1762 considered the variational problem of finding the surface z = z(x, y) of least area stretched across a given closed contour. He derived the Euler‚ÄìLagrange equation for the solution\n",
        "\n",
        "> $\\frac{d}{d x}\\left(\\frac{z_{x}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)+\\frac{d}{d y}\\left(\\frac{z_{y}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)=0$\n",
        "\n",
        "He did not succeed in finding any solution beyond the plane. In 1776 Jean Baptiste Marie Meusnier discovered that the helicoid and catenoid satisfy the equation and that the differential expression corresponds to twice the mean curvature of the surface, concluding that surfaces with zero mean curvature are area-minimizing.\n",
        "\n",
        "By expanding Lagrange's equation to\n",
        "\n",
        "> $\\left(1+z_{x}^{2}\\right) z_{y y}-2 z_{x} z_{y} z_{x y}+\\left(1+z_{y}^{2}\\right) z_{x x}=0$\n",
        "\n",
        "Gaspard Monge and Legendre in 1795 derived representation formulas for the solution surfaces. While these were successfully used by Heinrich Scherk in 1830 to derive his surfaces, they were generally regarded as practically unusable. Catalan proved in 1842/43 that the helicoid is the only ruled minimal surface.\n",
        "\n",
        "Eine Minimalfl√§che ist eine Fl√§che im Raum, die lokal minimalen Fl√§cheninhalt hat. Derartige Formen nehmen beispielsweise Seifenh√§ute an, wenn sie √ºber einen entsprechenden Rahmen (wie etwa einen Blasring) gespannt sind. In mathematischer Sprache sind Minimalfl√§chen die kritischen Punkte des Fl√§cheninhaltsfunktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\int \\sqrt{g(u)} \\mathrm{d}^{n} u$\n",
        "\n",
        "\n",
        "Hierbei sind die Gr√∂√üen $g(u):=\\operatorname{det}\\left(g_{i j}(u)\\right)_{i, j=1, \\ldots, n}$ und $g_{i j}(u)=\\left(\\frac{\\partial \\mathbf{x}}{\\partial u_{i}}\\right)^{T} \\frac{\\partial \\mathbf{x}}{\\partial u_{j}}$ f√ºr $i, j=1, \\ldots, n$  erkl√§rt (vgl. Hesse-Matrix).\n",
        "\n",
        "**Man beachte, dass eine Minimalfl√§che nicht notwendig minimalen Fl√§cheninhalt hat, sondern lediglich ein station√§rer Punkt des Fl√§cheninhaltsfunktionals ist.** Man kann zeigen, dass das Verschwinden der ersten Variation des Fl√§cheninhaltsfunktionals in zwei Raumdimensionen √§quivalent zum Verschwinden der mittleren Kr√ºmmung H ist, falls die betrachtete Mannigfaltigkeit hinreichend regul√§r ist.\n",
        "\n",
        "\n",
        "**Formulierung als Variationsproblem**\n",
        "\n",
        "Eine Fl√§che ist genau dann eine [Minimalfl√§che](https://de.m.wikipedia.org/wiki/Minimalfl√§che), wenn sie an jedem Punkt die mittlere Kr√ºmmung null hat. Damit stellt sich eine Minimalfl√§che als Spezialfall einer Fl√§che vorgeschriebener mittlerer Kr√ºmmung dar. Diese entziehen sich ebenfalls nicht der Variationsrechnung, sie sind Minima des Hildebrandtschen Funktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\iint\\left(\\left|\\mathbf{x}_{u} \\times \\mathbf{x}_{v}\\right|+2\\left(Q(\\mathbf{x}), \\mathbf{x}_{u}, \\mathbf{x}_{v}\\right)\\right) \\mathrm{d} u \\mathrm{~d} v$\n",
        "\n",
        "Die Eulerschen Gleichungen als notwendige Minimalit√§tsbedingungen dieses Funktionals sind das nach Franz Rellich benannte H-Fl√§chen-System\n",
        "\n",
        "Siehe auch: [Schwarz minimal surface](https://en.m.wikipedia.org/wiki/Schwarz_minimal_surface)\n",
        "\n",
        "$\n",
        "\\Delta \\mathbf{x}=2 H \\mathbf{x}_{u} \\times \\mathbf{x}_{v}, \\quad \\mathbf{x}_{u}^{2}-\\mathbf{x}_{v}^{2}=0=\\mathbf{x}_{u} \\mathbf{x}_{v}\n",
        "$\n",
        "\n",
        "Hierbei ist $H=\\operatorname{div} Q$ die mittlere Kr√ºmmung."
      ],
      "metadata": {
        "id": "D5Q9F5n4hpl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Vektoranalysis*"
      ],
      "metadata": {
        "id": "DfJ1ATjNLmjN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH8b8Mbvy_Fe"
      },
      "source": [
        "###### <font color=\"blue\">*Grundbegriffe der Vector Analysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zusammenfassung: https://www.maths2mind.com/geometrie/vektorrechnung-ebene-im-raum/vektoranalysis"
      ],
      "metadata": {
        "id": "I2kLug49-L9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu k√∂nnen, ist eine Basis n√∂tig. Im n-dimensionalen Raum besteht diese aus n linear unabh√§ngigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur f√ºr lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) m√∂glich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabh√§ngigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): F√ºr echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabh√§ngigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekr√ºmmt sind, sind die Koordinatenachsen nicht r√§umlich fest, wie es f√ºr kartesische Koordinaten gilt. Dies f√ºhrt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abh√§ngt ‚Äì im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenfl√§chen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgew√§hlten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "AobQvOfI9Az_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maxwell equations: https://youtu.be/hJD8ywGrXks"
      ],
      "metadata": {
        "id": "UFjY6T-dTfym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Deepest Intuition for the 4 Maxwell Equations](https://youtu.be/hJD8ywGrXks)"
      ],
      "metadata": {
        "id": "C8MAzljP8ydI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxw_I_chlfSU"
      },
      "source": [
        "https://youtube.com/playlist?list=PLBh2i93oe2quqhn4TZjMnBM2v7lG5QhDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r66BzwvTX8Ue"
      },
      "source": [
        "* Die [Vektoranalysis](https://de.m.wikipedia.org/wiki/Vektoranalysis) ist ein **Teilgebiet der Tensoranalysis**, besch√§ftigt sich haupts√§chlich mit **Vektorfeldern in zwei oder mehr Dimensionen**\n",
        "\n",
        "* Die Vektoranalysis **verallgemeinert die Differential- und der Integralrechnung** (z.B. werden Verzerrungen auf Oberflachen bei der Integration berucksichtigt, oder Stroemungen bei Wegen)\n",
        "\n",
        "* Betrachtet werden **Vektorfelder**, die jedem Punkt des Raumes einen Vektor zuordnen, und **Skalarfelder**, die jedem Punkt des Raumes einen Skalar zuordnen.\n",
        "\n",
        "*  Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwHPxe488bdG"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Polarkoordinaten#Vektoranalysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib8wN8Qsy42W"
      },
      "source": [
        "* **Kurven** in $\\mathbb{R}^{2}$ oder $\\mathbb{R}^{3}$: Wege bzw. **parametrisierte** Kurven in $\\mathbb{R}^{n}$ sind **stetige** Abbildungen $\\gamma$ [a, b] -> $\\mathbb{R}^{n}$\n",
        "\n",
        "* **Regul√§re Wege** bzw. Kurven: **stetig + differenzierbar** und Norm der Ableitung ist die Summe der Komponenten: $\\|\\dot{\\gamma}(t)\\|^{2}=\\left|\\dot{\\gamma}_{1}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{2}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{3}(t)\\right|^{2} \\neq 0$ fur alle t $\\in$ [a,b]. Bedeutet auch: es gibt uberall einen Tangentialvektor.\n",
        "\n",
        "* **Tangentialvektor**: Ableitung / Geschwindigkeitsvektor an einem Punkt, der in eine Richtung zeigt, der tangential zur Kurve zeigt. Den Vektor normiert man (dividiert durch Norm): $T_{\\gamma}(t):=\\frac{\\dot{\\gamma}(t)}{\\|\\dot{\\gamma}(t)\\|}$\n",
        "\n",
        "* **Normalenvektor**: nur definiert in einer Ebene, also in R2. Sollte senkrecht auf der Kurve / senkrecht auf dem Tangentialvektor stehen (man muss also diesen Punkt $\\dot{\\gamma}(t)=\\left(\\begin{array}{l}\\dot{\\gamma}_{1}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$ um 90 Grad drehen, damit er senkrech steht): $N_{\\gamma}(t):=\\frac{1}{\\|\\dot{\\gamma}(t)\\|}\\left(\\begin{array}{c}-\\dot{\\gamma}_{2}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$.\n",
        "\n",
        "* **Die Norm im Normalenvektor (Jacobi-Determinante) gibt zB die Kruemmung / Verzerrung einer Flaeche an, die bei der Integration beruecksichtigt werden muss.**\n",
        "\n",
        "* Eine [**vektorielle Gr√∂√üe**](https://de.wikipedia.org/wiki/Vektorielle_Gr√∂√üe) oder gerichtete Gr√∂√üe ist eine physikalische Gr√∂√üe, die ‚Äì im Gegensatz zu den skalaren Gr√∂√üen ‚Äì einen Richtungscharakter hat.\n",
        "  * Typische vektorielle Gr√∂√üen sind die kinematischen Gr√∂√üen Geschwindigkeit und Beschleunigung, die dynamischen Gr√∂√üen Impuls und Kraft bzw. Drehimpuls und Drehmoment sowie die Feldst√§rken der elektrischen und magnetischen Felder der Elektrodynamik.\n",
        "  * Vektorielle Gr√∂√üen werden sowohl zeichnerisch als auch rechnerisch wie geometrische Vektoren behandelt, wobei einige Besonderheiten zu beachten sind."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Arc Length (Bogenl√§nge)**"
      ],
      "metadata": {
        "id": "zLvShGPKxHL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $\\begin{aligned} \\text { Arclength } &=\\int_{a}^{b} \\sqrt{f^{\\prime}(t)^{2}+g^{\\prime}(t)^{2}+h^{\\prime}(t)^{2}} d t \\\\ &=\\int_{a}^{b}|\\vec{v}(t)| d t \\end{aligned}$\n",
        "\n",
        "The arc length is that you integrate out the magnitude of the velocity vector (see first derivative in each!).\n",
        "\n",
        "> **The arc length tells you the distance that you travelled along the curve**\n",
        "\n",
        "It doesnt matter what parametrization you choose. It doesnt matter of one drove twice as fast as another one. It‚Äòs always going to give the same arc length. That makes it independent of arbitrary choices, like specific paths which we drive along the curve.\n",
        "\n",
        "Arc length is intrinsic to the curve, not the specific path that you take along the curve.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLBtmKqDxJxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Arclength Parameter**\n",
        "$\n",
        "s(t)=\\int_{a}^{t}|\\vec{v}(\\tau)| d \\tau\n",
        "$\n",
        "\n",
        "* arc length parameter um zwischenwerte zu erhalten, zB wenn ich 5 minuten gefahren bin, wieviel weg habe ich hinter mir?\n",
        "\n",
        "**Bogenl√§nge** (L√§nge der Kurve / [Arc Length](https://en.wikipedia.org/wiki/Arc_length)): Integration der Geschwindigkeitsvektoren in der Norm: $L[\\gamma]=\\int_{a}^{b}\\|\\dot{\\gamma}(t)\\| d t$\n",
        "\n",
        "  * A curve in the plane can be approximated by connecting a finite number of points on the curve using [line segments](https://en.m.wikipedia.org/wiki/Line_segment) (Strecke) to create a polygonal path.\n",
        "\n",
        "  * Since it is straightforward to calculate the length of each linear segment (using the Pythagorean theorem in Euclidean space, for example), the total length of the approximation can be found by summation of the lengths of each linear segment;\n",
        "\n",
        "  * that approximation is known as the (cumulative) [chordal distance](https://en.m.wikipedia.org/wiki/Chord_(geometry)) (Sehne).\n",
        "\n"
      ],
      "metadata": {
        "id": "pW6s4YdZOkb5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZoCkWyJeNH"
      },
      "source": [
        "**Beispielaufgabe in der Vektoranalysis**\n",
        "\n",
        "* Betrachten Sie die folgende parametrisierte Kurve: $\n",
        "\\gamma:[0, \\infty) \\rightarrow \\mathbb{R}^{2}, \\quad \\gamma(t)=\\left(\\begin{array}{c}\n",
        "t \\cos (t) \\\\\n",
        "t \\sin (t)\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "  * (a) Skizzieren Sie die Kurve, d. h. das Bild der Abbildung.\n",
        "\n",
        "  * (b) Handelt es sich um einen regul√§ren Weg?\n",
        "\n",
        "  * (c) Geben Sie Tangenteneinheitsvektor und Normaleneinheitsvektor f√ºr jeden Punkt an."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArSX-a3HWv_P"
      },
      "source": [
        "**Mehrdimensionale Integration (Kurven, Oberfl√§chen, Volumen)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUq40bq5F6GH"
      },
      "source": [
        "Mehrdimensionale Integration im Skalar- & Vektorfeld\n",
        "\n",
        "* zB zur Berechnung der Laenge einer Kurve (zB an einer Spirale), die Oberflache eines Volumens, der Masse eines Koerpers\n",
        "\n",
        "* Kurven-, Oberfl√§chen-, Volumenintegral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uimHevy5O9az"
      },
      "source": [
        "https://www.youtube.com/watch?v=H1Pj4SMVZ8s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Felder*"
      ],
      "metadata": {
        "id": "aGvecdZZd-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Aolz28MRggQ"
      },
      "source": [
        "###### <font color=\"black\">*Skalarfeld*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbKijNH7RjtR"
      },
      "source": [
        "* In der mehrdimensionalen Analysis, der Vektorrechnung und der Differentialgeometrie ist ein [skalares Feld (kurz Skalarfeld)](https://de.wikipedia.org/wiki/Skalarfeld) $\\varphi$ **eine Funktion, die jedem Punkt eines Raumes eine reelle Zahl (Skalar) zuordnet**, z. B. eine Temperatur, Luftdruck.\n",
        "\n",
        "* Wichtige Operationen im Zusammenhang mit Skalarfeldern sind:\n",
        "\n",
        "  * **[Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) eines Skalarfeldes, der ein Vektorfeld ist**.\n",
        "\n",
        "  * [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) eines Skalarfeldes: die Richtungsableitung einer von mehreren Variablen abh√§ngigen Funktion ist die **momentane √Ñnderungsrate dieser Funktion in einer durch einen Vektor vorgegebenen Richtung**. (Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale R√§ume ist das [G√¢teaux-Differential](https://de.m.wikipedia.org/wiki/G%C3%A2teaux-Differential).)\n",
        "  * Ein Skalarfeld ist das einfachste [Tensorfeld](https://de.m.wikipedia.org/wiki/Tensorfeld).\n",
        "\n",
        "* Skalarfelder sind von gro√üer Bedeutung in der Feldbeschreibung der Physik und in der mehrdimensionalen Vektoranalysis.\n",
        "\n",
        "* Man unterscheidet dabei zwischen **reellwertigen** Skalarfeldern $\\varphi\\colon M\\to \\mathbb {R}$ und **komplexwertigen** Skalarfeldern $\\displaystyle \\varphi \\colon M\\to \\mathbb {C} $.\n",
        "\n",
        "* Man spricht von einem **station√§ren Skalarfeld**, wenn die Funktionswerte nur vom Ort abh√§ngen. H√§ngen sie auch von der Zeit ab, handelt es sich um ein **instation√§res Skalarfeld**.\n",
        "\n",
        "* Beispiele f√ºr Skalarfelder in der Physik sind der Luftdruck, die Temperatur, Dichte oder allgemein **Potentiale (= Skalarpotentiale)**.\n",
        "\n",
        "* Im Gegensatz zum Skalarfeld ordnet ein Vektorfeld jedem Punkt einen Vektor zu. Ein Skalarfeld ist das einfachste Tensorfeld\n",
        "\n",
        "* Ein **Niveaufeld** ist ein Feld, wo √ºberall die gleiche skalare Gr√∂sse vorliegt (zB die gleiche Temperatur ohne √Ñnderungen (Steigungen, Senken)) = english: Level Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLeX-skuQC_8"
      },
      "source": [
        "###### <font color=\"black\">*Vektorfeld*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you give a **magnitude and direction** at any given point, then you get a vector field."
      ],
      "metadata": {
        "id": "VpsrL-yaTDrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Vector_field"
      ],
      "metadata": {
        "id": "L-8XfsBVb0Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Fields in 2D: https://www.geogebra.org/m/cXgNb58T"
      ],
      "metadata": {
        "id": "U2cFaHn5DMyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector field 3D: https://www.geogebra.org/m/u3xregNW"
      ],
      "metadata": {
        "id": "1MGNXwBADQfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_027.jpg)"
      ],
      "metadata": {
        "id": "uIZ25FIrUv-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn man ein Vector Field erstellt an jedem Punkt im Feld mit x,y Input"
      ],
      "metadata": {
        "id": "P478Tq7zU67k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_028.PNG)"
      ],
      "metadata": {
        "id": "0l0U-BU-U4U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three different vector fields and their functions:"
      ],
      "metadata": {
        "id": "mR2pSjaVWrYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_029.jpg)"
      ],
      "metadata": {
        "id": "brVAzf60Wo4p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3O9MS9VNgPz"
      },
      "source": [
        "**Der Gradient eines Skalarfeldes ist ein Vektorfeld (zB: Skalar ist die Temperatur in einem Pool, und Temperaturveranderung uber Zeit ist die erste Ableitung dessen und dann ein Vektorfeld), und die zweite Ableitung ist die Beschleunigung der Temperaturveranderung.**\n",
        "\n",
        "* In der mehrdimensionalen Analysis und der Differentialgeometrie ist ein [Vektorfeld](https://de.m.wikipedia.org/wiki/Vektorfeld) eine Funktion, die jedem Punkt eines Raumes einen Vektor zuordnet.\n",
        "\n",
        "* Eine Abbildung $V: D \\rightarrow \\mathbb{R}^{n}, D \\subseteq \\mathbb{R}^{n}$ mit n = 1,2,3..\n",
        "\n",
        "* Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat.\n",
        "\n",
        "* Meist sind Vektorfelder stetig differenzierbar = Komponenten sind stetig differenzierbar, zB Vektor $v1$ mit (x,y,z) und ihren drei Ableitungen als Komponente des Vektorfeldes $V$\n",
        "\n",
        "* Das duale Konzept zu einem Vektorfeld ist eine Funktion, die jedem Punkt eine [Linearform](https://de.m.wikipedia.org/wiki/Linearform), [zB [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale)] zuordnet, eine solche Abbildung wird [pfaffsche Form](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) (One-Form) genannt. (Pfaffsche Formen sind die nat√ºrlichen Integranden f√ºr Wegintegrale.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HITrObGEbo2I"
      },
      "source": [
        "*Beispiele von Vektorfeldern*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilI3SG0Pb4xw"
      },
      "source": [
        "**Gradientenfeld**: Ist $f: \\Omega \\rightarrow \\mathbb{R}$ eine differenzierbare Funktion auf einer offenen Menge $\\Omega \\subset \\mathbb{R}^{n},$ so wird das Gradientenfeld grad $f: \\Omega \\rightarrow \\mathbb{R}^{n}$ von $f$ definiert durch die Zuordnung\n",
        "\n",
        ">$\n",
        "x \\mapsto \\operatorname{grad} f(x)=\\left(\\frac{\\partial f}{\\partial x_{1}}(x), \\ldots, \\frac{\\partial f}{\\partial x_{n}}(x)\\right)\n",
        "$\n",
        "\n",
        "Oft schreibt man es mit dem Nabla-Symbol: grad $f=\\nabla f$. **Ist ein Vektorfeld $v$ das Gradientenfeld einer Funktion $f$ (=Skalarfeld ist abgeleitet nach Ort), das hei√üt $v=\\nabla f,$ so bezeichnet man $f$ als Potential**. Man sagt auch $v$ besitzt ein Potential. Beispiele von Gradientenfeldern sind das von einer Punktquelle nach allen Seiten\n",
        "gleichm√§√üig flie√üende Feld einer Str√∂mung und das elektrische Feld um eine Punktladung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djFC2M4cKnO"
      },
      "source": [
        "**Zentralfelder**: Sei $I$ ein Intervall, welches die Null enth√§lt, und $K(I)=\\left\\{x \\in \\mathbb{R}^{n}:\\|x\\| \\in I\\right\\} \\subset \\mathbb{R}^{n}$ eine Kugelschale. Zentralfelder auf der Kugelschale sind definiert durch $v(x)=a(\\|x\\|) \\cdot x$ mit $a: I \\rightarrow \\mathbb{R}$. In $\\mathbb{R}^{3} \\backslash\\{0\\}$ ist das **Gravitationsfeld** $v(x)=-\\frac{x}{\\|x\\|^{3}}$ ein solches Zentralfeld.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWyWoFmfcM22"
      },
      "source": [
        "Weitere Beispiele sind im $\\mathbb{R}^{3}$ die mathematisch diffizileren sogenannten **\"Wirbelfelder\"**. Sie lassen sich als Rotation eines Vektorpotentials A beschreiben, nach der Formel $\\mathbf{v}(\\mathbf{r})=\\operatorname{rot} \\mathbf{A}(\\mathbf{s} . \\mathbf{u}) .$\n",
        "Pr√§gnantes Beispiel eines Wirbelfeldes ist das in Kreislinien um den\n",
        "Ausfluss einer , Badewanne\" herumwirbelnde Str√∂mungsfeld, oder das Magnetfeld um einen stromdurchflossenen Draht."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* siehe mehr unten unter \"Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus\""
      ],
      "metadata": {
        "id": "HvXCED_SNk6W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T3n7jhWXtHv"
      },
      "source": [
        "###### <font color=\"black\">*Gradientenfeld*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Gradientenfeld"
      ],
      "metadata": {
        "id": "7hmWkoIikHtz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6-0Gw75XzxE"
      },
      "source": [
        "* Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde**, bzw. ‚Äì k√ºrzer formuliert ‚Äì der Gradient des Skalarfelds (= **ein Skalarfeld, das nach dem Ort abgeleitet wird, ist ein Gradientenfeld**, mit Skalarpotenzial inkl. angegeben).\n",
        "\n",
        "* Zur besseren Abgrenzung zwischen dem Gradienten als mathematischem Operator und dem Resultat seiner Anwendung bezeichnen manche Autoren die Vektoren, aus denen sich Gradientenfelder zusammensetzen, auch als Gradientvektoren, andere dagegen mit Blick auf die Potentiale, aus denen sie sich herleiten, als Potentialvektoren.\n",
        "\n",
        "* Analog verwendet die √ºberwiegende Zahl der Autoren den Begriff Potentialfeld nicht f√ºr das skalare Feld des Potentials selbst, sondern das sich aus ihm ableitende Gradientenfeld\n",
        "\n",
        "> Ein Vektorfeld $\\vec{F}: \\vec{r} \\mapsto \\vec{F}(\\vec{r})$ hei√üt Gradientenfeld, wenn es ein Skalarfeld $\\Phi: \\vec{r} \\mapsto \\Phi(\\vec{r})$ gibt, so dass\n",
        "$\n",
        "\\vec{F}(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "*  Dabei nennt man $\\Phi$  das zu $F$ geh√∂rige **Skalarpotential** oder einfach kurz das ‚ÄûPotential‚Äú des Gradientenfelds\n",
        "${\\vec {F}}$. (*Der Begriff darf jedoch nicht mit dem physikalischen Begriff des ‚ÄûPotentials‚Äú verwechselt werden, mit dem die F√§higkeit eines konservativen Kraftfelds bezeichnet wird, einen dem Feld ausgesetzten K√∂rper eine Arbeit verrichten zu lassen.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a1CyVnAX8Xu"
      },
      "source": [
        "**Vektorfelder, die Gradienten eines Skalarfelds sind**, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als **konservative Vektorfelder** bezeichnet - ihnen allen gemeinsam sind dabei die folgenden drei einander √§quivalenten Eigenschaften:\n",
        "\n",
        "1. **Wegunabh√§ngigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve $S$ innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abh√§ngig, nicht dagegen von ihrer L√§nge.\n",
        "\n",
        "2. **Verschwinden des Ringintegrals f√ºr beliebige Randkurven** $S$ :\n",
        "$\n",
        "\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0\n",
        "$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit** des Feldes:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Potentiale*"
      ],
      "metadata": {
        "id": "rc8jFaF5d0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Aus Physik > Mechanik > Kraft > Gleichgewicht\n",
        "\n",
        "* Wieviel Kraft oder Moment kann auf ein System einwirken, bis dieses an Stabilitaet verliert?\n",
        "\n",
        "* Potenzial U = negative Arbeit einer konservativen Kraft (nur Beginn und Endpunkt z√§hlen)\n",
        "\n",
        "* Betrachte das Potenzial bzw. die Arbeit, aus der sich das Potenzial ergibt"
      ],
      "metadata": {
        "id": "IkLYcJeER1mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Potential_theory\n",
        "\n",
        "potential theory is the study of harmonic functions."
      ],
      "metadata": {
        "id": "MCUoN9j7Igw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Harmonic_function\n",
        "\n",
        "In der Analysis hei√üt eine reellwertige, zweimal stetig differenzierbare Funktion harmonisch, wenn die Anwendung des Laplace-Operators auf die Funktion null ergibt, die Funktion also eine L√∂sung der Laplace-Gleichung ist. Das Konzept der harmonischen Funktionen kann man auch auf Distributionen und Differentialformen √ºbertragen."
      ],
      "metadata": {
        "id": "jIJBSRewJJli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Potentiale, Potentialtheorie & Potentialstr√∂mung*"
      ],
      "metadata": {
        "id": "Ntt3OPDuYpRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See also: Fundamental Theorem of Calculus"
      ],
      "metadata": {
        "id": "8aIfNr6TW2X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potentiale (Conservative Vector Field)** - Die [Potentialtheorie](https://de.m.wikipedia.org/wiki/Potentialtheorie) oder die Theorie der wirbelfreien Vektorfelder behandelt die mathematisch-physikalischen Grundlagen konservativer (wirbelfreier) Kraftfelder."
      ],
      "metadata": {
        "id": "asYW2qDvknUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Potentialtheorie"
      ],
      "metadata": {
        "id": "W09k8H2gkkp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das Potential oder auch Potenzial (lat. potentia, ‚ÄûMacht, Kraft, Leistung‚Äú) ist in der Physik die F√§higkeit eines konservativen Kraftfeldes, eine Arbeit zu verrichten. Es beschreibt die Wirkung eines konservativen Feldes auf Massen oder Ladungen unabh√§ngig von deren Gr√∂√üe und Vorzeichen. Damit wird eine R√ºckwirkung des Probek√∂rpers zun√§chst ausgeschlossen, kann aber auch gesondert ber√ºcksichtigt werden. Als Formelzeichen f√ºr das Potential wird meist\n",
        "Œ¶\n",
        "\\Phi , der gro√üe griechische Buchstabe Phi, benutzt."
      ],
      "metadata": {
        "id": "rAoAKrZ4YjWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Potential_(Physik)"
      ],
      "metadata": {
        "id": "ZFOWx2hjYg6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Str√∂mung eines Fluids (Fl√ºssigkeit oder Gas) ist eine Potentialstr√∂mung, wenn das Vektorfeld der Geschwindigkeiten mathematisch so geartet ist, dass es ein Potential besitzt. Das Potential kann man sich anschaulich als die H√∂he in einer Reliefkarte vorstellen, wo dann die Richtung der gr√∂√üten Steigung in einem Punkt der dortigen Geschwindigkeit entspricht. Ein solches Potential ist in einem homogenen Fluid vorhanden, wenn die Str√∂mung rotationsfrei (wirbel- bzw. vortizit√§tsfrei) ist und keine Z√§higkeitskr√§fte (Reibungskr√§fte) auftreten oder diese vernachl√§ssigbar klein sind. Jede aus der Ruhe heraus beginnende Str√∂mung eines homogenen, viskosit√§tsfreien Fluids besitzt ein solches Potential."
      ],
      "metadata": {
        "id": "I_1QbpaRZQZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Potentialstr√∂mung"
      ],
      "metadata": {
        "id": "8vDVeCbmZMjW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUy8UbeIYqx1"
      },
      "source": [
        "###### <font color=\"black\">*Skalarpotential*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Das Skalarpotenzial ist ein Ma√ü f√ºr die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein K√∂rper entgegen der wirkenden Kraft bewegt, dann erh√∂ht sich seine potenzielle Energie.\n",
        "\n",
        "https://youtu.be/tb3KSeF0WrQ"
      ],
      "metadata": {
        "id": "87gT5L69aUhd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NleEXk9Z_pB"
      },
      "source": [
        "Das [Skalarpotential](https://de.wikipedia.org/wiki/Skalarpotential), oft einfach auch nur Potential genannt, ist in der Mathematik ein - im Unterschied zum Vektorpotential - skalares Feld $\\Phi(\\vec{r})$, dessen Gradient gem√§√ü folgender Formel\n",
        "\n",
        "> $\n",
        "\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "ein als \"Gradientenfeld\" genanntes Vektorfeld $\\vec{F}(\\vec{r})$ liefert.\n",
        "\n",
        "Kurz: **Die erste Ableitung des Skalarpotentials (= ein Skalarfeld) ergibt das Gradientenfeld (= ein spezielles Vektorfeld).**\n",
        "\n",
        "* Ist $\\vec{F}(\\vec{r})$ ein **konservatives** Kraftfeld, in dem die Kraft $\\vec{F}$ dem Prinzip des kleinsten Zwanges folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Definition\n",
        "$\n",
        "\\vec{F}(\\vec{r})=-\\operatorname{grad} \\Phi(\\vec{r})=-\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Skalarpotentiale bilden u. a. die mathematische Grundlage der Untersuchung konservativer Kraftfelder wie des elektrischen und des Gravitationsfelds, aber auch von wirbelfreien sogenannten Potentialstr√∂mungen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scez_eQlbpPg"
      },
      "source": [
        "**Ein Skalarfeld $\\Phi: \\vec{r} \\mapsto \\Phi(\\vec{r})$ ist genau dann ein Skalarpotential**, wenn es in einem einfach zusammenh√§ngenden Gebiet\n",
        "\n",
        "1. zweimal stetig differenzierbar ist, das hei√üt keine , Spr√ºnge\", Stufen oder andere\n",
        "Unstetigkeitsstellen enth√§lt;\n",
        "\n",
        "2. zu ihm ein Vektorfeld $\\vec{F}: \\vec{r} \\mapsto \\vec{F}(\\vec{r})$ existiert, so dass gilt:\n",
        "$\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})$\n",
        "\n",
        "$\\vec{F}$ wird daher oft auch das zugeh√∂rige Gradientenfeld genannt, das als Gradient des Skalarpotentials $\\Phi$ seinerseits stets folgende Bedingungen erf√ºllt:\n",
        "\n",
        "1. **Wegunabh√§ngigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve S innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abh√§ngig, nicht dagegen von ihrer L√§nge.\n",
        "\n",
        "2. **Verschwinden des geschlossenen Kurvenintegrals f√ºr beliebige Randkurven S**:\n",
        "$\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit des Feldes**:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msznOfm3bCGA"
      },
      "source": [
        "![cc](https://upload.wikimedia.org/wikipedia/commons/d/d9/GravityPotential.jpg)\n",
        "\n",
        "*Das Gravitationspotential einer homogenen Kugel*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuqLkrgCPdB1"
      },
      "source": [
        "*(Skalar-) Potential bei Kurvenintegralen in $\\mathbb{R}^{3}$*\n",
        "\n",
        "* Ein [Skalarpotential](https://de.wikipedia.org/wiki/Skalarpotential) ist eine reelle Funktion, die auf einem einfach zusammenh√§ngenden Gebiet zwei mal stetig differenzierbar ist.\n",
        "\n",
        "* **Wenn ein Vektorfeld so ein Potential besitzt, lassen sich viele Rechnungen mit z.B. Kurvenintegralen vereinfachen**. Man kann dieses Potential berechnen.\n",
        "\n",
        "Wenn es eine Funktion F : d -> $\\mathbb{R}$ gibt, mit (Nabla ist der Tangent der Funktion mit den drei Komponenten x,y,z):\n",
        "\n",
        "> $\\vec{\\nabla} F(x, y, z)=\\vec{v}(x, y, z)=\\left(\\begin{array}{l}v_{1}(x, y, z) \\\\ v_{2}(x, y, z) \\\\ v_{3}(x, y, z)\\end{array}\\right)$\n",
        "\n",
        "* Anmerkungen:\n",
        "\n",
        "  * leiten wir Nabla in die x-Richtung ab, bekommt man v1, fur y-Richtung v2 und fur z-Richtung v3\n",
        "\n",
        "  * ist wie die Stammfunktion, bezeichnet man hier aber als Potential\n",
        "\n",
        "> so gilt fur Kurve $\\gamma:[a, b] \\rightarrow R^{3}$,\n",
        "\n",
        "  * Kurvenintegral ist unabh√§ngig vom gewaehlten Weg, man kann verschiedene gehen. Wichtig ist nur von a nach b zu kommen.\n",
        "\n",
        "* Nabla berechnen und es kommt Vektorfeld als Ergebnis heraus. **Solch eine Funktion F heisst Stammfunktion bzw. Potenzial zum Vektorfeld $\\vec{v}$.**\n",
        "\n",
        "* Ein Vektorfeld $\\vec{v}$ das eine solche Stammfunktion hat, nennt man '**konservativ**' (dann kann man so ein Kointegral leicht ausrechnen)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrnWapTgOqWX"
      },
      "source": [
        "Wichtiger Satz: Auf einem **einfach zusammenh√§ngenden** Gebiet $G \\subseteq \\mathbb{R}^{3}$ (=keine L√∂cher, alle Kurven die geschlossen sind kann man immer kleiner zusammenziehen) hat ein stetig differenzierbares Vektorfeld $\\vec{v}$ : G -> ${R}^{3}$ genau dann ein Potential, wenn $\\frac{\\partial v_{j}}{\\partial x_{k}}=\\frac{\\partial v_{k}}{\\partial x_{j}} \\quad$ fur alle $j, k=1,2,3$.\n",
        "\n",
        "In drei Dimension, weil man das oft in Rotationen anwendet / ist aquivalent zu $\\operatorname{rot}(\\vec{v})=0$ (ich muss nur die Rotation ausrechnen, und weiss dann ob das Vektorfeld ein konservatives ist oder nicht, hat also eine Stammfunktion oder nicht)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQWLOqidYl8z"
      },
      "source": [
        "https://www.youtube.com/watch?v=KPRvYY9WXGg&list=PLBh2i93oe2quqhn4TZjMnBM2v7lG5QhDD&index=9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2de7qmNDYkPv"
      },
      "source": [
        "https://www.youtube.com/watch?v=aM4ktp8yO-s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2od0Mz9XkRX"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Skalarpotential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybqAMWlqJu7y"
      },
      "source": [
        "###### <font color=\"black\">*Vektorpotential*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2pH152ScafZ"
      },
      "source": [
        "**Ist ein Vektorfeld $v$ das Gradientenfeld einer Funktion $f,$ das hei√üt $v=\\nabla f$  (=Skalarfeld ist abgeleitet nach Ort), so bezeichnet man $f$ als Potential**.\n",
        "\n",
        "* Wirbelfelder, die Rotationen eines anderen Vektorfelds sind, sind stets quellenfrei ‚Äì quellenfreie Vektorfelder k√∂nnen daher umgekehrt immer auch als Rotation eines anderen Vektorfelds interpretiert werden, das man in diesem Fall als ‚ÄûVektorpotential‚Äú des betreffenden quellenfreien Vektorfelds bezeichnet\n",
        "\n",
        "* Mathematisch ist das Vektorpotential (im Unterschied zum Skalarpotential) ein Vektorfeld $\\mathbf{A}(\\mathbf{r}),$ dessen Rotation ein zweites Vektorfeld $\\mathbf{B}(\\mathbf{r})$ liefert gem√§√ü folgender Formel:\n",
        "\n",
        "> $\n",
        "\\mathbf{B}(\\mathbf{r}) \\stackrel{\\text { def }}{=} \\operatorname{rot} \\mathbf{A}(\\mathbf{r})=\\nabla \\times \\mathbf{A}(\\mathbf{r})\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeHrm5Yc8Hur"
      },
      "source": [
        "Vektorpotentiale lassen sich u. a. dazu verwenden, die zur Beschreibung des elektromagnetischen Felds verwendeten Maxwell-Gleichungen zu entkoppeln und dadurch leichter l√∂sbar zu machen.\n",
        "\n",
        "Obwohl es zun√§chst nur als mathematisches Hilfsmittel eingef√ºhrt wurde, kommt ihm in der Quantenmechanik physikalische Realit√§t zu, wie das [Aharonov-Bohm-Experiment](https://de.wikipedia.org/wiki/Aharonov-Bohm-Effekt) zeigte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63WNYqjy8X76"
      },
      "source": [
        "**Beziehungen zwischen Vektor- und Skalarpotential**: Gem√§√ü dem helmholtzschen Theorem kann (fast) jedes Vektorfeld $\\mathrm{K}(\\mathrm{r})$\n",
        "\n",
        "* als **Superposition zweier Komponenten $\\mathbf{F}(\\mathbf{r})$ und $\\mathbf{G}(\\mathbf{r})$ aufgefasst werden**,\n",
        "\n",
        "* deren erste der Gradient eines Skalarpotentials $\\Phi(\\mathbf{r})$ ist, die zweite dagegen die Rotation eines Vektorpotentials $\\mathbf{\\Gamma}(\\mathbf{r}):$\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "* Ist $\\mathbf{F}(\\mathbf{r})$ ein konservatives Kraftfeld, in dem die Kraft $\\mathbf{F}$ dem [Prinzip des kleinsten Zwanges](https://de.wikipedia.org/wiki/Prinzip_des_kleinsten_Zwanges) folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Schreibweise\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=-\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=-\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNahSbyncc2R"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Skalarpotential#Beziehungen_zwischen_Skalar-_und_Vektorpotential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh0bxMZQXmXO"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Vektorpotential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HEOBISTrpoC"
      },
      "source": [
        "###### <font color=\"blue\">*Differentialoperatoren*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vCM9-DX-j2"
      },
      "source": [
        "###### *Differentialoperator*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Differential_operator"
      ],
      "metadata": {
        "id": "eaKMapzgjGyh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKlWj90F3T4"
      },
      "source": [
        "Die drei kovarianten Differentialoperatoren (Covector-Maps!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyJV3RVZ5gc"
      },
      "source": [
        "**Folgende drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei r√§umlicher Drehung des urspr√ºnglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz **spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden**. Diese Eigenschaft folgt aus den **koordinatenunabh√§ngigen** Definitionen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpGcMJaKroS6"
      },
      "source": [
        "**Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.).\n",
        "\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "\n",
        "  * [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-XFwoBXDaqt"
      },
      "source": [
        "Interpretiert man das Vektorfeld als Str√∂mungsfeld einer Gr√∂√üe, f√ºr die die Kontinuit√§tsgleichung gilt, dann ist die [Divergenz](https://physik.cosmos-indirekt.de/Physik-Schule/Divergenz_eines_Vektorfeldes) die Quelldichte. Senken haben negative Divergenz. Ist die Divergenz √ºberall gleich null, so bezeichnet man das Feld als quellenfrei."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGSV9UrCvfQh"
      },
      "source": [
        "###### *Gradient (grad)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld.\n",
        "\n",
        "> $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n"
      ],
      "metadata": {
        "id": "LuXfeqBLrPgC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O844e1Y6yuPP"
      },
      "source": [
        "* Der Gradient als Operator verallgemeinert die bekannten Gradienten, die den Verlauf von physikalischen Gr√∂√üen beschreiben.\n",
        "\n",
        "* **Als Differentialoperator kann er beispielsweise auf ein Skalarfeld angewandt werden und wird in diesem Fall ein Vektorfeld liefern, das Gradientenfeld genannt wird.**\n",
        "\n",
        "* Der Gradient ist eine Verallgemeinerung der Ableitung in der mehrdimensionalen Analysis. Zur besseren Abgrenzung zwischen Operator und Resultat seiner Anwendung bezeichnet man solche Gradienten skalarer Feldgr√∂√üen in manchen Quellen auch als Gradientvektoren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMsKuyqyziD"
      },
      "source": [
        "*Gradientenfeld*\n",
        "\n",
        "Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld**, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde, bzw. ‚Äì k√ºrzer formuliert ‚Äì der Gradient des Skalarfelds."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient is a vector field and not a one-form. The associated one-form is a different object. It can only be defined with the help of a scalar product or some other isomorphism from the tangent space to it's dual. [Source](https://math.stackexchange.com/questions/1103667/how-to-visualize-the-gradient-as-a-one-form)\n",
        "\n",
        "* To clear up some confusion in the comments: when Carroll refers to the gradient of a function $f$ as a 1 -form he probably intends to refer to the exterior derivative $d f$ of $f$. This is a 1 -form containing all of the information which is contained in the gradient, but which can be defined in the absence of a (pseudo-)Riemannian metric.\n",
        "\n",
        "* In particular it contains precisely the data needed to differentiate $f$ with respect to any tangent vector; the result tells you how fast $f$ is changing in a particular direction at a particular point, which is what the gradient is supposed to do.\n",
        "\n",
        "* In the presence of a (pseudo-)Riemannian metric, one can identify 1 -forms and vector fields, and the vector field corresponding to $d f$ along this identification is what people usually call the gradient or gradient vector field of $f$. Unlike $d f$, it depends on a choice of (pseudo-)Riemannian metric. It's unusual and imprecise to refer to $d f$ itself as the gradient."
      ],
      "metadata": {
        "id": "VmSNGjk3uNKp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzwtAAJ2vik-"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Gradient_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4njkcQPvoYD"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R68NS5iUvjnn"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Partielle_Ableitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9e3prN3rXj-"
      },
      "source": [
        "###### *Divergence (div)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beispiel: elektrostatische Verbindung (von Ionenbindung): https://de.m.wikipedia.org/wiki/Elektrostatik"
      ],
      "metadata": {
        "id": "eGKblfp_cvMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen.\n",
        "\n",
        "> $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n"
      ],
      "metadata": {
        "id": "qWrlMbD_rC7g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwBaz9bh-qg"
      },
      "source": [
        "* Die [Divergenz eines Vektorfeldes](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes) ist ein Skalarfeld, das an jedem Punkt angibt, wie sehr die Vektoren in einer kleinen Umgebung des Punktes auseinanderstreben (lateinisch divergere).\n",
        "\n",
        "* Interpretiert man das Vektorfeld als Str√∂mungsfeld einer Gr√∂√üe, f√ºr die die Kontinuit√§tsgleichung gilt, dann ist die Divergenz die Quelldichte. Senken haben negative Divergenz. Ist die Divergenz √ºberall gleich null, so bezeichnet man das Feld als quellenfrei.\n",
        "\n",
        "* Man betrachtet zum Beispiel eine ruhige Wasseroberfl√§che, auf die ein d√ºnner Strahl √ñl trifft. Die Bewegung des √ñls auf der Oberfl√§che kann durch ein zweidimensionales (zeitabh√§ngiges) Vektorfeld beschrieben werden: An jedem Punkt ist zu jedem beliebigen Zeitpunkt die Flie√ügeschwindigkeit des √ñls in Form eines Vektors gegeben. Die Stelle, an der der Strahl auf die Wasseroberfl√§che trifft, ist eine ‚Äû√ñlquelle‚Äú, da von dort √ñl wegflie√üt, ohne dass es einen Zufluss auf der Oberfl√§che geben w√ºrde. Die Divergenz an dieser Stelle ist positiv. Im Gegensatz dazu bezeichnet man eine Stelle, an der das √ñl beispielsweise am Rand aus dem Wasserbecken abflie√üt, als Senke. Die Divergenz an dieser Stelle ist negativ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2zTmFCuVE-u"
      },
      "source": [
        "###### *Rotation (curl)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Curl = Circulation Density of a Vector Field (it's a velocity field)\n",
        "\n",
        "Video: https://youtu.be/gWbfdPUynz0"
      ],
      "metadata": {
        "id": "yoXLGemvkgto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren.\n",
        "\n",
        "> $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$"
      ],
      "metadata": {
        "id": "oy_Z5HhlrJwU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP_Y-_K6qxVV"
      },
      "source": [
        "* Als [Rotation oder Rotor](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes) bezeichnet man in der Vektoranalysis, einem Teilgebiet der Mathematik, einen bestimmten Differentialoperator, der einem Vektorfeld im dreidimensionalen euklidischen Raum mit Hilfe der Differentiation ein neues Vektorfeld zuordnet.\n",
        "\n",
        "* Die Rotation eines [Str√∂mungsfeldes](https://de.wikipedia.org/wiki/Str√∂mungsfeld) gibt f√ºr jeden Ort das Doppelte der Winkelgeschwindigkeit an, mit der sich ein mitschwimmender K√∂rper dreht (‚Äûrotiert‚Äú). Dieser Zusammenhang ist namensgebend.\n",
        "\n",
        "* Es muss sich aber nicht immer um ein Geschwindigkeitsfeld und eine Drehbewegung handeln; beispielsweise betrifft das Induktionsgesetz die Rotation des elektrischen Feldes.\n",
        "\n",
        "* [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "* Siehe auch [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation#Drehung_(Rotation)) sowie [Drehmatrix](https://de.wikipedia.org/wiki/Drehmatrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Depiction of a two-dimensional vector field with a uniform curl.*:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Uniform_curl.svg/480px-Uniform_curl.svg.png)"
      ],
      "metadata": {
        "id": "e4uCFpK0krEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Curl_(mathematics)"
      ],
      "metadata": {
        "id": "5VpZxXE_knUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curl in the 2-dimensional vector field:**"
      ],
      "metadata": {
        "id": "hY3qwmF-Dk-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we saw in Line Integrals that you can calculate the rotatioon along  apth in a vector field: $\\oint_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "* but in this case I want to calculate it for a specific point\n",
        "\n",
        "* what I do is I take a tiny rectangle and run its limit: you take the line integral along the entire curve (rectangle) by breaking it uop and taking the line integral for each of the four sides\n",
        "\n",
        "* look at formular below on the bottom. You will still take a few re\b-arrangements to get partial derivatives and you get:\n",
        "\n",
        "> $\\oint_{C} \\vec{F} \\cdot d \\vec{r}=\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right) \\Delta x \\Delta y$"
      ],
      "metadata": {
        "id": "58MdsKwOmRF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_032.PNG)"
      ],
      "metadata": {
        "id": "gB6JX-cmoH0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the formula you get and see what its component mean:"
      ],
      "metadata": {
        "id": "oLLccG-gpJJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_033.PNG)"
      ],
      "metadata": {
        "id": "jdqIYdoko9qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curl in the 3-dimensional vector field:**"
      ],
      "metadata": {
        "id": "jqBnGzKQDfWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_083.jpg)"
      ],
      "metadata": {
        "id": "xmujVR0YFrH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_084.jpg)"
      ],
      "metadata": {
        "id": "EIZpTpWHFti9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_085.jpg)"
      ],
      "metadata": {
        "id": "ixIfKR2rFvR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I turn this spinner around this way then it's pointing in the $\\hat{i}$ direction, so it represents the $M \\hat{i}$ component:"
      ],
      "metadata": {
        "id": "XZazTyeIF6Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_086.jpg)"
      ],
      "metadata": {
        "id": "1PPFDmXjGo-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which means if I put all three directions together, I get the 3 dimensional curl (geometric intepretation):"
      ],
      "metadata": {
        "id": "An4PtAnUGrW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_087.jpg)"
      ],
      "metadata": {
        "id": "gkuuasJgGxj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look what the del Operator is doing:\n",
        "\n",
        "$\\nabla=\\frac{\\partial}{\\partial x} \\hat{\\imath}+\\frac{\\partial}{\\partial y} \\hat{\\jmath}+\\frac{\\partial}{\\partial z} \\hat{i}$\n",
        "\n",
        "We saw this earlier here when using in the gradient:\n",
        "\n",
        "$\\nabla f=\\frac{\\partial f}{\\partial x} \\hat{\\imath}+\\frac{\\partial f}{\\partial y} \\hat{\\jmath}+\\frac{\\partial f}{\\partial z} \\hat{k}$\n",
        "\n",
        "Here we do something different, we compute the cross product of the del Operator with the vector field F (the result is a determinant):\n",
        "\n",
        "$\\nabla \\times \\vec{F}=\\left|\\begin{array}{ccc}\\hat{\\imath} & \\hat{\\jmath} & \\hat{k} \\\\ \\frac{\\partial}{\\partial x} & \\frac{\\partial}{\\partial y} & \\frac{\\partial}{\\partial z} \\\\ M & N & P\\end{array}\\right|$\n",
        "\n",
        "$\\left[\\frac{\\partial P}{\\partial y}-\\frac{\\partial N}{\\partial z}\\right] \\hat{\\imath}+\\left[\\frac{\\partial M}{\\partial z}-\\frac{\\partial P}{\\partial x}\\right] \\hat{\\jmath}+\\left[\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right] \\hat{k}$\n",
        "\n",
        "$=\\operatorname{curl} \\vec{F}$\n",
        "\n",
        "This is our curl definition from previously!"
      ],
      "metadata": {
        "id": "q4Q-Wg3oH6_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And what the curl of the gradient of f?**"
      ],
      "metadata": {
        "id": "zj2drKCAItEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\nabla \\times \\nabla f=\\left|\\begin{array}{ccc}\\hat{\\imath} & \\hat{\\jmath} & \\hat{k} \\\\ \\frac{\\partial}{\\partial x} & \\frac{\\partial}{\\partial y} & \\frac{\\partial}{\\partial z} \\\\ f_{x} & f_{y} & f_{z}\\end{array}\\right|$\n",
        "\n",
        "$=\\left[f_{z y}-f_{y z}\\right] \\hat{\\imath}+\\left[f_{z x}-f_{x z}\\right] \\hat{\\jmath}+\\left[f_{y z}-f_{z y}\\right] \\hat{k}$\n",
        "\n",
        "$=\\overrightarrow{0}$ if $f$ has continuous\n",
        "mixed $2^{\\text {nd }}$ derivatives"
      ],
      "metadata": {
        "id": "zEfBR3ilJQKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I can use this to test for conservativesness:\n",
        "\n",
        "Recall:\n",
        "$\\vec{F}$ is conservative $\\Leftrightarrow \\vec{F}=\\nabla f$\n",
        "\n",
        "$\n",
        "\\Rightarrow \\nabla \\times \\vec{F}=\\overrightarrow{0}\n",
        "$\n",
        "\n",
        "This means that gravitational or electric fields have no curl!\n",
        "\n",
        "$\\Rightarrow\\left[\\frac{\\partial P}{\\partial y}-\\frac{\\partial N}{\\partial z}\\right] i+\\left[\\frac{\\partial M}{\\partial z}-\\frac{\\partial P}{\\partial x}\\right] \\hat +\\left[\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right] \\hat{k}=\\overrightarrow{0}$\n",
        "\n",
        "Test for conservative:\n",
        "\n",
        "$\n",
        "\\frac{\\partial P}{\\partial y}=\\frac{\\partial N}{\\partial z}, \\quad \\frac{\\partial M}{\\partial z}=\\frac{\\partial P}{\\partial x}, \\quad \\frac{\\partial N}{\\partial x}=\\frac{\\partial M}{\\partial y}\n",
        "$"
      ],
      "metadata": {
        "id": "VWCpCUWQKD2d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Zw_t0Ptro-"
      },
      "source": [
        "###### *Nabla-Operator (Del operator)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Del"
      ],
      "metadata": {
        "id": "7f7adio6i_sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differential operator del, also called nabla, is an important vector differential operator. It appears frequently in physics in places like the differential form of Maxwell's equations. In three-dimensional Cartesian coordinates, del is defined as\n",
        "\n",
        "> $\\nabla=\\hat{\\mathbf{x}} \\frac{\\partial}{\\partial x}+\\hat{\\mathbf{y}} \\frac{\\partial}{\\partial y}+\\hat{\\mathbf{z}} \\frac{\\partial}{\\partial z}$\n",
        "\n",
        "Del defines the gradient, and is used to calculate the curl, divergence, and Laplacian of various objects."
      ],
      "metadata": {
        "id": "ARBZopZZd73L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHqNsnOYsVdm"
      },
      "source": [
        "*Koordinatenunabh√§ngige Definition mit dem Nabla-Operator*\n",
        "\n",
        "* Der [Nabla-Operator](https://de.wikipedia.org/wiki/Nabla-Operator) ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, um kontextabh√§ngig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren.\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der Nabla-Operator ist auch in anderen Koordinatensystemen definiert und so kann mit ihm zum Beispiel die Rotation [koordinatenunabh√§ngig](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes#Koordinatenunabh√§ngige_Definition_mit_dem_Nabla-Operator) durch\n",
        "\n",
        "> $\\operatorname{rot} \\vec{F}:=\\nabla \\times \\vec{F}$\n",
        "\n",
        "definiert werden. Mit dem Nabla-Operator k√∂nnen auch der Gradient- sowie die Divergenz eines Vektorfeldes dargestellt und Produktregeln hergeleitet werden.\n",
        "\n",
        "Formal ist der Nabla-Operator ein Vektor, dessen Komponenten die partiellen\n",
        "Ableitungsoperatoren $\\frac{\\partial}{\\partial x_{i}}$ sind:\n",
        "\n",
        "> $\n",
        "\\vec{\\nabla}=\\left(\\frac{\\partial}{\\partial x_{1}}, \\ldots, \\frac{\\partial}{\\partial x_{n}}\\right)\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Laplace Operator*"
      ],
      "metadata": {
        "id": "eDHpbm7feTwJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSvDHxVmgzEU"
      },
      "source": [
        "Der [**Laplace-Operator**](https://de.wikipedia.org/wiki/Laplace-Operator) ist ein linearer Differentialoperator innerhalb der [mehrdimensionalen Analysis](https://de.wikipedia.org/wiki/Analysis#Mehrdimensionale_reelle_Analysis) ($\\Delta \\colon D(\\Delta )\\to L^{2}(\\mathbb{R} ^{n})$ und ein unbeschr√§nkter Operator). Der Laplace-Operator kommt in vielen Differentialgleichungen vor, die das Verhalten physikalischer Felder beschreiben. Beispiele sind die Poisson-Gleichung der Elektrostatik, die **Navier-Stokes-Gleichungen** f√ºr Str√∂mungen von Fl√ºssigkeiten oder Gasen und die Diffusionsgleichung f√ºr die W√§rmeleitung.\n",
        "\n",
        "Der Laplace-Operator ordnet einem zweimal differenzierbaren Skalarfeld $f$ **die Divergenz seines Gradienten zu**,\n",
        "\n",
        ">$\n",
        "\\Delta f=\\operatorname{div}(\\operatorname{grad} f)\n",
        "$\n",
        "\n",
        "oder mit dem Nabla-Operator notiert\n",
        "\n",
        ">$\n",
        "\\Delta f=\\nabla \\cdot(\\nabla f)=(\\nabla \\cdot \\nabla) f=\\nabla^{2} f\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian in the Heat Equation**\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ],
      "metadata": {
        "id": "1A83bXJDwHDF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwBG9AzZb-C"
      },
      "source": [
        "[**Kovariant**](https://de.wikipedia.org/wiki/Kovarianz_(Physik)) nennt man ein Transformationsverhalten, bei dem sich die Basisvektoren und die darin dargestellten Vektoren (Gr√∂√üen) in gleicher Weise transformieren. **Kontravariant** nennt man ein Transformationsverhalten, wenn sich die Basisvektoren und die darin dargestellten Vektoren (Gr√∂√üen) in unterschiedlicher Weise transformieren.\n",
        "\n",
        "Das kovariante Transformationsverhalten garantiert die Formerhaltung von Gleichungen beim Wechsel des Bezugsystems (Koordinatensystems) bzw. bei Gruppentransformationen. Diese Aussagen gelten auch f√ºr die tensorielle Schreibweise.\n",
        "\n",
        "Zum Beispiel: Unter Galilei-Transformationen transformieren sich die Beschleunigung und die Kraft in den newtonschen Bewegungsgleichungen im gleichen Sinne wie die Ortsvektoren. Daher sind die Newtonschen Bewegungsgleichungen und damit die klassische Mechanik kovariant bzgl. der Gruppe der Galilei-Transformationen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcm5d0r2fDD0"
      },
      "source": [
        "###### <font color=\"gray\">*Fundamentalzerlegung (Fundamentalsatz der Vektoranalysis)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9TPhq5fEux"
      },
      "source": [
        "* der [Helmholtzscher Zerlegungssatz](https://de.m.wikipedia.org/wiki/Helmholtz-Theorem) ist der Fundamentalsatz der Vektoranalysis. Beschreibt den allgemeinen Fall.\n",
        "\n",
        "* Jedes Vektorfeld $\\vec{F}$ l√§sst sich als eine √úberlagerung eines Quellenanteils $\\vec{F}_{Q}$ und eines Wirbelanteils $\\vec{F}_{W}$ beschreiben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLvbjOH0rbDs"
      },
      "source": [
        "###### <font color=\"gray\">*Koordinatentransformationen und Funktionaldeterminante*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujcsbmyfEJDt"
      },
      "source": [
        "**Koordinatentransformation**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Differentialgeometrie#Koordinatentransformationen\n",
        "\n",
        "* Fur die Berechnung des Fl√§chen- oder Volumenintegrals eine geeignete Substitutionsfunktion zu finden ist nicht trivial. Sie transformiert das Volumenintegral oft von einem Koordinatensystem in ein anderes, um die Berechnung zu vereinfachen oder √ºberhaupt zu erm√∂glichen.\n",
        "\n",
        "* Bei der Integration √ºber geometrische Objekte ist es sogar oft unpraktisch, √ºber kartesische Koordinaten zu integrieren. So l√§sst sich in der Physik das Integral √ºber ein radialsymmetrisches Potentialfeld, dessen Wert nur von einem Radius r abh√§ngt, wesentlich leichter in Kugelkoordinaten berechnen. Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$  an.\n",
        "\n",
        "* Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$ an. Nach dem [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) gilt dann in diesem Beispiel:\n",
        "\n",
        "> $\n",
        "\\int_{\\Omega} U(\\vec{r}) d V=\\int_{\\Phi^{-1}(\\Omega)} U(\\Phi(r, \\theta, \\varphi)) \\cdot|\\operatorname{det} D \\Phi(r, \\theta, \\varphi)| \\mathrm{d} r \\mathrm{~d} \\theta \\mathrm{d} \\varphi\n",
        "$\n",
        "\n",
        "* Der vektorielle Faktor ist das [Spatprodukt](https://de.wikipedia.org/wiki/Spatprodukt) aller partiellen Ableitungen von $\\vec{\\xi}(u, v, w)$\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}\n",
        "$\n",
        "\n",
        "Generell lassen sich Spatprodukte auch als Determinanten schreiben, so gilt hier:\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}=\\operatorname{det}\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\frac{\\partial \\vec{\\xi}}{\\partial v} \\frac{\\partial \\vec{\\xi}}{\\partial w}\\right)=\\operatorname{det}\\left(J_{\\vec{\\xi}}\\right)\n",
        "$\n",
        "\n",
        "Die aneinandergereihten partiellen Gradienten $(\\vec{\\xi}$ ist eine vektorwertige Funktion)\n",
        "formen gerade die Elemente der $3 \\times 3$ Jacobi-Matrix. Die zugeh√∂rige Jacobi-Determinante, auch als [**Funktionaldeterminante**](https://de.wikipedia.org/wiki/Funktionaldeterminante) bezeichnet, berechnet genau den\n",
        "zus√§tzlichen Faktor f√ºr eine Koordinatentransformation.\n",
        "\n",
        "Ist das Volumenelement skalar, reduziert sich der Faktor auf dessen euklidische Norm $\\|\\vec{N}\\| .$ Nachdem das Volumenintegral parametrisiert ist, kann mit Hilfe des [Satzes von Fubini](https://de.wikipedia.org/wiki/Satz_von_Fubini) das Integral Schritt f√ºr Schritt berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrJh93jH1D-"
      },
      "source": [
        "**Transformationssatz**\n",
        "\n",
        "* Der [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) (auch Transformationsformel) beschreibt in der Analysis das Verhalten von Integralen unter Koordinatentransformationen. Er ist somit die Verallgemeinerung der Integration durch Substitution auf Funktionen h√∂herer Dimensionen.\n",
        "\n",
        "* Der Transformationssatz wird als Hilfsmittel bei der Berechnung von Integralen verwendet, wenn sich das Integral nach √úberf√ºhrung in ein anderes Koordinatensystem leichter berechnen l√§sst.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{R}^{d}$ eine offene Menge und $\\Phi: \\Omega \\rightarrow \\Phi(\\Omega) \\subseteq \\mathbb{R}^{d}$ ein [Diffeomorphismus](https://de.wikipedia.org/wiki/Diffeomorphismus) (=eine bijektive, stetig differenzierbare Abbildung, deren Umkehrabbildung auch stetig differenzierbar ist). Dann ist die Funktion $f$ auf $\\Phi(\\Omega)$ genau dann integrierbar, wenn die Funktion $x \\mapsto f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))|$ auf $\\Omega$ integrierbar ist. In diesem Fall gilt:\n",
        "\n",
        "> $\n",
        "\\int_{\\Phi(\\Omega)} f(y) \\mathrm{d} y=\\int_{\\Omega} f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))| \\mathrm{d} x\n",
        "$\n",
        "\n",
        "* Dabei ist $D \\Phi(x)$ die Jacobi-Matrix und $\\operatorname{det}(D \\Phi(x))$ die Funktionaldeterminante von $\\Phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUDAAXziE4Pj"
      },
      "source": [
        "**Funktionaldeterminante**\n",
        "\n",
        "* Die [Funktionaldeterminante oder Jacobi-Determinante](https://de.wikipedia.org/wiki/Funktionaldeterminante) ist eine mathematische Gr√∂√üe, die in der mehrdimensionalen Integralrechnung, also der Berechnung von **Oberfl√§chen- und Volumenintegralen**, eine Rolle spielt. Insbesondere findet sie in der [Fl√§chenformel](https://de.wikipedia.org/wiki/Fl√§chenformel) und dem aus dieser hervorgehenden Transformationssatz Verwendung.\n",
        "\n",
        "* [Exkurs Determinante](https://de.wikipedia.org/wiki/Determinante): *die Determinante eine Zahl (ein Skalar), die einer quadratischen Matrix zugeordnet wird und aus ihren Eintr√§gen berechnet werden kann. Sie gibt an, wie sich das **Volumen (oder der Fl√§cheninhalt)** bei der durch die Matrix beschriebenen linearen Abbildung √§ndert*\n",
        "\n",
        "* **Lokales Verhalten einer Funktion**: Die Funktionaldeterminante gibt zu einem gegebenen Punkt wichtige Informationen √ºber das Verhalten der Funktion $f$ in der N√§he dieses Punktes.\n",
        "\n",
        "  * Wenn beispielsweise die Funktionaldeterminante einer stetig differenzierbaren Funktion in einem Punkt $p$ ungleich null ist, so ist die Funktion in einer Umgebung von $p$ invertierbar.\n",
        "\n",
        "  * Weiterhin gilt, dass bei positiver Determinante in $p$ die Funktion ihre Orientierung beibeh√§lt und bei negativer Funktionaldeterminante die Orientierung umkehrt.\n",
        "\n",
        "  * Der absolute Wert der Determinante im Punkt $p$ gibt den Wert an, mit dem die Funktion in der N√§he von $p$ expandiert oder schrumpft.\n",
        "\n",
        "* F√ºr eine differenzierbare Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ ist die Funktionaldeterminante definiert als die Determinante der Jacobi-Matrix von $f,$ also als\n",
        "det $D f(x)$\n",
        "mit\n",
        "\n",
        "> $\n",
        "D f(x)=\\left(\\frac{\\partial f_{i}}{\\partial x_{j}}(x)\\right)_{i, j=1, \\ldots, n}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Aix1sXHVnV"
      },
      "source": [
        "**Beispiel: Polarkoordinaten**\n",
        "\n",
        "**1. Koordinatentransformation**: Die Umrechnungsformeln von Polarkoordinaten in kartesische Koordinaten lauten:\n",
        "\n",
        "> $\n",
        "\\begin{array}{l}\n",
        "x=r \\cos \\varphi \\\\\n",
        "y=r \\sin \\varphi\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**2. Funktionaldeterminante** lautet also:\n",
        "\n",
        "> $\n",
        "\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}=\\operatorname{det}\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial \\varphi} \\\\\n",
        "\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial \\varphi}\n",
        "\\end{array}\\right)=\\operatorname{det}\\left(\\begin{array}{cc}\n",
        "\\cos \\varphi & -r \\sin \\varphi \\\\\n",
        "\\sin \\varphi & r \\cos \\varphi\n",
        "\\end{array}\\right)=r \\cdot(\\cos \\varphi)^{2}+r \\cdot(\\sin \\varphi)^{2}=r\n",
        "$\n",
        "\n",
        "**3. Fl√§chen- oder Volumenintegral**: Folglich ergibt sich f√ºr das Fl√§chenelement $\\mathrm{d} A$ (alternativ kann man bei dreidimensionalen Kugelkoordinaten an dieser Stelle auch das Volumenelement $\\mathrm {d} V$ mit der Funktionaldeterminante berechnen):\n",
        "\n",
        "> $\n",
        "\\mathrm{d} A=\\left|\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}\\right| \\mathrm{d} r \\mathrm{~d} \\varphi=r \\mathrm{~d} r \\mathrm{~d} \\varphi\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Flow (Circulation) & Flux*"
      ],
      "metadata": {
        "id": "6Nb2j8QPdg3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flow (special case: Circulation)*"
      ],
      "metadata": {
        "id": "DKh7A9IGTuCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{F}$ - Vectorfield\n",
        "\n",
        "$\\vec{T}$ - Curve or Path\n",
        "\n",
        "$ds$ - Weg√§nderung / Wegl√§nge"
      ],
      "metadata": {
        "id": "yifJa830fn79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_001.png)"
      ],
      "metadata": {
        "id": "NKS13DPE-neM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Flow - the degree to which my path is aligned with the vectorfield = Tangential to curve\n"
      ],
      "metadata": {
        "id": "oqjxWkjy_Ve5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* you will at inner product of tangent (derivative) of my path with each vector from the vectorfield\n",
        "\n",
        "* **Flow Integral is equal to Line Integral**\n",
        "\n",
        "For velocity field $\\vec{F}=M(x, y) \\hat{\\imath}+N(x, y)$ and smooth curve $C$ parametrized by $\\vec{r}(t)$\n",
        "\n",
        "> $\n",
        "F l o w=\\int_{C} \\vec{F} \\cdot \\vec{T} d s\n",
        "$\n",
        "\n",
        "* $\\vec{T}$ -  tangential vector to the curve = I want to add up the contributions of the field in that tangential direction"
      ],
      "metadata": {
        "id": "fm5eCmBJT3wY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_002.png)"
      ],
      "metadata": {
        "id": "gsqKtwoz_TKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Flow Integrals and Circulation // Big Idea, Formula & Examples // Vector Calculus](https://www.youtube.com/watch?v=iXQpDrWrizc&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=11)"
      ],
      "metadata": {
        "id": "Y7OZ8uMjTx8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a parametrized version of this fornula for the use case below:\n",
        "\n",
        "> $=\\int_a^b \\vec{F}(\\vec{r}(t)) \\cdot \\frac{d \\vec{r}}{d t} dt$\n",
        "\n",
        "> $=\\int_a^b M dx + \\int_a^b N dx$\n",
        "\n",
        "* dx = g'(t) dt\n",
        "\n",
        "* dy = h'(t) dt"
      ],
      "metadata": {
        "id": "tpUNF6B9V1OT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the curve is closed, it's called a Circulaiton and has the sign\n",
        "\n",
        "> Circulation $=\\oint_{C} \\vec{F} \\cdot \\vec{T} d s$"
      ],
      "metadata": {
        "id": "eWnH4IntW_pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_001.png)"
      ],
      "metadata": {
        "id": "dDPj985nVbci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example - Let's check left side:**\n",
        "\n",
        "> $\\vec{F}(x, y)=x \\hat{\\imath}+y \\hat{\\jmath}$\n",
        "\n",
        "> $\\vec{r}(t)=\\cos t \\hat{\\imath}+\\sin t \\hat{\\jmath}$ $\\quad$ $t \\in[0,2 \\pi]$\n",
        "\n",
        "\n",
        "* $\\vec{r}(t)$ is the parametrization with $t \\in$ being the limits\n",
        "\n",
        "$\\begin{aligned} F l o w &=\\int_{a}^{b} M d x+\\int_{a}^{b} N d y \\\\ &=\\int_0^{2\\pi} \\cos t(-\\sin t) d t \\\\ &=\\int_0^{2\\pi} \\sin t(\\cos t)dt  \\\\ &= 0\\end{aligned}$\n",
        "\n",
        "(the flow is 0)"
      ],
      "metadata": {
        "id": "MLdUK7dBXn29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example - Let's check right side:**\n",
        "\n",
        "> $\\vec{F}(x, y)=-y \\hat{\\imath}+x \\hat{\\jmath}$\n",
        "\n",
        "> $\\vec{r}(t)=\\cos t \\hat{\\imath}+\\sin t \\hat{\\jmath}$ $\\quad$ $t \\in[0,2 \\pi]$\n",
        "\n",
        "\n",
        "* $\\vec{r}(t)$ is the parametrization with $t \\in$ being the limits\n",
        "\n",
        "$\\begin{aligned} F l o w &=\\int_{a}^{b} M d x+\\int_{a}^{b} N d y \\\\ &=\\int_0^{2\\pi} -\\sin t(-\\sin t) d t \\\\ &=\\int_0^{2\\pi} \\cos t(\\cos t)dt  \\\\ &= 2 \\pi \\end{aligned}$\n",
        "\n",
        "(the flow is $2\\pi$)"
      ],
      "metadata": {
        "id": "xGN7vOn-9r8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flux*"
      ],
      "metadata": {
        "id": "_3o5ynDavqaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Flux Integrals // Big Idea, Formula & Examples // Vector Calculus](https://www.youtube.com/watch?v=sX9s0JiWATM&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=12)"
      ],
      "metadata": {
        "id": "ol6Z1UCMDIBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Flow - the degree to which my path is normal to the vectorfield = Normal to curve\n"
      ],
      "metadata": {
        "id": "5WhKJcP6_eoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For continuous field $\\vec{F}=M(x, y) \\hat{\\imath}+N(x, y) \\hat{\\jmath}$, smooth, simple, closed curve C\n",
        "and $\\vec{n}$ the outward normal\n",
        "\n",
        ">$\n",
        "F l u x=\\oint_{C} \\vec{F} \\cdot \\vec{n} \\, d s\n",
        "$\n",
        "\n",
        "* smooth means you can take the first derivative at any point on the curve\n",
        "\n",
        "* simple means that the curve doesn't overlap with itself (not intersections because otherwise inward ad outward looses its meaning)"
      ],
      "metadata": {
        "id": "XtS6mbfBAMIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_003.png)"
      ],
      "metadata": {
        "id": "GhBa1zhu_rJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the normal vector:\n",
        "\n",
        "* BTW: there are actually 2 different normals on the plane, one outward and one upward\n",
        "\n",
        "* how do you know what $\\vec{n}$ is given a parametrization\n",
        "\n",
        "* if I know one vector (the tangential one) I can take the **cross product** with the upward normal to get the outward normal\n",
        "\n",
        "* Achtung: the direction is anticlockwise (right hand rule). If the curve is clockwise, it needs to be minus"
      ],
      "metadata": {
        "id": "nC9G2_ZZBPTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_004.png)"
      ],
      "metadata": {
        "id": "DI4hsEzoCXWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example how to compute the normal $\\vec{n}$ and get the Flux formular. This the differential form of computing a Flux integral:"
      ],
      "metadata": {
        "id": "dMR1iFhoDV1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_005.png)"
      ],
      "metadata": {
        "id": "eW8DOuG5Dbo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling that in for the two curves in the two different vector fields:"
      ],
      "metadata": {
        "id": "Yf05v2e8Etcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_006.png)"
      ],
      "metadata": {
        "id": "eUqyR4xaExWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following: place positive cos(t) instead of -cos(t) as your N. I am confused because how are you getting a positive cosine from a negative force in the \"j\" direction. Nonetheless, based on your answer, placing a negative in front of the cosine will grant the same answer, however, it won't need to integrate sin(2t), but instead 0."
      ],
      "metadata": {
        "id": "pL9LFrR-E8XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_007.png)"
      ],
      "metadata": {
        "id": "MA8LC8ADEyao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flow and Flux (Summary)*"
      ],
      "metadata": {
        "id": "5BSzjyZFFBdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Fields can represent different things, like force fields. Here we consider velcoity fields, like in some turbulent water or wind system. The vector tells you magnitude and direction water or wind is flowing at that particular point."
      ],
      "metadata": {
        "id": "fIhFQh9EFNWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_008.png)"
      ],
      "metadata": {
        "id": "L133duiiEzn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sites.und.edu/timothy.prescott/apex/web/apex.Ch15.S4.html"
      ],
      "metadata": {
        "id": "gTGeWVW7v3hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\vec{F}=\\langle M, N\\rangle$ be a vector field with continuous components defined on a smooth curve $C$, parameterized by $\\vec{r}(t)=\\langle f(t), g(t)\\rangle$, let $\\vec{T}$ be the unit tangent vector of $\\vec{r}(t)$, and let $\\vec{n}$ be the clockwise $90^{\\circ}$ degree rotation of $\\vec{T}$.\n",
        "- The flow of $\\vec{F}$ along $C$ is\n",
        "\n",
        "> $\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{T} \\mathrm{~d} s=\\int_{C} \\vec{F} \\cdot \\overrightarrow{d r} .\n",
        "$\n",
        "\n",
        "- The flux of $\\vec{F}$ across $C$ is\n",
        "\n",
        ">$\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{n} \\mathrm{~d} s=\\int_{C} M \\mathrm{~d} y-N \\mathrm{~d} x=\\int_{C}\\left(M g^{\\prime}(t)-N f^{\\prime}(t)\\right) d\n",
        "$"
      ],
      "metadata": {
        "id": "zBJu_o0zwhge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Flux"
      ],
      "metadata": {
        "id": "bOYtbL4ow5sM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolnoE19IyYG"
      },
      "source": [
        "###### <font color=\"blue\">*Kurvenintegral (Line Integral)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Line Integrals (Kurvenintegral) - Introduction*"
      ],
      "metadata": {
        "id": "Gy3Md6zcdZRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [What is a LINE INTEGRAL? // Big Idea, Derivation & Formula](https://www.youtube.com/watch?v=WA5_a3C2iqY&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=3)"
      ],
      "metadata": {
        "id": "Bu2F3cxGQ7gS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_011.png)"
      ],
      "metadata": {
        "id": "8olsjAk4P8p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_012.png)"
      ],
      "metadata": {
        "id": "5lDXKzyTQIf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_013.png)"
      ],
      "metadata": {
        "id": "cCAnNZU_QJWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_014.png)"
      ],
      "metadata": {
        "id": "C1UC39mBQK-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now what is the area of this rectangle? How can we calculate that?**\n",
        "\n",
        "* the height is just the function value at whatever particular point you are at $f\\left(x_{k}, y_{k}\\right)$\n",
        "\n",
        "* what is the size of the base? It's $\\Delta s_k$, which is the Arc length"
      ],
      "metadata": {
        "id": "remNbl5LQNZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_015.png)"
      ],
      "metadata": {
        "id": "u8wvrRvMRHe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard definition of the line integral**"
      ],
      "metadata": {
        "id": "2KkccTMARuNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $\\Delta A_{k}=f\\left(x_{k}, y_{k}\\right) \\Delta s_{k}$\n",
        "\n",
        "wird zu:\n",
        "\n",
        "> $\\int_{C} f(x, y) d s$\n",
        "\n",
        "> $= \\lim _{n \\rightarrow \\infty} \\sum_{k=1}^{\\infty} f(x_{k}, y_{k}) \\Delta s_{k}$\n",
        "\n",
        "(standard Riemann integral definition)"
      ],
      "metadata": {
        "id": "xTBJnZ2Z3NbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you get a quick calculation without computing the limits of sums, which is tedious?**"
      ],
      "metadata": {
        "id": "X41bFzj7Rkr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to Pythagoras:\n",
        "\n",
        "$\\Delta s_{k}=$$\\sqrt{\\left(\\Delta x_{k}\\right)^{2}+\\left(\\Delta y_{k}\\right)^{2}}$\n",
        "\n",
        "Which makes:\n",
        "$\\Delta A_{k}=f\\left(x_{k}, y_{k}\\right)\\sqrt{\\left(\\Delta x_{k}^{2}\\right)^{2}+\\left(\\Delta y_{k}\\right)^{2}}$\n",
        "\n",
        "Now I multiply with $\\Delta t$\n",
        "\n",
        "$=f\\left(x_{k}, y_{k}\\right) \\sqrt{\\left(\\frac{\\Delta x_{k}}{\\Delta t}\\right)^{2}+\\left(\\frac{\\Delta y_{k}}{\\Delta t}\\right)^{2}} \\cdot \\Delta t$\n",
        "\n",
        "And when I go to infinity $\\boldsymbol{n} \\rightarrow \\infty$ I get to the first derivative of the x snd y component, which is easier to compute that integrals up to infinity:\n",
        "\n",
        "> $d A=f(g(t), h(t))$ $\\sqrt{g^{\\prime}(t)^{2}+h^{\\prime}(t)^{2}} d t$\n",
        "\n",
        "And this is now my new formula for my line integral:\n",
        "\n",
        "> $\\int_{c} f(x, y) d s=$\n",
        "\n",
        "> <font color=\"blue\">$\\int_{a}^{b} f(g(t), h(t))$ $\\sqrt{g^{\\prime}(t)^{2}+h^{\\prime}(t)^{2}} d t$\n"
      ],
      "metadata": {
        "id": "G4kE9U186y7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Line_integral"
      ],
      "metadata": {
        "id": "Caw_iC0HyOoE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILqKxv9BF4QY"
      },
      "source": [
        "https://www.youtube.com/watch?v=d_UX4_0KIGY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEoDCIYLOem"
      },
      "source": [
        "* z.B. zur Berechnung des Umfangs eines Objekts. welche Kraft ist noetig, um durch ein Vektorfeld auf einer Kurve entlang zu gehen?\n",
        "\n",
        "* Das [Kurven-, Linien-, Weg- oder Konturintegral](https://de.wikipedia.org/wiki/Kurvenintegral) erweitert den gew√∂hnlichen Integralbegriff f√ºr die Integration\n",
        "\n",
        "  * in der komplexen Ebene (Funktionentheorie) oder\n",
        "\n",
        "  * im mehrdimensionalen Raum (Vektoranalysis).\n",
        "\n",
        "* Den Weg, die Linie oder die Kurve, √ºber die integriert wird, nennt man den Integrationsweg.\n",
        "\n",
        "* Wegintegrale √ºber geschlossene Kurven werden auch als Ringintegral, Umlaufintegral oder Zirkulation bezeichnet und mit dem Symbol\n",
        "‚àÆ bzw. $\\textstyle \\oint$  geschrieben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Oybmkc-zAl"
      },
      "source": [
        "Auch Linienintegral. Berechne z.B. den kurzesten Weg zwischen zwei Punkten unter Berucksichtigung der Geschwindigkeit (eine gerade Linie ist nicht immer der kurzeste Weg oder ein moglicher Weg)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_jWgM7A9OMt"
      },
      "source": [
        "![ff](https://upload.wikimedia.org/wikipedia/commons/4/42/Line_integral_of_scalar_field.gif)\n",
        "\n",
        "*The line integral over a scalar field f can be thought of as the area under the curve C along a surface z = f(x,y), described by the field*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2b6mZKu-Pe1"
      },
      "source": [
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pyf0MPG-VkL"
      },
      "source": [
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNdoF6CN-ucN"
      },
      "source": [
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNcecwnJ-vac"
      },
      "source": [
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F0Cp7AP-wRr"
      },
      "source": [
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_lY1plvJOZ-"
      },
      "source": [
        "**Kurvenintegral 1. Art (√ºber Skalarfelder, nicht-orientiert)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Y7uIIJGQcJ"
      },
      "source": [
        "* zB zur Berechnung der Masse eines Drahtes [entlang einer Helix](https://www.youtube.com/watch?v=8XcqTg1NPKg) mit einer gegebenen Dichte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHbjxfJVJMrP"
      },
      "source": [
        "Wegintegral erster Art ist das **Wegintegral einer stetigen Funktion**, $\n",
        "f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\n",
        "$ entlang eines st√ºckweise stetig differenzierbaren Weges $\n",
        "\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}\n",
        "$ ist definiert als\n",
        "\n",
        "> $\\int_{\\mathcal{C}} f \\mathrm{~d} s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\|_{2} \\mathrm{~d} t$\n",
        "\n",
        "F√ºr eine stetige Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ und einem regul√§ren Weg $\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}$ definiert man das Kurvenintegral von $f$ l√§ngs $\\gamma$ durch: $\\int_{\\gamma} f d s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\| d t$\n",
        "\n",
        "  * $ds$ sowie $ \\|\\dot{\\gamma}(t)\\| d t$ sind das '**Linienelement**'\n",
        "\n",
        "  * $ \\|\\dot{\\gamma}(t)\\|$ ist die Norm von der Ableitung, die man berucksichtigen muss beim Integrieren wie schnell man durch die Kurve lauft (Gewichtungsfaktor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvuqF-aJDBO"
      },
      "source": [
        "**Kurvenintegral 2. Art (√ºber Vektorfelder, orientiert)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ5fqgZGGky6"
      },
      "source": [
        "* nicht mehr skalare, sondern vektorielle Funktion integrieren (Vektorfeld)\n",
        "\n",
        "* ps: jedes Kurvenintegral zweiter Art ist auch ein Kurvenintegral erster Art\n",
        "\n",
        "* z.B. um eine Arbeit, Zirkulation oder elektrische Spannung [zu berechnen](https://www.youtube.com/watch?v=HmgkyI_Q0Oo)\n",
        "\n",
        "* nennt man daher auch \"Arbeitsintegral\", wenn man sich zB ein Kraftfeld v vorstellt. Oder Zirkulation mit Integral entlang des Weges berechnen, wenn v ein Geschwindigkeitsfeld ist. Oder elektrische Spannung, wenn es ein elektrisches Feld ist.\n",
        "\n",
        "* Zun√§chst fragen: hat v ein Skalarpotential? Ist die betrachtete Menge einfach zusammenh√§ngend und ist zB die Rotation des Vektorfeldes gleich der Nullvektor? Berechnung des Skalarpotentials: mit der Ansatzmethode oder mit der Kurvenintegralmethode\n",
        "\n",
        "* Danach fragen, ob der Weg geschlossen oder offen ist? (geschlossen: Anfangspunkt = Endpunkt, wie bei Kreis oder Dreieck). Ist er geschlossen, ist der Wert des Kurvenintegrals gleich Null. (**Remember**: Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ f√ºr beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLe3mhoqDP36"
      },
      "source": [
        "Das Wegintegral zweiter Art ist das **Wegintegral √ºber ein stetiges Vektorfeld** $\\mathbf{f}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ mit einer ebenfalls so parametrisierten Kurve ist definiert als das Integral √ºber das Skalarprodukt aus $\\mathrm{f} \\circ \\gamma$ und $\\dot{\\gamma}$ :\n",
        "\n",
        "> $\\int_{\\mathcal{C}} \\mathbf{f}(\\mathbf{x}) \\cdot \\mathrm{d} \\mathbf{x}:=\\int_{a}^{b} \\mathbf{f}(\\gamma(t)) \\cdot \\dot{\\gamma}(t) \\mathrm{d} t$\n",
        "\n",
        "* Ist ein **Kurvenintegral 2. Art (weil man Vektorfelder integriert)** und ist orientiert (es ist wichtig, wie man den Weg durchlauft wegen den Vektoren!)\n",
        "\n",
        "* F√ºr ein stetiges Vektorfeld $\\mathbf{v}: D \\rightarrow \\mathbb{R}^{3}$ mit $D \\subset \\mathbb{R}^{3}$ und einen regul√§ren Weg $\\gamma:[a, b] \\rightarrow \\mathbb{R}^{3}$ definiert man das Kurvenintegral von v l√§ngs $\\gamma$ durch:\n",
        "\n",
        "> $\\int_{\\gamma} v \\cdot d \\vec{s}$ $:=\\int_{a}^{b} v(\\gamma(t)) \\cdot \\dot{\\gamma}(t) d t$\n",
        "\n",
        "$v$ ist die Kraft\n",
        "\n",
        "$d \\vec{s}$ ist der Weg\n",
        "\n",
        "Kraft * Weg = Arbeit, aber Weg ist nicht geradlinig, sondern eine verschnoerkelte Kurve. Man nimmt unendlich kleine Wegelemente und berechnet die Arbeit, und summieren die Teile auf (Integral).\n",
        "\n",
        "* Anmerkungen:\n",
        "\n",
        "  * der dicke Punkt steht fur das Skalarprodukt.\n",
        "\n",
        "  * $\\dot{\\gamma}(t) d t$ - keine Norm der Ableitung der Kurve notig (dieser Tangentialvektor ist nicht normiert), wie bei Kurvenintegralen uber Funktionen\n",
        "\n",
        "  * $ d \\vec{s}$ ist ein **vektorielles Linienelement**\n",
        "\n",
        "* **Komponente des Vektorfeldes in Richtung des Weges - Das ist, was man ber√ºcksichtigen will im Integral: Wie viel von dem Vektorfeld ist in Wegrichtung? Und nur das macht einen Einfluss in das Integral.**\n",
        "\n",
        "* Beim Durchlaufen des Weges werden wir entweder mit dem Vektorfeld (in Richtung des Vektorfeldes) getrieben, oder es haelt uns zur√ºck. = die Energie, die wir brauchen, um den Weg zu durchlaufen (wenn das Vektorfeld uns hilft oder daran hindert, wie eine Str√∂mung).\n",
        "\n",
        "* **Die Komponente in Richtung des Weges ist wichtig, und die Komponente bekommt man durch das Skalarprodukt.** (siehe Bild unten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-v9nefMJY9-"
      },
      "source": [
        "![vv](https://raw.githubusercontent.com/deltorobarba/repo/master/kurvenintegral_vektorfeld.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Line Integrals (Kurvenintegral) - Example*"
      ],
      "metadata": {
        "id": "4_WdsJ1dILCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Line Integral: Full Example](https://youtu.be/fqVEuFldFuA)"
      ],
      "metadata": {
        "id": "xyBFooXnJ-0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I have the following circle on the x\b-y-plane and a function on top of it. I want to calculate the surface between the ccircle on the bottom and the function (=line integral)\n",
        "\n",
        "* I have the form of the function on top, and the parametriztaion of the circle on the bottom\n",
        "\n",
        "* then i want to write my function with x and y in terms of mz circle. So I add the components of the circle into the function"
      ],
      "metadata": {
        "id": "5Yr8XOUvIsy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_021.PNG)"
      ],
      "metadata": {
        "id": "QZ2zY4-kM3i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_020.PNG)"
      ],
      "metadata": {
        "id": "Pm5dRPiZMQYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_017.PNG)"
      ],
      "metadata": {
        "id": "3v5mfKs-IPjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_018.PNG)"
      ],
      "metadata": {
        "id": "uOtgTNC2MUm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_019.PNG)"
      ],
      "metadata": {
        "id": "ZH30icG4LwPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Line Integrals (Kurvenintegral) - in 3D*"
      ],
      "metadata": {
        "id": "tJjGPdHbO379"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I have a curve $r(t)$ in 3D that's in the form of a Helix.\n",
        "\n",
        "And above this curve I have a certain function f(x,y):"
      ],
      "metadata": {
        "id": "ymG-tJU6PJTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_022.PNG)"
      ],
      "metadata": {
        "id": "5wyVaHgxPAGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_024.PNG)"
      ],
      "metadata": {
        "id": "TjvpSwtqQSYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_023.PNG)"
      ],
      "metadata": {
        "id": "OnibumK9QReY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line integral doesn't represent the surface anymore like it did in 2D, but what could it represent?"
      ],
      "metadata": {
        "id": "YFsKBxKfQoOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_025.PNG)"
      ],
      "metadata": {
        "id": "ltyEhlgFR7q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or the water that fits into a pipe with different widths:"
      ],
      "metadata": {
        "id": "IbAH1b1mRN8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_026.PNG)"
      ],
      "metadata": {
        "id": "gyP-jPR1R9q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Line Integrals (Kurvenintegral) - of Vector Fields*"
      ],
      "metadata": {
        "id": "_JMvUMNWZHG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/059LjQ2oLBM"
      ],
      "metadata": {
        "id": "wh2CVtboZTB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Measures the degree to which the vector field goes along the path of the curve (how much are they aligned?)**"
      ],
      "metadata": {
        "id": "kr1QchO6ZLbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at physics: in this case we don't want to measure the whole force. We take the dot product - the proportion of the force of gravity in that tangential direction. That‚Äòs the force we are interested in, it‚Äòs not the entire force of gravity, just the proportion of the force in the tangential direction. And then you multiply that by the distance d."
      ],
      "metadata": {
        "id": "XJCTQnA6aD3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_030.PNG)"
      ],
      "metadata": {
        "id": "y4m63jAWZ-z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case: what is the work done by that field when I move along the curve? **How much work is done is going to depend on the degree to which this curve is going tangential to the field.**. I compute the inner product to project my force on the tangential."
      ],
      "metadata": {
        "id": "e5s_DT-aa6z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_031.jpg)"
      ],
      "metadata": {
        "id": "Uw6nL8sVcbAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Line Integrals (Kurvenintegral) - Fundamental Theorem of Line Integral*"
      ],
      "metadata": {
        "id": "3iZYeNImKkLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [The Fundamental Theorem of Line Integrals // Big Idea & Proof // Vector Calculu](https://www.youtube.com/watch?v=we88mTXj6Yc&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=14)"
      ],
      "metadata": {
        "id": "m9HVYzv8KsL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This is now only valid for conservative vector fields!"
      ],
      "metadata": {
        "id": "NYYq12cbLkhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reminder: Fundamental Theory of Calculus**:  if you integrate the derivative, then you get the endpoints - $\\int_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "Other way of writing this: If $f^{\\prime}(x)$ is continuous on $[a, b]$\n",
        "\n",
        ">$\n",
        "\\int_a^b f^{\\prime}(x) d x=f(b)-f(a)\n",
        "$\n"
      ],
      "metadata": {
        "id": "WoyJvsPBK85_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental Theorem of Line Integral**\n",
        "\n",
        "Let $C$ be a smooth curve parameterized by $\\vec{r}(t)$ from $\\vec{r}(a)=A$ to $\\vec{r}(b)=B$. For continuous $\\vec{F}=\\nabla f$\n",
        "\n",
        ">$ \\int_C\n",
        "\\vec{F} \\cdot d \\vec{r}=f(B)-f(A)\n",
        "$\n"
      ],
      "metadata": {
        "id": "BxvAt-2kM8IZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQdL1yhzPGzI"
      },
      "source": [
        "###### <font color=\"blue\">*Oberfl√§che (Surface Area)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Introduction (Explicit, Implicit and Parametric)*"
      ],
      "metadata": {
        "id": "Rs9afq_oyhKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Surface_integral"
      ],
      "metadata": {
        "id": "EsFuukjmxCb8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChIebVaPLd6"
      },
      "source": [
        "* z.B. zur Berechnung der Oberfl√§che eines Balls, Funktion der Temperatur an jedem Punkt an einer Oberfl√§che, um die durchschnittliche Temperatur auf der Oberfl√§che zu berechnen (https://youtu.be/AIxiYG-gZ00 at min 3:00)\n",
        "\n",
        "* Durch Parametrisierung wird z.B. die Kruemmung einer Ebene in $R^3$ beruecksichtigt. Man berechnet naemlich die Flaeche, in dem man von $R^3$ auf $R^2$ projiziert und dann das Integral berechnet (das geht, weil Flaeche in $R^3$ ist offen, stetig differenzierbar und bijektiv)\n",
        "\n",
        "* Das Oberfl√§chenintegral oder Fl√§chenintegral ist eine Verallgemeinerung des Integralbegriffes auf ebenen oder gekr√ºmmten Fl√§chen. Das Integrationsgebiet $\\mathcal{F}$ ist also nicht ein eindimensionales Intervall, sondern eine zweidimensionale Menge im dreidimensionalen Raum $\\mathbb{R}^{3}$. F√ºr eine allgemeinere Darstellung im $\\mathbb{R}^{n}$ mit $n \\geq 2$ siehe: Integration auf Mannigfaltigkeiten.\n",
        "\n",
        "Es wird generell zwischen einem skalaren und einem vektoriellen Oberfl√§chenintegral unterschieden, je nach Form des Integranden und\n",
        "des sogenannten Oberfl√§chenelements. Sie lauten\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} f \\mathrm{~d} \\sigma$ mit skalarer Funktion $f$ und skalarem Oberfl√§chenelement $\\mathrm{d} \\sigma$ sowie\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} \\vec{v} \\cdot \\mathrm{d} \\vec{\\sigma}$ mit vektorwertiger Funktion $\\vec{v}$ und vektoriellem Oberfl√§chenelement $\\mathrm{d} \\vec{\\sigma}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcvZH7qLBOzs"
      },
      "source": [
        "Allgemein l√§sst sich eine Fl√§che im $\\mathbb{R}^{3}$ mit zwei Parametern $u$ und $v$ in\n",
        "folgender Form darstellen:\n",
        "\n",
        "> $\n",
        "\\varphi: B \\rightarrow \\mathbb{R}^{3}, \\quad(u, v) \\mapsto \\vec{\\varphi}(u, v)=\\left(\\begin{array}{l}\n",
        "x(u, v) \\\\\n",
        "y(u, v) \\\\\n",
        "z(u, v)\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Auf der Fl√§che $\\vec{\\varphi}(u, v)$ bilden die Kurvenscharen $u=$ const bzw. $v=$ const die Koordinatenlinien. Diese √ºberziehen die Fl√§che mit einem Koordinatennetz.\n",
        "wobei durch jeden Punkt zwei Koordinatenlinien verlaufen. Somit hat ieder Punkt auf der Fl√§che eindeutige Koordinaten $\\left(u_{0}, v_{0}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45jm0deB13y"
      },
      "source": [
        "Mit den Parametrisierungen und den Oberfl√§chenelementen kann man nun die Oberfl√§chenintegrale definieren. Diese mehrdimensionalen Integrale sind Lebesgue-Integrale, k√∂nnen aber in den meisten Anwendungsf√§llen als mehrfache Riemann-Integrale berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yO9AukUtSzo"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Oberfl√§chenintegral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction Video: [Describing Surfaces Explicitly, Implicitly & Parametrically // Vector Calculus](https://youtu.be/jZRqCfi5_Uo)"
      ],
      "metadata": {
        "id": "8vVdeCClvrPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_047.PNG)"
      ],
      "metadata": {
        "id": "2l2A7LKvxN6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_048.PNG)"
      ],
      "metadata": {
        "id": "Vy8LqMptxP8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_049.PNG)"
      ],
      "metadata": {
        "id": "txf7BHR5xRyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_050.PNG)"
      ],
      "metadata": {
        "id": "NOOvynWUxTWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_051.PNG)"
      ],
      "metadata": {
        "id": "shyAhgDcxUy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this a transformation. The parametrization is a way to map the simple rectangle case to the complex cone case:"
      ],
      "metadata": {
        "id": "HbWGt8pbwJIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_052.PNG)"
      ],
      "metadata": {
        "id": "0nkMWGDIxXS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Calculate surface area*"
      ],
      "metadata": {
        "id": "9F9ROmFYzbtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Surface Area = $\\int_{c}^{d} \\int_{a}^{b}\\left|\\vec{r}_{u} \\times \\vec{r}_{v}\\right| d u \\, d v$\n",
        "\n",
        "* ich parametrize zwischen a und b (zB zwischen 0 und 2 $\\pi$ fur $\\theta$) bzw c und d (zB zwischen 0 und $\\pi$ fur $\\phi$)\n",
        "\n",
        "* ich bestimme das partielle Differential von ${r}$ with respect to $_{u}$ und dann nochmal das partielle Differential von ${r}$ with respect to $_{v}$\n",
        "\n",
        "* dann nehme ich beide partiellen Differentiale und berechne das Cross Product"
      ],
      "metadata": {
        "id": "Imn0bOPskCfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/hnjfY9hRIXk"
      ],
      "metadata": {
        "id": "jPhsipagzjxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I can describe a surface parametrically, that means there is somewhere there is a u/v coordinate system with a region. And there is a map / transformation to a more squiggely version in 3D for example:"
      ],
      "metadata": {
        "id": "ZRcm323Cz4qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_053.PNG)"
      ],
      "metadata": {
        "id": "zxlu5_9s2CZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r_u}$ is the partial derivative in the direction where $v$ is constant\n",
        "\n",
        "$\\vec{r_v}$ is the partial derivative in the direction where $u$ is constant\n",
        "\n",
        "And above pointing is the cross product of $\\vec{r_u}$  with $\\vec{r_v}$"
      ],
      "metadata": {
        "id": "DorAizAb1CDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now comes the most important: the length of the cross product is the same as the area of the parallelogram of the two vectors that form the cross product. $\\rightarrow$ With that I can approximate the surface area when I take the limit (and takes the definition of an integral)."
      ],
      "metadata": {
        "id": "Mp_2P1Y_2UAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_054.PNG)"
      ],
      "metadata": {
        "id": "skuzWvoB3IIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here you get the final formula:"
      ],
      "metadata": {
        "id": "2DXWCQfO3iWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_055.PNG)"
      ],
      "metadata": {
        "id": "g57LTd053g-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Example 1: Calculation for a Sphere*"
      ],
      "metadata": {
        "id": "sOKlhEom4-yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/iTkdDnC0seQ"
      ],
      "metadata": {
        "id": "IE91hyiV3vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see example calculation here for the surface of a sphere:"
      ],
      "metadata": {
        "id": "o_WQFs7q3ndU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_058.PNG)"
      ],
      "metadata": {
        "id": "IbRBNcSD5TG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First we take spherical coordinate, because it's more natural for a sphere"
      ],
      "metadata": {
        "id": "W4da3T_U34-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_057.PNG)"
      ],
      "metadata": {
        "id": "BEYNs3Ty5Q0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_056.PNG)"
      ],
      "metadata": {
        "id": "lOWlLVQL5Pvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_059.PNG)"
      ],
      "metadata": {
        "id": "lqQekDTy5U8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r}_{\\phi}$ und $\\vec{r}_{\\theta}$ sind die partial derivatives with respect ti phi and theta jeweils\n",
        "\n",
        "$\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}$ ist das cross product (das ist die Determinante)\n",
        "\n",
        "Dann nehme ich davon $\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right|$ (square root), um die Fl√§che der Oberfl√§che zu erhalten"
      ],
      "metadata": {
        "id": "9ueegH1T5ttN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_060.PNG)"
      ],
      "metadata": {
        "id": "Zm6soIMU7FIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_061.PNG)"
      ],
      "metadata": {
        "id": "CeifKd7M7HFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I plug it in into $A=\\int_{0}^{2 \\pi} \\int_{0}^{\\pi}\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right| d \\phi \\, d \\theta$\n",
        "\n",
        "* mit den Limits dass phi is zwischen 0 und pi\n",
        "\n",
        "* und theta is zwischen 0 und 2*pi"
      ],
      "metadata": {
        "id": "shp6Hska7QI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_062.PNG)"
      ],
      "metadata": {
        "id": "Ui1roOrb7v0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Example 2: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "Pf3GY2W7nkkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 1](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "ZZkvuyz8nryU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find surface area of a plane z = -x that lies inside a cylinder $x^2 + y^2 = 4$\n",
        "\n",
        "* First find a parametrization for the plane: i use standard polar coordinate with sin and cosine, and for the z-component (the height of the surface area) I can use -x\n",
        "\n",
        "> $\\vec{r}(r, \\theta)=\\langle r \\cos \\theta, r \\sin \\theta,-r \\cos \\theta \\rangle$\n",
        "\n",
        "* second, i know what my constraints are: $2\\pi$, because the cylinder = 4 so radius = 2\n",
        "\n",
        "> $0 \\leq r \\leq 2 \\quad 0 \\leq \\theta \\leq 2 \\pi$"
      ],
      "metadata": {
        "id": "4W0gn2ven_XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_063.png)"
      ],
      "metadata": {
        "id": "1Mtbl0FwqNG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Example 3: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "WS0hoJa5wico"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 2](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "mNac0ltVrYme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I make my parameters $\\theta$ and $z$ (height) $\\rightarrow$ die richtige (iS von einfachste) Parametrisierung ist sehr wichtig"
      ],
      "metadata": {
        "id": "4Kxxwv2vwjxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_064.png)"
      ],
      "metadata": {
        "id": "Vo5gBKjswuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_065.png)"
      ],
      "metadata": {
        "id": "42E-WtVtxPNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_066.png)"
      ],
      "metadata": {
        "id": "XvNkFTfcxQae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Surface Area for Implicit Surfaces*"
      ],
      "metadata": {
        "id": "xDFYb8uW7h9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Surface Area for Implicit & Explicit Surfaces](https://www.youtube.com/watch?v=k13kwLzoTpo&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1)"
      ],
      "metadata": {
        "id": "Q2FhPOJS7mFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cases where I don't have a parametrized version (like above), but an implicit version"
      ],
      "metadata": {
        "id": "elSoMeoZ8OBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine two surfaces: one flat on the bottom and one curved above it.\n",
        "\n",
        "* Take the normal vector of both and compare them.\n",
        "\n",
        "* If both surfaces are flat, the inner product = 1 and its integrand of the above surface will give you the surface area of the bottom (since they are equal).\n",
        "\n",
        "* If the inner product is not 1, then there is a difference and it will compute the surface area of the above surface."
      ],
      "metadata": {
        "id": "JGd3syYr7rqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_067.png)"
      ],
      "metadata": {
        "id": "EP-BKu5x8HLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implicit Surface F(x, y, z) = C\n",
        "\n",
        "Assume:\n",
        "\n",
        "* Smooth (i.e. F differentiable, $\\nabla F \\neq \\vec{0}$\n",
        "and continuous)\n",
        "\n",
        "* The surface is 'above' a region in the xy-plane with $\\nabla F \\bullet \\hat{k} \\neq 0$\n",
        "\n",
        "> Surface Area $=\\iint_{R} \\frac{|\\nabla F|}{|\\nabla F \\cdot \\hat{k}|} d A$"
      ],
      "metadata": {
        "id": "XxRR64jL8p5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_069.png)"
      ],
      "metadata": {
        "id": "qyMsoMtf-Okp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The more the plane above is twisted, the more the gradient $|\\nabla F \\cdot \\hat{k}|$ gets smaller, and if the denominator gets smaller, the total surface is bigger overall. We practically get a ratio between the original length of the gradient vector and then this portion of the gradient vector in the $\\hat{k}$ direction -  that is our scaling factor!"
      ],
      "metadata": {
        "id": "l-81RyUh_gBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I change this now with a generic plane, because choosing the $\\hat{k}$ direction was a bit arbitrary:"
      ],
      "metadata": {
        "id": "6GLNqJEd99f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_068.png)"
      ],
      "metadata": {
        "id": "AZrhMZvg95h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Surface Area for Explicit Surfaces*"
      ],
      "metadata": {
        "id": "-TPsQ4p0KZ2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's turn to explicit surfaces**"
      ],
      "metadata": {
        "id": "wcq8cfnW-TM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_070.png)"
      ],
      "metadata": {
        "id": "N4Jpyv30-lYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example video: [Computing the Surface Area of an Implicitly Defined Surface](https://www.youtube.com/watch?v=810yT1zQtxA&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=2)"
      ],
      "metadata": {
        "id": "TAeCA5jt_97i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Oberfl√§chenintegral (Surface integral)*"
      ],
      "metadata": {
        "id": "CQu9zwVlu7sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Formulas & Examples*"
      ],
      "metadata": {
        "id": "w3UozhzpvHs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repetition of Surface Areas computed only with the cross product:"
      ],
      "metadata": {
        "id": "NKIcTkPkv5n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_071.jpg)"
      ],
      "metadata": {
        "id": "uAY51nj6v3Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on to surface integrals:\n",
        "\n",
        "Now $G(x, y, z)$ defined on a smooth surface\n",
        "\n",
        "> Surface Integral $=\\iint_S G(x, y, z) d \\sigma$\n",
        "\n",
        "* g (x,y,z) is a function that is defined on that surface\n",
        "\n",
        "> the surface integral is adding up the values of that function along each elements of the surface area\n",
        "\n",
        "> It's like some 'height' is added: you have the position x and y. And z can be the temperature at each point on a surface, or the thickness, or else. It's like an abstract 'height'.\n",
        "\n",
        "Changing our Surface Area (SA) definitions to Surface Integral (SI) definitions with cross product and function:"
      ],
      "metadata": {
        "id": "7uge7W-FwA9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_072.PNG)"
      ],
      "metadata": {
        "id": "UEjkFbRByToC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potential Applications of Surface Integrals:**"
      ],
      "metadata": {
        "id": "_8WGC-mMy34V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here: add up density * areas = surface integral to compute thickness"
      ],
      "metadata": {
        "id": "f02swzD0zHoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_073.jpg)"
      ],
      "metadata": {
        "id": "GQSMkISxy9IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_074.jpg)"
      ],
      "metadata": {
        "id": "p_LbZwyN1Rtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_075.jpg)"
      ],
      "metadata": {
        "id": "01thcI9p1TN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Orientable vs Non-Orientable Surfaces*"
      ],
      "metadata": {
        "id": "L5JSmpl71sGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/S48JsV-pCBo"
      ],
      "metadata": {
        "id": "imvAz-081zR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <font color=\"blue\">**Definition: A smooth surface is orientable if there is a continuous, field of unit normal vectors**"
      ],
      "metadata": {
        "id": "XPJ1xZdl22iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_076.jpg)"
      ],
      "metadata": {
        "id": "xBTAZRLF3xYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is this important: I cannot assign a continuous, field of unit normal vectors to a Mobius strip**"
      ],
      "metadata": {
        "id": "INUjCg7I4LUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_077.jpg)"
      ],
      "metadata": {
        "id": "apJ6kMoz4qxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flux across a Surface*"
      ],
      "metadata": {
        "id": "nbxMBXJa5GRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example surface is located in a vector field. We will see why it is important to have an orientable surface (a continuous, field of unit normal vectors)"
      ],
      "metadata": {
        "id": "qw79YhYd5S_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_078.jpg)"
      ],
      "metadata": {
        "id": "O6-neCqr5Kyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_079.jpg)"
      ],
      "metadata": {
        "id": "3ZYINm3S8XaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of Flux across a Surface:**"
      ],
      "metadata": {
        "id": "YHyr1xAl8i-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_080.jpg)"
      ],
      "metadata": {
        "id": "Af1J-Xnq9LTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can simplify:\n",
        "\n",
        "> $Flux =\\iint_{S} \\vec{F} \\cdot \\frac{\\vec{r}_{u} \\times \\vec{r}_{v}}{\\left|\\vec{r}_{u} \\times \\vec{r}_{v}\\right|}\\left|\\vec{r}_{u} \\times \\vec{r}_{v}\\right| d u d v$\n",
        "\n",
        "to this due to possible cancellations:\n",
        "\n",
        "> $F l u x=\\iint_{S} \\vec{F} \\cdot\\left(\\vec{r}_{u} \\times \\vec{r}_{v}\\right) du \\, dv$\n"
      ],
      "metadata": {
        "id": "71Wiy42-9vdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For implicit surfaces:**"
      ],
      "metadata": {
        "id": "wu1jQVut-nwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_082.jpg)"
      ],
      "metadata": {
        "id": "7lZqFj4q9OLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auch das kann ich wieder vereinfachen:\n",
        "\n",
        "> $F l u x=\\iint_{S} \\vec{F} \\cdot \\frac{\\nabla g}{|\\nabla g|} \\frac{|\\nabla g|}{|\\nabla g \\cdot \\vec{p}|} d u d v$\n",
        "\n",
        "in folgende Definition:\n",
        "\n",
        "> $F l u x=\\iint_{S} \\vec{F} \\cdot \\frac{\\nabla g}{|\\nabla g \\cdot \\vec{p}|} d u d v$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K1u2LnMe_L5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Calculation: https://youtu.be/XiFu9GfTakw"
      ],
      "metadata": {
        "id": "yHFUp3KMAPuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Fundamental Theorem of Calculus*"
      ],
      "metadata": {
        "id": "0USsGJE7Tfz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus*"
      ],
      "metadata": {
        "id": "xSQIpQcXex8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ein konservatives Feld kann geschrieben werden als Derivative eines **Potential Function** (siehe Skalarpotenzial):\n",
        "\n",
        "> $\\vec{F} = \\Delta f$"
      ],
      "metadata": {
        "id": "28SWd6uzYXYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Das Skalarpotenzial ist ein Ma√ü f√ºr die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein K√∂rper entgegen der wirkenden Kraft bewegt, dann erh√∂ht sich seine potenzielle Energie.\n",
        "\n",
        "https://youtu.be/tb3KSeF0WrQ"
      ],
      "metadata": {
        "id": "t--VuFa7aLVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Conservative Vector Fields // Vector Calculus](https://www.youtube.com/watch?v=76nzOtupeRc&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=13)"
      ],
      "metadata": {
        "id": "5Plqa1ISKfWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like electric (electromagnetic) fields or gravitational fields - the work done (i.e. by the force of gravity on a moving ball in the air) only depends on the endpoints, the differences in the height in this example (now matter which way the ball went)."
      ],
      "metadata": {
        "id": "EmnhQ8ROFkBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A field $\\vec{F}$ is conservative on an open domain if (the line integral along the curve c):\n",
        "\n",
        ">$\\int_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "is the same for ALL paths $C$ between points $A$ and $B$ in the domain."
      ],
      "metadata": {
        "id": "nzg_wKmLGdFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The work done is in all 3 cases the same, even though the paths are different:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_009.png)"
      ],
      "metadata": {
        "id": "WLBFyBWGHxXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you compute it you get the integral of the derivative at the end. We know from the **Fundamental Theory of Calculus** that if you integrate the derivative, then you get the endpoints! In the above case the work done is zero."
      ],
      "metadata": {
        "id": "J8iTls62Idze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_010.png)"
      ],
      "metadata": {
        "id": "XiqIPxM8I83W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When is a field conservative?**\n",
        "\n",
        "A continuous field $\\vec{F}$ is conservative * if and only if\n",
        "\n",
        ">$\n",
        "\\vec{F}=\\nabla f\n",
        "$\n",
        "\n",
        "For some differentiable 'potential function' $f$ (=scalar function)."
      ],
      "metadata": {
        "id": "-SPeqJszJW4T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56sZZ-KLKsX_"
      },
      "source": [
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des ‚Äûkonservativen Kraftfelds‚Äú oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ f√ºr beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.\n",
        "\n",
        "* Deutet man $\\vec{X}(\\vec{r})$ als Kraftfeld, so ist das Kurvenintegral die gesamte l√§ngs der Randkurve $S$ gegen die Kraft $\\vec{X}(\\vec{r})$ verrichtete Arbeit.\n",
        "\n",
        "* Wirbelfrei sind z. B. das ruhende elektrische Feld in der Elektrostatik und das Gravitationsfeld, aber auch Felder wie das Geschwindigkeitsfeld einer Potentialstr√∂mung.\n",
        "\n",
        "> Ist $\\vec{X}(\\vec{r})$ wirbelfrei, dann gilt: $\\operatorname{rot} \\vec{X}(\\vec{r})=\\overrightarrow{0}$\n",
        "d. h. die Rotation des Vektorfeldes ist gleich null.\n",
        "\n",
        "* Ist der Definitionsbereich einfach **zusammenh√§ngend, so gilt auch die Umkehrung**.\n",
        "\n",
        "> Wirbelfreie Vektorfelder lassen sich stets als **Gradient eines zugrundeliegenden skalaren Felds** $\\Phi(\\vec{r})$ formulieren (siehe Gradientenfeld): $\n",
        "\\vec{X}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Daraus folgt, dass f√ºr das skalare Feld $\\Phi$ gilt:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\overrightarrow{0}$ (**siehe auch unten auch Gradientfeld & Skalarpotenzial**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLH2-gTPd43O"
      },
      "source": [
        "*Quellenfreie und wirbelfreie Vektorfelder; Zerlegungssatz*\n",
        "\n",
        "Ein mindestens **zweimal stetig differenzierbares Vektorfeld** $\\mathbf{v}(\\mathbf{r})$ im $\\mathbb{R}^{3}$ hei√üt quellenfrei (beziehungsweise wirbelfrel), wenn seine **Quellendichte (Divergenz) beziehungsweise Wirbeldichte (Rotation) dort √ºberall Null ist**. Unter der weiteren\n",
        "Voraussetzung, dass die Komponenten von $\\mathbf{v}$ im Unendlichen hinreichend rasch verschwinden, gilt der sogenannte Zerlegungssatz: Jedes Vektorfeld $\\mathbf{v}(\\mathbf{r})$ ist\n",
        "eindeutig durch seine Quellen bzw. Wirbel bestimmt, und zwar gilt die folgende\n",
        "Zerlegung in einen wirbelfreien beziehungsweise quellenfreien Anteil:\n",
        "\n",
        "> $\n",
        "\\mathbf{v}(\\mathbf{r}) \\equiv-\\operatorname{grad}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{div}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}+\\operatorname{rot}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{rot}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}\n",
        "$\n",
        "\n",
        "Dies entspricht der Zerlegung eines statischen elektromagnetischen Feldes in den elektrischen beziehungsweise magnetischen Anteil (siehe Elektrodynamik). Es sind also genau die Gradientenfelder (d. h. die , elektrischen Feldkomponenten\") wirbelfrei bzw. genau die Wirbelfelder (d. h. die ,magnetischen Feldkomponenten\") quellenfrei. Dabei sind grad $\\phi(\\mathbf{r}):=\\nabla \\phi,$ div $\\mathbf{v}:=\\nabla \\cdot \\mathbf{v}$ und rot $\\mathbf{v}:=\\nabla \\times \\mathbf{v}$ die bekannten, mit dem Nabla-Operator $(\\nabla)$ der Vektoranalysis\n",
        "gebildeten Operationen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do I know if a vector field is conservative?**"
      ],
      "metadata": {
        "id": "Q8yG2xqwbngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">$\n",
        "\\vec{F}=M(x, y, z) \\hat{\\imath}+N(x, y, z) \\hat{\\jmath}+P(x, y, z) \\hat{k}\n",
        "$\n",
        "\n",
        "is conservative* if and only if\n",
        "\n",
        ">$\n",
        "\\frac{\\partial M}{\\partial y}=\\frac{\\partial N}{\\partial x} \\quad \\frac{\\partial N}{\\partial z}=\\frac{\\partial P}{\\partial y} \\quad \\frac{\\partial M}{\\partial z}=\\frac{\\partial P}{\\partial x}\n",
        "$\n",
        "\n",
        "*for $M, N, P$ continuous first partials, on an open simply connected domain"
      ],
      "metadata": {
        "id": "zzH5PUI8brn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/ZGUvyGeNT44"
      ],
      "metadata": {
        "id": "y3nKw6R6b0BS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do we find the (scalar) potential function so $\\vec{F}=\\nabla f ?$**"
      ],
      "metadata": {
        "id": "-w6SBRW9cbqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You take M, N and P and integrate them to get $\\vec{F}$, because\n",
        "\n",
        "> $\\vec{F} = \\nabla f=$ $\\langle \\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta z},\\rangle$ = $M, N, P$ from above\n",
        "\n",
        "* then you fill in a beginning and endpoint of the curve (parametrization), and according to the Fundamental theorem of Line Integrals (where only the beginning and endpoints count - no matter which path has been taken - you take the difference of the endpoint minus beginning"
      ],
      "metadata": {
        "id": "_FEO-D_ziCJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/jlza4rEFXKM"
      ],
      "metadata": {
        "id": "cippgw72cmAz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQKZZ-1M-GE"
      },
      "source": [
        "###### <font color=\"blue\">*Green's Theorem*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"black\">*Green's Theorem: Curl or Circulation (Flow) Density (Circulation-Curl Form)*"
      ],
      "metadata": {
        "id": "il2F19KA_-h1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRhCrZ_fK_8D"
      },
      "source": [
        "[Satz von Green](https://de.wikipedia.org/wiki/Satz_von_Green)\n",
        "\n",
        "* Der Satz von Green (auch Green-Riemannsche Formel oder Lemma von Green, gelegentlich auch Satz von Gau√ü-Green)\n",
        "\n",
        "* **erlaubt es, das Integral √ºber eine ebene Fl√§che durch ein Kurvenintegral auszudr√ºcken**.\n",
        "\n",
        "* Der Satz ist ein Spezialfall des Satzes von Stokes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWuiG_igrWtv"
      },
      "source": [
        "Integrals Ãàatze (Gauss, Stokes, Greenschen Formeln, Lo Ãàsung der Poissongleichung, Fundamentalsatz der Vektoranalysis II)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have the **local** Curl Density (seen under Differentialoperator: Rotation) at a specific point\n",
        "\n",
        "> Circulation Density: $\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)$\n",
        "\n",
        "* And we have the **global** flow (seen in Line Integral)\n",
        "\n",
        "> Circulation: $=\\oint_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "* with Green's Theorem we can now bring both together"
      ],
      "metadata": {
        "id": "B8hdiYEhrzRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source Video: https://youtu.be/JB99RbQAilI"
      ],
      "metadata": {
        "id": "pxOCoj0nukLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_034.PNG)"
      ],
      "metadata": {
        "id": "ijQLoOxMs47m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I take my region and cut in in half. The Intersection cancels each other out. I'm left with the path outside the region (what I want):"
      ],
      "metadata": {
        "id": "oB95FguIuZeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_035.PNG)"
      ],
      "metadata": {
        "id": "uVu2LUBVuMCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I can start seeing how the local and the global apporach starting to get connected:"
      ],
      "metadata": {
        "id": "Q1RVltxPuSKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_036.PNG)"
      ],
      "metadata": {
        "id": "ynKEQXp8uONY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So finally I take the double integral: At each point along the boundary this tendency to spin at that particular point is going to contribute to that portion of the circulation along the boundary."
      ],
      "metadata": {
        "id": "sIKuLLIjuniG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_037.PNG)"
      ],
      "metadata": {
        "id": "MXgREVJ1uP7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's at first a bit strange how the points inside are relevant for the curl on the path at the boundary. Circulation only occurs at the boundary. It doesn't matter what happens on the inside. Circluation is only a measure along the boundary.\n",
        "\n",
        "But we are saying circulation is equal to a double integral. An area integral. An area integral with all our circulation densities where it really does matter what's going along in the middle, what these spinners are doing in the inside. They contribute at the end to the curl that happens at the boundary.\n",
        "\n",
        "It's like in the Fundamental Theorem of Calculus (and Line Integral): you take the integral of a derivative and end up with the boundaries. It represents what happens at the boundary.  of some interval. And somehow the difference of the boundary of some interval is related to integrating over the entire integral.\n",
        "\n",
        "And similarly here we are relating information over the entiore region, theb information of the circulation density and accumulating it all up, and that results in just a circulation along the boundary."
      ],
      "metadata": {
        "id": "IhjyNptYvpah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"black\">*Green's Theorem: Divergence or Flux Density (Divergence-Flux Form or Normal Form)*"
      ],
      "metadata": {
        "id": "TJ0lhhj3e3Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/GsjJs71SBec"
      ],
      "metadata": {
        "id": "qQDFOBfOfApb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The concept of Flux along a boundary: The tendency of the vector field to be aligned with the outward normal."
      ],
      "metadata": {
        "id": "WSAT8IA-gLjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_038.PNG)"
      ],
      "metadata": {
        "id": "RCzyzi9kh71d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Flux is a global property\n",
        "\n",
        "* at the picture of can be gas leaving a source in the center\n",
        "\n",
        "* If I want to have some notion of the degree to which my vector field is leaving away from one specific point, around which I draw a rectangle\n",
        "\n",
        "* Let's draw this rectangle (like before, but we analysed whsat is the circlution around it) but this time compute flux:"
      ],
      "metadata": {
        "id": "FMx3J4M-hNF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The Flux Density = is also called the Divergence ! (how much the vector field is spreading away from some point at one specific point)"
      ],
      "metadata": {
        "id": "H0QXIwFTi2qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_039.PNG)"
      ],
      "metadata": {
        "id": "pokVnlLVjltv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_040.PNG)"
      ],
      "metadata": {
        "id": "GI0Dlh7Sl0uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_041.PNG)"
      ],
      "metadata": {
        "id": "fCcHmbYPm7eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_042.PNG)"
      ],
      "metadata": {
        "id": "NLnyS_9dm8tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_043.PNG)"
      ],
      "metadata": {
        "id": "rFOCcfCTn8LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"black\">*Green's Theorem: Exercise for 2D (Circulation & Flux)*"
      ],
      "metadata": {
        "id": "R40nIcrRpM0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/ig62HfJLCG0"
      ],
      "metadata": {
        "id": "kmwWODfmpZ2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My field is: $\\vec{F} = x^2 \\hat{i} + xy \\hat{j}$\n",
        "\n",
        "* with M = $x^2$ and N = xy\n",
        "\n",
        "My path around an area is a square: x = $\\pm 1$, y = $\\pm 1$\n",
        "\n",
        "Remember when you compute a double integral: inside first for x and then outside after for y\n",
        "\n",
        "How to get to the terms inside the double integral?\n",
        "\n",
        "* Circulation-Curl Form: $\\oint_{C} \\vec{F} \\cdot d \\vec{r}=\\iint_{R}\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right) d x d y$\n",
        "\n",
        "* Divergence Flux Form: $\\oint_{C} \\vec{F} \\cdot \\vec{n} d s=\\iint_{R}\\left(\\frac{\\partial M}{\\partial x}+\\frac{\\partial N}{\\partial y}\\right) d x d y$"
      ],
      "metadata": {
        "id": "pgHM5j6EqjIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's calculate the circulation:"
      ],
      "metadata": {
        "id": "m6gcyW8UsM0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_044.PNG)"
      ],
      "metadata": {
        "id": "Z-Ddy5-OqNsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example has 0 circulation:"
      ],
      "metadata": {
        "id": "kE3nuzy9qJba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_045.PNG)"
      ],
      "metadata": {
        "id": "Uk-C1apQqf7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate the flux - and it's also 0 in this case:"
      ],
      "metadata": {
        "id": "y-2Cyuk9sQKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_046.PNG)"
      ],
      "metadata": {
        "id": "vNTvwRJpsTVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Stoke's Theorem*"
      ],
      "metadata": {
        "id": "521_dSZMrpGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Stokes Theorem is a generalization of Green's Theorem on Curl or Circulation (Flow)"
      ],
      "metadata": {
        "id": "UABWYGES6lea"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y64AmnhcNBpb"
      },
      "source": [
        "[Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes)\n",
        "\n",
        "* sehr grundlegenden Satz √ºber die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "*  eine Verbindungslinie von der Differentialgeometrie zur Algebraischen Topologie er√∂ffnet.\n",
        "\n",
        "* Dieser Zusammenhang wird durch den Satz von de Rham beschrieben, f√ºr den der Satz von Stokes grundlegend ist.\n",
        "\n",
        "* Im Folaenden ist $n=3$ und es wird die Schreibweise mit Mehrfachintegralen\n",
        "verwendet.\n",
        "\n",
        "Das qeschlossene Kurvenintegral einer vektoriellen Gr√∂√üe (rechte Seite) kann\n",
        "mittels der Rotation in ein Fl√§chenintegral √ºber eine von dem geschlossenen\n",
        "Integrationsweg $\\Gamma=\\partial A$ berandete, nicht notwendig ebene Fl√§che\n",
        "umgewandelt werden (linke Seite). Dabei werden - wie auch beim\n",
        "Gau√ü'schen Satz - die gew√∂hnlichen Orientierungseigenschaften\n",
        "vorausgesetzt. Es gilt:\n",
        "\n",
        "> $\n",
        "\\iint_{A} \\operatorname{rot} \\vec{F} \\cdot \\mathrm{d} \\vec{A}=\\oint_{\\Gamma=\\partial A} \\vec{F}(\\vec{r}) \\cdot \\mathrm{d} \\vec{r}\n",
        "$\n",
        "\n",
        "Der Vektor $\\mathrm{d} \\vec{A}$ ist gleich dem Betrag der zur betrachteten Fl√§che $A$ bzw. zu\n",
        "$\\partial V$ geh√∂renden infinitesimalen Fl√§chenelemente multipliziert mit dem\n",
        "zugeh√∂rigen Normalenvektor. Auf der rechten Seite wird durch das\n",
        "Kreissymbol im Integralzeichen daran erinnert, dass √ºber eine geschlossene\n",
        "Kurve integriert wird."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation Video: https://youtu.be/0UvNF_cfBJ4"
      ],
      "metadata": {
        "id": "tHsrwz4u1F1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Calculation: https://youtu.be/ms4JjH0BANU"
      ],
      "metadata": {
        "id": "Be-b2uMuENE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: What is the circulation around the boundary??**"
      ],
      "metadata": {
        "id": "CVSNaucG13gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* it is influenced by the curling at each point along the boundary\n",
        "\n",
        "* the tendency to curl within the surface is given by (cross product of del with F vector field) and inner product of that with the normal vector at each point"
      ],
      "metadata": {
        "id": "G25WFhk52AjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_088.jpg)"
      ],
      "metadata": {
        "id": "ybAwanWO65Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green's theorem is a 2D version of Stokes Theorem:**"
      ],
      "metadata": {
        "id": "ZfIpFEUd7Nvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we are rewriting Green's theorem (upgrade it):"
      ],
      "metadata": {
        "id": "Zz3RszXq7VRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_089.jpg)"
      ],
      "metadata": {
        "id": "j2fLnqmP9P4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what happens (reminder! Look at Differentialoperator -> Curl (Rotation)):"
      ],
      "metadata": {
        "id": "a8JU7bA59YKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_090.jpg)"
      ],
      "metadata": {
        "id": "gVEgq_3g9TzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stoke‚Äòs Theorem (difference to Green):**\n",
        "\n",
        "* First difference is in the visualization: my surface is now a 3D surface versus a 2D surface, and also the vector field is 3 dimensional. But it‚Äòs still the circulation around that boundary curve.\n",
        "\n",
        "* The right side of the equation: instead of having the curl dotted with k hat, it‚Äòs the curl dotted with the normal vector. That‚Äòs because as you move to different places on the surface, your normal vectors are different things (it‚Äòs not just k hat everywhere as it was before).\n",
        "\n",
        "* Instead if Green‚Äòs Theorem where I was taking a little area down in the x y area (2D flat space), this is now a surface integral d sigma as opposed to dx dy or da as we had before."
      ],
      "metadata": {
        "id": "ozx5u2-wANlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_091.jpg)"
      ],
      "metadata": {
        "id": "-EBBVQeLA2ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditions on Stokes Theorem: For $S$ a piecewise smooth **oriented** surface with piecewise smooth boundary $C$ and $\\vec{F}$ a field with **continuous first partials** for each component on an open region containing $S$"
      ],
      "metadata": {
        "id": "pr_WvUxvA-BA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Divergence Theorem (Volumenintegral / Gau√üscher Integralsatz)*"
      ],
      "metadata": {
        "id": "LxkcVzsECAEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See also Volume Form: https://en.m.wikipedia.org/wiki/Volume_form"
      ],
      "metadata": {
        "id": "JdmqQyXMp5fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Verallgemeinerung des Green'schen Theorem on Divergence"
      ],
      "metadata": {
        "id": "Vr71t5GCJgwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deutsch: [Gau√üscher Integralsatz](https://de.m.wikipedia.org/wiki/Gau√üscher_Integralsatz)"
      ],
      "metadata": {
        "id": "1OPORiZkJytg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In vector calculus, the [divergence theorem](https://en.m.wikipedia.org/wiki/Divergence_theorem), also known as Gauss's theorem or Ostrogradsky's theorem, is a theorem which relates the flux of a vector field through a closed surface to the divergence of the field in the volume enclosed.\n",
        "\n",
        "> More precisely, **the divergence theorem states that the surface integral of a vector field over a closed surface, which is called the flux through the surface, is equal to the [volume integral](https://en.m.wikipedia.org/wiki/Volume_integral) of the divergence over the region inside the surface**.\n",
        "\n",
        "* Intuitively, it states that the sum of all sources of the field in a region (with sinks regarded as negative sources) gives the net flux out of the region.\n",
        "\n"
      ],
      "metadata": {
        "id": "__Wa7KUKTZBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beispiel: elektrostatische Verbindung (von Ionenbindung): https://de.m.wikipedia.org/wiki/Elektrostatik"
      ],
      "metadata": {
        "id": "7_eS8JlQc6oR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stokes and Greens theorem on manifolds (derivative and exterior derivative): https://youtu.be/1lGM5DEdMaw"
      ],
      "metadata": {
        "id": "Lrkk8P-S7GUP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-ifFM9ZKy3p"
      },
      "source": [
        "[Integralsatz](https://de.wikipedia.org/wiki/Integralsatz) ist ein Namensbestandteil bestimmter mathematischer S√§tze, in deren Aussage ein Integral vorkommt.\n",
        "\n",
        "Unter dem Begriff der klassischen Integrals√§tze werden der **Satz von Gau√ü, der Satz von Green, der Satz von Stokes** und einige ihrer Spezialf√§lle zusammengefasst. Diese S√§tze der Vektoranalysis h√§ngen eng miteinander zusammen: der Integralsatz von Stokes umfasst die anderen beiden S√§tze als Spezialf√§lle.\n",
        "\n",
        "Au√üerdem gibt es neben den klassischen Integrals√§tzen noch weitere S√§tze, die man kurz als Integrals√§tze bezeichnet. Zu diesen z√§hlt beispielsweise der cauchysche Integralsatz, der ein zentrales Resultat aus der Funktionentheorie ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBGhKHdBNUv2"
      },
      "source": [
        "[Integralsatz von Gau√ü](https://de.m.wikipedia.org/wiki/Gau√üscher_Integralsatz)\n",
        "\n",
        "* Im Folgenden sei das ‚ÄûIntegrationsvolumen‚Äú V n-dimensional.\n",
        "\n",
        "* Das [Volumenintegral](https://de.m.wikipedia.org/wiki/Volumenintegral) √ºber den Gradienten einer skalaren Gr√∂√üe $\\phi$, kann dann in ein [Oberfl√§chenintegral](https://de.m.wikipedia.org/wiki/Oberfl√§chenintegral) (bzw. Hyperfl√§chenintegral) √ºber den Rand dieses Volumens umgewandelt werden:\n",
        "\n",
        "> $\n",
        "\\int_{V} \\operatorname{grad} \\phi(\\vec{x}) \\mathrm{d} V=\\oint_{\\partial V} \\phi \\mathrm{d} \\vec{A}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation Video: https://youtu.be/pY4t-ikhzhU"
      ],
      "metadata": {
        "id": "p2XMsEq3KIvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_092.jpg)"
      ],
      "metadata": {
        "id": "919T0tZTGPSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplify with the del operator:\n",
        "\n",
        "$\\nabla=\\frac{\\partial}{\\partial x} \\hat{\\imath}+\\frac{\\partial}{\\partial y} \\hat{\\jmath}+\\frac{\\partial}{\\partial z} \\hat{k}$\n",
        "\n",
        "$\\nabla \\cdot \\vec{F}=\\frac{\\partial M}{\\partial x}+\\frac{\\partial N}{\\partial y}+\\frac{\\partial P}{\\partial z}$\n",
        "\n",
        "$=D i v \\vec{F}$"
      ],
      "metadata": {
        "id": "KjZxu7CtIfql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_093.jpg)"
      ],
      "metadata": {
        "id": "DeV78PDdIasN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dV kann zB sein: dx dy dz"
      ],
      "metadata": {
        "id": "u_U5SMz3MR96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditions: For $S$ a piecewise smooth eriented closed surface, with piecewise smooth boundary $C$ and $\\vec{F}$ a field with continuous first partials for each component on an open region containing $S$"
      ],
      "metadata": {
        "id": "jas98aejLTZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Calculation: https://youtu.be/E7RUu1K8UDM"
      ],
      "metadata": {
        "id": "9p98Hka0Lete"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_094.jpg)"
      ],
      "metadata": {
        "id": "FN-EgLptNMJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Example calculation: [Divergence Theorem for regions bounded by two surfaces](https://youtu.be/RZE7iB71X7o) (Inside and outside surface, like inner sphere and outer sphere)"
      ],
      "metadata": {
        "id": "oKLPgwpyNfYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Example calculation: [Deriving Gauss's Law for Electric Flux via the Divergence Theorem from Vector Calculus](https://youtu.be/8PmIarVZmc8) (see here some background: https://de.m.wikipedia.org/wiki/Gau√üsches_Gesetz)"
      ],
      "metadata": {
        "id": "Yc1hw-XtO2Y6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwK1ZsvrPIY"
      },
      "source": [
        "<font color=\"black\">**Exkurs: Volumenintegrale**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Volume integrals](https://en.m.wikipedia.org/wiki/Volume_integral) are especially important in physics for many applications, for example, to calculate flux densities."
      ],
      "metadata": {
        "id": "xENPxUiWXnJr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llLY5ZbxPR5j"
      },
      "source": [
        "* z.B. zur Berechnung des Volumeninhaltes eines Objektes\n",
        "\n",
        "* Das [Volumenintegral](https://de.wikipedia.org/wiki/Volumenintegral) erweitert das Oberfl√§chenintegral auf die Integration √ºber ein beliebiges dreidimensionales Integrationsgebiet, wobei eine Funktion dreimal hintereinander integriert wird, jeweils √ºber eine Richtung eines dreidimensionalen Raumes.\n",
        "\n",
        "* **Dabei muss es sich jedoch nicht notwendigerweise um ein Volumen eines geometrischen K√∂rpers handeln**.\n",
        "\n",
        "* Zur vereinfachten Darstellung wird oft nur ein einziges Integralzeichen geschrieben und die Volumenintegration lediglich durch das Volumenelement $\\mathrm {d} V$ angedeutet:\n",
        "\n",
        "> $\\iiint_{V} f(r) d^{3} r=\\int_{V} f(\\vec{x}) \\mathrm{d} V$\n",
        "\n",
        "* wobei die zu integrierende Funktion zumindest von drei Variablen ${\\vec {x}}=(x,y,z)$ f√ºr eine (kartesische) Beschreibung im dreidimensionalen Raum $\\mathbb{R^3}$ abh√§ngt, **es sind aber auch h√∂herdimensionale R√§ume m√∂glich**.\n",
        "\n",
        "* Es handelt sich um ein **skalares Volumenintegral**, wenn der Integrand $f$ und das Volumenelement $\\mathrm{d} V$ skalar sind. Bei einem **vektoriellen Integranden**, z. B. einem Vektorfeld $\\vec{f}$, ist auch das Volumenelement $\\mathrm{d} \\vec{V}$ ein Vektor, sodass sich ein vektorielles Volumenintegral ergibt.\n",
        "\n",
        "* Um ein Volumenintegral zu berechnen, ist meist eine Parametrisierung des Integrationsgebiets n√∂tig."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jX_Bp75Nurq"
      },
      "source": [
        "**Volumenform**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy2PCJfRNxDB"
      },
      "source": [
        "* Eine [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) ist ein mathematisches Objekt, welches zur Integration √ºber Raumbereiche ben√∂tigt wird, insbesondere bei der Verwendung spezieller Koordinatensysteme, also ein Spezialfall eines Volumens.\n",
        "\n",
        "* In der Physik und im Ingenieurwesen sind auch Bezeichnungen wie infinitesimales Volumenelement oder Ma√üfaktor gebr√§uchlich.\n",
        "\n",
        "* Beispiele in 3 Dimensionen:\n",
        "\n",
        "  * [Kartesische Koordinaten](https://de.m.wikipedia.org/wiki/Kartesisches_Koordinatensystem): $\\mathrm{d} V=\\mathrm{d} x \\cdot \\mathrm{d} y \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Zylinderkoordinaten](https://de.m.wikipedia.org/wiki/Polarkoordinaten#Zylinderkoordinaten) (ebene Polarkoordinaten um eine dritte Koordinate erg√§nzt): $\\mathrm{d} V=\\rho \\cdot \\mathrm{d} \\rho \\cdot \\mathrm{d} \\varphi \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Kugelkoordinaten](https://de.m.wikipedia.org/wiki/Kugelkoordinaten): $\\mathrm{d} V=r^{2} \\cdot \\sin \\theta \\cdot \\mathrm{d} r \\cdot \\mathrm{d} \\theta \\cdot \\mathrm{d} \\varphi$\n",
        "\n",
        "* Das Volumenelement in drei Dimensionen l√§sst sich nach dem [Transformationssatz](https://de.m.wikipedia.org/wiki/Transformationssatz) mit Hilfe der [Funktionaldeterminante](https://de.m.wikipedia.org/wiki/Funktionaldeterminante) det $J$ berechnen. Die [Jacobi-Matrix](https://de.m.wikipedia.org/wiki/Jacobi-Matrix) f√ºr die Transformation von den Koordinaten $\\left\\{x_{1}, x_{2}, x_{3}\\right\\}$ zu $\\left\\{x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right\\}$ ist hierbei definiert durch\n",
        "\n",
        ">$\n",
        "J=\\frac{\\partial\\left(x_{1}, x_{2}, x_{3}\\right)}{\\partial\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right)}\n",
        "$\n",
        "\n",
        "* Das Volumenelement ist dann gegeben durch\n",
        "\n",
        ">$\n",
        "\\mathrm{d} V^{\\prime}=|\\operatorname{det} J| \\mathrm{d} x_{1}^{\\prime} \\mathrm{d} x_{2}^{\\prime} \\mathrm{d} x_{3}^{\\prime}\n",
        "$\n",
        "\n",
        "* Aus mathematischer Sicht ist **eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$**. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird [Riemann'sche Volumenform](https://de.m.wikipedia.org/wiki/Hodge-Stern-Operator#Riemannsche_Volumenform) genannt (Hodge-Stern-Operator in der Differentialgeometrie)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">*Summary & Unifying Principle*"
      ],
      "metadata": {
        "id": "xcWGR3zwT-Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Unifying Principle: Integrating a differential operator acting on a field over a domain is the same as adding the field components along the boundary (local to global transition!)**"
      ],
      "metadata": {
        "id": "7wCvJGyaUTuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [A unified view of Vector Calculus (Stoke's Theorem, Divergence Theorem & Green's Theorem)](https://m.youtube.com/watch?v=PIoqMNL7tV0&feature=youtu.be)"
      ],
      "metadata": {
        "id": "kImQcAQMVsdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamental Theorem of Calculus: If $f(x)$ differentiable on $[a, b]$\n",
        "\n",
        ">$\n",
        "\\int_a^b f^{\\prime}(x) d x=f(b)-f(a)\n",
        "$"
      ],
      "metadata": {
        "id": "fVsowGpyUm48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_097.jpg)"
      ],
      "metadata": {
        "id": "Ddm461kwVgkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_095.jpg)"
      ],
      "metadata": {
        "id": "691EMDoJUEQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_096.jpg)"
      ],
      "metadata": {
        "id": "_l0OHdTfVfGz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VI_FXm0fQLDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJlRjb0F8ULh"
      },
      "source": [
        "##### <font color=\"blue\">*Tensor Calculus*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOpIknui8ULm"
      },
      "source": [
        "###### *Differential Geometry of Smooth Surfaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential geometry of smooth surfaces**\n",
        "\n",
        "* Wikipedia: [First and second fundamental forms, the shape operator, and the curvature](https://en.m.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature)\n",
        "\n",
        "* Artikel: [A-quick-and-dirty-introduction-to-the-curvature-of-surfaces](http://wordpress.discretization.de/geometryprocessingandapplicationsws19/a-quick-and-dirty-introduction-to-the-curvature-of-surfaces/)\n",
        "\n",
        "* [Principal curvatures](https://en.m.wikipedia.org/wiki/Principal_curvature)\n",
        "\n",
        "* [Gauss map](https://en.wikipedia.org/wiki/Gauss_map) provides a mapping from every point on a curve or a surface to a corresponding point on a unit sphere\n",
        "\n",
        "* [Shape operator](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature) or Weingarten map\n",
        "\n",
        "* [Normal plane](https://en.wikipedia.org/wiki/Normal_plane_(geometry)): a normal vector that is at right angles to the surface \n",
        "\n",
        "* [Normal section](https://en.wikipedia.org/wiki/Normal_plane_(geometry)#Normal_section): Intersection of normal plane and  surface (*For most points on most surfaces, different normal sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures, call these Œ∫1, Œ∫2*)\n",
        "\n",
        "* **Extrinsic Properties**: Rely on an embedding of a surface in Euclidean space. Illustrated by the non-linear [Euler‚ÄìLagrange equations](https://en.m.wikipedia.org/wiki/Euler‚ÄìLagrange_equation) in [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations). Lagrange: [minimal surfaces](https://en.m.wikipedia.org/wiki/Minimal_surface) (can only be defined in terms of an embedding).\n",
        "\n",
        "* **Intrinsic Properties / [Gaussian curvature](https://en.m.wikipedia.org/wiki/Gaussian_curvature)**: Geometric properties determined by geodesic distances (not embeddings). \n",
        "\n",
        "  * Gaussian curvature of a surface = how curves on the surface change directions in three dimensional space\n",
        "\n",
        "  * An important role play Lie groups, namely the [symmetry groups](https://en.m.wikipedia.org/wiki/Symmetry_group) of the Euclidean plane, the sphere and the hyperbolic plane.  **These Lie groups can be used to describe surfaces of constant Gaussian curvature**; they also provide an essential ingredient in the modern approach to intrinsic differential geometry through [connections](https://en.m.wikipedia.org/wiki/Connection_(mathematics)).\n",
        "\n",
        "  * [Gaussian curvature](https://en.wikipedia.org/wiki/Gaussian_curvature) is an intrinsic invariant, i.e. invariant under local [isometries](https://en.m.wikipedia.org/wiki/Isometry). This point of view was extended to higher-dimensional spaces by Riemann and led to what is known today as [Riemannian geometry](https://en.m.wikipedia.org/wiki/Riemannian_geometry).\n",
        "\n",
        "* [First fundamental form](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature): Gaussian curvature can be calculated from the first fundamental form => curvature and metric properties of a surface such as length and area, called [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) of surface.\n",
        "\n",
        "> $\\mathrm{I}(x, y)=\\langle x, y\\rangle .$\n",
        "\n",
        "* [Second fundamental form](https://en.wikipedia.org/wiki/Second_fundamental_form) encodes how lengths and angles of curves on the surface are distorted when the curves are pushed off of the surface. Together with First fundamental form, it serves to define extrinsic invariants of the surface, **its principal curvatures**.\n",
        "\n",
        ">$L d x^{2}+2 M d x d y+N d y^{2}$ (quadratic from ins 3D)\n",
        "\n",
        "* [Riemann curvature tensor](https://en.m.wikipedia.org/wiki/Riemann_curvature_tensor): Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VJjblo9jZJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1443.png)\n"
      ],
      "metadata": {
        "id": "dcgQer6SUB0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1444.png)\n"
      ],
      "metadata": {
        "id": "TZkbzLiaUAzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jj](https://raw.githubusercontent.com/deltorobarba/repo/master/gaussiancurvature.png)\n"
      ],
      "metadata": {
        "id": "WHI-KoMUT_dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Definition of second fundamental form*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/5/5e/Second_fundamental_form.svg)\n",
        "\n",
        "[Source](https://www.researchgate.net/figure/Measures-of-surface-curvature-a-The-principal-curvatures-are-calculated-from-the_fig6_321165318)\n"
      ],
      "metadata": {
        "id": "xCMvunOBT9v1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTS59KF68ULu"
      },
      "source": [
        "**Levi-Civita-Zusammenhang (Connection)**\n",
        "\n",
        "* Der [Levi-Civita-Zusammenhang](https://de.wikipedia.org/wiki/Levi-Civita-Zusammenhang#Hauptsatz_der_riemannschen_Geometrie) ist ein wesentliches Hilfsmittel zum Aufbau der riemannschen Kr√ºmmungstheorie. Denn der Kr√ºmmungstensor wird mit Hilfe eines Zusammenhangs definiert, daher bietet es sich an, in der riemannschen Geometrie den eindeutig ausgezeichneten Levi-Civita-Zusammenhang f√ºr die Definition des riemannschen Kr√ºmmungstensors zu verwenden.\n",
        "\n",
        "* Der [Zusammenhang](https://de.wikipedia.org/wiki/Zusammenhang_(Differentialgeometrie)) (Connection) ist in der Differentialgeometrie ein Hilfsmittel, um Richtungs√§nderungen im Laufe einer Bewegung zu quantifizieren und Richtungen in verschiedenen Punkten miteinander in Beziehung zu setzen.\n",
        "\n",
        "* Dieser Artikel behandelt im Wesentlichen den Zusammenhang auf einer differenzierbaren Mannigfaltigkeit beziehungsweise auf einem [Vektorb√ºndel](https://de.wikipedia.org/wiki/Vektorb√ºndel). Ein ausgezeichneter Zusammenhang auf einem Tensorb√ºndel, einem besonderen Vektorb√ºndel, hei√üt kovariante Ableitung. \n",
        "\n",
        "* Allgemeiner existieren auch [Zusammenh√§nge auf Prinzipalb√ºndeln](https://de.wikipedia.org/wiki/Zusammenhang_(Prinzipalb√ºndel)) mit analogen definierenden Eigenschaften.\n",
        "\n",
        "* In der Differentialgeometrie interessiert man sich f√ºr die Kr√ºmmung von Kurven, insbesondere von Geod√§ten. In euklidischen R√§umen ist die Kr√ºmmung einfach durch die zweite Ableitung gegeben. \n",
        "\n",
        "* **Auf differenzierbaren Mannigfaltigkeiten ist die zweite Ableitung nicht direkt zu bilden. Ist $\\gamma$ eine Kurve, so muss man f√ºr die zweite Ableitung dieser Kurve den Differenzenquotienten mit den Vektoren $\\gamma^{\\prime}(t)$ und $\\gamma^{\\prime}\\left(t_{0}\\right)$ bilden. Diese Vektoren befinden sich jedoch in unterschiedlichen Vektorr√§umen, daher kann man nicht einfach die Differenz der beiden bilden**. \n",
        "\n",
        "* **Um das Problem zu l√∂sen, hat man eine Abbildung definiert, welche man Zusammenhang nennt. Diese Abbildung soll einen Zusammenhang zwischen den beteiligten Vektorr√§umen bereitstellen und tr√§gt daher auch diesen Namen**.\n",
        "\n",
        "In diesem Abschnitt bezeichnet $M$ eine glatte Mannigfaltigkeit, $T M$ das Tangentialb√ºndel und $\\pi: E \\rightarrow M$ ein Vektorb√ºndel. Mit $\\Gamma(E)$ wird die Menge der glatten Schnitte im Vektorb√ºndel $E$ notiert.\n",
        "\n",
        "* **Indem man sagt, was die Richtungsableitung eines Vektorfeldes in Richtung eines Tangentialvektors ist, erh√§lt man einen Zusammenhang auf einer differenzierbaren Mannigfaltigkeit $M$**. Demgem√§√ü definiert man einen Zusammenhang auf einem Vektorb√ºndel als eine Abbildung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "\\nabla: \\Gamma(T M) \\times \\Gamma(E) & \\rightarrow \\Gamma(E) \\\\\n",
        "(X, s) & \\mapsto \\nabla_{X} s\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "* die einem Vektorfeld $X$ auf $M$ und einem Schnitt $s$ im Vektorb√ºndel $E$ wieder einen Schnitt in $E$ zuordnet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-V3Q8YM8ULv"
      },
      "source": [
        "**Vektorb√ºndel & Tensorb√ºndel**\n",
        "\n",
        "* [Vektorb√ºndel](https://de.wikipedia.org/wiki/Vektorb√ºndel) oder manchmal auch Vektorraumb√ºndel sind Familien von Vektorr√§umen, die **durch die Punkte eines topologischen Raumes parametrisiert sind**.\n",
        "\n",
        "* Vektorb√ºndel geh√∂ren damit auch zu den [Faserb√ºndeln](https://de.m.wikipedia.org/wiki/Faserb√ºndel). Remind: Faser ist ein Urbild von einem Element (\"Faser der Abbildung √ºber einem Element\") - surjektiv! kann also mehrere Elemente im Urbild haben. Daher Faser $\\mathbb{R}$<sup>2</sup> zu Punkt auf $\\mathbb{R}$ (siehe [hier](https://de.m.wikipedia.org/wiki/Vektorb√ºndel) die Illustration:\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Vectorbundle.svg/320px-Vectorbundle.svg.png)\n",
        "\n",
        "*Illustration des Vektorb√ºndels $(E, B, \\pi)$. Hier ist der **Totalraum** $E=\\mathbb{R}^{2}$ und der **Basisraum** $B=\\mathbb{R} .$ Die Abbildung $\\pi: E \\rightarrow B$ projiziert jede Gerade $E_{x}$ auf den Punkt $x$. Der Raum $E_{x}=\\{p \\in E \\mid \\pi(p)=x\\}$ wird **Faser √ºber $x$** genannt. Au√üerdem ist der Totalraum $E$ die Vereinigung aller Fasern.* (Comment: also Totalraum E ist Urbild mit Faser und Basisraum B ist Zielbild mit Element das von der Faser stammt)\n",
        "\n",
        "* [Tangentialb√ºndel](https://de.wikipedia.org/wiki/Tangentialb√ºndel)\n",
        "\n",
        "* [Tensorb√ºndel](https://de.m.wikipedia.org/wiki/Tensoranalysis#Tensorb√ºndel) ist ein bestimmtes Vektorb√ºndel. Tensorfelder sind dann besondere glatte Abbildungen, die in dieses Vektorb√ºndel hinein abbilden.\n",
        "\n",
        "* **Schnitt (Faserb√ºndel)**\n",
        "\n",
        "  * [Schnitte](https://de.m.wikipedia.org/wiki/Schnitt_(Faserb√ºndel)) sind Abbildungen, welche in der algebraischen Topologie, insbesondere in der Homotopietheorie, untersucht werden. Insbesondere interessiert man sich daf√ºr, unter welchen Bedingungen solche Abbildungen existieren. \n",
        "\n",
        "  * Das bekannteste Beispiel von Schnitten sind die [**Differentialformen**](https://de.m.wikipedia.org/wiki/Differentialform).\n",
        "\n",
        "  * Ein Schnitt kann als **Verallgemeinerung des Graphen einer Funktion** aufgefasst werden. \n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bundle_section.svg/187px-Bundle_section.svg.png)\n",
        "\n",
        "*Die Abbildung s ist ein Schnitt in einem Faserb√ºndel $p: E \\rightarrow B$. Dieser Schnitt s erlaubt es, den Basisraum $B$ mit dem Teilraum $s(B)$ von $E$ zu identifizieren.*\n",
        "\n",
        "* Die [Schnittkr√ºmmung](https://de.wikipedia.org/wiki/Schnittkr√ºmmung) ist eine Gr√∂√üe der riemannschen Geometrie, eines Teilgebiets der Mathematik. Mit ihrer Hilfe kann man die Kr√ºmmung einer n-dimensionalen riemannschen Mannigfaltigkeit beschreiben. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUvl0oGH8ULh"
      },
      "source": [
        "###### *Geodesics in curved space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEwWL-tS8ULh"
      },
      "source": [
        "Video: [Tensor Calculus 15: Geodesics and Christoffel Symbols (extrinsic geometry)](https://www.youtube.com/watch?v=1CuTNveXJRc)\n",
        "\n",
        "Video: [Tensor Calculus 16: Geodesic Examples on Plane and Sphere](https://www.youtube.com/watch?v=8sVDceI70HM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBYhjtwK8ULh"
      },
      "source": [
        "> **Geodesic curve: In curved space, a straight path has zero tangential acceleration when we travel along it at constant speed.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_153.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIiXHmpq8ULi"
      },
      "source": [
        "**This means that the tangential component of the acceleration vector has to be zero**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_154.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smSfoh6s8ULi"
      },
      "source": [
        "*To get acceleration vector, we take the derivative of the velocity vector (like second derivative of position):*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_155.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This gives us the Second Fundamental Form:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_156.png)"
      ],
      "metadata": {
        "id": "SKBEJ9FGSw9d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIyt9bB8ULi"
      },
      "source": [
        "*And the Christoffel symbols (capital gamma) give us the tangential components of the second order derivative vector = acceleration (at a point in a curved space), so how much of each of the basis vectors $u$ we need:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_157.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIgivMBq8ULj"
      },
      "source": [
        "Here is our 3D vector bases with the surface normal vector and two tangent vectors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_158.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PszNj4ag8ULj"
      },
      "source": [
        "This is the second order derivative $\\frac{\\partial^{2} \\vec{R}}{\\partial u^{i} \\partial u^{j}}$ that we want (lila vector) and the three component $\\Gamma_{i j}^{1}$, $\\Gamma_{i j}^{2}$ and $L_{i j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_159.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svmYLd_88ULj"
      },
      "source": [
        "*The equator is a geodesic curve, and the shortest one at any given point there:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_178.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9WVFP5u8ULj"
      },
      "source": [
        "This makes sense: if we travel around thr equator our acceleration vector points directly inward towards the center of the circle which is also the center of the sphere. Because remember: \n",
        "\n",
        "> <font color=\"blue\">**To compute geodesic curves, we need to find curves where the acceleration vector is normal to the surface.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_179.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm8pGqaq8ULj"
      },
      "source": [
        "*So setting the following framed part on the bottom to zero is an effective way to detect whether a curve is a geodesic*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_160.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHo92aHs8ULj"
      },
      "source": [
        "*So setting the tangential component equal to zero gives us the geodesic equation. Any curve parameterized by lambda which satisfies this geodesic equation is in fact a geodesic curve.* \n",
        "\n",
        "> **And geodesic curves are the paths that beams of light will follow in curved spacetime.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_161.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXjnQUuX8ULj"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_162.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_163.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjFey3T8ULx"
      },
      "source": [
        "###### *Covariant Derivative in Flat Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVN1fZ2C8ULx"
      },
      "source": [
        "> **Covariant Derivative = understanding the rate of change of vector (tensor) fields that takes changing basis vectors into account**\n",
        "\n",
        "* Video: [Tensor Calculus 17: The Covariant Derivative (flat space)](https://www.youtube.com/watch?v=U5iMpOn5IHw&t=4s)\n",
        "\n",
        "Challenge: different sources define covariant derivative in different ways.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_164.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-KPyG8R8ULx"
      },
      "source": [
        "*Reminder about Cartesian and polar coordinate system: <font color=\"blue\">the basis vectors in each system are equivalent to the partial derivatives of a position vector $R$ with respect to the coordinate variables (x and y, and r and $\\theta$) (see blue and orange box)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_165.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtobyFO8ULx"
      },
      "source": [
        "**Consider following two examples:**\n",
        "\n",
        "* the in the first picture the vector field is constant everywhere. hence the deriative of this vector field is zero in the x and y direction.\n",
        "\n",
        "* in the second image below the vector field is moving. So the vector field is NOT zero in the $r$ and $\\theta$ direction. The components 2 and 1 are constant, but the basis vectors $e_r$ and $e_\\theta$ are changing from point to point that causes the vectors in the vector field to change length and direction. \n",
        "\n",
        "> **Constant Components ‚â† Constant Vector Field**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_167.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_168.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaH6P4Hj8ULx"
      },
      "source": [
        "*Summary for calculating covariant derivative in flat space*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_169.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpj8KK6r8ULy"
      },
      "source": [
        "*Replace with Christoffel symbol, it goes to zero in cartesian flat space*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_170.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_172.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxVbxzIk8ULy"
      },
      "source": [
        "*Extension: Covariant Derivative (Component Definition)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_173.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_174.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6SGVoL8ULy"
      },
      "source": [
        "*The covariant derivative of a vector field is a new vector field that depends on the direction of differentiation. So if we look at the two main directions in 2D space, we get actually 2 vector fields for the covariant derivative (one for each covariant derivative in the main coordinate directions).*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_175.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO7gX8dd8ULy"
      },
      "source": [
        "*The covariant derivative components with the semicolon form a (1,1) tensor: one contravatiant transformation rule and one covariant transformation rule (where the inverse Jacobian transforms the contravariant index, and the Jacobian transforms the covariant index.)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_177.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if8_02pL8ULy"
      },
      "source": [
        "###### *Covariant Derivative in Curved Space Definition (extrinsic)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVok3GM8ULy"
      },
      "source": [
        "Video: [Tensor Calculus 18: Covariant Derivative (extrinsic) and Parallel Transport](https://youtu.be/Af9JUiQtV1k)\n",
        "\n",
        "* Parallel Transport plays an important role\n",
        "\n",
        "* Objective: Dealing with rates of change of vector fields on curved surfaces\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_180.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD-5x3MW8ULz"
      },
      "source": [
        "On a flat surface it's clear: the first vector field is constant (because all arrows have the same length and direction) and the covariant derivative is zero, and the second vector field  is not constant and the covariant derivative is non-zero (because the arrows change length and direction):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_181.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAB5IBRU8ULz"
      },
      "source": [
        "When we say it's constant, we mean all vectors are the same. But how we know they are the same? We can out them on top:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJwPDQA8ULz"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_182.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhG99DQv8ULz"
      },
      "source": [
        "That unfortunately doesn't work on curved space: arrows that seems to point in the same direction (red and blue) are in fact pointing in different directions. The two blue arrows are the same, but look like pinting into different directions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_183.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3NWyG1d8ULz"
      },
      "source": [
        "You could parallel transport them, but there is one issue: if you transport them around, they end up showing in different directions. This means:\n",
        "\n",
        "> **A constant vector field on a curved surface makes no sense from the perspective of someone living on it.**\n",
        "\n",
        "We need to live with it, that it changes direction afetr a parallel transport. It's the best we can do.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_184.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGZVBXy18ULz"
      },
      "source": [
        "Parallel transport means we keep the arrow as constant as possible. But how do we describe this constance mathematically? Look at the green arrows that show the difference: **When we paralle transport a vector, the rate of change of that vector always points at a direction that;s normal to a surface.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_185.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1E7-4QV8UL1"
      },
      "source": [
        "> **So if we parallel transport this vector along a path on the curved surface, we end up with a vector field defined at every point on the curve.**\n",
        "\n",
        "The rate of change if the vector field will always be some normal vector $\\vec{n}$ pointing out of the surface. \n",
        "\n",
        "> $\\frac{d \\vec{v}}{d \\lambda}=\\vec{n}$\n",
        "\n",
        "In other words: if we take the rate of change of the vector field and subtract the normal part we'll always get zero:\n",
        "\n",
        "> $\\frac{d \\vec{v}}{d \\lambda}-\\vec{n}=\\overrightarrow{0}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_187.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gZx5TQ8UL1"
      },
      "source": [
        "And here come the covariant derivatives:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_188.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TASl8lsP8UL1"
      },
      "source": [
        "**Example: Curve parameter $\\lambda$ (in yellow) with a vector field (lila) along this curve, and we want to find the rate of change of the vector as we travel along the curve.**  $\\nabla_{\\vec{w}} \\vec{v}=\\overrightarrow{0}$ means the vector $\\vec{v}$ is parallel transported in the direction $\\vec{W}$.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_189.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DPyAvxc8UL1"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_191.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cJKTnF8UL1"
      },
      "source": [
        "Now with the exception that we change v and w with u1 and u2, where u1 and u2 mean different things:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_192.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79XzInS8UL1"
      },
      "source": [
        "U1 curves: Longitude from north to south. U2 curves: Latitudes from west to east\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_193.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MCk9RnY8UL1"
      },
      "source": [
        "The tangent basis vectors of the sphere are given by these two partial derivatives of a position vector (which are sometimes written as the basis vectors e1 and e2). Where the e1 basis vectors are just the tangent vectors along the u1 curves, and the e2 basis vectors are just the tangent vectors along the u2 curves:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_190.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODp46WyO8UL2"
      },
      "source": [
        "> **The covariant derivative is just the ordinary derivative with the normal component substratected!** $\\nabla \\frac{\\partial}{\\partial u^{i}} \\vec{v}=\\frac{\\partial \\vec{v}}{\\partial u^{i}}-\\vec{n}$\n",
        "\n",
        "*Let's work on an example: Let's take some tangent vector field V on the sphere in the direction of the $u^I$ coordinate (and we deal with a vector field where all the vectors are tangent to the surface -0 happens in physics very often).*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_197.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl7-7EtW8UL2"
      },
      "source": [
        "* Since the $\\vec{v}$ are always tangent to the sphere, we can always expand it as a linear combination of the $e_1$ and $e_2$ basis tangent vectors\n",
        "\n",
        "* Product rule: so this derivative $\\left[\\begin{array}{l}v^{j} \\overrightarrow{e_{j}}\\end{array}\\right]$ becomes two terms: $\\frac{\\partial v^{j}}{\\partial u^{i}} \\overrightarrow{e_{j}}+v j \\frac{\\partial \\overrightarrow{e_{j}}}{\\partial u^{i}}$\n",
        "\n",
        "\t* $\\frac{\\partial v^{j}}{\\partial u^{i}} \\overrightarrow{e_{j}}$  where we differentiate the vector field components\n",
        "\n",
        "\t* $v j \\frac{\\partial \\overrightarrow{e_{j}}}{\\partial u^{i}}$ where we differentiate the basis vectors\n",
        "  \n",
        "  \n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_196.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8erNDHs8UL2"
      },
      "source": [
        "And we can rewrite it as following:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_195.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_t3fs5f8UL2"
      },
      "source": [
        "**See the following image:** \n",
        "\n",
        "* 3D basis consisting of 2 tangent basis vectors $e$ and the normal vector $n$ to the surface\n",
        "\n",
        "* Lila is the derivative vector that we want to  expand in a linear combination\n",
        "\n",
        "* the two Christoffel symbols tell us how much of the $e_1$ and $e_2$ basis vectors we need\n",
        "\n",
        "* the second fundamental form $L_{ij}$ tells us how much of the normal vector we need\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_194.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFFVYaS8UL2"
      },
      "source": [
        " The normal component $\\vec{n}$ is always zero in flat space!\n",
        " \n",
        " ![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_198.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6BwNND8UL3"
      },
      "source": [
        "Summary\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_199.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZECPGokG8UL3"
      },
      "source": [
        "###### *Covariant Derivative in Curved Space Definition (Intrinsic)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g21QIodZ8UL3"
      },
      "source": [
        "Video: [Tensor Calculus 19: Covariant Derivative (Intrinsic) and Geodesics](https://www.youtube.com/watch?v=EFKBp52LtDM)\n",
        "\n",
        "* Geodesics play an important role!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_200.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv81SnEl8UL3"
      },
      "source": [
        "We can't compute the Christoffel symbol like this anymore on instrinsic space, because we expanded the basis vectors and the basis vectors derivatives in terms of the x,y,z basis. And this let us compute the dot products to get the Christoffel.\n",
        "\n",
        "But now we have to pretend the x,y,z variables don't exist and we can only see the coordinate lines directly on the surface.\n",
        "\n",
        "This is what we have to give up now on instrinsic space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_201.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp_YZBBd8UL3"
      },
      "source": [
        "We don't have an origin that we can use to extend position vectors from since space is curved. **So we can no longer think of basis vectors as derivatives of a position vector $R$. Instead in instrinsic geometry the basis vectors we are using are the partial derivative operators themselves (they point in a specific direction)** \n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_202.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLeGdtpC8UL3"
      },
      "source": [
        "> **And these new basis vectors live in the called \"Tangent Vector Space $T_pM$ at some given point p. And there is a different Tangent Vector Space at every point p on the surface:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_203.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TeDMHCW8UL3"
      },
      "source": [
        "So how we calculate the covariant derivative for intrinsically curved space?\n",
        "\n",
        "* We can't use the normal vector anymore to substract it from the ordinary derivative ($\\nabla \\frac{\\partial}{\\partial u^{i}} \\vec{v}=\\frac{\\partial \\vec{v}}{\\partial u^{i}}-\\vec{n}$) so we get the covariant derivative. Because there is no outside anymore.\n",
        "\n",
        "* which means we ignore the normal part (just like in the flat space exmaple before).\n",
        "\n",
        "So we can compute the formula for the covariant derivative but at the end we have the Christoffel symbols, that we cannot calculate with 'external' coordinates:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_204.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7OH46d8UL4"
      },
      "source": [
        "We can try to get the intrinsic Christoffel symbol (image below with the part on top). \n",
        "\n",
        "Remember: the metric tensor components are just the dot products of the basis vectors! And since the order of the dot product doesn't matter. So the metric tensor components are symmetric in the i and j indexes. Below you can see the derivative operator notation as well. \n",
        "\n",
        "But now: how do we compute the dot product, if we can't expand the vectors in the x,y,z space?\n",
        "\n",
        "**The metric tensor is so important, it needs to be given to us** (like with the Einstein field equation in General Relativity):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_205.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko8_64qc8UL4"
      },
      "source": [
        "The derivative of the $e_j$ in the direction of the $u^I$ coordinate $\\frac{\\partial}{\\partial u^{i}}\\left(\\overrightarrow{e_{j}}\\right)$\n",
        ", really just means the second order derivative here: $\\frac{\\partial}{\\partial u^{i}}\\left(\\frac{\\partial}{\\partial u^{j}}\\right)$. \n",
        "\n",
        "And since the order of differentiation doesn't matter, it means that it's equivalent to $\\frac{\\partial}{\\partial u^{j}}\\left(\\overrightarrow{e_{i}}\\right)$.\n",
        "\n",
        "So the derivatives in the green boxes are equivalent to the ones in purple, it means that also the two Christoffel symbols are the same. \n",
        "\n",
        "The Christoffel symbols are symmetric in their lower indexes.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_206.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaV1ArBW8UL4"
      },
      "source": [
        "So given that we can't solve the Christoffel symbols:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_207.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N-vPkbw8UL4"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_208.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljN_-btb8UL4"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_209.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0qiATCj8UL4"
      },
      "source": [
        "and we get this as our new formula for the Christoffel symbol:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElUTzChl8UL4"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_210.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tkj_vH08UL4"
      },
      "source": [
        "*This completes our definition of covariant derivative for any intrinsically curved space as long as we are given the metric tensor:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_211.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NrLfp5a8UL4"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_215.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAkS59k08UL4"
      },
      "source": [
        "**Finally new way to define Geodesics (instrinsic definition):**\n",
        "\n",
        "* Geodesic = straightest possible way between two points on a curved surface\n",
        "\n",
        "* Equal to paralle transporting a vector along itself: $\\nabla_{\\vec{w}} \\vec{v}=\\overrightarrow{0}$\n",
        "\n",
        "* And parallel transport means that the covariant derivative of a vector is equal to zero\n",
        "\n",
        "* So when we say a vector is parallel transported itself we mean that the vector that we're taking the covariant derivative of and the direction that we choose are the same. \n",
        "\n",
        "* So when the covariant derivative of a vector in the direction of itself is equal to zero, the resulting curve that we get is a **geodesic curve**.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_212.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_213.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_214.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbSSWxK78UL5"
      },
      "source": [
        "###### *Covariant Derivative: Abstract Definition with Levi-Civita Connection (Fundamental Theorem of Riemannian Geometry)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qS8lKVO8UL5"
      },
      "source": [
        "Video: [Tensor Calculus 20: The Abstract Covariant Derivative (Levi-Civita Connection)](https://www.youtube.com/watch?v=cEEahoUUGyc)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_164.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPDmbBKk8UL5"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_216.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z7Ioyaw8UL5"
      },
      "source": [
        "Before we come to defining the abstract definition of covariant derivatives, let's remember the abstract definition of a vector space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_217.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmSJlUk78UL5"
      },
      "source": [
        "With that definition we can take an example of a vector, generalize it to a vector space with above rules, and out of those rules form other vector spaces that obey the same rules:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_219.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvWSlXT8UL5"
      },
      "source": [
        "*Same is for covariant derivatives: we look at the properties and then check other examples of covariant derivatives that follow the same rules*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_220.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3A-YBzZ8UL5"
      },
      "source": [
        "*Property: Linearity (Addition and Scaling). Covariant derivative is also additive for the vector field part*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_222.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZy4xkf98UL5"
      },
      "source": [
        "So a covariant derivative is an operator with 2 input slots: one for direction and one for input field. Output is rate of change, which is a new field.\n",
        "\n",
        "> **Sometimes the covariant derivative operator is also called \"Connection\"**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_223.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duKhG9m48UL5"
      },
      "source": [
        "*What parallel transport is doing it is connecting the tangent vector space at red point p to tangent vector space at blue point q. So parallel transport gives us a way to map vectors from TpS to TqS. And since parallel transport is defined using the covariant derivative, it's really the covariant derivative that's providing the connection between the tangent spaces in this curved space*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_224.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvJf5_Qv8UL6"
      },
      "source": [
        "Christoffel symbols are **not unique**, which results that there are many ways to define these connection coefficients. \n",
        "\n",
        "But there is ba way to get a unique solution for the Christoffel symbols. And that's by introducing 2 new properties called the **Torsian free** property and the **Metric Compatibility** property. \n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_225.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bARbIArr8UL6"
      },
      "source": [
        "**First new property: Torsion Free (with Lie Bracket)**\n",
        "\n",
        "The torsion free property says that the covariant derivative of each $e^j$ in the $e^i$ direction is the same as the covariant derivative of $e^i$ in the $e^j$  direction (i'ts inspired by the rule of traditional derivatives where the order of partial differention doesn't matter).\n",
        "\n",
        "With it's expansion incl. the Christoffel symbol on the right side: The torsion free property says that are free to swap the lower indexes of the Christoffel symbols without changing their value.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_226.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZShsfxq8UL6"
      },
      "source": [
        "**Second new property: Metric Compatibility**\n",
        "\n",
        "The covariant derivative of the dot product is zero, or in another words: the dot product stays constant\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_227.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_228.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vwknqej8UL6"
      },
      "source": [
        "**Visualized this means following two things:**\n",
        "\n",
        "1. when two vectors are parallel transported along a curve (see below), their angle stays constant (because rate of change of their dot product is zero)\n",
        "\n",
        "2. and also the length of the vector stays constant\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_229.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzhUlT_-8UL6"
      },
      "source": [
        "*And both together allows us to solve for the Christoffel symbols:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_230.png)\n",
        "\n",
        "> **This means we found one particular version of the covariant derivative.**\n",
        "\n",
        "> <font color=\"blue\">**And the covariant derivative that use these Christoffel symbols is called Levi-Civita Connection**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_231.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsF0LpQX8UL6"
      },
      "source": [
        "<font color=\"blue\">*And this result is stated by the Fundamental Theorem of Riemannian Geometry*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_232.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q2rrkTa8UL7"
      },
      "source": [
        "**But there are also other covariant derivatives, for example one where all the Christoffel symnols are zero: $\\widetilde{\\Gamma_{i j}^{k}}=0$**. This is not a  Levi-Civita Connection!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_233.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RylES5B8UL7"
      },
      "source": [
        "Both perform parallel transport, but the result is different: end up connecting tangent spaces on the spheres in different ways.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_234.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ifPygdW8UL7"
      },
      "source": [
        "We can define all sorts of connections, but the  Levi-Civita Connection is the most important one!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_235.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WFIcfDB8UL7"
      },
      "source": [
        "Now let's move on to extending covariant derivatives to all tensor fields\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_236.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdkeljfR8UL7"
      },
      "source": [
        "> **The covariant derivative of a covector field has basically the same formula as the covariant derivative of a vector field except we have a negative sign:**\n",
        "\n",
        "(ps: here lambda comes from this: $\\nabla_{\\partial_{i}} \\epsilon^{j}=\\Lambda_{i k}^{j} \\epsilon^{k}$. We use the lambda coefficients who play a similar role as the Christoffel symbols, but for covectors instead (to expand the covariant derivative of the basis covectors)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_237.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7HeyHiE8UL7"
      },
      "source": [
        "And for Covariant Derivatives of a Tensor:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_238.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_239.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh5MuBU98UL7"
      },
      "source": [
        "The metric compatibility property means that the covariant derivative of the metric tensor in any direction is always going to be zero. Another way of writing is on the bottom where we leave out the direction of the covariant derivative because the covariant derivative of the metric tensor is zero in every direction:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_240.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5-qEmcz8UL8"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_241.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_242.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnVjB73B8UL8"
      },
      "source": [
        "Using the torsion free and metric compatibility properties we can derive a unique set of Christoffel symbols which define a unique covariant derivative called the Levi-Civita-connection (which is called Fundamental Theorem of Riemannian geometry):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_244.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuDciba-8UL8"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_243.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVXy2sb_8UL8"
      },
      "source": [
        "Levi-Civita-connection is the most important one. But we can also define different covariant derivatives by picking different Christoffel symbols\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_245.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtc_jrLK8UL8"
      },
      "source": [
        "And different covariant derivatives will result in different instructions for parallel transports:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_246.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejOfvz678UL-"
      },
      "source": [
        "**We have shown the covariant derivative of a (in the image below):**\n",
        "\n",
        "> scalar field\n",
        "\n",
        "> vector field\n",
        "\n",
        "> covector field\n",
        "\n",
        "> metric tensor field\n",
        "\n",
        "> and we can use this property on the bottom in a black box to get the covariant derivative of any general tensor field\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_247.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPdLE5H8UL-"
      },
      "source": [
        "###### *Lie Bracket (Commutator in Flow Curves)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoq7AWyk8UL-"
      },
      "source": [
        "**Flow Curve (Integral Curve)**\n",
        "\n",
        "https://www.youtube.com/watch?v=SfOiOPuS2_U&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=24&t=69s\n",
        "\n",
        "In the following image:\n",
        "\n",
        "* The partial derivatives of the coordinate variables are basically like basis vectors (framed in blue)\n",
        "\n",
        "* the derivatives with respect to lambda (framed in red) are the components \n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_258.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_271.png)\n",
        "\n",
        "Looking at the components we have 2 partial derivatives to solve with initial position we can choose (here (0,0):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_257.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_256.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZk1Snb48UL-"
      },
      "source": [
        "**Lie Bracket (the commutator of 2 vector fields $\\vec{u}$ & $\\vec{v}$)**\n",
        "\n",
        "* Lie Bracket takes 2 vector fields and tells us if the rectangle of flow curves closes properly\n",
        "\n",
        "* Flow curves do not close properly if Lie bracket is non zero\n",
        "\n",
        "Task of calculating Lie bracket:\n",
        "\n",
        "1. Find change of $\\vec{u}$ in the direction of $\\vec{v}$\n",
        "\n",
        "2. Find change of $\\vec{v}$ in the direction of $\\vec{u}$\n",
        "\n",
        "> **Lie Bracket (Commutator) = measures how much vector field flow curves fail to close.**\n",
        "\n",
        "> $[\\vec{u}, \\vec{v}]=\\vec{u}(\\vec{v})-\\vec{v}(\\vec{u})$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_248.png)\n",
        "\n",
        "In this example the two derivatives are not the same:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_253.png)\n",
        "\n",
        "...and this has geometrical meaning: the separation vector is the $e_y$ vector measured by the Lie bracket:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_252.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_249.png)\n",
        "\n",
        "We can also calculate the result:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_255.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_254.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KC5vBVO8UL-"
      },
      "source": [
        "**Coordinate lines are just flow curves along basis vectors**\n",
        "\n",
        "Because coordinate curves always close without a curve, so Lie bracket always has to be zero for them\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_250.png)\n",
        "\n",
        "Like in polar coordinates the Lie bracket goes to zero where it's not totally obvious:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_251.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_272.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_273.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu1qRXiA8UL-"
      },
      "source": [
        "###### *Torsion Tensor*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Torsion_tensor"
      ],
      "metadata": {
        "id": "WsY7r0C08UL-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2he7nIF8UL-"
      },
      "source": [
        "https://www.youtube.com/watch?v=SfOiOPuS2_U&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=24&t=69s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OYRqqJM8UL-"
      },
      "source": [
        "To define the Torsion Tensor we need to define the derivative of V in the direction of u and the derivative of U in the direction of V:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJTRAij8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_259.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doAXygHE8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_260.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOGJqzPJ8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_261.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZZEEgj8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_262.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iQ321N18UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31olClZ78UL_"
      },
      "source": [
        "Different definition of the connection coefficients will give us different instructions for performing parallel transport:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvKKnydZ8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_264.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMNkebeF8UL_"
      },
      "source": [
        "**Difference between Lie Bracket and Torsion tensor**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuwtZtsE8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_264.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEQgdTRu8UL_"
      },
      "source": [
        "In the following image:\n",
        "\n",
        "* With proper parallel transport the covariant derivative is zero. \n",
        "\n",
        "* The arrows framed in red (the difference vectors) tell us how much the vector fields u and v deviate aways from parallel transport\n",
        "\n",
        "* So when the covariant derivative are non zero it means that the vector fields are deviating away from the dashed lines (parallel transport)\n",
        "\n",
        "* And remember: the separation between the tips of the vectors is the Lie bracket (in black)\n",
        "\n",
        "* The blue separation vector: this is the torsion tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZMZsT9G8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_265.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYKCYpu38UL_"
      },
      "source": [
        "If it's zero, then the vectors match, and we get a completely closed 4 sided shape:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngWsmWm28UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_267.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vYXzt7P8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_266.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlpmSxPa8UL_"
      },
      "source": [
        "And how do we get the Torsion tensor components?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eprqlcf8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_268.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKv74pek8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_269.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69q8fJb8UMA"
      },
      "source": [
        "So when a connection is Torsion free, we mean that we can swap the lower indexes of the connection coefficients:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EDChBx8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_270.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaY46qpC8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_274.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIJL6tVt8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_275.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiU5eKGE8UMH"
      },
      "source": [
        "###### *Detect Curvatur: Holonomy + Geodesic Deviation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M77pSP6M8UMI"
      },
      "source": [
        "> **Use Case: wie findet man heraus, ob eine Surface flach oder gekruemmt ist?**\n",
        "\n",
        "* On curved space we can always find a coordinate system called Riemann normal coordinate (local inertial frame) where the metric tensor is the identity matrix and the connection coefficients are all zero for a specific point. \n",
        "\n",
        "* So the connection coefficients being zero is not enough to detect whether a space is curved at a given point (because we can always pick a coordinate system that makes the connection coefficients go to zero):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_283.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4B8QnY8UMI"
      },
      "source": [
        "So we need a new tool, that works everyhwere:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_282.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuQX8qW8UMI"
      },
      "source": [
        "* With a correct parallel transport we can see if a surface is curved (when vectors at starting and at end point point in different directions)\n",
        "\n",
        "* the is holonomy! It's the twisting of a vector when transport it around in a loop\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_279.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFrt8FL78UMI"
      },
      "source": [
        "In flat space the vectors will point in same directions, on curved space in different:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_280.png)\n",
        "\n",
        "for example on a sphere:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_281.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p55ouuxV8UMI"
      },
      "source": [
        "Annahme: paralellogram, das geschlossen ist (also parallel transport ist korrekt):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_278.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRdzwyqN8UMI"
      },
      "source": [
        "Grundidee des Riemann Curvature Tensor :\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_277.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ErFwUxZ8UMI"
      },
      "source": [
        "> $R(\\vec{u}, \\vec{v}) \\vec{w}=\\nabla_{\\vec{u}} \\nabla_{\\vec{v}} \\vec{w}-\\nabla_{\\vec{v}} \\nabla_{\\vec{u}} \\vec{w}-\\nabla_{[\\vec{u}, \\vec{v}]} \\vec{w}$\n",
        "\n",
        "* $R(\\vec{u}, \\vec{v})$ is an operator which acts on vector $\\vec{w}$ and produces change vector - This is the change vector after is is parallel transported around a small parallelogram (the blue handwritten arrow is this result !!!!)\n",
        "\n",
        "* this means the Riemann curvature tensor takes 3 vector inputs: 2 vectors defining the parallelogram and one starting input vector, and it outputs the change vector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_276.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3gBpVa48UMJ"
      },
      "source": [
        "> **However this works only when the Lie bracket is zero** (so the parallel transport was correct and no gap between end vectors left):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_284.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVDbudpp8UMJ"
      },
      "source": [
        "> **When the Lie bracket is non-zero we need an additional term at the end of the equation:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_285.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOF2ksjI8UMJ"
      },
      "source": [
        "Exkurs: Proof the linearity (for the second point of required to make $R$ a tensor) that the Riemann tensor is linear for the $u$ input:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_286.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3QMTNkq8UMJ"
      },
      "source": [
        "> **Another geometric interpretation of the Riemann tensor: Geodesic deviation (Is the space curved or flat?)**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_287.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opESTVjL8UMJ"
      },
      "source": [
        "* In flat space: geodesics are straight lines. \n",
        "\t\n",
        "\t* The separation if these geodesic changes at a constant rate (imagine we drew a separation vector $\\vec{s}$ between two of the paths at various points, this separation vector will be changing in size. \n",
        "\n",
        "\t* the rate of change (or derivative) of this vector along a geodesic would be constant (since rate of growth of the vector is constant). \n",
        "\n",
        "\t* And this means the second derivative would be zero. \n",
        "\n",
        "* In curved space: the separation between geodesics on the sphere does not change at a constant rate, the curves are accelerating away from each other, and then accelerate towards each after after crossing the equator. \n",
        "\t\n",
        "\t* so in this case the covariant derivative of the separation vector is nonzero, because there is acceleration going on. \n",
        "  \n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOL7MoEm8UMJ"
      },
      "source": [
        "* If we have a family of geodesics  coming out of a point p we can describe the tangent vectors along these geodesics by the vector field v\n",
        "\n",
        "* Since the v vectors are tangent to the geodesics, they are a bit like velocity vectors along the geodesics\n",
        "\n",
        "* and we can describe the separation between these geodesics at various points in time by these separation curves (red dashed curves!!) - we can think of these curves as marking point where we've been traveled along the geodesics\n",
        "\n",
        "* the tangent vectors along the red separation curves will be given by the vector field S. These s vectors would be the separation vectors between the geodesics\n",
        "\n",
        "* now we want to compute the geodesic deviation, which is the second rate of change of the separation vector s as we travel along the geodesics in the direction of v\n",
        "\n",
        "* we assume that we're in a space where the connection is torsion free. In other words: the difference between these two covariant derivatives is equal to the Lie bracket of the v in s.\n",
        "\n",
        "* But since the v and s vector field flow curves form a grid with nice coded parallelograms (left side of the image below), the Lie bracket is zero. \n",
        "\n",
        "* bringing this to the other side we can swap the order of these two vectors in the covariant derivative if the connection is torsion free\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_289.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL4zjq9G8UMJ"
      },
      "source": [
        "Given that the curves tangent to v  are geodesics this means the covariant deriative of V along itself is zero (definition of geodesics for curves that are parallel transported along themselves!)\n",
        "\n",
        "* and we can take the covariant derivative with respect to s on both sides, the right hand side stays zero.\n",
        "\n",
        "* Formula on the bottom: If the Riemann tensor is zero everywhere this must mean the geodesic deviation is also zero, so the space is flat!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_290.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_291.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg26k2KQ8UMK"
      },
      "source": [
        "###### *Riemann curvature tensor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethu1_YS8UMK"
      },
      "source": [
        "Video: [Tensor Calculus 23: Riemann Curvature Tensor Components and Symmetries](https://www.youtube.com/watch?v=optrC-0HhMI)\n",
        "\n",
        "**Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_294.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USnmyezS8UMK"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_293.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIk_0HOP8UMK"
      },
      "source": [
        "It would be hard to compute all these components, but luckily the Riemann curvature tensor has a few symmetries:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_292.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOb4d4mH8UMK"
      },
      "source": [
        "These symmetries reduce a lot the computation effort:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_305.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPyFy_Qo8UMK"
      },
      "source": [
        "*Proof of first symmetry, the 34 symmetry*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_295.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIrZo8e18UMK"
      },
      "source": [
        "**Proof of the First Bianchi identity** - applies only to Torsion free connections (remember: for a connection that is Torsion free, we can swap the connection coefficients):\n",
        "\n",
        "*same colors, are euqal for a torsion free connection*:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_297.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_298.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udDUea418UMK"
      },
      "source": [
        "If we make all upper indexes equal and add all cycles of the lower indexes, the result is zero:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_296.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "douiaiYJ8UMM"
      },
      "source": [
        "**Proof of first symmetry, the 12 symmetry**: (acting on a dot product)\n",
        "\n",
        "this is a longer proof, pls watch the video)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_299.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNOOrDdF8UMM"
      },
      "source": [
        "**Flip Symmetrie**\n",
        "\n",
        "The last symmetrie can be dervived from the previous 3 ones:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_300.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5fFHaYO8UMM"
      },
      "source": [
        "A lot of the Riemann curvature tensor components go to zero when we perform a swap on two indexes that are the same:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_301.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJXCzwrG8UMM"
      },
      "source": [
        "In 2D we can save us computing 16 components, there is only one:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_302.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvoXUuRq8UMN"
      },
      "source": [
        "*Example: Riemann tensor components in Polar coordinates in 2D are zero. Since Rimenan tensor in 2D has only 1 free parameter which means all the components in the tensor are zero. This confirms that the space described by 2D is flat. Parallel Transporting a vector in a loop in 2D polar coordinates, doesn't change the direction.*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_303.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKkCBDDJ8UMN"
      },
      "source": [
        "In contrsat Riemann tensor will have non-zero components on surface of a sphere: (see video for proof)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_304.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOOfnwwC8UMN"
      },
      "source": [
        "because remember:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_306.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zliXjCNj8UMN"
      },
      "source": [
        "Also keep in mind: Riemann tensor is a multilinear map (it's linear in all its input components. And we only need to know how the riemann tensor acts on a set of basis vectors (given by partial derivaties):\n",
        "\n",
        "Components of the output of the Riemann tensor are these $R_{k i j}^{m}$, and the formula for computing these components is on the bottom, by using a formula involving the connection coefficients.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_307.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnkSoEwu8UMN"
      },
      "source": [
        "###### *Sectional Curvature, Riemann curvature tensor, Ricci-Tensor & Ricci-Scalar*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Riemann curvature tensor (Kr√ºmmungstensor), Ricci-Tensor, Ricci-Scalar** \n",
        "\n",
        "* Kr√ºmmung wird beschrieben, indem man Vektoren auf geschlossenen Wegen durch die Raumzeit schiebt und feststellt, ob sie ver√§ndert zur√ºckkommen. Nehmen Sie vielleicht einen Globus und einen Bleistift in die Hand. Der Bleistift stellt einen Vektor dar. Legen Sie den Bleistift am Nordpol tangential an den Globus an. Verschieben Sie ihn nun l√§ngs irgendeines L√§ngenkreises (Meridian) zum √Ñquator. Danach verschieben Sie den Bleistift l√§ngs des √Ñquators ein St√ºck nach Osten oder Westen und beachten dabei, dass der Bleistift tangential zum Globus bleiben muss. Dann verschieben Sie den Bleistift wieder l√§ngs eines Meridians zum Nordpol zur√ºck. Wenn er dort ankommt, wird er in eine andere Richtung als zu Beginn seines Weges zeigen. \n",
        "\n",
        "* Der Unterschied zwischen den Richtungen zu Beginn und am Ende des Weges ist ein Ma√ü f√ºr die Kr√ºmmung des Globus. Dass der Bleistift w√§hrend des gesamten Weges tangential zum Globus verschoben werden muss, dr√ºckt aus, dass er die Oberfl√§che des Globus w√§hrend der Verschiebung nicht verlassen darf.\n",
        "\n",
        "* Genau diese Operation, die kennzeichnet, wie sich ein Vektor ver√§ndert, wenn man ihn l√§ngs eines geschlossenen Weges durch die Raumzeit verschiebt, wird mathematisch durch den so genannten Kr√ºmmungstensor ausgedr√ºckt. \n",
        "\n",
        "* **Der Kr√ºmmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar**. \n",
        "\n",
        "* Der Ricci-Skalar ordnet jedem Punkt der Raumzeit einen einzelnen Zahlenwert zu, der die lokale Kr√ºmmung der Raumzeit an diesem Punkt kennzeichnet.\n",
        "\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "8XJQVKeDBC-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l2nQaUk8UMN"
      },
      "source": [
        "Video: [Tensor Calculus 24: Ricci Tensor Geometric Meaning (Sectional Curvature)](https://www.youtube.com/watch?v=ZhDNijOEw0Y&t=332s)\n",
        "\n",
        "**Ricci Tensor: Track volume change along geodesics**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_310.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REcGWHAD8UMO"
      },
      "source": [
        "**Ricci Tensor**\n",
        "\n",
        "$R_{\\mu \\nu}$ = Einstein Field equation\n",
        "\n",
        "* The change of the size of the circle, given by the Ricci tensor, represents how quickly these two people get drawn together\n",
        "\n",
        "* The more spacetime is curved, the more quickly bodies will get drawn together\n",
        "\n",
        "* In general relativity gravitational attraction is just the natural result of curved spacetime (it doesn't require any forces like we see with Newtonian)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_309.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLiiCzKO8UMO"
      },
      "source": [
        "**Ricci Scalar**\n",
        "\n",
        "Helps us to quantify how much more volume / space we can fit into a circle on a curved space in comparison to a flat space circle:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_308.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQYPe_Rt8UMO"
      },
      "source": [
        "###### *Geodesic Deviation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8kCHo18UMO"
      },
      "source": [
        "Remember:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_311.png)\n",
        "\n",
        "and Riemann tensor with its components (using the connection coefficients):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_312.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LerlWi_i8UMO"
      },
      "source": [
        "* Recall when we have a family of geodesic curves with a tangent velocity vector field V, we also look at the separation vectors along these curves (given by vector field s). \n",
        "\n",
        "* We showed that, starting with the standard definition of a geodesic (=transporting a vector along itself), so that the covariant derivative of v in the direction of v is zero, we can take the covariant derivative again in direction to s on both sides, and then subtract and add the two terms in red marked\n",
        "\n",
        "* on the left side you get the definition of the Riemann tensor when the Lie bracket is zero\n",
        "\n",
        "* on the right side after the + symbol you can swap the order of s and v using the Torsion free property\n",
        "\n",
        "* Result is that the second derivative of this separation vector s is called the geodesic deviation. It tells us how the geodesic move relative to one another.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_313.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_314.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbBSjEZ8UMO"
      },
      "source": [
        "So on the left on the bottom ytou can see that the second derivative is zero because the separation vectors do not change in lenghts because the geodesics to not move apart or are drawn together.\n",
        "\n",
        "In the picture in the middle the geodesics move together, and the separation vectors get smaller. This means the second derivative points ot the left direction (blue): $\\nabla_{\\vec{v}} \\nabla_{\\vec{v}} \\vec{S}$.\n",
        "\n",
        "Because s vector and geodesic deviation vector (the second derivative) point into opposite direction, their dot product is negative < 0. \n",
        "\n",
        "And since the geodesic deviation vector is equal to the negative of the Riemann tensor $-R(\\vec{S}, \\vec{v}) \\vec{v}$ means that the dot product of the Riemann term and the s vector is positive > 0. And this is when geodesics accelerate towards each other.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_315.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmHevWQ8UMO"
      },
      "source": [
        "**Our goal: detect curvature in space, but spreading of geodesics can happen also in flat space and there is no curvature. That's why we use the second devriative**.\n",
        "\n",
        "> We choose the second derivative because it will tell us about the spreading due to the curvature of space and not the spreading that could happen in flat space where the geodesics are angled in specific directions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_316.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_317.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYuEKhzl8UMP"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_318.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_319.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kmg30Zv8UMP"
      },
      "source": [
        "Let's plug it in and evaluate the formula on another two vectors r and t who are linear combinations of v and w and hence live in the same vector space, the resuls of the two formulas is equal:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_320.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_321.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQstACJK8UMP"
      },
      "source": [
        "**Ricci Tensor: Sectional Curvature**\n",
        "\n",
        "The terming the numerator (on top) gives us the sign we need to determine how the geodesics travel in direction v are either converging or diverging. And the noralization in the denominator (bottom) keeps the result the same regardless of the length of s and v.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_322.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPZhD5tP8UMP"
      },
      "source": [
        "**Ricci Curvature**\n",
        "\n",
        "> **Direction vector v is equal to the last basis vector. The Ricci curvature in direction vector v is equal to the sum of all sectional curvatures along ever other basis vector direction**.\n",
        "\n",
        "So the Ricci tensor is like the sum of all possible sectional curvatures containing the vector v. \n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_323.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZazmJqJ8UMP"
      },
      "source": [
        "Geometrical Interpretation:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_324.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_325.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_326.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI2XuLpv8UMP"
      },
      "source": [
        "Since all sectional curvature are zero, their sum is also zero (Ricci curvature):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_327.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkMz8Nki8UMQ"
      },
      "source": [
        "Let's take a situation where all the geodesic are consverging to each other:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_328.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_329.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcCxkff8UMQ"
      },
      "source": [
        "Since the sectional curvatures are positive, their sum (Ricci curvature) will also be positive:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_330.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXFzOb768UMQ"
      },
      "source": [
        "In this case the ball moving along the geodesic will shrink: Piositive Ricci curvature means the ball with shrink in volume:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_331.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em9ugC1S8UMQ"
      },
      "source": [
        "And now the other way around: the Ricci curvature is negative:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_332.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_333.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_334.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_335.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACz7amAx8UMQ"
      },
      "source": [
        "Now an interesting case and when some geodesics are diverging, and some are converging -they cancel each other out in the sum (the Ricci curvature):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_336.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_337.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_338.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGjtHw198UMQ"
      },
      "source": [
        "Note that the shape of the ball can change, just the volume is the same.\n",
        "\n",
        "> **The Ricci Curvature can only tell us about volume, but not shape!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_339.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KPsL1yk8UMQ"
      },
      "source": [
        "> **So when the Ricci tensor acts on the same vector twice the result we get is the Ricci curvature:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_341.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAD5NUkX8UMR"
      },
      "source": [
        "And the result is always the same, no matter which orthonormal basis we choose (for example when we change basis with the forward matrix and the vector components with a backwrad matrix since they are contravariant):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_342.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCZ_oiLU8UMR"
      },
      "source": [
        "Special Case Sphere:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_343.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_344.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lguL736S8UMR"
      },
      "source": [
        "Just one tiny problem: Ricci tensor so far for oorthonormal basis only:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_345.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0hPCoGn8UMR"
      },
      "source": [
        "###### *Geometric Meaning Ricci Tensor/Scalar (Volume Form)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzTqK5_X8UMR"
      },
      "source": [
        "Video: [Tensor Calculus 25 - Geometric Meaning Ricci Tensor/Scalar (Volume Form)](https://www.youtube.com/watch?v=oQZTYt_Pxcc&t=570s)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_345.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp0M-XTx8UMR"
      },
      "source": [
        "**Volume form or Volume element omega $\\omega$**\n",
        "\n",
        ".. gives us the volume enclosed by a set of vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_346.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppUkDQ-c8UMR"
      },
      "source": [
        "So for an orthonormal basis, the volume form $\\omega$ is always = 1:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_347.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJYLNgOe8UMR"
      },
      "source": [
        "Determinants for non orthonormal basis, because remember: the change in volume that results from a linear map is given by the determinant of the matrix for that linear map.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_348.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnU-rW-88UMS"
      },
      "source": [
        "* The map containing the vector components forms a linear map from the square of area 1 to the parallelogram, the area of the parallelogram should be the determinant of this matrix.\n",
        "\n",
        "* And taking the determinant of the matrix $\\left[\\begin{array}{ll}u^{1} & w^{1} \\\\ u^{2} & w^{2}\\end{array}\\right]$  we find that the area of this parallelogram is $u^{1} w^{2}-u^{2} w^{1}$. \n",
        "\n",
        "* Another way of writing this more compactly is the Levi Civita symbol $\\epsilon$ to write a summation:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_349.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHQ3ZsVL8UMS"
      },
      "source": [
        "Odd or even permutation ergibt sich auch der Anzahl der Umstellungen der Zahlen, zB um von 123 auf 213 zu kommen, muss man nur 1x eine Anderung vornehmen, und das dann eine odd permutation = -1:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_350.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkyTpXHc8UMS"
      },
      "source": [
        "So for volumes there are 27 terms, but ony 6 are non zero, and they result in the correct formula for this 3x3 matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_351.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os-PJXHW8UMS"
      },
      "source": [
        "but that works only in an orthonormal basis:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_352.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXQA52w68UMS"
      },
      "source": [
        "**What if we change to a basis that isn't orthonormal?**\n",
        "\n",
        "here for 2D, sphere and curved space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_353.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMd9EanZ8UMS"
      },
      "source": [
        "* Recall that the components of the metric tensor in a given basis are just the dot products of the basis vectors\n",
        "\n",
        "* And if we expand the new basis vectors in terms of the old basis vectors using summations with the forward F coefficients we get the dot product (in Klammern) of the components of the metric tensor in the old basis.\n",
        "\n",
        "The square of the determinant of F is equal to the determinant of the metric tensor matrix in the new basis. And so the volume change is equal to the determinant of F but it is also equal to the square root of the determinant of the metric tensor in the new basis.:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_354.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP_8M9fA8UMS"
      },
      "source": [
        "Same for volume in curvilinear coordinate:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_355.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOYDeAlr8UMT"
      },
      "source": [
        "So there are different ways to write the volume enclosed by a new set of basis vectors: deF, detJ or square root of detg:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_356.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAUJCApO8UMT"
      },
      "source": [
        "> **So how do we get the volume in a new basis that isn't orthonormal?**\n",
        "\n",
        "* First we take the square root of the determinant of the metric tensor matrix detg in the new basis. This computes the change in volume that owe get from changing from an orthonormal basis to the new non-orthonormal basis\n",
        "\n",
        "*Second we multiply that with the determinant detMatrix with u and v of the matrix of vector components as measured in the new basis. This takes care of the volume change we get by building up the u and w vectors from the new basis vectors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_357.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXo5Wj1f8UMT"
      },
      "source": [
        "If we write this determinant using the Levi Civita symbol, we get this formula:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_358.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_359.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_360.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ_hG3s8UMT"
      },
      "source": [
        "**Second derivative (2 covariant derivatives) of the Volume Form tensor along some geodesic path:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_361.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_362.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_363.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEir3of_8UMT"
      },
      "source": [
        "Their covariant derivative is zero by definition. And since omega is a tensor and a multilinear map when one of the inputs is zero, that means that the entire output is zero. \n",
        "\n",
        "* So the covariant derivative of the volume form must be equal to zero. In other words: the derivative of the volume form components along this path are zero.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_364.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhuwbbAe8UMT"
      },
      "source": [
        "**Consider the vectors u, w and t as separation vectors and call them s1, s2 and s3:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_365.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmHJ754F8UMU"
      },
      "source": [
        "Formula at the end:\n",
        "\n",
        "> $=\\frac{d}{d \\lambda} \\frac{d}{d \\lambda}\\left[\\left(\\prod_{i=1}^{D} s_{i}^{\\mu_{i}}\\right)\\right] \\sqrt{\\operatorname{det} g} \\epsilon_{\\mu_{1} \\ldots \\mu_{D}}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_366.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_367.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_368.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w-8ngR_8UMU"
      },
      "source": [
        "**The first term measures changing from curved space, meanwhile the second term measures changes coming from flat space:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_369.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgmQwyE8UMU"
      },
      "source": [
        "**Ricci Scalar**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_370.png)\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_371.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_372.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbpnv7MB8UMU"
      },
      "source": [
        "**Ricci scalar compares by radius - but along a path someone would travel, not the classic straight line radius:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_373.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHO_pcQK8UMU"
      },
      "source": [
        "Here on the bottom is the new formula of the surface area of the ball:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_374.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqklGQjt8UMU"
      },
      "source": [
        "This is the formula for the surface area of the upside-down ball as a function of the ball's radius as measured using the walking distance from the North Pole:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_375.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_376.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_377.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_378.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNV-O-Z78UMV"
      },
      "source": [
        "**The two meanings of positive curvature: more area for the same circumference or less area for the same radius:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_379.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrauG_Mm8UMV"
      },
      "source": [
        "In negatively curved space it would be the opposite:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_380.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA6DMS9e8UMV"
      },
      "source": [
        "This is what the Taylor series looks like for the ratio of circle areas in the specific case where our curved space is a sphere (on top). For general curved spaces in d dimensions the ratio of the volumes has the Taylor expansion on the bottom:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_381.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9cYkVi78UMV"
      },
      "source": [
        "###### *Algebraic Properties, Einstein Field Equations and Cosmological Constant $\\Lambda$*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_cGwXAR8UMV"
      },
      "source": [
        "Video: [Riemann curvature tensor](https://www.youtube.com/watch?v=hhhYzfozon0)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_382.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw7IK5H-8UMV"
      },
      "source": [
        "**Einstein tensor**\n",
        "\n",
        "* Ricci tensor $R^{mn}$ alone is not enough since its covariant derivative is nonzero.\n",
        "\n",
        "* but with the extra correction term we get the Einstein tensor, which has a zero covariant derivative\n",
        "\n",
        "* Einstein tensor with its 16 field equations for general relativity is used to describe black holes and the expansion of the universe.\n",
        "\n",
        "* but given some symmetries there are only 10 unique equations.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_383.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVH3meZ68UMV"
      },
      "source": [
        "Here: capital lambda in red $\\Lambda$ is a cosmological constant, which is one possible form of what physicists call dark energy (related to the expansion of the universe):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_384.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Tropical Geometry*"
      ],
      "metadata": {
        "id": "lh3x3_1K_e1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Sturmfels - Tropical Geometry](https://youtu.be/TNwCzl02uck)"
      ],
      "metadata": {
        "id": "1gdFgCSJfTg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Geometry**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_geometry\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_variety\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/System_of_polynomial_equations\n",
        "\n",
        "Particle physics and cosmology are linked to combinatorics and algebraic geometry in an unexpected way. Novel geometric objects hint at new mathematical structures that challenge our current understanding of the laws of nature. The workshop ‚ÄûPositive Geometry\" on January 23 at the Campus of the Technische Universit√§t M√ºnchen will discuss the latest developments. The event is organized by Bernd Sturmfels, Director at the Max-Planck-Institut f√ºr Mathematik in den Naturwissenschaften, J√ºrgen Richter-Gebert, Chair for Geometry and Visualization at TUM, Johannes Henn, Director at MPP in collaboration with #SFB Transregio 109, Discretization in Geometry and Dynamics.\n",
        "More information: https://Inkd.in/eQVnYtyD\n",
        "particlephysics geometry cosmology"
      ],
      "metadata": {
        "id": "mFfIs1nSCsxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tropical Geometry**\n",
        "\n",
        "http://www.mittag-leffler.se/langa-program/tropical-geometry-amoebas-and-polytopes-0\n",
        "\n",
        "Video: [What is tropical mathematics?](https://youtu.be/DV-8kEn8udY)"
      ],
      "metadata": {
        "id": "hjvAO915_hkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Analysis**"
      ],
      "metadata": {
        "id": "b8EVGNJmEfDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Number Theory*"
      ],
      "metadata": {
        "id": "Qa3-DIbUt0Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mathworld.wolfram.com/topics/NumberTheory.html"
      ],
      "metadata": {
        "id": "oTRwcbs24qtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Number theory Full Course [A to Z]](https://www.youtube.com/watch?v=19SW3P_PRHQ)"
      ],
      "metadata": {
        "id": "QQ94xvSShDVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [The Continuity of Splines](https://www.youtube.com/watch?v=jvPPXbo87ds)"
      ],
      "metadata": {
        "id": "1nsbz4yAiP6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zahlentheorie**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebraische_Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Diophantische_Gleichung: Polynomfunktion mit ganzzahligen Koeffizienten ist und nur ganzzahlige L√∂sungen gesucht werden"
      ],
      "metadata": {
        "id": "NGT5msJ1sIRa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g878nE6q7lX"
      },
      "source": [
        "**Zahlenarten**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Zahldarstellung\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/number_001.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carmichael-Zahl**\n",
        "\n",
        "Eine [Carmichael-Zahl](https://de.m.wikipedia.org/wiki/Carmichael-Zahl) ist eine nat√ºrliche Zahl mit besonderer Primfaktorzerlegung\n",
        "\n",
        "Eine Carmichael-Zahl ist stets ungerade und enth√§lt mindestens 3 verschiedene Primfaktoren. Die kleinsten Carmichael-Zahlen sind 561, 1105, 1729.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/carmichael-zahl/1414\n",
        "\n",
        "https://www.faz.net/aktuell/wissen/daniel-larsen-findet-einen-mathebeweis-zu-carmichael-zahlen-18583248.html"
      ],
      "metadata": {
        "id": "dImJp0SNWxsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Narcissistic Number**\n",
        "\n",
        "number theory, a narcissistic number is a number that can be expressed as the sum of its own digits raised to the power of the number of digits.\n",
        "\n",
        "> $153 = 1^3 + 5^3 + 3^3$\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Narcissistic_number"
      ],
      "metadata": {
        "id": "04WEUT_eCbW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex Numbers**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/155762/complex-numbers-in-quantum-mechanics-and-in-special-relativity\n",
        "\n",
        "* you can use split-complex numbers in relativity, but ironically complex numbers have proved more popular for this)."
      ],
      "metadata": {
        "id": "klTQgi0O_Vnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quaternions are useful. In particular for representation of rotations of 3-space. ‚Äì\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Hyperkomplexe_Zahlen.svg/519px-Hyperkomplexe_Zahlen.svg.png)"
      ],
      "metadata": {
        "id": "ueOfwEOht3VB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y_nhc_-u7P"
      },
      "source": [
        "**Hyperreelle Zahlen & Nichtstandardanalysis (Infinitesimalrechnung)**\n",
        "\n",
        "* There are also applications of nonstandard analysis to the theory of stochastic processes, particularly constructions of Brownian motion as random walks.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Nichtstandardanalysis\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Hyperreelle_Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Infinitesimalrechnung\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Surreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Hyperreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Infinitesimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2-azta81gIJ"
      },
      "source": [
        "**Hyperkomplexe Zahlen**\n",
        "\n",
        "* [Hyperkomplexe Zahlen](https://de.m.wikipedia.org/wiki/Hyperkomplexe_Zahl) sind Verallgemeinerungen der komplexen Zahlen.\n",
        "\n",
        "https://www.quantamagazine.org/the-octonion-math-that-could-underpin-physics-20180720/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNHMgPsrj4B"
      },
      "source": [
        "**Transcendental numbers:**\n",
        "\n",
        "* Zahlen, die L√∂sung einer algebraischen Gleichung: (Wurzel aus 2) - 2 = 0\n",
        "\n",
        "* Transzendente Zahlen (in der Menge der reellen Zahlen): Zahlen, die nicht L√∂sung einer algebraischen Gleichung sind: e oder pi\n",
        "\n",
        "> from: https://www.youtube.com/watch?v=P24tmohytXs\n",
        "\n",
        "* pie œÄ or Euler number\n",
        "\n",
        "* Never end after comma: 3.14159265358979323846....\n",
        "\n",
        "* Cannot be displayed as fraction\n",
        "\n",
        "* [Transzedente Zahl](https://de.m.wikipedia.org/wiki/Transzendente_Zahl) heisst eine reelle Zahl (oder allgemeiner eine komplexe Zahl), wenn sie nicht Nullstelle eines (vom Nullpolynom verschiedenen) Polynoms mit ganzzahligen Koeffizienten ist. Andernfalls handelt es sich um eine algebraische Zahl. Jede reelle transzendente Zahl ist √ºberdies irrational.\n",
        "\n",
        "* omnem rationem transcendunt, lat.: Sie sind jenseits aller Vernunft\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplex Numbers**\n",
        "\n",
        "* --> real tessarines\n",
        "\n",
        "* also: [split complex number](https://en.m.wikipedia.org/wiki/Split-complex_number), hyperbolic number, double number)\n",
        "\n",
        "* In algebra, a split complex number (or hyperbolic number, also perplex number, double number) has two real number components x and y, and is written $z = x + y‚Äâj$, where $j^2$ = 1. The conjugate of z is $z^{‚àó}$ = x ‚àí y j. Since j2 = 1, the product of a number z with its conjugate is $zz^{‚àó}$ = $x^2 ‚àí y^2$, an [isotropic quadratic form](https://en.m.wikipedia.org/wiki/Isotropic_quadratic_form), =N(z) = $x^2 ‚àí y^2$.\n",
        "\n",
        "* Very perplexing, right? They are defined as of form a+hb, where a and b are real numbers, h¬≤=1.\n",
        "\n",
        "  * But wait, that sounds too easy! Until you realize that h isn't +1 or -1. It is \"something else\".\n",
        "\n",
        "* Perplex numbers have many applications outside of algebra and geometry, such as in Quantum Mechanics and the Theory of Relativity.\n",
        "\n",
        "* Special Relativity:https://aapt.scitation.org/doi/10.1119/1.14605\n",
        "\n",
        "* Quantum Mechanics: https://www.intlpress.com/site/pub/files/_fulltext/journals/cis/2014/0014/0003/CIS-2014-0014-0003-a001.pdf"
      ],
      "metadata": {
        "id": "VKqM5OuK6jYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bicomplex number**\n",
        "\n",
        "* --> \"Tessarine\" redirects here. For real tessarines, see Split-complex number.\n",
        "\n",
        "* In abstract algebra, a bicomplex number is a pair (w, z) of complex numbers constructed by the Cayley‚ÄìDickson process that defines the bicomplex conjugat ${\\displaystyle (w,z)^{*}=(w,-z)}$, and the product of two bicomplex numbers as\n",
        "\n",
        "> {\\displaystyle (u,v)(w,z)=(uw-vz,uz+vw).}\n",
        "\n",
        "* Then the bicomplex norm is given by\n",
        "\n",
        "> ${\\displaystyle (w,z)^{*}(w,z)=(w,-z)(w,z)=(w^{2}+z^{2},0),}$\n",
        "\n",
        "* a quadratic form in the first component.\n",
        "\n",
        "* https://hsm.stackexchange.com/questions/12866/why-are-quaternions-more-popular-than-tessarines-despite-being-non-commutative\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1200.png)\n",
        "\n",
        "Images source:https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.527.356&rep=rep1&type=pdf\n"
      ],
      "metadata": {
        "id": "bZqRMeca9IqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVu9D8nrvsY"
      },
      "source": [
        "**p-adic numbers**\n",
        "\n",
        "* Size is different in p-adic numbers\n",
        "\n",
        "* they have nothing to do with the real number line: here two numbers are close, when their first several digits are the same\n",
        "\n",
        "* in p-adic absolute value, two numbers are close when their last digits are the same. so two numbers are close in this field, when they agree on\n",
        "\n",
        "https://youtu.be/3gyHKCDq1YA\n",
        "\n",
        "* infinite before comma: ....985356295141.3\n",
        "\n",
        "* [p-adische Zahl](https://de.m.wikipedia.org/wiki/P-adische_Zahl) ist eine Zahl, die sich in einer Potenzreihe zu einer Primzahl darstellen l√§sst\n",
        "\n",
        "* p-adic number systems emerge from modular arithmetic\n",
        "\n",
        "* https://www.quantamagazine.org/how-the-towering-p-adic-numbers-work-20201019/\n",
        "\n",
        "* https://www.quantamagazine.org/peter-scholze-and-the-future-of-arithmetic-geometry-20160628/\n",
        "\n",
        "* https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/\n",
        "\n",
        "* \"Das Dualsystem ist das Stellenwertsystem mit der Basis 2, liefert also die dyadische (2-adische) Darstellung von Zahlen (Dyadik) (gr. Œ¥œçŒø = zwei).\" [Source](https://de.m.wikipedia.org/wiki/Dualsystem#Grundrechenarten_im_Dualsystem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPr6YYL5n9sy"
      },
      "source": [
        "**Modulform**\n",
        "\n",
        "* used in stringtheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Modulform\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Ramanujan-Thetafunktion\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Thetafunktion\n",
        "\n",
        "* https://www.quantamagazine.org/moonshine-link-discovered-for-pariah-symmetries-20170922/\n",
        "\n",
        "* https://www.quantamagazine.org/universal-math-solutions-in-dimensions-8-and-24-20190513/\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Kaluza-Klein-Theorie"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infinity $\\infty$**\n",
        "\n",
        "* Kontinuumshypothese (Cantor) kann man nicht beweisen oder widerlegen (G√∂del). Die Mathematik kann funktionieren, wenn man sowohl davon ausgeht, dass die Kontiuumshypothese gilt, als auch, dass sie nicht gilt.\n",
        "\n",
        "* term of size (how many?) is: **Cardinality**\n",
        "\n",
        "* Rules for comparing cardinalities: injection, subjection, bijection\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_319.png)\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite\n",
        "\n",
        "* [How Big are All Infinities Combined? (Cantor's Paradox) | Infinite Series](https://www.youtube.com/watch?v=TbeA1rhV0D0&list=WL&index=12&t=86s)\n",
        "\n",
        "**Smallest Sizes of Infinity**\n",
        "\n",
        "* intuition is often misleading in mathematics!\n",
        "\n",
        "* There are infinitely many sizes of infinity\n",
        "\n",
        "* Smallest infinity: natural numbers (counting numbers). Here: even numbers and natural numbers are the same size!\n",
        "\n",
        "  * Natural numbers: $\\aleph$ \"Aleph-naught\" (the least infinity cardinality)\n",
        "\n",
        "* We need to find a way to tell which infinity is bigger without counting them.\n",
        "\n",
        "* You use something called Bijection: if you can pair up two sets, they are the same size\n",
        "\n",
        "\t* even numbers and natural numbers are the same size, because there is a bijection between the two sets! Each natural number is [aired with 2 times itself: 1 with 2, 2 with 4, 4 with 6 etc. Damit sind alles odd numbers out, but the set didn't get smaller\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_318.png)\n",
        "\n",
        "\t* the natural numbers are also the same size as the integers: Integers include all natural numbers + all the negative whole numbers. We can pair them up exactly, so they must be the same size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_317.png)\n",
        "\n",
        "* Above the natural numbers are the real numbers in terms of infinity size. After that the real numbers.\n",
        "\n",
        "\t* an interval on the real number line is also an infinity. And an interval on the real line between 0 and 5 has the same size as an interval between 0 and 10. They are ll as big as the real numbers!!\n",
        "\n",
        "\t* You can show that any interval is the same size as the entire real number line !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_316.png)\n",
        "\n",
        "* Cantor wondered if there is an infinity between the natural and the real numbers?\n",
        "\n",
        "\t* CONTINUUM HYPOTHESIS: there is no size of infinity between the natural numbers and the real numbers\n",
        "\n",
        "\t* Decades later, mathematicians found out that the Continuum hypothesis is independent of the Zermela-Fraenkel set theory with choice (ZFC) - the Continuum hypothesis can not be proved or disproved using the standard rules of mathematics\n",
        "\n",
        "* So in one model of the tower of infinities the real numbers sit directly above the natural numbers\n",
        "\n",
        "* But in other models there are many infinities in between\n",
        "\n",
        "* The rules of maths don't say that one tower is correct and the other wrong.\n",
        "\n",
        "> So it seems that mathematics seems to be surprisingly agnostics with regards to which hierarchy of infinities is correct\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite"
      ],
      "metadata": {
        "id": "rniZL8bpenwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFaA3R5k-Wi"
      },
      "source": [
        "**Primzahlen**\n",
        "\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Skewes-Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahlsatz\n",
        "\n",
        "*Riemannsche Vermutung (Primzahlverteilung & Zeta-Funktion)*\n",
        "\n",
        "* Die Verteilung der Primzahlen ist sehr merkw√ºrdig und damit interessant. So zeigt die Verteilung von Primzahlen in (relativ) kurzen Intervallen eine gewisse ‚ÄûZuf√§lligkeit‚Äú, w√§hrend andererseits beliebig lange Intervalle existieren, die keine Primzahl enthalten.\n",
        "\n",
        "* Bernhard Riemann setzte sich in seiner Arbeit ‚ÄûUeber die Anzahl der Primzahlen unter einer gegebenen Gr√∂sse‚Äú (1859) zum Ziel, die Verteilung der Primzahlen mit analytischen Methoden zu bestimmen, stie√ü dabei auf Riemannsche Œ∂-Funktion und formulierte die Riemannsche Vermutung. Basierend auf den Riemannschen Ideen gelang 1896 der Beweis des Primzahlsatzes, mit dem man f√ºr gro√üe Zahlen x mit immer gr√∂√üerer relativer Genauigkeit sagen kann, wieviele Primzahlen ‚â§ x es gibt.\n",
        "\n",
        "* Will man diese Anzahlen noch genauer wissen, so kommt man schnell in einen Bereich mathematischer Fragestellungen mit zahlreichen offenen Problemen, z. B. den Goldbach-Problemen oder Fragen √ºber Primzahlzwillinge.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/primzahlverteilung/8085\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Vermutung\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Zetafunktion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Graph Theory*"
      ],
      "metadata": {
        "id": "sxfXe3_nNrjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Graphs*"
      ],
      "metadata": {
        "id": "8yH2OFqH-37c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Quantum Application with Graphs: https://medium.com/quandela/exploring-graph-problems-with-single-photons-and-linear-optics-4f3d5848add8"
      ],
      "metadata": {
        "id": "Yye9sR8Zq-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph & Graph Theory**\n",
        "\n",
        "* In [discrete mathematics](https://en.m.wikipedia.org/wiki/Discrete_mathematics), and more specifically in [graph theory](https://en.m.wikipedia.org/wiki/Graph_theory), a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\".\n",
        "\n",
        "* The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called link or line).\n"
      ],
      "metadata": {
        "id": "b-7FPIb42kSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Graph Theory**\n",
        "\n",
        "* [Algebraic graph theory](https://en.m.wikipedia.org/wiki/Algebraic_graph_theory) is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches.\n",
        "\n",
        "* There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.\n",
        "\n",
        "* **Using linear algebra**:\n",
        "\n",
        "  * The first branch of algebraic graph theory involves the study of graphs in connection with linear algebra. Especially, it studies the [spectrum (Eigendecomposition of a matrix)](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the adjacency matrix, or the Laplacian matrix of a graph - this part of algebraic graph theory is also called [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory).\n",
        "\n",
        "  * **Several theorems relate properties of the spectrum to other [graph properties (invariant)](https://en.m.wikipedia.org/wiki/Graph_property).** As a simple example, a connected graph with diameter D will have at least D+1 distinct values in its spectrum. Aspects of graph spectra have been used in analysing the synchronizability of networks.\n",
        "\n",
        "* Video: [Daniel Spielman ‚ÄúMiracles of Algebraic Graph Theory‚Äù](https://www.youtube.com/watch?v=CDMQR422LGM)\n",
        "\n",
        "* Video: [The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry and Physics](https://www.youtube.com/watch?v=8XJes6XFjxM)"
      ],
      "metadata": {
        "id": "sg75zioGELCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Types of Graphs*"
      ],
      "metadata": {
        "id": "9ELeT7od--k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete graph**\n",
        "\n",
        "* In graph theory, a [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge.\n",
        "\n",
        "* A complete digraph is a directed graph in which every pair of distinct vertices is connected by a pair of unique edges (one in each direction).\n",
        "\n",
        "*K7, a complete graph with 7 vertices with $n$ vertices and ${\\displaystyle \\textstyle {\\frac {n(n-1)}{2}}}$ edges*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Complete_graph_K7.svg/245px-Complete_graph_K7.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "suoc9GoR3FCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete bipartite graph**\n",
        "\n",
        "* [Complete bipartite graph](https://en.m.wikipedia.org/wiki/Complete_bipartite_graph) (or biclique), a special [bipartite graph](https://en.m.wikipedia.org/wiki/Bipartite_graph) where every vertex on one side of the bipartition is connected to every vertex on the other side\n",
        "\n",
        "* A complete bipartite graph with m = 5 and n = 3:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Biclique_K_3_5.svg/320px-Biclique_K_3_5.svg.png)\n",
        "\n",
        "Bipartite Graph https://www.youtube.com/watch?v=HqlUbSA9cEY\n",
        "\n"
      ],
      "metadata": {
        "id": "p0YwqBrL3qCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directed vs Undirected Graphs**\n",
        "\n",
        "The applications for directed graphs are many and varied. They can be used to analyze electrical circuits, develop project schedules, find shortest routes, analyze social relationships, and construct models for the analysis and solution of many other problems.\n",
        "\n",
        "https://faculty.cs.niu.edu/~freedman/241/241notes/241graph.htm\n",
        "\n",
        "\n",
        "Undirected graphs are more restrictive kinds of graphs. They represent only whether or not a relationship exists between two vertices. They don't however represent a distinction between subject and object in that relationship. One type of graph can sometimes be used to approximate the other.\n",
        "\n",
        "https://www.baeldung.com/cs/graphs-directed-vs-undirected-graph"
      ],
      "metadata": {
        "id": "V0ExwcX52H9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hamiltonian path**\n",
        "\n",
        "A [Hamiltonian path](https://en.m.wikipedia.org/wiki/Hamiltonian_path) (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle around a network of six vertices:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/b/be/Hamiltonian.png)"
      ],
      "metadata": {
        "id": "E26jLergoS4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Graph Properties*"
      ],
      "metadata": {
        "id": "oAiyy65E-8Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Length** is the number of edges in a graph.\n",
        "\n",
        "A **closed walk** occurs when x = y.\n",
        "\n",
        "A **trail** is a walk with no repeated edges.\n",
        "\n",
        "A closed trail is a **circuit**.\n",
        "\n",
        "A **path** is a walk with no repeated vertices (A path is a sequence of distinct edges you can follow through the graph)\n",
        "\n",
        "A closed path is a **cycle** (and circuit)\n",
        "\n",
        "A **simple graph** is loop-free, undirected and has no multiple edges.\n",
        "\n",
        "[INTRODUCTION to GRAPH THEORY - DISCRETE MATHEMATICS](https://www.youtube.com/watch?v=HkNdNpKUByM)\n",
        "\n",
        "[Introduction to Graph Theory: A Computer Science Perspective](https://www.youtube.com/watch?v=LFKZLXVO-Dg)\n"
      ],
      "metadata": {
        "id": "yOjUf8ACsQ8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wichtige Zyklen und Pfade in der Graphentheorie**\n",
        "\n",
        "* [**Eulerian path** (Eulerkreisproblem)](https://de.m.wikipedia.org/wiki/Eulerkreisproblem) = every edge only once\n",
        "  * **Euler path** (every edge once only, vertices can be multiple: exists if either 0 or 2 odd degree vertices exist with all else even degree\n",
        "  * **Euler circuit** (more constraint): same as Euler path, but return to starting point (das Haus vom weihachtsmann). Use also Fleurys algorithm: works if all vertices have even degrees\n",
        "\n",
        "* [**Hamiltonian circuit / cycle** (Hamiltonian path problem)](https://de.m.wikipedia.org/wiki/Hamiltonkreisproblem): every vertex only once\n",
        "  * every vertex only once, but edges don‚Äòt matter how often. And return to starting point.\n",
        "  * **Minimum cost hamiltonian circuit = [traveling salesman problem](https://de.m.wikipedia.org/wiki/Problem_des_Handlungsreisenden).**. Is np-complete. Look for [shortest path](https://de.m.wikipedia.org/wiki/K√ºrzester_Pfad), siehe auch [Pathfinding](https://de.m.wikipedia.org/wiki/Pathfinding)\n",
        "  * https://www.quora.com/Why-are-quantum-computers-unable-to-solve-the-travelling-salesman-problem-in-polynomial-time: *Quantum computation offers speed improvements for some very specialized problems like integer factorization, but it isn‚Äôt known or expected to offer polynomial-time algorithms for generic NP-complete problems like the Traveling Salesman Problem. If it does then NP ‚äÜ BQP, and most experts don‚Äôt regard this as likely at all $^{*}$*.\n",
        "  * Heuristic algorithms as alternative to brute force:\n",
        "    * [Dijkstra algorithmus](https://de.m.wikipedia.org/wiki/Dijkstra-Algorithmus)\n",
        "    * Nearest neighbor algorithm is a heuristic, non optimal, but feasible - [greedy algorithm](https://de.m.wikipedia.org/wiki/Greedy-Algorithmus), cheapest route\n",
        "    * Repeated Nearest neighbor algorithm\n",
        "    * Sorted edges algorithm\n",
        "    * Kruskal algorithm (spanning tree) for minimum cost spanning tree (optimal and efficient). For example energy lines\n",
        "\n",
        "If you have a fully connected [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) and and want to compute unique Hamiltonain circuits:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1389.png)\n",
        "\n",
        "Source: [Graph theory full course for Beginners](https://www.youtube.com/watch?v=sWsXBY19o8I)\n",
        "\n",
        "$^{*}$ *Why can‚Äôt a quantum computer solve NP complete problems in a polynomial running time (relative to their input)?\n",
        "Quantum computers, like classical computers, are not currently believed to be able to solve NP-complete problems in polynomial time.\n",
        "\n",
        "Contrary to Kurt Behnke‚Äôs answer, this is not true by definition. It‚Äôs a profound open question in mathematics.\n",
        "\n",
        "As Vadim Yakovlevich correctly points out, it‚Äôs partly a matter of no one having found a polynomial-time quantum algorithm for NP-complete problems after decades of trying‚Äîjust like no one has found a polynomial-time classical algorithm.\n",
        "\n",
        "But it‚Äôs possible to say more than that. From reading popular articles, many people have the vague impression that a quantum computer could just try every possible solution in parallel. And if that‚Äôs all you know about them, then it‚Äôs indeed a mystery why they couldn‚Äôt solve NP-complete problems in polynomial time!\n",
        "\n",
        "The central difficulty is that, for a computer to be useful, at some point you need to measure it to observe an answer. And if you just measured an equal superposition over all possible answers, not having done anything else, the rules of QM dictate that you‚Äôll just see a random answer. And of course, if you‚Äôd just wanted a random answer, you could‚Äôve picked one yourself with a lot less trouble!\n",
        "\n",
        "So the name of the game, with every quantum algorithm, is somehow orchestrating a pattern of constructive and destructive quantum interference that boosts the probability of the correct answer (even though you yourself don‚Äôt know in advance which answer is the correct one!), and that does so efficiently. Famously, in 1994 Peter Shor showed how to do that for the problem of factoring integers, and a few related problems in number theory‚Äîbut he was able to do so only by exploiting extremely special structure in those problems.\n",
        "\n",
        "For more ‚Äúgeneric‚Äù search problems, like NP-complete problems, the best we generally know is Grover‚Äôs algorithm: a quantum algorithm that‚Äôs able to find the right answer (i.e., concentrate a large fraction of the amplitude on that answer) in roughly the square root of the number of steps that would be needed classically. So you get some speedup, but not an exponential one.\n",
        "\n",
        "But we also know that, if your search problem is a ‚Äúblack box‚Äù‚Äîi.e., you can just pick candidate solutions and evaluate if they‚Äôre correct, and you don‚Äôt know anything more about the problem‚Äôs structure‚Äîthen Grover‚Äôs algorithm is the best that even a quantum computer can do. This is the content of the so-called BBBV (Bennett, Bernstein, Brassard, Vazirani) Theorem, which has several proofs, all of them crucially relying on the linearity of unitary evolution. The BBBV Theorem gives a fundamental explanation for why any fast quantum algorithm for NP-complete problems would have to look very different from anything that we know today‚Äîmuch like a fast classical algorithm would have to*."
      ],
      "metadata": {
        "id": "6M6WUYqnfgAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph Properties (Invariants)**\n",
        "\n",
        "* A [graph property (invariant)](https://en.m.wikipedia.org/wiki/Graph_property) is a property of graphs that depends only on the abstract structure, not on graph representations such as particular labellings or drawings of the graph.\n",
        "\n",
        "* An example graph, with the properties of\n",
        "\n",
        "  * being [planar](https://en.m.wikipedia.org/wiki/Planar_graph) (it can be drawn on the plane in such a way that its edges intersect only at their endpoints / no edges cross each other)\n",
        "\n",
        "  * and being [connected](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)),\n",
        "\n",
        "  * and with order 6,\n",
        "\n",
        "  * size 7,\n",
        "\n",
        "  * [diameter (distance](https://en.m.wikipedia.org/wiki/Distance_(graph_theory)) 3 (number of edges in a shortest path, also called a graph geodesic, connecting them)\n",
        "\n",
        "  * [girth](https://en.m.wikipedia.org/wiki/Girth_(graph_theory)) 3 (girth of an undirected graph is the length of a shortest cycle contained in the graph)\n",
        "\n",
        "  * vertex [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) 1, and\n",
        "\n",
        "  * [degree sequence](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (3, 3, 3, 2, 2, 1):\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/333px-6n-graf.svg.png)"
      ],
      "metadata": {
        "id": "03Ad29FMyOx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph enumeration**\n",
        "\n",
        "* In combinatorics, an area of mathematics, [graph enumeration](https://en.m.wikipedia.org/wiki/Graph_enumeration) describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph.\n",
        "\n",
        "* These problems may be solved either exactly (as an [algebraic enumeration](https://en.m.wikipedia.org/wiki/Algebraic_enumeration) problem) or asymptotically.\n",
        "\n",
        "* The number of labeled n-vertex simple undirected graphs is $2^{\\frac{n(n‚Ää‚àí1)}{2}}$\n",
        "\n",
        "* The number of labeled n-vertex simple directed graphs is $2^{n(n‚Ää‚àí1)}$\n",
        "\n",
        "* **Number of connected labeled graphs with n nodes**: The number of distinct connected labeled graphs with n nodes is tabulated in the [On-Line Encyclopedia of Integer Sequences](https://en.m.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences) as sequence [A001187](https://oeis.org/A001187). The first few non-trivial terms are\n",
        "\n",
        "$\\begin{array}{|l|l|}\n",
        "\\hline {\\text { n }} & {\\text { graphs }} \\\\\n",
        "\\hline 1 & 1 \\\\\n",
        "\\hline 2 & 1 \\\\\n",
        "\\hline 3 & 4 \\\\\n",
        "\\hline 4 & 38 \\\\\n",
        "\\hline 5 & 728 \\\\\n",
        "\\hline6 & 26704 \\\\\n",
        "\\hline 7 & 1866256 \\\\\n",
        "\\hline 8 & 251548592 \\\\\n",
        "\\hline\n",
        "\\end{array}$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9u6fmPYO1EZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genus of a Graph**\n",
        "\n",
        "[genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (plural genera) has a few different, but closely related, meanings. Intuitively, the genus is the number of \"holes\" of a surface.[1] A sphere has genus 0, while a torus has genus 1.\n",
        "\n",
        "A genus-2 surface:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Double_torus_illustration.png/219px-Double_torus_illustration.png)\n",
        "\n",
        "**The genus of a graph is the minimal integer n such that the graph can be drawn without crossing itself on a sphere with n handles** (i.e. an oriented surface of the genus n). Thus, a planar graph has genus 0, because it can be drawn on a sphere without self-crossing.\n",
        "\n"
      ],
      "metadata": {
        "id": "4eb_PjW92TEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph embedding**\n",
        "\n",
        "In topological graph theory, an [embedding](https://en.m.wikipedia.org/wiki/Graph_embedding) (also spelled imbedding) of a graph G on a surface $\\Sigma$ is a representation of G on $\\Sigma$ in which points of $\\Sigma$ are associated with vertices and simple arcs (homeomorphic images of $[0,1])$ are associated with edges in such a way that:\n",
        "\n",
        "the endpoints of the arc associated with an edge $e$ are the points associated with the end vertices of e, no arcs include points associated with other vertices, two arcs never intersect at a point which is interior to either of the arcs.\n",
        "\n",
        "> Often, an embedding is regarded as an equivalence class (under homeomorphisms of $\\Sigma$) of representations of the kind just described."
      ],
      "metadata": {
        "id": "UiscJVPa4BY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spanning tree**\n",
        "\n",
        "A [spanning tree](https://en.m.wikipedia.org/wiki/Spanning_tree) T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/4x4_grid_spanning_tree.svg/252px-4x4_grid_spanning_tree.svg.png)\n",
        "\n",
        "A special kind of spanning tree, the Xuong tree, is used in topological graph theory to find graph embeddings with maximum [genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (is the number of \"holes\" of a surface)"
      ],
      "metadata": {
        "id": "Z72Y0JpX169N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of random walks on graphs**\n",
        "\n",
        "Cover times: A cover time is the number of steps needed for a random walk to visit all vertices in a given graph.\n",
        "\n",
        "http://uu.diva-portal.org/smash/get/diva2:319078/FULLTEXT01.pdf"
      ],
      "metadata": {
        "id": "ChVvi_4nAOHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of graphs and the Gaussian free field**\n",
        "\n",
        "Max point of a gaussian free field is the max number of time steps to visit each vertex in a graph\n",
        "\n",
        "https://tcsmath.wordpress.com/2010/12/09/open-question-cover-times-and-the-gaussian-free-field/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Gaussian_free_field\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/9/93/Discrete_Gaussian_free_field_on_60_x_60_square_grid.png)"
      ],
      "metadata": {
        "id": "6gIbaAJjACax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit rank**\n",
        "\n",
        "* [Circuit rank](https://en.m.wikipedia.org/wiki/Circuit_rank), cyclomatic number, cycle rank, or nullity of an undirected graph is the **minimum number of edges that must be removed from the graph to break all its cycles, making it into a tree or forest**.\n",
        "\n",
        "* It is equal to the number of independent cycles in the graph (the size of a cycle basis).\n",
        "\n",
        "* This graph has circuit rank r = 2 because it can be made into a tree by removing two edges, for instance the edges 1‚Äì2 and 2‚Äì3, but removing any one edge leaves a cycle in the graph:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/320px-6n-graf.svg.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wJ0FciA8LKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Degree**\n",
        "\n",
        "* Degree of a matrix = how many neighbours in each vertex?\n",
        "\n",
        "* [Degree](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (or valency) of a vertex of a graph is the number of edges that are incident to the vertex; in a multigraph, a loop contributes 2 to a vertex's degree, for the two ends of the edge.\n",
        "\n",
        "* The maximum degree of a graph G, denoted by $\\Delta (G)$, and the minimum degree of a graph, denoted by ${\\displaystyle \\delta (G)}$, are the maximum and minimum of its vertices' degrees. In the multigraph shown on the right, the maximum degree is 5 and the minimum degree is 0.\n",
        "\n",
        "* In a [regular graph](https://en.m.wikipedia.org/wiki/Regular_graph) (=each vertex has the same number of neighbors), every vertex has the same degree, and so we can speak of the degree of the graph.\n",
        "\n",
        "* A [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) (denoted $K_{n}$, where n is the number of vertices in the graph) is a special kind of regular graph where all vertices have the maximum possible degree, $n-1$.\n",
        "\n",
        "* Degree sum formula states that, given a graph $G=(V,E)$: ${\\displaystyle \\sum _{v\\in V}\\deg(v)=2|E|\\,}$ (Handshaking lemma)\n",
        "\n",
        "*Two non-isomorphic graphs with the same degree sequence (3, 2, 2, 2, 2, 1, 1, 1):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Conjugate-dessins.svg/240px-Conjugate-dessins.svg.png)"
      ],
      "metadata": {
        "id": "iMnZwQUq6F8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Connected) Component (isolated subgraph) = Betti number $B_0$**\n",
        "\n",
        "* The number of (connected) [components](https://en.m.wikipedia.org/wiki/Component_(graph_theory)) of a topological space is an important topological invariant, the zeroth Betti number $B_0$, and the number of components of a graph is an important graph invariant, and **in topological graph theory it can be interpreted as the zeroth Betti number of the graph**.\n",
        "\n",
        "* The number of components arises in other ways in graph theory as well: **In algebraic graph theory number of components equals the multiplicity of 0 as an eigenvalue of the Laplacian matrix of a finite graph**.\n",
        "\n",
        "* bzw: The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**. (Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen.) Taken from: https://de.m.wikipedia.org/wiki/Spektrum_(Graphentheorie)\n",
        "\n",
        "* Let G be a d-regular graph. The algebraic multiplicity of eigenvalue 0 for the Laplacian matrix is exactly 1 iff G is connected. https://math.uchicago.edu/~may/REU2013/REUPapers/Marsden.pdf\n",
        "\n",
        "* https://www.sciencedirect.com/science/article/pii/S0024379518300156"
      ],
      "metadata": {
        "id": "0tvO21EovW33"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgO45hovFfpL"
      },
      "source": [
        "**Connected Components**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Component_(graph_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Connected_space\n",
        "* How the graph can be used to capture connected component\n",
        "* Any graph gives us a topological space (like joining a square as a connected component)\n",
        "* Graph can capture connected components via putting an equivalence relation on the set of vertices.\n",
        "* **equivalence relation is generated by tail of an edge (vertex 1) is equivalent to a head of an edge (vertex 2) / between the vertices of an edge (which together forms a connected component)**.\n",
        "* To generate equivalence relation we have to make sure that we enforce reflexivity, symmetry and transivity.\n",
        "* You get equivalence classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a graph**\n",
        "\n",
        "* the [rank of an undirected graph](https://en.m.wikipedia.org/wiki/Rank_(graph_theory)) has two unrelated definitions. Let n equal the number of vertices of the graph.\n",
        "\n",
        "* In the [matrix theory](https://en.m.wikipedia.org/wiki/Matrix_(mathematics)) of graphs the rank r of an undirected graph is defined as the rank of its adjacency matrix. Analogously, the [nullity](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) of the graph is the nullity of its adjacency matrix, which equals n ‚àí r.\n",
        "\n",
        "* In the [matroid theory](https://en.m.wikipedia.org/wiki/Matroid) of graphs the rank of an undirected graph is defined as the number n ‚àí c, where c is the number of connected components of the graph.\n",
        "\n",
        "  * Equivalently, the rank of a graph is the rank of the oriented [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) associated with the graph.\n",
        "\n",
        "  * Analogously, the [nullity of the graph](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) is the [nullity (Kernel)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) of its oriented incidence matrix, given by the formula m ‚àí n + c, where n and c are as above and m is the number of edges in the graph.\n",
        "\n",
        "  * The nullity is equal to the first [Betti number](https://en.m.wikipedia.org/wiki/Betti_number) of the graph. The sum of the rank and the nullity is the number of edges.\n"
      ],
      "metadata": {
        "id": "Hju_JaTe8iLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity** (Menger's theorem, max-flow min-cut)\n",
        "\n",
        "* In mathematics and computer science, [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) is one of the basic concepts of graph theory: **it asks for the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more [isolated subgraphs (= Connected component)](https://en.m.wikipedia.org/wiki/Component_(graph_theory)).**\n",
        "\n",
        "* It is closely related to the theory of network flow problems. The connectivity of a graph is an important measure of its resilience as a network.\n",
        "\n",
        "* One of the most important facts about connectivity in graphs is [Menger's theorem](https://en.m.wikipedia.org/wiki/Menger%27s_theorem): for any two vertices u and v in a connected graph G, the numbers Œ∫(u, v) and Œª(u, v) can be determined efficiently using the [max-flow min-cut theorem](https://en.m.wikipedia.org/wiki/Max-flow_min-cut_theorem) algorithm.\n",
        "\n",
        "* The vertex- and edge-connectivities of a disconnected graph are both 0.\n",
        "\n",
        "* 1-connectedness is equivalent to connectedness for graphs of at least 2 vertices.\n",
        "\n",
        "* The [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) on n vertices has edge-connectivity equal to n ‚àí 1. Every other simple graph on n vertices has strictly smaller edge-connectivity.\n",
        "\n",
        "\n",
        "*This graph becomes disconnected when the right-most node in the gray area on the left is removed:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/389px-Network_Community_Structure.svg.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8L5-Pd_FzCPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique**\n",
        "\n",
        "* Eine [Clique](https://de.m.wikipedia.org/wiki/Clique_(Graphentheorie)) bezeichnet in der Graphentheorie **eine Teilmenge von Knoten in einem ungerichteten Graphen, bei der jedes Knotenpaar durch eine Kante verbunden ist**.\n",
        "\n",
        "* Zu entscheiden, ob ein Graph eine Clique einer bestimmten Mindestgr√∂√üe enth√§lt, wird [Cliquenproblem](https://de.m.wikipedia.org/wiki/Cliquenproblem) genannt und gilt, wie das Finden von gr√∂√üten Cliquen, als algorithmisch schwierig (NP-vollst√§ndig).\n",
        "\n",
        "* Das Finden einer Clique einer bestimmten Gr√∂√üe in einem Graphen ist ein [NP-vollst√§ndiges Problem](https://de.m.wikipedia.org/wiki/NP-Vollst√§ndigkeit) (= schwierigsten Problemen in der Klasse NP geh√∂rt, also sowohl in NP liegt als auch NP-schwer ist) und somit auch in der Informationstechnik ein relevantes Forschungs- und Anwendungsgebiet.\n",
        "\n",
        "*Ein Graph mit einer Clique der Gr√∂√üe 3:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/6n-graf-clique.svg/350px-6n-graf-clique.svg.png)\n"
      ],
      "metadata": {
        "id": "3CZpsY2coXHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique complex (Whitney complexes)**\n",
        "\n",
        "[Clique complexes](https://en.m.wikipedia.org/wiki/Clique_complex), independence complexes, flag complexes, Whitney complexes and conformal hypergraphs are closely related mathematical objects in graph theory and geometric topology that each describe the cliques (complete subgraphs) of an undirected graph.\n",
        "\n",
        "Siehe auch: [Topological Graph Theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory)"
      ],
      "metadata": {
        "id": "NENSroXr7Oh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CW Complex**\n",
        "\n",
        "A [CW complex](https://en.m.wikipedia.org/wiki/CW_complex) (also called cellular complex or cell complex) is a kind of a topological space that is particularly important in algebraic topology.[1] It was introduced by J. H. C. Whitehead[2] to meet the needs of homotopy theory. This class of spaces is broader and has some better categorical properties than simplicial complexes, but still retains a combinatorial nature that allows for computation (often with a much smaller complex). The C stands for \"closure-finite\", and the W for \"weak\" topology\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))"
      ],
      "metadata": {
        "id": "XdMET4Yk8PJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract simplicial complex**\n",
        "\n",
        "In combinatorics, an [abstract simplicial complex](https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex) (ASC), often called an abstract complex or just a complex, is a family of sets that is closed under taking subsets, i.e., every subset of a set in the family is also in the family. It is a purely combinatorial description of the geometric notion of a simplicial complex.[1]\n",
        "\n",
        "For example, in a 2-dimensional simplicial complex, the sets in the family are the triangles (sets of size 3), their edges (sets of size 2), and their vertices (sets of size 1).\n",
        "\n",
        "Geometric realization of a 3-dimensional abstract simplicial complex:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Simplicial_complex_example.svg/247px-Simplicial_complex_example.svg.png)\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Let G be an undirected graph. The [clique complex](https://en.m.wikipedia.org/wiki/Clique_complex) (flag complexes, Whitney complexes) of G is an ASC whose faces are all cliques (complete subgraphs) of G. The independence complex of G is an ASC whose faces are all independent sets of G (it is the clique complex of the complement graph of G). Clique complexes are the prototypical example of flag complexes. A flag complex is a complex K with the property that every set of elements that pairwise belong to faces of K is itself a face of K.\n",
        "\n",
        "* Let M be a metric space and Œ¥ a real number. The [Vietoris‚ÄìRips complex](https://en.m.wikipedia.org/wiki/Vietoris‚ÄìRips_complex) is an ASC whose faces are the finite subsets of M with diameter at most Œ¥. It has applications in homology theory, hyperbolic groups, image processing, and mobile ad hoc networking. It is another example of a flag complex (clique complex).\n",
        "\n"
      ],
      "metadata": {
        "id": "wAytmxq5-zzX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbqQuo7L2du"
      },
      "source": [
        "**Simplices**\n",
        "\n",
        "A [simplex](https://de.m.wikipedia.org/wiki/Simplex_(Mathematik)) consists of 3 components: vertices, edges and faces\n",
        "\n",
        "* 0-simplex: point\n",
        "* 1-simplex: edge (line)\n",
        "* 2-simplex: 3 connected points\n",
        "* 3-simplex: solid (3 dimensions)\n",
        "\n",
        "k-simplex is k-dimensional is formed using (k+1) vertices ('convex hull')\n",
        "\n",
        "* Complete graphs (every vertice is connected to all others) can be interpreted as simplices\n",
        "\n",
        "* you can interpret any graph as simplicial complex\n",
        "\n",
        "**Simplicial Complex**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex\n",
        "\n",
        "* Mit einem Simplizialkomplex k√∂nnen die entscheidenden Eigenschaften von triangulierbar topologischen R√§umen algebraisch charakterisiert werden k√∂nnen\n",
        "\n",
        "* Warum? Definition von Invarianten im topologischen Raum. Simplicial complexes can be seen as higher dimensional generalizations of neighboring graphs.\n",
        "\n",
        "* Wie? Untersuchung eines topologischen Raums durch Zusammenf√ºgen von Simplizes womit eine Menge im d-dimensionalen euklidischen Raum konstruiert wird, die [hom√∂omorph](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist zum gegebenen topologischen Raum.\n",
        "\n",
        "* Die ‚ÄûAnleitung zum Zusammenbau‚Äú der Simplizes, das hei√üt die Angaben dar√ºber, wie die Simplizes zusammengef√ºgt sind, wird dann in Form einer Sequenz von [Gruppenhomomorphismen](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus) rein algebraisch charakterisiert.\n",
        "\n",
        "* Cells can have various dimensions: vertices, edges, triangles, tetrahedra and their higher dimensional analogues. complexes reflect the correct topology of the data\n",
        "\n",
        "* If we glue many simplices together in such a way that the intersection is also a simplex (along an edge for example), we obtain a simplicial complex. If we see three points connected by edges that form a triangle, we fill in the triangle with a 2-dimensional face. Any four points that are all pairwise connected get filled in with a 3-simplex etc. The resulting simplicial complex is called (Vietoris) Rips complex.\n",
        "\n",
        "* The persistence diagram is not computed directly from LÙè∞ëŒµ. Instead, one forms an object called a Cech complex. Simplicial and cubical complexes are examples of cell complexes. Cech complex is an example of a simplicial complex. - in practice, often used Vietoris-Rips complex VŒµ - the persistent homology defined by VŒµ approximates the persistent homology defined by CŒµ.\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_03.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hSqsZ5LLdvz"
      },
      "source": [
        "**Triangulation**\n",
        "\n",
        "* In der Topologie ist eine [Triangulierung](https://en.m.wikipedia.org/wiki/Triangulation_(topology)) oder Triangulation eine **Zerlegung eines Raumes in Simplizes** (Dreiecke, Tetraeder oder deren h√∂her-dimensionale Verallgemeinerungen).\n",
        "\n",
        "* Mannigfaltigkeiten bis zur dritten Dimension sind stets triangulierbar.\n",
        "\n",
        "* Urspr√ºngliche Motivation f√ºr die Hauptvermutung war der Beweis der topologischen Invarianz kombinatorisch definierter Invarianten wie der [simplizialen Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie). Trotz des Scheiterns der Hauptvermutung lassen sich Fragen dieser Art oftmals mit dem [simplizialen Approximationssatz](https://de.m.wikipedia.org/wiki/Simpliziale_Approximation) beantworten.\n",
        "\n",
        "* Triangulation ist eine Zerlegung eines Raumes in Simplizes (Dreiecke, Tetraeder oder h√∂her-dimensionale Verallgemeinerungen) to tease out properties of manifolds\n",
        "\n",
        "* **A triangulation of a topological space X is a simplicial complex K, homeomorphic to X, together with a homeomorphism h: K ‚Üí X**\n",
        "\n",
        "* triangulation offers a concrete way of visualizing spaces that are difficult to see, and helps computing an invariant.\n",
        "\n",
        "* Ist gegeben durch einen (abstrakten) Simplizialkomplex K und Hom√∂omorphismus h : | K | ‚Üí X der geometrischen Realisierung | K | auf X.\n",
        "https://en.wikipedia.org/wiki/Triangulation_(topology)\n",
        "\n",
        "* a two-dimensional sphere (surface of a solid ball) can be approximated by gluing together two-dimensional triangles, and a three-dimensional sphere can be approximated by gluing together three-dimensional tetrahedra.\n",
        "\n",
        "* Triangles and tetrahedra are examples of more general shapes called simplices, which can be defined in any dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRA0yDZJGOjK"
      },
      "source": [
        "**[Filtration](https://de.m.wikipedia.org/wiki/Filtrierung_(Mathematik)) & Inclusion Maps**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtered_algebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtration_(mathematics)\n",
        "\n",
        "* **With increasing size of d, we are dealing with a sequence of simplicial complexes, each a sub-complex of the next. That is, a simplicial complex constructed from data for some small distance is a subset of the simplicial complex constructed for a larger distance**.\n",
        "\n",
        "* Equally there is an **[inclusion map](https://en.m.wikipedia.org/wiki/Inclusion_map) from each simplicial complex to the next**.\n",
        "\n",
        "* **This sequence of simplicial complexes, with inclusion maps, is called a filtration**.\n",
        "\n",
        "* When we apply homology to a filtration, we obtain an algebraic structure called persistent modul.\n",
        "\n",
        "* So if we want to compute **ith homology with coefficients from a field k**.\n",
        "\n",
        "* The **homology of any complex Cj is a vector space**, and the **inclusion maps between complexes induce linear maps between homology vector spaces**.\n",
        "\n",
        "* The direct sum of the homology vector spaces is an algebraic module - in fact a **graded module over the polynomial ring** k[x]. The variable x acts as a **shift map**, taking each homology generator to its image in the next vector space.\n",
        "\n",
        "* Furthermore, a structure theorem tells us that **a persistent module decomposes nicely into a direct sum of simple modules**, each corresponding to a bar in the barcode. This means: a barcode reall is an algebraic structure.\n",
        "\n",
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS9TpoyGEmjj"
      },
      "source": [
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)\n",
        "\n",
        "![cscsvs](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_02.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Laplacian*"
      ],
      "metadata": {
        "id": "f4lHqJyMa24i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=M9F7zT9Gg-k\n",
        "\n",
        "https://www.youtube.com/watch?v=skaXrTtQyJA\n",
        "\n",
        "https://www.youtube.com/watch?v=2h12m-3zQ0M\n",
        "\n",
        "https://www.youtube.com/watch?v=-Afa1WI3iug"
      ],
      "metadata": {
        "id": "X13DUDyLt9db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Graph Theory**\n",
        "\n",
        "* In [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory), the [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) of a [graph](https://en.m.wikipedia.org/wiki/Graph_(discrete_mathematics)) is the characteristic polynomial of its [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix).\n",
        "\n",
        "* Spectral graph theory is the study of the properties of a graph in relationship to the **characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph**, such as its adjacency matrix or [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix) = Edge Matrix - Adjacency Matrix\n",
        "\n",
        "* One of the reasons that the eigenvalues of matrices have meaning is that they arise as the solution to natural optimization problems\n",
        "\n",
        "> Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen. (The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**.)\n",
        "\n",
        "\n",
        "\n",
        "http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf\n",
        "\n",
        "https://www.cs.cmu.edu/~venkatg/teaching/15252-sp20/notes/Spectral-graph-theory.pdf"
      ],
      "metadata": {
        "id": "Zxze3ZZQpmmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian matrix:**\n",
        "\n",
        "> $L(G) = D - A = B^T B \\quad$ for some matrix $B$\n",
        "\n",
        "* $L$ = [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix)\n",
        "* $D$ = [degree matrix](https://en.m.wikipedia.org/wiki/Degree_matrix), entries contain degrees of each vertex\n",
        "* $A$ = [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix): 1 if edge, 0 if no edge\n",
        "* $B$ = the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "**Description of Laplacian matrix**\n",
        "* Laplacian matrix is an operator mapping from graph to the real numbers\n",
        "* Diagonals have degrees of vertices\n",
        "* Off-diagonal contain edges marked as -1\n",
        "* Contains all information about graph (you can recreate the graph just from laplacian).\n",
        "* And you can nicely apply linear algebra to analyse the graph\n",
        "\n",
        "**Properties of Laplacian matrix:**\n",
        "1. L(G) is symmetric\n",
        "2. L(G) has:\n",
        "  * real-valued, non-negative eigenvalues and\n",
        "  * real-valued, orthogonal eigenvectors\n",
        "* Is always positive-semidefinite matrix\n",
        "* $\\lambda$ = 0 is always an Eigenvalue with Eigenvector 1 (because all rows add to zero and multiplied by vector with 1‚Äòs leads to zero)\n"
      ],
      "metadata": {
        "id": "yk02m201n6n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of calculating Eigenvalues and Eigenvectors of Laplacian matrix**\n",
        "* second smallest Eigenvalue (**spectral gap**) is a solution (i.e. minimal energy) for many problems, if computable.\n",
        "* You can use the spectrum of the Laplacian matrix to solve\n",
        "  * **optimization problems** (i.e. shortest path, Hamiltonain circuit), like traveling salesman problem\n",
        "  * **graph partitioning problem** (optimal cut). reveal sparse connections and where there are more clusters, find bottlenecks (in social media it relates to communities, in networks it's a critical point\n",
        "  * both problems are np-hard\n",
        "* In physics: **Fundamental modes (harmonics)** are given by the Eigenvectors of the graph laplacian [Spectral Partitioning Part 3 Algebraic Connectivity](https://www.youtube.com/watch?v=Vng9lkibGEE)"
      ],
      "metadata": {
        "id": "9MumAIvqp8cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 1: Graph laplacian spectrum, meaning it's eigenvalues, tells us something about the underlying connectivity of the graph**\n",
        "* = Graph has k connected components if and only if the k-smallest Eigenvalues are identically zero: $\\lambda_0$ = $\\lambda_1$ = .. $\\lambda_{k-1}$ = 0 (Fiedler 1973).\n",
        "* Spectrum of L(G) $\\rightleftharpoons$ Connectivity of G\n",
        "* = The number of (connected) components in the graph is the dimension of the nullspace (=kernel / set of solutions) of the Laplacian and the algebraic multiplicity (=how many times the same Eigenvalue) of the 0 eigenvalue.\n",
        "* Laplacian: its Eigenvectors, which do not rotate, you can see an ellipse like section of space they draw.\n",
        "* **Spectrograph theory asked questions about this ellipse to determine graph behavior, what the fastest way from dot-to-dot is**, how many pathways there are dot to dot [Source](https://www.youtube.com/watch?v=njatNunHC_o)\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem **Ellipsoid** (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)"
      ],
      "metadata": {
        "id": "cZ4ROEDbn9zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 2: $L(G)$ contains $B^T B$ for some matrix $B$**\n",
        "* the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "> $\\min _{y \\in \\Re^{\\prime \\prime}} f(y)=\\sum_{(i, j) \\in E^{\\prime}}\\left(y_i-y_j\\right)^2=y^T L y$\n",
        "\n",
        "* [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient) with Rayleigh theorem:\n",
        "  * $\\lambda_2=\\min f(y)$ : The minimum value of $f(y)$ is given by the $2^{\\text {nd }}$ smallest eigenvalue $\\lambda_2$ of the Laplacian matrix $\\boldsymbol{L}$\n",
        "  * $\\mathrm{x}=\\arg \\min _{\\mathrm{y}} \\boldsymbol{f}(\\boldsymbol{y})$ : The optimal solution for $y$ is given by the corresponding eigenvector $\\boldsymbol{x}$, referred as the [Fiedler vector (Algebraic_connectivity)](https://en.m.wikipedia.org/wiki/Algebraic_connectivity)\n",
        "\n",
        "* see how to create Laplacian matrix from incidence matrix: [Spectral Partitioning, Part 1 The Graph Laplacian](https://www.youtube.com/watch?v=rVnOANM0oJE)\n",
        "* B captures relationship between different nodes and edges, rows are nodes and columns are edges of G (pair start-sink)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1390.png)\n",
        "\n",
        "Source: [(Lecture 14) Graph Laplacians](https://www.youtube.com/watch?v=M9F7zT9Gg-kI)\n",
        "\n",
        "**How can you use this property? - The number of cut edges equals $\\frac{1}{4} x^T L(G) x$ under specific constraints**.\n",
        "  * Means: if you want to minimize edge cuts, you should try minimizing this product (it‚Äôs a combinatorial optimization problem).\n",
        "  * Cut edges: find minimal number of connections, bottleneck, borders between two clusters.\n",
        "  * This problem is however np complete. You need to relax sum contraints.\n",
        "\n",
        "*If you relax it you can combine it with the [Courant-Fisher minimax theorem (Satz von Courant-Fischer)](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer):*\n",
        "* if we allowed to use any vector y, where y is normalized in a certain way, and it‚Äôs elements sum to 0, then the vector y that minimizes this quantity is actually q1.\n",
        "* And q1 is the Eigenvector corresponding to the second smallest Eigenvalue.\n",
        "* And in fact the minimum value simplifies to something that is proportional to that eigenvalue.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1388.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1387.png)"
      ],
      "metadata": {
        "id": "Nw6nUk4bb4ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application example: Problem statement for using the Laplacian matrix of a graph:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1384.png)\n",
        "\n",
        "Source: [Lecture 30 ‚Äî The Graph Laplacian Matrix (Advanced) | Stanford University](https://www.youtube.com/watch?v=FRZvgNvALJ4)\n",
        "\n",
        "\n",
        "Sum over all neighbouring edges:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1385.png)\n",
        "\n",
        "\n",
        "**Solution siehe** [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), um den zweitkleinsten Eigenwert der Laplacian matrix zu finden (spectral gap):\n",
        "\n",
        "> $(R_{A}(x)=) \\quad \\lambda_2 = {min \\frac {x^{T} M x}{x^{T}x}}$ with matrix $M$ being the Laplacian matrix $L$\n",
        "\n",
        "Meaning of (min $x^T L x$): I take the label of one endpoint edge, subtract the value of the other endpoint of an edge, then square it up, and sum this over all the edges. (see also in green on the bottom why quadratic forms are relevant here!)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1381.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1382.png)\n",
        "\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1383.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "> **Note that the largest eigenvalue of the adjacency matrix corresponds to the smallest eigenvalue of the Laplacian.** But: Where the smallest eigenvector of the Laplacian is a constant vector, the largest eigenvector of an adjacency matrix, called the Perron vector, need not be. The Perron-Frobenius theory tells us that the largest eigenvector of an adjacency matrix is non-negative, and that its value is an upper bound on the absolute value of the smallest eigenvalue. These are equal precisely when the graph is bipartite.\n"
      ],
      "metadata": {
        "id": "CYNJpwY7BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRiQv6BoMmA"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionalanalysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "ppApZ0cS7_F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green's Function**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Greensche_Funktion\n",
        "\n",
        "Video: [Green's functions: the genius way to solve DEs](https://youtu.be/ism2SfZgFJg)\n",
        "\n",
        "Video: [Green's Functions](https://youtu.be/-riPW1yt_fA)\n",
        "\n",
        "Video: [Introducing Green's Functions for Partial Differential Equations (PDEs)](https://youtu.be/xNqLZnM-PPY)\n",
        "\n",
        "Video: [Green's functions, Delta functions and distribution theory](https://youtu.be/AqfYSNsrnhI)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1334.jpg)\n"
      ],
      "metadata": {
        "id": "qvEaRxI18Q6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4I7IyMVeJte"
      },
      "source": [
        "**Schwache Ableitung & schwache L√∂sung**\n",
        "\n",
        "* Eine [schwache Ableitung](https://de.wikipedia.org/wiki/Schwache_Ableitung) bzw. [weak derivative](https://en.wikipedia.org/wiki/Weak_derivative) ist in der Funktionalanalysis, einem Teilgebiet der Mathematik, eine Erweiterung des Begriffs der gew√∂hnlichen (klassischen) Ableitung.\n",
        "\n",
        "* Er erm√∂glicht es, Funktionen eine Ableitung zuzuordnen, die nicht (stark bzw. im klassischen Sinne) differenzierbar sind.\n",
        "\n",
        "* Schwache Ableitungen spielen eine gro√üe Rolle in der Theorie der partiellen Differentialgleichungen. **R√§ume schwach differenzierbarer Funktionen sind die Sobolev-R√§ume**.\n",
        "\n",
        "* Ein noch allgemeinerer Begriff der Ableitung ist die Distributionenableitung.\n",
        "\n",
        "* In mathematics, a weak derivative is a generalization of the concept of the derivative of a function (strong derivative) **for functions not assumed differentiable, but only integrable**, i.e., to lie in the $L^{p}$ space $L^{1}([a, b])$. See distributions for a more general definition.\n",
        "\n",
        "**This concept gives rise to the definition of [weak solutions](https://en.wikipedia.org/wiki/Weak_solution) in Sobolev spaces, which are useful for problems of differential equations and in functional analysis.**\n",
        "\n",
        "The **absolute value function** u : [‚àí1, 1] ‚Üí [0, 1], u(t) = |t|, which is not differentiable at t = 0, has a weak derivative v known as the [**sign function**](https://de.wikipedia.org/wiki/Vorzeichenfunktion) given by\n",
        "\n",
        "![gg](https://mathepedia.de/img/Abs_x.png)\n",
        "\n",
        "*Signumfunktion als schwache Ableitung der Betragsfunktions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbR-3EnGQ33f"
      },
      "source": [
        "**Anfangswertprobleme (Cauchy-Problem)**\n",
        "\n",
        "* Beispielsweise werden alle schwingenden Pendel durch eine Differentialgleichung beschrieben (siehe: Pendelgleichung), und der generelle Bewegungsablauf folgt immer dem gleichen Prinzip. Der konkrete Bewegungsablauf ist jedoch durch die Rand- oder Anfangsbedingung(en) (wann wurde das Pendel angesto√üen, und wie weit) bestimmt. Die L√∂sbarkeit von Anfangswertproblemen bei gew√∂hnlichen Differentialgleichungen 1. Ordnung wird durch den Satz von Picard-Lindel√∂f beschrieben. https://mathepedia.de/Gewoehnliche_Differentialgleichungen.html\n",
        "\n",
        "* Wer zu einer Differentialgleichung eine [**Anfangsbedingung**](https://de.wikipedia.org/wiki/Anfangsbedingung) hinzuf√ºgt, stellt damit ein **Anfangswertproblem**. Eine besonders spannende Frage lautet dabei, wie eine Anfangsbedingung zu einer gegebenen Differentialgleichung beschaffen sein muss, damit das entstehende Anfangswertproblem **genau eine eindeutig bestimmte L√∂sung zul√§sst**.\n",
        "\n",
        "Der freie Fall (etwa eines Apfels vom Baum) wird beschrieben durch die Bewegungsgleichung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "y^{\\prime \\prime}(t) &=-g \\\\\n",
        "\\Rightarrow y^{\\prime}(t) &=-g \\cdot t+v_{0}\n",
        "\\end{aligned}$\n",
        "\n",
        "mit der Konstanten $g \\approx 9,81 \\mathrm{~m} / \\mathrm{s}^{2}$ (Erdbeschleunigung).\n",
        "Die L√∂sungsmenge dieser Differentialgleichung besteht zun√§chst aus allen Funktionen der Form\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=-\\frac{1}{2} g t^{2}+v_{0} \\cdot t+y_{0}\n",
        "$\n",
        "\n",
        "mit beliebigen Integrationskonstanten $y_{0}$ und $v_{0}$.\n",
        "Eine m√∂gliche Anfangsbedingung sagt z. B. aus, dass der Apfel zu Beginn der Bewegung an einem\n",
        "Ast in drei Metern H√∂he h√§ngt:\n",
        "\n",
        ">$\n",
        "y(0)=y_{0}=3 \\mathrm{~m}\n",
        "$\n",
        "\n",
        "und sich in Ruhe befindet:\n",
        "\n",
        ">$\n",
        "y^{\\prime}(0)=v_{0}=0 \\mathrm{~m} / \\mathrm{s}\n",
        "$\n",
        "\n",
        "Diese Anfangsbedingung zeichnet nun in der L√∂sungsmenge der Differentialgleichung die eine Funktion\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=3 \\mathrm{~m}-\\frac{1}{2} g t^{2}\n",
        "$\n",
        "\n",
        "als die eindeutig bestimmte L√∂sung des Anfangswertproblems aus (Loesung dann zB uber das [Runge-Kutta-Verfahren](https://de.wikipedia.org/wiki/Runge-Kutta-Verfahren) - Einschrittverfahren zur n√§herungsweisen L√∂sung von Anfangswertproblemen in der numerischen Mathematik)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfUxHFgakA7f"
      },
      "source": [
        "**Randwertproblem (Boundary value)**\n",
        "\n",
        "* Bei partiellen Differentialgleichungen, wenn also die gesuchte Funktion nicht nur von einer, sondern von mehreren Variablen abh√§ngt, werden oftmals [Randbedingungen](https://de.wikipedia.org/wiki/Randbedingung) an Stelle von Anfangsbedingungen verwendet. Manchmal wird dann der Spezialfall einer Randbedingung, deren Definitionsbereich eine Hyperebene im vollen Definitionsbereich der Differentialgleichung bildet, Anfangsbedingung genannt.\n",
        "\n",
        "* In der Betriebswirtschaftslehre und der Volkswirtschaftslehre entsprechen die Randbedingungen den kurzfristig oder gar nicht durch den Entscheidungstr√§ger beeinflussbaren Datenparametern wie beispielsweise die Umweltzust√§nde der Witterung oder der Gesetze.\n",
        "\n",
        "Sei die gegebene Differentialgleichung $y^{\\prime \\prime}(x)=-y(x)$. Die L√∂sungsmenge dieser Gleichung ist $a \\sin (x)+b \\cos (x)$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=1$ und $y(\\pi / 2)=0 \\Rightarrow$ Die L√∂sung ist $y=\\cos (x)$.\n",
        "\n",
        "* Periodische Randbedingung: Gesucht≈Çist die L√∂sung mit $y(0)=0$ und $y(\\pi)=0 \\Rightarrow$ Es gibt unendlich viele L√∂sungen der Form $a \\sin (x)$ mit beliebigem $a$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=0$ und $y(2 \\pi)=1 \\Rightarrow$ Es gibt keine L√∂sung.\n",
        "\n",
        "Arten von Randbedingungen (Es gibt unterschiedliche M√∂glichkeiten, auf dem Rand des betrachteten Gebietes Werte vorzuschreiben):\n",
        "\n",
        "* Werte der L√∂sung vorschreiben; im Fall einer auf dem Intervall $[a, b]$ definierten gew√∂hnlichen Differentialgleichung schreibt man also $y(a)$ und $y(b)$ vor und spricht\n",
        "dann von [**Dirichlet-Randbedingungen**](https://de.wikipedia.org/wiki/Dirichlet-Randbedingung).\n",
        "\n",
        "* Bedingungen an die Ableitungen stellen, also $y^{\\prime}(a)$ und $y^{\\prime}(b)$ vorgeben, dann spricht man [**von Neumann-Randbedingungen**](https://de.wikipedia.org/wiki/Neumann-Randbedingung) (bei gew√∂hnlichen Differentialgleichungen, wie oben ausgef√ºhrt, von Anfangsbedingungen).\n",
        "\n",
        "* Ein Spezialfall sind [**periodische Randbedingungen**](https://de.wikipedia.org/wiki/Periodische_Randbedingung), hier muss (im Beispiel einer auf dem Intervall $[a, b]$ betrachteten gew√∂hnlichen Differentialgleichung) gelten: $y(a)=y(b)$ bzw. $y^{\\prime}(a)=y^{\\prime}(b)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partielle Differentialgleichung**\n",
        "\n",
        "> https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419\n",
        "\n",
        "* **[Partielle Differentialgleichungen](\n",
        "https://de.wikipedia.org/wiki/Partielle_Differentialgleichung) werden in erster Linie durch Trennung der Variablen und sp√§tere Integration gel√∂st.**\n",
        "\n",
        "* Sobolevr√§ume sind ein grundlegendes Werkzeug bei der Behandlung von PDE's (rein und angewandt).\n",
        "\n",
        "* See also Variational Formulations - wichtig fur partielle Differentialgleichungen\n",
        "\n",
        "* [Euler-Gleichungen (Str√∂mungsmechanik)](https://de.wikipedia.org/wiki/Euler-Gleichungen_(Str√∂mungsmechanik)) bildet dann ein System von **nichtlinearen partiellen** Differentialgleichungen **erster Ordnung** Wird die Viskosit√§t vernachl√§ssigt ($\\eta =\\lambda =0$), so erh√§lt man die Euler-Gleichungen (hier f√ºr den kompressiblen Fall)\n",
        "\n",
        "> $\\rho \\frac{\\partial \\vec{v}}{\\partial t}+\\rho(\\vec{v} \\cdot \\nabla) \\vec{v}=-\\nabla p+\\vec{f}$\n",
        "\n",
        "* [**Navier-Stokes-Gleichungen**](https://de.wikipedia.org/wiki/Navier-Stokes-Gleichungen) bilden dann ein System von **nichtlinearen partiellen** Differentialgleichungen **zweiter Ordnung**\n",
        "\n",
        "  * Die Gleichungen sind eine **Erweiterung der Euler-Gleichungen** der Str√∂mungsmechanik **um Viskosit√§t beschreibende Terme** (Die Navier-Stokes-Gleichungen beinhalten die Euler-Gleichungen als den Sonderfall, in dem die innere Reibung (Viskosit√§t) und die W√§rmeleitung des Fluids vernachl√§ssigt werden.)\n",
        "\n",
        "  * Die Navier-Stokes-Gleichungen bilden das Verhalten von Wasser, Luft und √ñlen ab und werden daher in diskretisierter Form bei der Entwicklung von Fahrzeugen wie Autos und Flugzeugen angewendet.\n",
        "\n",
        "  * Dies geschieht in N√§herungsform, da keine exakten analytischen L√∂sungen f√ºr diese komplizierten Anwendungsf√§lle bekannt sind.\n",
        "\n",
        "  * Siehe auch https://de.wikipedia.org/wiki/Numerische_Str√∂mungsmechanik\n",
        "\n",
        "> $\\rho \\overrightarrow{\\vec{v}}=\\rho\\left(\\frac{\\partial \\vec{v}}{\\partial t}+(\\vec{v} \\cdot \\nabla) \\vec{v}\\right)=-\\nabla p+\\mu \\Delta \\vec{v}+(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\vec{v})+\\vec{f}$\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Potentialstr√∂mung\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Black-Scholes-Modell\n",
        "\n",
        "* Loesungsverfahren\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Finite-Elemente-Methode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Spektralmethode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Liste_numerischer_Verfahren\n",
        "\n",
        "  * https://en.wikipedia.org/wiki/Method_of_characteristics\n",
        "\n",
        "* Die Methode der Charakteristiken ist eine Methode zur L√∂sung partieller\n",
        "Differentialgleichungen (PDGL/PDE), die typischerweise erster Ordnung und quasilinear sind\n",
        "\n",
        "* Die grundlegende Idee besteht darin, **die PDE durch eine geeignete Koordinatentransformation auf ein System gew√∂hnlicher Differentialgleichungen auf bestimmten Hyperfl√§chen, sogenannten Charakteristiken, zur√ºckzuf√ºhren.**\n",
        "\n",
        "* Die PDE kann dann als Anfangswertproblem in dem neuen System mit Anfangswerten auf den die Charakteristik schneidenden Hyperfl√§chen gel√∂st werden. St√∂rungen breiten sich l√§ngs der Charakteristiken aus.\n",
        "\n",
        "* Charakteristiken spielen eine Rolle in der qualitativen Diskussion der L√∂sung bestimmter PDE und in der Frage, wann Anfangswertprobleme f√ºr diese PDE korrekt gestellt sind.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdOgQKlZAssy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving the heat equation:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}=\\alpha \\nabla^{2} T$\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$\n",
        "\n",
        "* can be solved with Fourier series\n",
        "\n",
        "* [DE2: But what is a partial differential equation?](https://www.youtube.com/watch?v=ly4S0oi3Yz8&list=PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6&index=2)\n",
        "\n",
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$"
      ],
      "metadata": {
        "id": "aO7NYOE50lp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_001.png)"
      ],
      "metadata": {
        "id": "RRBeNGa92iKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small change in time:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_002.png)"
      ],
      "metadata": {
        "id": "EQLycVW52oK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small step in space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_003.png)"
      ],
      "metadata": {
        "id": "RKaZoj0J2pCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What that actually encodes is that we look at the limit of that ratio (temperature/space or time change) for smaller and smaller nudges to the input rather than a specific finitely small value\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_004.png)"
      ],
      "metadata": {
        "id": "YE3q91CB3c03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$ tells us that\n",
        "\n",
        "* the way this function changes with respect to time $\\frac{\\partial T}{\\partial t}(x, t)$\n",
        "\n",
        "* depends on how it changes with respect to space $\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$.\n",
        "\n",
        "* <font color=\"blue\">More specifically it's proportional to the second partial derivative with respect to x.</font>\n",
        "\n",
        "* at a high level, the intuition is that at points where the temperature distribution curves, it tends to change more quickly in the direction of that curvature.\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_005.png)"
      ],
      "metadata": {
        "id": "3cBh_epu3xyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$\n",
        "\n",
        "*It checks how different is a point from the average of its neigbours.* (you use second derivative for it)\n",
        "\n",
        "$\\frac{dT_2}{dt}$ = $\\Delta T_2$ - $\\Delta T_1$ (difference of differences) = $\\Delta \\Delta T_1$ (second difference = second derivative)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_007.png)"
      ],
      "metadata": {
        "id": "w9EjvvKl_LWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "* imagine discrete points. and then check the average of the neighbouring points.\n",
        "\n",
        "* if a point is much higher temperature than its neighbours, it will decrease. If it's lower, it will increase.\n",
        "\n",
        "* the equation reflects this relationship between the points and their differences\n",
        "\n",
        "* and the \"second difference\" is for continuous cases the \"second derivative\":\n",
        "\n",
        "> $\\frac{d T_{2}}{d t}=\\frac{\\alpha}{2} \\Delta \\Delta T_{1}$ $\\rightarrow$ $\\frac{\\partial T}{\\partial t}=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}$\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.** (you use second derivative for it)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_006.png)\n",
        "\n",
        "For higher dimensions than 1 (like 3D) you use also the second derivative, but add the other dimensions as well, and the result is called nabla, the Laplacian:\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ],
      "metadata": {
        "id": "qyKPD8oU7rEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Fourier, Laplace & Taylor*"
      ],
      "metadata": {
        "id": "O8cFhBpbIFFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fourier Analysis*"
      ],
      "metadata": {
        "id": "7bWObSh9Mh0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Discrete Fourier Transform](https://youtu.be/yYEMxqreA10)"
      ],
      "metadata": {
        "id": "CJhXQ68Ua1uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The maths behind fourier transform](https://youtu.be/FOOQrrOo-II)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1343.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1344.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1346.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1345.jpg)"
      ],
      "metadata": {
        "id": "Clt-PAZNLm0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $f(x)$ is a periodic function with period $2 \\pi$. If we want to approximate this function with a trigonometric polynomial of degree $n$.\n",
        "\n",
        "**Step 1: Approximate periodic functions using infinite sums of sines and cosines:**\n",
        "\n",
        "> <font color=\"blue\">$F_n(x)=a_0+a_1 \\cos (x)+b_1 \\sin (x)+a_2 \\cos (2 x)+b_2 \\sin (2 x)+\\cdots+a_n \\cos (n x)+b_n \\sin (n x)$\n",
        "\n",
        "For $k \\geq 1$ the \"best\" coefficients to use are the following Fourier coefficients:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "&a_0=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(x) d x \\\\\n",
        "&a_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\cos (k x) d x \\\\\n",
        "&b_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\sin (k x) d x\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Step 2: Replace sin and cosine with $e$ using Eulers Formula:**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$\n",
        "\n",
        "\n",
        "we get this euqation:\n",
        "\n",
        "> <font color=\"blue\">$X_k=x_0 e^{-b_0 j}+x_1 e^{-b_1 j}+\\ldots+x_n e^{-b_{N-1} j}$</font>\n",
        "\n",
        "**Step 3: Taking the sum where where ${\\frac{2 \\pi k n}{N}} = b_n$** for the Discrete Fourier Transform (oft hat man nur einige Messpunkte / Samples aus Experiment oder Simulation)\n",
        "\n",
        "> <font color=\"blue\">$X_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-\\frac{j 2 \\pi k n}{N}}$</font>\n",
        "\n",
        "\n",
        "$\\omega=\\frac{2 \\pi n}{T}$ ist die Kreisfrequenz der diskreten Fourier Transform f√ºr nicht-periodische Funktionen, wodurch man auch schreiben kann:\n",
        "\n",
        "> <font color=\"blue\">$F(\\omega)_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-i \\omega k}$</font> $\\quad$ ([Source](https://de.m.wikipedia.org/wiki/Fourierreihe#Zusammenhang_mit_der_Fourier-Transformation_f√ºr_nicht-periodische_Funktionen))\n",
        "\n",
        "$\\omega_{k}=\\frac{k \\pi}{T}$ is the basic frequency and you can expand f(x) as a sum of sines and cosines that are also periodic in 2 T, and **higher and higher harmonics of those basic sines and cosines (and each with a contribution factor)**.\n",
        "\n",
        "$k = \\frac{2 \\pi}{wavelength}$, e.g. wavelength = $4 \\pi$ (2 complete turns within one period of time), then $k = \\frac{1}{2}$.\n",
        "\n",
        "**Step 4: Understand orthogonal projection**: Dabei ist folgendes ein Skalarprodukt / inner product / projection:\n",
        "\n",
        "> $\\langle x_{n}, e^{-i \\omega k}\\rangle$\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$\n",
        "\n",
        "**Step 5: Get Continous Fourier Transform:** For very large N (the resolution with which we can resolve different frequencies becomes infinitesimally small) the sum $\\sum$ is becoming a Riemann integral (continous case) -  If a Fourier series can approximate a function well on an intervall, can it do it also on the whole real line between -$\\infty$ and $\\infty$?\n",
        "\n",
        "> <font color=\"blue\">$X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$</font>\n",
        "\n",
        "where we calculate the coefficients $a_k$ and $b_k$ at each particular frequency with function $x(t)$ and **analyzing function (sinusoids) $e^{-j 2 \\pi F t} d t$**.\n",
        "\n",
        "  * you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "  * wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient\n",
        "\n",
        "  * and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "*Appendix: Instead of getting one complex coefficient per frequency you can also work with two real coefficients per frequency and end up with two integrals:*\n",
        "\n",
        "* one to correlate with signal with the cosine function:\n",
        "\n",
        "> $x_{a}(F)=\\int_{-\\infty}^{\\infty} x(t) \\cos 2 \\pi \\, Ft \\,d t$\n",
        "\n",
        "* and one to correlate the signal with the sine function:\n",
        "\n",
        "> $X_{b}(F)=\\int_{-\\infty}^{\\infty} x(t) \\sin 2 \\pi \\, Ft \\, d t$\n"
      ],
      "metadata": {
        "id": "xto8Ha2DK0Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Whereas a Taylor Series attempts to approximate a function locally about the point where the expansion is taken, a Fourier series attempts to approximate a periodic function over its entire domain. That is, a Tavlor series approximates a function point wise and a Fourier series approximates a function globally.*\n",
        "\n",
        "* [Steve Brunton: The Fourier Transform](https://www.youtube.com/watch?v=jVYs-GTqm5U)\n",
        "\n",
        "* [Looking Glass: Fourier Transform](https://youtu.be/Xxut2PN-V8Q)\n",
        "\n",
        "* https://sites.oxy.edu/ron/math/120/03/labs/lab8.PDF\n",
        "\n",
        "* https://math.stackexchange.com/questions/47430/is-fourier-series-an-inverse-of-taylor-series\n",
        "\n",
        "* http://dev.ipol.im/~coco/website/taylorfourier.html\n",
        "\n",
        "* https://math.stackexchange.com/questions/7301/connection-between-fourier-transform-and-taylor-series"
      ],
      "metadata": {
        "id": "Z6mGDVo2QU7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Start with approximation of f(x) between -$\\pi$ and $\\pi$ with some orthogonal base vectors made by sin and cos and their coefficients A and B calculated via the inner product with f(x) with each frequency of sin and cos (all are orthogonal base vectors)**\n",
        "\n",
        "**Approximate <u>periodic functions</u>: [Fourier Series](https://de.m.wikipedia.org/wiki/Fourierreihe)**\n",
        "\n",
        "You expand f(x) as a sum of sines and cosines (Grundt√∂ne) that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics (Obert√∂ne) of those basic sines and cosines.\n",
        "\n",
        "> <font color=\"blue\">$f(t)=$$\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left[a_{n} \\cdot \\cos \\left(n \\omega_{0} t\\right)+b_{n} \\sin \\left(n \\omega_{0} t\\right)\\right]$\n",
        "\n",
        "mit folgenden Termen:\n",
        "\n",
        "* <font color=\"blue\">$a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$</font>\n",
        "\n",
        "* <font color=\"blue\">$\\omega_{0}=\\frac{2 \\pi}{T}$</font> $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "* $T$ *ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "* <font color=\"blue\">$a_n$ und $b_n$</font>: siehe weiter unten unter 'orthogonale Projektion'\n",
        "\n",
        "\n",
        "We can approximate f(x) with an expansion of sine and cosines of higher and higher frquency (and shorter wavelength):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos (k x)+B_{k} \\sin (k x)\\right)$\n",
        "\n",
        "How do we calculate the coefficients? You calculate the (Hilbert space) inner product between f(x) and cos(kx) (=the particular k-th frequency cosine wave) bzw. sin(kx). I project f(x) into the cosine k-direction (and I need to normalize by this function):\n",
        "\n",
        "> $A_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos (k x) d x$ = $\\frac{1}{||cos(kx)||^2}$ $\\langle f(x), cos(k x)\\rangle$\n",
        "\n",
        "> $B_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin (k x) d x$ = $\\frac{1}{||sin(kx)||^2}$ $\\langle f(x), sin(k x)\\rangle$\n",
        "\n",
        "**Fourier transform is just another representation on another orthogonal basis vectors (sine and cosine with different frequencies).**\n"
      ],
      "metadata": {
        "id": "7NB7Ne-NuLnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Inner product / projection on orthogonal basis vectors to get coefficients**\n",
        "\n",
        "**<u>Orthogonal Projection (Inner Product)</u>: Calculate the coefficients $a_k$ and $b_k$ bzw. $e$ at each particular frequency**\n",
        "\n",
        "*Why do we use inner product / orthogonal projection? - If you`re familiar with calculating correlations, the Fourier transform is essentially the same:*\n",
        "\n",
        "* you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "* <font color=\"blue\">wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient (=inner product large when vectors are not orthogonal)</font>\n",
        "\n",
        "* and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "> <font color=\"blue\">$a_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"blue\">$b_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"orange\">$\\langle f(t) , \\cos (x) \\rangle$</font> = Inner Product / Projection of f(t) on cos(x) to get coefficient (how similar?)\n",
        "\n",
        "*Die Koeffizienten berechnen den Anteil den f(x) jeweils an der analysing function hat. Oben multipliziert man diesen Anteil dann mit der jeweiligen analysing function (sehr √§hnlich zu probabilities of states in quantum mechanics). Das ist eine orthogonale Projektion (inner product f(x) mit analysing function) am unteren Beispiel:*\n",
        "\n",
        "> $a_n$ = $\\frac{2}{2} \\int_{0}^{1}(1-t) \\cdot \\cos (n \\pi t) d t$\n",
        "\n",
        "$\\rightarrow$ dieser Teil ist das inner product / Projektion von $f(x) = 1-t$ auf die analysing function $\\cos (n \\pi t) d t$\n",
        "\n",
        "\n",
        "*For when you use $e$ instead of sin & cos: We want to know the $c_k$ coefficients by projecting f(x) onto orthogonal basis vectors sin and cos*\n",
        "\n",
        "<font color=\"black\">The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$\n",
        "\n",
        "> <font color=\"blue\">$c_{k}$$=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$</font>$=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle$\n",
        "\n",
        "\n",
        "* we want to calculate the coefficients $a_k$ and $b_k$ at each particular frequency (and each frequency is orthogonal to each other, both cos vs sine as well as cos k vs cos j)\n",
        "\n",
        "> <font color=\"blue\">$c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$ (inner product)\n",
        "\n",
        "\n",
        "**Most important is the inner product / projection on orthogonal basis vectors to get coefficients!!**\n",
        "\n",
        "The Fourier series definition is exactly the same as how we write a vector F in an orthogonal basis in $R^2$, in a 2 dimensional vector space.\n",
        "\n",
        "* I pick some basis (picture on top right) for example x and y,\n",
        "\n",
        "  * and then I take the inner product of F in the x-direction and then the inner product of F in the y-direction,\n",
        "\n",
        "  * and I take those coefficients, let's call it $a_1$ and $a_2$, and I take them and multiply them by the X unit vector and the Y unit vector, and I add them up\n",
        "\n",
        "* and in Fourier: sine and cosine functions are orthogonal, just like x and y are orthogonal vectors.\n",
        "\n",
        "  * And then i take my function f and project it on the sine and cosine to see how much of f is in this cosine direction and how much of f is in the sine direction\n",
        "\n",
        "  * from that I get my $A_k$-th coefficient and $B_k$-th coefficient, and then I multiply that by my cosine function (and sine function) and I add all of those up\n",
        "\n",
        "\n",
        "> $a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$\n",
        "\n",
        "> $a_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $b_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $\\omega_{0}=\\frac{2 \\pi}{T}$ $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "*T ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pY8yHtyJwMjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Finetune with approximation of f(x) to get between 0 and $L$, which is periodic in L and repeats beyond 0 and L**\n",
        "\n",
        "* we go now in the domain 0 to L (before above it was -$\\pi$ to $\\pi$)\n",
        "\n",
        "* now we make the sines and cosines periodic between 0 and L (=space of Lebesgue integrable functions)\n",
        "\n",
        "> $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "We the Fourier series for L-periodic functions (and not just 2 $\\pi$ like before):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos \\left(\\frac{2 \\pi k x}{L}\\right)+B_{k} \\sin \\left(\\frac{2 \\pi k x}{L}\\right)\\right)$\n",
        "\n",
        "How do we get A and B?\n",
        "\n",
        "* We take the inner product of f(x) with $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$ (und entsprechend auch fur sin)\n",
        "\n",
        "* this inner product is the projection of f(x) onto the orthogonal basis vector (here each k - $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$)\n",
        "\n",
        "* and then normalized by the norm of the cosine function, which in thhis case the norm squared is $\\frac{2}{L}$:\n",
        "\n",
        "> $A_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "> $B_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\sin \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "* these cosine and sine functions are an orthogonal basis for my function space (Hilbert space of functions f(x))\n",
        "\n",
        "* If I plugged in a cosine k and a sine j, these functions are orthogonal. Like if I plug in two cosines with different k's, their inner product is 0 (because they should be orthogonal). Umgekehrt: if I plug in cos kx and cos kx then I would get a non zero inner product.\n",
        "\n",
        "* btw: the approximation of f(x) between 0 and L will repeat infineily beyond 0 and l."
      ],
      "metadata": {
        "id": "52T4qM5-A2r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Introduce $e^{ikx}$ to combine sin and cos in one complex coefficient**\n",
        "\n",
        "**<u>Analysing Function</u>: Statt sin und cos kann man auch $e$ verwenden**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$ (Eulers formula)\n",
        "\n",
        "$e^{i k x}$ is the analyzing function made of sinusoids (eigentlich: $e^{-j 2 \\pi F t} d t$)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i x \\frac{k \\pi}{L}}$</font> (becoming a Riemann integral) with $\\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$</font> $= \\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "\n",
        "\n",
        "Anderes Beispiel mit einem Integral statt Summe geschrieben:\n",
        "\n",
        "> $X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$\n",
        "\n",
        "* basics: remember the Euler expansion:\n",
        "\n",
        "> $e^{i k x}=\\cos (k x)+i \\sin (k x)$\n",
        "\n",
        "* now we talk about the fourier series in terms of a complex basis:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$\n",
        "\n",
        "(you could expand that in sin and cos: $f(x)=\\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "See also [Inner Products in Hilbert Space](https://www.youtube.com/watch?v=g-eNeXlZKAQ&list=PLMrJAkhIeNNT_Xh3Oy0Y4LTj0Oxo8GqsC&index=4)\n",
        "\n",
        "* inner product of functions is consistent with definition of inner products of vectors\n",
        "\n",
        "* similar functions should have a large inner product\n",
        "\n",
        "* if I go to infinity with delta x (make it finer and finer), then the Riemann approximation $\\langle f,g \\rangle$ becomes the continuous integral formulation $\\int$\n",
        "\n",
        "You can expand f(x) as a sum of sines and cosines that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics of those basic sines and cosines.\n",
        "\n",
        "You can represent this as a complex Fourier series:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k \\pi \\frac{x}{L}} \\quad \\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency / Kreisfrequenz)\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$"
      ],
      "metadata": {
        "id": "dLbwYXpgvwx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√§herungsverfahren f√ºr periodische Funktionen: Fourier Series**\n",
        "\n",
        "* nutzt man weil zB manche periodische Funktionen sehr kompliziert sind, um sie mit einer Funktion f(x) zu beschreiben\n",
        "\n",
        "**N√§herungsverfahren f√ºr nicht-periodische Funktionen: Fourier Transform**\n",
        "\n",
        "**N√§herungsverfahren f√ºr Polynome: Taylor Polynom**\n",
        "\n",
        "https://medium.com/sho-jp/fourier-transform-101-part-1-b69ea3cb4837\n",
        "\n",
        "**Harmonic Analysis**\n",
        "\n",
        "Aus Sicht der [abstrakten harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) sind sowohl die Fourier-Reihen und die Fourier-Integrale als auch die Laplace-Transformation, die Mellin-Transformation oder auch die Hadamard-Transformation Spezialf√§lle einer **allgemeineren (Fourier-)Transformation**."
      ],
      "metadata": {
        "id": "MWiANZ7vSEOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "*We can clearly observe a peak value at 10 Hz with a magnitude of one while all other frequencies hover around zero. We can verify this from the original signal where there are 10 complete cycles in a second with an amplitude of one:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_49.png)\n",
        "\n",
        "*When a similar principle is applied to a more complicated time series as shown in the green plot below, we can deduce from its Fourier transform that the data comprises of 3 different elementary components with 3 different frequencies (2, 5 and 10 Hz) at 3 different amplitudes (0.5, 1 and 2):*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_50.png)\n",
        "\n",
        "Source: https://medium.com/@khairulomar/deconstructing-time-series-using-fourier-transform-e52dd535a44e"
      ],
      "metadata": {
        "id": "1q0EqJnVRvQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetrien**\n",
        "\n",
        "Die Kosinusfunktion ist achsensymmetrisch:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_51.png)\n",
        "\n",
        "In Worten: Ein x-Wert und der negative x-Wert haben denselben Kosinuswert. Als Formel: cos(‚àíùë•)=cosùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        ">$\\cos \\left(\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        ">$\\cos \\left(-\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        "Die Sinusfunktion ist punktsymmetrisch zum Koordinatenursprung. Stelle dir vor, wie du den rechten Arm des Graphen um (0|0) drehst.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_52.png)\n",
        "\n",
        "In Worten: sin(‚àíùë•)\n",
        "sin\n",
        "(\n",
        "-\n",
        "x\n",
        ")\n",
        " ist sinùë•\n",
        "sin\n",
        "x\n",
        " mit umgedrehtem Vorzeichen.\n",
        "Als Formel: sin(‚àíùë•)=‚àísinùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "> $\\sin \\left(\\frac{\\pi}{4}\\right)=0,71$\n",
        "\n",
        "> $\\sin \\left(-\\frac{\\pi}{4}\\right)=-0,71$\n",
        "\n",
        "Source: https://www.kapiert.de/sinus-und-kosinusfunktionen-eigenschaften/\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax6bbzUGSd6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_53.png)\n",
        "\n",
        "*Source: [Mathe mit Nina](https://youtu.be/u6Fqi8596qA)*\n"
      ],
      "metadata": {
        "id": "bdefArXifISa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeO_i6Z9-rLD"
      },
      "source": [
        "###### *Laplace Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhmFbOnmaUS"
      },
      "source": [
        "**Fourier** $\\quad X(\\omega)=\\int_{-\\infty}^{\\infty} x(t) e^{-i \\omega t} d t$\n",
        "\n",
        "**Laplace** $\\quad X(s)=\\int_{0}^{\\infty} x(t) e^{-s t} d t$\n",
        "\n",
        "with s = ${\\alpha + i \\omega}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1193.png)"
      ],
      "metadata": {
        "id": "msM9RjFYPKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1194.png)"
      ],
      "metadata": {
        "id": "qke32YL1QCrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Taylor Series*"
      ],
      "metadata": {
        "id": "xDUD9L1G7HIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor Series Expansion to approximate $e^x$**\n",
        "\n",
        "> <font color=\"blue\">$e^{x} \\approx \\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !} \\approx 1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\ldots$\n",
        "\n",
        "*(N√§herungsverfahren f√ºr Polynome: Taylor Polynom)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1347.jpg)\n",
        "\n",
        "Funktionsvorschrift:\n",
        "\n",
        "$a_{i}=a_{1} \\cdot q^{i-1}$ bzw $a_{i}=a_{0} \\cdot q^{i}$ f√ºr Anfangsglied a1. Oder auch (andere Folge): $a_{i}=a_{0} \\cdot q^{i}$\n",
        "\n",
        "Rekursionsvorschrift (wie Fibonacci):\n",
        "\n",
        "$a_{i+1}=a_{i} \\cdot q$. Oder auch (andere Folge): $a_{i}=q \\cdot a_{i-1}$\n",
        "\n",
        "*The following code examples are taken from ['Python for Undergraduate Engineers'](https://pythonforundergradengineers.com/creating-taylor-series-functions-with-python.html)*"
      ],
      "metadata": {
        "id": "rkbWd2CM6u6w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3QOyKV6llLn"
      },
      "source": [
        "**Taylor Series Expansion to approximate $cos(x)$**\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x) \\approx \\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !} \\approx 1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\frac{x^{8}}{8 !}-\\frac{x^{10}}{10 !}+\\ldots$\n",
        "\n",
        "* We can code this formula into a function that contains a for loop. Note the variable x is the value we are trying to find the cosine of, the variable n is the number of terms in the Taylor Series, and the variable i is the loop index which is also the Taylor Series term number.\n",
        "* We are using a separate variable for the coefficient coef which is equal to (‚àí1)<sup>i</sup>, the numerator num which is equal to x<sup>2i</sup> and the denominator denom which is equal to (2i!). Breaking the Taylor Series formula into three parts can cut down on coding errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Beziehung zwischen Sinus, Kosinus und Exponentialfunktion**\n",
        "\n",
        "Die [trigonometrischen Funktionen](https://de.m.wikipedia.org/wiki/Trigonometrische_Funktion) sind eng mit der [Exponentialfunktion](https://de.m.wikipedia.org/wiki/Exponentialfunktion) verbunden, wie die [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel) zeigt:\n",
        "\n",
        "Eigentlich hat $e$ nichts mit periodischen Funktionen wie $sin$ oder $cos$ zu tun. Aber f√ºgt man $i$ in den exponent von $e$, dann wird daraus eine rotierende, periodische Funktion.*\n",
        "\n",
        "**Part I: Let`s get the Taylor expansions for three common functions**\n",
        "\n",
        "> $e^{x}=1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{x^{5}}{5 !}+\\cdots$\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x)=1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\cdots$\n",
        "\n",
        "> <font color=\"red\">$\\sin (x)=x-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !}-\\frac{x^{7}}{7 !}+\\cdots$\n",
        "\n",
        "* there is some relation between these three:\n",
        "\n",
        "  * all all built upon $\\frac{x^{n}}{n !}$\n",
        "\n",
        "  * cosinus has all even numbers, sinus all odd numbers, and exponential both\n",
        "\n",
        "  * Vorzeichen is alternating for cos and sin, but all posisitv for exponential\n",
        "\n",
        "**Part II: Add $i$ to relate all three functions**\n",
        "\n",
        "* You can't just add up cos and sin to get exp! You need to add the $i = \\sqrt{-1}$\n",
        "\n",
        "* We need to replace all $x$ with an $i x$ in the exp series\n",
        "\n",
        "> $e^{ix}=1+ix+\\frac{(ix)^{2}}{2 !}+\\frac{(ix)^{3}}{3 !}+\\frac{(ix)^{4}}{4 !}+\\frac{(ix)^{5}}{5 !}+\\cdots$\n",
        "\n",
        "* It introduces the following: $i=i \\quad i^{2}=-1 \\quad i^{3}=-i \\quad i^{4}=1$\n",
        "\n",
        "* this turns the exponential into the following:\n",
        "\n",
        "> $e^{i x}=1+i x-\\frac{x^{2}}{2 !}-\\frac{ix^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{i x^{5}}{5 !}+$\n",
        "\n",
        "* finally, let's separate the terms with $i$ (all odd) from those without $i$ (all even):\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$\\left[1-\\frac{x^{2}}{2!}+\\frac{x^{4}}{4!}-\\cdots \\right]$</font> + $i$ <font color=\"red\">$\\left[x-\\frac{x^{3}}{3!}+\\frac{x^{5}}{5!}-\\cdots\\right]$</font>\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$cos(x)$</font> + $i$ <font color=\"red\">$ \\, sin(x)$</font>\n",
        "\n",
        "*$sin(x)$ ist der imagin√§re Anteil, und $cos(x)$ ist der reale Anteil f√ºr $z=e^{i \\varphi}=x+i y$*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/400px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "\n",
        "**Part III: Let`s replace $x$ with $\\pi$**\n",
        "\n",
        "> $e^{i \\pi}=\\cos (\\pi)+i \\sin (\\pi)$\n",
        "\n",
        "* sin and cos repeat and one full period is $2 \\pi$ radians (=360¬∞)\n",
        "\n",
        "* $cos(\\pi)$ is half a rotation = -1 and $sin(\\pi)$ = 0, which means:\n",
        "\n",
        "> $e^{i \\pi}= -1$ $\\,$ and $\\,$ $e^{2\\pi i} = 1$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIb2bU82PFrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Music Theory*"
      ],
      "metadata": {
        "id": "fq5mO57kWnjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A beginner's guide to music theory](https://youtu.be/n2z02J4fJwg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1372.jpg)"
      ],
      "metadata": {
        "id": "enDRiPrwWj_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The mathematical problem with music, and how to solve it](https://youtu.be/nK2jYk37Rlg)\n",
        "\n",
        "* Frequency (in Hertz) = no. of vibrations per second. Typically between 50 to a few thousand Hertz. Higher frequency = higher pitch (i.e. 200. vs 1000)\n",
        "\n",
        "* all musical tones are multiples of pure tones (harmonics) = behave to the mathematical sine function. For example tone f consists of frequencies of several integer multiples of a pure tone f, 2f, 3f.. (first harmonic, second harmonic, third harmonic..).\n",
        "\n",
        "* Melodies = sequences of tones. Strictly speaking these two melodies have not even one frequency in common, but they sound similar. But both melodies have the **same ratios between the frequencies of their tones.** Frequencies are different, but ratios are the same.\n",
        "\n",
        "* Changing from melody 1 to melody 2 is called **transposition from one key to another**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1373.jpg)\n",
        "\n",
        "* Another important concept is the **Musical intervall**, which is the frequency ratio mentioned above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1374.jpg)\n",
        "\n",
        "* One important interval is called the octave, and it corresponds to a frequency ratio of 2:1. The Octave is very pleasent to the ear. **Octave equivalence**: two tones that are an Octave apart sound highly similar: hence have same name and belong to the same pitch class.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1375.jpg)\n",
        "\n",
        "* Another important is called the **Fifth** with ration 3 over 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1376.jpg)\n"
      ],
      "metadata": {
        "id": "A5ewjWi3Xe1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGNbw_qosx7"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionentheorie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-vaCcWrF4t"
      },
      "source": [
        "Die [Funktionentheorie](https://de.wikipedia.org/wiki/Funktionentheorie) befasst sich mit der Theorie differenzierbarer komplexwertiger Funktionen mit komplexen Variablen. Da insbesondere die Funktionentheorie einer komplexen Variablen reichlich Gebrauch von Methoden aus der reellen Analysis macht, nennt man das Teilgebiet auch komplexe Analysis.\n",
        "\n",
        "* F√ºr holomorphe Funktionen gilt, dass Real- und Imagin√§rteil [harmonische Funktionen](https://de.wikipedia.org/wiki/Harmonische_Funktion) sind, also die [Laplace-Gleichung](https://de.wikipedia.org/wiki/Laplace-Gleichung) erf√ºllen. Dies verkn√ºpft die Funktionentheorie mit den partiellen Differentialgleichungen, beide Gebiete haben sich regelm√§√üig gegenseitig beeinflusst.\n",
        "\n",
        "* Das Wegintegral einer holomorphen Funktion ist vom Weg unabh√§ngig. Dies war historisch das erste Beispiel einer Homotopieinvarianz. Aus diesem Aspekt der Funktionentheorie entstanden viele Ideen der algebraischen Topologie, beginnend mit Bernhard Riemann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmbwvHZAec-"
      },
      "source": [
        "**Komplexe Funktion**\n",
        "\n",
        "* Eine [komplexe Funktion](https://de.wikipedia.org/wiki/Funktionentheorie#Komplexe_Funktionen) ordnet einer komplexen Zahl eine weitere [komplexe Zahl](https://de.m.wikipedia.org/wiki/Komplexe_Zahl) zu\n",
        "\n",
        "* Da jede komplexe Zahl durch zwei reelle Zahlen in\n",
        "der Form $x+$ iy geschrieben werden kann, l√§sst sich eine allgemeine Form einer komplexen Funktion darstellen durch:\n",
        "\n",
        "> $\n",
        "x+i y \\mapsto f(x+i y)=u(x, y)+i v(x, y)\n",
        "$\n",
        "\n",
        "* Dabei sind $u(x, y)$ und $v(x, y)$ reelle Funktionen, die von zwei reellen Variablen $x$ und $y$ abh√§ngen.\n",
        "\n",
        "* $u(x, y)$ hei√üt der Realteil und $v(x, y)$ der Imagin√§rteil der Funktion.\n",
        "\n",
        "* **Insofern ist eine komplexe Funktion nichts anderes als eine Abbildung von $\\mathbb{R}^{2}$ nach $\\mathbb{R}^{2}$ (also eine Abbildung, die zwei reellen Zahlen wieder zwei reelle Zahlen zuordnet).**\n",
        "\n",
        "* **Tats√§chlich k√∂nnte man die Funktionentheorie auch mit Methoden der reellen Analysis aufbauen.**\n",
        "\n",
        "* Der Unterschied zur reellen Analysis wird erst deutlicher, wenn man **komplex-differenzierbare Funktionen** betrachtet und dabei die <u>**multiplikative Struktur des K√∂rpers der komplexen Zahlen**</u> ins Spiel bringt, die dem Vektorraum $\\mathbb{R}^{2}$ fehlt.\n",
        "\n",
        "* Wie auch bei reellwertigen und reellen Funktionen ist die Verwendung des Begriffes einer komplexen Funktion in der Literatur aber nicht eindeutig. Teilweise wird er synonym mit einer komplexwertigen Funktion verwendet, teilweise wird er auch nur f√ºr komplexwertige Funktionen einer komplexen Variablen verwendet, also Funktionen\n",
        "\n",
        "> $\n",
        "f: D \\rightarrow \\mathbb{C}\n",
        "$\n",
        "\n",
        "bei denen $D \\subseteq \\mathbb{C}$ ist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDS2jqpHCZov"
      },
      "source": [
        "**Komplexwertige Funktion**\n",
        "\n",
        "* Eine [komplexwertige Funktion](https://de.wikipedia.org/wiki/Komplexwertige_Funktion) ist eine Funktion, deren Funktionswerte komplexe Zahlen sind\n",
        "\n",
        "* Eng damit verwandt ist der Begriff der **komplexen Funktion**, der in der Literatur aber nicht eindeutig verwendet wird\n",
        "\n",
        "* Komplexwertige Funktionen werden in der Analysis und in der Funktionentheorie untersucht und haben vielf√§ltige Anwendungen wie zum Beispiel in der Physik und der Elektrotechnik, wo sie beispielsweise zur Beschreibung von Schwingungen dienen.\n",
        "\n",
        "* Eine komplexwertige Funktion ist eine Funktion\n",
        "$f: D \\rightarrow \\mathbb{C}$ bei der die Zielmenge die Menge der komplexen Zahlen ist.\n",
        "\n",
        "* **An die Definitionsmenge $D$ sind keine Anforderungen gestellt**.\n",
        "\n",
        "* **Aufgrund der Einbettung der reellen Zahlen in die komplexen Zahlen lassen sich alle reellwertigen Funktionen auch als komplexwertige Funktionen auffassen.**\n",
        "\n",
        "* Beispiel: Die Funktion $f: \\mathbb{R} \\rightarrow \\mathbb{C}$ definiert durch $f(x)= \\mathrm{e}^{\\mathrm{i} x}=\\cos (x)+\\mathrm{i} \\sin (x)$ ist eine komplexwertige Funktion einer reellen Variable, und zwar die [Eulersche Formel](https://de.wikipedia.org/wiki/Eulersche_Formel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpqu4CjYvL7"
      },
      "source": [
        "**Holomorphe Funktion**\n",
        "\n",
        "* Holomorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* [Holomorphe Funktionen](https://de.wikipedia.org/wiki/Holomorphe_Funktion) sind **an jedem Punkt komplex differenzierbar**.\n",
        "\n",
        "* Eine Funktion $f\\colon U\\to {\\mathbb  {C}}$ mit einer offenen Menge $ U\\subseteq \\mathbb {C}$ hei√üt holomorph, falls sie in jedem Punkt von $U$ komplex differenzierbar ist.\n",
        "\n",
        "* Holomorphie eine sehr starke Eigenschaft ist, die im Reellen kein Pendant besitzen (z.B. ist jede holomorphe Funktion beliebig oft (stetig) differenzierbar und l√§sst sich lokal in jedem Punkt in eine Potenzreihe entwickeln.)\n",
        "\n",
        "Es sei $U \\subseteq \\mathbb{C}$ eine offene Teilmenge der komplexen Ebene und $z_{0} \\in U$ ein Punkt dieser Teilmenge. Eine Funktion $f: U \\rightarrow \\mathbb{C}$ hei√üt komplex differenzierbar im Punkt $z_{0}$, falls der Grenzwert\n",
        "\n",
        ">$\n",
        "\\lim _{h \\rightarrow 0} \\frac{f\\left(z_{0}+h\\right)-f\\left(z_{0}\\right)}{h}\n",
        "$\n",
        "\n",
        "existiert. Man bezeichnet ihn dann als $f^{\\prime}\\left(z_{0}\\right)$.\n",
        "\n",
        "Die Funktion $f$ hei√üt holomorph im Punkt $z_{0}$, falls eine Umgebung von $z_{0}$ existiert, in der $f$ komplex differenzierbar ist. Ist $f$ auf ganz $U$ holomorph, so nennt man $f$ holomorph.\n",
        "\n",
        "**Ist weiter $U=\\mathbb{C},$ so nennt man $f$ eine <u>ganze Funktion</u>**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeegSuMPXcu"
      },
      "source": [
        "**Biholomorphe Funktionen**\n",
        "\n",
        "* Eine Funktion, die holomorph, bijektiv und deren Umkehrfunktion holomorph ist, nennt man [biholomorph](https://de.wikipedia.org/wiki/Biholomorphe_Abbildung).\n",
        "\n",
        "* Im Fall einer komplexen Ver√§nderlichen ist das √§quivalent dazu, dass die Abbildung bijektiv und konform ist.\n",
        "* Aus Sicht der Kategorientheorie ist eine biholomorphe Abbildung ein Isomorphismus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StjTRqJOsq4"
      },
      "source": [
        "**Cauchy-Riemannsche Differentialgleichungen**\n",
        "\n",
        "* Siehe [Cauchy-Riemannsche Differentialgleichungen](https://de.wikipedia.org/wiki/Cauchy-Riemannsche_partielle_Differentialgleichungen) - Mit einer Einleitung [hier](https://de.wikipedia.org/wiki/Holomorphe_Funktion#Cauchy-Riemannsche_Differentialgleichungen).\n",
        "\n",
        "* Zerlegt man eine Funktion $f(x+i y)=u(x, y)+i v(x, y)$ in ihren Real-und Imagin√§rteil mit reellen Funktionen $u, v,$ so hat die totale Ableitung $L$ als Darstellungsmatrix die **Jacobi-Matrix**\n",
        "\n",
        ">$\n",
        "\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
        "\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Folglich ist die Funktion $f$ genau dann komplex differenzierbar, wenn sie reell differenzierbar ist und f√ºr $u, v$ die Cauchy-Riemannschen Differentialgleichungen\n",
        "\n",
        ">$\n",
        "\\begin{array}{l}\n",
        "\\frac{\\partial u}{\\partial x}=\\frac{\\partial v}{\\partial y} \\\\\n",
        "\\frac{\\partial u}{\\partial y}=-\\frac{\\partial v}{\\partial x}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "erf√ºllt sind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNafAzsW_vP"
      },
      "source": [
        "**Analytische Funktion (=holomorph)**\n",
        "\n",
        "* Als [analytische Funktion](https://de.wikipedia.org/wiki/Analytische_Funktion) bezeichnet man eine Funktion, die lokal durch eine **konvergente Potenzreihe** gegeben ist.\n",
        "\n",
        "* Aufgrund der Unterschiede zwischen reeller und komplexer Analysis spricht man zur Verdeutlichung oft auch explizit von reell-analytischen oder komplex-analytischen Funktionen.\n",
        "\n",
        "* **Im Komplexen sind die Eigenschaften analytisch und holomorph √§quivalent.**\n",
        "\n",
        "* **Ist eine Funktion in der gesamten komplexen Ebene definiert und analytisch, nennt man sie ganz.**\n",
        "\n",
        "\n",
        "Es sei $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{K}=\\mathbb{C} .$ Es sei $D \\subseteq \\mathbb{K}$ eine offene Teilmenge. Eine Funktion $f: D \\rightarrow \\mathbb{K}$ hei√üt analytisch im Punkt $x_{0} \\in D,$ wenn es eine **Potenzreihe**\n",
        "\n",
        ">$\n",
        "\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "gibt, die auf einer Umgebung von $x_{0}$ gegen $f(x)$ konvergiert. Ist $f$ in jedem Punkt von $D$ analytisch, so hei√üt $f$ analytisch.\n",
        "\n",
        "Viele g√§ngige Funktionen der reellen Analysis wie beispielsweise Polynome, Exponential- und Logarithmusfunktionen, trigonometrische Funktionen und rationale Ausdr√ºcke in diesen Funktionen sind analytisch.\n",
        "\n",
        "Unter einer [Potenzreihe](https://de.wikipedia.org/wiki/Potenzreihe) $P(x)$ versteht man in der Analysis eine unendliche Reihe der Form\n",
        "\n",
        ">$\n",
        "P(x)=\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "mit einer beliebigen Folge $\\left(a_{n}\\right)_{n \\in \\mathbb{N}_{0}}$ reeller oder komplexer Zahlen\n",
        "dem Entwicklungspunkt $x_{0}$ der Potenzreihe.\n",
        "\n",
        "Potenzreihen spielen eine wichtige Rolle in der Funktionentheorie und **erlauben oft eine sinnvolle Fortsetzung reeller Funktionen in die komplexe Zahlenebene**. Insbesondere stellt sich die Frage, f√ºr welche reellen oder komplexen Zahlen eine Potenzreihe konvergiert. Diese Frage f√ºhrt zum Begriff des [Konvergenzradius](https://de.wikipedia.org/wiki/Konvergenzradius).\n",
        "\n",
        "* Jede Polynomfunktion l√§sst sich als Potenzreihe auffassen, bei der fast alle Koeffizienten $a_{n}$ gleich 0 sind.\n",
        "\n",
        "* Wichtige andere Beispiele sind **Taylorreihe** und **Maclaurinsche Reihe**.\n",
        "\n",
        "* Funktionen, die sich durch eine Potenzreihe darstellen lassen, werden auch [analytische Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) genannt.\n",
        "\n",
        "**Beispielhaft die Potenzreihendarstellung einiger bekannter Funktionen**:\n",
        "\n",
        "* **Exponentialfunktion**: $e^{x}=\\exp (x)=\\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !}=\\frac{x^{0}}{0 !}+\\frac{x^{1}}{1 !}+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\cdots$ f√ºr alle\n",
        "$x \\in \\mathbb{R},$ d. h., der Konvergenzradius ist unendlich.\n",
        "\n",
        "* **Sinus**: $\\sin (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n+1}}{(2 n+1) !}=\\frac{x}{1 !}-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !} \\mp \\cdots$\n",
        "\n",
        "* **Kosinus**: $\\cos (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !}=\\frac{x^{0}}{0 !}-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !} \\mp \\cdots$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GiFSM9LMFFp"
      },
      "source": [
        "**Ganze Funktion**\n",
        "\n",
        "* In der Funktionentheorie ist eine [ganze Funktion](https://de.wikipedia.org/wiki/Ganze_Funktion) eine Funktion, **die in der gesamten komplexen Zahleneben**e $\\mathbb {C}$  holomorph (also analytisch) ist.\n",
        "\n",
        "* an entire function, also called an integral function, is a complex-valued function that is holomorphic at all finite points over the whole complex plane.\n",
        "\n",
        "* **Every entire function f(z) can be represented as a power series**\n",
        "\n",
        "* Typische Beispiele ganzer Funktionen sind Polynome oder die **Exponentialfunktion** sowie Summen, Produkte und Verkn√ºpfungen davon, etwa die **trigonometrischen Funktionen** und die **Hyperbelfunktionen**.\n",
        "\n",
        "* Jede ganze Funktion kann als eine √ºberall konvergierende Potenzreihe um ein beliebiges Zentrum dargestellt werden. Weder der Logarithmus noch die Wurzelfunktion sind ganz.\n",
        "\n",
        "* Eine ganze Funktion kann eine **isolierte Singularit√§t**, insbesondere sogar eine wesentliche Singularit√§t im komplexen Punkt im Unendlichen (und nur da) besitzen.\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* der Kehrwert der Gammafunktion $1 / \\Gamma(z)$\n",
        "\n",
        "* die Fehlerfunktion $\\operatorname{erf}(z)$\n",
        "\n",
        "* der Integralsinus $\\operatorname{Si}(z)$\n",
        "\n",
        "* die Airy-Funktionen $\\operatorname{Ai}(z)$ und $\\operatorname{Bi}(z)$\n",
        "\n",
        "* die Fresnelschen Integrale $S(z)$ und $C(z)$\n",
        "\n",
        "* die Riemannsche Xi-Funktion $\\xi(z)$\n",
        "\n",
        "* die Besselfunktionen erster Art $J_{n}(z)$ f√ºr ganzzahlige $n$\n",
        "\n",
        "* die Struve-Funktionen $H_{n}(z)$ f√ºr ganzzahlige $n>-2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Era7KDwoxIS"
      },
      "source": [
        "**Laurent-Reihe**\n",
        "\n",
        "**Exkurs**: Die Laurent-Reihe ist eine **unendliche Reihe √§hnlich einer Potenzreihe**, aber zus√§tzlich **mit negativen Exponenten**. Allgemein hat eine Laurent-Reihe in $x$ mit Entwicklungspunkt $c$ diese Gestalt (Dabei sind die $a_{n}$ und $c$ meist komplexe Zahlen):\n",
        "\n",
        ">$\n",
        "f(x)=\\sum_{n=-\\infty}^{\\infty} a_{n}(x-c)^{n}\n",
        "$\n",
        "\n",
        "Es sei $D$ eine nichtleere offene Teilmenge der Menge $\\mathbb{C}$ der komplexen Zahlen und $P_{f}$ eine weitere Teilmenge von $\\mathbb{C}$, die nur aus isolierten Punkten besteht. Eine Funktion $f$ hei√üt meromorph, wenn sie f√ºr Werte aus $D \\backslash P_{f}$ definiert und holomorph ist und f√ºr Werte aus $P_{f}$ Pole hat. $P_{f}$ wird als Polstellenmenge von $f$ bezeichnet.\n",
        "\n",
        "* Zerlegung einer komplex differenzierbaren Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4Q4vb9RlIR"
      },
      "source": [
        "**Meromorphe Funktion**\n",
        "\n",
        "* Meromorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* F√ºr viele Fragestellungen der Funktionentheorie ist der **Begriff der holomorphen Funktion zu speziell**.\n",
        "\n",
        "* Dies liegt daran, dass der Kehrwert $\\frac{1}{f}$ einer holomorphen Funktion $f$ an einer Nullstelle von $f$ eine Definitionsl√ºcke hat und somit $\\frac{1}{f}$ dort auch **nicht komplex differenzierbar ist**.\n",
        "\n",
        "* Man f√ºhrt daher den allgemeineren Begriff der [meromorphen Funktion](https://de.wikipedia.org/wiki/Meromorphe_Funktion) ein, die auch **isolierte Polstellen** besitzen kann.\n",
        "\n",
        "* Meromorphe Funktionen lassen sich lokal als [Laurentreihen](https://de.wikipedia.org/wiki/Laurent-Reihe) mit abbrechendem Hauptteil darstellen. Ist $U$ ein Gebiet von $\\mathbb{C},$ so bildet die Menge der auf $U$ meromorphen Funktionen einen K√∂rper.\n",
        "\n",
        "* Alle holomorphen Funktionen sind auch meromorph, da ihre Polstellenmenge leer ist.\n",
        "\n",
        "Zum Beispiel ist die Gamma-Funktion meromorph, weil sie holomorph ist auf $\\mathbb{C}$, abgesehen von abzahlbar vielen nicht-hebbaren Singularitaeten (hierbei in allen negativen ganzen Zahlen, da der Definitionsbereich einer Gammafunktion $\\mathbb{C}$  \\ -$\\mathbb{N}$ <sub>0</sub> ist).\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* [Gamma Funktion](https://en.wikipedia.org/wiki/Gamma_function)\n",
        "\n",
        "* [Elliptische Funktion](https://de.wikipedia.org/wiki/Elliptische_Funktion)\n",
        "\n",
        "*Der Absolutwert der Gammafunktion geht nach Unendlich an den Polstellen (links). Rechts hat sie keine Polstellen und steigt nur schnell an.*\n",
        "\n",
        "*The [gamma function](https://en.wikipedia.org/wiki/Gamma_function) is meromorphic in the whole complex plane.*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/3/33/Gamma_abs_3D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjZAY8WRubkK"
      },
      "source": [
        "**Isolierte Singularit√§t**\n",
        "\n",
        "* [Isolierte Singularit√§ten](https://de.wikipedia.org/wiki/Isolierte_Singularit√§t) sind besondere [**isolierte Punkte**](https://de.wikipedia.org/wiki/Isolierter_Punkt) in der Quellmenge einer holomorphen Funktion.\n",
        "\n",
        "* Man unterscheidet bei isolierten Singularit√§ten zwischen **hebbaren Singularit√§ten**, **Polstellen** und **wesentlichen Singularit√§ten**.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{C}$ eine offene Teilmenge, $z_{0} \\in \\Omega$. Ferner sei $f: \\Omega \\backslash\\left\\{z_{0}\\right\\} \\rightarrow \\mathbb{C}$ eine holomorphe komplexwertige Funktion. Dann hei√üt $z_{0}$ isolierte Singularit√§t von $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAgJBUpT0Oo"
      },
      "source": [
        "**Klasse 1: Hebbare Singularit√§t (Definitionsl√ºcke)**\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [hebbare Singularit√§t](https://de.wikipedia.org/wiki/Definitionsl√ºcke), wenn $f$ auf $\\Omega$ holomorph fortsetzbar ist.\n",
        "\n",
        "* Hat Grenzwert in Form von einer Zahl bzw. nach dem riemannschen Hebbarkeitssatz, wenn $f$ in einer Umgebung von $z_{0}$ beschr√§nkt ist.\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z)=c$\n",
        "\n",
        "> $f(z)$ beschrankt in $U\\left(z_{0}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAm2tWA_vART"
      },
      "source": [
        "**Klasse 2: Polstelle**\n",
        "\n",
        "* im Prinzip das gleiche wie bei hebbaren Singularitaten, aber mit dem Unterschied, dass die Funktion nah an der Singularitat unbeschraenkt ist = also ins unendliche geht $\\lim _{z \\rightarrow z_{0}}|f(z)|=\\infty$\n",
        "\n",
        "Alternative Definition:\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z) \\cdot\\left(z-z_{0}\\right)^{k}=c \\neq 0(k \\geqslant 1)$\n",
        "\n",
        "Man multipliziert an den unendlichen Funktionswert immer oefter eine fast Null in Form des Polynoms $\\left(z-z_{0}\\right)^{k}$, bis man eine endliche komplexe Zahl als Grenzwert hat, und nicht mehr unendlich (aber nicht Null). Die Anzahl der Polynome k multipliziert mit dem Funktionswert ist der Grad / Ordnung der Polstelle.\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [Polstelle](https://de.wikipedia.org/wiki/Polstelle) oder $P o l$, wenn\n",
        "\n",
        "  * $z_{0}$ **keine hebbare Singularit√§t ist** und\n",
        "  * es eine nat√ºrliche $\\operatorname{Zahl} k$ gibt, sodass $\\left(z-z_{0}\\right)^{k} \\cdot f(z)$ **eine hebbare Singularit√§t bei $z_{0}$ hat**.\n",
        "\n",
        "* Ist das $k$ minimal gew√§hlt, dann sagt man $f$ habe in $z_{0}$ einen Pol $k$ -ter Ordnung.\n",
        "\n",
        "* Man bezeichnet eine einpunktige Definitionsl√ºcke einer Funktion als Polstelle oder auch k√ºrzer als Pol, wenn die Funktionswerte in jeder Umgebung des Punktes (betragsm√§√üig) beliebig gro√ü werden.\n",
        "\n",
        "* Damit geh√∂ren die Polstellen zu den isolierten Singularit√§ten.\n",
        "\n",
        "* Das Besondere an Polstellen ist, dass sich die Punkte in einer Umgebung **nicht chaotisch verhalten, sondern in einem gewissen Sinne gleichm√§√üig gegen unendlich streben**. Deshalb k√∂nnen dort Grenzwertbetrachtungen durchgef√ºhrt werden.\n",
        "\n",
        "* Generell spricht man nur bei [glatten](https://de.wikipedia.org/wiki/Glatte_Funktion) (stetig & differenzierbar) oder [analytischen Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) von Polen.\n",
        "\n",
        "*Fur reelle Funktionen: f(x)=1/x hat einen Pol erster Ordnung an der Stelle x=0**\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/9/90/GraphKehrwertfunktion.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNpaYyHVD4o"
      },
      "source": [
        "**Klasse 3: Wesentliche Singularit√§t**\n",
        "\n",
        "* nicht hebbar und keine Polstelle, dann hei√üt $z_{0}$ eine wesentliche Singularit√§t von $f$\n",
        "\n",
        "* zum Beispiel fur $f(z)=e^{-\\frac{1}{z}}, z_{0}=0$, eine Funktion die von links ins unendliche strebt, und von rechts nach Null\n",
        "  * keine hebbare Singularitaet, weil zwei verschiedene Grenzwerte existieren und eine davon nicht endlich ist.\n",
        "  * keine Polstelle, weil ein Grenzwert gleich Null ist (muss aber unbeschraenkt sein bei Polstellen)\n",
        "\n",
        "\n",
        "*Plot der Funktion $\\exp(1/z)$. Sie hat im Nullpunkt eine wesentliche Singularit√§t (Bildmitte). Der Farbton entspricht dem komplexen Argument des Funktionswertes, w√§hrend die Helligkeit seinen Betrag darstellt. Hier sieht man, dass sich die wesentliche Singularit√§t unterschiedlich verh√§lt, je nachdem, wie man sich ihr n√§hert (im Gegensatz dazu w√§re ein Pol gleichm√§√üig wei√ü).*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/0/0b/Essential_singularity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSHsrLNpJDv"
      },
      "source": [
        "**Cauchysche Integralformel**\n",
        "\n",
        "* Die [cauchysche Integralformel](https://de.wikipedia.org/wiki/Cauchysche_Integralformel) ist eine der fundamentalen Aussagen der Funktionentheorie, eines Teilgebietes der Mathematik.\n",
        "\n",
        "* Sie besagt in ihrer schw√§chsten Form, dass die Werte einer holomorphen Funktion $f$ im Inneren einer Kreisscheibe bereits durch ihre Werte auf dem Rand dieser Kreisscheibe bestimmt sind.\n",
        "\n",
        "* Eine starke Verallgemeinerung davon ist der Residuensatz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVLX7lrIqBtX"
      },
      "source": [
        "**Residuensatz**\n",
        "\n",
        "* Der [Residuensatz](https://de.wikipedia.org/wiki/Residuensatz) ist ein wichtiger Satz der Funktionentheorie. Er stellt eine **Verallgemeinerung des cauchyschen Integralsatzes und der cauchyschen Integralformel** dar. Seine Bedeutung liegt nicht nur in den weitreichenden Folgen innerhalb der Funktionentheorie, sondern auch in der Berechnung von Integralen √ºber reelle Funktionen.\n",
        "\n",
        "* Er besagt, dass das Kurvenintegral l√§ngs einer geschlossenen Kurve √ºber eine bis auf isolierte Singularit√§ten holomorphe Funktion lediglich vom Residuum in den Singularit√§ten im Innern der Kurve und der Umlaufzahl der Kurve um diese Singularit√§ten abh√§ngt. Anstelle eines Kurvenintegrals muss man also nur Residuen und Umlaufzahlen berechnen, was in vielen F√§llen einfacher ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivq0YtFjjOPz"
      },
      "source": [
        "**Cauchyscher Integralsatz**\n",
        "\n",
        "* Der [cauchysche Integralsatz](https://de.wikipedia.org/wiki/Cauchyscher_Integralsatz)  ist einer der wichtigsten S√§tze der Funktionentheorie.\n",
        "\n",
        "* Er handelt von Kurvenintegralen f√ºr holomorphe (auf einer offenen Menge komplex-differenzierbare) Funktionen.\n",
        "\n",
        "* Im Kern besagt er, dass zwei dieselben Punkte verbindende Wege das gleiche Wegintegral besitzen, falls die Funktion √ºberall zwischen den zwei Wegen holomorph ist.\n",
        "\n",
        "* Der Satz gewinnt seine Bedeutung unter anderem daraus, dass man ihn zum Beweis der cauchyschen Integralformel und des Residuensatzes benutzt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHwO7VW3o75X"
      },
      "source": [
        "##### <font color=\"blue\">*Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JijVDdYpeTw"
      },
      "source": [
        "###### *Chaos Theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJSt90Zne8M"
      },
      "source": [
        "**Chaos Theory**\n",
        "\n",
        "* Many phenomena in nature can be described by dynamical systems; [chaos theory](https://en.m.wikipedia.org/wiki/Chaos_theory) makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n",
        "\n",
        "* [Complexity theory](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory. Chaos theory is a method of qualitative and quantitative analysis to investigate the behavior of [dynamical systems](https://en.m.wikipedia.org/wiki/Dynamical_system) that cannot be explained and predicted by single data relationships, but must be explained and predicted by whole, continuous data relationships.\n",
        "\n",
        "* Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random.\n",
        "\n",
        "* The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things:\n",
        "\n",
        "  * how much uncertainty can be tolerated in the forecast (uncertainty in a forecast increases exponentially with elapsed time)\n",
        "  * how accurately its current state can be measured,\n",
        "  * and a time scale depending on the dynamics of the system, called the [Lyapunov time](https://en.m.wikipedia.org/wiki/Lyapunov_time) (chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years)\n",
        "\n",
        "* In practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Lyapunov exponent*"
      ],
      "metadata": {
        "id": "RTdYXBryoZqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lyapunov exponent**\n",
        "\n",
        "* the Lyapunov exponent or Lyapunov characteristic exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories.\n",
        "\n",
        "* Quantitatively, two trajectories in phase space with initial separation vector $\\delta \\mathbf {Z} _{0}$ diverge (provided that the divergence can be treated within the linearized approximation) at a rate given by\n",
        "\n",
        "> ${\\displaystyle |\\delta \\mathbf {Z} (t)|\\approx e^{\\lambda t}|\\delta \\mathbf {Z} _{0}|}$\n",
        "\n",
        "where $\\lambda$ is the Lyapunov exponent.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1197.png)"
      ],
      "metadata": {
        "id": "uoo8xNAEmiam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complex Systems*"
      ],
      "metadata": {
        "id": "fhdD4JnqZHKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on4GTn_0efWt"
      },
      "source": [
        "**Complex Systems**\n",
        "\n",
        "* [Complex Systems](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* [Chaos](https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory) is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "*Properties:*\n",
        "\n",
        "1. **Agentenbasiert**: Komplexe Systeme bestehen aus einzelnen Teilen, die miteinander in Wechselwirkung stehen (Molek√ºle, Individuen, Software-Agenten etc.).\n",
        "2. **Nichtlinearit√§t**: Kleine St√∂rungen des Systems oder minimale Unterschiede in den Anfangsbedingungen f√ºhren oft zu sehr unterschiedlichen Ergebnissen (Schmetterlingseffekt, Phasen√ºberg√§nge). Die Wirkzusammenh√§nge der Systemkomponenten sind im Allgemeinen nichtlinear.\n",
        "3. [**Emergenz**](https://de.wikipedia.org/wiki/Emergenz): Im Gegensatz zu lediglich komplizierten Systemen zeigen komplexe Systeme Emergenz. Entgegen einer verbreiteten Vereinfachung bedeutet Emergenz nicht, dass die Eigenschaften der emergierenden Systemebenen von den darunter liegenden Ebenen vollst√§ndig unabh√§ngig sind. Emergente Eigenschaften lassen sich jedoch auch nicht aus der isolierten Analyse des Verhaltens einzelner Systemkomponenten erkl√§ren und nur sehr begrenzt ableiten.\n",
        "4. **Wechselwirkung** (Interaktion): Die Wechselwirkungen zwischen den Teilen des Systems (Systemkomponenten) sind lokal, ihre Auswirkungen in der Regel global.\n",
        "5. **Offenes System**: Komplexe Systeme sind √ºblicherweise offene Systeme. Sie stehen also im Kontakt mit ihrer Umgebung und befinden sich fern vom thermodynamischen Gleichgewicht. Das bedeutet, dass sie von einem permanenten Durchfluss von Energie bzw. Materie abh√§ngen.\n",
        "6. **Selbstorganisation**: Dies erm√∂glicht die Bildung insgesamt stabiler Strukturen (Selbststabilisierung oder Hom√∂ostase), die ihrerseits das thermodynamische Ungleichgewicht aufrechterhalten. Sie sind dabei in der Lage, Informationen zu verarbeiten bzw. zu lernen.\n",
        "7. **Selbstregulation**: Dadurch k√∂nnen sie die F√§higkeit zur inneren Harmonisierung entwickeln. Sie sind also in der Lage, aufgrund der Informationen und derer Verarbeitung das innere Gleichgewicht und Balance zu verst√§rken.\n",
        "8. **Pfade**: Komplexe Systeme zeigen Pfadabh√§ngigkeit: Ihr zeitliches Verhalten ist nicht nur vom aktuellen Zustand, sondern auch von der Vorgeschichte des Systems abh√§ngig.\n",
        "9. **Attraktoren**: Die meisten komplexen Systeme weisen so genannte Attraktoren auf, d. h., dass das System unabh√§ngig von seinen Anfangsbedingungen bestimmte Zust√§nde oder Zustandsabfolgen anstrebt, wobei diese Zustandsabfolgen auch chaotisch sein k√∂nnen; dies sind die ‚Äûseltsamen Attraktoren‚Äú der Chaosforschung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YzAyotrq51A"
      },
      "source": [
        "* Complexity theory is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIwXOz85zxW_"
      },
      "source": [
        "###### *Fractal Dimensions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzaMSBJmGj_F"
      },
      "source": [
        "**Fractal Geometry**\n",
        "\n",
        "* a [fractal](https://en.m.wikipedia.org/wiki/Fractal) is a subset of Euclidean space with a fractal dimension that strictly exceeds its topological dimension. Fractals appear the same at different scales, as illustrated in successive magnifications of the Mandelbrot set.\n",
        "\n",
        "* Fractals exhibit similar patterns at increasingly smaller scales, a property called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge,it is called affine self-similar.\n",
        "\n",
        "* idealization is that everything is smooth (rebellion against calculus, differentiable, where assumption is things look smooth if you zoom in enough). Mandelbrot: nature is fractal (capture roughness)\n",
        "\n",
        "* self-similar shapes give a basis for modeling the regularity in some forms of roughness. but that doesn't mean all is only perfectly self-similar either!! (Perfect self similar are: Von Koch snowflake, Sierpensky triangle)\n",
        "\n",
        "> <font color=\"blue\">Fractal dimension: Sierpensky triangle is 1,585 dimensional, Von Koch snowflake is 1,262 dimensional, Britain coast line 1,21 dimensional, Norway: 1,52 dimensional, calm sea 2,05 dimensional, waves 2,3 dimensional</font>\n",
        "\n",
        "> **Fractals are shapes with a non-integer dimension, captures idea of roughness, but dimension can vary depending on how much you zoom in**. But a shape is considered a fractal only when the measures dimension stays approximately constant across multiple different scales.\n",
        "\n",
        "> Useful in security: **Is something fractal? Yes - then probably from nature. No - then probably man-made**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deterministic fractals**:\n",
        "  * [Julia set](https://de.m.wikipedia.org/wiki/Julia-Menge) - siehe auch [Newtonfraktal](https://de.m.wikipedia.org/wiki/Newtonfraktal). The Julia set and the Fatou set are two complementary sets (Julia \"laces\" and Fatou \"dusts\").\n",
        "  * [Mandelbrot set](https://en.m.wikipedia.org/wiki/Mandelbrot_set) (is kind of like a map of Julia sets)\n",
        "  * [Logistic map](https://de.m.wikipedia.org/wiki/Logistische_Gleichung) (Feigenbaum Attractor) with [Feigenbaum-Konstante](https://de.m.wikipedia.org/wiki/Feigenbaum-Konstante),\n",
        "    * [Vergesst den Pi-Tag, feiert lieber den Feigenbaum-Tag](https://www.spektrum.de/kolumne/die-feigenbaum-konstante-ist-die-wichtigste-groesse-der-dynamik/2120670)\n",
        "    * [Bifurcation Diagram](https://en.m.wikipedia.org/wiki/Bifurcation_diagram)\n",
        "  * Peano curve, Pentaflake, 3D Hilbert curve etc.\n",
        "  * A [fractal curve](https://en.m.wikipedia.org/wiki/Fractal_curve) is, loosely, a mathematical curve whose shape retains the same general pattern of irregularity, regardless of how high it is magnified, that is, its graph takes the form of a fractal. They do NOT have finite length. A famous example is the boundary of the Mandelbrot set. [Space-filling curves](https://en.m.wikipedia.org/wiki/Space-filling_curve) are special cases of fractal curves. Its range contains the entire 2-dimensional unit square (or more generally an n-dimensional unit hypercube). Space-filling curves in the 2-dimensional plane are sometimes called [Peano curve](https://en.m.wikipedia.org/wiki/Peano_curve). Peano curves are constructed by [Hilbert curves](https://en.m.wikipedia.org/wiki/Hilbert_curve) to form a single continuous loop over the entire sphere.\n",
        "\n",
        "\n",
        "**Random and natural fractals**:\n",
        "  * Zeros of a Wiener process,\n",
        "  * Brownian motion,\n",
        "  * [Coastline of Ireland, Great Britain or Norway](https://en.m.wikipedia.org/wiki/Coastline_paradox)  (Coastline paradox)\n",
        "  * von [Koch curve](https://de.m.wikipedia.org/wiki/Koch-Kurve) with random orientation\n",
        "  * The surface of Broccoli or human brain,\n",
        "  * Distribution of [galaxy clusters](https://en.m.wikipedia.org/wiki/Galaxy_cluster)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1196.png)\n",
        "\n",
        "*Zeros of a Wiener process:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/8/83/Wiener_process_set_of_zeros.gif)"
      ],
      "metadata": {
        "id": "PeB3l9rViknz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fractal Dimensions**: die [fraktale Dimension](https://de.m.wikipedia.org/wiki/Fraktale_Dimension) einer Menge ist eine Verallgemeinerung des Dimensionsbegriffs von geometrischen Objekten wie Kurven (eindimensional) und Fl√§chen (zweidimensional).\n",
        "\n",
        "  * [Hausdorff dimension](https://de.m.wikipedia.org/wiki/Hausdorff-Dimension):  Hausdorff dimension of a single point is zero, of a line segment is 1, of a square is 2, and of a cube is 3. That is, for sets of points that define a smooth shape or a shape that has a small number of corners‚Äîthe shapes of traditional geometry and science‚Äîthe Hausdorff dimension is an integer agreeing with the usual sense of dimension, also known as the [topological dimension (inductive dimension)](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "  * [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "  * [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension)\n",
        "  * [Box-counting dimension](https://en.m.wikipedia.org/wiki/Minkowski‚ÄìBouligand_dimension) (Minkowski‚ÄìBouligand dimension)\n",
        "  * [List of fractals by Hausdorff dimension](https://en.m.wikipedia.org/wiki/List_of_fractals_by_Hausdorff_dimension)\n",
        "\n",
        "*Example of Box Counting dimension:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Great_Britain_Box.svg/640px-Great_Britain_Box.svg.png)\n"
      ],
      "metadata": {
        "id": "b-iO6R8pfzwE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcbvY2o-JyH"
      },
      "source": [
        "###### *Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-OG2Rw8EzzH"
      },
      "source": [
        "**Dynamisches System**\n",
        "\n",
        "> **A dynamical system involves one or more variables that change over time according to autonomous differential equations.**\n",
        "\n",
        "> $\\dot{x}=$ rate of change of $x$ as time changes\n",
        "\n",
        "> $\\dot{y}=$ rate of change of $y$ as time changes\n",
        "\n",
        "They depend on t:\n",
        "\n",
        "$\\frac{d x}{d t}=$ rate of change of $x$ as time changes $\\frac{d y}{d t}=$ rate of change of $y$ as time changes\n",
        "\n",
        "but the differential equation that describe x dot and y dot don‚Äôt actually involve time t:\n",
        "\n",
        "> $\\dot{x}=-y-0.1 x$\n",
        "\n",
        "> $\\dot{y}=x-0.4 y$\n",
        "\n",
        "This makes them autonomous. Each combination of x and y to only corresponds to one combination of x dot and y dot. You can represent it in a dynamical system called the phase space. Each point in space is a unique state of the system. And has its own rate of change shown as a vector.\n",
        "\n",
        "> <font color=\"red\">Siehe auch: [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. STS is interesting because it bridges the two major parts of mathematical physics ‚Äì the [dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory) and [topological (quantum) field theories](https://en.m.wikipedia.org/wiki/Topological_quantum_field_theory). Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.</font>\n",
        "\n",
        "* [Dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory). Ein (deterministisches) [dynamisches System](https://de.m.wikipedia.org/wiki/Dynamisches_System) ist ein Modell eines zeitabh√§ngigen Prozesses, der homogen bez√ºglich der Zeit ist (Verlauf h√§ngt nur vom Anfangszustand, aber nicht von der Wahl des Anfangszeitpunkts)\n",
        "\n",
        "* Wichtige Fragestellungen: Langzeitverhalten (zum Beispiel [Stabilit√§t](https://de.wikipedia.org/wiki/Stabilit√§tstheorie), [Periodizit√§t](https://de.wikipedia.org/wiki/Periodische_Funktion), [Chaos](https://de.wikipedia.org/wiki/Chaosforschung) und [Ergodizit√§t](https://de.wikipedia.org/wiki/Ergodizit√§t)), die [Systemidentifikation](https://de.wikipedia.org/wiki/Systemidentifikation) und ihre [Regelung](https://de.wikipedia.org/wiki/Regelung_(Natur_und_Technik))\n",
        "\n",
        "* Formal betrachte man ein **dynamisches System** bestehend aus einem topologischen Raum $X$ und einer Transformation $f: \\mathcal{T} \\times X \\longrightarrow X$, wobei $\\mathcal{T}$ ein linear geordnetes Monoid ist wie $\\mathcal{T}=\\mathbb{N}, \\mathbb{Z},[0, \\infty[$ oder $\\mathbb{R}$ und $f$ normalerweise stetig oder mindestens messbar ist (oder mindestens wird verlangt, dass $f(t, \\cdot): X \\longrightarrow X$ stetig/messbar ist f√ºr jedes $t \\in \\mathcal{T}$ ) und erf√ºllt $f(t+s, x)=f(t, f(s, x))$ f√ºr alle ¬ªZeiten ¬´ $t, s \\in \\mathcal{T}$ und Punkte $x \\in X$.\n",
        "\n",
        "\n",
        "Ein dynamisches System ist ein Tripel $(T, X, \\Phi)$, bestehend aus\n",
        "\n",
        "* **Zeitraum**: einer Menge $T=\\mathbb{N}_{0}, \\mathbb{Z}, \\mathbb{R}_{0}^{+}$ oder $\\mathbb{R}$,\n",
        "* **Zustandsraum** (dem Phasenraum): einer nichtleeren Menge $X$,\n",
        "* **Operation** $\\Phi: T \\times X \\rightarrow X$ von $T$ auf $X,$\n",
        "\n",
        "so dass f√ºr alle Zust√§nde $x \\in X$ und alle Zeitpunkte $t, s \\in T$ gilt:\n",
        "\n",
        "1. **Identit√§tseigenschaft**: $\\Phi(0, x)=x$\n",
        "\n",
        "2. **Halbgruppeneigenschaft**: $\\Phi(s, \\Phi(t, x))=\\Phi(s+t, x)$\n",
        "\n",
        "* Wenn $T=\\mathbb{N}_{0}$ oder $T=\\mathbb{Z}$ ist, dann hei√üt $(T, X, \\Phi)$ **zeitdiskret** oder kurz diskret, und mit $T=\\mathbb{R}_{0}^{+}$ oder $T=\\mathbb{R}$ nennt man $(T, X, \\Phi)$ **zeitkontinuierlich** oder kontinuierlich.\n",
        "\n",
        "* Beispiele: [Exponentielles Wachstum und Federpendel](https://de.m.wikipedia.org/wiki/Dynamisches_System#Einf√ºhrende_Beispiele)\n",
        "\n",
        "  * das Str√∂mungsverhalten von Fl√ºssigkeiten und Gasen\n",
        "  * Bewegungen von Himmelsk√∂rpern unter gegenseitiger Beeinflussung durch die Gravitation\n",
        "  * Populationsgr√∂√üen von Lebewesen unter Ber√ºcksichtigung der R√§uber-Beute-Beziehung\n",
        "  * die Entwicklung wirtschaftlicher Kenngr√∂√üen unter Einfluss der Marktgesetze.\n",
        "\n",
        "* Das Langzeitverhalten eines dynamischen Systems l√§sst sich durch den globalen Attraktor beschreiben, da bei physikalischen oder technischen Systemen oft **Dissipation vorliegt, insbesondere Reibung**.\n",
        "\n",
        "* Die [Determiniertheit (als Systemeigenschaft)](https://de.wikipedia.org/wiki/Systemeigenschaften#Determiniertheit) ist der Grad der ‚ÄûVorbestimmtheit‚Äú des Systems: Ein System geht von einem Zustand Z1 in den Zustand Z2 √ºber: Z1 ‚Üí Z2. Bei deterministischen Systemen ist dieser √úbergang bestimmt (zwingend), bei stochastischen wahrscheinlich. Deterministische Systeme erlauben prinzipiell die Ableitung ihres Verhaltens aus einem vorherigen Zustand, stochastische Systeme nicht. **Aus der Komplexit√§t eines Systems l√§sst sich keine Aussage √ºber die Vorhersagbarkeit treffen**: Es gibt einfache deterministische Systeme, die chaotisch sind (z. B. Doppelpendel) und komplexe deterministische Systeme (Chloroplasten bei der Photosynthese).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYGXfTQpV2tS"
      },
      "source": [
        "**Phasenraum und Trajektorie**\n",
        "\n",
        "* Der [Phasenraum](https://de.wikipedia.org/wiki/Phasenraum) beschreibt die Menge aller m√∂glichen Zust√§nde eines dynamischen Systems. Ein Zustand wird durch einen Punkt im Phasenraum eindeutig abgebildet.\n",
        "\n",
        "* **Jeder Zustand ist ein Punkt im Phasenraum und wird durch beliebig viele [Zustandsgr√∂√üen](https://de.m.wikipedia.org/wiki/Zustandsgr√∂√üe) dargestellt, welche die Dimensionen des Phasenraums bilden.**\n",
        "\n",
        "* kontinuierliche Systeme werden durch Linien (Trajektorien) repr√§sentiert\n",
        "* diskrete Systeme werden durch Mengen isolierter Punkte repr√§sentiert.\n",
        "\n",
        "* In der Mechanik besteht er aus verallgemeinerten Koordinaten (Konfigurationsraum) und zugeh√∂rigen verallgemeinerten Geschwindigkeiten, siehe [Prinzip der virtuellen Leistung](https://de.m.wikipedia.org/wiki/Prinzip_der_virtuellen_Leistung)\n",
        "\n",
        "* Die zeitliche Entwicklung eines Punktes im Phasenraum wird durch **Differentialgleichungen** beschrieben und durch **Trajektorien** (Bahnkurven, Orbit) im Phasenraum dargestellt. Dies sind **Differentialgleichungen erster Ordnung in der Zeit** und durch einen Anfangspunkt eindeutig festgelegt (ist die Differentialgleichung zeitunabh√§ngig, sind dies **autonome Differentialgleichungen**). Dementsprechend kreuzen sich zwei Trajektorien im Phasenraum auch nicht, da an einem Kreuzungspunkt der weitere Verlauf nicht eindeutig ist. Geschlossene Kurven beschreiben oszillierende (periodische) Systeme.\n",
        "\n",
        "* *Konstruktion eines Phasen(raum)portr√§ts f√ºr ein [mathematisches Pendel](https://de.wikipedia.org/wiki/Mathematisches_Pendel)*\n",
        "\n",
        "![hh](https://upload.wikimedia.org/wikipedia/commons/c/cd/Pendulum_phase_portrait_illustration.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m066sar7fCUi"
      },
      "source": [
        "###### *Attractor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxw1WlN9hhlY"
      },
      "source": [
        "**Attraktor**\n",
        "\n",
        "* [Attraktor](https://de.wikipedia.org/wiki/Attraktor) ist ein Begriff aus der Theorie dynamischer Systeme und beschreibt **eine Untermenge eines Phasenraums** (d. h. eine gewisse Anzahl von Zust√§nden), auf die sich ein dynamisches System im Laufe der Zeit zubewegt und die unter der Dynamik dieses Systems nicht mehr verlassen wird.\n",
        "\n",
        "* Das hei√üt, eine Menge von Variablen n√§hert sich im Laufe der Zeit (asymptotisch) einem bestimmten Wert, einer Kurve oder etwas Komplexerem (also einer Region im n-dimensionalen Raum) und bleibt dann im weiteren Zeitverlauf in der N√§he dieses Attraktors.\n",
        "\n",
        "* Bekannte Beispiele sind der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor), der [R√∂ssler-Attraktor](https://de.wikipedia.org/wiki/R%C3%B6ssler-Attraktor) und die [Nullstellen](https://de.wikipedia.org/wiki/Nullstelle) einer differenzierbaren Funktion, welche Attraktoren des zugeh√∂rigen Newton-Verfahrens sind.\n",
        "\n",
        "> **Die Menge aller Punkte des Phasenraums, die unter der Dynamik demselben Attraktor zustreben, hei√üt Attraktions- oder Einzugsgebiet dieses Attraktors**.\n",
        "\n",
        "\n",
        "Unter einem Attraktor versteht man eine Teilmenge $A \\subseteq X$,\n",
        "die den folgenden Bedingungen gen√ºgt\n",
        "1. $A$ ist vorw√§rts invariant;\n",
        "2. Das Sammelbecken $B(A)$ ist eine Umgebung von $A$;\n",
        "3. $A$ ist eine minimale nicht leere Teilmenge von $X$ mit Bedingungen 1\n",
        "und 2 .\n",
        "\n",
        "Bedingung 1 erfordert eine gewisse Stabilit√§t des Attraktors. Daraus folgt offensichtlich, dass $A \\subseteq B(A)$. Anhand Bedingung 2 wird weiterhin verlangt, dass $A \\subseteq B(A)^{\\circ}$ und bedeutet u. a., jeder Punkt in einer gewissen N√§he von $A$ n√§here sich dem Attraktor beliebig. Manche Autoren lassen Bedingung 2 weg. Bedingung 3 erfordert, dass der Attraktor nicht in weitere Komponenten zerlegt werden kann (ansonsten\n",
        "w√§re bspw. der ganze Raum trivialerweise ein Attraktor).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cai3DPiwy4"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine ganzzahlige Dimension besitzen**\n",
        "\n",
        "*Bei der Untersuchung dynamischer Systeme interessiert man sich ‚Äì ausgehend von einem bestimmten [Anfangszustand (=Anfangsbedingung)](https://de.wikipedia.org/wiki/Anfangsbedingung) ‚Äì vor allem f√ºr das Verhalten f√ºr $t\\to \\infty$ . Der Grenzwert in diesem Fall wird als Attraktor bezeichnet. Typische und h√§ufige Beispiele von Attraktoren sind:*\n",
        "\n",
        "* **asymptotisch stabile Fixpunkte**: Das System n√§hert sich immer st√§rker einem bestimmten Endzustand an, in dem die Dynamik erliegt; ein statisches System entsteht. Typisches Beispiel ist ein ged√§mpftes Pendel, das sich dem Ruhezustand im tiefsten Punkt ann√§hert.\n",
        "\n",
        "* **(asymptotisch) stabile Grenzzyklen**: Der Endzustand ist die Abfolge gleicher Zust√§nde, die periodisch durchlaufen werden (periodische Orbits). Ein Beispiel daf√ºr ist die Simulation der R√§uber-Beute-Beziehung, die f√ºr bestimmte Parameter der R√ºckkoppelung auf ein periodisches Ansteigen und Sinken der Populationsgr√∂√üen hinausl√§uft.\n",
        "\n",
        "* F√ºr ein hybrides dynamisches System mit chaotischer Dynamik konnte im $\\mathbb {R} ^{n}$ die Oberfl√§che eines [n-Simplex](https://de.wikipedia.org/wiki/Simplex_(Mathematik)) als Attraktor identifiziert werden: (asymptotisch stabile) Grenztori: Treten mehrere miteinander inkommensurable Frequenzen auf, so ist die Trajektorie nicht geschlossen, und der Attraktor ist ein Grenztorus, der von der Trajektorie asymptotisch vollst√§ndig ausgef√ºllt wird. Die zu diesem Attraktor korrespondierende Zeitreihe ist quasiperiodisch, d. h., es gibt keine echte Periode, aber das Frequenzspektrum besteht aus scharfen Linien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMS8uZlNlFTx"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine fraktale Dimension besitzen**\n",
        "\n",
        "*Die Existenz von Attraktoren mit komplizierterer Struktur war zwar schon l√§nger bekannt, man betrachtete sie aber zun√§chst als instabile Sonderf√§lle, deren Auftreten nur bei bestimmter Wahl des Ausgangszustands und der Systemparameter beobachtet wird. Dies √§nderte sich mit der Definition eines neuen, speziellen Typs von Attraktor:*\n",
        "\n",
        "* [Seltsamer Attraktor](https://de.wikipedia.org/wiki/Seltsamer_Attraktor): In seinem Endzustand zeigt das System h√§ufig ein chaotisches Verhalten (es gibt jedoch auch Ausnahmen, z. B. quasiperiodisch angetriebene nichtlineare Systeme).  Ein seltsamer Attraktor ist ein Attraktor, also ein Ort im Phasenraum, der den Endzustand eines dynamischen Prozesses darstellt, **dessen fraktale Dimension nicht ganzzahlig und dessen Kolmogorov-Entropie echt positiv ist**. Es handelt sich damit um ein Fraktal, das nicht in geschlossener Form geometrisch beschrieben werden kann. Gelegentlich wird auch der Begriff chaotischer Attraktor bevorzugt, da die ‚ÄûSeltsamkeit‚Äú dieses Objekts sich mit den Mitteln der Chaostheorie erkl√§ren l√§sst. Der dynamische Prozess zeigt ein aperiodisches Verhalten.\n",
        "\n",
        "    * [Lorenz Attraktor](https://de.m.wikipedia.org/wiki/Lorenz-Attraktor)\n",
        "\n",
        "    * [R√∂ssler attractor](https://en.m.wikipedia.org/wiki/R√∂ssler_attractor)\n",
        "\n",
        "  * [Multiscroll attractor](https://en.m.wikipedia.org/wiki/Multiscroll_attractor)\n",
        "\n",
        "  * [H√©non map](https://en.m.wikipedia.org/wiki/H√©non_map)\n",
        "\n",
        "* Der seltsame Attraktor l√§sst sich **nicht in einer geschlossenen geometrischen Form** beschreiben und **besitzt keine ganzzahlige Dimension**. Attraktoren nichtlinearer dynamischer Systeme weisen dann eine **fraktale Struktur** auf.\n",
        "\n",
        "* Wichtiges Merkmal ist das chaotische Verhalten, d. h., jede noch so geringe √Ñnderung des Anfangszustands f√ºhrt im weiteren Verlauf zu signifikanten Zustands√§nderungen. Prominentestes Beispiel ist der **Lorenz-Attraktor, der bei der Modellierung von Luftstr√∂mungen in der Atmosph√§re entdeckt wurde**.\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Attractor#Fixed_point\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Limit_cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAvwOlbmQn8"
      },
      "source": [
        "**Special: Lorenz-Attraktor**\n",
        "\n",
        "Der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor) oder englisch: [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) ist der seltsame Attraktor eines Systems von drei gekoppelten, nichtlinearen **gew√∂hnlichen Differentialgleichungen**:\n",
        "\n",
        "$\\dot{X}=a(Y-X)$\n",
        "\n",
        "$\\dot{Y}=X(b-Z)-Y$\n",
        "\n",
        "$\\dot{Z}=X Y-c Z$\n",
        "\n",
        "* Formuliert wurde das System um 1963 von dem Meteorologen Edward N. Lorenz, der es als Idealisierung eines [hydrodynamischen Systems (Fluiddynamik)](https://de.wikipedia.org/wiki/Fluiddynamik) entwickelte. Basierend auf einer Arbeit von Barry Saltzman (1931‚Äì2001) ging es Lorenz dabei um eine Modellierung der Zust√§nde in der Erdatmosph√§re zum Zweck einer Langzeitvorhersage.\n",
        "\n",
        "* Allerdings betonte Lorenz, dass das von ihm entwickelte System allenfalls f√ºr sehr begrenzte Parameterbereiche von $a,b,c$ realistische Resultate liefert.\n",
        "\n",
        "* Die mathematische Beschreibung des Modells durch die Navier-Stokes-Gleichungen f√ºhrt √ºber verschiedene Vereinfachungen, beispielsweise endlich abgebrochene Reihendarstellungen, zu dem oben angegebenen Gleichungssystem.\n",
        "\n",
        "* Die numerische L√∂sung des Systems zeigt bei bestimmten Parameterwerten deterministisch chaotisches Verhalten, die Trajektorien folgen einem seltsamen Attraktor. Damit spielt der Lorenzattraktor f√ºr die mathematische Chaostheorie eine Rolle, denn die Gleichungen stellen wohl eines der einfachsten Systeme mit chaotischem Verhalten dar.\n",
        "\n",
        "* Die typische Parametereinstellung mit chaotischer L√∂sung lautet: $a=10,b=28$ und $c=8/3$, wobei\n",
        "  * $a$ mit der [Prandtl-Zahl](https://de.wikipedia.org/wiki/Prandtl-Zahl) (=dimensionslose Kennzahl von Fluiden, das hei√üt von Gasen oder tropfbaren Fl√ºssigkeiten. Sie ist definiert als Verh√§ltnis zwischen kinematischer Viskosit√§t und Temperaturleitf√§higkeit. Die Prandtl-Zahl stellt die Verkn√ºpfung des Geschwindigkeitfeldes mit dem Temperaturfeld eines Fluids dar.)\n",
        "  * $b$ mit der [Rayleigh-Zahl](https://de.wikipedia.org/wiki/Rayleigh-Zahl) (=eine dimensionslose Kennzahl, die den Charakter der W√§rme√ºbertragung innerhalb eines Fluids beschreibt) identifiziert werden kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xyzZzNOEhv"
      },
      "source": [
        "###### *Summary: Attractor, Trajectory in phase space, Lyapunov exponent (forecast error)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIujOlBtNqx-"
      },
      "source": [
        "For this specific system shown, the vector field looks like this, and let scatter a bunch of random points around to represent different possible states see how they evolve and move around: They all spiral towards the center.\n",
        "\n",
        "For this exampe the attraction is zero, and the basin is every point in space. Notice that at the origin x dot and y dot also equal zero.\n",
        "\n",
        "$\\dot{x}=-(0)-0.1(0)=0$\n",
        "\n",
        "$\\dot{y}=(0)-0.4(0)=0$\n",
        "\n",
        "The origin is in this case **fixed point (attractor)**, because every point in there will stay forever. It seem like an inevitability, hence determinism (as chaos is).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkfT8XP8QmJd"
      },
      "source": [
        "But there are also other types of attractors, like the **Van der Pol oscillator**:\n",
        "\n",
        "> $\\dot{x}=\\mu\\left(x-\\frac{1}{3} x^{3}-y\\right)$\n",
        "\n",
        "> $\\dot{y}=\\frac{1}{\\mu} x$\n",
        "\n",
        "It creates something called: \"**Limit Cycle Attractor**\":\n",
        "\n",
        "* it doesn't end up in a point\n",
        "\n",
        "* typically appear in physical systems with some sort of oscillation (electrical circuits, tectonic plates, etc).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luo2ipo2RaUH"
      },
      "source": [
        "**Strange Attractor** (has a fractal dimension)\n",
        "\n",
        "Lorenz Attractor: Lorenz system describes convection cycles in the atmoshphere. Very sensitive to changes in initial conditions already after a short time:\n",
        "\n",
        "> $\\dot{x}=\\sigma(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(\\rho-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\beta z$\n",
        "\n",
        "The Lorenz equations have a few parameters that can be tweaked to alter the behavior of a system. This is what is known as ‚Äòstrange attractor‚Äô:\n",
        "\n",
        "> $\\dot{x}=10(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(28-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\frac{8}{3} z$\n",
        "\n",
        "1. No point in the space is ever visited more than once by the same trajectory - it that happens, the trajectory would travel in a predictable loop.\n",
        "2. And no 2 trajectories will ever intersect. If that happened, they would merge into the same path, giving two different sets of initial conditions the same outcome.\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_03.png)\n",
        "\n",
        "A single trajectory will visit an infinite number of points in this limited space, and this limited space will have an infinite number of trajectories.\n",
        "\n",
        "* Trajectories are just curved, so they should be 1-dimensional..normally!\n",
        "* For the strange attractor: no matter how much you zoom in on this attractor, you can always find more and more trajectories everywhere.\n",
        "* That‚Äôs why this attractor is said to have a **non integer dimension** - it‚Äôs **made up of infinite long curves in a finite space, which are so detailed, that they start to partially fill up higher dimensions**. It‚Äôs not 1, 2 or 3 dimensional, its somewhere in between. (=detail at arbitrarily small scales)\n",
        "\n",
        "Conclusion: Lorenz attractor is a fractal space, and hence a strange attractor.\n",
        "\n",
        "Even is both points started at the same point, small initial differences can bring them to complete different paths:\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chros_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOAWTDsoV7-F"
      },
      "source": [
        "The difference in trajectories increases exponentially:\n",
        "\n",
        "**$\\lambda$ stands for Lyapunov exponent**\n",
        "\n",
        "$\\lambda$ > 0, trajectories will increase exponentially (= chaotic)\n",
        "\n",
        "$\\lambda$ = 0, distance will stay constant\n",
        "\n",
        "$\\lambda$ < 0, distance will converge to zero.\n",
        "\n",
        "*Unfortunately there is no way to find the Lyapunov exponent only by looking at the equations. It is measured by running the simulation, keeping track of mainy pairs of trajectories, and find the average rate of change in their distance. But it provides a simple metric to comunicate how chaotic a system is*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_06.png)\n",
        "\n",
        "**For the Lorenz attractor it is $\\lambda$ ‚âà 0.9**\n",
        "\n",
        "Used to find out duration in which the predictions are valid: \"Predictability Horizon\". We get this by re-arranging our previous equation $d_{t}=d_{0} e^{\\lambda t}$:\n",
        "\n",
        "> Predictability horizon $=\\frac{1}{\\lambda} \\ln \\frac{a}{d_{0}}$\n",
        "\n",
        "$d_{0}=$ initial error\n",
        "\n",
        "$a=$ maximum allowed error\n",
        "\n",
        "**For the Lorenz attractor, after 10 time steps, any error would have multiplied by 8,000 already.**\n",
        "\n",
        "Example what that means with this exponential divergence:\n",
        "\n",
        "We have a simulation that predicts where ocean currents flow, and you want to keep the error less than 1,000 km.\n",
        "* If you ran it twice, once an initial error of 1m and once the initial error was a million times smaller than one micrometer (0,000001m), how much longer  the simulation with the smaller error would stay below the margin of error.\n",
        "* The simulation would be valid 9 days instead of 3 days, but for an initial error that is a million times smaller !!\n",
        "\n",
        "Let‚Äôs write the expressions for the two predictability horizons, and put them in a fraction:\n",
        "\n",
        "> $\\frac{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{0.000001}\\right)}{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{1}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(\\frac{1}{\\lambda} \\ln 10^{9}\\right)}{\\left(\\frac{1}{\\lambda} \\ln 10^{3}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(9 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}{\\left(3 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}$\n",
        "\n",
        "> = $\\frac{9}{3}$.\n",
        "\n",
        "= It‚Äôs 3 times longer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Stochastic**"
      ],
      "metadata": {
        "id": "82Lb_gBqUzi8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1flpzoQFTVa"
      },
      "source": [
        "##### <font color=\"blue\">*Stochastic Analysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Analysis**\n",
        "\n",
        "* [Stochastic calculus](https://en.m.wikipedia.org/wiki/Stochastic_calculus) bzw. [Stochastische Analysis](https://de.m.wikipedia.org/wiki/Stochastische_Analysis) is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. This field is created and started by the Japanese mathematician Kiyoshi It√¥ during World War 2.\n",
        "\n",
        "* [Stochastische_Integration](https://de.m.wikipedia.org/wiki/Stochastische_Integration): Es sind stochastische Prozesse mit unendlicher Variation, insbesondere der Wiener-Prozess, als Integratoren zugelassen.\n",
        "\n",
        "* [Stochastischer Prozess](https://de.m.wikipedia.org/wiki/Stochastischer_Prozess): (auch Zufallsprozess) zeitlich geordnete, zuf√§llige Vorg√§nge. Theorie der stochastischen Prozesse ist Erweiterung der Wahrscheinlichkeitstheorie dar und bildet Grundlage f√ºr stochastische Analysis.\n",
        "\n",
        "* > Stochastic: Statistik + Probability Theory (incl stochastic / random processes)\n",
        "\n",
        "* Bei einem Sparguthaben entspr√§che dies dem **exponentiellen Wachstum** durch Zinseszins. Bei Aktien wird dieses Wachstumsgesetz hingegen **in der Realit√§t offenbar durch eine komplizierte Zufallsbewegung √ºberlagert**.\n",
        "\n",
        "* Bei zuf√§lligen St√∂rungen, die sich aus vielen kleinen Einzel√§nderungen zusammensetzen, **wird von einer Normalverteilung als einfachstem Modell ausgegangen**.\n",
        "\n",
        "* Au√üerdem zeigt sich, dass die Varianz der St√∂rungen proportional zum betrachteten Zeitraum $\\Delta t$ ist. Der Wiener-Prozess $W_t$ besitzt alle diese gew√ºnschten Eigenschaften, eignet sich also als ein Modell f√ºr die zeitliche Entwicklung der Zufallskomponente des Aktienkurses.\n"
      ],
      "metadata": {
        "id": "DC-KHFg7J17I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0iE2bRTLxY"
      },
      "source": [
        "**Stochastic Differential Equation ( ‚Üí  Black Scholes, Spontaneous Symmetry Breaking)**\n",
        "\n",
        "* Eine [stochastischen Differentialgleichung](https://de.wikipedia.org/wiki/Stochastische_Differentialgleichung) ist eine Verallgemeinerung des Begriffs der gew√∂hnlichen Differentialgleichung auf stochastische Prozesse.\n",
        "\n",
        "> Stochastische Differentialgleichungen werden in zahlreichen Anwendungen eingesetzt, **um zeitabh√§ngige Vorg√§nge zu modellieren**, die neben deterministischen Einfl√ºssen zus√§tzlich **stochastischen St√∂rfaktoren (Rauschen)** ausgesetzt sind.\n",
        "\n",
        "* Die formale Theorie der stochastischen Differentialgleichungen wurde erst in den 1940er Jahren durch den japanischen Mathematiker It≈ç Kiyoshi formuliert.\n",
        "\n",
        "* Gemeinsam mit der **stochastischen Integration** begr√ºndet die Theorie der **stochastischen Differentialgleichungen** die **stochastische Analysis**.\n",
        "\n",
        "* **Objective**: Genau wie bei deterministischen Funktionen m√∂chte man auch bei stochastischen Prozessen den Zusammenhang zwischen dem Wert der Funktion und ihrer momentanen √Ñnderung (ihrer Ableitung) in einer Gleichung formulieren.\n",
        "\n",
        "* **Challenge**: Was im einen Fall zu einer gew√∂hnlichen Differentialgleichung f√ºhrt, ist im anderen Fall problematisch, da viele stochastische Prozesse, wie beispielsweise **der Wiener-Prozess, nirgends differenzierbar sind**.\n",
        "\n",
        "* Loesung ist wie bei gewohnlichen Differentialgleichungen plus einen stochastischen Term\n",
        "\n",
        "*Beim Typus der stochastischen Differentialgleichungen treten in der Gleichung stochastische Prozesse auf. Eigentlich sind stochastische Differentialgleichungen keine Differentialgleichungen [im obigen Sinne](https://de.m.wikipedia.org/wiki/Differentialgleichung#Weitere_Typen), sondern lediglich gewisse Differentialrelationen, welche als Differentialgleichung interpretiert werden k√∂nnen.*\n",
        "\n",
        "*Beispiele fur stochastische Differentialgleichungen*\n",
        "\n",
        "* Die SDGL f√ºr die geometrische brownsche Bewegung lautet $\\mathrm{d} S_{t}=r S_{t} \\mathrm{~d} t+\\sigma S_{t} \\mathrm{~d} W_{t} .$ Sie wird beispielsweise im Black-Scholes-Modell zur Beschreibung von Aktienkursen verwendet.\n",
        "\n",
        "* Die SDGL f√ºr einen Ornstein-Uhlenbeck-Prozess ist $\\mathrm{d} X_{t}=\\theta\\left(\\mu-X_{t}\\right) \\mathrm{d} t+\\sigma \\mathrm{d} W_{t} .$ Sie wird unter anderem im Vasicek-Modell zur finanzmathematischen Modellierung von Zinss√§tzen √ºber den Momentanzins\n",
        "verwendet.\n",
        "\n",
        "* Die SDGL f√ºr den Wurzel-Diffusionsprozess nach William Feller lautet $\\mathrm{d} X_{t}=\\kappa\\left(\\theta-X_{t}\\right) \\mathrm{d} t+\\sigma \\sqrt{X_{t}} \\mathrm{~d} W_{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelei85Xp8rU"
      },
      "source": [
        "**Supersymmetric theory of stochastic dynamics & Spontaneous Symmetry breaking**\n",
        "\n",
        "* [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) **is an exact theory of stochastic (partial) differential equations (SDEs)**, the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Chaos_theory#Chaos_as_a_spontaneous_breakdown_of_topological_supersymmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpmflh50O1X"
      },
      "source": [
        "##### <font color=\"blue\">*Markov Process & Monte Carlo Simulation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_psDmmPW0P"
      },
      "source": [
        "A Markov chain is a **discrete-time stochastic process** that progresses from one state to another with certain probabilities that can be **represented by a graph and state transition matrix P** as indicated below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/markov.PNG\" alt=\"markov\">\n",
        "\n",
        "* Markov chains may be modeled by **finite state machines, and random walks**\n",
        "\n",
        "* A Stochastic (State Transition) Matrix describes a Markov chain X<sub>t</sub> over a [**finite state space (Probability space) S**](https://en.m.wikipedia.org/wiki/Probability_space) with cardinality S.\n",
        "\n",
        "  * Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties.\n",
        "\n",
        "  * Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* Let P be the transition matrix of Markov chain {X0, X1, ...}.\n",
        "\n",
        "  * **Reducibility**: a Markov chain is said to be irreducible if it is possible to get to any state from any state. In other words, a Markov chain is irreducible if there exists a chain of steps between any two states that has positive probability.\n",
        "\n",
        "  * **Periodicity**: a state in a Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. Thus, starting in state 'i', the chain can return to 'i' only at multiples of the period 'k', and k is the largest such integer. State 'i' is aperiodic if k = 1 and periodic if k > 1.\n",
        "\n",
        "  * **Transience and Recurrence**: A state 'i' is said to be transient if, given that we start in state 'i', there is a non-zero probability that we will never return to 'i'. State i is recurrent (or persistent) if it is not transient. A recurrent state is known as positive recurrent if it is expected to return within a finite number of steps and null recurrent otherwise. Transience and recurrence issues are central to the study of Markov chains and help describe the Markov chain's overall structure. The presence of many transient states may suggest that the Markov chain is absorbing, and a strong form of recurrence is necessary in an ergodic Markov chain.\n",
        "\n",
        "  * **Ergodicity**: a state 'i' is said to be ergodic if it is aperiodic and positive recurrent. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic. Ergodic Markov chains are, in some senses, the processes with the \"nicest\" behavior.\n",
        "\n",
        "  * **Absorbing State**: a state i is called absorbing if it is impossible to leave this state. Therefore, the state 'i' is absorbing if pii = 1 and pij = 0 for i ‚â† j. If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain. Absorbing states are crucial for the discussion of absorbing Markov chains. A common type of Markov chain with transient states is an absorbing one. An absorbing Markov chain is a Markov chain in which it is impossible to leave some states, and any state could (after some number of steps, with positive probability) reach such a state. It follows that all non-absorbing states in an absorbing Markov chain are transient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcP1_FkDDDOv"
      },
      "source": [
        "**Monte Carlo Simulation**\n",
        "\n",
        "* Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response.\n",
        "\n",
        "* Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on **repeated random sampling to obtain numerical results**. The underlying concept is to use randomness to solve problems that might be deterministic in principle.\n",
        "\n",
        "* Monte Carlo methods are useful for **simulating systems with many coupled degrees of freedom**, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean‚ÄìVlasov processes, kinetic models of gases). Other examples include **modeling phenomena with significant uncertainty in inputs** such as the calculation of risk in business\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6jdLMMCTSD"
      },
      "source": [
        "**Markov Chain Monte Carlo (MCMC)**\n",
        "\n",
        "* Markov Chain Monte Carlo refers to a **class of methods** for sampling from a probability distribution in order to **construct the most likely distribution**.\n",
        "\n",
        "> MCMC can be considered as a **random walk** that gradually converges to the true distribution.\n",
        "\n",
        "* We cannot directly calculate the (i.e. logistic) distribution, so instead we generate thousands of values ‚Äî called samples ‚Äî for the parameters of the function (alpha and beta) to **create an approximation of the distribution**.\n",
        "\n",
        "* The idea behind MCMC is that **as we generate more samples, our approximation gets closer and closer to the actual true distribution**.\n",
        "\n",
        "* Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. **Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chCbs1o_Iud"
      },
      "source": [
        "##### <font color=\"blue\">*White Noise*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0ShhpknOvG"
      },
      "source": [
        "\n",
        "**A white noise process has following conditions**\n",
        "\n",
        "* Mean (level) is zero (does not change over time - stationary process)\n",
        "* Variance is constant (does not change over time - stationary process)\n",
        "* Zero autocorrelation (values do not correlate with lag values)\n",
        "\n",
        "**White Noise: Independent & Identically Distributed**\n",
        "\n",
        "* Hence, in a time series is white noise if the variables are independent and identically distributed (IID) with a mean of zero.\n",
        "\n",
        "* The term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies\n",
        "\n",
        "* **Necessary Condition**: Independence: variables are statistically uncorrelated = their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element R·µ¢·µ¢ is the variance of component w·µ¢; and the correlation matrix must be the n by n identity matrix.\n",
        "\n",
        "* **Sufficient Condition**: every variable in w has a normal distribution with zero mean and the same variance, w is said to be a Gaussian white noise vector. In that case, the joint distribution of w is a multivariate normal distribution; the independence between the variables then implies that the distribution has spherical symmetry in n-dimensional space. Therefore, any orthogonal transformation of the vector will result in a Gaussian white random vector. In particular, under most types of discrete Fourier transform, such as FFT and Hartley, the transform W of w will be a Gaussian white noise vector, too; that is, the n Fourier coefficients of w will be independent Gaussian variables with zero mean and the same variance.\n",
        "\n",
        "* A random vector (that is, a partially indeterminate process that produces vectors of real numbers) is said to be a white noise vector or white random vector if its components each have a probability distribution with zero mean and finite variance, and are statistically independent: that is, their joint probability distribution must be the product of the distributions of the individual components.\n",
        "\n",
        "* First moment has to be zero and second moment has to be finite though. (iid ) White noise is always an independent process but reverse may not be true.\n",
        "\n",
        "* The technical definition of white noise is that it has equal intensity at all frequencies. This corresponds to a delta function autocorrelation. This is only possible if there is no correlation between any sequential values. So yes, the independence is true both backwards and forwards. Note that the actual distribution is irrelevant.\n",
        "\n",
        "\n",
        "\n",
        "**Types of White Noise Processes**\n",
        "* If the variables in the series are drawn from a Gaussian distribution, the series is called Gaussian white noise\n",
        "* There are also white noise processes, like Levy etc.\n",
        "\n",
        "**Relationship to Stochastic Processes**\n",
        "* White noise is the generalized mean-square derivative of the Wiener process or Brownian motion (so Wiener is an integrated White Noise)\n",
        "\n",
        "**White noise is an important concept in time series analysis and forecasting**\n",
        "\n",
        "* **Predictability**: If your time series is white noise, then, by definition, it is random. You cannot reasonably model it and make predictions.\n",
        "* **Model Diagnostics**: The statistics and diagnostic plots can be uses on time series to check if it is white noise. The series of errors from a time series forecast model should ideally be white noise. If the series of forecast errors are not white noise, it suggests improvements could be made to the predictive model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8idRoUCpsoKR"
      },
      "source": [
        "##### <font color=\"blue\">*Random Walk*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdW8o7tCtbPh"
      },
      "source": [
        "**Random Walk**\n",
        "\n",
        "* Random walk is another time series model where the current observation is equal to the previous observation with a random step up or down. Known as a stochastic or random process.\n",
        "\n",
        "> y<sub>(t)</sub> = B<sub>0</sub> + B<sub>1</sub> * X<sub>(t-1)</sub> + e<sub>(t)</sub>\n",
        "\n",
        "* A random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence.\n",
        "\n",
        "* The process used to generate the series forces dependence from one-time step to the next. This dependence provides some consistency from step-to-step rather than the large jumps that a series of independent, random numbers provides. It is this dependency that gives the process its name as a ‚Äúrandom walk‚Äù or a ‚Äúdrunkard‚Äôs walk‚Äù.\n",
        "\n",
        "> **A simple random walk is a martingale**\n",
        "\n",
        "* In higher dimensions, the set of randomly walked points has interesting geometric properties. In fact, one gets a discrete fractal, that is, a set which exhibits stochastic self-similarity on large scales.\n",
        "* Examples include the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler: all can be approximated by random walk models, even though they may not be truly random in reality\n",
        "* Random walks serve as a fundamental model for the recorded stochastic activity / stochastic processes.\n",
        "* As a more mathematical application, the value of œÄ can be approximated by the use of random walk in an agent-based modeling environment.\n",
        "\n",
        "* There are many types of time-dependent processes referred to as random walks - most often refers to a special category of Markov chains or Markov processes. A random walk on the integers (and the gambler's ruin problem) are examples of **Markov processes in discrete time**.\n",
        "\n",
        "* Specific cases or limits of random walks include the L√©vy flight and diffusion models such as Brownian motion. **A Wiener process (~ Brownian motion) is the integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments. A Wiener process is the scaling limit of random walk in dimension 1.\n",
        "\n",
        "* Random walks can take place on a variety of spaces: graphs, on the integers or real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* **Random Walk and Autocorrelation**\n",
        "\n",
        "  * We can calculate the correlation between each observation and the observations at previous time steps. Given the way that the random walk is constructed, we would expect a strong autocorrelation with the previous observation and a linear fall off from there with previous lag values.\n",
        "\n",
        "* **Stationarity**\n",
        "\n",
        "  * A stationary time series is one where the values are not a function of time. Given the way that the random walk is constructed and the results of reviewing the autocorrelation, we know that the observations in a random walk are dependent on time.\n",
        "  * The current observation is a random step from the previous observation. Therefore we can expect a random walk to be non-stationary. In fact, all random walk processes are non-stationary. Note that not all non-stationary time series are random walks.\n",
        "  * Additionally, a non-stationary time series does not have a consistent mean and/or variance over time. A review of the random walk line plot might suggest this to be the case. We can confirm this using a statistical significance test, specifically the Augmented Dickey-Fuller test.\n",
        "\n",
        "**IID**\n",
        "\n",
        "  * This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (‚Äúi.i.d.‚Äù).\n",
        "\n",
        "  * This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied. So, if you begin with a time series that wanders all over the map, but you find that its first difference looks like it is an i.i.d. sequence, then a random walk model is a potentially good candidate.\n",
        "\n",
        "* **Prediction**\n",
        "\n",
        "  * A random walk is unpredictable; it cannot reasonably be predicted. Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step. Simply because we know that the next time step will be a function of the prior time step.\n",
        "  * This is often called the naive forecast, or a persistence model. We can implement this in Python by first splitting the dataset into train and test sets, then using the persistence model to predict the outcome using a rolling forecast method. Once all predictions are collected for the test set, the mean squared error is calculated.\n",
        "\n",
        "* **Drift**\n",
        "\n",
        "  * A random walk model is said to have ‚Äúdrift‚Äù or ‚Äúno drift‚Äù according to whether the distribution of step sizes has a non-zero mean or a zero mean. At period n, the k-step-ahead forecast that the random walk model without drift gives for the variable Y is\n",
        "\n",
        "  > $\\hat{Y}_{n+k}=Y_{n}$\n",
        "\n",
        "  * In others words, it predicts that all future values will equal the last observed value. This doesn‚Äôt really mean you expect them to all be the same, but just that you think they are equally likely to be higher or lower, and you are staying on the fence as far as point predictions are concerned. If you extrapolate forecasts from the random walk model into the distant future, they will go off on a horizontal line, just like the forecasts of the mean model. So, qualitatively the long-term point forecasts of the random walk model look similar to those of the mean model, except that they are always ‚Äúre-anchored‚Äù on the last observed value rather than the mean.of the historical data.\n",
        "\n",
        "  * For the random-walk-with-drift model, the k-step-ahead forecast from period n is:\n",
        "\n",
        "  > $\\hat{\\mathrm{Y}}_{\\mathrm{n}+\\mathrm{k}}=\\mathrm{Y}_{\\mathrm{n}}+\\mathrm{k} \\hat{\\mathrm{d}}$\n",
        "\n",
        "  * where dÀÜ is the estimated drift, i.e., the average increase from one period to the next. So, the long-term forecasts from the random-walk-with-drift model look like a trend line with slope dÀÜ , but it is always re-anchored on the last observed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk3roPmAuCDi"
      },
      "source": [
        "**Applications of Random Walks**\n",
        "\n",
        "* In computer networks, random walks can model the number of transmission packets buffered at a server.\n",
        "* In population genetics, random walk describes the statistical properties of genetic drift.\n",
        "* In image segmentation, random walks are used to determine the labels (i.e., ‚Äúobject‚Äù or ‚Äúbackground‚Äù) to associate with each pixel.\n",
        "* In brain research, random walks and reinforced random walks are used to model cascades of neuron firing in the brain.\n",
        "Random walks have also been used to sample massive online graphs such as online social networks.\n",
        "\n",
        "* **Random Walks in Financial Time Series**\n",
        "\n",
        "  * **Is time series a random walk?**: Your time series may be a random walk. Some ways to check if your time series is a random walk are as follows:\n",
        "The time series shows a strong temporal dependence that decays linearly or in a similar pattern.\n",
        "  * The time series is non-stationary and making it stationary shows no obviously learnable structure in the data.\n",
        "The persistence model provides the best source of reliable predictions.\n",
        "  * This last point is key for time series forecasting. Baseline forecasts with the persistence model quickly flesh out whether you can do significantly better. If you can‚Äôt, you‚Äôre probably working with a random walk. Many time series are random walks, particularly those of security prices over time. The random walk hypothesis is a theory that stock market prices are a random walk and cannot be predicted.\n",
        "  * \"A random walk is one in which future steps or directions cannot be predicted on the basis of past history. When the term is applied to the stock market, it means that short-run changes in stock prices are unpredictable.\" - Page 26, A Random Walk down Wall Street: The Time-tested Strategy for Successful Investing. https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/\n",
        "\n",
        "* https://towardsdatascience.com/random-walks-with-python-8420981bc4bc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbYRNP_zXe4"
      },
      "source": [
        "##### <font color=\"blue\">*Wiener Process (Brownian Motion)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldqy2UtMpa8P"
      },
      "source": [
        "**Wiener Process (Brownian Motion)**\n",
        "\n",
        "* Video: Physics of Randomness: https://youtu.be/5jBVYvHeG2c\n",
        "\n",
        "* Brownian Motion: Flower particles move randomly because they are hit by particles that move. And particles move faster with more heat.\n",
        "\n",
        "* Same in astrophysics with stars under the gravitational influence of smaller stars, big stars move a bit randomly\n",
        "\n",
        "* Same with stock market where the price is influenced by many factors all the time (and hence the price moves randomly up and down)\n",
        "\n",
        "**The Wiener process is a real valued continuous-time (continuous state-space) stochastic process**\n",
        "\n",
        "* W<sub>0</sub> = 0 (P-almost certain)\n",
        "* The Wiener process has (stochastically) independent increments.\n",
        "* The increases are therefore stationary and normally distributed with the expected value zero and the variance t - s.\n",
        "* The individual paths are (P-) almost certainly continuous.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* In physics it is used to study Brownian motion, the diffusion of minute particles suspended in fluid, and other types of diffusion via the Fokker‚ÄìPlanck and Langevin equations.\n",
        "* It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman‚ÄìKac formula, a solution to the Schr√∂dinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology.\n",
        "* It is also prominent in the mathematical theory of finance, in particular the Black‚ÄìScholes option pricing model.\n",
        "\n",
        "**Properties of a Wiener Process**\n",
        "\n",
        "1. The Wiener process belongs to the family of **Markov processes** and there specifically to the class of **Levy processes**. It also fulfills the strong markov property. It is one of the best known L√©vy processes (**c√†dl√†g** stochastic processes with stationary independent increments).\n",
        "2. The Wiener Process is a **special Gaussian process** with an expected value function E(W<sub>t</sub>)  = 0 and and the covariance function Cov (W<sub>s</sub>, W<sub>t</sub>) = min (s,t)\n",
        "3. The Wiener process is a (continuous time) **martingale** (L√©vy characterisation: the Wiener process is an almost surely continuous martingale with W0 = 0 and quadratic variation [Wt, Wt] = t, which means that Wt2 ‚àí t is also a martingale).\n",
        "4. The Wiener process is a **Levy process** with steady paths and constant expectation 0.\n",
        "\n",
        "*Another characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent N(0, 1) random variables. This representation can be obtained using the Karhunen‚ÄìLo√®ve theorem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9PQvsR1bKfq"
      },
      "source": [
        "**Wiener process as a limit of random walk (& Differences between Wiener Process & Random Walk)**\n",
        "\n",
        "* https://www.quora.com/What-is-an-intuitive-explanation-of-a-Wiener-process?top_ans=3955819\n",
        "\n",
        "* A Wiener process (~ Brownian motion) is the **integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments.\n",
        "\n",
        "* Let X<sub>1</sub>, X<sub>2</sub>, X<sub>n</sub> be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1.  The central limit theorem asserts that W<sup>(n)</sup> (1) converges in distribution to a standard Gaussian random variable W(1) as n ‚Üí ‚àû.\n",
        "\n",
        "* [Donsker's theorem](https://en.m.wikipedia.org/wiki/Donsker%27s_theorem) asserts that as n ‚Üí ‚àû , W<sub>n</sub> approaches a Wiener process, which explains the ubiquity of Brownian motion. **Donsker's invariance principle** states that: As random variables taking values in the Skorokhod space D [0,1], the random function W<sup>(n)</sup> converges in distribution to a standard Brownian motion W := (W(t))<sub>t ‚àà [0,1]</sub> as n ‚Üí ‚àû.\n",
        "\n",
        "![Donsker's Invariance Principle](https://upload.wikimedia.org/wikipedia/commons/8/8c/Donskers_invariance_principle.gif)\n",
        "\n",
        "*Donsker's invariance principle for simple random walk on Z*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/wiener.jpg\" alt=\"wiener\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKFiwhd7X_5I"
      },
      "source": [
        "**Differences to other stochastic processes**\n",
        "\n",
        "**Random Walk**\n",
        "\n",
        "\n",
        "* **A Wiener process is the [scaling limit](https://en.m.wikipedia.org/wiki/Scaling_limit) of random walk in dimension 1**. The **convergence of a random walk toward the Wiener process is controlled by the central limit theorem**, and by **Donsker's theorem**. The **Green's function** of the diffusion equation that controls the Wiener process, suggests that, **after a large number of steps, the random walk converges toward a Wiener process**.\n",
        "\n",
        "* A random walk is a discrete fractal (a function with integer dimensions; 1, 2, ...), but a **Wiener process trajectory is a true fractal**, and there is a connection between the two (a Wiener process walk is a fractal of **Hausdorff dimension** 2).\n",
        "\n",
        "* Unlike the random walk, a **Wiener Process is scale invariant**. A Wiener process enjoys many symmetries random walk does not. For example, a **Wiener process walk is invariant to rotations, but the random walk is not**, since the underlying grid is not. This means that in many cases, problems on a random walk are easier to solve by translating them to a Wiener process, solving the problem there, and then translating back.\n",
        "\n",
        "**(Gaussian) White Noise**\n",
        "\n",
        "* The Wiener process is used to represent the integral (from time zero to time t) of a zero mean, unit variance, delta correlated **<u>Gaussian</u> white noise process**\n",
        "\n",
        "**Brownian Motion**\n",
        "\n",
        "* **\"Brownian motion\" is a phenomenon that can be modeled with a Wiener Process**, because a Wiener process is a stochastic process with similar behavior to Brownian motion, the physical phenomenon of a minute particle diffusing in a fluid.\n",
        "\n",
        "* The Brownian motion process (and the Poisson process in one dimension) are both examples of **Markov processes in continuous time**\n",
        "\n",
        "* It≈ç also paved the way for the Wiener process from physics to other sciences: the **stochastic differential equations** he set up made it possible to adapt the Brownian motion to more statistical problems.\n",
        "\n",
        "* The **geometric Brownian motion** derived from a stochastic differential equation solves the problem that the **Wiener process, regardless of its starting value, almost certainly reaches negative values over time, which is impossible for stocks**. Since the development of the famous **Black-Scholes model**, the geometric Brownian movement has been the standard.\n",
        "\n",
        "**Ornstein-Uhlenbeck-Process**\n",
        "\n",
        "* The problem raised by the **[non-rectifiable paths](https://en.m.wikipedia.org/wiki/Arc_length)** of the Wiener process in the modeling of Brownian paths leads to the Ornstein-Uhlenbeck process and also makes the need for a theory of stochastic integration and differentiation clear\n",
        "* here it is not the motion but the speed of the particle as one that is not rectifiable process derived from the Wiener process, from which one obtains rectifiable particle paths through integration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0K1gPOoznzE"
      },
      "source": [
        "##### <font color=\"blue\">*Gaussian Process*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoS5J618HxN"
      },
      "source": [
        "* A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. **every finite linear combination of them is normally distributed**.\n",
        "\n",
        "* The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
        "\n",
        "* Gaussian Processes are a class of stationary, zero-mean stochastic processes which are completely dependent on their autocovariance functions. This class of models can be used for both regression and classification tasks.\n",
        "\n",
        "* Gaussian Processes provide estimates about uncertainty, for example giving an estimate of how sure an algorithm is that an item belongs to a class or not.\n",
        "\n",
        "* In order to deal with situations which embed a certain degree of uncertainty is typically made use of probability distributions.\n",
        "\n",
        "* Gaussian processes can allow us to describe probability distributions of which we can later update the distribution using Bayes Rule once we gather new training data.\n",
        "\n",
        "* **Relation to other Stochastic Processes**\n",
        "\n",
        "  * A **Wiener process (~ Brownian motion)** is the integral of a white noise generalized Gaussian process. It is not stationary, but it has stationary increments.\n",
        "\n",
        "  * The **fractional Brownian motion** is a Gaussian process whose covariance function is a generalisation of that of the Wiener process.\n",
        "\n",
        "  * The **Ornstein‚ÄìUhlenbeck** process is a stationary Gaussian process.\n",
        "\n",
        "  * The **Brownian bridge** is (like the Ornstein‚ÄìUhlenbeck process) an example of a Gaussian process whose increments are not independent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOw3me6qbku"
      },
      "source": [
        "##### <font color=\"blue\">*Further Stochastic Processes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEr4Ub47xgN1"
      },
      "source": [
        "**Brownian Bridge**\n",
        "\n",
        "* A Brownian bridge is a **continuous Gaussian process** with X<sub>0</sub> = X<sub>1</sub> = 0, and with mean and covariance functions given in (c) and (d), respectively.\n",
        "\n",
        "* The **Brownian bridge is (like the Ornstein‚ÄìUhlenbeck process)** an example of a Gaussian process **whose increments are not independent**.\n",
        "\n",
        "* There are several ways of constructing a Brownian bridge from a standard Brownian motion [Source](https://www.randomservices.org/random/brown/Bridge.html)\n",
        "\n",
        "* **Applications: Path Simulation for Stock Shares**: The simple Monte Carlo method with Euler method supplemented by the Brownian bridge correction for the possibility of falling below or exceeding the barriers between discretization times.\n",
        "\n",
        "  * By merely discreetly viewing (simulating) the (log) share price, those paths can also lead to a positive final payment in which the share price between the selected times k delta t has exceeded the lower barrier or exceeded the upper barrier without this is noticed in the discretized model.\n",
        "\n",
        "  * To calculate the probability of such an unnoticed barrier violation, Brownian Bridge is used (with the help of the independence and stationarity of its growth).\n",
        "\n",
        "  * With the help of the statements about the Brown Bridge, one can formally. Specify the Monte Carlo algorithm that can be used to evaluate double barrier options without having to discretize the price path.\n",
        "\n",
        "* **Application: Bond Prices**\n",
        "\n",
        "  * Computation of bond prices in a structural default model with jumps with an unbiased Monte-Carlo simulation.\n",
        "\n",
        "  * The algorithm requires the evaluation of integrals with the density of the first-passage time of a Brownian bridge as the integrand. (Metwally and Atiya (2002) suggest an approximation of these integrals.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0CRhOchiHXb"
      },
      "source": [
        "**Ornstein-Uhlenbeck Process**\n",
        "\n",
        "> Simulating a stochastic differential equation\n",
        "\n",
        "* the Ornstein‚ÄìUhlenbeck process is a **stochastic process** with applications in financial mathematics and the physical sciences.\n",
        "\n",
        "* Its original application in physics was as a model for the **<u>velocity</u> of a massive Brownian particle under the influence of <u>friction</u>**. It is named after Leonard Ornstein and George Eugene Uhlenbeck.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is a **stationary Gauss‚ÄìMarkov process**, which means that it is a **Gaussian process, a Markov process, and is temporally homogeneous**. In fact, it is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables.\n",
        "\n",
        "* Over time, the process tends to **drift towards its mean function**: such a process is called **mean-reverting**.\n",
        "\n",
        "* The process can be considered to be a **modification of the random walk in continuous time, or Wiener process**, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the center.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can also be considered as the **continuous-time analogue of the discrete-time AR(1) process**.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can be interpreted as a **scaling limit of a discrete process**, in the same way that Brownian motion is a scaling limit of random walks.\n",
        "\n",
        "* Generalization: It is possible to extend Ornstein‚ÄìUhlenbeck processes to processes where the background driving process is a L√©vy process (instead of a simple Brownian motion).\n",
        "\n",
        "* In addition, in finance, stochastic processes are used where the volatility increases for larger values of C.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process (just like the Brownian Bridge) a is an example of a **Gaussian process whose increments are not independent**. Look for stock returns devoid of explanatory factors, and analyze the corresponding residuals as stochastic processes. (e.g. mean reverting?). Can residuals be fitted to (increments of) OU processes or other MR processes? If so, what is the typical correlation time-scale? Mean reversion days: how long does it take to converge (e.g. model distribution of days).\n",
        "\n",
        "**In Financial Mathematics**\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is one of several approaches used to model (with modifications) interest rates, currency exchange rates, and commodity prices stochastically.\n",
        "\n",
        "* The parameter Œº (mu) represents the equilibrium or mean value supported by fundamentals; œÉ (signa) the degree of volatility around it caused by shocks, and Œ∏ (theta) the rate by which these shocks dissipate and the variable reverts towards the mean.\n",
        "\n",
        "* One application of the process is a trading strategy known as pairs trade.\n",
        "\n",
        "* Stationary and mean-reverting around mean=10 (red dotted line)\n",
        "* **In financial engineering: how long does it take in average to converge? mean-reversion is an investment opportunity!**\n",
        "* Now, we are going to take a look at the time evolution of the distribution of the process. To do this, we will simulate many independent realizations of the same process in a vectorized way. We define a vector X that will contain all realizations of the process at a given time (that is, we do not keep all realizations at all times in memory). This vector will be overwritten at every time step. We will show the estimated distribution (histograms) at several points in time:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqrw3_Nqev5"
      },
      "source": [
        "**Jump diffusion**\n",
        "\n",
        "* [Jump diffusion](https://en.m.wikipedia.org/wiki/Jump_diffusion) is a stochastic process that involves jumps and diffusion.\n",
        "\n",
        "* It has important applications in magnetic reconnection, coronal mass ejections, condensed matter physics, option pricing, and pattern theory and computational vision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQP6Jd68WsL"
      },
      "source": [
        "**L√©vy Process**\n",
        "\n",
        "* A L√©vy process is a stochastic process with **[independent, stationary increments](https://de.m.wikipedia.org/wiki/Prozess_mit_unabh√§ngigen_Zuw√§chsen)** (= the course of the future of the process is independent of the past): it represents the motion of a point whose successive displacements are random and independent, and statistically identical over different time intervals of the same length.\n",
        "\n",
        "* A L√©vy process may thus be viewed as the **continuous-time analog of a random walk**.\n",
        "\n",
        "* The most well known **examples of L√©vy processes are Wiener process (~ Brownian motion), and Poisson process**. Aside from Brownian motion with drift, all other proper (that is, not deterministic) L√©vy processes have discontinuous paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAvA4JovM_-"
      },
      "source": [
        "**Bernoulli Process**\n",
        "\n",
        "* The outcomes of a Bernoulli process will follow a [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution).\n",
        "\n",
        "* https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/\n",
        "\n",
        "* A Bernoulli process is a finite or infinite sequence of binary random variables, so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1.\n",
        "\n",
        "* The component Bernoulli variables X<sub>i</sub> are [identically distributed and independent](https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). Prosaically, a Bernoulli process is a repeated coin flipping, possibly with an unfair coin (but with consistent unfairness).\n",
        "\n",
        "* Every variable X<sub>i</sub> in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided dice); this generalization is known as the **Bernoulli scheme**.\n",
        "\n",
        "* The problem of determining the process, given only a limited sample of Bernoulli trials, may be called the problem of checking whether a coin is fair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPTaHcB54U"
      },
      "source": [
        "**Poisson (Point) Process**\n",
        "\n",
        "* Poisson point process is a type of random mathematical object that consists of points randomly located on a mathematical space\n",
        "\n",
        "* Its name derives from the fact that if a collection of random points in some space forms a Poisson process, then the number of points in a region of finite size is a random variable with a Poisson distribution.\n",
        "\n",
        "* The process was discovered independently and repeatedly in several settings, including experiments on radioactive decay, telephone call arrivals and insurance mathematics. The Poisson point process is often defined on the real line, where it can be considered as a stochastic process.\n",
        "\n",
        "* On the real line, the Poisson process is a type of **continuous-time Markov process** known as a birth-death process (with just births and zero deaths) and is called a pure or simple birth process.\n",
        "\n",
        "* In this setting, it is used, for example, in queueing theory to model random events, such as the arrival of customers at a store, phone calls at an exchange or occurrence of earthquakes, distributed in time. In the plane, the point process, also known as a spatial Poisson process, can represent the locations of scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest.\n",
        "\n",
        "* In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process.\n",
        "\n",
        "* Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena where there is sufficiently strong interaction between the points. This has inspired the proposal of other point processes, some of which are constructed with the Poisson point process, that seek to capture such interaction.\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-process-everything-you-need-to-know-322aa0ab9e9a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Measure & Probability Theory*"
      ],
      "metadata": {
        "id": "ECcHaTG2nKiM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4KDiI20GSS8"
      },
      "source": [
        "###### *Measure (Ma√ü)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FMu2VsVHXMJ"
      },
      "source": [
        "* Ein Ma√ü ist in der Mathematik **eine Funktion**, die geeigneten Teilmengen einer Grundmenge Zahlen zuordnet, die als ‚ÄûMa√ü‚Äú f√ºr die Gr√∂√üe dieser Mengen interpretiert werden k√∂nnen.\n",
        "\n",
        "* Es sei $\\mathcal{A}$ eine o-Algebra √ºber einer nicht-leeren Grundmenge $\\Omega .$ Eine Funktion $\\mu: \\mathcal{A} \\rightarrow[0, \\infty]$ hei√üt Ma√ü auf $\\mathcal{A}$, wenn die beiden folgenden Bedingungen erf√ºllt sind:\n",
        "\n",
        "> $\\mu(\\emptyset)=0$\n",
        "\n",
        "> Additivit√§t: F√ºr jede Folge $\\left(A_{n}\\right)_{n \\in \\mathbb{N}}$ paarweise disjunkter Mengen aus $\\mathcal{A}$ gilt $\\mu\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right)$\n",
        "\n",
        "* Ist die o-Algebra aus dem Zusammenhang klar, so spricht man auch von einem Ma√ü auf $\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tOKtjbyH7Ai"
      },
      "source": [
        "* Eine Teilmenge von $\\Omega$, die in $\\mathcal{A}$ liegt, wird messbar genannt. F√ºr solch ein $A \\in \\mathcal{A}$ hei√üt $\\mu(A)$ das Ma√ü der Menge $A$.\n",
        "\n",
        "* Das **Tripel $(\\Omega, \\mathcal{A}, \\mu)$ wird Ma√üraum** genannt.\n",
        "\n",
        "* Das Paar $(\\Omega, \\mathcal{A})$ bestehend aus der Grundmenge und der darauf definierten $\\sigma$-Algebra **hei√üt Messraum oder auch messbarer Raum**.\n",
        "\n",
        "* **Ein Ma√ü $\\mu$ ist also eine auf einem Messraum definierte nicht-negative [$\\sigma$ -additive](https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t) [Mengenfunktion](https://de.m.wikipedia.org/wiki/Mengenfunktion) mit $\\mu(\\emptyset)=0$.**\n",
        "\n",
        "* Das Ma√ü $\\mu$ hei√üt Wahrscheinlichkeitsma√ü (oder normiertes Ma√ü), wenn zus√§tzlich $\\mu(\\Omega)=1$ gilt. Ein Ma√üraum $(\\Omega, \\mathcal{A}, \\mu)$ mit einem Wahrscheinlichkeitsma√ü $\\mu$ ist ein Wahrscheinlichkeitsraum.\n",
        "\n",
        "* Ist allgemeiner $\\mu(\\Omega)<\\infty,$ so nennt man $\\mu$ ein [endliches Ma√ü](https://de.m.wikipedia.org/wiki/Endliches_Ma√ü). Existieren abz√§hlbar viele Mengen, deren Ma√üendlich ist und deren Vereinigung ganz $\\Omega$ ergibt, dann wird $\\mu$ ein [$\\sigma$ -endliches](https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit) (oder $\\sigma$ -finites) Ma√ü genannt. $^{[4]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGw0qfYwrCJ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Messbare_Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtT12NSGYu4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbegVOVJGqzz"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1v7eQtS4gqu"
      },
      "source": [
        "**Z√§hlma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbVSW8Q6RdZ"
      },
      "source": [
        "$\\mu$ (A) :=\n",
        "* #A, wenn A endlich\n",
        "* ‚àû, sonst\n",
        "\n",
        "Rechenregeln in [0, ‚àû] :\n",
        "* x + ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* x * ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* 0 * ‚àû := 0 (in der Ma√ütheorie sinnvoll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzLXsG6C4Lx"
      },
      "source": [
        "**Dirac Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpN2hkk7KTX"
      },
      "source": [
        "p ‚àà X (Punktma√ü, weil es einem Punkt ein Ma√ü zuordnet, und sonst Null).\n",
        "\n",
        "Ist p innerhalb eine Menge A als Teilmenge von X, dann ist Diracma√ü = 1, sonst 0.\n",
        "\n",
        "Œ¥<sub>p</sub> (A) :=\n",
        "* 1, wenn p ‚àà A\n",
        "* 0, wenn sonst.\n",
        "\n",
        "(Œ¥ = delta klein)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJL1d3ifDByw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diracma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkR6CvL_C0uu"
      },
      "source": [
        "**Lebesgue Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAnnK16Dbua"
      },
      "source": [
        "* Das Lebesgue-Ma√ü ist das **Ma√ü im euklidischen Raum**, das geometrischen Objekten ihren Inhalt (L√§nge, Fl√§cheninhalt, Volumen, ‚Ä¶) zuordnet.\n",
        "\n",
        "* Das Lebesgue-Borel-Ma√ü auf der Borel-$\\sigma$-Algebra $\\mathcal{B}\\left(\\mathbb{R}^{n}\\right)$ (auch als Borel-Lebesgue-Ma√ü oder nur Bore/-Ma√ü bezeichnet) ist das eindeutige Ma√ü $\\lambda$ mit der Eigenschaft, dass es $n$ -dimensionalen\n",
        "Hyperrechtecken ihr $n$ -dimensionales Volumen zuordnet:\n",
        "\n",
        ">$\\lambda\\left(\\left[a_{1}, b_{1}\\right] \\times \\cdots \\times\\left[a_{n}, b_{n}\\right]\\right)=\\left(b_{1}-a_{1}\\right) \\cdots \\cdots\\left(b_{n}-a_{n}\\right)$\n",
        "\n",
        "* Das hei√üt, **es ist das Ma√ü, das Intervallen inre L√§nge zuordnet (im Eindimensionalen), Rechtecken ihren Fl√§cheninhalt zuordnet (im Zweidimensionalen), Quadern ihr Volumen zuordnet (im Dreidimensionalen) usw.**\n",
        "\n",
        "* Durch diese Bedingung wird der Inhalt $\\lambda(B)$ beliebiger Borel-Mengen eindeutig festgelegt. Die BorelMengen werden auch Borel-messbar oder $B$ -messbar genannt. **Das Borel-Ma√ü ist bewegungsinvariant und normiert, aber nicht vollst√§ndig.**\n",
        "\n",
        "* Das Lebesgue-Ma√ü ist das Haar-Ma√ü **auf der lokalkompakten topologischen Gruppe** $\\mathbb {R} ^{n}$ mit der Addition, die Existenz folgt daher bereits aus der Existenz des Haarma√ües.\n",
        "\n",
        "* Insbesondere ist es translationsinvariant, das bedeutet, dass sich das Ma√ü einer Menge unter Translation nicht √§ndert. Zudem ist es invariant unter Spiegelungen und Drehungen, also sogar bewegungsinvariant. Das Lebesgue-Ma√ü ist œÉ-endlich und regul√§r."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XsGc9DFRb"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymG9QHcCF1Hg"
      },
      "source": [
        "**Haarsches Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLkzlmPEF8cn"
      },
      "source": [
        "* Das Haarsche Ma√ü wurde eingef√ºhrt, um Ergebnisse der **Ma√ütheorie in der Gruppentheorie anwendbar zu machen**.\n",
        "\n",
        "* Beispiel: Das Lebesgue-Ma√ü $B$ auf $\\mathbb{R}^{n}$ und $\\mathbb{C}^{n}$ ist das Haarsche Ma√ü auf den additiven Gruppen $\\left(\\mathbb{R}^{n},+\\right)$ bzw. $\\left(\\mathbb{C}^{n},+\\right)$.\n",
        "\n",
        "* Es ist eine Verallgemeinerung des Lebesgue-Ma√ües. Das Lebesgue-Ma√ü ist ein Ma√ü auf dem euklidischen Raum, das unter Translationen invariant ist.\n",
        "\n",
        "* Der euklidische Raum ist eine lokalkompakte topologische Gruppe bez√ºglich der Addition. Das Haarsche Ma√ü ist f√ºr jede lokalkompakte (im Folgenden immer als hausdorffsch vorauszusetzende) topologische Gruppe definierbar, insbesondere also f√ºr jede Lie-Gruppe.\n",
        "\n",
        "* Lokalkompakte Gruppen mit ihren Haarschen Ma√üen werden in der [harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) untersucht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzktY0hJF4LR"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Haarsches_Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw99f_qvDl6i"
      },
      "source": [
        "**Haussdorf Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2DTGGZGDv5H"
      },
      "source": [
        "* Das Hausdorff-Ma√ü ist eine Verallgemeinerung des Lebesgue-Ma√ües auf nicht notwendig ganzzahlige Dimensionen. Mit seiner Hilfe l√§sst sich die Hausdorff-Dimension definieren, ein Dimensionsbegriff, mit dem beispielsweise fraktale Mengen untersucht werden k√∂nnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9M76qlDrVX"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Hausdorff-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V27J3X8qM9cN"
      },
      "source": [
        "**Jordan Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6tM4coFNIxa"
      },
      "source": [
        "* Mit dem Jordan-Ma√ü kann man beschr√§nkten Teilmengen des $\\mathbb {R} ^{n}$ einen Inhalt zuordnen und erh√§lt einen Integralbegriff, der dem [riemannschen Integralbegriff](https://de.m.wikipedia.org/wiki/Riemannsches_Integral) analog ist.\n",
        "\n",
        "* Das Jordan-Ma√ü ist kein Ma√ü im Sinne der Ma√ütheorie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcW8Znq_NB8I"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Jordan-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ef5-QUNE22"
      },
      "source": [
        "Siehe auch: https://de.m.wikipedia.org/wiki/Inhalt_(Ma√ütheorie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomYI_xzCurv"
      },
      "source": [
        "**Radon Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGB7cfQ9VeD"
      },
      "source": [
        "Es handelt sich um ein spezielles Ma√ü auf der Borelschen œÉ-Algebra eines Hausdorff-Raums mit bestimmten Regularit√§tseigenschaften. Der Begriff wird in der Fachliteratur jedoch nicht einheitlich verwendet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AnX69MC1wo"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Radonma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0PG2uLQIPM"
      },
      "source": [
        "**Measurable Space (Messbarer Raum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraYrSFNKL4d"
      },
      "source": [
        "Das Tupel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge (universe / Set) genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine $\\sigma$ -Algebra √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "Zusammen mit einem Ma√ü $\\mu$ wird aus einem messbaren Raum ein Ma√üraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrI52A0M-lGw"
      },
      "source": [
        "* **A measurable space or Borel space is a basic object in measure theory**. It consists of a set and a œÉ-algebra, which defines the subsets that will be measured.\n",
        "\n",
        "* Note that in contrast to a measure space, **no measure is needed for a measurable space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCr-8c-d62V"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZG-bj9KyHs"
      },
      "source": [
        "**Example**\n",
        "\n",
        "Look at the set\n",
        "\n",
        "X=\\{1,2,3\\}\n",
        "\n",
        "One possible $\\sigma$ -algebra would be\n",
        "\n",
        "$\\mathcal{A}_{1}=\\{X, \\emptyset\\}$\n",
        "\n",
        "Then $\\left(X, \\mathcal{A}_{1}\\right)$ is a measurable space.\n",
        "\n",
        "Another possible $\\sigma$ -algebra would be the power set on $X$ :\n",
        "\n",
        "$\\mathcal{A}_{2}=\\mathcal{P}(X)$\n",
        "\n",
        "With this, a second measurable space on the set $X$ is given by $\\left(X, \\mathcal{A}_{2}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kIK_3HLLTT"
      },
      "source": [
        "**Types**\n",
        "\n",
        "* If $X$ is finite or countable infinite, **the $\\sigma$ -algebra is most of the times the power set on $X$**, so $\\mathcal{A}=\\mathcal{P}(X) .$ This leads to the measurable space $(X, \\mathcal{P}(X))$\n",
        "\n",
        "* **If $X$ is a topological space, the $\\sigma$ -algebra is most commonly the Borel $\\sigma$ -algebra $\\mathcal{B}$**, so $\\mathcal{A}=\\mathcal{B}(X)$. This leads to the measurable space $(X, \\mathcal{B}(X))$ that is common for all topological spaces such as\n",
        "the real numbers $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHhdyPQ-hgE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measurable_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0juq9WKIedRw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clc0uHou3vFE"
      },
      "source": [
        "**Riemann vs Lebesgue Integral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpeX84I3z0Q"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Integralrechnung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8ajZ5T31sN"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ütheorie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3G8b13_MBD3"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Satz_von_Fubini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGhitrlOQEHM"
      },
      "source": [
        "###### *Measure Space (Ma√üraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> One important example of a measure space is a probability space."
      ],
      "metadata": {
        "id": "pPIoNoM1u-dH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsroJGMFRuY"
      },
      "source": [
        "Ein Ma√üraum ist eine spezielle mathematische Struktur, die eine essentielle Rolle in der Ma√ütheorie und dem axiomatischen Aufbau der Stochastik spielt.\n",
        "\n",
        "Das Tripel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine [$\\sigma$ -Algebra](https://de.m.wikipedia.org/wiki/Œ£-Algebra) √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "* $\\mu$ ein [Ma√ü](https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)) ist, das auf $\\mathcal{A}$ definiert ist.\n",
        "\n",
        "Alternativ kann man einen Ma√üraum auch als einen [Messraum](https://de.m.wikipedia.org/wiki/Messraum_(Mathematik)) $(\\Omega, \\mathcal{A})$ versehen mit einem Ma√ü $\\mu$ definieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqmDCJDWy4sq"
      },
      "source": [
        "A measure space is a triple $(X, \\mathcal{A}, \\mu),$ where\n",
        "\n",
        "* $X$ is a set\n",
        "\n",
        "* $\\mathcal{A}$ is a $\\sigma$ -algebra on the set $X$\n",
        "\n",
        "* $\\boldsymbol{\\mu}$ is a measure on $(X, \\mathcal{A})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34X5FmJ4d--B"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDp68lYUypzs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measure_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIqej8jGuOt"
      },
      "source": [
        "**Beispiele**\n",
        "\n",
        "* Ein einfaches Beispiel f√ºr einen Ma√üraum sind die nat√ºrlichen Zahlen als Grundmenge $\\Omega=\\mathbb{N}$, als $\\sigma$ Algebra w√§hlt man die Potenzmenge $\\mathcal{A}=\\mathcal{P}(\\mathbb{N})$ und als Ma√ü das Diracma√ü auf der $1: \\mu=\\delta_{1}$\n",
        "\n",
        "* Ein bekannter Ma√üraum ist die Grundmenge $\\mathbb{R}$, versehen mit der borelschen $\\sigma$ -Algebra $\\mathcal{B}(\\mathbb{R})$ und dem Lebesgue-Ma√ü. **Dies ist der kanonische Ma√üraum in der Integrationstheorie.**\n",
        "\n",
        "* Die in der Wahrscheinlichkeitstheorie verwendeten **[Wahrscheinlichkeitsr√§ume](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum)** $(\\Omega, \\mathcal{A}, P)$ sind allesamt Ma√ür√§ume. Sie bestehen aus der Ergebnismenge $\\Omega$, der Ereignisalgebra $\\mathcal{A}$ und dem Wahrscheinlichkeitsma√ü $P$ (= synonym 'Wahrscheinlichkeitsverteilung' oder einfach 'Verteilung'). Mit den Eigenschaften: Die drei Forderungen Normiertheit, œÉ-Additivit√§t und Werte im Intervall zwischen 0 und 1 werden auch die Kolmogorow-Axiome genannt. ([Source](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTp6RhYL8Je"
      },
      "source": [
        "**Klassen von Ma√ür√§umen**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollst√§ndiges_Ma√ü\n",
        "\n",
        "und mehr.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKgmO9kQZPo"
      },
      "source": [
        "* Verzichtet man auf Abst√§nde und Winkel, beh√§lt jedoch das Volumen geometrischer K√∂rper bei, gelangt man in das Gebiet der Ma√ütheorie. (f√ºr ein wahrscheinlichkeitsma√ü volumen = 1)\n",
        "\n",
        "* Der Ma√ütheorie gelang es, den Begriff des Volumens (oder eines anderen Ma√ües) auf eine enorm gro√üe Klasse von Mengen auszudehnen, die sogenannten messbaren Mengen. In vielen F√§llen ist es jedoch unm√∂glich, allen Mengen ein Ma√ü zuzuordnen (siehe Ma√üproblem).\n",
        "Die messbaren Mengen bilden dabei eine œÉ-Algebra. Mit Hilfen von messbaren Mengen lassen sich messbare Funktionen zwischen Messr√§umen definieren.\n",
        "\n",
        "* Um einen topologischen Raum zu einem Messraum zu machen, muss man ihn mit einer œÉ-Algebra ausstatten. Die œÉ-Algebra der Borel-Mengen ist die verbreitetste, aber nicht die einzige Wahl.\n",
        "\n",
        "* Ein Ma√üraum ist ein Messraum, der mit einem Ma√ü versehen ist. Ein euklidischer Raum mit dem [Lebesgue-Ma√ü](https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü) ist beispielsweise ein Ma√üraum. In der Integrationstheorie werden Integrierbarkeit und Integrale messbarer Funktionen auf Ma√ür√§umen definiert. Mengen vom Ma√ü null werden Nullmengen genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0omcCEyljo"
      },
      "source": [
        "* A measure space is a basic object of measure theory, a branch of mathematics that studies generalized notions of volumes.\n",
        "\n",
        "* It contains an underlying set, the subsets of this set that are feasible for measuring (the œÉ-algebra) and the method that is used for measuring (the measure).\n",
        "\n",
        "* **One important example of a measure space is a probability space**.\n",
        "\n",
        "* A measurable space consists of the first two components without a specific measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZqzchtzaSz"
      },
      "source": [
        "A **complete measure** (or, more precisely, a complete measure space) is a measure space in which every subset of every null set is measurable (having measure zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtShSxHrzeGE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Complete_measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9JLE04AcXav"
      },
      "source": [
        "**Probability Space (Wahrscheinlichkeitsraum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2axZJitcXav"
      },
      "source": [
        "Gegeben sei\n",
        "\n",
        "* eine Menge $\\Omega$, der sogenannte **Ergebnisraum**,\n",
        "\n",
        "* eine o-Algebra $\\Sigma$ auf dieser Menge, das **Ereignissystem**.\n",
        "\n",
        "Dann hei√üt eine Abbildung (=Funktion)\n",
        "\n",
        ">$P: \\Sigma \\rightarrow[0,1]$\n",
        "\n",
        "mit den Eigenschaften\n",
        "\n",
        "*  Normiertheit: Es ist $P(\\Omega)=1$\n",
        "\n",
        "* $\\sigma$-Additivit√§t: F√ºr jede abz√§hlbare Folge von paarweise disjunkten Mengen $A_{1}, A_{2}, A_{3}, \\ldots$ aus $\\Sigma$ gilt $P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right)$\n",
        "\n",
        "ein Wahrscheinlichkeitsma√ü oder eine Wahrscheinlichkeitsverteilung.\n",
        "\n",
        "Die drei Forderungen Normiertheit, $\\sigma$-Additivit√§t und Werte im Intervall zwischen O und 1 werden auch die [Kolmogorow-Axiome](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitstheorie#Axiome_von_Kolmogorow) genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qf8jvmMcXaw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3sQ4P3KcXaw"
      },
      "source": [
        "* **One important example of a measure space is a probability space.**\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum, bei dem das Ma√ü des ganzen Raums gleich 1 ist.\n",
        "\n",
        "* In der Wahrscheinlichkeitstheorie werden f√ºr die verwendeten ma√ütheoretischen Begriffe meist eigene Bezeichnungen verwendet, die der Beschreibung von Zufallsexperimenten angepasst sind: Messbare Mengen werden Ereignisse und messbare Funktionen zwischen Wahrscheinlichkeitsr√§umen werden Zufallsvariable genannt; ihre Integrale sind Erwartungswerte.\n",
        "\n",
        "* Das Produkt einer endlichen oder unendlichen Familie von Wahrscheinlichkeitsr√§umen ist wieder ein Wahrscheinlichkeitsraum. Im Gegensatz dazu ist f√ºr allgemeine Ma√ür√§ume nur das Produkt endlich vieler R√§ume definiert. Dementsprechend gibt es zahlreiche unendlichdimensionale Wahrscheinlichkeitsma√üe, beispielsweise die Normalverteilung, aber kein unendlichdimensionales Lebesgue-Ma√ü.\n",
        "Diese R√§ume sind weniger geometrisch. Insbesondere l√§sst sich die Idee der Dimension, wie sie in der einen oder anderen Form auf alle anderen R√§ume anwendbar ist, nicht auf Messr√§ume, Ma√ür√§ume und Wahrscheinlichkeitsr√§ume anwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFqotKf4cXaw"
      },
      "source": [
        "* Es handelt sich um ein mathematisches Modell zur Beschreibung von Zufallsexperimenten. Hierbei werden die verschiedenen m√∂glichen Ausg√§nge des Experiments zu einer Menge zusammengefasst. Teilmengen dieser Ergebnismenge k√∂nnen dann unter bestimmten Voraussetzungen Zahlen zwischen 0 und 1 zugeordnet werden, die als Wahrscheinlichkeiten interpretiert werden.\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum (Œ©, Œ£, P) dessen Ma√ü P ein Wahrscheinlichkeitsma√ü ist. Im Einzelnen bedeutet das:\n",
        "\n",
        "* Œ© ist eine beliebige nichtleere Menge, genannt die Ergebnismenge. Ihre Elemente hei√üen Ergebnisse.\n",
        "\n",
        "* Œ£ (Sigma) ist eine œÉ-Algebra √ºber der Grundmenge Œ© (Omega), also eine Menge bestehend aus Teilmengen von Œ©, die Œ© enth√§lt und abgeschlossen gegen√ºber der Bildung von Komplementen und abz√§hlbaren Vereinigungen ist. Die Elemente von Œ£ hei√üen Ereignisse. Die œÉ-Algebra Œ£ selbst wird auch Ereignissystem oder Ereignisalgebra genannt.\n",
        "\n",
        "* P : Œ£ ‚Äì> [0,1] ist ein Wahrscheinlichkeitsma√ü, das hei√üt eine Mengenfunktion, die den Ereignissen Zahlen zuordnet, derart dass P(‚àÖ) = 0 ist, P (A1 ‚à™ A2 ‚à™ ‚Ä¶ ) = P(A1) + P(A2) + ‚Ä¶ f√ºr paarweise disjunkte (d. h. sich gegenseitig ausschlie√üende) Ereignisse A1, A2, ‚Ä¶ gilt (3. Kolmogorow-Axiom) und P(Œ©) = 1 ist (2. Kolmogorow-Axiom).\n",
        "\n",
        "* Der Messraum (Œ©, Œ£) wird auch Ereignisraum genannt. Ein Wahrscheinlichkeitsraum ist also ein Ereignisraum, auf dem zus√§tzlich ein Wahrscheinlichkeitsma√ü gegeben ist.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4A4TeH7SewU"
      },
      "source": [
        "###### *Measurable Space (Messraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXZ_kJWGTey"
      },
      "source": [
        "* [Messraum](https://en.wikipedia.org/wiki/Measurable_space) und Ma√üraum sind spezielle œÉ-Algebren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErN__bggDh9m"
      },
      "source": [
        "###### *Field of Sets (Algebra oder Mengensystem)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIdFJLmUG_C"
      },
      "source": [
        "**Mengensystem** oder **Mengenalgebra** oder **Fields of Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3axOdZgGNHd"
      },
      "source": [
        "* **A field of sets is a pair $\\langle X, \\mathcal{F}\\rangle$ where $X$ is a set and $\\mathcal{F}$ is an algebra over $X$**\n",
        "\n",
        "* i.e., a subset of the power set of $X$, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying $X \\in \\mathcal{F}$.\n",
        "\n",
        "* In other words, $\\mathcal{F}$ forms\n",
        "a subalgebra of the power set Boolean algebra of $X$ (with the same identity element $X \\in \\mathcal{F}$ ).\n",
        "(Many authors refer to $\\mathcal{F}$ itself as a field of sets.) Elements of $X$ are called points and those of $\\mathcal{F}$\n",
        "are called complexes and are said to be the admissible sets of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct9dwFuXEuFS"
      },
      "source": [
        "Œ© (Omega) sei eine beliebige (Grund-)Menge. Ein System $\\mathcal{A}$ (oder $\\mathcal{F}$) von Teilmengen von Œ© hei√üt eine Mengenalgebra oder Algebra √ºber Œ©, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "1. $\\mathcal{A} \\neq \\emptyset$ ( $\\mathcal{A}$ ist nicht leer)\n",
        "\n",
        "2. $A, B \\in \\mathcal{A} \\Rightarrow A \\cup B \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Vereinigung)\n",
        "\n",
        "3. $A \\in \\mathcal{A} \\Rightarrow A^{\\mathrm{c}} \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Komplementbildung $\\left.A^{c}=\\Omega \\backslash A\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ksvdizjJS-"
      },
      "source": [
        "* In der Mathematik ist (Mengen-)Algebra ein Grundbegriff der Ma√ütheorie. Er beschreibt ein nicht-leeres Mengensystem, das vereinigungs- und komplementstabil ist.\n",
        "\n",
        "* A field of sets is a **pair ‚ü®X,F‚ü©** where X is a set and F is an algebra over X i.e., a subset of the power set of X, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying X ‚àà F.\n",
        "\n",
        "* In other words, F forms a subalgebra of the power set Boolean algebra of X (with the same identity element X ‚àà F). (Many authors refer to F itself as a field of sets.)\n",
        "\n",
        "* **Elements of X are called points and those of F are called complexes and are said to be the admissible sets of X.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRO-7ajId2V"
      },
      "source": [
        "* For arbitrary set $Y$, its power set (Potenzmenge) $2^{Y}$ (or, somewhat pedantically, the pair $\\left\\langle Y, 2^{Y}\\right\\rangle$ of this set and its power set) is a field of sets.\n",
        "\n",
        "* If $Y$ is finite (namely, $n$ -element), then $2^{Y}$ is finite (namely, $2^{n}$ element).\n",
        "\n",
        "* It appears that every finite field of sets (it means, $\\langle X, \\mathcal{F}\\rangle$ with $\\mathcal{F}$ finite, while $X$ may be infinite) admits a representation of the form $\\left\\langle Y, 2^{Y}\\right\\rangle$ with finite $Y ;$ it means a function $f: X \\rightarrow Y$ that establishes a one-to-one correspondence between $\\mathcal{F}$ and $2^{Y}$ via inverse image:\n",
        "$S=f^{-1}[B]=\\{x \\in X \\mid f(x) \\in B\\}$ where $S \\in \\mathcal{F}$ and $B \\in 2^{Y}$ (that is, $B \\subset Y$ ).\n",
        "\n",
        "* One notable consequence: the number of complexes, if finite, is always of the form $2^{n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkRhwrZHSGp"
      },
      "source": [
        "**Beispiele f√ºr Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFXRKkuHYNd"
      },
      "source": [
        "* F√ºr jede beliebige Menge $\\Omega$ ist $\\{\\emptyset, \\Omega\\}$ die kleinste und die Potenzmenge $\\mathcal{P}(\\Omega)$ die gr√∂√ütm√∂gliche Mengenalgebra.\n",
        "* Jede $\\sigma$ -Algebra ist eine Mengenalgebra.\n",
        "* F√ºr jede Menge $\\Omega$ ist das Mengensystem $\\mathcal{A}=\\left\\{A \\subseteq \\Omega \\mid A \\text { endlich oder } A^{c} \\text { endlich }\\right\\}$ eine Mengenalgebra. Wenn $\\Omega$ unendich ist, dann ist $\\mathcal{A}$ keine $\\sigma$ -Algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded5mK7pcYoX"
      },
      "source": [
        "**Separative and compact fields of sets: towards Stone duality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7fxFxMgcbMU"
      },
      "source": [
        "* A field of sets is called **separative (or differentiated)** if and only if for every pair of distinct points there is a complex containing one and not the other.\n",
        "\n",
        "* A field of sets is called **compact** if and only if for every proper filter over X the intersection of all the complexes contained in the filter is non-empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL_pBvidGa5"
      },
      "source": [
        "Given a field of sets $\\mathbf{X}=\\langle X, \\mathcal{F}\\rangle$ the complexes form a base for a topology. We denote by $T(\\mathbf{X})$ the corresponding topological space, $\\langle X, \\mathcal{T}\\rangle$ where $\\mathcal{T}$ is the topology formed by taking arbitrary unions of complexes. Then\n",
        "\n",
        "1. $T(\\mathbf{X})$ is always a [zero-dimensional space](https://en.m.wikipedia.org/wiki/Zero-dimensional_space)\n",
        "\n",
        "2. $T(\\mathbf{X})$ is a [Hausdorff space](https://en.m.wikipedia.org/wiki/Hausdorff_space) if and only if $\\mathbf{X}$ is separative.\n",
        "\n",
        "3. $T(\\mathbf{X})$ is a compact space with compact open sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is compact.\n",
        "\n",
        "4. $T(\\mathbf{X})$ is a Boolean space with clopen sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is both separative and compact (in which case it is described as being descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I7YepgQlABC"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Field_of_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8T8hpoiWFR"
      },
      "source": [
        "###### *œÉ-algebra (Sigma Algebra)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpgaCc5SSQx"
      },
      "source": [
        "**œÉ-algebra (Sigma Algebra)**\n",
        "\n",
        "* https://mathepedia.de/Sigma-Algebren.html\n",
        "\n",
        "* Sei $\\Omega \\neq \\emptyset$ eine Menge, $\\mathfrak{P}(\\Omega)$ die Potenzmenge und $\\mathcal{F} \\subseteq \\mathfrak{P}(\\Omega)$ ein Mengensystem (=field of sets).\n",
        "\n",
        "Definition\n",
        "\n",
        "$\\mathcal{F}$ hei√üt Algebra, wenn folgende Eigenschaften gelten:\n",
        "\n",
        "(1) $\\quad \\emptyset \\in \\mathcal{F}$\n",
        "\n",
        "(2) $\\quad A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}$\n",
        "\n",
        "(3) $\\quad A, B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}$\n",
        "\n",
        "$\\mathcal{F}$ hei√üt $\\sigma$ -Algebra, wenn die Punkte (1) und (2) gelten\n",
        "und zus√§tzlich\n",
        "\n",
        "(4) $\\quad A_{1}, A_{2}, \\ldots \\in \\mathcal{F} \\Rightarrow \\bigcup A_{k} \\in \\mathcal{F}$\n",
        "\n",
        "gilt.\n",
        "\n",
        "Algebren sind bez√ºglich der endlichen Vereinigung abgeschlossene Mengensysteme und $\\sigma$ -Algebren sind bez√ºglich der abz√§hlbaren Vereinigung abgeschlossene\n",
        "Mengensysteme. Wegen (1) und (2) gilt stets $\\Omega \\in \\mathcal{F}$.\n",
        "\n",
        "* If an algebra over a **set is closed under countable unions** (hence also under countable intersections), it is called a **sigma algebra** and **the corresponding field of sets (Mengensystem) is called a measurable space**. The complexes of a measurable space are called measurable sets.\n",
        "\n",
        "* * **A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it.** (Alternative: $\\langle$ Œ© , $\\mathcal{F}$, $\\mu$ $\\rangle$)\n",
        "\n",
        "* If $\\mu$ is in fact a probability measure we speak of a probability space and call its underlying measurable space a sample space.\n",
        "\n",
        "* The points of a sample space are called samples and represent potential outcomes while the measurable sets (complexes) are called events and represent properties of outcomes for which we wish to assign probabilities. (Many use the term sample space simply for the underlying set of a probability space, particularly in the case where every subset is an event.)\n",
        "\n",
        "* Measure spaces and probability spaces play a foundational role in measure theory and probability theory respectively.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebra_(Mengensystem)\n",
        "\n",
        "**Pain Point**\n",
        "\n",
        "* How to define a messbare Menge?\n",
        "\n",
        "* Wenn wir eine Menge aus den reellen Zahlen haben und mochten eine Teilbereich [a bis b] messen, dann brauchen wir einen allgemeinen Massbegriff unabhangig von der konkreten Menge. Hier kommt Masstheorie und Sigma-Algebra.\n",
        "\n",
        "**Definition**\n",
        "\n",
        "* A œÉ-algebra defines the **set of events that can be measured**, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".\n",
        "\n",
        "* Exkurs: Ergebnis vs Ereignis. Die m√∂glichen Ausg√§nge eines Zufallsexperimentes nennt man **Ergebnisse** (zB auf einem W√ºfel die Zahlen 1,2,3..). Wenn man alle m√∂glichen Ergebnisse eines Zufallsexperimentes in einer Menge zusammenfasst, erh√§lt man die **Ergebnismenge**. Sie wird √ºblicherweise mit dem Symbol Œ© (sprich Omega) bezeichnet. Beim W√ºrfeln ist Œ©= {1; 2; 3; 4; 5; 6} die Ergebnismenge. Jede Zusammenfassung von einem oder mehreren Ergebnissen eines Zufallsexperimentes in einer Menge nennt man **Ereignis** (zB auf einem W√ºrfel die Menge an geraden Zahlen {2,4,6} und ungeraden Zahlen {1,3,5}.\n",
        "\n",
        "* Eine Sigma-Algebra F ist ein System, um alle m√∂glichen **Ereignisse** (nicht Ergebnisse!) eines Zufallsexperiment zu beschreiben. Ereignisse sind an sich selbst Mengen, die man wie jede Menge vereinigen oder schneiden bzw. auch das Komplement bilden kann um so das Gegenereignis zu erhalten. Fasst man hier alle m√∂glichen Kombinationen an Ereignissen in einer Menge zusammen, bekommt man eine Menge, die wiederum Mengen als Elemente enth√§lt - eine Menge von Mengen sozusagen. Oft sagt man dazu auch einfach Mengensystem. Welche Eigenschaften ein Mengensystem genau haben muss, damit es eine Sigma-Algebra ist steht weiter unten.\n",
        "\n",
        "* Beispiel: Gl√ºcksrad mit blau, rot und gr√ºn. Dann haben wir folglich drei Ergebnisse, die wir auch abk√ºrzen k√∂nnen: Œ©={B,R,G}. Generell kann man sich schon merken: Œ© und ‚àÖ sind immer Elemente einer Sigma-Algebra. Daher haben wir hier 8 m√∂gliche Teilmengen von Œ©, die wir als Ereignis betrachten k√∂nnen und demnach als Menge in der Sigma-Algebra zusammenfassen (Potenzmenge von Omega): F ={‚àÖ, {B}, {R}, {G}, {B,R}, {B,G}, {R,G}, {B,R,G}} ([Source](https://www.massmatics.de/merkzettel/#!876:Ereignisraum_&_Sigma-Algebra)).\n",
        "\n",
        "* Bei diskreten Ergebnismengen kann man f√ºr die Sigma-Algebra immer die Potenzmenge P(Œ©)nehmen und hat demnach dann stets diesen **Ereignisraum: (Œ©,P(Œ©))**\n",
        "\n",
        "* Und f√ºr die reellen Zahlen gibt es die sogenannte **Borelsche Sigma-Algebra B**, die man dann auch in der Regel benutzt. Ist die Ergebnismenge Œ© eine Teilmenge der reellen Zahlen (oder ‚Ñù selbst), so nehmen wir die Borelsche-Sigma B und der Ereignisraum lautet (Œ©,B).\n",
        "\n",
        "* Wenn wir eine Sigma Algebra A gegeben haben, dann heisst jede Teilmenge in diesem Mengensystem (jedes Element aus dieser Sigma Algebra A) eine messbare Teilmenge (=die Mengen die wir messen wollen).\n",
        "\n",
        "* **<u>Die Elemente der Sigma Algebra sind die messbaren Teilmengen von unserer Grundmenge X</u>** (Und messbar ist der wesentliche Begriff). Das ist zB die Menge an vergangenen Trading-Events am Finanzmarkt bis zum Zeitpunkt t.\n",
        "\n",
        "* Sigma Algebra ist ein **Mengensystem von einer Teilmenge einer gegebenen Grundmenge** = der Raum, **den wir beschreiben wollen** (mit drei Eigenschaften). Die Menge einer Sigma-Algebra nennt man ‚Äû**messbare Teilmengen**‚Äú.\n",
        "\n",
        "**Eigenschaften**\n",
        "\n",
        "* **A $\\subseteq$ P(X) (=Potenzmenge) heisst Sigma Algebra, wenn gilt** (Die Mengen, die in dieser Sigma Algebra liegen, das sind jene, die folgende drei Eigenschaften erf√ºllen, und sind die, die wir messen wollen (=diesen Mengen wollen wir ein Mass zuordnen). Potenzmenge selbst soll eine Sigma Algebra sein. Sollten gewissen Eigenschaften der Potenzmenge fordern). **<u>A collection of subsets</u> A is called a œÉ-algebra on a set X if the following properties are met:**\n",
        "\n",
        "1. **A contains X (the set itself)**: $\\quad \\phi, X \\in A$ (Leere Menge (sollte L√§nge oder Volumen Null haben) und ganze Grundmenge selbst haben wir im Mengensystem / sollen messbar sein. Das ist was Sigma Algebra sagt). **Œ© ‚àà F (Ergebnismenge muss enthalten sein)**\n",
        "\n",
        "2. **If A contains a subset S, then A also contains the complement of S**: $A \\in A \\Rightarrow A^{c}:=X \\backslash A \\in A$ (Irgendein Element in der Algebra: dann sollte auch dessen Komplement im Mengensystem enthalten sein.) Hiermit ist auch Regel 1 eingeschlossen! Deswegen liegt auch die leere Menge (Gegenereignis von Œ©) in F.\n",
        "\n",
        "3. **Consider a countable collection of subsets. If each subset is included in A, then A must also contain their reunion.**: $A_{i} \\in A$ fur i $\\in N \\Rightarrow \\bigcup_{i=1}^{\\infty} A_{i} \\in A$ ((Letzter Punkt macht das Sigma aus): Abz√§hlbarkeit, abz√§hlbare Summe (A i‚Äòs aus unseren Mengensystem A): wir haben endlich viele bzw. abz√§hlbar viele, dann k√∂nnen wir die Vereinigung bilden / abziehbare Vereinigung. Die abz√§hlbare Vereinigung soll wieder in der Sigma Algebra liegen = Wenn wir L√§ngen haben, dann sollten wir die auch addieren k√∂nnen, auch wenn sich die Addition bis unendlich streckt! (blick auf messbarkeit))\n",
        "\n",
        "Having defined such a œÉ-algebra A, we call **the elements of œÉ-algebra A measurable sets** and the couple (X, A) a measurable space. An arbitrary set X can be a member of a multitude of œÉ-algebras. We denote the set of all œÉ-algebras that contain X with M(X). The **intersection of all those œÉ-algebras is called the œÉ-algebra generated by X**.\n",
        "\n",
        "**A œÉ-algebra (also œÉ-field) on a set X is a collection Œ£ of subsets of X that includes X itself, is closed under complement, and is closed under countable unions**. The definition implies that it also includes the empty subset and that it is closed under countable intersections. The pair (X, Œ£) is called a measurable space or Borel space. A œÉ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of finitely many subsets, which is a weaker condition.\n",
        "\n",
        "**Borel‚Äòsche Sigma-Algebra**\n",
        "\n",
        "* T ist ein topologischer Raum (oder ein metrischer Raum im engeren Sinn.) und X eine Menge darin. ‚ÄûOffene Mengen‚Äú.\n",
        "\n",
        "* Die Borel‚Äôsche Sigma Algebra auf topologischen Raum X ist jene kleinste Sigma Algebra, die von den offenen Mengen erzeugt wird.\n",
        "\n",
        "* B(X) := (T)\n",
        "\n",
        "**Measurable function**\n",
        "\n",
        "* **A set is measurable when it‚Äôs included in a œÉ-algebra.**\n",
        "\n",
        "* We can also extend the ‚Äúmeasurable‚Äù attribute to functions. Here‚Äôs how:\n",
        "\n",
        "* Let‚Äôs consider (X, A) and (Y, B) two measurable spaces. A function f from A to B is called measurable if every set from B comes from applying f to a set from A. Formally, we say that for any element S of B, the pre-image of S under the function f is in A.\n",
        "\n",
        "\n",
        "**Application**\n",
        "\n",
        "* The main use of œÉ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a œÉ-algebra.\n",
        "\n",
        "* This concept is important in mathematical analysis as the **foundation for Lebesgue integration**, and in probability theory, where it is **interpreted as the collection of events which can be assigned probabilities**.\n",
        "\n",
        "* Also, **in probability, œÉ-algebras are pivotal in the definition of conditional expectation**.\n",
        "\n",
        "* In statistics, (sub) œÉ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "1. **Minimum**: Sigma Algebra A enth√§lt leere Menge und Grundmenge selbst (kleinste Sigma Algebra die m√∂glich ist): A = {ùúô,X}\n",
        "2. **Maximum**: Sigma Algebra enth√§lt die Potenzmenge (beinhaltet alle Teilmengen von X): A = P(X)\n",
        "\n",
        "* If {A1, A2, A3, ‚Ä¶} is a countable partition of X then the **collection of all unions of sets in the partition** (including the empty set) is a œÉ-algebra.\n",
        "\n",
        "* A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the œÉ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. \"coarsest\") œÉ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZiS2oORjId"
      },
      "source": [
        "###### *Measure-theoretic probability theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-XQOB0Rk43"
      },
      "source": [
        "**Measure-theoretic probability theory**\n",
        "\n",
        "* The raison d'√™tre of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.\n",
        "\n",
        "* Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density.\n",
        "\n",
        "* The modern approach to probability theory solves these problems using measure theory to define the probability space:\n",
        "\n",
        "Given any set $\\Omega$ (also called sample space) and a $\\sigma$ -algebra $\\mathcal{F}$ on it, a measure $P$ defined on $\\mathcal{F}$ is\n",
        "called a probability measure if $P(\\Omega)=1$\n",
        "\n",
        "If $\\mathcal{F}$ is the Borel $\\sigma$ -algebra on the set of real numbers, then there is a unique probability measure on\n",
        "$\\mathcal{F}$ for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf.\n",
        "\n",
        "This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.\n",
        "\n",
        "The probability of a set $E$ in the $\\sigma$ -algebra $\\mathcal{F}$ is defined as\n",
        "\n",
        "$P(E)=\\int_{\\omega \\in E} \\mu_{F}(d \\omega)$\n",
        "\n",
        "where the integration is with respect to the measure $\\mu_{F}$ induced by $F$\n",
        "\n",
        "Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside R<sup>n</sup>, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.\n",
        "\n",
        "When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure.\n",
        "\n",
        "* Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes.\n",
        "\n",
        "* Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure.\n",
        "\n",
        "* If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Probability_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S4z_HeLRbtc"
      },
      "source": [
        "###### *Filtrations*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdKxs43TtLO"
      },
      "source": [
        "**Filtrations**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Filtrierung_(Wahrscheinlichkeitstheorie)\n",
        "\n",
        "* In martingale theory and the theory of stochastic processes, a **filtration is an increasing sequence of œÉ-algebras on a measurable space**.\n",
        "\n",
        "* That is, given a measurable space $(\\Omega, \\mathcal{F}),$ a filtration is a sequence of $\\sigma$ -algebras $\\left\\{\\mathcal{F}_{t}\\right\\}_{t \\geq 0}$ with $\\mathcal{F}_{t} \\subseteq \\mathcal{F}$ where each $t$ is a non-negative real number and\n",
        "\n",
        "> $t_{1} \\leq t_{2} \\Longrightarrow \\mathcal{F}_{t_{1}} \\subseteq \\mathcal{F}_{t_{2}}$\n",
        "\n",
        "* The exact range of the \"times\" $t$ will usually depend on context: the set of values for $t$ might be discrete or continuous, bounded or unbounded. For example,\n",
        "\n",
        "> $t \\in\\{0,1, \\ldots, N\\}, \\mathbb{N}_{0},[0, T]$ or $[0,+\\infty)$\n",
        "\n",
        "* **A œÉ-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".**\n",
        "\n",
        "* **Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information**.\n",
        "\n",
        "* A typical example is in mathematical finance, where a filtration represents the information available up to and including each time t, and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n",
        "\n",
        "A Filtration is a growing sequence of sigma algebras\n",
        "\n",
        "> $\\mathcal{F}_{1} \\subseteq \\mathcal{F}_{2} \\ldots \\subseteq \\mathcal{F}_{n}$\n",
        "\n",
        "When talking of martingales we need to talk of conditional expectations, and in particular conditional expectations w.r.t œÉ algebra's. So whenever we write\n",
        "\n",
        "> $E\\left[Y_{n} \\mid X_{1}, X_{2}, \\ldots, X_{n}\\right]$\n",
        "\n",
        "which can be written as\n",
        "\n",
        "> $E\\left[Y_{n+1} \\mid \\mathcal{F}_{n}\\right]$\n",
        "\n",
        "where Fùëõ is a sigma algebra that makes random variables\n",
        "\n",
        "> $X_{1}, \\ldots, X_{n}$\n",
        "\n",
        "measurable. Finally a flitration F1,‚Ä¶Fn is simply an increasing sequence of sigma algebras. That is **we are conditioning on growing amounts of information**.\n",
        "\n",
        "> **Der Begriff der Filtrierung ist unerl√§sslich, um, ausgehend vom Begriff des stochastischen Prozesses, wichtige Begriffe wie Martingale oder Stoppzeiten einzuf√ºhren.**\n",
        "\n",
        "* Als Menge $T$ wird wie bei stochastischen Prozessen meist $\\mathbb{R}_{+}$ oder $\\mathbb{N}_{0}$ gew√§hlt und $t \\in T$ als Zeitpunkt interpretiert.\n",
        "\n",
        "* **$\\sigma$ -Algebren modellieren verf√ºgbare Information**. Die Mengen der $\\sigma$ -Algebra $\\mathcal{F}_{t}$ geben zu jedem Zeitpunkt $t$ an, wie viele Informationen zur Zeit bekannt sind. F√ºr jedes Ereignis $A \\subseteq \\Omega$ bedeutet $A \\in \\mathcal{F}_{t}$ √ºbersetzt, dass zum Zeitpunkt $t$ die Frage $,$ ist $\\omega \\in A ?^{\\prime \\prime}$ eindeutig mit $,$ ja\" oder $,$ nein\" beantwortet werden kann.\n",
        "\n",
        "* Dass die Filtrierung stets aufsteigend geordnet ist, bedeutet demnach, **dass eine einmal erlangte Information nicht mehr verloren geht.**\n",
        "\n",
        "* Ist ein stochastischer Prozess $\\left(X_{t}\\right)_{t \\in T}$ an eine Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ adaptiert, bedeutet dies also, dass der Verlauf der Funktion $s \\mapsto X_{s}(\\omega)$ im Intervall $[0, t]$ zum Zeitpunkt $t$ (f√ºr beliebiges, aber unbekanntes $\\omega \\in \\Omega$ und in Hinsicht auf die durch Ereignisse $A \\in \\mathcal{F}_{s}, s \\in[0, t]$ formulierbaren Fragen bekannt ist.\n",
        "\n",
        "* Der Begriff wird aufgrund seiner Bedeutung in den meisten fortgeschrittenen Lehrb√ºchern √ºber stochastische Prozesse definiert. In einigen Lehrb√ºchern, zum Beispiel im Buch Probability von Albert N. Schirjajew, wird der Begriff aus didaktischen Gr√ºnden zun√§chst umfassend f√ºr Prozesse mit diskreten\n",
        "Werten in diskreter Zeit eingef√ºhrt.\n",
        "\n",
        "**Filtration in Finance**\n",
        "\n",
        "* In a multiperiod market, information about the market scenario is revealed in stages.\n",
        "\n",
        "* Some events may be completely determined by the end of the first trading period, others by the end of the second, and others not until the termination of all trading.\n",
        "\n",
        "* This suggests the following classification of events: for each t ‚â§ T ,\n",
        "\n",
        "(1) Ft = {all events determined in the first t trading periods}.\n",
        "\n",
        "* The finite sequence (Ft)0‚â§t‚â§T is a filtration of the space Œ© of market scenarios.\n",
        "\n",
        "* In general, a filtration of a set Œ© (not necessarily finite) is defined to be a collection Ft, indexed by a time parameter t (time may be either discrete or continuous), such that\n",
        "\n",
        "(a) each Ft is a œÉ‚àíalgebra of subsets (events) of Œ©; and\n",
        "\n",
        "(b) if s<t then Fs ‚äÜFt.\n",
        "\n",
        "**Beispiel**\n",
        "\n",
        "* Betrachtet man als Beispiel einen Wahrscheinlichkeitsraum $(\\mathbb{Z}, \\mathcal{P}(\\mathbb{Z}), P)$ mit abz√§hlbarer Grundmenge $\\mathbb{Z}$ die standardm√§√üig mit der Potenzmenge als $\\sigma$ -Algebra ausgestattet ist, so w√§re eine m√∂gliche Filtrierung beispielsweise\n",
        "\n",
        "> $\\mathcal{F}_{n}:=\\sigma(\\mathcal{P}(\\{-n, \\ldots, n\\}))$\n",
        "\n",
        "* Sie modelliert die Informationen, dass man bis zum n-ten Zeitschritt sich bis zu n Schritte vom Ursprung entfernt hat und w√§re beispielsweise die passende Filtrierung f√ºr einen einfachen symmetrischen Random\n",
        "Walk.\n",
        "\n",
        "**Filtration and Stochastic Processes**\n",
        "\n",
        "*  https://almostsure.wordpress.com/2009/11/08/filtrations-and-adapted-processes/\n",
        "\n",
        "* In mathematics, a filtration $\\mathcal{F}$ is an indexed family $\\left(S_{i}\\right)_{i \\in I}$ of subobjects of a given algebraic structure $S,$ with the index $i$ running over some totally ordered index set $I$, subject to the condition\n",
        "that\n",
        "\n",
        "> if $i \\leq j$ in $I,$ then $S_{i} \\subset S_{j}$\n",
        "\n",
        "* If the index i is the time parameter of some stochastic process, then the filtration can be interpreted as **representing all historical but not future information available about the stochastic process**, with the algebraic structure S<sub>i</sub> gaining in complexity with time.\n",
        "\n",
        "* Hence, a process that is adapted to a filtration F, is also called **non-anticipating**, i.e. one that cannot see into the future.\n",
        "\n",
        "* Eine Filtrierung (auch Filtration, Filterung oder Filtern) ist in der Theorie der stochastischen Prozesse eine Familie von verschachtelten œÉ-Algebren. Sie modelliert die zu verschiedenen Zeitpunkten verf√ºgbaren Informationen zum Verlauf eines Zufallsprozesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "###### *Martingale*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFV1b0SVIDD"
      },
      "source": [
        "**Martingale**\n",
        "\n",
        "* Let (Œ©,F,P) be a probability space and (Ft)0‚â§t‚â§T or (Ft)0‚â§t<‚àû a filtration by sub- œÉ‚àíalgebras of F. An adapted sequence Xt of integrable random variables is defined to be a\n",
        "\n",
        "  * martingale if E(Xt+1|Ft) = Xt ‚àÄt (=for all t).\n",
        "  * submartingale if E(Xt+1|Ft) ‚â• Xt ‚àÄt.\n",
        "  * supermartingale if E(Xt+1|Ft) ‚â§ Xt ‚àÄt.\n",
        "\n",
        "> A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it. If $\\mu$ is in fact a probability measure we speak of a probability space and call its\n",
        "underlying measurable space a sample space. The points of a sample space are called samples\n",
        "and represent potential outcomes while the measurable sets (complexes) are called events and\n",
        "represent properties of outcomes for which we wish to assign probabilities. (Many use the term\n",
        "sample space simply for the underlying set of a probability space, particularly in the case where\n",
        "every subset is an event.) Measure spaces and probability spaces play a foundational role in\n",
        "measure theory and probability theory respectively.\n",
        "\n",
        "* In probability theory, a martingale is a sequence of random variables (for example **a stochastic process**) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value.\n",
        "\n",
        "* A **martingale is characterized by the fact that it is fair on average**. Martingales arise naturally from the modeling of fair gambling.(In a fair game of chance, **the expected value of each win is zero**)\n",
        "\n",
        "* The closely related to the martingales are the super martingales, which are stochastic processes with an average loss and submartingales, which are stochastic processes with an average gain.\n",
        "\n",
        "* The property of being a (sub- / super-) martingale does not belong to stochastic processes alone, but always to a stochastic process **in combination with filtration**. Therefore, the filtration should always be specified.\n",
        "\n",
        "A **basic definition** of a discrete-time martingale is a discrete-time stochastic process (i.e., a sequence of random variables) X1, X2, X3, ... that satisfies for any time n,\n",
        "\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|X_{n}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(X_{n+1} \\mid X_{1}, \\ldots, X_{n}\\right)=X_{n}\n",
        "\\end{array}$\n",
        "\n",
        "That is, the conditional expected value of the next observation, given all the past observations, is equal to the most recent observation.\n",
        "\n",
        "A **continuous-time martingale** with respect to the stochastic process X<sub>t</sub> is a stochastic process Y<sub>t</sub> such that for all t\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|Y_{t}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(Y_{t} \\mid\\left\\{X_{\\tau}, \\tau \\leq s\\right\\}\\right)=Y_{s} \\quad \\forall s \\leq t\n",
        "\\end{array}$\n",
        "\n",
        "This expresses the property that the conditional expectation of an observation at time t, given all the observations up to time s, is equal to the observation at time s (of course, provided that s ‚â§ t). Note that the second property implies that Yn is measurable with respect to X1 ‚Ä¶ Xn.\n",
        "\n",
        "In **full generality**, a stochastic process Y : T √ó Œ© ‚Üí S taking value in a [Banach space](https://en.m.wikipedia.org/wiki/Banach_space) S is a martingale with respect to a filtration Œ£‚àó and probability measure P if\n",
        "\n",
        "**Examples of martingales**\n",
        "\n",
        "* An unbiased random walk (in any number of dimensions)\n",
        "\n",
        "* A Wiener process Wt is a martingale, and for a Wiener process the processes W<sub>t</sub><sup>2</sup> - t and the geometric Brownian movement without drift are martingales.\n",
        "\n",
        "* Stopped Brownian motion, which can be used to model the trajectory of such games\n",
        "\n",
        "* A gambler's fortune (capital) is a martingale if all the betting games which the gambler plays are fair. To be more specific: suppose Xn is a gambler's fortune after n tosses of a fair coin, where the gambler wins USD 1 if the coin comes up heads and loses USD 1 if it comes up tails. The gambler's conditional expected fortune after the next trial, given the history, is equal to their present fortune. This sequence is thus a martingale.\n",
        "\n",
        "* If { Nt : t ‚â• 0 } is a Poisson process with intensity Œª, then the compensated Poisson process { Nt ‚àí Œªt : t ‚â• 0 } is a continuous-time martingale with [right-continuous/left-limit](https://en.m.wikipedia.org/wiki/Classification_of_discontinuities) sample paths.\n",
        "\n",
        "* ([Likelihood-ratio testing](https://en.m.wikipedia.org/wiki/Likelihood-ratio_test) in statistics) A random variable X is thought to be distributed according either to probability density f or to a different probability density g. A random sample X1, ..., Xn is taken. Let Yn be the \"likelihood ratio\":\n",
        "\n",
        "> $Y_{n}=\\prod_{i=1}^{n} \\frac{g\\left(X_{i}\\right)}{f\\left(X_{i}\\right)}$\n",
        "\n",
        "If X is actually distributed according to the density f rather than according to g, then { Yn : n = 1, 2, 3, ... } is a martingale with respect to { Xn : n = 1, 2, 3, ... }.\n",
        "\n",
        "* [**Stopped Brownian Motion**](https://en.m.wikipedia.org/wiki/Stopped_process#Brownian_motion): a stopped process is a stochastic process that is forced to assume the same value after a prescribed (possibly random) time.\n",
        "\n",
        "Martingale Property vs Markov Property\n",
        "\n",
        "* In order to formally define the concept of Brownian motion and utilise it as a basis for an asset price model, it is necessary to define the Markov and Martingale properties. These provide an intuition as to how an asset price will behave over time.\n",
        "\n",
        "* The **Markov property** states that a stochastic process essentially has \"no memory\". This means that the conditional probability distribution of the future states of the process are independent of any previous state, with the exception of the current state.\n",
        "\n",
        "* The **Martingale property** states that the future expectation of a stochastic process is equal to the current value, given all known information about the prior events.\n",
        "\n",
        "* https://www.quantstart.com/articles/The-Markov-and-Martingale-Properties/\n",
        "\n",
        "**Simulate Martingale Process**: Toss a coin. toss results (1=lose 0=win). The first step is to find the edges of the losing runs, (steps + edges). You then need to take the difference of the sizes of the steps and shove those values back into the original data. When you take a cumsum of toss2 it gives you the current length of your losing streak. Your bet is then 2 ** cumsum(toss2)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Adapted (Stochastic) Process*"
      ],
      "metadata": {
        "id": "wc37qhDWywkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPqovDyR3ll"
      },
      "source": [
        "[Adapted (Stochastic) Process](https://de.m.wikipedia.org/wiki/Adaptierter_stochastischer_Prozess)\n",
        "\n",
        "* for exmaple in Finance\n",
        "\n",
        "* The share prices of assets in a multiperiod market depend on market scenarios, but evolve in such a way that their values at any time t, being observable at time t, do not depend on the unobservable post-t futures of the scenarios.\n",
        "\n",
        "* Thus, the price process St of a traded asset is **adapted to the natural filtration** (Ft)0‚â§t‚â§T defined by (1).\n",
        "\n",
        "* In general, a sequence Xt of random variables is said to be **adapted to a filtration** (Ft)0‚â§t‚â§T if, for each t, the random variable Xt is **Ft‚àímeasurable**, that is, if all events of the form {œâ : Xt(œâ) ‚àà B}, where **B is a Borel** subset of the real numbers R, are members of the œÉ‚àíalgebra Ft."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stopping Time*"
      ],
      "metadata": {
        "id": "_j3LHUI5y6Co"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWnh7gmSp_P"
      },
      "source": [
        "**Stopping Time**\n",
        "\n",
        "* [Stoppzeit (Markov Moment)](\n",
        "https://de.m.wikipedia.org/wiki/Stoppzeit): In der Stochastik bezeichnet der Begriff der Stoppzeit eine spezielle Art von Zufallsvariablen, die auf filtrierten Wahrscheinlichkeitsr√§umen definiert werden.\n",
        "\n",
        "  * Stoppzeiten sind nicht nur von Bedeutung f√ºr die Theorie der stochastischen Prozesse (beispielsweise bei der Lokalisierung von Prozessklassen oder Untersuchungen von gestoppten Prozessen), sondern auch von praktischer Relevanz, etwa f√ºr das Problem des optimalen Aus√ºbungszeitpunkts f√ºr amerikanische Optionen.\n",
        "\n",
        "  * Eine Stoppzeit kann man als die Wartezeit interpretieren, die vergeht, bis ein bestimmtes zuf√§lliges Ereignis eintritt. Wenn wie √ºblich die Filtrierung die vorhandene Information zu verschiedenen Zeitpunkten angibt, bedeutet die obige Bedingung also, dass zu jeder Zeit bekannt sein soll, ob dieses Ereignis bereits eingetreten ist oder nicht.\n",
        "\n",
        "* Optional Stopping Theorem: Das [Optional Stopping Theorem](https://de.m.wikipedia.org/wiki/Optional_Stopping_Theorem) ist ein mathematischer Satz √ºber Martingale, eine spezielle Klasse von stochastischen Prozessen, und damit der Wahrscheinlichkeitstheorie zuzuordnen.\n",
        "\n",
        "* [Optional Sampling Theorem](https://de.m.wikipedia.org/wiki/Optional_Sampling_Theorem): Eine popul√§re Version dieses Theorems besagt, dass es bei einem fairen, sich wiederholenden Spiel keine Abbruchstrategie gibt, mit der man seinen Gesamtgewinn verbessern kann.\n",
        "\n",
        "* [Starke Markoweigenschaft](https://de.m.wikipedia.org/wiki/Starke_Markoweigenschaft)\n",
        "\n",
        "* Filtrierung von Stoppzeiten:\n",
        "\n",
        "  * Eine Stoppzeit $\\tau: \\Omega \\rightarrow[0, \\infty]$ bez√ºglich einer beliebigen Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in[0, \\infty)}$ erzeugt in Analogie zur nat√ºrlichen Filtrierung eine $\\sigma$ -Algebra, die sogenannte $\\sigma$ -Algebra der $\\tau$ -Vergangenheit\n",
        "\n",
        "  > $\\mathcal{F}_{\\tau}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t}\\right\\} \\text { mit } \\mathcal{F}_{\\infty}=\\sigma\\left(\\bigcup_{t \\in[0, \\infty)} \\mathcal{F}_{t}\\right)$\n",
        "\n",
        "  * Sei nun $\\left(\\tau_{j}\\right)_{j \\in J}$ eine geordnete Familie von Stoppzeiten mit $P\\left(\\tau_{i} \\leq \\tau_{j}\\right)=1$ f√ºr alle $i, j \\in J$ mit $i \\leq j$ dann ist die Familie $\\left(\\mathcal{F}_{\\tau_{j}}\\right)_{j \\in J}$ eine Filtrierung, diese ist beim Studium von Stoppzeiten stochastischer Prozesse von Bedeutung.\n",
        "\n",
        "  * In Analogie erzeugt man die rechtsstetige Version der Filtrierung $\\left(\\mathcal{F}_{\\tau_{j}+}\\right)_{j \\in J}$ wobei:\n",
        "\n",
        "  > $\\mathcal{F}_{r+}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t+}\\right\\} \\text { und } \\mathcal{F}_{t+}=\\bigcap_{u \\in(t, \\infty)} \\mathcal{F}_{u}$\n",
        "\n",
        "  * Es gilt immer $\\mathcal{F}_{\\tau} \\subseteq \\mathcal{F}_{r+}$\n",
        "\n",
        "* [Vorhersagbarer Prozess](https://de.m.wikipedia.org/wiki/Vorhersagbarer_Prozess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *œÉ-Algebra der œÑ-Vergangenheit*"
      ],
      "metadata": {
        "id": "qCTiyeYsy0Y0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhsdp5wBSiAq"
      },
      "source": [
        "**œÉ-Algebra der œÑ-Vergangenheit**\n",
        "\n",
        "* Die [œÉ-Algebra der œÑ-Vergangenheit](https://de.m.wikipedia.org/wiki/Œ£-Algebra_der_œÑ-Vergangenheit) ist ein **Mengensystem**, sowie ein von der Stoppzeit abgeleitetes Konzept\n",
        "\n",
        "* Die œÉ-Algebra der œÑ-Vergangenheit ist eine **spezielle œÉ-Algebra**, welche √ºber die Filtrierung und die Stoppzeit definiert wird. Sie findet beispielsweise Anwendung bei der Definition der starken Markow-Eigenschaft und dem Optional Sampling Theorem.\n",
        "\n",
        "* Sie entsteht durch Kombination einer Filtrierung mit einer Stoppzeit und findet meist Anwendung bei Aussagen √ºber gestoppte Prozesse, also stochastische Prozesse, die an einem zuf√§lligen Zeitpunkt angehalten werden. Zu diesen Aussagen geh√∂ren beispielsweise das Optional Stopping Theorem, das Optional Sampling Theorem und die Definition der starken Markow-Eigenschaft.\n",
        "\n",
        "* Gegeben sei ein Wahrscheinlichkeitsraum $(\\Omega, \\mathcal{A}, P)$ sowie eine Filtrierung $\\mathbb{F}=\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ bez√ºglich der Ober- $\\sigma$ -Algebra $\\mathcal{A}$ und eine Stoppzeit $\\tau$ bez√ºglich $\\mathbb{F}$. Dann hei√üt\n",
        "\n",
        "$\\mathcal{F}_{\\tau}=\\left\\{A \\in \\mathcal{A} \\mid A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t} \\text { f√ºr alle } t \\in T\\right\\}$\n",
        "\n",
        "die $\\sigma$ -Algebra der $\\tau$ -Vergangenheit."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*(Vector) Norms for Regularization*"
      ],
      "metadata": {
        "id": "fk2nIlNKjMY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- List of 17 similarity metrics: https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681\n",
        "- Advantages and disadvantages of 9 distance metrics: https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "Mahalanobis distance is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables."
      ],
      "metadata": {
        "id": "qLE1qfquKDn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Regression*"
      ],
      "metadata": {
        "id": "F4uWOy1kiJIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms/notebook\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Residual_sum_of_squares"
      ],
      "metadata": {
        "id": "4x7F_UT4iLrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "h8VPlSUFiaHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "z7hExSBmilec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "2LHQ82bvinIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "CuQf_KldipMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "aPzEvAhTirT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "sCrxJgguiyCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "zym317e9i6qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "mTcFGaEmi8kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "fiBHTDOMi-iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "E7EvqMAjjAgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost, loss, risk or error function*"
      ],
      "metadata": {
        "id": "F1FnAXf0DeCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost, loss, risk or error function**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/cost.jpg)\n",
        "\n",
        "* The loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set.\n",
        "\n",
        "* Also: objective function, error, cost & loss function. A loss function measures the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).\n",
        "gradient of cost function tells each weight how to change to improve overall prediction\n",
        "MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.\n",
        "\n",
        "* We want to find the local minimum of the cost function\n",
        "\n",
        "*  Quadratic cost (mean squared error MSE):\n",
        "also maximum likelihood, and sum squared error.\n",
        "Most common. Used in regression.\n",
        "Mean squared error is appropriate to regression (line/curve fitting) where the goal is to minimize the mean squared error between the training set (points) and the fitted curve.\n",
        "\n",
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n",
        "\n",
        "* In most cases, our parametric model defines a distribution [‚Ä¶] and we simply use the **principle of maximum likelihood**. This means we use the cross-entropy between the training data and the model‚Äôs predictions as the cost function.\n",
        "\n",
        "* It is important, therefore, that the function faithfully represent our design goals. If we choose a poor error function and obtain unsatisfactory results, the fault is ours for badly specifying the goal of the search.\n",
        "\n",
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "* Maximum likelihood seeks to find the optimum values for the parameters by maximizing a likelihood function derived from the training data.\n",
        "\n",
        "* Given input, the model is trying to make predictions that **match the data distribution of the target variable**. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
        "\n",
        "* One way to interpret maximum likelihood estimation is to view it as **minimizing the dissimilarity** between the empirical distribution [‚Ä¶] defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. [‚Ä¶] **Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions**.\n",
        "\n",
        "* Under appropriate conditions, the maximum likelihood estimator has the **property of consistency** [‚Ä¶], meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter.\n",
        "\n",
        "* Under the framework maximum likelihood, the error between two probability distributions is measured using cross-entropy. Under maximum likelihood estimation, we would seek a set of model weights that minimize the difference between the model‚Äôs predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
        "\n",
        "When using the framework of maximum likelihood estimation, we will implement a cross-entropy loss function, which often in practice means:\n",
        "* a **cross-entropy** loss function for classification problems and\n",
        "* a **mean squared error** loss function for regression problems.\n",
        "\n",
        "* Under the framework of maximum likelihood estimation and assuming a **Gaussian distribution for the target variable**, mean squared error can be considered the cross-entropy between the distribution of the model predictions and the distribution of the target variable.\n",
        "\n",
        "* Many authors use the term ‚Äúcross-entropy‚Äù to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer.\n",
        "\n",
        "* Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
        "\n",
        "* For example, **mean squared error is the cross-entropy between the empirical distribution and a Gaussian model**\n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "* https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "* https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "* https://allenkunle.me/deriving-ml-cost-functions-part1\n",
        "\n",
        "\n",
        "* Properties of ideal Cost functions:\n",
        "  * smooth,\n",
        "  * continuous,\n",
        "  * symmetric (but i.e. Non-symmetric losses: e.g., for spam classification)\n",
        "  * differentiable\n",
        "\n",
        "**Similarity learning** is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, identity of indiscernibles, symmetry and subadditivity (or the triangle inequality). **In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.**\n",
        "\n",
        "> $\\min _{W}\\left\\{L(W):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(W ; x_{i}, y_{i}\\right)+\\lambda r(W)\\right\\}$\n",
        "\n",
        "**Similarity Learning & Distance Metric Learning**\n",
        "\n",
        "* √Ñhnlichkeitsma√üe werden f√ºr nominal oder ordinal skalierte Variablen genutzt\n",
        "\n",
        "* Distanzma√üe werden f√ºr metrisch skalierte Variablen (d. h. f√ºr Intervall- und Verh√§ltnisskala) genutzt.\n",
        "\n",
        "Complete list of [Loss / Cost Functions in TF](https://www.tensorflow.org/api_docs/python/tf/keras/losses/)"
      ],
      "metadata": {
        "id": "w3-c_-luhCKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Regression & Forecasting (mostly distance-based)*"
      ],
      "metadata": {
        "id": "DjYXE5LZhFXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions that belong to the category \"distance-based\" are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.\n",
        "\n",
        "> Great overview: http://juliaml.github.io/LossFunctions.jl/stable/losses/distance/\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/regression_loss.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm_tzCTnhHNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Linear) Least Squares**\n",
        "\n",
        "* Least Squares: Deren Parameter werden so bestimmt, dass die Summe der Abweichungsquadrate e der Beobachtungen y von den Werten der Funktion minimiert wird.\n",
        "\n",
        "* Da die Kleinste-Quadrate-Sch√§tzung die Residuenquadratsumme minimiert, ist es dasjenige Sch√§tzverfahren, welches das [Bestimmtheitsma√ü](https://de.wikipedia.org/wiki/Bestimmtheitsma√ü) maximiert.\n",
        "\n",
        "* Das Bestimmtheitsma√ü der Regression, auch empirisches Bestimmtheitsma√ü, ist eine dimensionslose Ma√üzahl die den Anteil der Variabilit√§t in den Messwerten der abh√§ngigen Variablen ausdr√ºckt, der durch das lineare Modell ‚Äûerkl√§rt‚Äú wird. Mithilfe dieser Definition k√∂nnen die Extremwerte f√ºr das Bestimmtheitsma√ü aufgezeigt werden. F√ºr das\n",
        "Bestimmtheitsma√ü gilt, dass es umso nƒÉher am Wert 1 ist, je kleiner die Residuenquadratsumme ist. Es wird maximal gleich 1 wenn $\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=0$ ist, also alle Residuen null sind. In diesem Fall ist die Anpassung an die Daten perfekt, was bedeutet, dass f√ºr jede Beobachtung $y_{i}=\\hat{y}_{i}$ ist.\n",
        "\n",
        "* [Least Squares](https://en.wikipedia.org/wiki/Least_squares) / [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) & [Linear Least Squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
      ],
      "metadata": {
        "id": "cNJH7IHehJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gauss‚ÄìMarkov theorem (BLUE)**\n",
        "\n",
        "*  states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, **if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero**.\n",
        "\n",
        "* stellt eine theoretische Rechtfertigung der Methode der kleinsten Quadrate dar\n",
        "\n",
        "* Der Satz besagt, dass in einem linearen Regressionsmodell, in dem die **St√∂rgr√∂√üen (error term) einen Erwartungswert von null und eine konstante Varianz haben sowie unkorreliert sind** (Annahmen des klassischen linearen Regressionsmodells), der Kleinste-Quadrate-Sch√§tzer ‚Äì vorausgesetzt er existiert ‚Äì ein bester linearer erwartungstreuer Sch√§tzer ist (englisch Best Linear Unbiased Estimator, kurz: BLUE).\n",
        "\n",
        "* Hierbei bedeutet der ‚Äûbeste‚Äú, dass er ‚Äì innerhalb der Klasse der linearen erwartungstreuen Sch√§tzer ‚Äì die ‚Äûkleinste‚Äú Kovarianzmatrix aufweist und somit minimalvariant ist. Die St√∂rgr√∂√üen m√ºssen nicht notwendigerweise normalverteilt sein. Sie m√ºssen im Fall der verallgemeinerten Kleinste-Quadrate-Sch√§tzung auch nicht unabh√§ngig und identisch verteilt sein.\n",
        "\n",
        "The Gauss-Markov assumptions concern the set of error random variables, $\\varepsilon_{i}:$\n",
        "\n",
        "1. They have mean zero: $\\mathrm{E}\\left[\\varepsilon_{i}\\right]=0$\n",
        "\n",
        "2. They are homoscedastic, that is all have the same finite variance: $\\operatorname{Var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}<\\infty$ for all $i$,\n",
        "3. Distinct error terms are uncorrelated: $\\operatorname{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0, \\forall i \\neq j$.\n",
        "\n",
        "A linear estimator of $\\beta_{j}$ is a linear combination $\\widehat{\\beta}_{j}=c_{1 j} y_{1}+\\cdots+c_{n j} y_{n}$\n",
        "\n",
        "* The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n",
        "\n",
        "* The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the [James‚ÄìStein estimator](https://en.wikipedia.org/wiki/James‚ÄìStein_estimator) (which also drops linearity), [ridge regression(Tikhonov_regularization)](https://en.wikipedia.org/wiki/Tikhonov_regularization), or simply any [degenerate estimator](https://en.wikipedia.org/wiki/Degenerate_distribution).\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem"
      ],
      "metadata": {
        "id": "wqLJa7izhK3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "* Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.\n",
        "\n",
        "* ‚ÄúOrdinary Least Squares‚Äù (OLS) method is used to find the best line intercept (b) and the slope (m). [in y = mx + b, m is the slope and b the intercept]\n",
        "\n",
        "\n",
        "> $m=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
        "\n",
        "> $b=\\bar{y}-m * \\bar{x}$\n",
        "\n",
        "* In other words ‚Üí with OLS Linear Regression the goal is to find the line (or hyperplane) that minimizes the vertical offsets. We define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
        "\n",
        "* OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
        "\n",
        "* The OLS method minimizes the sum of squared residuals, and leads to a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression) for the estimated value of the unknown parameter vector Œ≤.\n",
        "\n",
        "* It is important to point out though that OLS method will work for a univariate dataset (ie., single independent variables and single dependent variables). Multivariate dataset contains a single independent variables set and multiple dependent variables sets, requiring a machine learning algorithm called ‚ÄúGradient Descent‚Äù.\n",
        "\n",
        "* [Wiki](https://en.wikipedia.org/wiki/Ordinary_least_squares) & [Medium](https://medium.com/@jorgesleonel/linear-regression-307937441a8b)"
      ],
      "metadata": {
        "id": "YvnQ8gFphMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Least Squares (WLS)**\n",
        "\n",
        "* are used when heteroscedasticity is present in the error terms of the model.\n",
        "* https://en.wikipedia.org/wiki/Weighted_least_squares"
      ],
      "metadata": {
        "id": "0xWob81xhO3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized Least Squares (GLS)**\n",
        "\n",
        "* is an extension of the OLS method, that **allows efficient estimation of Œ≤ when either heteroscedasticity, or correlations, or both are present among the error terms of the model**, as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
        "\n",
        "* To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ith case is inversely proportional to var(Œµi). This special case of GLS is called \"weighted least squares\".\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Generalized_least_squares"
      ],
      "metadata": {
        "id": "L1U8vBsUhRcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE, SAE & SSE**\n",
        "\n",
        "**Sum of Errors (SE)** the difference in the predicted value and the actual value.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma(\\hat{Y}-Y)$\n",
        "\n",
        "Errors terms cancel each other out.\n",
        "\n",
        "**Sum of Absolute Errors (SAE)** takes the absolute values of the errors for all iterations.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma (|\\hat{Y}-Y|)$\n",
        "\n",
        "This loss function is not differentiable at 0.\n",
        "\n",
        "**Sum of Squared Errors (SSE)** is differentiable at all points and gives non-negative errors. But you could argue that why cannot we go for higher orders like 4th order or so. Then what if we consider to take 4th order loss function, which would look like:\n",
        "\n",
        "$\\mathbf{L}=\\left[\\Sigma(\\hat{Y}-Y)^{2}\\right]$\n",
        "\n",
        "The gradient of the loss function will vanish at minima & maxima. And the error will grow with the sample size.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/sumoferrors.png)\n",
        "\n",
        "* Minimizing Sum of Squared Errors / SSE ([wiki](https://de.m.wikipedia.org/wiki/Residuenquadratsumme) and [medium](https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511)).  We can think of the SSE loss as the (unscaled) variance of the model errors.\n",
        "* Therefore **minimizing the SEE loss is equivalent to minimizing the variance of the model residuals**. For this reason, the sum of squares loss is often referred to as the Residual Sum of Squares error (RSS) for linear models. We can think of minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.\n",
        "* Ideal when distribution of residuals in normal: the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem) states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the [best unbiased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) for the parameters .\n",
        "* Problem: Because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss\n"
      ],
      "metadata": {
        "id": "QegFiqDEhTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE (L2) & RMSE (Squared Euclidean Distance)**\n",
        "\n",
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis.\n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/3d-function-2.svg/566px-3d-function-2.svg.png)\n",
        "\n",
        "*A paraboloid, the graph of squared Euclidean distance from the origin*\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/3d-function-5.svg/566px-3d-function-5.svg.png)\n",
        "\n",
        "*A cone, the graph of Euclidean distance from the origin in the plane*"
      ],
      "metadata": {
        "id": "nPIA8NHThWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "$\\mathrm{MSE}={\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Mean Squared Error (L2 or Quadratic Loss). Error decreases as we increase our sample data as the distribution of our data becomes more and more narrower (referring to normal distribution). The more data we have, the less is the error.\n",
        "* Can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better. It is always non ‚Äì negative and values close to zero are better. The MSE is the second moment of the error (about the origin) and thus incorporates both the variance of the estimator and its bias.\n",
        "* Problem: Sensitive to outliers and the order of loss is more than that of the data. As my data is of order 1 and the loss function, MSE has an order of 2 (squared). So we cannot directly correlate data with the error.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate)\n",
        "\n",
        "**Mean Squared Logarithmic Error (MSLR)**\n",
        "\n",
        "* Mean Squared Logarithmic Error\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\n",
        "\n",
        "**RMSE** (Root-Mean-Square Error)\n",
        "\n",
        "$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Root-Mean-Square Error is the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
        "* The **RMSE is directly interpretable in terms of measurement units**, and so is a better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit. Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable\n",
        "* https://www.sciencedirect.com/science/article/pii/S096014811831231X\n",
        "* The **RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian**.\n",
        "https://www.geosci-model-dev-discuss.net/7/C473/2014/gmdd-7-C473-2014-supplement.pdf\n",
        "* When both metrics are calculated, the MAE tends to be much smaller than the RMSE because the RMSE penalizes large errors while the MAE gives the same weight to all errors.\n",
        "* They summarized that the **RMSE tends to become increasingly larger than the MAE** (but not necessarily in a monotonic fashion) as the distribution of error magnitudes becomes more variable. The RMSE tends to 1 grow larger than the MAE with n2 since its lower limit is fixed at the MAE and its upper 11 limit (n2 ¬∑ MAE) increases with n2 .\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Root-mean-square_deviation) & [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)"
      ],
      "metadata": {
        "id": "VTqsbCowhYav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE (L1) & MAPE**\n",
        "\n",
        "$\\mathrm{MAE}=\\frac{1}{n} \\sum_{j=1}^{n}\\left|y_{j}-\\hat{y}_{j}\\right|$\n",
        "\n",
        "* If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. MBE can convey useful information, but should be interpreted cautiously because positive and negative errors will cancel out.\n",
        "\n",
        "* Mean Absolute Error (L1 Loss)\n",
        "* Computes the mean of absolute difference between labels and predictions\n",
        "* measures the average magnitude of the errors in a set of predictions, without considering their direction. It‚Äôs the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "* On some regression problems, the **distribution of the target variable may be mostly Gaussian, but may have outliers**, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "* Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Extremwerte als Ausrei√üer mit geringerem Einfluss auf das Modell ansehen: MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously receive unrealistically huge negative/positive values in our training environment, but not our testing environment).\n"
      ],
      "metadata": {
        "id": "eX6fU9qohaM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE vs MSE**\n",
        "\n",
        "* One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values.\n",
        "\n",
        "* This isn‚Äôt good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate.\n",
        "\n",
        "* The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/mae_vs_mse.PNG)"
      ],
      "metadata": {
        "id": "F71Ij_KchcFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "$\\mathrm{M}=\\frac{1}{n} \\sum_{t=1}^{n}\\left|\\frac{A_{t}-F_{t}}{A_{t}}\\right|$\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is a statistical measure of **how accurate a forecast** system is.\n",
        "\n",
        "* It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values. Where At is the actual value and Ft is the forecast value.\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is the most common measure used to forecast error, and works best if there are no extremes to the data (and no zeros).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "MZzrxESQheD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
        "\n",
        "* There are 3 different definitions of sMAPE. Two of them are below:\n",
        "\n",
        "$\\operatorname{SMAPE}=\\frac{100 \\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(\\left|A_{t}\\right|+\\left|F_{t}\\right|\\right) / 2}$\n",
        "\n",
        "* Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
        "\n",
        "* At is the actual value and Ft is the forecast value\n",
        "\n",
        "* The absolute‚ÄÖdifference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n",
        "\n",
        "* Armstrong's original definition is as follows:\n",
        "\n",
        "$\\mathrm{SMAPE (old)}=\\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(A_{t}+F_{t}\\right) / 2}$\n",
        "\n",
        "* The problem is that it can be negative (if ${\\displaystyle A_{t}+F_{t}<0}$) or even undefined (if ${\\displaystyle A_{t}+F_{t}=0}$). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator.\n",
        "\n",
        "* In contrast to the mean‚ÄÖabsolute‚ÄÖpercentage‚ÄÖerror, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator)\n",
        "\n",
        "* One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the second SMAPE formula:\n",
        "\n",
        "  * Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76%\n",
        "\n",
        "  * Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) & [Wiki2](https://wiki2.org/en/Symmetric_mean_absolute_percentage_error) & [other](https://www.brightworkresearch.com/the-problem-with-using-smape-for-forecast-error-measurement/)"
      ],
      "metadata": {
        "id": "QsaM9Z0whfuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean absolute scaled error (MASE)**\n",
        "\n",
        "* mean absolute scaled error (MASE) is a measure of the accuracy of forecasts.\n",
        "\n",
        "*  It is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast. It was proposed in 2005.\n",
        "\n",
        "* The mean absolute scaled error has the following desirable propertie: [Wiki](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error)"
      ],
      "metadata": {
        "id": "6HSFPTdVhhon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss (Smooth Mean Absolute Error)**\n",
        "\n",
        "* TLDR: will better find a minimum than L1, but less exposed to outliers than L2. However one has to tune the hyperparameter delta. The larger (3+), the more it is L2, the smaller (1), the more it is L1.\n",
        "\n",
        "* The Huber loss **combines the best properties of MSE and MAE** (Mean Absolute Error). It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter.\n",
        "\n",
        "* It's **less sensitive to outliers** in data than the squared error loss. It‚Äôs **also differentiable at 0**. It‚Äôs basically absolute error, which becomes quadratic when error is small.  How small that error has to be to make it quadratic depends on a hyperparameter ùõø.\n",
        "\n",
        "* Once differentiable.\n",
        "\n",
        "$L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
        "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
        "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
        "\\end{array}\\right.$\n",
        "\n",
        "* **Huber loss approaches MSE when ùõø ~ 0 and MAE when ùõø ~ ‚àû**\n",
        "\n",
        "* The choice of delta is critical because it determines what you‚Äôre willing to consider as an outlier. Residuals larger than delta are minimized with L1 (which is less sensitive to large outliers), while residuals smaller than delta are minimized ‚Äúappropriately‚Äù with L2.\n",
        "\n",
        "* One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n",
        "Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it‚Äôs more robust to outliers than MSE. Therefore, **it combines good properties from both MSE and MAE**.\n",
        "\n",
        "* However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Huber_loss) * [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
        "\n",
        "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
        "\n",
        "* The biggest problem with using MAE to train neural networks is the constant large gradient, which may cause the minimum point to be missed when the gradient descent is about to end. For MSE, the gradient will decrease as the loss decreases, making the result more accurate.\n",
        "\n",
        "* In this case, Huber loss is very useful. It will fall near the minimum value due to the decreasing gradient. It is more robust to outliers than MSE. Therefore, Huber loss combines the advantages of MSE and MAE. However, the problem with Huber loss is that we may need to constantly adjust the hyperparameters\n",
        "\n",
        "* https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "* When you compare this statement with the benefits and disbenefits of both the MAE and the MSE, you‚Äôll gain some insights about how to adapt this delta parameter:\n",
        "\n",
        "* **If your dataset contains large outliers**, it‚Äôs likely that your model will not be able to predict them correctly at once. In fact, it might take quite some time for it to recognize these, if it can do so at all. This results in large errors between predicted values and actual targets, because they‚Äôre outliers. Since MSE squares errors, large outliers will distort your loss value significantly. If outliers are present, you likely don‚Äôt want to use MSE. Huber loss will still be useful, but you‚Äôll have to use small values for ùõø.\n",
        "\n",
        "* If it does not contain many outliers, it‚Äôs likely that it will generate quite accurate predictions from the start ‚Äì or at least, from some epochs after starting the training process. In this case, you may observe that the errors are very small overall. Then, one can argue, it may be worthwhile to let the largest small errors contribute more significantly to the error than the smaller ones. In this case, MSE is actually useful; hence, with Huber loss, you‚Äôll likely want to use quite large values for ùõø.\n",
        "\n",
        "* If you don‚Äôt know, you can always start somewhere in between ‚Äì for example, in the plot above, ùõø = 1 represented MAE quite accurately, while ùõø = 3 tends to go towards MSE already. What if you used ùõø = 1.5 instead? You may benefit from both worlds.\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/\n",
        "\n",
        "* For target = 0, the loss increases when the error increases. However, the speed with which it increases depends on this ùõø value. In fact, Grover (2019) writes about this as follows: Huber loss approaches MAE when ùõø ~ 0 and MSE when ùõø ~ ‚àû (large numbers.)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/320px-Huber_loss.svg.png)\n",
        "\n",
        "*Huber loss (green,\n",
        "Œ¥\n",
        "=\n",
        "1) and squared error loss (blue) as a function of\n",
        "y\n",
        "‚àí\n",
        "f\n",
        "(\n",
        "x\n",
        ")*\n",
        "\n",
        "![huber](https://raw.githubusercontent.com/deltorobarba/repo/master/huberloss.jpg)"
      ],
      "metadata": {
        "id": "ZBE611qIhjXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-Cosh Loss**\n",
        "\n",
        "* TLDR: Similar to MAE, will not be affected by outliers. Log-Cosh has all the points of Huber loss, and no need to set hyperparameters. Compared with Huber, Log-Cosh derivation is more complicated, requires more computation, and is not used much in deep learning.\n",
        "\n",
        "* * Log-cosh is another function used in regression tasks that‚Äôs smoother than L2 (is smoothed towards large errors (presumably caused by outliers) so that the final error score isn‚Äôt impacted thoroughly.)\n",
        "* Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. ‚ÄúLog-cosh is the logarithm of the hyperbolic cosine of the prediction error.‚Äù (Grover, 2019). Oops, that‚Äôs not intuitive but nevertheless quite important ‚Äì this is the maths behind Logcosh loss:\n",
        "\n",
        "> $\\log \\cosh (t)=\\sum_{p \\in P} \\log (\\cosh (p-t))$\n",
        "\n",
        "* Similar to Huber Loss, but twice differentiable everywhere\n",
        "* [Wiki Hyperbolic Functions](https://en.m.wikipedia.org/wiki/Hyperbolic_functions), [TF Class](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh), [Machinecurve](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "* However, Log-Cosh is second-order differentiable everywhere, which is still very useful in some machine learning models. For example, XGBoost uses Newton's method to find the best advantage. Newton's method requires solving the second derivative (Hessian). Therefore, for machine learning frameworks such as XGBoost, the second order of the loss function is differentiable. But the Log-cosh loss is not perfect, and there are still some problems. For example, if the error is large, the first step and Hessian will become fixed, which leads to the lack of split points in XGBoost.\n",
        "\n",
        "https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "![logcosh](https://raw.githubusercontent.com/deltorobarba/repo/master/logcosh.jpeg)"
      ],
      "metadata": {
        "id": "vP9h6RbchlyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile Loss (Pinball Loss)**\n",
        "\n",
        "* TLDR: for Heteroskedastizit√§t, i.e. for risk management when variance changes. Quantile Loss, you can set different quantiles to control the proportion of overestimation and underestimation in loss.\n",
        "\n",
        "* estimates conditional ‚Äúquantile‚Äù of a response variable given certain values of predictor variables\n",
        "* is an extension of MAE (**when quantile is 50th percentile, it‚Äôs MAE**)\n",
        "* Im Gegensatz zur Kleinste-Quadrate-Sch√§tzung, die den Erwartungswert der Zielgr√∂√üe sch√§tzt, ist die Quantilsregression dazu geeignet, ihre Quantile zu sch√§tzen.\n",
        "* Fitting models for many percentiles, you can estimate the entire conditional distribution. Often, the answers to important questions are found by modeling percentiles in the tails of the distribution. For that reason **quantile regression provides critical insights in financial risk management & fraud detection**.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Quantilsregression), [TF Class](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/PinballLoss) & [TF Function](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/pinball_loss)\n",
        "\n",
        "* project where predictions were subject to high uncertainty. The client required for their decision to be driven by both the predicted machine learning output and a measure of the potential prediction error. The quantile regression loss function solves this and similar problems by replacing a single value prediction by prediction intervals.\n",
        "\n",
        "* The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
        "\n",
        "* For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.\n",
        "\n",
        "* The usual regression algorithm is to fit the expected or median training data, and the quantile loss function can be used to fit different quantiles of training data by giving different quantiles.\n",
        "\n",
        "![sdd](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss.jpg)\n",
        "\n",
        "* Set different quantiles to fit different straight lines: This function is a piecewise functio., Œ≥ is the quantile coefficient. y is the true value, f(x) is the predicted value. According to the size of the predicted value and the true value, there are two cases to consider.\n",
        "\n",
        "* y> f(x) For overestimation, the predicted value is greater than the true value;\n",
        "\n",
        "* y< f(x) to underestimate, the predicted value is smaller than the real value.\n",
        "\n",
        "* Use different pass coefficients to control the weight of overestimation and underestimation in the entire loss value.\n",
        "\n",
        "* Especially when Œ≥=0.5 When the quantile loss degenerates into the mean absolute error MAE, **MAE can also be regarded as a special case of quantile loss-median loss**. The picture below is taken with different median points [0.25,0.5,0.7] Obtaining different quantile loss function curves can also be seen as MAE at 0.5.\n",
        "\n",
        "![fgfgf](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss2.jpg)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Pinball_Loss_Function.svg/320px-Pinball_Loss_Function.svg.png)\n",
        "\n",
        "*Pinball-Verlustfunktion mit\n",
        "œÑ\n",
        "=0,9. F√ºr\n",
        "Œµ\n",
        "<\n",
        "0 betr√§gt der Fehler\n",
        "‚àí\n",
        "0\n",
        ",\n",
        "1\n",
        "Œµ, f√ºr\n",
        "Œµ\n",
        "‚â•\n",
        "0 betr√§gt er\n",
        "0\n",
        ",\n",
        "9\n",
        "Œµ.*\n",
        "\n",
        "* https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/"
      ],
      "metadata": {
        "id": "geqM6xx5hnqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson Loss**\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-103abfddc312\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459\n"
      ],
      "metadata": {
        "id": "-qbix6xwhpXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "UvgTnPqOhs3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "xZOPqB8Bhuad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "6CYGi1_hhwM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "nhpMRXmPhyIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "> In machine learning, many optimization problems formulated as minimization problems wrt Kullback-Leibler divergence.\n",
        "interpreted as *information projections*, uniqueness projections proved by a generalization of the Pythagoras' theorem.\n",
        "PDF: https://Inkd.in/g9ETtTQp\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "iM9yRTVah0Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "_cMWr-Beh3wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "0kz2AVcBh5at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "R2dM3lxKh7QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "vlb98XbTh9Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "5yivnYJoh-uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Regularization*"
      ],
      "metadata": {
        "id": "G1TQJG1ufr-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*An L2-regularized version of the cost function used in SGD for NN [Source](https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db)*\n",
        "\n",
        "> $J_{\\text {regularited }}=\\underbrace{-\\frac{1}{m} \\sum_{i=1}^m\\left(y^{(i)} \\log \\left(a^{[L](i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-a^{[L](i)}\\right)\\right)}_{\\text {crossentropy cost }}+\\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum_l \\sum_l \\sum_j W_{i, j}^{[m 2}}_{\\text {L. reguatization cos }}$\n"
      ],
      "metadata": {
        "id": "OCuAdZfBf4BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "* Regularization is a technique for preventing a model from overfitting\n",
        "* Overfitting = a complicated model that gives worse predictions than a simpler model\n",
        "* Solution: e.g. preventing over-fitting by penalizing a model for having large weights (A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output)\n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called [weight regularization](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/).\n",
        "\n",
        "**Trivial Regularization Approaches**\n",
        "\n",
        "* Add more data\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data)\n",
        "* Use ensemble models\n",
        "\n",
        "**Particular Regularization Techniques**\n",
        "* Weight regularization\n",
        "* Vectornorm (L1, L2, or Elastic Net): Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.\n",
        "* Dropout\n",
        "* Jitter (add noise)\n",
        "* Batch size\n",
        "* Early stopping (this is not a formal regularization method, but can effectively limit overfitting).\n",
        "\n",
        "**Overfitting: Consider Variance-Bias-Tradeoff**: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)\n",
        "\n",
        "**Benefits of regularization from a mathematical optimization point of view**\n",
        "\n",
        "* Minimize a cost function. Neural networks are non-convex cost functions. Numerical optimization methods (gradient descent) can easily get stuck in local minima (stationary points)\n",
        "\n",
        "* Regularization can be used as a way of ‚Äöconvexifying‚Äò a non-convex cost function.\n",
        "\n",
        "* The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible.\n",
        "\n",
        "**Theoretical Foundation**\n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ],
      "metadata": {
        "id": "cjZzhH2ef5uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 (Lasso) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds ‚Äúabsolute value of magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
        "* Learn more on [Google Course](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda): Regularization for Simplicity: Lambda\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "\n",
        "$d_{1} \\equiv d_{\\mathrm{SAD}}:(x, y) \\mapsto\\|x-y\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|$\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature‚Äôs coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn‚Äôt easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**:\n",
        "  * if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero.\n",
        "  * very useful when you want to understand exactly which features are contributing to a decision.\n",
        "  * if you can ignore the ouliers in your dataset or you need them to be there.\n",
        "  * use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features¬† at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUERgs6Hf8C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 (Ridge) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds ‚Äúsquared magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it‚Äôs important how lambda is chosen.\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply ‚Äúweight decay,‚Äù with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn‚Äôt give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge.\n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ],
      "metadata": {
        "id": "sfAtZ0ekf-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Analytical Equivalence**\n",
        "\n",
        "* Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "* https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n",
        "\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed Œª2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+Œª2)."
      ],
      "metadata": {
        "id": "Dnq1CBbfgAFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "\n",
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(k√ºnstliches_neuronales_Netz)\\\n",
        "\n",
        "**Is dropout outdated?**\n",
        "\n",
        "Neural Network: ¬†Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don‚Äôt Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance.\n",
        "\n",
        "**Use Dropout along with L1/L2 Regularization?**\n",
        "\n",
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ‚Ñì 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of ‚Äúswitching off‚Äù some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network‚Äôs over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can ‚Äúwork together‚Äù or rather end up ‚Äúfighting each other‚Äù. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014)"
      ],
      "metadata": {
        "id": "dC-z_kHhgB8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jitter (Noise)**\n",
        "\n",
        "* adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise"
      ],
      "metadata": {
        "id": "DW1FugwRgDxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization & Size**\n",
        "\n",
        "* https://towardsdatascience.com/understanding-batch-normalization-for-neural-networks-1cd269786fa6\n",
        "\n",
        "* Small batches can oÔ¨Äer a regularizing eÔ¨Äect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process.\n",
        "\n",
        "* Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)\n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        "* Batch normalization is another method to regularize a (convolutional) network.\n",
        "* On top of a regularizing effect, batch normalization also gives your convolutional network a resistance to vanishing gradient during training. This can decrease training time and result in better performance.\n",
        "* Batch Normalization Combats Vanishing Gradient\n",
        "* Batch normalization replaces¬†dropout.\n",
        "* Even if you don‚Äôt need to worry about overfitting there are many benefits to implementing batch normalization. Because of this, and its regularizing effect, batch normalization has largely replaced dropout in modern convolutional architectures.\n",
        "* ‚ÄúWe presented an algorithm for constructing, training, and performing inference with batch-normalized networks. The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.‚Äù -[Ioffe and Svegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "Why use batches?\n",
        "To avoid that small datasets increase overfitting to this datasets and worsen overall accuracy. But batch size shouldnt be too big either (computation time, speed of convergence of an algorithm)\n",
        "\n",
        "* Research 1: a low batch size means a very noisy gradient (because computed on a very small subset of the dataset), and a high learning rate means noisy steps.\n",
        "https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914\n",
        "\n",
        "* Research 2: How do you choose your batch size in deep learning/SGD?¬†- An interesting concept so-called \"generalization gap\": Train longer, generalize better: closing the generalization gap in large batch training of neural networks:\n",
        "https://arxiv.org/abs/1705.08741\n",
        "\n",
        "**Covariate Shift**\n",
        "\n",
        "pending...\n",
        "\n"
      ],
      "metadata": {
        "id": "PokQre2wgFWs"
      }
    }
  ]
}