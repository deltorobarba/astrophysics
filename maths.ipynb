{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hsDRYsMxUjfS",
        "iQFAjnERdvsq",
        "iptu2LVks6If",
        "jBh1Tr-vn_4f",
        "cqdowsM5oHEP",
        "fdcELPZDs-X5",
        "1fAAZcgWoiJB",
        "cyrUknMLo8bl",
        "vmUk0nAHzPCb",
        "M0kD-s4t82ez",
        "gXZXzSSsrrUL",
        "RscthBMzOLUM",
        "7sf5tauFtttx",
        "jdJ5yNTZibYy",
        "AZ-w7Fv4O91T",
        "GuoalDqRyRyC",
        "p9cyxvqKIhGk",
        "BLIqNf8wLAzg",
        "t0wVu7UaIENk",
        "kYZ-u0aXqvlz",
        "bIoYM5RYq5BD",
        "wdOoLdM0Fp_I",
        "qmblAsDKA1Sl",
        "ncQDxd2nxffx",
        "66-byTWA1_ig",
        "UnzMHYy2j-mZ",
        "utryf_rC5-o5",
        "UMSa4Pt2dvys",
        "Op-cit2CFEiK",
        "TiqOEUrSLR_5",
        "pF0aBln0tpWG",
        "71kEC0TTtwdt",
        "CtFnUPlheUKW",
        "zpgWYl3LkxVc",
        "vSshMr2W7qVl",
        "LAH8nB-d7vrY",
        "13sqDDC5l_f4",
        "jb3vfKj27tso",
        "mUfkh74aiXp4",
        "R-pP8oZC7zxV",
        "Rjqwr4miC9_F",
        "bOsPQjh-8FP5",
        "OWqmJDTmXzvc",
        "3Ifz4t5vA6Y9",
        "rMqIXS4-cxRG",
        "f0O6pdoUYhec",
        "MkJxJltjAV7a",
        "dpys_ybfYkxU",
        "GYcJxiyZYuYO",
        "hZTQLaKGel6i",
        "KSaDCK89Y3_r",
        "v41iaGUgY8hR",
        "kSmK9R6Hbm2z",
        "cNo9Qsp7ZTQe",
        "QHclLheAYhtq",
        "Hz-PMZ5rZvs6",
        "0PltPkkfZ1Uw",
        "PkHbbQ0YHcmW",
        "oQZJ3rU97I-z",
        "-BAUgIGmirEj",
        "siEtU4-r3-Hu",
        "PdiySKKbZ63A",
        "HN2IgU6_4bCC",
        "5eNnEuegu2nL",
        "EBe-6i6B2drL",
        "KftCv8qy8vrf",
        "xioS_hE_0hkj",
        "5iVvtH_jA15X",
        "0nJWHdt9gbt3",
        "Y-1lpMAgXzD9",
        "04vVZZU1Q4rZ",
        "s8I3t063LsOf",
        "30ncah90297J",
        "YfOrrAOFvF6m",
        "-wUJznaApPv0",
        "yIoPrdtMpRd_",
        "xemut5KHpjBk",
        "cLeG1PswpSaL",
        "919bw_THpTYb",
        "h0hcBLukpUcp",
        "0p0KFlAypVZ9",
        "IbwJ8QbNvEbX",
        "0ChC-FyHZ41O",
        "PREcYcRRPEQ4",
        "0-Lu47J5MYMj",
        "UDNzWru2eKpK",
        "uOwaDiWZDTxx",
        "f4lHqJyMa24i",
        "8yH2OFqH-37c",
        "lh3x3_1K_e1m",
        "gS-QUpcFmXh1",
        "iqHduf9WDZMP",
        "jNQs-MXfBmVV",
        "Nror5UBfDJ3o",
        "Q-zfzmbO4ZOP",
        "gmN2IqZ4FDsw",
        "nrkLBUH1hb36",
        "qYW5u_0Oi7l5",
        "0JfDpLK4KBqT",
        "4SpYIJ602L38",
        "SZkKviQcpLi5",
        "F1l9_Erf5T5S",
        "6quxRLo3U9Qf",
        "32OR42wtUS3y",
        "_Syw4dL96u31",
        "fk2nIlNKjMY4",
        "F4uWOy1kiJIi",
        "h8VPlSUFiaHF",
        "sCrxJgguiyCW",
        "F1FnAXf0DeCY",
        "DjYXE5LZhFXn",
        "UvgTnPqOhs3q",
        "_cMWr-Beh3wr",
        "G1TQJG1ufr-D",
        "emoYR7MX-Q2h",
        "Nj_F9wX47fTp",
        "ZiAK4jblJhPd",
        "Wkacn855ct_Q",
        "WqR3v9EaJrvV",
        "JYooOdCtJ76C",
        "MMxCx0fZKDii",
        "LUxoBU9TKYpU",
        "vD3xBKSIK3Uj",
        "q09fihf25FPw",
        "HrtIDW9V0p96",
        "YWqVgPOwPCIJ",
        "Ym5Ax5nCOtuS",
        "QBuL0NbxO6cP",
        "M5NxmpI85zgL",
        "ur1A1K3aOl3W",
        "SN4YJmrE6HF3",
        "fVQl5cbp7iqO",
        "PsodM6QW8wsN",
        "zTjh6Dkf9Imc",
        "icJDI9EQ9gc0",
        "VkuVX0zf959A",
        "Eg2UlbUr-YCc",
        "GCJMH1Oj-vBg",
        "YgcZdTDc_TCw",
        "V44oHA2a_1hG",
        "VcEin6DXApdH",
        "457rg_0-BFtj",
        "vq-kSppvBbU7",
        "F4gfVMumBdd4",
        "pBNDxf7EBkj-",
        "pAHNvHgpB0r-",
        "r-rllUswCV7X",
        "8h7LKwE9CYwF",
        "ihDto5NgCay8",
        "BJaouQGICdBE",
        "QHq7RwnYCe8j",
        "Gdr6F1zuU4MG",
        "9AR9PIXysIa4",
        "eloiR80j3LKg",
        "sprbeM9PikBv",
        "KYzlWwuA8hXl",
        "puX57AODxaUG",
        "SqYBnfUAD1yS",
        "w5zER_lMlN46",
        "b8EVGNJmEfDo",
        "2KRiQv6BoMmA",
        "O8cFhBpbIFFd",
        "7bWObSh9Mh0N",
        "OeO_i6Z9-rLD",
        "xDUD9L1G7HIj",
        "Qa3-DIbUt0Ri",
        "fq5mO57kWnjQ",
        "OkGNbw_qosx7",
        "pHwO7VW3o75X",
        "4JijVDdYpeTw",
        "RTdYXBryoZqW",
        "fhdD4JnqZHKf",
        "hIwXOz85zxW_",
        "_DcbvY2o-JyH",
        "m066sar7fCUi",
        "U3xyzZzNOEhv",
        "82Lb_gBqUzi8",
        "y1flpzoQFTVa",
        "8lpmflh50O1X",
        "8chCbs1o_Iud",
        "8idRoUCpsoKR",
        "vUbYRNP_zXe4",
        "m0K1gPOoznzE",
        "LVOw3me6qbku",
        "ECcHaTG2nKiM",
        "P4KDiI20GSS8",
        "JGhitrlOQEHM",
        "T4A4TeH7SewU",
        "ErN__bggDh9m",
        "Vc8T8hpoiWFR",
        "IoZiS2oORjId",
        "4S4z_HeLRbtc",
        "2Aoj2PpxvDQV",
        "wc37qhDWywkD",
        "_j3LHUI5y6Co",
        "qCTiyeYsy0Y0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/maths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Mathematics ü¶ã**"
      ],
      "metadata": {
        "id": "2VU6BC9nYCGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1000.png)"
      ],
      "metadata": {
        "id": "EChUkdiQsKjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Quantum**"
      ],
      "metadata": {
        "id": "hsDRYsMxUjfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Mechanics*"
      ],
      "metadata": {
        "id": "iQFAjnERdvsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Mathematical Formulation & Postulates of Quantum Mechanics*"
      ],
      "metadata": {
        "id": "iptu2LVks6If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2023-01-scientists-quantum-harmonic-oscillator-room.html"
      ],
      "metadata": {
        "id": "6VMJ5J0fVOgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video: [Map of Quantum Physics](https://youtu.be/gAFAj3pzvAA)"
      ],
      "metadata": {
        "id": "OEoP77S6dgsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"There are (at least) two possible ways to formulate precisely (i.e. mathematically) elementary\n",
        "QM. The eldest one, historically speaking, is due to von Neumann in essence, and is formulated using the language of Hilbert spaces and the spectral theory of unbounded operators. A more recent and mature formulation was developed by several authors in the attempt to solve quantum field theory problems in mathematical physics. It relies on the theory of abstract algebras (*-algebras and C* -algebras) that are built mimicking the operator algebras defined and studied, again, by von Neumann (nowadays known as W* -algebras or von Neumann algebras), but freed from the Hilbert-space structure. The core result is the celebrated GNS theorem (after Gelfand, Najmark and Segal), that we will prove in Chap. 14. The newer formulation can be considered an extension of the former one, in a very precise sense that we shall not go into here, also by virtue of the novel physical context it introduces and by the possibility of treating physical systems with infinitely many degrees of freedom, i.e. quantum fields. In particular, this second formulation makes precise sense of the demand for locality and covariance of relativistic quantum field theories, and allows to extend quantum field theories to a curved spacetime.\"\n",
        "-Valter Moretti"
      ],
      "metadata": {
        "id": "VNk-lacMqzQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2022-11-common-misconceptions-quantum-physics.html\n",
        "\n",
        "https://www.derstandard.de/story/2000140294674/wie-die-quantenphysik-mit-unserer-vorstellung-von-realitaet-aufraeumt\n",
        "\n",
        "https://physicsworld.com/a/how-the-stern-gerlach-experiment-made-physicists-believe-in-quantum-mechanics/"
      ],
      "metadata": {
        "id": "eT_D-OhRlWtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Korrespondenzprinzip](https://de.m.wikipedia.org/wiki/Korrespondenzprinzip): Klassische Gr√∂√üen werden durch Operatoren ersetzt. Die quantenmechanische Aufenthaltswahrscheinlichkeitsdichte eines Teilchens ist proportional zum Quadrat der Wellenfunktion der Materiewelle an jener Stelle. F√ºr gro√üe Quantenzahlen geht die quantenmechanische Wahrscheinlichkeitsdichte asymptotisch in die klassische √ºber.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Korrespondenzprinzip.svg/640px-Korrespondenzprinzip.svg.png)\n"
      ],
      "metadata": {
        "id": "MSQgI9GjQAv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKKG0DSv8FhZ"
      },
      "source": [
        "*Postulate der Quantenmechanik (Kopenhagener Interpretation)*\n",
        "\n",
        "1. **Zustand**: Der Zustand eines physikalischen Systems zu einem Zeitpunkt $t_{0}$ wird durch die Angabe eines zum Zustandsraum $\\mathcal{H}$ geh√∂renden komplexen Zustandsvektors $\\left|\\psi\\left(t_{0}\\right)\\right\\rangle$ definiert. Vektoren, die sich nur um einen von 0 verschiedenen Faktor $c \\in \\mathbb{C}$ unterscheiden, beschreiben denselben Zustand. Der Zustandsraum des Systems ist ein Hilbertraum.\n",
        "\n",
        "2. **Observable**: Jede Gr√∂√üe $A$, die physikalisch , gemessen\" werden kann, ist durch einen im Zustandsraum wirkenden hermiteschen Operator $\\hat{A}$ beschrieben. Dieser Operator wird als Observable bezeichnet und hat ein reelles Spektrum mit einer vollst√§ndigen sogenannten Spektralschar, bestehend aus einem , diskreten\" Anteil mit Eigenvektoren und Eigenwerten (Punktspektrum) und aus einem Kontinuum.\n",
        "\n",
        "3. **Messresultat**: Resultat der Messung einer physikalischen Gr√∂√üe $A$ kann nur einer der Eigenwerte der entsprechenden Observablen $\\hat{A}$ sein oder bei kontinuierlichem Spektrum des Operators eine messbare Menge aus dem Kontinuum.\n",
        "\n",
        "4. **Messwahrscheinlichkeit im Fall eines diskreten nichtentarteten Spektrums**: Wenn die physikalische Gr√∂√üe $A$ an einem System im Zustand $|\\psi\\rangle$ gemessen wird, ist die Wahrscheinlichkeit $P\\left(a_{n}\\right)$, den nichtentarteten Eigenwert $a_{n}$ der entsprechenden Observable $\\hat{A}$ zu erhalten (mit dem zugeh√∂rigen Eigenvektor $\\left|u_{n}\\right\\rangle$ ) $P\\left(a_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$. Dabei seien $\\psi$ und $u_{n}$ normiert.\n",
        "\n",
        "5. **Die Zeitentwicklung des Zustandsvektors** $|\\psi(t)\\rangle$ ist gegeben durch die folgende Schr√∂dingergleichung, wobei $\\hat{H}(t)$ die der totalen Energie des Systems zugeordnete Observable ist:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t}|\\psi(t)\\rangle=\\hat{H}(t)|\\psi(t)\\rangle$\n",
        "\n",
        "http://vergil.chemistry.gatech.edu/notes/quantrev/node20.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Mathematical_formulation_of_quantum_mechanics](https://en.m.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics)\n",
        "* [C*-algebra](https://en.m.wikipedia.org/wiki/C*-algebra)\n",
        "* [Quantum_geometry](https://en.m.wikipedia.org/wiki/Quantum_geometry)\n",
        "* [Noncommutative_geometry](https://en.m.wikipedia.org/wiki/Noncommutative_geometry)\n",
        "* [Geometric_quantization](https://en.m.wikipedia.org/wiki/Geometric_quantization)"
      ],
      "metadata": {
        "id": "wGmG22vunC0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_209.png)\n"
      ],
      "metadata": {
        "id": "q8MuXUSiqRrJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE--P8idzoP-"
      },
      "source": [
        "**Video: [Crash course in density matrices](https://www.youtube.com/watch?v=1tserF6VGqI)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWlXYxSBzPS3"
      },
      "source": [
        "**Single spin one half particle, focus on spin degrees of freedom & Pauli-matrices**\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad |\\downarrow\\rangle=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvOyiyJtzRtd"
      },
      "source": [
        "**States we are used to use in quantum mechanics are called 'pure states'.**\n",
        "\n",
        "* Any state can be written as the linear combination of the up and down vectors in the z-direction\n",
        "\n",
        "> $|\\psi\\rangle=a|\\uparrow\\rangle+b|\\downarrow\\rangle$\n",
        "\n",
        "* a and b are complex numbers whose modulus squared sum to 1\n",
        "\n",
        "> $\\langle\\psi \\mid \\psi\\rangle=|a|^{2}+|b|^{2}$\n",
        "\n",
        "* a and b can be referred to as the probability amplitudes, where their modulus squared are the probabilities that the system is in a particular configuration.\n",
        "\n",
        "* If we perform an ensemble of measurements on the system, we will find that the mean or expected value, for example of the Pauli-Z matrix, will be given by:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZoOoevVzUPa"
      },
      "source": [
        "**Dynamics: if we want to study how a quantum system changes in time, we usually refer to the Schroedinger equation.**\n",
        "\n",
        "> $i \\frac{d}{d t}|\\psi(t)\\rangle=\\hat{H}|\\psi\\rangle$\n",
        "\n",
        "* the operator $\\hat{H}$ is called the Hamiltonian and it tells us about the total energy in a system, and how things in a system interact with each other.\n",
        "\n",
        "* the easiest way to solve this problem is to first solve the Eigenvalue problem, which involves finding the Eigenvectors and the Eigenvalues of the hamiltonian. We are able the different values of the Eigenvalues and Eigenvectors with the label k:\n",
        "\n",
        "> $\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* This allows us to know that the Eigenvectors, when plugged into the Schroedinger equation ...\n",
        "\n",
        "> $i \\frac{d}{d t}\\left|E_{k}\\right\\rangle=\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* ... just pick up a phase in time depending on the energy they correspond to:\n",
        "\n",
        "> $\\left|E_{k}(t)\\right\\rangle=e^{-i E_{k} t}\\left|E_{k}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitINkMkzWk-"
      },
      "source": [
        "**Now to sum more general situation with some generic state $\\psi$**\n",
        "\n",
        "* we can decompose $\\psi$ in terms of energy Eigenbasis:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* where $c_{m}$ is given by the inner product of the energy Eigenvector labelled by m and $\\psi$ itself:\n",
        "\n",
        "> $c_{m}=\\left\\langle E_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* We can then time-evolve the state by simply time-evolving the energy Eigen-Kets:\n",
        "\n",
        "> $|\\psi(t)\\rangle=\\sum_{m} c_{m} e^{-i E_{m} t}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* Tracking the expectation value of an observable is quite easy: Simply applying the state vector in time to both sides of the matrix gives the following equation:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\sum_{m, n} \\bar{c}_{n} c_{m} A_{n, m} e^{i\\left(E_{n}-E_{m}\\right) t}$\n",
        "\n",
        "* Where we have labelled the matrix entries of a by the energy Eigenbasis in the following way:\n",
        "\n",
        "> $A_{n, m}=\\left\\langle E_{n}|A| E_{m}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utZdbq6QzZD9"
      },
      "source": [
        "**Propeties of the trace of a matrix**\n",
        "\n",
        "* We have a basis of states labeled by j $|j\\rangle, j=1,2$.. N that form a complete orthonormal basis. Then I write the trace of a matrix in the following way:\n",
        "\n",
        "> $\\operatorname{Tr}(A)=\\sum_{j}\\langle j|A| j\\rangle$\n",
        "\n",
        "* the trace is a linear mapping which tells us that we can separate the trace of the sum of two matrices apart like this, and we can also pull scalar multiples outside of the trace:\n",
        "\n",
        "> $\\operatorname{Tr}(A+B)=\\operatorname{Tr} A+\\operatorname{Tr} B$ with $\\operatorname{Tr}(c A)=c \\operatorname{Tr} A$\n",
        "\n",
        "* When we take the trace of two matrices multiplied by each other, the trace is invariant under swapping the two matrices inside of the trace.\n",
        "\n",
        "> $\\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Om09eqlza4O"
      },
      "source": [
        "**Density Matrices**\n",
        "\n",
        "* Going from a pure state represented by a Ket we can introduce the density matrix which is a completely equivalent way to represent the state of a quantum system\n",
        "\n",
        "* the density matrix in this context is just the outer product of the state with itself:\n",
        "\n",
        "> $\\rho=|\\psi\\rangle\\langle\\psi|$\n",
        "\n",
        "* the expectation value is rewritten in terms of a trace, but it is mathematically equivalent to the earlier expression:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\operatorname{Tr}\\left(\\rho \\sigma^{2}\\right)=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$\n",
        "\n",
        "* the density matrix has two fundamental properties: it's trace is 1 in the context of a pure state:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1$\n",
        "\n",
        "> $\\operatorname{Tr}|\\psi\\rangle\\left\\langle\\left.\\psi|=|\\langle\\psi \\mid \\psi\\rangle\\right|^{2}=1\\right.$\n",
        "\n",
        "* and it's a positive operator:\n",
        "\n",
        "> $\\rho \\geq 0$\n",
        "\n",
        "* If I take any state vector our state space and perform the following operation, then the result is always greater than or equal to zero:\n",
        "\n",
        "> $\\langle\\phi|\\rho| \\phi\\rangle=|\\langle\\phi \\mid \\psi\\rangle|^{2} \\geq 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuLO_P1u6FBJ"
      },
      "source": [
        "**Dynamics: von Neumann equation of time evolution**\n",
        "\n",
        "* show from the Schroedinger equation we can derive the von Neumann equation of time evolution:\n",
        "\n",
        "> $i \\frac{\\partial \\rho}{\\partial t}=[\\hat{H}, \\rho]$\n",
        "\n",
        "* It features the commutation relationship between the Hamiltonian and the density matrix itself\n",
        "\n",
        "* then the expectation value for an observable in time can be rewritten in the following way:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\operatorname{Tr}(\\rho(t) A)$\n",
        "\n",
        "* where the density matrix $\\rho$ evolves with the Hamiltonian being applied to it in time:\n",
        "\n",
        "> $\\rho(t)=e^{-i \\hat{H} t} \\rho e^{i \\hat{H} t}$\n",
        "\n",
        "*  similarly re-expressed in the basis of the energy Eigenvalues\n",
        "\n",
        "> $\\rho(t)=\\sum_{m, n} \\rho_{m, n} e^{-i\\left(E_{m}-E_{n}\\right) t}\\left|E_{m}\\right\\rangle\\left\\langle E_{n}\\right|$\n",
        "\n",
        "* where $\\rho_{m,n}$ can be written in the following way, where we have re-expressed its entries in terms of the energy Eigenbasis:\n",
        "\n",
        "> $\\rho_{m, n}=\\left\\langle E_{m}|\\rho| E_{n}\\right\\rangle$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmo42wRj8v6C"
      },
      "source": [
        "**Mixed States**\n",
        "\n",
        "* what if we are unsure of what pure state our system is in? (We are somehow ignorant / unwissend to what pure state we are in)\n",
        "\n",
        "* then we can describe a statistical ensemble of pure states which we call a mixed states\n",
        "\n",
        "> $\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|$\n",
        "\n",
        "* the ensemble is written as a sum of pure states with probabilities $p_j$ and the probabilities are greater than zero and sum up to one:\n",
        "\n",
        "> $p_{j} \\geq 0 \\quad \\sum_{j} p_{j}=1$\n",
        "\n",
        "* Mixed states are also trace 1 and are positive operators:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1, \\quad \\rho \\geq 0$\n",
        "\n",
        "* Dynamics and the expectation values work in an identical way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUFROLA7BPGU"
      },
      "source": [
        "**Non-uniqueness of mixed states decomposition**\n",
        "\n",
        "* Interestingly it's not completely correct to interpret $p_j$ as the probability of being in a particular state labeled by j.\n",
        "\n",
        "* To see this consider the following example let $\\rho$ be a mixed state written in the following way:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* we say the system is in state down with probability 4/5 and in state up with 1/5.\n",
        "\n",
        "* What if I prepared two other states with the following new state vectors a and b with the following probability amplitudes of being in down or up state:\n",
        "\n",
        "> $|a\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle+\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "> $|b\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle-\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "* Then if we prepare these states with probability one-half:\n",
        "\n",
        "> $\\rho=\\frac{1}{2}|a\\rangle\\left\\langle a\\left|+\\frac{1}{2}\\right| b\\right\\rangle\\langle b|$\n",
        "\n",
        "* We see that we get the same density matrix as before working this out expanding the definitions of a and b allows us to arrive at our original density matrix:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* Therefore two different ensembles of pure states can give rise to the same mixed state and we must be careful when we interpret $p_j$ as strictly probabilities of a particular system being strictly in its associated pure state in the sum.\n",
        "\n",
        "* Finally, if you get a matrix how could you tell if it's pure or mixed? Mathematically quite simple way to check: square the matrix and take its trace. If the trace is still 1, we say that the density matrix is pure if it is less than one then we say it's mixed. This is actually independent of the time evolution.\n",
        "\n",
        "> $\\operatorname{Tr}\\left(\\rho^{2}\\right) \\leq 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket $|\\psi\\rangle$ - State Space $V$(Vector)*** *- also: Wave Function (Pure State, Postulate I)*"
      ],
      "metadata": {
        "id": "jBh1Tr-vn_4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcIVsSgCSld"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate I of Quantum Mechanics</u>: The state of a physical system is characterized by a state vector that belongs to a complex vector space $\\mathcal{V}$, called the state space of the system**\n",
        "\n",
        "* State Space Formalism: unification of Schrodinger wave mechanics and matrix mechanics (Heisenberg, Born, Jordan), both are equivalent.\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin). Then it reduces to the rules of simple matrix multiplication**. Kronecker delta\n",
        "\n",
        "* **Schrodinger wave mechanics: for continuous basis (like position)** Dirac delta function\n",
        "\n",
        "* State Space: the vector space in which quantum systems live\n",
        "\n",
        "  * Euclidean space: classical physics, 3D, real, inner product (Hilbert Space)\n",
        "\n",
        "  * State space: quantum physics, infinite dimensions, complex numbers, inner product (Hilbert Space)\n",
        "\n",
        "Video: [Dirac notation: state space and dual space](https://www.youtube.com/watch?v=hJoWM9jf0gU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlC5y6Hpsj_E"
      },
      "source": [
        "<font color=\"blue\">*Ket Algebra - Quantum State Vector*\n",
        "\n",
        "**A Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state', <u>represents</u> the wave function of a quantum system (\"Psi\")**, consisting of **probability amplitudes**. This Quantum state is a **stochastic vector**, written as a column vector.\n",
        "\n",
        "> **A Ket is technically a pure quantum state**.\n",
        "\n",
        "* Wave function notation to describe superposition of Pure States.\n",
        "\n",
        "So, you have an (orthonormal) basis with 3 vectors and coefficients, to describe a vector in space (in Hilbert space):\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$2$</font>$\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)$<font color='blue'>$+3$</font>$\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)$<font color='blue'>$+0$</font>$\\left(\\begin{array}{l}0 \\\\ 0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "The basis vectors will all entries in 0 and only one with 1 correspond to possibe measurement outcomes (spin up or down).\n",
        "\n",
        "The coefficients can be collected in one vector and are complex numbers:\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$\\left(\\begin{array}{l}2 \\\\ 3 \\\\ 0\\end{array}\\right)$</font>\n",
        "\n",
        "An arbitrary state for a qubit can be written as a linear combination of the Pauli matrices, which provide a basis for $2 \\times 2$ self-adjoint matrices:\n",
        "\n",
        "> $\n",
        "\\rho=\\frac{1}{2}\\left(I+r_{x} \\sigma_{x}+r_{y} \\sigma_{y}+r_{z} \\sigma_{z}\\right)\n",
        "$\n",
        "\n",
        "* where the real numbers $\\left(r_{x}, r_{y}, r_{z}\\right)$ are the coordinates of a point within the unit ball and\n",
        "\n",
        "> $\n",
        "\\sigma_{x}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{y}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{z}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siehe auch: [Projective Hilbert Space](https://en.m.wikipedia.org/wiki/Projective_Hilbert_space) with rays or projective rays"
      ],
      "metadata": {
        "id": "_sFD45vxEg5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSzA4IwURCu"
      },
      "source": [
        "<font color=\"blue\">*Multiply Ket with a Scalar: Vector-Scalar-Multiplication*\n",
        "\n",
        "Vector-Scalar:\n",
        "\n",
        "$\\left[\\begin{array}{l}x_{0} \\\\ x_{1}\\end{array}\\right] \\otimes\\left[y_{0}\\right]=\\left[\\begin{array}{l}x_{0}\\left[y_{0}\\right] \\\\ x_{1}\\left[y_{0}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}x_{0} y_{0} \\\\ x_{1} y_{0}\\end{array}\\right]$\n",
        "\n",
        "\n",
        "<font color=\"blue\">*Multiply Ket with another Ket: Ket - Tensor Product (Vector-Vector-Multiplication, Kronecker Product)*\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "*Zur Kombination von Quantum States:*\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$.\n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH8JeO4mW4q6"
      },
      "source": [
        "**How do we represent Ket's in a particular basis?**\n",
        "\n",
        "> <font color=\"blue\">$|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle \\quad$ where: $c_{i}=\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "Video: [Representations in quantum mechanics](https://www.youtube.com/watch?v=rp2k2oR5ZQ8)\n",
        "\n",
        "\n",
        "> $\\left\\{c_{i}\\right\\}$ are the representation of $|\\psi\\rangle$ in the $\\left\\{\\left|u_{i}\\right\\rangle\\right\\}$ basis\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_210.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_211.png)\n",
        "\n",
        "* a und b coefficients in euclidean space sind wie $u_i$ coefficients in quantum state space. (difference is bra-ket, wo bra das conjugate complex ist als dot product mit ket: es ist im erstem argument antilinear!).\n",
        "\n",
        "* expansion coefficients c are given by projection of the Ket onto the basis states u:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_212.png)\n",
        "\n",
        "* Here we can take out $|\\Psi\\rangle$ because it doesnt explicitely depend on $_i$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_213.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xfgmufRt0W"
      },
      "source": [
        "**Additional Ket-Algebra**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_202.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_203.png)\n",
        "\n",
        "For euclidean space: the scalar product is linear both in first and second argument:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_204.png)\n",
        "\n",
        "For state space: the scalar product is only linear in the second argument (and antilinear in the first argument):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_205.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra $\\langle \\psi |$ - Dual Space $V^{*}$ (Covector)***"
      ],
      "metadata": {
        "id": "cqdowsM5oHEP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOxv3yFtjD1"
      },
      "source": [
        "**How do we represent Bra's in a particular basis?**\n",
        "\n",
        "* Bra $\\langle \\Psi|$ is an element of the dual vector space $V^*$\n",
        "\n",
        "* Bra Psi times the identity operator ($\\langle \\Psi| * \\mathbb{I}$), and then write the identity operator out (its resolution in the u basis)\n",
        "\n",
        "* psi doesnt explicitylt depend on i, so we can write it into the summation, which brings us to this expression\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi \\mid u_{i}\\right\\rangle\\left\\langle u_{i}\\right|$\n",
        "\n",
        "* This is an expression for the bra psi in terms of the basis bra's u, and then the expansion coefficients are the brackets between psi and u $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$\n",
        "\n",
        "\n",
        "If we look at these expansion coefficients $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$, we can use the conjugation property of the scalar product to rewrite them like this:\n",
        "\n",
        "> $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$ = $\\left\\langle u_{i} \\mid \\psi \\right\\rangle^*$ = $c_i^*$\n",
        "\n",
        "**The expansion coefficients are the complex conjugates of the expansion coefficients of the Ket**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_214.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Changing Basis*"
      ],
      "metadata": {
        "id": "fdcELPZDs-X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB17Qq-GLGBT"
      },
      "source": [
        "**Changing Basis**\n",
        "\n",
        "* <font color=\"blue\">**For example: A particles position can be expressed as the superposition of momentum states**</font>\n",
        "\n",
        "* Goal: choose a 'good' basis that makes the maths as simple as possible\n",
        "\n",
        "Video: [Changing basis in quantum mechanics](https://www.youtube.com/watch?v=CDmXvPDMIFs)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_252.png)\n",
        "\n",
        "If we go from one representation to another: we need to calculate the overlaps between the corresponding basis states (with overlap matrix):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_253.png)\n",
        "\n",
        "How do we get back from the new to the old basis?\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_254.png)\n",
        "\n",
        "How we do transform the representation of operators between basis? (resolve identities in the u basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_255.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_256.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra-Ket $\\langle \\psi |\\psi\\rangle = c$ - Linear Form (Covector-Vector)*** *- also: Projective Measurement*"
      ],
      "metadata": {
        "id": "1fAAZcgWoiJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A6siRC6YyVY"
      },
      "source": [
        "> **Bra-Ket - Projective Measurement (Kovector-Vector-Multiplication, Born Rule)**\n",
        "\n",
        "* Dirac delta function (a distribution!) = quantum measurement\n",
        "\n",
        "* See also [wiki: Bra‚Äìket notation](https://en.m.wikipedia.org/wiki/Bra‚Äìket_notation)\n",
        "\n",
        "* From 'Exterior algebra' $\\rightarrow$ 'Multilinear Forms':\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti‚Äìisomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra‚Äìket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© = 1.\n",
        "\n",
        "*See also: [dot product between a covector and a vector is a scalar, after all!](https://cafephysics35698708.wordpress.com/2017/12/23/moving-away-from-ortho-land-vectors-covectors-and-all-that/) but read also why [There are a couple ways to view a dot product as a linear map by changing your view slightly](https://math.stackexchange.com/questions/2856198/is-dot-product-a-kind-of-linear-transformation)*\n",
        "\n",
        "* Inner Product / Bra-Ket, conjugate transpose of Ket.\n",
        "You get a scalar as output.\n",
        "\n",
        "* The Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$  represents the inner product in the Hilbert space\n",
        "\n",
        "* Zur Messung von Zustaenden in einer Basis (zB ich will die Probability wissen, mit der man den State=1 erh√§lt)\n",
        "\n",
        "* Quantum mechanical systems are represented by Hilbert spaces, which are anti‚Äìisomorphic to their own dual spaces.\n",
        "\n",
        "* **A state of a quantum mechanical system can be identified with a linear functional**.\n",
        "\n",
        "> $\\mathbf{u}^{\\top} \\mathbf{v}=\\left[\\begin{array}{llll}u_{1} & u_{2} & \\cdots & u_{n}\\end{array}\\right]\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n}\\end{array}\\right]=\\left[u_{1} v_{1}+u_{2} v_{2}+\\cdots+u_{n} v_{n}\\right]$ = scalar\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_248.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 1: Inner product of two basis vectors**:\n",
        "\n",
        "\n",
        "* ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), im Detail fur $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$\n",
        "\n",
        "* ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© ergeben inner product 1 (mit sich selbst multipliziert / die beiden Berechnungsbasisvektoren sind orthonormal)\n",
        "\n",
        "**Case 2: Berechne Wahrscheinlichkeit fur Messung eines Eigenstates (Born's Rule)**\n",
        "\n",
        "* Die orthonormalen Eigenschaften sind im folgenden Beispiel n√ºtzlich\n",
        "\n",
        "* Es gibt einen Zustand mit der Wave Function / Wahrscheinlichkeit fur beide Zustaende $|\\psi\\rangle=\\frac{3}{5}|1\\rangle+\\frac{4}{5}|0\\rangle$\n",
        "\n",
        "* die Wahrscheinlichkeit des Messens 1 ist dann (1 ist hierbei die basis, weil ich es messen will) [Source](https://docs.microsoft.com/de-de/azure/quantum/concepts-dirac-notation):\n",
        "\n",
        ">$\n",
        "|\\langle 1 \\mid \\psi\\rangle|^{2}=\\left|\\frac{3}{5}\\langle 1 \\mid 1\\rangle+\\frac{4}{5}\\langle 1 \\mid 0\\rangle\\right|^{2}=\\frac{9}{25}\n",
        "$\n",
        "\n",
        "* weil $\\langle 1 \\mid 0\\rangle=0$ faellt der zweite Term weg: $\\frac{4}{5}\\langle 1 \\mid 0\\rangle$ = $\\frac{4}{5} * 0$\n",
        "\n",
        "* Another example: $\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left(\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]\\right)=2 x+1 y$. **The covector [2, 1] can be thought of as a function, rather than a row vector, that acts on another vector.**\n",
        "\n",
        "> <font color=\"red\">*The probability of a particular measurement is then the absolute square of the scalar product with the basis vector that corresponds to the outcome (probability of measuring $X$ is). This is <u>Born's Rule</u>. This scalar product of the wave function with a basis vector is also sometimes called a <u>projection</u> on that basis vector:*\n",
        "\n",
        "> $|\\langle X \\mid \\Psi\\rangle|^{2}=a_{1} a_{1}^{*}$\n",
        "\n",
        "* where $\\mid \\Psi\\rangle$ is the wavefunction of the probability superposition and $\\langle X \\mid$ is the basis vector of one outcome.\n",
        "\n",
        "**Case 3: Sum over all basis vectors**\n",
        "\n",
        "> $\\langle\\Psi^* |\\Psi\\rangle$ = $ (a{_1}^*, a{_2}^*, a{_3}^*) \\left(\\begin{array}{l}a_{1} \\\\ a_{2} \\\\ a_{3}\\end{array}\\right)$ = $(a{_1}^*a_1 + a{_2}^*a_2 + a{_2}^*a_2)$\n",
        "\n",
        "**The probability to get ANY measurement outcome is equal to one, which means that the sum over the squared scalar products with all basis vectors has to be one. Which is just the length of the vector (all wave functions have length 1)**:\n",
        "\n",
        "> $1=a_{1} a_{1}^{*}+a_{2} a_{2}^{*}+a_{3} a_{3}^{*}=|\\langle \\Psi \\mid \\Psi\\rangle|^{2}$\n",
        "\n",
        "* With this bra-ket notation it's now very easy to write dot products = the **inner product between Bra and Ket which is $\\langle\\psi \\mid \\psi\\rangle$ = 1**, and it is normalized the result is 1(it's a particular way of writing the 2-norm when using complex inputs).\n",
        "\n",
        "* With the dot product of bra and ket you will get a **scalar as a result**, like total probability is 1, here for the coefficients (probability amplitudes):\n",
        "\n",
        "> $\\langle\\psi^* \\mid \\psi\\rangle = \\left|a_{0}\\right|^{2}+\\left|a_{1}\\right|^{2}= 1$\n",
        "\n",
        "* **We expand the Ket $\\psi$ in the basis u**, where the expansion coefficients c are given by the Braket between u and psi:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle = c_{1} * u_{1} + c_{2} * u_{2} .. + c_{i}* u_{i} = c_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)+{c_{2}}\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)+.. c_{i}\\left(\\begin{array}{l}0 \\\\ 0 \\\\ i\\end{array}\\right) \\quad =\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">This c cofficients are what we call **representation of the Ket psi in the u basis**.\n",
        "\n",
        "> $\\left(\\begin{array}{c}\\langle u_{1} \\mid \\psi\\rangle \\\\ \\left\\langle u_{2} \\mid \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{i} \\mid \\psi\\right\\rangle \\\\ \\vdots\\end{array}\\right)=\\left(\\begin{array}{c}c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{i} \\\\ \\vdots \\\\ \\end{array}\\right)$"
      ],
      "metadata": {
        "id": "JhSZ3kxevCQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Measurements from an Observable (via its Operator) - Use Case: What is the most likely Eigenvalue of an Observable like Momentum or Position?**\n",
        "\n",
        "Video: [Measurements in quantum mechanics || Concepts](https://www.youtube.com/watch?v=u1R3kRWh1ek)\n",
        "\n",
        "> **$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "* Physical quantity $\\mathcal{A} \\longrightarrow$ Observable $\\hat{A}$\n",
        "\n",
        "* To understand what it means to measure $\\hat{A}$ in quantum mechanics, the key equation to consider is the Eigenvalue equation of the operator $\\hat{A}$ here:\n",
        "\n",
        "> $\n",
        "\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle\n",
        "$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ When you apply a measurement operator in an Eigenstate, you will measure /get the Eigenvalue of this Eigenstate (postulate III of quantum mechanics). Challenge: we just dont know in which Eigenstate our system $|\\Psi\\rangle$ is.*</font>\n",
        "\n",
        "* with $\\lambda_{n}$ Eigenvalues and $u_{n}$ Eigenstates\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "\n",
        "* This means: when we want to measure property $\\hat{A}$ we first need to solve the corresponding Eigenvalue equation, which allows us to find all Eigenvalues $\\lambda_{1}$, $\\lambda_{2}$, .. $\\lambda_{n}$\n",
        "\n",
        "* So the operator $\\hat{A}$ encodes all the possible outcomes of the measurement, irrespective of what the state of the system is.\n",
        "\n",
        "* **So the question is: if we measure $\\hat{A}$ in state $|\\Psi\\rangle$: which Eigenvalue will we get?** (Postulate III will tell us we will get one of the Eigenvalues $\\lambda_{n}$, but not which one)\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate VI of quantum mechanics</u>: The measurement of $\\mathcal{A}$ in a system in normalized state $|\\psi\\rangle$ gives eigenvalue $\\lambda_{n}$ with probability:**\n",
        "\n",
        "\n",
        "> $\n",
        "P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\n",
        "$ $\\quad (= \\left|c_{n}\\right|^{2})$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ $u_n$ are the possible Eigenstates to which we project our actual system state $|\\Psi\\rangle$. We square its norm and then see which one has the highest value = highest probability that the state is in this Eigenstate (with a givne Eigenvalue $\\lambda_{n}$). For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$*</font>\n",
        "\n",
        "\n",
        "* This means: for an operator $\\hat{A}$ we have a list of Eigenvalues, where each of them corresponds to an Eigenstate:\n",
        "\n",
        "  * $\\lambda_{1}$ for $|u_1\\rangle$,\n",
        "\n",
        "  * $\\lambda_{2}$ for $|u_2\\rangle$,\n",
        "\n",
        "  * ...\n",
        "\n",
        "  * $\\lambda_{n}$ for $|u_n\\rangle$\n",
        "\n",
        "* these relations and values come from the Eigenvalue equation of the specific operator $\\mathcal{A}$ and are independent of the state of our system.\n",
        "\n",
        "* **But when we measure $\\mathcal{A}$, we measure it in a specific state $|\\Psi\\rangle$**. We will get $\\lambda_{1}$ with $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* So what we get as result depends (1) on the intrinsic properties $\\mathcal{A}$ with its Eigenvalues and Eigenstates,  and (b) on the specific state $|\\Psi\\rangle$ in which our system is.\n",
        "\n",
        "* So with postulate 4: rather then telling us the precise outcome of a measurement it tells us the probability associated with any given outcome\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "\n",
        "**How does the state $|\\Psi\\rangle$ of a system encodes the possible outcomes of a measurement?**\n",
        "\n",
        "> <font color =\"blue\">$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We can write the state $|\\Psi\\rangle$ in a complete basis of our state space and the Eigenstates $u_n$ over Hermitian operator like $\\hat{A}$ provide such a basis\n",
        "\n",
        "* this means we can write $|\\Psi\\rangle$ in the u-Basis (Eigenstates) like this:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{n} c_{n}\\left|u_{n}\\right\\rangle$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ $\\left|c_{m}\\right|^{2}$ is the probability and hence $c_n$ the squared root of the probability of a given Eigenvalue for each Eigenstate $\\left|u_{n}\\right\\rangle$ from this part here above:\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "* and these expansion coefficients c which we call **the representation of $|\\Psi\\rangle$ in the u-Basis**, are given by the projection of $|\\Psi\\rangle$ onto the u-Basis states:\n",
        "\n",
        "> $c_{n}=\\left\\langle u_{n} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* This means we can rewrite the probability p of measuring Eigenvalue $\\lambda_n$ as also equal to absolute value of $c_n$ squared:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{m}\\right|^{2}$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "**What does this mean?**\n",
        "\n",
        "* imagine our system is in state $\\Psi\\rangle$ and we want to measure a property $|\\Psi\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow \\hat{A} \\longrightarrow ?$\n",
        "\n",
        "* Then what we do is to write the state $|\\Psi\\rangle$ in the basis of Eigenstates of $\\hat{A}$, and <font color =\"blue\">the expansion coefficient $c_n$ tell us the relative contribution of Eigenstate $|u_n\\rangle$ to state $|\\Psi\\rangle$, which in turn tell us how likely it is to measure the associated Eigenvalue $\\lambda_n$</font>\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} \\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">here you can re-arrange and get an outer product (is this true???)\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} |u_{m}\\rangle \\langle u_{m}| \\psi\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "Passt zu weiter unten (video prof m mit projection operators):\n",
        "\n",
        "> $(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_199.png)\n",
        "\n",
        "Imagine we have n copies of the state $|\\Psi\\rangle$ and measure each. We get an Eigenvalue from the Eigenvalue equation, but with different probabilities. As p approaches $\\infty$ with N copies (= $p_n$), we will reach the most probable Eigenvalue P($\\lambda_m$) (postulates 4):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_200.png)\n",
        "\n",
        "**A special case if our system with state $|\\Psi\\rangle$ is in an Eigenstate of the property that we are measuring (the operator $\\hat{A}$, for example momentum, positon etc), say $|u_m\\rangle$**:\n",
        "\n",
        "> $|\\psi\\rangle=\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* This corresponds to the expansion coefficient $c_m$ = 1, while all other coefficients vanish.\n",
        "\n",
        "* In this particular case we do know with absolute certainty what the outcome of the measurement will be. Probability = 1:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|c_{m}\\right|^{2}=1$\n",
        "\n",
        "**This means: If the system is in an Eigenstate of the property that we are measuring (=momentum, position etc, measured by an operator), then the outcome of the measurement is the associated Eigenvalue with probability 1.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_287.png)"
      ],
      "metadata": {
        "id": "nKVQn725YOeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi | = P$ - Linear Map (Vector-Covector)*** *- also: Projection Operator & Mixed States (Density Matrix)*"
      ],
      "metadata": {
        "id": "cyrUknMLo8bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case 1: Projection Operator (Outer Product)**"
      ],
      "metadata": {
        "id": "vmUk0nAHzPCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A measurement outcome is actually a projection (to the first basis vector)**\n",
        "\n",
        "* So if we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$> bzw. | 0 > < 0 | $\\Phi$ >\n",
        "\n",
        "* this is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "> The probability of measuring a value with probability amplitude $\\phi$ is $1 \\geq|\\phi|^{2} \\geq 0$, where $|\\cdot|$ is the [modulus](https://en.m.wikipedia.org/wiki/Absolute_value#Complex_numbers).\n",
        "\n",
        "**Ket-Bra (also: Projection Operator, Outer Product or Density Matrix)**\n",
        "\n",
        "* Use Case 1: used for mixed states\n",
        "\n",
        "* Use Case 2: used when I want to measure a specific Eigenstate / Eigenvector to get its probability.\n",
        "\n",
        "* Is a pair: Vector-Covector\n",
        "\n",
        "> $\\mathbf{u v}^{T}=\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right]\\left[\\begin{array}{llll}v_{1} & v_{2} & \\cdots & v_{n}\\end{array}\\right]$= $\\left[\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{1}\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\ldots \\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\right]$ = $\\left[\\begin{array}{cccc}u_{1} v_{1} & u_{1} v_{2} & \\cdots & u_{1} v_{n} \\\\ u_{2} v_{1} & u_{2} v_{2} & \\cdots & u_{2} v_{n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ u_{n} v_{1} & u_{n} v_{2} & \\cdots & u_{n} v_{n}\\end{array}\\right]$\n",
        "\n",
        "> $|0\\rangle\\langle 0| =$ $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$\n",
        "\n",
        "* A measurement outcome is actually a projection (to the first basis vector)\n",
        "\n",
        "* If we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$>. This is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_307.jpg)\n",
        "\n",
        "From this video: https://youtu.be/U6fn5LvevEE\n",
        "\n",
        "**Example: if you want to measure with which probability the quantum system is in state 0, you apply $|0\\rangle \\langle0|$ operator which is $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$, because: $|0\\rangle\\langle 0|=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$**\n",
        "\n",
        "* \"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since **we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix**. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "https://physics.stackexchange.com/questions/394258/what-is-the-standard-definition-of-projector-projection-and-projection-ope\n",
        "\n",
        "The density matrix is a representation of a linear operator called the density operator. The density matrix is obtained from the density operator by choice of basis in the underlying space. In practice, the terms density matrix and density operator are often used interchangeably.\n",
        "\n",
        "In operator language, a density operator for a system is a positive semidefinite, Hermitian operator of trace one acting on the Hilbert space of the system. This definition can be motivated by considering a situation where a pure state $\\left|\\psi_{j}\\right\\rangle$ is prepared with probability $p_{j}$, known as an ensemble. The probability of obtaining projective measurement result $m$ when using projectors $\\Pi_{m}$ is given by\n",
        "\n",
        "> $\n",
        "p(m)=\\sum_{j} p_{j}\\left\\langle\\psi_{j}\\left|\\Pi_{m}\\right| \\psi_{j}\\right\\rangle=\\operatorname{tr}\\left[\\Pi_{m}\\left(\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\\right)\\right]\n",
        "$\n",
        "\n",
        "which makes the density operator, defined as\n",
        "\n",
        "> <font color=\"red\">$\n",
        "\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\n",
        "$\n",
        "\n",
        "= probability of getting a state |0> for example with outer product / density matrix $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$ PLUS probability of getting state |1> etc."
      ],
      "metadata": {
        "id": "5bWbx2kRFnsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">*Projection Operator (Outer Product) & Eigenvalues in Systems of Linear Equations (HHL)*\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "> $A|x\\rangle=|b\\rangle \\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle \\quad \\text { (the solution is: }|x\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle \\text { ) }\n",
        "$\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "(if I want to know get the Eigenvector from a measurement, I create the outer product / density matrix of the Eigenvector like above. this is like a measurement)\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        "> $\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$"
      ],
      "metadata": {
        "id": "MXGA0EYlF9iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Case 1 - Bra-Ket (Inner Product)**: *I want to know the Eigenvalue of an observable (but I don't know which is the most probable Eigenvector)? Then I apply an operator to the quantum state (position operator, momentum operator etc). I will get the most probable Eigenvector / Eigenstate with an according Eigenvalue*\n",
        "\n",
        "> $\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$\n",
        "\n",
        "= apply a certain operator (=observable) on a quantum state and you get the Eigenvalue of this state (here u can also be considered the eigenstate / eigenvector)\n",
        "\n",
        "<font color=\"blue\">**Case 2 - Ket-Bra (Outer Product)**: *I want to measure a specific Eigenstate / Eigenvector to get its probability? Then I take the outer product / density matrix of the desired outcome and make a measurement on the quantum state / system (=project it onto the desired Eigenvector)*\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "*  mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m}\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* here you can re-arrange and get an outer product (true?):\n",
        "\n",
        "> $\n",
        "|\\psi\\rangle=\\sum_{m}\\left|u_{m}\\right\\rangle\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "\n",
        "* Passt zu weiter unten (video prof $m$ mit projection operators):\n",
        "\n",
        "> $\n",
        "(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle\n",
        "$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_250.png)\n",
        "\n",
        "Video: [Projection operators in quantum mechanics](https://www.youtube.com/watch?v=M9V4hhqyrKQ)\n",
        "\n",
        "> **Projection Operators project one quantum state on another or onto a subspace of the state space**\n",
        "\n",
        "* for example: when we measure a property of a quantum particle, then the state of the particle collapses onto a different state\n",
        "\n",
        "* the projection operator mathematically describes this collapse\n",
        "\n",
        "> **projection operator associated with a Ket Psi: $| \\Psi \\rangle$ is the outer product of psi with itself: $|\\Psi\\rangle \\langle \\Psi|$**\n",
        "\n",
        "* then we check what it does on an arbirary state phi\n",
        "\n",
        "* the projection operator projects an arbitrary state (here Phi) onto the reference state (here Psi).\n",
        "\n",
        "* the proportionality constant C is given by the overlap between the initial state phi and the state psi that defines the projection operator\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_264.png)\n",
        "\n",
        "Properties of the projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_265.png)\n",
        "\n",
        "Eigenvalues and Eigenstates of Projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_266.png)\n",
        "\n",
        "The Projection operator allows us to write any Ket as a sum of two other Kets:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_267.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_268.png)\n",
        "\n",
        "One common way in which the projection operator is used in quantum mechanics is to project onto a subspace of the whole state space.\n",
        "\n",
        "* Most convenient way to write this down is in terms of basis states of our state space.\n",
        "\n",
        "* For a basis u we consider a subset of basis states u1, to un which soon an n-dimensional subspace of the full state space.\n",
        "\n",
        "* We then write the projection operator onto this n-dimensional subspace as pn\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_269.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_270.png)\n",
        "\n",
        "**Projective measurement (PVM - Projection-valued measure)**\n",
        "\n",
        "* See [this article](https://en.m.wikipedia.org/wiki/Measurement_in_quantum_mechanics#Projective_measurement) and longer article on [PVM](https://en.m.wikipedia.org/wiki/Projection-valued_measure)\n",
        "\n",
        "* The eigenvectors of a von Neumann observable form an orthonormal basis for the Hilbert space, and each possible outcome of that measurement corresponds to one of the vectors comprising the basis. A density operator is a positive-semidefinite operator on the Hilbert space whose trace is equal to 1.\n",
        "\n",
        "> <font color =\"red\">**For each measurement that can be defined, the probability distribution over the outcomes of that measurement can be computed from the density operator. The procedure for doing so is the Born rule, which states that**</font>\n",
        "\n",
        "> $\n",
        "P\\left(x_{i}\\right)=\\operatorname{tr}\\left(\\Pi_{i} \\rho\\right)\n",
        "$\n",
        "\n",
        "* where $\\rho$ is the density operator, and $\\Pi_{i}$ is the [projection operator](https://en.m.wikipedia.org/wiki/Projection_(linear_algebra)) onto the basis vector corresponding to the measurement outcome $x_{i}$.\n",
        "\n",
        "* The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the [expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)) of that observable. For an observable $A$, the expectation value given a quantum state $\\rho$ is\n",
        "\n",
        ">$\n",
        "\\langle A\\rangle=\\operatorname{tr}(A \\rho)\n",
        "$\n",
        "\n",
        "* A density operator that is a rank-1 projection is known as a pure quantum state, and all quantum states that are not pure are designated mixed. Pure states are also known as wavefunctions."
      ],
      "metadata": {
        "id": "qy0sjtMGU9XY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kD-s4t82ez"
      },
      "source": [
        "**Use Case 2: Mixed States (Density Matrix)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die [Quantenstatistik](https://de.m.wikipedia.org/wiki/Quantenstatistik) wendet zur Untersuchung makroskopischer Systeme die Methoden und Begriffe der klassischen statistischen Physik an und ber√ºcksichtigt zus√§tzlich die quantenmechanischen Besonderheiten im Verhalten der Teilchen.\n",
        "\n",
        "Wie die Quantenmechanik ber√ºcksichtigt auch die Quantenstatistik die folgende doppelte Unkenntnis:\n",
        "\n",
        "> 1. Kennt man den Zustand eines Systems genau ‚Äì liegt also ein **reiner Zustand (pure state)** vor ‚Äì und ist dieser kein Eigenzustand der Observablen, so kann man den Messwert einer Einzelmessung dennoch nicht exakt vorhersagen.\n",
        "\n",
        "* A pure state can be written in terms of a Ket state $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$\n",
        "\n",
        "  * <font color =\"blue\">$|\\psi\\rangle$</font> $=\\sum_{i} c_{i}\\left|\\varphi_{i}\\right\\rangle$\n",
        "\n",
        "  * Spin up or spin down = **pure states**, can be written as a single wave function $\\mid \\psi\\rangle$.\n",
        "\n",
        "> 2. Kennt man den Zustand des Systems nicht genau, so muss von einem **gemischten Zustand (mixed state)** ausgegangen werden. Ebenso: when one wants to describe a physical system which is entangled with another, as its state can not be described by a pure state. For mixed states with noise in qubits.\n",
        "\n",
        "* A mixed state can be described with a density matrix:\n",
        "\n",
        "  * <font color =\"blue\">$\\rho$ </font>$=\\sum_{i} p_{i}\\left|\\varphi_{i} \\times \\varphi_{i}\\right|$\n",
        "  * Sum over probabilities times the corresponding projection operators onto certain basis states $\\varphi_{i}$\n",
        "\n",
        "  * It allows for the calculation of the probabilities of the outcomes of any measurement performed upon this system, using the Born rule.\n",
        "\n",
        "**There are two more ways to distinguish a pure state from a mixed state:**\n",
        "\n",
        "1. Take the trace of the square of the density matrix:\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]=1 \\rightarrow$ Pure state (like Summe der Eigenwerte entlang der Spur: 1-0-0-0..)\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]< 1 \\rightarrow$ Mixed state. <font color=\"red\">But in the examples below the trace is always = 1? (and some off-diagonal elements)</font>\n",
        "\n",
        "2. Geometricaly, pure states lie on the surface of the Blochsphere. Mixed states are confined within the Bloch sphere.\n",
        "\n",
        "> $\n",
        "\\begin{array}{c|c|c}\n",
        "& \\begin{array}{c}\n",
        "\\text { Pure } \\\\\n",
        "\\text { State }\n",
        "\\end{array} & \\begin{array}{c}\n",
        "\\text { Mixed } \\\\\n",
        "\\text { State }\n",
        "\\end{array} \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Density } \\\\\n",
        "\\text { Matrix } \\\\\n",
        "|\\psi\\rangle\\langle\\psi|\n",
        "\\end{array} & \\color{green} ‚úî & \\color{green}‚úî \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Wave } \\\\\n",
        "\\text { Function } \\\\\n",
        "|\\Psi\\rangle\n",
        "\\end{array} & \\color{green}‚úî & \\color{red} ‚ùå\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Why do we need this alternative formalism, these density matrices?**\n",
        "\n",
        "* To illustrate the difference, think about this Ket $|\\psi\\rangle$, which is the equal superposition of 0 and 1. We can write out the vector form $\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right]$. And if we write the corresponding $\\rho$, it will have 0.5 for every element in the matrix.\n",
        "\n",
        "> $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+(1\\rangle)=\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right] \\rightarrow $$\\rho=\\left[\\begin{array}{ll}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* On the other hand if we create the uniform distribution over the density matrix corresponding to the 0-Ket and the density matrix corresponding to the 1-Ket, then this density matrix will be different:\n",
        "\n",
        "> $\\rho^{\\prime}=\\frac{1}{2}(|0 \\times 0|+|1 \\times 1|)=\\left[\\begin{array}{ll}0.5 & 0 \\\\ 0 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* It doesn't have off-diagonal elements. These off-diagonal elements are critical for many quantum operations, sometimes also called 'coherences'. This is then called a maximally mixed state = equivalent of a uniform distribution in classical probability theory. This means we have absolutely no predictive power of what's going to happen next. Entropy of the state is maximal.\n",
        "\n",
        "* Ideally we want quantum states with a high coherence, but in reality noise affects, and these coherences disappear.\n",
        "\n",
        "https://www.youtube.com/watch?v=BE8RxAESx5I&list=PLBn8lN0Dcvpla6a6omBni1rjyQJ4CssTP&index=47\n"
      ],
      "metadata": {
        "id": "XKh8o95dlsKS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiVIYzwp82e1"
      },
      "source": [
        "**Explanation 1: Density Matrix for Mixed States**\n",
        "\n",
        "*Source here point 2.1: https://qiskit.org/textbook/ch-quantum-hardware/density-matrix.html*\n",
        "\n",
        "* Let's consider the case where we initialize a qubit in the |0‚ü© state, and then apply a Hadamard gate.\n",
        "\n",
        "* **Now, unlike the scenario we described for pure states, this Hadamard gate is not ideal: Due to errors in the quantum-computer hardware, only 80% of the times the state is prepared**, this Hadamard gate produces the desired state:\n",
        "\n",
        "> $\\left|\\psi_{1}\\right\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "* The remaining $20 \\%$ of the times, the pulse applied to rotate the state is either too short or too long by $\\frac{\\pi}{6}$ radians about the $x$-axis. This means that when we use this Hadamard gate, we could end up with following two undesired outcome states:\n",
        "\n",
        ">$\n",
        "\\left|\\psi_{2}\\right\\rangle=\\frac{\\sqrt{3}}{2}|0\\rangle+\\frac{1}{2}|1\\rangle, \\quad\\left|\\psi_{3}\\right\\rangle=\\frac{1}{2}|0\\rangle+\\frac{\\sqrt{3}}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "* The figure below shows the Bloch representation for the three possible states our qubit could take if the short pulse happens 10% of the time, and the long pulse the remaining 10% of the time:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_198.png)\n",
        "\n",
        "* Since we do not know the outcome of our qubit everytime we prepare it, we can represent it as a mixed state of the form:\n",
        "\n",
        "**Step 1: how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:**\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "in this case:\n",
        "\n",
        "> $\n",
        "\\rho_{H}=\\frac{4}{5}\\left|\\psi_{1}\\right\\rangle\\left\\langle\\psi_{1}\\left|+\\frac{1}{10}\\right| \\psi_{2}\\right\\rangle\\left\\langle\\psi_{2}\\left|+\\frac{1}{10}\\right| \\psi_{3}\\right\\rangle\\left\\langle\\psi_{3}\\right|$\n",
        "\n",
        "* Here, the factors $\\frac{4}{5}, \\frac{1}{10}$ and $\\frac{1}{10}$ correspond to the classical probabilities of obtaining the states $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle$ and $\\left|\\psi_{3}\\right\\rangle$, respectively.\n",
        "\n",
        "**Step 2: Add the matrices together**: By replacing each of these three possible state vectors into  $\\rho$ , we can find the density matrix that represents this mixture:\n",
        "\n",
        "> $\\rho_{H}=\\frac{4}{5}\\left[\\begin{array}{ll}\\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4}\\end{array}\\right]$\n",
        "$\\rho_{H}=\\left[\\begin{array}{cc}\\frac{1}{2} & \\frac{\\sqrt{3}}{20}+\\frac{2}{5} \\\\ \\frac{\\sqrt{3}}{20}+\\frac{2}{5} & \\frac{1}{2}\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWPEnYF82e2"
      },
      "source": [
        "**Explanation 2: Density Matrix for Mixed States**\n",
        "\n",
        "*Source: Parth G: https://www.youtube.com/watch?v=ZAOc4eMTQiw*\n",
        "\n",
        "* **Pain Point**:  when we see this notation for two states spin up and spin down: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$ we think of it as being in a superposition of both states. But sometimes we don't know if the system is actually in this superposition, or not already collapsed to one.\n",
        "\n",
        "* So, in practice before we measure a system, it can be in following three states, because we lack information about it. This is a mixed state:\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\uparrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\downarrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$\n",
        "\n",
        "* **Mixed state**: lack of information about the system, hence for example giving equal probability to each possible outcome.\n",
        "\n",
        "* **When dealing with a mixed state we need to represent it with what's known as a density operator or density matrix**.\n",
        "\n",
        "* Wave function notation $|\\psi\\rangle$ can only be used to describe pure states. But density matrices $|\\psi\\rangle\\langle\\psi|$ can be used to describe mixed and pure states.\n",
        "\n",
        "* and how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "On top of that you can even add several mixed states to get one final mixed states. Mixed state: our knowledge of the system is limited. It could be either one of multiple psi states.\n",
        "\n",
        "* Image 1: We have to add up the density matrices of each possible spin state whilst making sure that we weight it with the probability of it being in that psi state\n",
        "\n",
        "* Image 2: That would give us the final density matrix, which is the mixture in the mixed state.\n",
        "\n",
        "* Image 3: it is for this reason that there is no way to write a mixed state as a single wave function or a single vector. We have to deal with matrices that represent the different pure states in which our system could be. And each of these density matrices has to be weighted by how likely it is that our system is on that pure that.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_006a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXuS8c94Iay"
      },
      "source": [
        "**Expectation Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Density matrix formalism</u> to calculate Expectation Values**\n",
        "\n",
        "\n",
        "* [Expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)): The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the expectation value of that observable. What is the average value of quantum measurement outcomes?\n",
        "\n",
        "* Large number of particle interactions when we are trying to measure a total effect from an aggregation of subatomic particles: run an experiment involving the observation of energy levels in a type of atom N times under fixed conditions. The expectation value e of the experiment turns out to be an illegitimate energy level for that atom, but E = Ne will give the correct total energy assuming N is large enough.\n",
        "\n",
        "* Expectation value of an observable $Q$ (like the momentum of a particle) in a (normalized) state $|\\Psi\\rangle$ gives the average of all possible values (weighted by their corresponding probabilities) that one may expect to observe in an experiment designed to measure $Q$ in state $|\\Psi\\rangle$ over many experiments where the average of all values of $Q$ will approach the expectation value $\\langle\\Psi|Q| \\Psi\\rangle$ (one set of measurement perturbs the system)"
      ],
      "metadata": {
        "id": "NWbNfdo0aS0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**First: The expectation value of an operator $Q$ is given by $\\langle Q\\rangle=\\langle\\psi|Q| \\psi\\rangle$ = $\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$(spectral decomposition)**</font>\n",
        "\n",
        "* Given a state vector for a pure state $|s\\rangle$, with its complex-conjugate transpose is $\\langle s|$, the operator whose expectation value you are evaluating could be written $|A|$ and $|s\\rangle$ has a component which is a $\\lambda$-eigenstate for $A$.\n",
        "\n",
        "\n",
        "* A mixture of states is a weighted average of such operators, and its expectation value is the weighted average of the expectation values of the pure states.\n",
        "\n",
        "* Suppose, $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle, \\ldots$ be the orthonormal basis of eigenstates of observable $Q$ with corresponding eigenvalues $\\lambda_{1}, \\lambda_{2}, \\ldots$ respectively. One can expand the given (normalized) state $|\\Psi\\rangle$ in the above basis as\n",
        "\n",
        "> $\n",
        "|\\Psi\\rangle=\\alpha_{1}\\left|\\psi_{1}\\right\\rangle+\\alpha_{2}\\left|\\psi_{2}\\right\\rangle+\\cdots+\\alpha_{n}\\left|\\psi_{n}\\right\\rangle+\\ldots\n",
        "$\n",
        "\n",
        "* According to a basic postulate of Quantum mechanics the above expansion implies that upon measuring $Q$ in state $|\\Psi\\rangle$ the outcome of the experiment will be eigenvalue $\\lambda_{1}$ with probability $\\left|\\alpha_{1}\\right|^{2}$, eigenvalue $\\lambda_{2}$ with probability $\\left|\\alpha_{2}\\right|^{2}$ and so on. The (weighted) average of all possible values of $Q$ that can be observed in state $|\\Psi\\rangle$ will be the expectation value of $Q$ in state $|\\Psi\\rangle$:\n",
        "\n",
        "> <font color=\"blue\">$\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$ = $\\left|\\alpha_{1}\\right|^{2} \\lambda_{1}+\\left|\\alpha_{2}\\right|^{2} \\lambda_{2}+\\ldots =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots| \\, | \\lambda_{1} \\alpha_{1} \\psi_{1}+\\lambda_{2} \\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots|Q| \\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\langle\\Psi|Q| \\Psi\\rangle$\n",
        "\n",
        "* This is because operator $Q$ can be written as a sum over its Eigenvalues $a_i$ times projection operators onto its Eigenstates.\n",
        "\n",
        "> $Q=\\sum_{i} \\lambda_{n} \\left| u_{n} \\rangle \\langle \\ u_{n}\\right|$ $\\quad$ with $Q\\left| u_{n}\\right\\rangle= \\lambda_{n}\\left| u_{n}\\right\\rangle$\n",
        "\n",
        "* By using decomposition in the definition of the expectation value, we get this result:\n",
        "\n",
        "> $\\langle Q\\rangle_{\\psi}=\\langle\\psi|Q| \\psi\\rangle=$ $\\sum_{i} \\lambda_{n}\\langle\\psi| u_{n} \\rangle $ <font color =\"orange\">$\\langle u_{u}| \\psi \\rangle$</font> $=\\sum_{i} \\lambda_{n}$ <font color =\"blue\">$\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We sum over all Eigenvalues, which are the possible outcomes of a measurement, weighted by the probabilities that this particular Eigenvalue occurs if we start in the state $\\Psi$.\n",
        "\n",
        "  * <font color =\"orange\">$\\langle u_{n} | \\psi\\rangle$</font> - this is called the **transition amplitude**\n",
        "\n",
        "  * <font color =\"blue\">$|\\langle u_{n} | \\psi\\rangle|^{2}$</font> - this is called the **transition probability** (absolute square of the transition amplitude)\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ and since: $P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$:\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|c_{n}\\right|^{2}$</font>\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow\\langle\\hat{A}\\rangle_{\\psi}=\\langle\\hat{A}\\rangle=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ <font color =\"blue\">$=\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* <font color =\"blue\">$\\rightarrow$ die Summe der Eigenwerte $\\lambda_{n}$ jeweils multipliziert mit der Wahrscheinlichkeit $P\\left(\\lambda_{n}\\right) = \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$, den jeweiligen Eigenwert zu erhalten = Expectation Value</font>\n",
        "\n",
        "* Beispielrechnung mit interessantem Ergebnis: Eigenwert -1 mit Amplitude 0.25 und Eigenwert +1 mit Amplitude 0.25\n",
        "\n",
        "  * = $(-1) * (0.25)^2$ + $(+1) * (0.25)^2$\n",
        "\n",
        "  * = $(-1) * (0,5)$ + $(+1) * (0.5)$ = -0.5 + 0.5\n",
        "\n",
        "  * = 0 $\\rightarrow$ Expectation value, aber kein erlaubtes physikalisches Ergebnis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Im0HXIwpZSfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Second: The expectation value $\\langle \\psi|A| \\psi\\rangle$ can be evaluated as the trace of $|\\psi\\rangle\\langle \\psi| A$.**</font>\n",
        "\n",
        "* The operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1, and the operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1.\n",
        "\n",
        "* $\\rho = |\\psi\\rangle \\langle \\psi|$ is an operator of trace=1. We can calculate the expectation value of $A$ by taking the trace over $\\rho$ times $A$:\n",
        "\n",
        "> **$\\langle A\\rangle_{\\rho}:= \\langle \\psi|A| \\psi\\rangle = \\operatorname{Tr}[\\rho A] =\\operatorname{Tr}[\\, |\\psi\\rangle \\langle \\psi| \\, A]$**</font>\n",
        "\n",
        "* Taking the trace over an operator means to sum over the matrix elements of that operator between states of a complete basis $\\operatorname{Tr}[B]=\\sum_{i}\\left\\langle\\eta_{i}|B| \\eta_{i}\\right\\rangle$. We are adding $\\rho = |\\psi\\rangle \\langle \\psi|$ and move the two complex numbers $\\eta$, since they are not operators:\n",
        "\n",
        "> $\\langle A\\rangle_{\\rho}:=\\operatorname{Tr}[\\rho A]$ = $\\sum_{i} {\\left\\langle\\eta_{i}\\right| \\psi \\rangle}{\\left\\langle\\psi|A| \\eta_{i}\\right\\rangle}$\n",
        "\n",
        "* We sum over the basis states. Since $| \\eta_{i} \\rangle \\langle \\eta_{i} |$ = 1, we get $\\langle\\psi|A| \\psi\\rangle$, which is the same result from working with density matrices:\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi|A| \\eta_{i} \\rangle \\langle \\eta_{i} | \\psi\\right\\rangle$ = $\\langle\\psi|A| \\psi\\rangle=\\langle A\\rangle_{\\psi}$\n"
      ],
      "metadata": {
        "id": "taewaWhtaMqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_195.png)"
      ],
      "metadata": {
        "id": "lX-bFe39aNwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_201.png)"
      ],
      "metadata": {
        "id": "RyU5akyieNvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Eigenvalues, Observables & Operators*"
      ],
      "metadata": {
        "id": "gXZXzSSsrrUL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpO0HLIBkdmU"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "Video: [Eigenvalues and eigenstates in quantum mechanics](https://www.youtube.com/watch?v=p1zg-c1nvwQ)\n",
        "\n",
        "> We consider the action of $\\hat{A}$ on a special Ket $|\\Psi\\rangle$ such that the only way in which $\\hat{A}$ changes $|\\Psi\\rangle$ is by scaling it by a constant and we obtain $\\lambda |\\Psi\\rangle$\n",
        "\n",
        "The Eigenvector of an operator are those special directions in the vector space which the operator doesn't change.\n",
        "\n",
        "The probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_257.png)\n",
        "\n",
        "> [C$*$-algebras](https://en.m.wikipedia.org/wiki/C*-algebra) were first considered primarily for their use in quantum mechanics **to model algebras of physical observables**. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. See also [Quantum Group](https://en.m.wikipedia.org/wiki/Quantum_group).\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_251.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Anti-)Commutators, Quantum Numbers und \"Vollst√§ndiger Satz kommutierender Observablen\"**\n",
        "\n",
        "Um einen quantenmechanischen Zustand eindeutig zu charakterisieren, sind oft mehrere Observablen notwendig. Beispielsweise ist es beim Wasserstoffatom nicht ausreichend, nur die Energie anzugeben (mittels der Hauptquantenzahl n), sondern es sind zwei weitere Observablen notwendig: der Betrag des Drehimpulses (Quantenzahl l) und die z-Komponente des Drehimpuls (Quantenzahl m). Diese drei Gr√∂√üen bilden dann einen vollst√§ndigen Satz kommutierender Observablen.\n",
        "Eine Menge von Observablen A, B, C,... bildet einen v.S.k.O., wenn eine orthonormale Basis des Zustandsraums aus gemeinsamen Eigenvektoren der Observablen existiert, und diese Basis (bis auf einen Phasenfaktor) eindeutig ist.\n",
        "\n",
        "Solch ein Verhalten ist in der Quantenmechanik allerdings eher die Ausnahme. Die meisten Paare von Observablen lassen sich nicht gleichzeitig beliebig genau messen, was eine Konsequenz aus der heisenbergschen Unsch√§rferelation ist. Man spricht dann auch von komplement√§ren Observablen.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Satz_kommutierender_Observablen"
      ],
      "metadata": {
        "id": "rRJWyBvzCSAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc9HeAnZvFPv"
      },
      "source": [
        "**Solving: How to find Eigenvalues and Eigenvectors?**\n",
        "\n",
        "* For an arbitrary operator (except the identity operator) it is generally not possible to figure out the eigenvalues and eigenstates simply by inspection of how the operator acts\n",
        "\n",
        "* More general approach needed:\n",
        "\n",
        "\t* consider basis u that is orthonormal.\n",
        "\n",
        "\t* Then we write the Eigenvalue equation in the u basis (just project both sides of the equation onto the basis states u).\n",
        "\n",
        "\t* next we insert the identity operator after the A operator on left and side\n",
        "\n",
        "\t* then write the resolution of the identity in the u basis\n",
        "\n",
        "\t* then take sum on the beginning\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_260.png)\n",
        "\n",
        "* the equation $\\sum_{j}\\left(A_{i j}-\\lambda \\delta_{i j}\\right) c_{j}=0$ is equivalent to the original Eigenvalue equation $\\hat{A}|\\psi\\rangle=\\lambda|\\psi\\rangle$, but now written in the u representation\n",
        "\n",
        "* finding the eigenvalues and eigenvectors now becomes finding the lambda and c in the new equation = \"matrix diagonalization\"\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_261.png)\n",
        "\n",
        "> **All we really need to do quantum mechanics is to get enough practice in diagonalizing matrices**\n",
        "\n",
        "* we have following operator A and state Psi\n",
        "\n",
        "* we want to find the eigenvalues and eigenvectors of the operator A\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYnM169ZFHeZ"
      },
      "source": [
        "<font color=\"blue\">**Operators acting on Quantum State Vectors: Matrix-Vector-Multiplication (Single Qubit)**\n",
        "\n",
        "*Applying a Hadamard gate to a single qubit (matrix-vector multiplication) - Simple dot product! You get a vector out.*\n",
        "\n",
        "$H |0\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 1 + 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right] = |+\\rangle$\n",
        "\n",
        "$H |1\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}0 + 1 \\\\ 0 -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right] = |-\\rangle$\n",
        "\n",
        "*Diese Zust√§nde k√∂nnen auch mithilfe der Dirac-Notation als Summen von |0‚ü© und |1‚ü© erweitert werden:*\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "*Zusammenfassend, das ist alles das gleiche, sieht aber komplett anders aus:*\n",
        "\n",
        "<font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "<font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*$|0\\rangle$ und $|1\\rangle$ stellen hier die Basis dar, in der die Quantenzustaende berechnet werden.* ***Man kann allerdings auch eine andere Basis waehlen, zB $|+\\rangle$ und $|-\\rangle$:***\n",
        "\n",
        "$|0\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle+|-\\rangle)$ vergleiche: <font color=\"blue\">$|+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "$|1\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle-|-\\rangle)$ vergleiche: <font color=\"blue\">$|-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*Applying an Identity Operator:*\n",
        "\n",
        "$I |0\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 0\\end{array}\\right]=\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "$I |1\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\left[\\begin{array}{ll}0 + 0 \\\\ 0 + 1\\end{array}\\right]=\\left[\\begin{array}{c}0 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "*Applying a Pauli-X Operator:*\n",
        "\n",
        "$X|0\\rangle=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 + 0 \\\\ 1 + 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTK-u1Q4nrGv"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Vector-Multiplication (Multi Qubit)**\n",
        "\n",
        "> $A \\mathbf{x}=\\left[\\begin{array}{cccc}a_{11} & a_{12} & \\ldots & a_{1 n} \\\\ a_{21} & a_{22} & \\ldots & a_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m 1} & a_{m 2} & \\ldots & a_{m n}\\end{array}\\right]\\left[\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right]=\\left[\\begin{array}{c}a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n} \\\\ a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n} \\\\ \\vdots \\\\ a_{m 1} x_{1}+a_{m 2} x_{2}+\\cdots+a_{m n} x_{n}\\end{array}\\right]$\n",
        "\n",
        "**First create the tensor product of two operators**: For construction of the desired two-qubit gate, you need the same tensor product operation as you used for the vectors. Here where $H_1$ is a one-qubit Hadamard gate in the two-qubit space $(\\hat{H} \\otimes \\mathbf{I})$, where Hadamard is applied only to one Qubit:\n",
        "\n",
        "> $H_{1} \\equiv H_{0} \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "**Second, apply the new 'tensored' operator on the tensor product of two state vectors**:\n",
        "\n",
        "See the tensor product of two vectors in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Now applying $H_1$ you mix up the first qubit states and keep the second qubit state unchanged:\n",
        "\n",
        "> $H_{1}(|0\\rangle \\otimes|0\\rangle)=H_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$= <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right) \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right)$</font>= $\\frac{1}{\\sqrt{2}}(|0\\rangle \\otimes|0\\rangle+|1\\rangle \\otimes|0\\rangle)$\n",
        "\n",
        "**Another way of writing this (=apply H to one qubit only in a 1 qubit in a 2 qubit system) for a joint state of two initialized qubits $\n",
        "|0\\rangle \\otimes|0\\rangle\n",
        "$ is:**\n",
        "\n",
        "> <font color=\"orange\">$\\hat{H}\\left|q_{0}\\right\\rangle \\otimes\\left|q_{1}\\right\\rangle$</font> = $\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "The tensor product is distributive, which in this case means it acts much like multiplication:\n",
        "\n",
        ">$\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\otimes|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "See [Tensor_product](https://en.wikipedia.org/wiki/Tensor_product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z7cDFlh_eL1"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Matrix-Multiplication (Kronecker / Tensor)**\n",
        "\n",
        "Zur Kombination von Operators in einem Timestep (zB H bei qubit 1 und I bei Qubit 2). Two systems being described as a joint system.\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right]=\\left[\\begin{array}{lll}a\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & b\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] \\\\ c\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & d\\left[\\begin{array}{llll}e & f \\\\ g & h\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{llll}a e & a f & \\text { be } & b f \\\\ a g & a h & b g & b h \\\\ c e & c f & d e & d f \\\\ c g & c h & d g & d h\\end{array}\\right]$\n",
        "\n",
        "$H \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\otimes\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{rrrr}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "$I \\otimes H=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\otimes \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\\\ 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1\\end{array}\\right)$\n",
        "\n",
        "$Y \\otimes X=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & -i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\\\ i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & 0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{cccc}0 & 0 & 0 & -i \\\\ 0 & 0 & -i & 0 \\\\ 0 & i & 0 & 0 \\\\ i & 0 & 0 & 0\\end{array}\\right]$\n",
        "\n",
        "$X \\otimes H=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\otimes \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] \\\\ 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cccc}0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0\\end{array}\\right] =\\left[\\begin{array}{cc}\n",
        "0 & H \\\\\n",
        "H & 0\n",
        "\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnohitquaGyf"
      },
      "source": [
        "**<font color=\"blue\">Matrix-Matrix-Multiplication (Usual)**\n",
        "\n",
        "> Used in serially wired gates (so NOT gates in one time step - here we use tensor product to combine them, but serial gates!)\n",
        "\n",
        "A line in the circuit is considered as a quantum wire and basically represents a single qubit. The product of operators keeps the same dimension.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Serially_wired_quantum_logic_gates.png/500px-Serially_wired_quantum_logic_gates.png)\n",
        "\n",
        "For example, putting the Pauli X gate after the Pauli Y gate, both of which act on a single qubit, can be described as a single combined gate C:\n",
        "\n",
        ">$\n",
        "C=X \\cdot Y=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right] \\cdot\\left[\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
        "i & 0 \\\\\n",
        "0 & -i\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "X \\cdot X=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89sYf-N0_6u"
      },
      "source": [
        "<font color=\"blue\">**Hamiltonian Operator $\\mathcal{H}$**\n",
        "\n",
        "> <font color=\"red\">**The sum of the possible outcomes of kinetic and potential energy of this entire system in quantum mechanics is referred to the Hamiltonian $\\mathcal{H}$ (to calculate the lowest total energy of a two atom system)**\n",
        "\n",
        "* der [Hamiltonoperator](https://de.m.wikipedia.org/wiki/Hamiltonoperator) (Energieoperator) ist in der Quantenmechanik ein Operator, **der (m√∂gliche) Energiemesswerte und die Zeitentwicklung angibt = describes the total energy of a system or particle**. <font color=\"red\">Er liefert beispielsweise die Energieniveaus des Elektrons im Wasserstoffatom.</font>\n",
        "\n",
        "* In der Quantenmechanik wird jeder Zustand des betrachteten physikalischen Systems durch einen zugeh√∂rigen Vektor $\\psi$ im Hilbertraum angegeben. Seine Zeitentwicklung wird nach der Schr√∂dingergleichung durch den Hamiltonoperator $\\hat{H}$ bestimmt:\n",
        "\n",
        ">$\n",
        "\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\psi(t)=\\hat{H} \\psi(t)\n",
        "$\n",
        "\n",
        "* **For every problem there is a different Hamiltonian and a different corresponding Eigenspectrum**. Spektrum: Bereich der m√∂glichen Messwerte. Eigenvalues = stabile Energielevel = Zustande, die die Elektronen in den Orbitalen beschreiben. Siehe auch [Hamilton-Funktion](https://de.m.wikipedia.org/wiki/Hamilton-Funktion).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwSGedvzNov"
      },
      "source": [
        "<font color=\"blue\">**Time Evolution Operator $\\mathcal{U}$ bzw. $\\mathcal{T}$**\n",
        "\n",
        "* Der [Zeitentwicklungsoperator](https://de.m.wikipedia.org/wiki/Zeitentwicklungsoperator) $\\mathcal{U}$ bzw. $\\mathcal{T}$ ist ein quantenmechanischer Operator, mit dem sich die zeitliche Entwicklung eines physikalischen Systems berechnen l√§sst. Siehe auch [Time evolution](https://en.m.wikipedia.org/wiki/Time_evolution)\n",
        "\n",
        "* Der quantenmechanische Operator ist eng verwandt mit dem [Propagator](https://de.m.wikipedia.org/wiki/Propagator) in der Quantenfeld- oder Vielteilchentheorie. √úblicherweise wird er als $U\\left(t, t_{0}\\right)$ geschrieben und bezeichnet die Entwicklung des Systems vom Zeitpunkt $t_{0}$ zum Zeitpunkt $t$.\n",
        "\n",
        "Der Zeitentwicklungsoperator $U\\left(t, t_{0}\\right)$ wird definiert √ºber die Zeitentwicklung eines beliebigen Zustandes $|\\psi\\rangle$ zu einem Zeitpunkt $t_{0}$ bis zum Zeitpunkt $t$ :\n",
        "\n",
        ">$\n",
        "|\\psi(t)\\rangle=U\\left(t, t_{0}\\right)\\left|\\psi\\left(t_{0}\\right)\\right\\rangle \\quad \\forall|\\psi\\rangle\n",
        "$\n",
        "\n",
        "Einsetzen in die Schr√∂dingergleichung liefert einen Satz gew√∂hnlicher Differentialgleichungen 1. Ordnung:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} U\\left(t, t_{0}\\right)=H(t) U\\left(t, t_{0}\\right)$\n",
        "\n",
        "Diese Gleichungen sind zur Schr√∂dingergleichung insofern √§quivalent, als sie die Erweiterung des Zeitentwicklungsoperators um einen infinitesimalen Zeitschritt $\\delta t$ beschreiben:\n",
        "\n",
        ">$\n",
        "U\\left(t+\\delta t, t_{0}\\right)=\\left(1-\\frac{i}{\\hbar} H(t) \\delta t\\right) U\\left(t, t_{0}\\right)+O\\left(\\delta t^{2}\\right)\n",
        "$\n",
        "\n",
        "mit dem Hamiltonoperator $H$, der den Erzeuger der Zeitentwicklungen darstellt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBWmyGiyVUq"
      },
      "source": [
        "<font color=\"blue\">**Position Operator $\\hat{x}$**\n",
        "\n",
        "Der [Ortsoperator](https://de.m.wikipedia.org/wiki/Ortsoperator) geh√∂rt in der Quantenmechanik zur Ortsmessung von Teilchen.\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch gegeben durch den zugeh√∂rigen Vektor eines Hilbertraumes $\\mathrm{H}$.\n",
        "\n",
        "* Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathrm{H}$ dargestellt.\n",
        "\n",
        "Speziell ist der Ortsoperator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{x}}=\\left(\\hat{x}_{1}, \\hat{x}_{2}, \\hat{x}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{x}_{j}\\right)=\\left\\langle\\hat{x}_{j} \\Psi, \\Psi\\right\\rangle_{\\mathrm{H}}, \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der j-ten Ortskoordinate des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_j3K_Pef4GA"
      },
      "source": [
        "<font color=\"blue\">**Momentum Operator $\\hat{p}$**\n",
        "\n",
        "Video: [Why Momentum in Quantum Physics is Complex](https://www.youtube.com/watch?v=kG-iihrYCG4&list=WL&index=22)\n",
        "\n",
        "Classical Momentum\n",
        "\n",
        "> p = m * v\n",
        "\n",
        "Quantum Mechanics: apply a measurement operator on wave function (eigenvalue equation:)\n",
        "\n",
        "> $\\hat{p}|\\Psi\\rangle=\\lambda|\\Psi\\rangle$\n",
        "\n",
        "Momentum measurement operator\n",
        "\n",
        "> $\\hat{p}=-i \\hbar \\frac{\\partial}{\\partial x}$ $\\quad$ with: $\\, i=\\sqrt{-1}$\n",
        "\n",
        "Der [Impulsoperator](https://de.m.wikipedia.org/wiki/Impulsoperator) $\\hat{p}$ ist in der Quantenmechanik der Operator zur Impulsmessung von Teilchen. In der Ortsdarstellung ist der Impulsoperator in einer Dimension gegeben durch (mit $\\frac{\\partial}{\\partial x}$ die partielle Ableitung in Richtung der Ortskoordinate $x$):\n",
        "\n",
        ">$\n",
        "\\hat{p}_{x}=-\\mathrm{i} \\hbar \\frac{\\partial}{\\partial x}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x}\n",
        "$\n",
        "\n",
        "Mit dem Nabla-Operator $\\nabla$ erh√§lt man in drei Dimensionen den Vektor:\n",
        "\n",
        ">$\n",
        "\\hat{\\mathbf{p}}=-\\mathrm{i} \\hbar \\nabla\n",
        "$\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch durch einen zugeh√∂rigen Vektor eines Hilbertraumes $\\mathcal{H}$ gegeben. Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathcal{H}$ dargestellt. Speziell ist der Impuls-Operator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{p}}=\\left(\\hat{p}_{1}, \\hat{p}_{2}, \\hat{p}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{p}_{j}\\right)=\\left\\langle\\Psi\\left|\\hat{p}_{j}\\right| \\Psi\\right\\rangle \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der $j$ -ten Komponente des Impulses des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Translation Operator $\\hat{T}$**\n",
        "\n",
        "* the translation operator is the operator that allows us to move quantum states from one point to another\n",
        "\n",
        "* it allows us to understand many properties of wave functions:\n",
        "\n",
        "> **the wave function in real space is related to the wave function in momentum space by a Fourier transform**\n",
        "\n",
        "> $[\\hat{x}, \\hat{p}]=i \\hbar$\n",
        "\n",
        "* position $\\hat{x}$ and momentum $\\hat{p}$ operators. Their most important property is their commutator which is equal to $i \\hbar$\n",
        "\n",
        "* **Translation operator**:\n",
        "\n",
        "> $\\hat{T}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} \\quad \\alpha \\in \\mathbb{R}$\n",
        "\n",
        "* this is an operator that translates by an amount $\\alpha$\n",
        "\n",
        "* What does it mean to have a **function of an operator**, like the exponential function here: the function of an operator is defined by its Taylor expansion\n",
        "\n",
        "* adjoint of an operator: tells us about what the operator looks like in the dual space (NOT hermitian as you can see):\n",
        "\n",
        "> $\\begin{aligned} \\hat{T}^{t}(\\alpha)=& e^{i \\alpha \\hat{r}^{\\dagger} / \\hbar}=e^{i \\alpha \\hat{p} / \\hbar}=e^{-i(-\\alpha) \\hat{p} / \\hbar}=\\hat{T}(-\\alpha) \\\\ & \\hat{p}^{+}=\\hat{p} \\end{aligned}$\n",
        "\n",
        "* Let's look at the action of T dagger alpha on T alpha:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=\\mathbb{1}$\n",
        "\n",
        "> $[\\hat{p}, \\hat{p}]=0$\n",
        "\n",
        "* remember: in general we cannot combine exponents of operators like if they were numbers, but here we can because the two exponents commute because the P operator commutes with itself\n",
        "\n",
        "* An operator whose adjoint is equal to its inverse is called a unitary operator\n",
        "\n",
        "> $\\left.\\begin{array}{l}\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=I \\\\ {[\\hat{p}, \\hat{p}]=0} \\\\ \\hat{T}(\\alpha) \\hat{T}^{\\dagger}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} e^{i \\alpha \\hat{p} / \\hbar}=I\\end{array}\\right\\} \\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)$\n",
        "\n",
        "* Overall:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)=\\hat{T}(-\\alpha)$"
      ],
      "metadata": {
        "id": "kHsMw4tA4gRv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscthBMzOLUM"
      },
      "source": [
        "###### *Schr√∂dinger Equation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIrMWsUSPKj2"
      },
      "source": [
        "> for fast moving electrons or electrons in electro-magnetic fields, the Schroedinger equation gives the wrong answers (see quantum field theory, spinor field)\n",
        "\n",
        "<font color=\"blue\">**From Newton's 2nd Law of Motion to the Schr√∂dinger Equation**\n",
        "\n",
        "**Step 1: Starting with Newton's 2nd law of motion**\n",
        "\n",
        "* let's assume a particle is moving along an x-axis and we apply several forces on it $\\overrightarrow{F}_1$, $\\overrightarrow{F}_2$, .. $\\overrightarrow{F}_n$\n",
        "\n",
        "* these forces depend on the position $x$ of the particle and the time elapsed $t$\n",
        "\n",
        "Then we can find the particle's position $x$ as a function of time using **Newton's 2nd law**:\n",
        "\n",
        "> $\\vec{F}_{net}=\\sum \\vec{F}_{i}(x, t)=m \\vec{a}  \\quad(\\text { Newton's 2nd law })$\n",
        "\n",
        "But the law of acceleration $\\vec{a}$ can be also written as the second time derivative of position, so we end up with a [governing equation](https://en.wikipedia.org/wiki/Governing_equation) like this, the **Equation of Motion**:\n",
        "\n",
        "> $m \\frac{d^{2} x}{d t^{2}}=\\sum_{i=1}^{n} F_{i}(x, t) \\quad(\\text { Equation of Motion })$\n",
        "\n",
        "* (another way of writing it is: $m \\ddot x = -kx$, see Colab 'Variationsrechnung')\n",
        "\n",
        "* Once we solve this equation for the particle's position, we could infer many things about the particle's state, such as its velocity, kinetic energy etc.\n",
        "\n",
        "* *Exkurs: The governing equations of a mathematical model describe how the values of the unknown variables (i.e. the dependent variables) change when one or more of the known (i.e. independent) variables change*\n",
        "\n",
        "**Step 2: Schr√∂dinger Equation & Operator /Functionals**\n",
        "* The goal of quantum mechanics is to solve the Schr√∂dinger Equation, which is very similar conceptually\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "\n",
        "* in contrast to classical equation of motion, where we solve for position and then get velocity, kinetic energy etc about particle's state, **in the Schrodinger equation we solve for wavefunction** $\\psi$! (because that is kind of it's position :)\n",
        "\n",
        "  * $\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}$ this is the Kinetic energy operator $L$ on the wavefunction $\\psi$\n",
        "\n",
        "  * $V$ represents the potential energy operator\n",
        "\n",
        "<font color=\"red\">**In quantum mechanics we use the term operator because you generally don't get fixed numerical values for kinetic and potential energy.**</font>\n",
        "\n",
        "* Instead you need to perform operations on the wavefunction to extract those kinetic and potential energy values. That's what these operators do.\n",
        "\n",
        "* You can think of Schrodinger equation as a statement of energy conservation: kinetic energy + potential energy = total energy (which is on the left side of the v)\n",
        "\n",
        "**Step 3: Wavefunction & (Dirac) Delta Function**\n",
        "\n",
        "* the wavefunction represents the state of a system - related to the probability of finding a particle at a particular region in the domain which it occupies\n",
        "\n",
        "* the square of the norm of the wavefunction gives the probability density function of a particle.\n",
        "\n",
        "* if you integrate this norm squared over the entire domain you will get 1\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$\n",
        "\n",
        "* you can't tell exactly where the position of a particle is before measurement, just a probability, same for velocity, momentum, kinetic energy etc.\n",
        "\n",
        "* But if you **apply the measurement operator, you change the wavefunction**! after one measurement, or further measurements will always get you the same result\n",
        "\n",
        "**So instead of being a probability distribution that covers multiple values, <font color=\"red\">by taking a measurement I change the wavefunction to a [delta function](https://de.wikipedia.org/wiki/Delta-Distribution) with one spike at what my measurement gave me. If I take more measurement on the same system, the delta function doesnt change.</font> This is called the wavefunction collapse.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_174.png)\n",
        "\n",
        "* However, if I let the system settle so that it eventually occupies its original wavefunction it had and then if I take my measurement, I might get something different according to the probability distribution corresponding to my original wavefunction\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_175.png)\n",
        "\n",
        "**Step 4: Partial Differential Equation**\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "* when solving a partial differential equation we also need auxiliary conditions in order to determine the unknown constants that you get from the integration process that's inherent in solving a differential equation\n",
        "\n",
        "* auxiliary conditions = initial conditions and boundary conditions\n",
        "\n",
        "* however the Schrodinger equation doesn't come with your typical boundary conditions that you might be used to seeing\n",
        "\n",
        "**Instead,the auxiliary condition we have on our solution is the normalization constraint: $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$**\n",
        "\n",
        "* Solutions to the Schrodinger equation have two be normalizable because they are wavefunctions, and in a way they represent probability density functions\n",
        "\n",
        "\t* if the solutions to Schrodinger's equation are not normalizable, we can't use them to represent a physical system\n",
        "\n",
        "\t* the trivial solution $\\psi$ (x,t) = 0 is not normalizable, because its integral from negative infinity to infinity will always be 0, it can never be 1, which is why $\\psi$ (x,t) = 0 is an unphysical solution, because the particle has to be somewhere\n",
        "\n",
        "**Step 5: Solving Schrodinger's Equation under the normalization condition**\n",
        "\n",
        "Let's say we solve Schrodinger's equation and we get following solution:\n",
        "\n",
        "> $\\Psi (x,t) = A f(x,t)$ with $A$ being an arbitrary constant\n",
        "\n",
        "* The process of normalization is to find the value of the constant $A$ so that the solution obeys the normalization condition:\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=\\int_{-\\infty}^{\\infty}|A f|^{2} d x = 1$\n",
        "\n",
        "* we might end up with a difficult task if the wavefunction is dependent on time, which means it has a different shape for different times, then wouldn't the normalization constant change with time as well?\n",
        "\n",
        "* the answer is NO!\n",
        "\n",
        "**Theorem**: If you normalize the wavefunction once then you don't need to normalize for other times = the normalization stays preserved !\n",
        "\n",
        "**Proof**:\n",
        "\n",
        "* the norm squared of a wavefunction is just the complex conjugate of that wavefunction time the wavefunction: $|\\psi|^{2}=(\\psi)^{*} \\psi$\n",
        "\n",
        "* if we go back to the Schrodinger equation we would see that the complex conjugate of the equation would be something like this with all they size turned into their conjugates and all the imaginary terms with their signs switched:\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "> $-i \\hbar \\frac{\\partial \\psi^{*}}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi^{*}}{\\partial x^{2}}+V \\psi^{*}$\n",
        "\n",
        "* the goal of the proof is to show that this normalization integral $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=$ const doesn't change with time, it stays the same:\n",
        "\n",
        "* the way to do this is to take the time derivate: if we can show that the time derivative of the normalization integral is 0 then our proof is complete $\\frac{d}{a t}\\left[\\int_{-\\infty}^{\\infty}|\\varphi|^{2} d x\\right]=0$\n",
        "\n",
        "* see complete proof in [video](https://www.youtube.com/watch?v=kUm4q0UIpio&list=WL&index=72&t=651s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HCh0gYIJv8x"
      },
      "source": [
        "<font color=\"blue\">**Schr√∂dinger Equation (Time Independent / Eigenvalue Equation)**</font>\n",
        "\n",
        "Time independent = Total energy of a system does NOT change with time\n",
        "\n",
        "**The Schroedinger equation can be written as a type of Eigenvalue equation**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle= -i \\hbar \\frac{d}{d t}|\\psi\\rangle =\\frac{\\hbar}{i} \\frac{d}{d t}|\\psi\\rangle$\n",
        "\n",
        "**Simplified (when system is not changing over time: time-independent Schroedinger equation):**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle=E |\\psi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_151.png)\n",
        "\n",
        "> $E \\Psi(x)=\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}+V \\Psi(x)$\n",
        "\n",
        "* E = **Energy the electron** is allowed to have\n",
        "\n",
        "* $\\Psi$ = **Wavefunction** (most likely position of an electron)\n",
        "\n",
        "* **Kinetic energy**: $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}$ (klassische Form: $K E=\\frac{1}{2} m v^{2}$)\n",
        "\n",
        "* **Potential energy**: $V \\Psi(x)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ELVzEm879_7"
      },
      "source": [
        "*What would a typical Schroedinger solution look like? - All the solutions to the wave function take these two forms:*\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$ when $n=1,3,5 \\ldots$ (is odd $)$\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\sin \\left(\\frac{\\pi n x}{L}\\right)$ when $n=2,4,6 \\ldots$ (is even)\n",
        "\n",
        "*Now looking at $\\psi$, the probable position of an electron:*\n",
        "\n",
        "* central question: where is the electron?\n",
        "\n",
        "* n is the energy state / level of an electron (look above at quantum numbers)\n",
        "\n",
        "* When an electron is state n=1 (its first energy state) we apply the first formula: $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$\n",
        "\n",
        "* then we get wave function for the electron that is in a given box in this case:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_147.png)\n",
        "\n",
        "* And if we square it, we get the probability distribution (the probable position of an electron):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_148.png)\n",
        "\n",
        "\n",
        "* And here some wave functions and probability densities for other energy states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_149.png)\n",
        "\n",
        "*And the solution that popped out was this:*\n",
        "\n",
        "> $E=\\frac{\\hbar^{2} n^{2} \\pi^{2}}{2 m L^{2}}$\n",
        "\n",
        "* Everything is a constant ($\\hbar$, $\\pi$, 2, m, L) or a whole number (here: n, which stands for the different states of an electron)\n",
        "\n",
        "* which means that energy E can ony have certain discrete (=quantum) values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIv5tk_B7h9Q"
      },
      "source": [
        "<font color=\"blue\">**How to solve the Schr√∂dinger Equation (Time Independent)**\n",
        "\n",
        "Assumption: particle is moving on one line between 0 and a and limited by infinite blocks left and right, where its probability is 0 and the potential energy infinite = this means that the particle can only be found on the line between 0 and a.\n",
        "\n",
        "[SOLVING the SCHRODINGER EQUATION | Quantum Physics by Parth G](https://www.youtube.com/watch?v=sPZWtZ8vt1w)\n",
        "\n",
        "**Step 1: Take Schrodinger Equation, remove $V$ and focus on differential equation of kinetic energy term**\n",
        "\n",
        "* now we want to compute the wavefunction of this particle with the (Time Independent) Schr√∂dinger Equation\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}+V \\Psi=E \\Psi$\n",
        "\n",
        "* the potential energy V is zero between 0 and a because nothing influences the particle, so it becomes this **differential equation** that we need to solve:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}=E \\Psi$\n",
        "\n",
        "* $\\frac{d^{2} \\Psi}{d x^{2}}$ is second derivative of $\\Psi$ with respect to x\n",
        "\n",
        "We want to solve this equation:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m}$ <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> $=E \\Psi$\n",
        "\n",
        "* Normally try to solve <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> which is second derivative of $\\Psi$ with respect to x, when you know what $\\Psi$ is,\n",
        "\n",
        "* but we don't. We want to go the other way around, which is trickier.\n",
        "\n",
        "**Step 2: Rearrange the constants**\n",
        "\n",
        "Luckily we have two constants in our equation (blue):\n",
        "\n",
        "> <font color=\"blue\">$\\frac{-\\hbar^{2}}{2 m}$</font> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$E$</font> $\\Psi$\n",
        "\n",
        "Which means we can rearrange the equation to this:\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$\n",
        "\n",
        "Now we can combine all constants in one constant $-k^2$ = $\\frac{-2m E}{\\hbar^{2}}$\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "**Step 3: Identify suitable function for this equation**\n",
        "\n",
        "* So which type of function obeys this relation $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$? - Would be a [sinusoid](https://de.wikipedia.org/wiki/Sinusoid)!\n",
        "\n",
        "* $\\frac{d^2 y}{d x^{2}}=-y$ - when you start with a sine and differentiate it twice you still end up with a sinusoidal term\n",
        "\n",
        "* so if we carefully account for the constants in our equation, our solution is going to look like a sinusoid:\n",
        "\n",
        "> $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ and replacing <font color=\"blue\">$\\frac{\\sqrt{2 m E}}{\\hbar}$</font> with $k$ $\\rightarrow$ $\\Psi$ = $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "*Compare this with before (above is no minus and root taken is first term):*\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$ =  <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_176.png)\n",
        "\n",
        "**Step 4: Work our the boundaries to solve the differential equation, which means to get wavefunction**\n",
        "\n",
        "\n",
        "**When x = 0 $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* so at the wall at point a $\\Psi$ = 0, so that we first derivative is not getting infinite!\n",
        "\n",
        "* this works quite nice with sinusoidal, since $\\Psi = sin(kx)$ $\\rightarrow$ 0 = sin(k(0)) = 0\n",
        "\n",
        "**When x = a $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* 0 = sin(k(a)) since $\\Psi$ = sin(kx) $\\rightarrow$ 0 = sin(k(a)) = 0\n",
        "\n",
        "* we essentially find a restriction on the kind of sine wave that we can have as a solution\n",
        "\n",
        "* for example half a sine wave is a possible solution\n",
        "\n",
        "  * it's y=0 at point x=0 and x=a (at the walls), so $sin(ka) = 0$\n",
        "\n",
        "  * so we went through half a sine wave which means that this part in brackets (ka) must be equal to 180 degrees (because that's half a sine wave) $y = \\frac{1}{2}sin(x)$\n",
        "\n",
        "  * and if we use radians instead of degrees, which is the other unit of measuring angles, and a much more natural unit of measuring angles, then 180 degrees is actually equal to <font color=\"blue\">$\\pi$ radians = (ka)</font>\n",
        "\n",
        "  * So: $y = \\frac{1}{2}sin(x) = \\pi$\n",
        "\n",
        "* this means that this equation holds true if our wavefunction is half a sine wave <font color=\"blue\">$(ka)$ = $\\pi$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$</font> , recall: k = $\\frac{\\sqrt{2m E}}{\\hbar}$\n",
        "\n",
        "* from earlier: $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ =  $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "**Step 5: Rearrange that equation to get the energy value**\n",
        "\n",
        "\n",
        "* and if we rearrange that <font color=\"blue\">$(ka)$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$ = $\\pi$ </font> we have something that tells us the value of the energy $E$:\n",
        "\n",
        "> $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* (with reduced Planck constant: $\\hbar=\\frac{h}{2 \\pi} =1.054571817 \\ldots \\times 10^{-34} \\mathrm{~J} \\cdot \\mathrm{s}$)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_178.png)\n",
        "\n",
        "* in other words: if our wavefunction looks like this (half a sine function), then the energy of our particle is this $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* another possibe solution is a full sine wave fitting into this region, just that the value at the end of the wall is 360 degrees, because we went through the whole sine wave = 2*$\\pi$ radians\n",
        "\n",
        "  * if the wavefunction looks like a whole sine wave <font color=\"blue\">$\\frac{\\sqrt{2m E}}{\\hbar}a$</font> = 2*$\\pi$\n",
        "\n",
        "  * then the  energy of the particle is $E=\\frac{4h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* We can continue doing this for lots of half sine waves, so we could have three or four half fine waves in our region - and in each case we can calculate the energy of a particle when its wavefunction looks like those sine waves.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_177.png)\n",
        "\n",
        "* This phenomenom is called \"Quantization\".\n",
        "\n",
        "  * because we can only have specific wave functions, and they correspond to specific energies, a particle can therefore only have specific energies\n",
        "\n",
        "  * so it cannot be anyhting in between and it cannot be less than the minimum of half a sine wave $E_{1}=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "  * this is also why for this particular setup cosine doesnt work (normally it does though) $\\Psi=\\cos \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "**Step 6: Normalization to get probabilities**\n",
        "\n",
        "*Normalization of the wavefunction*\n",
        "\n",
        "* there is one more thing to consider when finding a solution to the Schrodinger equation: Normalization\n",
        "\n",
        "> $\\Psi = \\sqrt{\\frac{2}{a}} \\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "* it adds a factor of $\\sqrt{\\frac{2}{a}}$ to our solution\n",
        "\n",
        "* physical meaning: if our particle is in the lowest energy level $E_1$. Then in our specific setup with the two walls the wavefunction looks like half a sine wave. And remember the wavefunction corresponds directly to the probability of us finding that particle at a particular point in space. And this relationshipn is if we square our wavefunction $|\\Psi|^2$ (we take the square modulus), then we get the probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnCYfCNzqmG9"
      },
      "source": [
        "<font color=\"blue\">**Schr√∂dinger Equation (Time Dependent)**\n",
        "\n",
        "**Consider: Difference of probability in the position basis (changes over time) and the energy basis (doesn't change):**\n",
        "\n",
        "> <font color=\"red\">**The electron is still in the same shell, represented by the principal quantum number for example, because if the electron changes the shell, energy needs to be added or removed from the overall system. however if energy stays the same, it means the electron is still in the same shell, but \"moving\" around = probability distribution of finding it somewhere in this shell changes over time which is represented by the rotation $e^{i \\frac{\\hat{H} * t}{\\hbar}}$**</font>\\\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right] \\psi(\\vec{r})=E \\psi(\\vec{r})\n",
        "$\n",
        "\n",
        "The object on the left that acts on $\\psi(x)$ is an example of an operator.\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right]\n",
        "$ = Operator\n",
        "\n",
        "In effect, what is says to do is \"take the second derivative of $\\psi(x)$, multiply the result by $-\\left(\\hbar^{2} / 2 m\\right)$ and then add $V(x) \\psi(x)$ to the result of that.\"\n",
        "\n",
        "Quantum mechanics involves many different types of operators. This one, however, plays a special role because it appears on the left side of the Schr√∂dinger equation. **It is called the Hamiltonian operator and is denoted as**\n",
        "\n",
        "> $\n",
        "\\hat{H}=-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\n",
        "$\n",
        "\n",
        "**Therefore the time-dependent Schr√∂dinger equation can be written as**:\n",
        "\n",
        "> $\n",
        "\\hat{H} \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "with $\\hat{H}$ = $(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r}))$ will be:\n",
        "\n",
        "> $\n",
        "(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})) \\; \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "bzw. rewritten:\n",
        "\n",
        "> $\\left[-\\frac{\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}+V(x, t)\\right] \\Psi(x, t) = i \\hbar \\frac{\\partial}{\\partial t} \\Psi(x, t)$\n",
        "\n",
        "bzw written in another way (single particle variant):\n",
        "\n",
        "> <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> + <font color=\"green\">$V(x, t)$</font> ]\n",
        " <font color=\"blue\">$|\\psi\\rangle$</font> = $i \\hbar \\frac{\\partial}{\\partial t}$ <font color=\"blue\">$|\\psi\\rangle$</font>\n",
        "\n",
        "* <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> Kinetic energy\n",
        "\n",
        "* <font color=\"green\">$V(x, t)$</font> Potential energy\n",
        "\n",
        "* <font color=\"blue\">$|\\psi\\rangle$</font>  the wave function\n",
        "\n",
        "<font color=\"blue\">*Schrodinger equation: Derivation and how to use it (in Time Evolution)*\n",
        "\n",
        "Important rules of physics:\n",
        "\n",
        "* Conservation of energy -> deeply integrated into Schrodinger equation\n",
        "* total energy doesn't change\n",
        "* you can't make of destroy energy\n",
        "\n",
        "**Since we can write a quantum state $|\\Psi \\rangle$ in whatever basis we want, we can choose the energy Eigenbasis**. <font color=\"red\">what is meant by that? is that the principle quantum number for example</font>\n",
        "\n",
        "* We can write a state as the superposition of different energies.\n",
        "\n",
        "* And if we measure the energy of the particle it will be one of these with their probability\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* with probability for example $|\\beta|^2$ for measuring second state\n",
        "\n",
        "**Say the state evolves in time, in other words we apply the time evolution $U$ (or $T$) to $|\\Psi \\rangle$, so $T |\\Psi \\rangle$**\n",
        "\n",
        "* what condition do we want to impose on the new energies of the state?\n",
        "\n",
        "* In other words: how we want conservation of energy to look in quantum mechanics?\n",
        "\n",
        "**Let's start where a particle just has one energy $E$ when we start**\n",
        "\n",
        "* means: it is an energy Eigenstart !!\n",
        "\n",
        "* we evolve it forward in time and look at the energy of the new state. That energy should be also $E$, otherwise energy wouldn't be conserved (Like in classical mechanics).\n",
        "\n",
        "> $|\\Psi \\rangle$ = $|1 \\rangle$ $\\rightarrow$ $T|\\Psi \\rangle$\n",
        "\n",
        "* Now also the average energy shouldn't change after some time, otherwise the energy wouldn't be conserved either.\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* if you measured athe particle's energy initially with a certain probability $|\\beta|^2$, and then after time evolution again, it should be the same probability to measure that energy!\n",
        "\n",
        "* this is so strong, it gives us the schroedinger equation\n",
        "\n",
        "\n",
        "**We need to how the coefficients have changed in the new equation after time evolution**:\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$ (before)\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $\\alpha' |\\Psi \\rangle + \\beta' |\\Psi \\rangle + \\gamma' |\\Psi \\rangle$ (after)\n",
        "\n",
        "* We want the probability to be the same, but that probability is just the lenght of this compex number squared $|\\gamma|^2 = |\\gamma'|^2 = 1$\n",
        "\n",
        "> <font color=\"red\">**So each coefficient can be represented as an arrow with equal length $|\\gamma|^2$ and $|\\gamma'|^2$ (hence the probability of measuring that energy this state is still the same!!), BUT $|\\gamma'|^2$ may be rotated by an angle $\\phi$**. This angle is new vector = rotation * old vector:</font>\n",
        "\n",
        "> <font color=\"red\">$\\gamma' = e^{i\\phi}\\gamma$</font>\n",
        "\n",
        "Let's plug that rotation $e^{i\\phi}$ in to our previous equation:\n",
        "\n",
        "> <font color=\"red\">$T |\\Psi \\rangle$ = $e^{i\\phi_1}\\alpha |\\Psi \\rangle + e^{i\\phi_2}\\beta |\\Psi \\rangle + e^{i\\phi_3}\\gamma |\\Psi \\rangle$</font>\n",
        "\n",
        "* where the angles / rotations $e^{i\\phi}$ are different for every energy = they are all rotated by a different amount !! Otherwise the rotation can be brought out and present and future state would be the essentially same:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}\\alpha |\\Psi \\rangle + e^{i\\phi}\\beta |\\Psi \\rangle + e^{i\\phi}\\gamma |\\Psi \\rangle$ = $e^{i\\phi} (\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle)$ (this is showing that it's wrong!)\n",
        "\n",
        "The overall rotation wouldn't affect any measurement outcomes. Means no matter in which crazy situation you brought the particle in, it does nothing, which can't be right.\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}|\\Psi \\rangle$ (this is showing that it's wrong!\n",
        "\n",
        "\n",
        "* also the amount of rotation depends on time (little going forwardf = little rotation). That suggests the right amount of angle to rotate is Energy x Time. Plus some constants to deal with units and scaling etc.\n",
        "\n",
        "> <font color=\"red\">$\\phi = \\frac{E * t}{\\hbar}$</font>\n",
        "\n",
        "* And that's what the Schroedinger equation will tell you will happen to the state:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i \\frac{E * t}{\\hbar}}\\alpha |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\beta |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\gamma |\\Psi \\rangle$\n",
        "\n",
        "* And that's the same: (with $\\hat{H}$ for energy measurement operator, Hamiltonian):\n",
        "\n",
        "> <font color=\"red\">$T(t) |\\Psi \\rangle = e^{i \\frac{\\hat{H} * t}{\\hbar}}|\\Psi \\rangle$</font>\n",
        "\n",
        "*Common result for 2 observations:*\n",
        "\n",
        "Time Evolution per each step, observer 1:\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_1$ = $e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$</font>\n",
        "\n",
        "Time Evolution at the end for observer 2 (not seeing time step 1):\n",
        "\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font>\n",
        "\n",
        "Where:\n",
        "\n",
        "> <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font> = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$\n",
        "\n",
        "Why? - because our angle of rotation depends on $t$ (and not $t^2$ or anything): $T\\left(t_{1}+t_{2}\\right)=T\\left(t_{2}\\right) T\\left(t_{1}\\right)$\n",
        "\n",
        "Taken from [Schrodinger equation comment response and homework answers video](https://www.youtube.com/watch?v=M_2h5uQ0SIc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Harmonic Oscillator (Hamiltonians / Solution of Schrodinger equation)**\n",
        "\n",
        "* The nuclear motion Schr√∂dinger equation can be solved in a space-fixed (laboratory) frame, **but then the translational and rotational (external) energies are not accounted for**. Only the (internal) atomic vibrations enter the problem.\n",
        "\n",
        "* Further, for molecules larger than triatomic ones, it is quite common to introduce the **harmonic approximation, which approximates the potential energy surface** as a [quadratic function](https://en.m.wikipedia.org/wiki/Quadratic_function) of the atomic displacements. **This gives the harmonic nuclear motion Hamiltonian**.\n",
        "\n",
        "* Making the harmonic approximation, we can **convert the Hamiltonian into a sum of uncoupled one-dimensional [harmonic oscillator](https://en.m.wikipedia.org/wiki/Harmonic_oscillator) Hamiltonians**.\n",
        "\n",
        "> **The one-dimensional harmonic oscillator is one of the few systems that allows an exact solution of the Schr√∂dinger equation.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Nullpunktsenergie#Harmonischer_Oszillator\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Harmonischer_Oszillator_(Quantenmechanik)"
      ],
      "metadata": {
        "id": "EKeKuaJkf4Pw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sf5tauFtttx"
      },
      "source": [
        "###### ***Matrix Mechanics*** *(Heisenberg, discrete basis, spin representation, Kronecker delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Mechanics (spin representation - discrete basis) - (Heisenberg)**\n",
        "\n",
        "* Matrix Mechanics (spin representation - discrete basis)\n",
        "\n",
        "* Video: [Matrix formulation of quantum mechanics](https://www.youtube.com/watch?v=wIwnb1ldYTI)\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin representation).**\n",
        "\n",
        "* **Matrix formulation of quantum mechanics reduces to the rules of simple matrix multiplication.**\n",
        "\n",
        "> $\\hat{A}=\\sum_{i j} A_{i j}\\left|u_{i}\\right\\rangle\\left\\langle u_{j}\\right| \\quad A_{i j}=\\left\\langle u_{i}|\\hat{A}| u_{j}\\right\\rangle$\n",
        "\n",
        "* An operator A can be written in the u basis as the sum over the outer products of the basis states\n",
        "* And the expansion coefficients Aij are given by the matrix elements of A with respect to the basis states\n",
        "* The expansion coefficients for an operator are labeled by 2 indices, so we will arrange them in a form of a square matrix, with the first index denoting the row of the matrix and the second index the column of the matrix\n",
        "* There operators are written as matrices\n",
        "\n",
        "> $\\left(\\begin{array}{ccccc}A_{11} & A_{12} & \\cdots & A_{1 j} & \\cdots \\\\ A_{21} & A_{22} & \\cdots & A_{2 j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\\\ A_{i 1} & A_{i 2} & \\cdots & A_{i j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\end{array}\\right)$\n",
        "\n",
        "Kets are written as column vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_218.png)\n",
        "\n",
        "Matrix formulation of Bra's: re-arrange complex, conjugate coefficient as a row vector:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_219.png)\n",
        "\n",
        "An operator A can be written in the u basis as the sum over the outer products of the basis states. An Aij are the expansion coefficient in this case. Operators are written as matrices:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_220.png)\n",
        "\n",
        "Summary of the matrix formulation of quantum mechanics for kets, bras and operators:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_221.png)\n",
        "\n",
        "We will see how simple matrix multiplication rules work for the following 4 operations:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_222.png)\n",
        "\n",
        "First, in the matrix formulation of quantum mechanics, a bracket is the matrix product of a row vector with a column vector, and gives a scalar:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_223.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_224.png)\n",
        "\n",
        "Adjoint operator: describe the action of an operator in the dual space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_225.png)\n",
        "\n",
        "Write an operator as an outer product of two states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_226.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_227.png)"
      ],
      "metadata": {
        "id": "FdyLdYgFeqFK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdJ5yNTZibYy"
      },
      "source": [
        "###### ***Wave Mechanics*** *(Schr√∂dinger, continuous basis, position representation, Dirac delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIaQDNv0C1c"
      },
      "source": [
        "**First: let's go from discrete basis $u_i$ to continuous basis $v_{\\alpha}$**\n",
        "\n",
        "* Much used **discrete basis**: Spin of quantum particles\n",
        "\n",
        "* Much used **continuous basis**: position of quantum particles (this one leads to the idea of wave function)\n",
        "\n",
        "> **The generalization is straightforward: it amounts to replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$ of two continuous variables and sum over these indices by integrals over continuous indices**.\n",
        "\n",
        "* first concept: we work with an orthonormal basis $\\left\\langle u_{i} \\mid u_{j}\\right\\rangle=\\delta_{i j}$. replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$\n",
        "\n",
        "* then we look at the expansion of Ket in a particular basis: replacing a sum over i with an integral over alpha\n",
        "\n",
        "* in yellow: just a proof why we would write a Dirac delta function only under an integral sign\n",
        "\n",
        "* last part: representation of an operator in a particular basis (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_217.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWBTMFaxI7Ie"
      },
      "source": [
        "**Wave Mechanics (Schr√∂dinger)**\n",
        "\n",
        "* Wave Mechanics: Wave Function (position representation - continuous basis)\n",
        "\n",
        "* Check also part under Operator: Translation Operators (**Wave Mechanics: Translation Operator**)\n",
        "\n",
        "* Video: [Wave functions in quantum mechanics](https://www.youtube.com/watch?v=2lr3aA4vaBs)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Wave_function\n",
        "\n",
        "* wave functions is one possible way looking at a quantum system\n",
        "\n",
        "* is the so called position representation of quantum mechanics\n",
        "\n",
        "* leads to wave mechanics (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_232.png)\n",
        "\n",
        "Computing scalar between two states and the normalization:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_233.png)\n",
        "\n",
        "How to get from the position representation to the momentum representation (via a first order differential equation):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_234.png)\n",
        "\n",
        "**Transformation matrix $\\langle x|p\\rangle$ to go from the position representation to the momentum representation**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_235.png)\n",
        "\n",
        "To change between both representations we use Fourier transform:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_236.png)\n",
        "\n",
        "If we go to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_237.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_238.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XCE4EGuJJij"
      },
      "source": [
        "**Wave Mechanics: Position and momentum operators acting on wave functions**\n",
        "\n",
        "* Video: [Position and momentum operators acting on wave functions](https://www.youtube.com/watch?v=Yw2YrTLSq5U)\n",
        "\n",
        "* Action of position and momentum operators on wave functions\n",
        "\n",
        "* The action of the position operator: it multiplies a wave function by x:\n",
        "\n",
        "> $x \\psi(x)$\n",
        "\n",
        "* the action of a momentum operator: it acts by calculating the derivative of a wave function\n",
        "\n",
        "> $-i \\hbar \\frac{d \\psi(x)}{d x}$\n",
        "\n",
        "* First: describe the act of a position operator in the position basis, and the act of a momentum operator in the momentum basis:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_239.png)\n",
        "\n",
        "Second: describe the act of the momentum operator on a state Psi when written in the position representation (=basis) is such that the momentum operator calculates the derivative of the wavefunction and then multiplies the result by minus i h-bar (we need the translation operator):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_240.png)\n",
        "\n",
        "Third: what happens when we act with the position operator in the momentum basis (proof: the momentum space wavefunction is related to the real space wavefcunction by a Fourier transform):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_241.png)\n",
        "\n",
        "Generalize this to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_242.png)\n",
        "\n",
        "Summary: as you can see there are different representations for the same thing, but the maths is differently difficult. So the task is to find a representation that is easy for a certain problem:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_243.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-w7Fv4O91T"
      },
      "source": [
        "###### *Wavefunction & Global Phase Difference (Why we factor it out and leave phase difference only)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIwU3Qnjifn-"
      },
      "source": [
        "**How does the Wavefunction (one type) look like?**\n",
        "\n",
        "> $\\psi=e^{\\frac{1}{\\hbar}(px - Et)}$\n",
        "\n",
        "* p = momentum in direction x, x = position along x direction, E = energy, t = time\n",
        "\n",
        "* e is the exponential function, normally it doesn't look like a wave, like $e^{-x}$ or $e^{x}$\n",
        "\n",
        "* but the imaginary number $i=\\sqrt{-1}$ turns an exponential function into a wave\n",
        "\n",
        "* sinoisdal functions (sine and cosine) can be written in terms of the exponential function with $i$ in the exponent\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_244.png)\n",
        "\n",
        "* we could take one complex wave function and break it down into simpler waves, then we apply same maths on the simpler waves:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_245.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVcEGLc6rS9p"
      },
      "source": [
        "Global phase factor $e^{i\\theta}$:\n",
        "\n",
        "* Eigenvalues and Eigenvectors exist also in other vector spaces than state spaces, but because state space is a complex vector space there is one important extra subtlety compared to real vector spaces which has to do with the global phase factor $e^{i\\theta}$\n",
        "\n",
        "* after choosing alpha to make the length of an Eigenstate equal to 1, we still have some extra freedom in the Eigenstate\n",
        "\n",
        "* multiplying $|\\Psi\\rangle$ with a Global phase factor $e^{i\\theta}$ makes the length of the resulting $|\\Psi'\\rangle$ still = 1\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_258.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens when we square the wave function:\n",
        "* the function is an oscillation in quantum possibilities moving through space and time\n",
        "* but it's a complex wave with one real and one imaginary component\n",
        "* **the components oscillate in sync with each other - but they are offset, shifted in phase by a constant amount**\n",
        "> phase is just the wave's current state in its up-down oscillation\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0964.png)"
      ],
      "metadata": {
        "id": "KiDe4HzbgNh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we apply the Born rule we are squaring these two waves and adding them together\n",
        "* but it turns out that this value doesn't depend on phase. The magnitude squared of the real and imaginary components stays the same, even as those components move up and down\n",
        "* It is that magnitude squared that we can observe, it determines the particles position\n",
        "* the phase itself is fundamentally unobservable. You can shift phase by any amount and you wouldnt change the resulting position of the particle, as long as you do the same shift to both the real and the imaginary components.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0965.png)\n"
      ],
      "metadata": {
        "id": "T6Kn28ZZgUsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact as long as you make the same shift across the entire wave function, all the observables are unchanged.\n",
        "* We call this form of transformation a global phase shift, and it's analogous to transforming our altitude zero point up or down by the same amount everywhere.\n",
        "* the equations of quantum mechanics have what we call **global phase invariance**\n",
        "* **Global phase is a Gauge symmetry of the system**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0966.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "70R04sWsgbFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminder: Trigonometrie:\n",
        "\n",
        "> $\\begin{aligned} \\mathrm{e}^{\\mathrm{i} x} &=\\sum_{k=0}^{\\infty} \\frac{(\\mathrm{i} x)^{k}}{k !}=\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l}}{(2 l) !}+\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l+1}}{(2 l+1) !} \\\\ &=\\underbrace{\\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l}}{(2 l) !}}_{\\cos x}+\\underbrace{\\mathrm{i} \\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l+1}}{(2 l+1) !}}_{\\sin x} \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos x+\\mathrm{i} \\sin x \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos \\varphi+\\mathrm{i} \\sin \\varphi \\\\ \\mathrm{e}^{\\mathrm{i} x}&=x+\\mathrm{i} y \\end{aligned}$ Das ist die sogenannte [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel)!\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/600px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_040.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/3/3b/Circle_cos_sin.gif)"
      ],
      "metadata": {
        "id": "QX7SeO65gi0O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBGbzlHLPGlp"
      },
      "source": [
        "* The only reason phase is important is because it brings about interference effects. And interference effects are only dependent on the difference in phase between the two waves (we will abstract basis states to waves for now).\n",
        "\n",
        "> **Therefore, we can say that it‚Äôs the difference that counts, and not the absolute value.**\n",
        "\n",
        "* For example, if the phase difference is œÄ radians then the waves would cancel each other out.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sd8WkT9PbZX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_179.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8WPjzqBPfSd"
      },
      "source": [
        "We can conclude that the absolute value of the phase shift for both waves is meaningless to the interference. For as long as they both retain the phase difference, then the interference effect will be constant, and that‚Äôs what matters!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOiHZhJKPkMX"
      },
      "source": [
        "**Why the global phase doesn‚Äôt matter**\n",
        "\n",
        "As observed above, what matters is the phase difference. So think about it, if you‚Äôre describing the phase of two waves, it‚Äôs redundant to state both phases. **The better approach is to just state the phase difference**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjLdM9G6PojT"
      },
      "source": [
        "* The global phase is the absolute value of the phase shift for both waves.\n",
        "\n",
        "* For example, wave one and two each have a phase of (œÄ/2) radians and (3œÄ/2) radians, respectively. In this case, the phase (œÄ/2) is a global phase since the phase difference (what actually counts) is œÄ radians ‚Üí (3œÄ/2 - œÄ/2 ).\n",
        "\n",
        "* Using the same example, we can define the relative phase (also known as the local phase) as the phase difference (œÄ rad)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUI_HZzP43M"
      },
      "source": [
        "We‚Äôve found a method to **bypass the need for a fourth dimension by factoring out the global phase** and replacing the second phase with the relative phase. We can represent this mathematically.\n",
        "\n",
        "A general qubit state can be written\n",
        "\n",
        "> $|\\psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$\n",
        "\n",
        "with complex numbers, Œ± and Œ≤, and the normalization constraint require that:\n",
        "\n",
        "\n",
        "> $|\\alpha|^{2}+|\\beta|^{2}=1$\n",
        "\n",
        "As previously stated, we can express the amplitudes in polar coordinates as (General equation for a qubit state):\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha} e^{i \\phi_{\\alpha}}|0\\rangle+r_{\\beta} e^{i \\phi_{\\beta}}|1\\rangle$\n",
        "\n",
        "with four real parameters:\n",
        "\n",
        "> $r_{\\alpha}, \\phi_{\\alpha}, r_{\\beta}$ and $\\phi_{\\beta}$\n",
        "\n",
        "**However, the only measurable quantities are the probabilities |Œ±|¬≤ and |Œ≤|¬≤, so multiplying the state by an arbitrary factor $e^{iŒ≥}$ (global phase) has no observable consequences, because**:\n",
        "\n",
        "> $\\left|e^{i \\gamma} \\alpha\\right|^{2}=\\left(e^{i \\gamma} \\alpha\\right)^{*}\\left(e^{i \\gamma} \\alpha\\right)=\\left(e^{-i \\gamma} \\alpha^{*}\\right)\\left(e^{i \\gamma} \\alpha\\right)=\\alpha^{*} \\alpha=|\\alpha|^{2}$\n",
        "\n",
        "Therefore, we can factor out $e^{iŒ¶}$ from the general equation:\n",
        "\n",
        "> $|\\psi\\rangle=e^{i \\phi_{\\alpha}}\\left(r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "Now, if you calculate the amplitude |œà|¬≤, the factor ($e^{iŒ¶_Œ±}$) in front will vanish by the argument above. This is why we called it the global phase. However, the relative phase is the phase difference noted as (Œ¶_Œ± - Œ¶_Œ≤). This is an observable-ish quantity which manifests through interference effects.\n",
        "\n",
        "Let‚Äôs consolidate the above equation into:\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i \\phi}|1\\rangle$\n",
        "\n",
        "> $r_{\\alpha} \\in \\mathbb{R}, r_{\\beta} \\in \\mathbb{R}, \\phi \\in \\mathbb{R} \\mid \\phi=\\phi_{\\beta}-\\phi_{\\alpha}$\n",
        "\n",
        "where r_Œ±, r_Œ≤ and Œ¶ all real parameters.\n",
        "\n",
        "**Notice that this equation can be represented in 3-D, as the global phase is gone.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odWaJ8hJPDMa"
      },
      "source": [
        "https://pavanjayasinha.medium.com/but-what-is-a-quantum-phase-factor-d05c15c321fe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8P-evW3U0Vx"
      },
      "source": [
        "**Physical Meaning of Phase**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Remember: $e^{2 \\pi i}$ = 1 (Identity)"
      ],
      "metadata": {
        "id": "lQpy1dj0iHji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNcqwo6R4lV"
      },
      "source": [
        "* In quantum mechanics, a phase factor is a complex coefficient $e^{i \\theta}$ that multiplies a ket $|\\psi\\rangle$ or bra $\\langle\\phi|$.\n",
        "\n",
        "* <font color=\"blue\">**It does not, in itself, have any physical meaning**, since the introduction of a phase factor does not change the expectation values of a Hermitian operator.\n",
        "\n",
        "> That is, the values of $\\langle\\phi|A| \\phi\\rangle$ and $\\left\\langle\\phi\\left|e^{-i \\theta} A e^{i \\theta}\\right| \\phi\\right\\rangle$ are the same.\n",
        "\n",
        "* <font color=\"red\">However, differences in phase factors between two interacting quantum states can sometimes be measurable (such as in the Berry phase) and this can have important consequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syr76IkvRoPD"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Phase_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T3nptBTPNo"
      },
      "source": [
        "* When people say that the phase doesn't matter, they mean the overall, \"global\" phase. In other words, the state $|0\\rangle$ is equivalent to $e^{i \\theta}|0\\rangle$, the state $|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime}}|1\\rangle$, and the state $|0\\rangle+|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime \\prime}}(|0\\rangle+|1\\rangle)$.\n",
        "\n",
        "> Siehe auch Eulersche Formel: $e^{i \\phi}$ https://mathepedia.de/Eulersche_Formel.html\n",
        "\n",
        "> Note that \"equivalence\" is not preserved under addition, since $e^{i \\theta}|0\\rangle+e^{i \\theta^{\\prime}}|1\\rangle$ is not equivalent to $|0\\rangle+|1\\rangle$, because there can be a relative phase $e^{i\\left(\\theta-\\theta^{\\prime}\\right)}$.\n",
        "\n",
        "* If we wanted to describe this very simple fact with unnecessarily big words, we could say something like \"the complex projective Hilbert space of rays, the set of equivalence classes of nonzero vectors in the Hilbert space under multiplication by complex phase, cannot be endowed with the structure of a vector space\".\n",
        "\n",
        "* Because the equivalence doesn't play nicely with addition, **it's best to just ignore the global phase ambiguity whenever you're doing real calculations**. Finally, when you're done with the entire calculation, and arrive at a state, you are free to multiply that final result by an overall phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBLDuK4vRpXR"
      },
      "source": [
        "https://physics.stackexchange.com/questions/552796/the-importance-of-the-phase-in-quantum-mechanics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hew4HucSHKJ"
      },
      "source": [
        "* Phase: Any one point or portion in a recurring series of changes, as in the changes of motion of one of the particles constituting a wave or vibration; one portion of a series of such changes, in distinction from a contrasted portion, as the portion on one side of a position of equilibrium, in contrast with that on the opposite side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxiSY8ddRqi1"
      },
      "source": [
        "https://courses.lumenlearning.com/boundless-chemistry/chapter/orbital-shapes/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CThWPZdFEXlO"
      },
      "source": [
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hamiltonian Simulation (Trotterization, Qubitization, Imaginary time evolution)*"
      ],
      "metadata": {
        "id": "GuoalDqRyRyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trotterization**\n",
        "\n",
        "https://vtomole.com/blog/2019/04/07/trotter"
      ],
      "metadata": {
        "id": "iOtuSjOEybKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system"
      ],
      "metadata": {
        "id": "yO5fzJsOye7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qubitization**\n",
        "\n",
        "https://arxiv.org/abs/1610.06546"
      ],
      "metadata": {
        "id": "Ph9a0fu1y9Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Machine Learning*"
      ],
      "metadata": {
        "id": "p9cyxvqKIhGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Block Encoding*"
      ],
      "metadata": {
        "id": "BLIqNf8wLAzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.wikipedia.org/wiki/Gibbs-Sampling"
      ],
      "metadata": {
        "id": "Sd2SdVyZPSrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "Arxiv: [Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics](https://arxiv.org/abs/1806.01838)\n",
        "\n",
        "In HHL f(x) = 1/x. Use Singular Value Transformation to approximate it! https://www.youtube.com/watch?v=L40UUDxPEbE&list=WL&index=3&t=215s\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18197/in-the-context-of-block-encoding-what-does-0-rangle-otimes-i-represent\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "7XFH8tKPLEJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Essentials*"
      ],
      "metadata": {
        "id": "t0wVu7UaIENk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Machine Learning: what do you mean?**\n",
        "\n",
        "* Enhance classical or physics\n",
        "* Classical vs quantum native data\n",
        "* Discriminative vs generative approaches\n",
        "* NISQ hybrid (annealing, variational) vs error-corrected\n",
        "* Kernel-methods vs neural nets\n"
      ],
      "metadata": {
        "id": "Ye7VLc7kH_Tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open Questions in Quantum Machine Learning**\n",
        "* Are quantum circuits actually useful models? (can they help generalize?)\n",
        "* How can we benchmark them?\n",
        "* Is there a connection between quantum theory and deep learning?\n",
        "* How does noise impact applications?\n",
        "* What data domains is QML good for?\n",
        "* Do OML ideas scale?\n",
        "* What optimization strategies work for quantum circuits?\n",
        "* Which circuit architectures are good for ML?"
      ],
      "metadata": {
        "id": "GSB6Zq9ZQLmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems with traditional quantum computing applied to QML**\n",
        "\n",
        "* Check how to apply Grovers search to machine learning or hot to speed up a neural network.\n",
        "* First, it talks about runtime speedups, but doesn't care about practicability (100Mio years vs forever to train is a strong speedup, but useless in practical machine learning)\n",
        "* Second, too much focus on if something is provable: lots of things in ML are not provable. optimization training is np-hard. for quantum computing it's unsolvable. Most of what we do in machine learning is not provable, it just works.\n",
        "* Near time quantum computing is on the other side too empirical: this and that is faster than if we run it on a small classical neural network. applied experiments on small regimes that no one cares about (a few qubits)\n",
        "* third step: develop a theory about QML, taken from classical ML\n",
        "> **Goal of machine learning is generaliation, not speedup** But we theoretical need to understand what's generalization, because we can't just try, QCs are too small and noisy."
      ],
      "metadata": {
        "id": "tj__oF4_Wcfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differentiation**\n",
        "\n",
        "* **Data classical and algorithm classical**:\n",
        "  * using classical methods to understand quantum physics for classical machine learning\n",
        "  * using tensor networks (developed to understand and calculate with quantum states for neural networks)\n",
        "  * methods of using neural networks to represent quantum states\n",
        "* **Data quantum and algorithm classical**:\n",
        "  * data from quantum experiments\n",
        "  * quantum error correction with machine learning\n",
        "  * interpret or shortcut [quantum state tomography](https://en.m.wikipedia.org/wiki/Quantum_tomography) with machine learning\n",
        "* **Data quantum and algorithm quantum**:\n",
        "  * we get quantum data, but not in terms of measurements but actually the  states of quantum data\n",
        "  * you get a cryptography experiment and you get your photons out. and now you can send those photons into a photonic quantum computer and can process them into a quantum machine learning on the quantum data.\n",
        "* **Data classical and algorithm quantum**:\n",
        "  * how to use quantum computers to do machine learning.\n",
        "  * the most common understanding of quantum machine learning\n"
      ],
      "metadata": {
        "id": "qaVQzNVxQ3zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "8A77f2AkHlWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qhat Circuit should I use?**\n",
        "\n",
        "\n",
        "* at the moment we don't know what circuit would be the best in QML, no theoretical foundation\n",
        "* i.e. tensor networks\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1409.png)"
      ],
      "metadata": {
        "id": "5zMe2MdUJNnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Approach**\n",
        "\n",
        "* in classical ML pipeline what you can get from a quantum computer is information about the gradients, what are partial derivatives of node/ quantum computation with respect to its parameters?\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1411.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1412.png)\n",
        "\n",
        "https://arxiv.org/abs/1909.02108 Quantum Natural Gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "LvFX3r1INUJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pRIE-on-H6D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum advantage in learning from experiments*"
      ],
      "metadata": {
        "id": "kYZ-u0aXqvlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://www.ipam.ucla.edu/programs/long-programs/mathematical-and-computational-challenges-in-quantum-computing/?tab=application"
      ],
      "metadata": {
        "id": "gqPzu4RmqL77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Video: [Quantum advantage in learning from experiments](https://www.youtube.com/watch?v=HY7IhKN03Vk&list=WL&index=3)\n",
        "\n",
        "* Paper [Quantum advantage in learning from experiments](https://arxiv.org/abs/2112.00778)\n",
        "\n",
        "* When can a quantum computer be replaced by a dataset?\n",
        "\n",
        "* See also: [The Power of data in quantum machine learning](https://arxiv.org/abs/2011.01938)\n",
        "\n",
        "* What kinds of problems are learnable from a little data?: [Provably efficient machine learning for quantum many-body problems](https://arxiv.org/abs/2106.12627): for interesting problems like learning ground states, in some cases a few pieces of data are sufficient to make interesting predictions about  ground state properties. - Is the fate of quantum computers to provide training data for classical models?\n",
        "\n",
        "* [Quantum Algorithmic Measurement](https://arxiv.org/abs/2101.04634) and [Information-theoretic bounds on quantum advantage in machine learning](https://arxiv.org/abs/2101.02464): quantum computers interfacing with the quantum parts of our world. you can't subsitute that classical computation in the same way that you can perhaps do in other places.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cty3agl-qx23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Natural Sciences Applications*"
      ],
      "metadata": {
        "id": "bIoYM5RYq5BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is quantum data?\n",
        "2. Use cases\n",
        "3. Quantum sensing\n",
        "4. Quantum agent vs classical agent (quantum advantage in learning from experiments)"
      ],
      "metadata": {
        "id": "yNuWNGCTBi8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A review on **classical machine learning methods to characterize quantum systems**, stemmed from a wide collaboration of experts in the field, is now out in its final revision on Nat Rev Phys! Very proud to have been part of the team :)\n",
        "\n",
        "https://arxiv.org/pdf/2207.00298.pdf\n",
        "\n",
        "https://www.nature.com/articles/s42254-022-00552-1"
      ],
      "metadata": {
        "id": "ATMKteAxUvFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications: **Detecting dark matter with quantum computers**\n",
        "\n",
        "* When dark matter particles traverse a strong magnetic field, they may produce photons that Chou and his team can measure with superconducting qubits inside aluminum photon cavities. Because the qubits have been shielded from all other outside disturbances, when scientists detect a disturbance from a photon, they can infer that it was the result of dark matter flying through the protective layers.\n",
        "\n",
        "* ‚ÄúThese disturbances manifest as errors where you didn‚Äôt load any information into the computer, but somehow information appeared, like zeroes that flip into ones from particles flying through the device,‚Äù he said.\n",
        "\n",
        "https://news.fnal.gov/2022/12/detecting-dark-matter-with-quantum-computers/"
      ],
      "metadata": {
        "id": "f_u86pL8IBsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications: **Generative Machine Learning with Classical Data**\n",
        "\n",
        "* Quantum Advantage with QML Models:\n",
        "\n",
        "* GANs (famous for collapsing) and VAE (issue of memorizing data and overfitting)\n",
        "\n",
        "* Probabilistic generative models seems a more prominent candidate for quantum advantage.\n",
        "\n",
        "* https://www.youtube.com/watch?v=uGpbP1tRDyQ&list=WL&index=3&t=465s\n",
        "\n",
        "* https://arxiv.org/abs/1708.09757\n",
        "\n",
        "* problem: lack of general metric (also in classical ML), for example \"inception score\" only works for images\n",
        "\n",
        "* Many of the evaluation techniques, e.g., divergences, favor memorization\n",
        "  - SOA models, e.g., VAE and GAN don't possess tractable likelihood functions.\n",
        "  - Scales badly in high dimensions\n",
        "Obscures distinct mode of failure into a single (uninterpretable) number\n",
        "\n",
        "* Possible solution: https://arxiv.org/abs/2201.08770 Evaluating Generalization in Classical and Quantum Generative Models\n",
        "\n",
        "* https://learnopencv.com/variational-autoencoder-in-tensorflow/\n",
        "\n",
        "* VAE in Astrophysics: We propose to use generative models based on deep neural networks, namely variational autoencoders (VAE), to learn probabilistic models directly from data. We train a VAE on images of centred, isolated galaxies, which we reuse, as a prior, in a second VAE-like neural network in charge of deblending galaxies. https://academic.oup.com/mnras/article/500/1/531/5919458"
      ],
      "metadata": {
        "id": "i03C7gyqLwfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Barren Plateaus*"
      ],
      "metadata": {
        "id": "wdOoLdM0Fp_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical methods to overcome vanishing gradient:\n",
        "\n",
        "* Better initialization\n",
        "* ResNet networks with skip connections\n",
        "* ReLU activation function"
      ],
      "metadata": {
        "id": "KFrb7_MvrXtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pennylane.ai/qml/demos/tutorial_barren_plateaus.html"
      ],
      "metadata": {
        "id": "5twlzWBHIQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic solution to solve barren Plateaus**\n",
        "\n",
        "Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - Overparamerization (too much capacity) arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren‚Äôt needed anymore. Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g.\n",
        "\n",
        "\n",
        "From: [QHack 2022: Marco Cerezo ‚ÄîBarren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "Exponentiate Lie algebra to get lie groups:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)"
      ],
      "metadata": {
        "id": "Vljfs_A8Fr9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use unbounded objective function**\n",
        "\n",
        "* Other barren plateaus also don't apply for unbounded objective function. Almost all of QML uses bounded operators.\n",
        "* KL divergence in classical, would be quantum relativ entropy, but that's too hard to compute. better: Maximal Quantum R√©nyi Divergence\n",
        "* compute with Extended swap test (Generalizes swap test and Hadamard test)\n",
        "* Learning thermal states: Generative algorithm to thermal state\n",
        "learning,Access to LCU decomposition of the Hamiltonian\n",
        "\n",
        "Video: [Maria Kieferova - Training quantum neural networks with an unbounded loss function - IPAM at UCLA](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "Abstract: Quantum neural networks (QNNs) are a framework for creating quantum algorithms that promises to combine the speedups of quantum computation with the widespread successes of machine learning. A major challenge in QNN development is a concentration of measure phenomenon known as a barren plateau that leads to exponentially small gradients for a range of QNNs models. In this work, we examine the assumptions that give rise to barren plateaus and show that an unbounded loss function can circumvent the existing no-go results. We propose a training algorithm that minimizes the maximal Renyi divergence of order two and present techniques for gradient computation. We compute the closed form of the gradients for Unitary QNNs and Quantum Boltzmann Machines and provide sufficient conditions for the absence of barren plateaus in these models. We demonstrate our approach in two use cases: thermal state learning and Hamiltonian learning. In our numerical experiments, we observed rapid convergence of our training loss function and frequently archived a 99% average fidelity in fewer than 100 epochs.\n"
      ],
      "metadata": {
        "id": "9PEgOiqS8-lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Expressibility vs Trainability*"
      ],
      "metadata": {
        "id": "qmblAsDKA1Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expressibility of circuits: Learning an unknown unitary**\n",
        "\n",
        "* Expressibility & trainability is a tradeoff!!\n",
        "\n",
        "* Zoe Holmes: reduce expressibility to increase trainability. Find ansatz that fits the use case problem.\n",
        "\n",
        "* https://pennylane.ai/qml/demos/tutorial_haar_measure.html\n",
        "\n",
        "* Barren plateaus, and hence expressibility issues, are not so much in classical ML, because vanishing cost gradients are not so much of an issue in classical ML because we don‚Äòt have precision limitations in the same way.  You don‚Äòt have to evauluate your cost function using many many shots, or is so resource intensive to get gradients.\n",
        "* In quantum ML: you can use ideas from control theory to try to assess whether or not your ansatz is gonna be trainable or not in advance. Thats an important strategy.\n",
        "* The other approach: use symmetries of your problem / you gonna have to use physics to come up with whats a good ansatz. ZB: use VGQ for some system with various (particle number conserving) translational symmetries, you want to build all of those symmetries into your ansatz and hence reduce expressibility while capturing some of the solution space.\n",
        "\n",
        "* Maria schuld paper: how expressive are circuits? You can distribute circuits and how flexible are they? - identity gate maps to one point only. If you have a couple of more gates it maps to more points.\n",
        "\n",
        "https://arxiv.org/abs/1905.10876: Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1408.png)"
      ],
      "metadata": {
        "id": "kuNW1Ap_Kzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Capacity and Generalization Measures*"
      ],
      "metadata": {
        "id": "ncQDxd2nxffx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computational Learning Theory**\n",
        "\n",
        "* Statistical Learning Theory (SLT): Formal study of learning algorithms.\n",
        "\n",
        "* Computational Learning Theory (CoLT): Formal study of learning tasks.\n",
        "\n",
        "  * PAC Learning (Theory of Learning Problems): PAC learning seeks to quantify the difficulty of a learning task and might be considered the premier sub-field of computational learning theory. Consider that in supervised learning, we are trying to approximate an unknown underlying mapping function from inputs to outputs. We don‚Äôt know what this mapping function looks like, but we suspect it exists, and we have examples of data produced by the function. PAC learning is concerned with how much computational effort is required to find a hypothesis (fit model) that is a close match for the unknown target function.\n",
        "\n",
        "  * VC Dimension (Theory of Learning Algorithms): The VC dimension estimates the capability or capacity of a classification machine learning algorithm for a specific dataset (number and dimensionality of examples). Formally, the VC dimension is the largest number of examples from the training dataset that the space of hypothesis from the algorithm can ‚Äúshatter.‚Äù The Vapnik-Chervonenkis dimension, VC(H), of hypothesis space H defined over instance space X is the size of the largest finite subset of X shattered by H.\n",
        "\n",
        "* https://machinelearningmastery.com/introduction-to-computational-learning-theory/\n",
        "\n",
        "* More:\n",
        "\n",
        "  * [Statistical learning theory, Wikipedia.](https://en.wikipedia.org/wiki/Statistical_learning_theory)\n",
        "  * [Computational learning theory, Wikipedia.](https://en.wikipedia.org/wiki/Computational_learning_theory)\n",
        "  * [Probably approximately correct learning, Wikipedia.](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning)\n",
        "  * [Vapnik‚ÄìChervonenkis theory, Wikipedia.](https://en.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_theory)\n",
        "  * [Vapnik‚ÄìChervonenkis dimension, Wikipedia.](https://en.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_dimension)\n",
        "\n"
      ],
      "metadata": {
        "id": "MaV5A-AMyTkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Vapnik‚ÄìChervonenkis (VC) dimension](https://en.m.wikipedia.org/wiki/Vapnik‚ÄìChervonenkis_dimension)\n",
        "\n",
        "* [Neural network capacity](https://en.m.wikipedia.org/wiki/Artificial_neural_network#Capacity): A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity."
      ],
      "metadata": {
        "id": "DU3cn6tJxprr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a relationship between Generalization and capacity (and the max capacity is not necessarily what we're looking for):\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1399.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1397.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1398.png)\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=fDIGmkq9xNE&t=2067s"
      ],
      "metadata": {
        "id": "937MJYN_zNKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Sensors*"
      ],
      "metadata": {
        "id": "66-byTWA1_ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Quantum Sensing](https://en.m.wikipedia.org/wiki/Quantum_sensor) **Differenzierung**\n",
        "\n",
        "* [Interferometry](https://en.m.wikipedia.org/wiki/Interferometry): technique which uses the interference of superimposed waves to extract information\n",
        "\n",
        "  * [Atom Interferometer](https://en.m.wikipedia.org/wiki/Atom_interferometer): interferometer which uses the wave character of atoms.\n",
        "\n",
        "  * Electromagnetic Wave Interferometry\n",
        "\n",
        "    * [Astronomical_optical_interferometry](https://en.m.wikipedia.org/wiki/Astronomical_optical_interferometry)\n"
      ],
      "metadata": {
        "id": "hWVAIMtl4Ljq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Astronomical Optical Interferometry**\n",
        "\n",
        "* https://www.quantamagazine.org/famous-quantum-experiment-offers-hope-for-earth-size-telescope-20210505/\n",
        "  * radio interferometry: relatively easy in radio astronomy, both because radio-emitting objects tend to be extremely bright, and because radio waves are relatively large and thus easy to line up.\n",
        "  * Optical interferometry is much harder. Visible wavelengths measure hundreds of nanometers long, leaving far less room for error in aligning waves according to when they arrived at different telescopes. Moreover, optical telescopes build images photon-by-photon from very dim sources. It‚Äôs impossible to save these grainy signals onto normal hard drives without losing information that‚Äôs vital for doing interferometry.\n",
        "  * ‚ÄúIf there was a way of recording photon events at an optical telescope with some kind of quantum device, that would be a great boon to the science.‚Äù\n",
        "* https://en.m.wikipedia.org/wiki/Astronomical_optical_interferometry\n",
        "* https://en.wikipedia.org/wiki/Photometric_system\n",
        "* https://en.wikipedia.org/wiki/List_of_astronomical_interferometers_at_visible_and_infrared_wavelengths"
      ],
      "metadata": {
        "id": "s_8-fq9p4Pqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix**\n",
        "\n",
        "sensing changes in motion, and electric and magnetic fields. The analyzed data is collected at the atomic level.\n",
        "\n",
        "https://www.baesystems.com/en-us/definition/what-is-quantum-sensing\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_sensor\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_metrology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_imaging\n",
        "\n",
        "diamond-based quantum sensors that can detect magnetic and electric fields\n",
        "* https://www.iaf.fraunhofer.de/en/researchers/quantum-systems/quantum-sensors.html\n",
        "* https://www.iaf.fraunhofer.de/en/media-library/press-releases/world-s-first-measurement-of-magnetic-field-dependent-stimulated.html\n",
        "* https://www.iaf.fraunhofer.de/en/media-library/press-releases/more-precise-diagnoses-and-personalized-therapies-due-to-hyperpolarized-nuclear-magnetic-resonance.html"
      ],
      "metadata": {
        "id": "FtAuAa_t6yQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Algorithms*"
      ],
      "metadata": {
        "id": "UnzMHYy2j-mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Scientific Computing*"
      ],
      "metadata": {
        "id": "utryf_rC5-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import math\n",
        "import sympy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t8AIbzBhdqYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.spektrum.de/news/zehn-algorithmen-die-die-wissenschaft-veraendert-haben/1836355"
      ],
      "metadata": {
        "id": "uGfyfq3Idyb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input\n",
        "x = 4\n",
        "y = 8\n",
        "z = 20\n",
        "\n",
        "# Define function\n",
        "def square_point(x, y, z):\n",
        "  x_squared = x * x\n",
        "  y_squared = y * y\n",
        "  z_squared = z * z\n",
        "  return x_squared, y_squared, z_squared\n",
        "\n",
        "# Run function\n",
        "data=square_point(x, y, z)\n",
        "print('Result:', square_point(x, y, z))\n",
        "\n",
        "# Visualize result\n",
        "sns.set(rc={'figure.figsize':(8, 4), \"lines.linewidth\": 1.0})\n",
        "table = pd.DataFrame(data, index=[x, y, z], columns=['Result'])\n",
        "table = table.reset_index().rename(columns={\"index\": \"Input\"})\n",
        "table.plot(x ='Input', y='Result', kind = 'bar')\t"
      ],
      "metadata": {
        "id": "7qbCgqImdz9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input\n",
        "x = 25\n",
        "\n",
        "# Define functions - Returning multiple values for same function (Iterating through a list)\n",
        "def square(i):\n",
        " return (i**2)\n",
        "\n",
        "def my_function(x):\n",
        "  for i in range(10,x,5):  # Start, Stop, Steps\n",
        "    print(i, square(i))\n",
        "\n",
        "# Run function\n",
        "my_function(x)"
      ],
      "metadata": {
        "id": "k6g4snNQd12P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple function for one value and Add a conditional\n",
        "\n",
        "x = 10\n",
        "\n",
        "def my_function(x):\n",
        "  if x < (10):\n",
        "    return (int(math.sqrt(x + 2)), math.sqrt(x + 2))\n",
        "  else:\n",
        "    return x**3\n",
        "\n",
        "my_function(x)"
      ],
      "metadata": {
        "id": "-yxgzRckd3ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For (Loop) with append function\n",
        "\n",
        "input_list = [1,4,9]\n",
        "result_list = []\n",
        "\n",
        "for n in input_list:\n",
        "    result_list.append(n**3)\n",
        "\n",
        "print(result_list)"
      ],
      "metadata": {
        "id": "BFCdu2vgd5Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For loop with range and as function\n",
        "\n",
        "x = 10\n",
        "\n",
        "def my_function(x):\n",
        "  for i in range(2,x,2):  # Start, Stop, Steps\n",
        "    print(i, i**2)\n",
        "\n",
        "my_function(x)"
      ],
      "metadata": {
        "id": "5HRJJ5eSd7Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the wavelengths of hydrogen lines\n",
        "R = 1.097e-2\n",
        "for m in range(1,4,2): # Start, Stop, Steps\n",
        "    print(\"Series for m =\",m)\n",
        "\n",
        "    for n in range(m+1,m+6):\n",
        "        invlambda = R*(1/m**2-1/n**2)\n",
        "        print(\"  \",1/invlambda,\"nm\")"
      ],
      "metadata": {
        "id": "ajCmqf6Xd-Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yield and enumerate\n",
        "\n",
        "# Yield gives you a generator. You'd use it where you would normally use a return in a function.\n",
        "# yield\tgenerator: ends a function, returns a generator\n",
        "# When you have a function that returns a sequence and you want to iterate over that sequence, \n",
        "# but you do not need to have every value in memory at once.\n",
        "\n",
        "# Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object\n",
        "\n",
        "def fibonacci():\n",
        "    a, b = 0, 1\n",
        "    while True:\n",
        "        yield a\n",
        "        a, b = b, a+b\n",
        "\n",
        "for i, f in enumerate(fibonacci()):\n",
        "    print(i, f)\n",
        "    if i >= 10: break"
      ],
      "metadata": {
        "id": "dyoAm5MdeHlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lambda Function: Add 10 to argument a, and return the result\n",
        "\n",
        "x = lambda a : a + 10\n",
        "print('Result 1:', x(5))\n",
        "\n",
        "# Multiply argument a with argument b and return the result:\n",
        "x = lambda a, b, c : (a * b) + c\n",
        "print('Result 2:', x(5, 6, 3))\n",
        "\n",
        "\n",
        "# function definition that takes one argument, and that argument will be multiplied with an unknown number:\n",
        "n = 3\n",
        "a = 12 \n",
        "def myfunc(n):\n",
        "  return lambda a : a * n\n",
        "\n",
        "mytripler = myfunc(n)\n",
        "\n",
        "print('Result 3:', mytripler(a))"
      ],
      "metadata": {
        "id": "6oVIv5QCeJTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lambda with filter, map, reduce, sorted etc\n",
        "# https://towardsdatascience.com/lambda-functions-in-python-15b60ff4207d\n",
        "\n",
        "# Calculate the squares of the elements in a given iterable:\n",
        "iterable = [1, 3, 5, 6, 9, 11, 15, 16, 21]\n",
        "list(map(lambda x: x ** 2, iterable))"
      ],
      "metadata": {
        "id": "0pFU_NlNeK-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a class with a property x:\n",
        "class MyClass:\n",
        "  x = 5\n",
        "\n",
        "# Create an object p1 using the class MyClass, and print the value of x:\n",
        "p1 = MyClass()\n",
        "print(p1.x)\n",
        "\n",
        "# Create a class named Person, use the __init__() function to assign values for name and age:\n",
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "\n",
        "print(p1.name)\n",
        "print(p1.age)"
      ],
      "metadata": {
        "id": "mNBwBkgoeOVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# __str__() function controls what should be returned when the class object is represented as a string\n",
        "class Person:\n",
        "  def __init__(my_object, name, age):    # Using \"my_object\" instead of \"self\"\n",
        "    my_object.name = name\n",
        "    my_object.age = age\n",
        "\n",
        "  def __str__(my_object):\n",
        "    return f\"{my_object.name}, {my_object.age}\"\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "print(p1)"
      ],
      "metadata": {
        "id": "MnxnmstjeQCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Including object methods:\n",
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "  def myfunc(self):\n",
        "    print(\"Hello my name is \" + self.name)\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "p1.myfunc()"
      ],
      "metadata": {
        "id": "uaKAov6peRZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install livelossplot -q\n",
        "from livelossplot import PlotLossesKerasTF\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9 # for RMSProp 0.0 / for SGD 0.9\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, \n",
        "                                     beta_1=0.9, \n",
        "                                     beta_2=0.999, \n",
        "                                     epsilon=1e-07, \n",
        "                                     amsgrad=False)\n",
        "\n",
        "# Load & Prepare Data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define Model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu')) \n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=2, \n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[PlotLossesKerasTF()],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "id": "mOSp2QJieCG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMSa4Pt2dvys"
      },
      "source": [
        "###### *Quantum Fourier Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Fourier Transform is the change from one basis (computational) to another (Fourier basis)**\n",
        "\n",
        "* Quantum Fourier Transform is the inverse Discrete Fourier Transform)\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_047.png)\n",
        "\n",
        "**General formula**\n",
        "\n",
        "* Remember: <font color=\"blue\">$e^{2\\pi i}$ = 1</font> (identity operation), and see why $e^{\\pi i}$ = -1 in [this video](https://youtu.be/-AyE1Wpgo3Q) \n",
        "\n",
        "\n",
        "* In QFT we change the <font color=\"blue\">$\\theta$ = phase in $e^{2\\pi i \\theta}$</font> = Eigenvalue of Oracle function $U$ associated with an eigenvector |u‚ü©\n",
        "\n",
        "* The phase $\\theta$ is expressed as: <font color=\"blue\">$\\theta$ = $\\frac{x_n}{2^{k_n}}$</font> with:\n",
        "\n",
        "  * <font color=\"blue\">$x_n$ = 0 or 1</font> state\n",
        "  \n",
        "  * <font color=\"blue\">$k_n$</font> number of Qubits\n",
        "\n",
        "* This is expressed in a so-called \"controlled-R quantum gate\" that **applies a relative phase change to |1>**\n",
        "\n",
        "* The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i \\frac{x_n}{ 2^{k_n}}}\\end{array}\\right)$</font>"
      ],
      "metadata": {
        "id": "I_dkpT8Cdvyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits: Introduction*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1, $k_2$ = 2, $k_3$ = 3</font>\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^1}+\\frac{x_{2}}{2^2}+\\frac{x_{3}}{2^3}\\right)}|1\\rangle\\right)$  = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_1}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 90¬∞ S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 45¬∞ T-rotation of $\\frac{\\pi}{4}$ radians = between 1 and i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_2}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^{k_1}}+\\frac{x_3}{2^{k_2}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^1}+\\frac{x_3}{2^2}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2}+\\frac{x_3}{4}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 90¬∞ S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If both $\\tilde{x_2}$ and $\\tilde{x_3}$ are activated, then it is a 180¬∞ + 90¬∞ = 170¬∞ rotation of $\\pi + \\frac{\\pi}{2}$ radians = -i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_3}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^1}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{\\pi \\mathrm{i}x_3}|1\\rangle\\right)$\n",
        "\n",
        "* If $\\tilde{x_3}$ is activated, then it is a 180¬∞ Z-rotation of $\\pi$ radians = -1\n"
      ],
      "metadata": {
        "id": "xOBGPt6Pdvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)\n",
        "\n",
        "*Here including the 8x8 matrix form for the complete operator:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0801.png)"
      ],
      "metadata": {
        "id": "J8n3fjsedvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 1 Qubit*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1</font>\n",
        "\n",
        "\n",
        "*Linear transformation of a qubit in the computational basis 0 and 1 each separately to the Fourier basis:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0813.png)\n",
        "\n",
        "**Computational Basis in $|0\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 0 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$  $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i} 0}$  = $\\mathrm{e}^{0}$ = 1 (no rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0810.png)\n",
        "\n",
        "**Computational Basis in $|1\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 1 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$ $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}\\right)}$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0811.png)"
      ],
      "metadata": {
        "id": "hBqueHp-dvyu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4Vdy6sdvyv"
      },
      "source": [
        "*Quantum Fourier Transform with 1 Qubit is a Hadamard transform!*\n",
        "\n",
        "**One qubit QFT matrix**: $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & \\mathrm{e}^{\\pi i}\\end{array}\\right)$, where $\\mathrm{e}^{\\pi \\mathrm{i}}$ = -1. So it is: <font color=\"blue\"> QFT f√ºr x=1 = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**Compare with Hadamard transform matrix:** \n",
        "\n",
        "In quantum computing, the Hadamard gate is a one-qubit rotation, mapping the qubitbasis states $|0\\rangle$ and $|1\\rangle$ to two **superposition** states with **equal weight of the computational basis** states $|0\\rangle$ and $|1\\rangle$. Usually the phases are chosen so that\n",
        "\n",
        ">$\n",
        "H=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}\\langle 0|+\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}\\langle 1|\n",
        "$\n",
        "\n",
        "in Dirac notation. This corresponds to the transformation matrix\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "H_{1}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "1 & -1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "in the $|0\\rangle,|1\\rangle$ basis, also known as the computational basis. The states $\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}$ and $\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}$ are known as $|+\\rangle$ and $|-\\rangle$ respectively, and together constitute the polar basis in quantum computing.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_073.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJhiyX4dvyw"
      },
      "source": [
        "**Why Hadamard transform is exactly a 1 qubit Quantum Fourier Transform:** (see result of + for 0 state and - for 1 state) - Matrix-Vector-Multiplication (Single Qubit)\n",
        "\n",
        "> <font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "> <font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_045.png)\n",
        "\n",
        "2 im denominator verschwindet hier. 2^n f√ºr n=1 qubit. mit 2 oben und unten verschwinden beide."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|001\\rangle$*\n",
        "\n",
        "**Computational Basis in $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0804.png)\n",
        "\n",
        "**Fourier Basis for $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0805.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 0</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{0}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$  = <font color=\"blue\">$\\frac{\\pi i}{4}$</font> (45¬∞ T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = <font color=\"blue\">$\\frac{\\pi i}{2}$</font> (90¬∞ S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)"
      ],
      "metadata": {
        "id": "XH-LUgBzdvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is not activated because $x_2$ = 0:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "tAr5mMp3dvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|111\\rangle$*\n",
        "\n",
        "**Computational Basis in $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0802.png)\n",
        "\n",
        "**Fourier Basis for $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0803.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$ = $\\mathrm{e}^{2 \\pi i 0.875} = \\mathrm{e}^{\\pi i 1.75}$ (180¬∞ Z-rotation + 90¬∞ S-rotation + 45¬∞ T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = $e^{\\pi i 1.5} =$ <font color=\"blue\">$-i$</font> (180¬∞ Z-rotation + 90¬∞ S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180¬∞ Z-rotation)"
      ],
      "metadata": {
        "id": "qXRBmM0advy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90¬∞ rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "HYY1eloedvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Cirq Code for Quantum Fourier Transform*\n",
        "\n",
        "*Compare the code above with the circuit activations below (how a circuits computes the results):* \n",
        "\n",
        "* $H$ gate = bring qubit in superposition. \n",
        "\n",
        "  * *For $x=0$, no further rotation*\n",
        "  \n",
        "  * *For $x=1$, then appy additional *$Z$ gate = 180¬∞ rotation = $\\pi$**\n",
        "\n",
        "* *$S$ gate = 90¬∞ rotation = $\\frac{\\pi}{2}$*\n",
        "\n",
        "* *$T$ gate = 45¬∞ rotation = $\\frac{\\pi}{4}$*\n",
        "\n",
        "$C R_{j}=C Z^{1 / 2^{j-1}}$\n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse) \n",
        "\n",
        "* $S$ entspricht $\\frac{\\pi}{2}$, also wenn qubit 1 = 1, dann bei qubit 0 das $S$ transform anwenden (0,5)\n",
        "\n",
        "  * S: The square root of Z gate, equivalent to cirq.Z ** 0.5\n",
        "\n",
        "  * See: [Cirq Gates](https://quantumai.google/cirq/gates)\n",
        "\n",
        "* $T$ entspricht $\\frac{\\pi}{4}$"
      ],
      "metadata": {
        "id": "lWJ2Oz2Ydvy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q\n",
        "import cirq\n",
        "\n",
        "def make_qft(qubits):\n",
        "\n",
        "    # Generate list of qubits\n",
        "    qreg = list(qubits)\n",
        "    \n",
        "    # Make sure list is longer than 0 qubits:\n",
        "    while len(qreg) > 0:\n",
        "    \n",
        "    # Remove first qubit from list and return its value (set as head-qubit):\n",
        "        q_head = qreg.pop(0)\n",
        "    \n",
        "    # Apply Hadamard superposition to this head-qubit\n",
        "        yield cirq.H(q_head)\n",
        "\n",
        "    # Enumerate through list with i (index position) and corresponding qubit value (0 or 1)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "\n",
        "    # Apply Controlled-Z * Theta-Phase-Shift on target ('q-head') if control-qubit ('qubit') is in state 1\n",
        "            yield (cirq.CZ ** (1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "    # Do the inverse QFT as subroutine in quantum phase estimation\n",
        "    #        yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Use inverse QFT as subroutine in quantum phase estimation\n",
        "# phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Iterating through until \"while len(qreg) = 0\", then processes stops\n",
        "\n",
        "\"\"\"Visually check the QFT circuit.\"\"\"\n",
        "qubits = cirq.LineQubit.range(17)\n",
        "qft = cirq.Circuit(make_qft(qubits))\n",
        "print(qft)"
      ],
      "metadata": {
        "id": "myOgAQXddvy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0815.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "mzHIk-hJdvy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTg0Uyudvy6"
      },
      "source": [
        "*Inverse Quantum Fourier Transform ('QFT Dagger' - Dagger is a complex conjugate operation!)*\n",
        "\n",
        "Reminder of QFT:\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$\n",
        "\n",
        "**Remember: Dagger is a complex conjugate operation!**\n",
        "\n",
        "QFT inverse (see -2 turning i in -i which is a complex conjugate operation):\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$ \n",
        "\n",
        "\n",
        "The operator is then (\n",
        "We have already seen that the Hadamard gate is self-inverse, and the same is clearly true for the SWAP gate; the inverse of the rotations gate $R_k$ is given by):\n",
        "\n",
        "> The matrix form of inverse QFT operator is: <font color=\"blue\">${R^{\\dagger}}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{-2 \\pi i / 2^{k}}\\end{array}\\right)$</font> and compare with QFT operator:  <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "https://www.cl.cam.ac.uk/teaching/1920/QuantComp/Quantum_Computing_Lecture_9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Phase Estimation*"
      ],
      "metadata": {
        "id": "Op-cit2CFEiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNOT Gate and Phase Kickback** \n",
        "\n",
        "<font color=\"blue\">*CNOT-Gate applied to the computational basis 0 and 1*\n",
        "\n",
        "* https://qiskit.org/textbook/ch-gates/phase-kickback.html\n",
        "\n",
        "* Main article about Phase Kickback: https://towardsdatascience.com/quantum-phase-kickback-bb83d976a448\n",
        "\n",
        "The CNOT-gate is a two-qubit gate. Thus, it transforms qubit states whose state we represent by a four-dimensional vector.\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\alpha|0\\rangle|0\\rangle+\\beta|0\\rangle|1\\rangle+\\gamma|1\\rangle|0\\rangle+\\delta|1\\rangle|1\\rangle=\\left[\\begin{array}{c}\n",
        "\\alpha \\\\\n",
        "\\beta \\\\\n",
        "\\gamma \\\\\n",
        "\\delta\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Remember Vector-Vector-Multiplikation (Kronecker / tensor product):\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$. \n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$ \n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Accordingly, the CNOT-gate has a $4 \\times 4$ transformation matrix.\n",
        "\n",
        ">$\n",
        "C N O T=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "There is no effect if the control qubit (at the left-hand position in the Dirac notation) is in state |0‚ü©, as in states |00‚ü© and |01‚ü©.\n",
        "\n",
        "> CNOT $\\cdot|00\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=|00\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|01\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=|01\\rangle$\n",
        "\n",
        "<font color=\"blue\">But if the control qubit is in state |1‚ü©, then the controlled (target) qubit switches from |0‚ü© to |1‚ü© and vice versa.</font>\n",
        "\n",
        "> CNOT $\\cdot|10\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=|11\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|11\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=|10\\rangle$\n",
        "\n",
        "When we describe the quantum states and operations in terms of mathematical formulae, we use the vectors |0‚ü© and |1‚ü© as a basis. |0‚ü© and |1‚ü© denote the standard or computational basis states. These states correspond to the possible measurements we might obtain when looking at the qubit. We measure a qubit in state |0‚ü© as 0 with absolute certainty. And, we measure a qubit in state |1‚ü© as 1, accordingly. While the basis {|0‚ü©,|1‚ü©} is convenient to work with mathematically, it is just a representation of the underlying physics.\n"
      ],
      "metadata": {
        "id": "-0AI_GVFZk0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">*CNOT-Gate applied to the superposition basis + and -*\n",
        "\n",
        "The mathematical basis we chose leads to a specific representation of the CNOT-transformation. But this is not the only possible representation. In fact, there are infinitely many other possible choices. Our qubits are not limited to these two states. Qubits can be in a superposition of both states. For instance, there are the states that result from applying the Hadamard-gate on the basis states:\n",
        "\n",
        "> $|+\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ and $|-\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}}\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "Now apply Hadamard gate on a qubit that is in the |1> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "Mathematically, the following matrix represents the application of Hadamard gates on each of the two qubits.\n",
        "\n",
        "> $H \\otimes H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}H & H \\\\ H & -H\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]$\n",
        "\n",
        "So, if we apply this matrix on two qubits in state |00‚ü©, they end up in state |++‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|00\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |01‚ü© results in state |+‚àí‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|01\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |10‚ü© results in state |‚àí+‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|10\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "Finally, if we apply this transformation on two qubits in state |11‚ü©, we put them into state |‚àí‚àí‚ü©.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|11\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$"
      ],
      "metadata": {
        "id": "7s9KUHDdV_5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let‚Äôs apply the CNOT-gate on qubits in superposition. We can calculate the overall transformation matrix by multiplying the matrices of the CNOT-gate and the H‚äóH transformation. The CNOT-gate switches the second and fourth columns of the H‚äóH-matrix.\n",
        "\n",
        "> $\\operatorname{CNOT}(H \\otimes H)=\\left[\\begin{array}{cccc}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot \\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right]$\n",
        "\n",
        "* And now, we apply this transformation to the four combinations of basis states.\n",
        "\n",
        "<font color=\"blue\">If the target qubit (at the right-hand side) is in state |1‚ü©, the state of the control qubit (at the left-hand side) flips from |+‚ü© to |‚àí‚ü© and vice versa:\n",
        "\n",
        "> \n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|00\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|01\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|10\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|11\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "In short, we can say:\n",
        "\n",
        "> $\\operatorname{CNOT}(|++\\rangle)=|++\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|+-\\rangle)=|--\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|-+\\rangle)=|-+\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|--\\rangle)=|+-\\rangle$\n",
        "\n",
        "The two states |+‚ü© and |‚àí‚ü© have the same measurement probabilities of |0‚ü© and |1‚ü©. They result in either value with a probability of 0.5. **So, the CNOT-gate does not have any directly measurable implications**. <font color=\"blue\">However, the control qubit switches its phase. It takes on the phase of the controlled (target) qubit.</font>\n",
        "\n",
        "> For the phase of the target qubit is kicked up to the control qubit, we call this phenomenon phase kickback.\n",
        "\n",
        "We learned the CNOT-gate is not a one-sided operation. It clearly has the potential to affect the state of the control qubit. Even though the phase is not directly measurable, there are ways to exploit differences in the phase between states. In fact, prominent algorithms, such as Grover‚Äôs search algorithm, exploit this effect.\n"
      ],
      "metadata": {
        "id": "Tg8PpY31YMh_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwuxaKm4lF2"
      },
      "source": [
        "**Quantum Phase Estimation** \n",
        "\n",
        "* algorithm for determining the eigenvalues of a unitary operator\n",
        "\n",
        "* the [quantum phase estimation algorithm](https://en.m.wikipedia.org/wiki/Quantum_phase_estimation_algorithm) (also referred to as quantum eigenvalue estimation algorithm), is a quantum algorithm to estimate the phase (or eigenvalue) of an eigenvector of a unitary operator. \n",
        "\n",
        "* More precisely, given a unitary matrix $U$ and a quantum state $|\\psi\\rangle$ such that $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$, the algorithm estimates the value of $\\theta$ with high probability within additive error $\\varepsilon$, using $O(\\log (1 / \\varepsilon))$ qubits (without counting the ones used to encode the eigenvector state) and $O(1 / \\varepsilon)$ controlled- $U$ operations. \n",
        "\n",
        "* The algorithm was initially introduced by Alexei Kitaev in 1995.\n",
        "\n",
        "* Phase estimation is frequently used as a subroutine in other quantum algorithms, such as Shor's algorithm and the quantum algorithm for linear systems of equations.\n",
        "\n",
        "<font color=\"blue\">*One Qubit Phase Estimation (with Hadamard Gate):*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_117.png)\n",
        "\n",
        "<font color=\"blue\">*Multi-Qubit Phase Estimation (with inverse Quantum Fourier Transform):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/a/a5/PhaseCircuit-crop.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7UKWz2y03L"
      },
      "source": [
        "Remember in **Quantum Fourier Transform**: \n",
        "\n",
        "\n",
        "> x1 = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ \n",
        "\n",
        "* <font color=\"blue\">$e^{2\\pi i}$ = 1 = identity</font>\n",
        "\n",
        "* In Quantum Fourier Transform we change the phase <font color=\"blue\">$\\theta$ in $e^{2\\pi i}$</font> <font color=\"red\">$^{\\theta}$</font>\n",
        "\n",
        "  * <font color=\"red\">= Eigenvalue of Oracle function $U$ associated with an eigenvector |u‚ü©</font>\n",
        "\n",
        "* Phase <font color=\"blue\">$\\theta$ is $\\frac{x_n}{2^{k}}$ with $x_n$ 0 or 1</font> state and $k$ number of Qubits.\n",
        "\n",
        "* A controlled-R quantum gate applies a relative phase change to |1>. The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "**Now in Phase Estimation**: \n",
        "\n",
        "> In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$ mit dem Operator: $U^{2^n} = \\phi$\n",
        "\n",
        "Quantum phase estimation addresses the following problem:\n",
        "* We have a $n$-qubit oracle function $U$, encoded in the form of a controlled- $U$ unitary.\n",
        "* **$U$ has an eigenvalue $e^{2 \\pi i \\phi}$, associated with an eigenvector $|u\\rangle$ which we can prepare.**\n",
        "* <font color=\"red\">**We wish to estimate the phase, $\\phi$, of the eigenvalue to $t$ bits of precision.**\n",
        "\n",
        "> <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$** $\\quad$ (based on Eigenvalue equation)</font>\n",
        "\n",
        "* Here $|\\psi\\rangle$ is an eigenvector / eigenstate and $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue. \n",
        "\n",
        "* <font color=\"red\">For example: the eigenvalues of X are ‚àí1 and 1 and have the eigenvectors |‚àí‚ü© and |+‚ü© respectively.*</font>\n",
        "\n",
        "*Since $U$ is unitary, all of its eigenvalues have a norm of 1.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-6ThVrEdXJ"
      },
      "source": [
        "Reminder: QFT\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_084.png)\n",
        "\n",
        ">**See below: <font color=\"red\">Remember that a unitary matrix has eigenvalues of the form $e^{i \\theta_{\\psi}}$ (ohne $2 \\pi$ wie oben bei QFT) and that it has eigenvectors that form an orthonormal basis**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_083.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p-zzXyW-tU6"
      },
      "source": [
        "The problem: in both cases the probability is 0,5, just differs by the phase added: $=e^{\\frac{i \\pi}{2}}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_078.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSvCLdgF_YQC"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_079.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rHCBrjo_ZWr"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_080.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scRiAADhBR3t"
      },
      "source": [
        "**The probability of measuring 0 and 1 is each 0,5, but there is a small factor that makes them differ from 0,5, depending on the phase (angle):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0oYz_yZBCoU"
      },
      "source": [
        "> **In the different between the probability of measuring 0 or 1, you've encoded that phase! (In other words: you've taken that phase information and turned it into and amplitude that you can measure.**\n",
        "\n",
        "* How to do this experimentally: you do a million shots of the experiment, collect statistics and check what the statistics say. How many times did I get zero? How many times did I get one? The hope is that the difference between the statistics of zero and one would allow us to back out theta\n",
        "\n",
        "* Next level: now getting more precision with more qubits: (there is another circuit to prepare Psi yet, which is assumed to be given here)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_082.png)\n",
        "\n",
        "writing out the calculation:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_085.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1qf44PHLJJ"
      },
      "source": [
        "**Comparing QPE with QFT (QPE is the same as QFT with a different phase):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_086.png)\n",
        "\n",
        "It's like applying a QFT of something (of a special phase $\\frac{\\theta_{\\psi}}{2^{n}} 2 \\pi$, the green box above!), and in order to get back to the original state you need to apply an inverse QFT at the end:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_087.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 1: Set up the unitary and number of bits to use in phase estimation*\n",
        "\n",
        "<font color=\"blue\">*Let's take as an example the T-gate, and use Quantum Phase Estimation to estimate its phase.*\n",
        "\n",
        "You will remember that the $T$-gate adds a phase of $e^{\\frac{i \\pi}{4}}$ to the state $|1\\rangle$ :\n",
        "\n",
        "$\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$\n",
        "\n",
        "Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$\n",
        "\n",
        "<font color=\"red\">We expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "We first perform a Hadamard gate on the first qubit to get the state \n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "  \n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "  \n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "  * <font color=\"blue\">this part above is the rule from tensor products: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition: $\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "</font>\n",
        "\n",
        "    * The Hadamard states ‚à£+‚ü© and ‚à£‚àí‚ü© are considered superposition states because they are a combination of the two computational states:\n",
        "\n",
        "    * State: $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$ so for + it is: $|\\+\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "  * we have intentionally omitted the normalization factor of 1/‚àö2 for clarity\n",
        "\n",
        "> $|+\\rangle \\otimes|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] \\otimes\\left[\\begin{array}{l}\\psi\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\, [\\psi] \\\\ 1 \\, [\\psi]\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $|+\\rangle$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ =  $\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$"
      ],
      "metadata": {
        "id": "mV4nEZ1l2VMu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zxsy6Cc5Wrx"
      },
      "source": [
        "# Value of Œ∏ which appears in the definition of the unitary U above.\n",
        "# Try different values.\n",
        "theta = 0.125\n",
        "\n",
        "# Define the unitary U-Gate:\n",
        "U = cirq.Z ** (2 * theta)\n",
        "\n",
        "# Accuracy of the estimate for theta. Try different values.\n",
        "n_bits = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8u3NsIlTNsO"
      },
      "source": [
        "Here details about unitary U-Gate: \n",
        "\n",
        "$U$ = $Z^{2^{n-n}}$ \n",
        "\n",
        "Z = $e^{\\pi}$ \n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse) \n",
        "\n",
        "\n",
        "then:\n",
        "\n",
        "> <font color=\"blue\">$U$ = $e^{\\pi * 2^{n-n}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2: Build the first part of the circuit for phase estimation with controlled U-gate (Phase Kickback)*\n",
        "\n",
        "We then perform a controlled U operation, which we have written as $U^{2^0}$. Here applies the **Phase Kickback!**\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+|1\\rangle$ <font color=\"red\">$U$</font> $|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle) \\otimes|\\psi\\rangle$\n",
        "\n",
        "* Here are 2 things very important: \n",
        "  \n",
        "    * The second qubit register containing |œà‚ü© hasn‚Äôt changed. We shouldn‚Äôt expect it to, **since |œà‚ü© is an eigenstate of U (Remember: <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$ based on the Eigenvalue equation**</font>). Thus, no matter how many times we apply U to this register, nothing happens to |œà‚ü©**. But if we apply it more often it will 'amplify' the phase (Not in the sense of amplitude amplification) - we amplify it with adding more qubits and hence more $\\phi$ to get more precision\n",
        "    \n",
        "    * what‚Äôs the point of applying U then? The effect was that **it wrote some information about the eigenvalue into the relative phase of the first qubit**. Namely, the entire effect was to\n",
        "map: $|0\\rangle+|1\\rangle \\mapsto|0\\rangle+e^{2 \\pi i 0. \\phi_{1}}|1\\rangle$"
      ],
      "metadata": {
        "id": "iQE_Unsl2YXo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMG3fjxw5aXU",
        "outputId": "bc0de073-cb3a-4fa4-c04e-009bff44710a"
      },
      "source": [
        "# Get qubits for the phase estimation circuit.\n",
        "qubits = cirq.LineQubit.range(n_bits)\n",
        "u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "# Build the first part of the phase estimation circuit.\n",
        "phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "# Set the input state of the eigenvalue register: Add gate to change initial state to |1>\n",
        "phase_estimator.insert(0, cirq.X(u_bit))\n",
        "\n",
        "# bit = cirq.LineQubit\n",
        "for i, bit in enumerate(qubits):\n",
        "    phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - i - 1)))\n",
        "    # explanation: U-rot control aktiviert wenn entsprechendes qubit in state 1 (??)\n",
        "    # dann aktiviere formel: U^2^(n-1) ...U^2^(n-2) ...U^2^(n-n)\n",
        "\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ\n",
            "1: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ\n",
            "2: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ   ‚îÇ\n",
            "u: ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄZ‚îÄ‚îÄ‚îÄS‚îÄ‚îÄ‚îÄT‚îÄ‚îÄ‚îÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoxXF9-W9orx"
      },
      "source": [
        "> <font color=\"blue\">$U$ = $Z^{2^{n-n}}$ = $e^{\\pi * 2^{n-n}}$ fur das erste Gate: = $e^{\\pi * (-0.128)}$ ????\n",
        "\n",
        "\n",
        "*Why are we adding Pauli-X? The initial state for u_bit is the  state, but the phase for this state is trivial with the operator we chose. Inserting a Pauli  operator at the begining of the circuit changes this to the  state, which has the nontrivial  phase.*\n",
        "\n",
        "*The controlled u gate*:\n",
        "\n",
        "$|00\\rangle \\mapsto|00\\rangle$\n",
        "\n",
        "$|01\\rangle \\mapsto|01\\rangle$\n",
        "\n",
        "$|10\\rangle \\mapsto|1\\rangle \\otimes U|0\\rangle=|1\\rangle \\otimes\\left(u_{00}|0\\rangle+u_{10}|1\\rangle\\right)$\n",
        "\n",
        "$|11\\rangle \\mapsto|1\\rangle \\otimes U|1\\rangle=|1\\rangle \\otimes\\left(u_{01}|0\\rangle+u_{11}|1\\rangle\\right)$\n",
        "\n",
        "The matrix representing the controlled $U$ is\n",
        "\n",
        ">$\n",
        "\\mathrm{C} U=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & u_{00} & u_{01} \\\\\n",
        "0 & 0 & u_{10} & u_{11}\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "**When U is one of the Pauli operators, X,Y, Z, the respective terms \"controlled-X\", \"controlled-Y\", or \"controlled-Z\" are sometimes used**. \n",
        "Sometimes this is shortened to just CX, CY and CZ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti0MmESYOnrd"
      },
      "source": [
        "<font color=\"blue\">*Why should we use more than one control Qubit?*\n",
        "\n",
        "**Remember from Eigenvalue problem: Ax = Œªx in our case with the unitary operator: Ux = Œªx**\n",
        "\n",
        "> $Ux =$ <font color=\"red\">$e^{2œÄi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$\n",
        "\n",
        "> Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2œÄi*0}$ = Œª = 1, so dass Ux = 1x. Damit ist Œª = 1 ist der Eigenwert von f.\n",
        "\n",
        "* Since |Œª| = 1, we can write it without loss of generality as Œª = $e^{2œÄiœÜ}$, where <font color=\"red\">$e^{2œÄi}$ = 1 (= identity, if you insert 2*œÄ*i into exponent at random, you will not change the result. Sometimes it can be a useful identity [Source](https://www.physicsforums.com/threads/e-2-pi-i-where-from.430393/), from Euler identity)</font> and **0 ‚â§ œÜ ‚â§ 1 is called the phase. This is what we want to estimate!**\n",
        "\n",
        "* We saw that in QFT, œÜ being between 0 and 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}$\n",
        "\n",
        "> **The term ‚Äúestimation‚Äù comes about not from the fact that quantum computation is probabilistic, but rather in the degree of precision that we are going to compute, or estimate, the phase to.**\n",
        "\n",
        "* The phase œÜ is going to be between zero and one, so we can write it as a decimal in binary notation as follows: $œÜ = 0.œÜ_1 œÜ_2 ¬∑¬∑¬∑œÜ_n$, where each œÜi is either zero or one\n",
        "\n",
        "  * The expression $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n}$ is equivalent to $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n} \\Longleftrightarrow \\phi=\\sum_{k=1}^{n} \\phi_{k} 2^{-k}$. Some numbers as binary decimals: \n",
        "  \n",
        "    * <font color=\"blue\">The number 0.5 in decimal is 0.1 in binary, since 0.1 ‚â° (1) ¬∑ $2^{‚àí1}$ = 1/2 = 0.5. So: $0.5_{10} = 0.1_2$. Note that 0.1 is the same as 0.100000....</font>\n",
        "\n",
        "    * <font color=\"blue\">The number 0.75 in decimal is 0.11 in binary, since 0.11 ‚â° (1)¬∑$2^{‚àí1}$ +1¬∑$2^{‚àí2}$ = 1/2+1/4 = 3/4 = 0.75. To get this we need 2 Qubits. So we get more precision with more qubits</font>\n",
        "\n",
        "    * 0.111 = 0.875\n",
        "\n",
        "    * 0.1111 = 0.9375 in decimal, because: $0 \\cdot 2^{0}+1 \\cdot 2^{-1}+1 \\cdot 2^{-2}+1 \\cdot 2^{-3}+1 \\cdot 2^{-4}=0 \\cdot 1+1 \\cdot 0.5+1 \\cdot 0.25+1 \\cdot 0.125+1 \\cdot 0.0625=0+0.5+0.25+0.125+0.0625=0.937510$\n",
        "\n",
        "  * Check also what is the value of the infinitely repeating binary decimal 0.1111111...\n",
        "  \n",
        "  * If it needed to be proved, the above exercise proves that 0 ‚â§ 0.œÜ1œÜ2 ¬∑ ¬∑ ¬∑ ‚â§ 1\n",
        "\n",
        "\n",
        "*Operator $U^{2^n}$ in QPE*\n",
        "\n",
        "* $U^{2^0}$: 1 (decimal) = 00001\n",
        "\n",
        "* $U^{2^1}$: 2 (decimal) = 00010\n",
        "\n",
        "* $U^{2^2}$: 4 (decimal) = 00100\n",
        "\n",
        "* $U^{2^3}$: 8 (decimal) = 01000\n",
        "\n",
        "* $U^{2^4}$: 16 (decimal) = 10000\n",
        "\n",
        "\n",
        "**So for falls die Phase 0.111 ist, wuerde bei 3 Qubits QPE berechnen:**\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "  * $2^0$ = 1 in decimal = 001 in binary\n",
        "\n",
        "  * $2^1$ = 2 in decimal = 010 in binary\n",
        "\n",
        "  * $2^2$ = 4 in decimal = 100 in binary\n",
        "\n",
        "* in this case the phase $\\theta$ = 0.111\n",
        "\n",
        "\n",
        "**Compare that with Quantum Fourier Transform:**\n",
        "\n",
        "* In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$ \n",
        "\n",
        "* Let's say all $x_1, x_2$ and $x_3$ = 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{{1}}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i}(0.5+0.25+0.125)}$ and in binary form: <font color=\"red\">$\\mathrm{e}^{2 \\pi \\mathrm{i}(0.100+0.010+0.001)}$</font>\n",
        "\n",
        "* **We see that in QFT and QPE it's the same (both in red)!**\n",
        "\n",
        "<font color=\"red\">Jedes $U^{2^n}$ wird immer dann aktiviert, wenn im Control-Qubit oben eine 1 gemessen wird (siehe Bild hier unten):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzz61dyQB_6X"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_118.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et3XRupiWg0n"
      },
      "source": [
        "*Step 3: Perform the inverse QFT on the estimation qubits and measure them*\n",
        "\n",
        "\n",
        "How can we read out this information from the quantum state? Consider the effect of applying another Hadamard transformation on the first qubit (without another H we will always measure 50/50 % a 0 or 1), which will produce (ignoring the normalization factor of 1/2): \n",
        "\n",
        "  * $H(|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0 \\cdot \\phi_{1}}$</font> $|1\\rangle)=$ $(1+$<font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font>$)|0\\rangle$ + $(1-$<font color=\"red\">$e^{2 \\pi i 0 . \\phi_{1}}$</font>$)|1\\rangle$\n",
        "\n",
        "  * this shares the phase with the first Qubit and allows us to read it out\n",
        "\n",
        "  * Now, $\\phi_{1}$ can only be zero or one. In the case that $\\phi_{1}=0, e^{2 \\pi i 0 . \\phi_{1}}=1$, hence the state is exactly $|0\\rangle$: $(\\frac{1}{2}\\left(1+e^{2 \\pi i 0 . 0}\\right)|0\\rangle+\\frac{1}{2}\\left(1-e^{2 \\pi i 0 . 0}\\right)|1\\rangle$ = $\\frac{1}{2}\\left(1+1\\right)|0\\rangle+\\frac{1}{2}\\left(1-1\\right)|1\\rangle$ = $|0\\rangle$\n",
        "\n",
        "  * these values in front of $|0\\rangle$ and $|1\\rangle$ are probabilities (here 0 has probability of being measured = 1, but small differences her reveal the phase and hence the Eigenvalue in other cases. See here:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)\n",
        "\n",
        "* **For 1 Qubit we can use a Hadamard Gate, and for more than 1 Qubit we use the inverse Fourier Transform**: on Quantum Phase Estimation $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ then the inverse Quantum Fourier transform:  <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font> so that: $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font>\n",
        "\n",
        "  * inverse QFT for 1 Qubit is: $ \\frac{1}{2^{\\frac{1}{2}}} \\sum_{x=0}^{2^{1}-1} e^{\\frac{-2 \\pi i k x}{2^{1}}}|x\\rangle$ = $\\frac{1}{\\sqrt{2}} e^{-1 \\pi i k x}$ fur $k$ = $\\varphi$ = 0 and $x$ = 0. --> somehting is not right here yet!\n",
        "\n",
        "Thus, we measure with certainty (i.e., not probabilistically) a state that tells us exactly what the phase,\n",
        "and hence the eigenvalue, is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv_xQnY__HDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9860e9b5-1f82-4d04-b9a6-696742dfb4d0"
      },
      "source": [
        "def make_qft_inverse(qubits):\n",
        "    \"\"\"Generator for the inverse QFT on a list of qubits.\"\"\"\n",
        "    qreg = list(qubits)[::-1]\n",
        "    while len(qreg) > 0:\n",
        "        q_head = qreg.pop(0)\n",
        "        yield cirq.H(q_head)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "            yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Do the inverse QFT\n",
        "phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "# Add measurements to the end of the circuit\n",
        "phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "0: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM('m')‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ       ‚îÇ         ‚îÇ                        ‚îÇ\n",
            "1: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ@^-0.5‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ             ‚îÇ           ‚îÇ            ‚îÇ\n",
            "2: ‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@^-0.25‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@^-0.5‚îÄ‚îÄ‚îÄH‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "          ‚îÇ   ‚îÇ   ‚îÇ\n",
            "u: ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄZ‚îÄ‚îÄ‚îÄS‚îÄ‚îÄ‚îÄT‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax explanation for list(qubits)[::-1]: list[<start>:<stop>:<step>]\n",
        "# So, when you do a[::-1], it starts from the end towards the first taking each element. \n",
        "# So it reverses a. This is applicable for lists/tuples as well.\n",
        "# Example: >>> a = '1234' >>> a[::-1] will get you: '4321'"
      ],
      "metadata": {
        "id": "4GAmFOoFH6wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 4: Simulate the circuit and convert from measured bit values to estimated Œ∏ values*"
      ],
      "metadata": {
        "id": "oJVEbP4OGB6p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPBFj8JyDYCd",
        "outputId": "91c04675-5f7f-4ad4-eb45-6c639f31a23a"
      },
      "source": [
        "# Simulate the circuit.\n",
        "sim = cirq.Simulator()\n",
        "result = sim.run(phase_estimator, repetitions=10)\n",
        "\n",
        "# Convert from output bitstrings to estimate Œ∏ values.\n",
        "theta_estimates = np.sum(2 ** np.arange(n_bits) * result.measurements['m'], axis=1) / 2**n_bits\n",
        "print(theta_estimates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(theta_estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of trials\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "7kifm9aPIcJd",
        "outputId": "da8725fb-30f2-48fa-8708-a6193bc8b7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iff49oDyK9"
      },
      "source": [
        "def phase_estimation(theta, n_bits, n_reps=10, prepare_eigenstate_gate=cirq.X):\n",
        "    # Define qubit registers.\n",
        "    qubits = cirq.LineQubit.range(n_bits)\n",
        "    u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "    # Define the unitary U.\n",
        "    U = cirq.Z ** (2 * theta)\n",
        "\n",
        "    # Start with Hadamards on every qubit.\n",
        "    phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "    # Do the controlled powers of the unitary U.\n",
        "    for i, bit in enumerate(qubits):\n",
        "        phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - 1 - i)))\n",
        "\n",
        "    # Do the inverse QFT.\n",
        "    phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Add measurements.\n",
        "    phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "\n",
        "    # Gate to choose initial state for the u_bit. Placing X here chooses the |1> state.\n",
        "    phase_estimator.insert(0, prepare_eigenstate_gate.on(u_bit))\n",
        "\n",
        "    # Code to simulate measurements\n",
        "    sim = cirq.Simulator()\n",
        "    result = sim.run(phase_estimator, repetitions=n_reps)\n",
        "\n",
        "    # Convert measurements into estimates of theta\n",
        "    theta_estimates = np.sum(2**np.arange(n_bits)*result.measurements['m'], axis=1)/2**n_bits\n",
        "\n",
        "    return theta_estimates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Analyze convergence vs n_bits.\"\"\"\n",
        "# Set the value of theta. Try different values.\n",
        "theta = 0.123456\n",
        "\n",
        "max_nvals = 16\n",
        "nvals = np.arange(1, max_nvals, step=1)\n",
        "\n",
        "# Get the estimates at each value of n.\n",
        "estimates = []\n",
        "for n in nvals:\n",
        "    estimate = phase_estimation(theta=theta, n_bits=n, n_reps=1)[0]\n",
        "    estimates.append(estimate)\n",
        "\n",
        "print(theta_estimates)\n",
        "print(estimates)\n",
        "\n",
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(nvals, estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of bits\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "id": "N5ZJqQs63tbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 5: Compute the Eigenvalues from the theta value*\n",
        "\n",
        "Eigenvalue: $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue, so for $\\theta$ = 0.125 $\\rightarrow$ $e^{2 * \\pi * i * 0.125}$ = <font color=\"blue\">0.707106781 + 0.707106781 i (Eigenvalue of T-gate)</font>\n",
        "\n",
        "* Verification: $\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$ $\\rightarrow$ <font color=\"blue\">$e^{\\frac{i \\pi}{4}}$ is the same as $e^{2 * \\pi * i * 0.125}$ bzw. $e^{\\frac{2 * \\pi * i}{8}}$</font>\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "* ps: $e^{2 \\pi i}$ =  1 (identity) - ohne theta, die phase\n",
        "\n",
        "* From Eigenvalue euqation: $Ux =$ <font color=\"red\">$e^{2œÄi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$. Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2œÄi*0}$ = Œª = 1, so dass Ux = 1x. Damit ist Œª = 1 ist der Eigenwert von f.\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/9577/how-to-find-eigenvalues-and-eigenvector-for-a-quantum-gate\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_115.png)\n"
      ],
      "metadata": {
        "id": "DvJzmiWfKQi0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiqOEUrSLR_5"
      },
      "source": [
        "###### *Shor's Algorithm*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6yeVZAuzrMk"
      },
      "source": [
        "**Classical Calculation**\n",
        "\n",
        "* <font color=\"blue\">Factoring is equivalent to finding a nontrivial squareroot of 1 mod N.\n",
        "\n",
        "* all we need to do is find this nontrivial squareroot of unity, and we can factor whatever number we need. As promised, we can do this with period finding, specifically by computing the order of a random integer\n",
        "\n",
        "* The order of some integer x modulo N is the smallest integer r such that $x^r$ = 1 mod N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCxHYDSsb6-y"
      },
      "source": [
        "*Modular Arithmetic*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_088.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_089.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axJsmDe1lF5Y"
      },
      "source": [
        "**<font color=\"blue\">Step 1: Pick coprime of N**\n",
        "\n",
        "*Drei m√∂gliche Verfahren zur Berechnung des ggT :*\n",
        "\n",
        "Erstes Verfahren: Euklidischer Algorithmus \n",
        "* 15\t:\t13\t  = \t1\t  Rest  \t2.\t  Also ist ggT (15,13)= ggT (13,2)\n",
        "* 13\t:\t2\t  = \t6\t  Rest  \t1.\t  Also ist ggT (13,2)= ggT (2,1)\n",
        "* 2\t:\t1\t  = \t2\t  Rest  \t0.\t  Also ist ggT (2,1)= ggT (1,0)\n",
        "* Ergebnis: Der ggT von 15 und 13 ist 1.\n",
        "\n",
        "Zweites Verfahren: Vergleichen der Teilermengen .\n",
        "* Die Teilermenge von 15 lautet: {1,3,5,15}.\n",
        "* Die Teilermenge von 13 lautet: {1,13}.\n",
        "* Die gr√∂√üte in beiden Teilermengen vorkommende Zahl ist 1. Also ist 1 der ggT von 15 und 13. \n",
        "\n",
        "Dritte M√∂glichkeit: Vergleichen der Primfaktorzerlegung\n",
        "* Die Primfaktorzerlegung von 15 lautet: 15= 3¬∑5.\n",
        "* Die Primfaktorzerlegung von 13 lautet: 13= 13.\n",
        "* Die gemeinsamen Primfaktoren sind: 1.\n",
        "* Also ist 1 der ggT.\n",
        "\n",
        "*Modulo (kurz: mod) berechnet den Rest einer Division zweier Zahlen. In Mathematischen Formeln wird modulo mit mod abgek√ºrzt, beispielsweise: 23 mod 8 = 7. Bei dieser Rechnung kommt 7 heraus, weil die 8 zweimal in die 23 passt und dann 7 √ºbrig bleiben.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48daYc8to4oz",
        "outputId": "0e23b039-edab-4720-c3ec-36000abf9910"
      },
      "source": [
        "# Product of two prime numbers (to check later if result is correct)\n",
        "N=5 * 3\n",
        "\n",
        "# Pick coprime (!) number to N to factorize N into primes\n",
        "a=13\n",
        "\n",
        "# Code Example to understand periodicity in the context of factoring prime numbers:\n",
        "\n",
        "import math\n",
        "# Compute greated common divisor between a and N\n",
        "math.gcd(a, N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqnmPhvoTQw"
      },
      "source": [
        "**<font color=\"blue\">Step 2: Find the period of $a^r$ $\\equiv$ 1 $(modN)$:**\n",
        "\n",
        "* <font color=\"blue\">the order of x is just the period of the function f(i) = $x^i$ mod N. \n",
        "\n",
        "* <font color=\"blue\">In quantum computing you use QFT in order to determine the period !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "3SoazOyVpdiU",
        "outputId": "90753159-c874-45b6-f12b-f94bf1bcbb72"
      },
      "source": [
        "import matplotlib. pyplot as plotter\n",
        "sns.set(rc={'figure.figsize':(12, 5), \"lines.linewidth\": 1.5})\n",
        "\n",
        "r = list(range(N))\n",
        "y= [a**r0 % N for r0 in r]\n",
        "\n",
        "plotter.plot (r, y)\n",
        "plotter.xlabel('r')\n",
        "plotter.ylabel('Rest:' f'{a}^r (mod{N})')\n",
        "plotter.title('Periode der Restwerte (aus den Multiples von r)')\n",
        "plotter.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qyWTdoZDtL"
      },
      "source": [
        "<font color=\"red\">**Beispiel: Choose any number $a$ and takes its multiple $r$ so many times, until the rest in modulo is 1, (except r=0)**</font>\n",
        "\n",
        "> $13^0$ (mod 15) = 1 (mod 15) = 1\n",
        "\n",
        "> $13^1$ (mod 15) = 13 \n",
        "\n",
        "> $13^2$ (mod 15) = 169 (mod 15) = 4\n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 11 = 165, bis zur 169 verbleibt ein Rest 4*\n",
        "\n",
        "> $13^3$ (mod 15) = 2197 (mod 15) = 7 \n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 146 = 2190, bis zur 2197 verbleibt ein Rest 7*\n",
        "\n",
        "> $13^4$ (mod 15) = 28561 (mod 15) = 1 (<font color=\"blue\"><u>hier started die Periode wieder, that's the r we are looking for!</u>)\n",
        "\n",
        "> usw.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dblciep4qxfM",
        "outputId": "3be061ad-66e6-4c88-8020-117f8c632318"
      },
      "source": [
        "r= r[y[1:].index(1)+1]\n",
        "print(f'r = {r}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HLzUiXj6AR"
      },
      "source": [
        "**<font color=\"blue\">Step 3: Bestimme $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$**. Mindestens einer der beiden Primfaktoren von N={p,q} is beinhalted in gcd(x+1, N) bzw. gcd(x-1, N)\n",
        "\n",
        "*In this case with a=13, N=15 and r=4:*\n",
        "\n",
        "* $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$\n",
        "\n",
        "* $x \\equiv 13^{\\frac{4}{2}}(\\operatorname{mod} 15)$\n",
        "\n",
        "* x = 169 (mod 15) = 4\n",
        "\n",
        "  * gcd(x-1, N) = 3 = p\n",
        "\n",
        "  * gcd(x+1, N) = 5 = q\n",
        "\n",
        "Achtung: in einem anderen Beispiel: N=11*7 (Primzahlen), a=18, ergibt x=43. \n",
        "\n",
        "* Davon x-1=42 und x+1=44. \n",
        "* Das sind naturlich keine Primzahlen, \n",
        "* Aber deren Faktoren sind: 44 = 2 * 2 * 11 und 42 = 2 * 3 * 7\n",
        "* das heisst, x-1 und x+1 kann auch die Primzahlen indirekt enthalten!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8MwnRiUrqhR",
        "outputId": "799b2778-511f-4484-dfa9-ff854e1d871e"
      },
      "source": [
        "if r % 2 == 0:\n",
        "  x = (a**(r/2.)) % N\n",
        "  print(f'x = {x}')\n",
        "  if ((x + 1) % N) != 0:\n",
        "    print(math.gcd((int(x)+1), N), math.gcd((int(x)-1), N))\n",
        "  else:\n",
        "      print (\"x + 1 is 0 (mod N)\")\n",
        "else:\n",
        "  print (f'r = {r} is odd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 4.0\n",
            "5 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rj2GI7_cC86"
      },
      "source": [
        "**Shor's Algorithm**\n",
        "\n",
        "* When finding order using the period finding algorithm, it is important to use enough qubits. A sensible rule is that you need to use m qubits so that $2^m$ >> $N^2$, where N is the number we are trying to factor, because the order of a random number might be as large as N\n",
        "\n",
        "* Example: Lets factor N=119. Suppose we pick the number 16 to start with. Wie viele Qubits m sollten wir mindestens nehmen? $N^2$ = $119^2$ =14.161 und $2^m$ muss deutlich grosser sein, also mindestens = $2^{14}$ = 16.384. Wir brauchen also mindestens 14 Qubits, um 119 zu faktorisieren.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_090.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrrdCnywczMT"
      },
      "source": [
        "* Because we know that the order of x will be even and $x^{s/2}$ will be a nontrivial square root with probability at least 1/2, we can be confident that we will be able to factor N in just a few runs of the algorithm. Because the time it takes to find the period grows as a polynomial in the number of bits, and the number of bits grows like 2logN(by the above requirement), we expect the time it takes to factor N to grow as a polynomial in logN.\n",
        "\n",
        "* Here is the circuit for Shor‚Äôs Algorithm. It relies heavily on period finding, and so the circuit looks a lot like the circuit for period finding. The key difference is that we are finding the period of f(i) = xi, and the number of bits we need to input is very large.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_091.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQthOfm_JfT"
      },
      "source": [
        "**How does it work in the quantum circuit?**\n",
        "\n",
        "That's the function in $U$: given an $x$, the $U$ will compute:\n",
        "\n",
        "> $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_092.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_093.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mApRUwSPydz"
      },
      "source": [
        "**<font color=\"blue\">Shor's Algorithm: Step by Step**\n",
        "\n",
        "**Beispiel: a=13 und N=15, was macht Shor's Algorithm genau im Circuit an der Stelle $U_{f_{(a,N)}}$ und $QFT^{\\dagger}$?**\n",
        "\n",
        "ps: a muss ein Coprime von N sein. Wenn es kein Coprime ist, muessen wir nicht durch Shor's Algorithm gehen, weil a dann einen Faktor mit N teilt :) Aber es ist very unlikely to find a coprime of a large number N.\n",
        "\n",
        "**First let's divide it into steps. 1-5:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrEvCMunLBeC"
      },
      "source": [
        "**Step 1**: Get Qubits in state 0 and apply Hadamard Superposition\n",
        "\n",
        "We start with 4 Qubits all in zeros, mit den Registers x und w, und jedes 4 Mal Tensorproduct multipliziert, weil wir 4 Qubits haben: \n",
        "\n",
        "> $|0\\rangle_{x}^{\\otimes 4}$ $|0\\rangle_{w}^{\\otimes 4}$\n",
        "\n",
        "All Hamadard Gates are applied to top 4 Qubits (x register), and right part (w register) gets nothing applied to it:\n",
        "\n",
        "> $[H^{\\otimes 4}|0\\rangle] \\,\\, |0\\rangle^{\\otimes^{4}}$\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle+|1\\rangle+|2\\rangle+\\cdots+|15\\rangle]$ $|0\\rangle$\n",
        "\n",
        "* Reminder 1: Multiplikation mit $\\frac{1}{4}$, weil 4 Qubits in Hadamard-Superposition\n",
        "\n",
        "* Reminder 2: this is the 4 bit representation of the decimal number, so for example 15 in binary = 1111. Daher kann man auch die 4 angeben als Erinnerung der Bit representation:\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle_4+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $|0\\rangle_4$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKOyXX5bG_i1"
      },
      "source": [
        "**Step 2**: Compute $U$ with $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$ - Was passiert genau in der Box mit $U_{f_{(a,N)}}$?\n",
        "\n",
        "**Given an $x$, the $U$ will compute: <font color=\"red\">$f_{a, N}(x) \\equiv a^{x}(\\bmod N)$</font>**\n",
        "\n",
        "Schauen wir nochmal im vorherigen Schritt und markieren eine Komponente:\n",
        "\n",
        "> = $\\frac{1}{4}[$ <font color=\"red\">$|0\\rangle_4$</font> $+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $\\,$ $|0\\rangle_4$\n",
        "\n",
        "<font color=\"red\">$U_{f_{(a,N)}}$</font> macht dann folgendes:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|  0 \\bigoplus 13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|0 \\bigoplus 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|0 \\bigoplus 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|0 \\bigoplus 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Remember: $\\bigoplus$ means \"addition modular 2\" bzw. \"XOR\". Anything XORs with 0, is thing itself: 0 $\\bigoplus$ Z = Z. damit ergibt sich folgende Rechnung:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|   13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left| 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left| 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left| 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "\n",
        "Aus der Modulo-Rechnung ergeben sich die Restwerte:\n",
        "\n",
        "* <font color=\"red\">$13^{0}(\\bmod 15)$ = 1</font>\n",
        "\n",
        "* $13^{1}(\\bmod 15)$ = 13\n",
        "\n",
        "* $13^{2}(\\bmod 15)$ = 4\n",
        "\n",
        "* $13^{3}(\\bmod 15)$ = 7\n",
        "\n",
        "* $13^{4}(\\bmod 15)$ = 1\n",
        "\n",
        "* usw..\n",
        "\n",
        "Since it's periodic, it will repeat, with the x and w register:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\,\\left|1\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|13\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|4\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|7\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Hier nochmal untereinander mit denselben Restwerten zur besseren Visualisierung:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_094.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-QZylnP7nv"
      },
      "source": [
        "**Step 3: Measurement of the w register / bottom 4 Qubits**\n",
        "\n",
        "* the outputs of the w-register measurements are either 1, 13, 4 or 7 (die Restwerte) with equal probability\n",
        "\n",
        "* let's say we measure 7, what happens to x? X becomes either 3, 7, 11 or 15 (the value in front of the qubit with 7!) with equal probability:\n",
        "\n",
        "  * after $|\\omega\\rangle$ = $|7\\rangle_4$ , $|x\\rangle$ becomes:\n",
        "\n",
        "  * <font color=\"blue\">$|x\\rangle$ $|\\omega\\rangle$ = $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$ $\\otimes |7\\rangle_4$\n",
        "\n",
        "  * Normalization has changed: before we had 16 combinations mit 1/4, here we have only 4 combinations with 1/2 (=one over square root of 4)\n",
        "\n",
        "* **For the next step 4, the Restwert doesn't matter anymore, here: $\\otimes |7\\rangle_4$. We can ignore it. Because it step 4 we apply the measured $|x\\rangle$ in the $QFT^{\\dagger}$, and don't care about $|\\omega\\rangle$ anymore**. And $|x\\rangle$ is in this case: $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exkurs: Eine komplexe Zahl $z=a+b i$ und die zu ihr konjugiert komplexe Zahl $\\bar{z}=a-b i$*:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Komplexe_konjugation.svg/294px-Komplexe_konjugation.svg.png)\n",
        "\n",
        "√Ñndert man das Vorzeichen des Imagin√§rteils $b$ einer komplexen Zahl \n",
        "\n",
        "> $z=a+b \\mathrm{i}$\n",
        "\n",
        "so erh√§lt man die zu $z$ konjugiert komplexe Zahl \n",
        "\n",
        "> $\\bar{z}=a-b \\mathrm{i}$ \n",
        "\n",
        "(manchmal auch $z^{*}$ geschrieben).\n",
        "\n",
        "Die Konjugation $\\mathbb{C} \\rightarrow \\mathbb{C}, z \\mapsto \\bar{z}$ ist ein (involutorischer) K√∂rperautomorphismus, da sie mit Addition und Multiplikation vertr√§glich ist, d. h., f√ºr alle $y, z \\in \\mathbb{C}$ gilt\n",
        "\n",
        ">$\n",
        "\\overline{y+z}=\\bar{y}+\\bar{z}, \\quad \\overline{y \\cdot z}=\\bar{y} \\cdot \\bar{z}\n",
        "$\n",
        "\n",
        "In der Polardarstellung hat die konjugiert komplexe Zahl $\\bar{z}$ bei unver√§ndertem Betrag gerade den negativen Winkel von $z$. \n",
        "\n",
        "* **Man kann die Konjugation in der komplexen Zahlenebene also als die Spiegelung an der reellen Achse interpretieren**. \n",
        "\n",
        "* <font color=\"blue\">**Insbesondere werden unter der Konjugation genau die reellen Zahlen auf sich selbst abgebildet**.\n",
        "\n",
        "Das Produkt aus einer komplexen Zahl $z=a+b$ i und ihrer komplex Konjugierten $\\bar{z}$ ergibt das Quadrat ihres Betrages:\n",
        "\n",
        "> $\n",
        "z \\cdot \\bar{z}=(a+b i)(a-b i)=a^{2}+b^{2}=|z|^{2}\n",
        "$\n",
        "\n",
        "Die komplexen Zahlen bilden damit ein triviales Beispiel einer [C*-Algebra](https://de.m.wikipedia.org/wiki/C*-Algebra)."
      ],
      "metadata": {
        "id": "imQLT4Zc7er9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_7uaW02Wyvg"
      },
      "source": [
        "**Step 4**: Apply inverse $QFT^{\\dagger}$ on the $|x\\rangle$ register\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$ (Reminder!)\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$ (see -2 turning i in -i which is a **complex conjugate operation**)\n",
        "\n",
        "* We want to know what QFT dagger is doing to (it is $\\frac{1}{\\sqrt{16}}$ because we have 4 Qubits)\n",
        "\n",
        "  * $QFT^{\\dagger}|3\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 3 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|7\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 7 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|11\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 11 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|15\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 15 y}{16}}|y\\rangle$\n",
        "\n",
        "Alltogether:\n",
        "\n",
        "  * $QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}} \\sum_{y=0}^{15}$ [ $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$] $|y\\rangle$\n",
        "\n",
        "    * with: $e^{-i\\frac{ 3 \\pi}{8}y}$ = $\\cos \\left(\\frac{3 \\pi}{8} y\\right)-i \\sin \\left(\\frac{3 \\pi}{8} y\\right)$ (und aquivalent fur alle anderen drei)\n",
        "\n",
        "    * siehe coding rechnung unten was genau passiert hier!\n",
        "  \n",
        "  * <font color=\"blue\">$QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}}$ [ $4|0\\rangle_4$ + $4i|4\\rangle_4$ $-4|8\\rangle_4$ $-4i|12\\rangle_4$ ]</font>\n",
        "  \n",
        "  * Remember we had a sum before: $\\frac{1}{{8}} \\sum_{y=0}^{15}$. And notice how all the other terms now vanished to zero, because you had equal contributions of plus and minus.\n",
        "\n",
        "    * **This is exactly what it means when people tell you that quantum computers take advantage of interference!! = when a lot of the terms vanish, and the answer only converges to the terms that we care about.**\n",
        "\n",
        "    * here is the calculation what happened, you see many zeros:\n",
        "\n",
        "<font color=\"red\">Hier Beispielrechnung fur y=1, um vanishing components zu verstehen</font>. Unten im Code die Ergebnisse, zum Beispiel fur y=1 als Ergebnis = 0, $QFT^{\\dagger}|x\\rangle$ fur y = 1: \n",
        "    \n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$ =\n",
        "    \n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}1}$ + $e^{-i\\frac{ 7 \\pi}{8}1}$ + $e^{-i\\frac{ 11 \\pi}{8}1}$ + $e^{-i\\frac{ 15 \\pi}{8}1}$ =\n",
        "\n",
        "    * $e^{-i\\frac{ 3 \\pi}{8}1}$ = <font color=\"green\">0,382683432 - 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 7 \\pi}{8}1}$ = <font color=\"orange\">-0,923879533 - 0,382683432 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 11 \\pi}{8}1}$ = <font color=\"green\">-0,382683432 + 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 15 \\pi}{8}1}$ = <font color=\"orange\">0,923879533 + 0,382683432 i</font>\n",
        "\n",
        "  * Wie man sieht canceln sich die Terme aus (in gleicher Farbe), weshalb als Ergebnis fur y=1 Null entsteht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWn_Jmu_OMSJ",
        "outputId": "20ecb4a6-6407-4ebd-abb1-14bece962df0"
      },
      "source": [
        "# Hier Beispiel fur y=1 und den ersten e-Term:\n",
        "y = 1\n",
        "pi = np.pi\n",
        "coeff = np.exp(-1j*3*pi/8 * y)\n",
        "if abs(coeff) < 1e-10: coeff= 0\n",
        "print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 (0.38268343236508984-0.9238795325112867j)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iERBMPR2y9hm",
        "outputId": "5ffcb615-a899-4d10-f474-8398f25e98f9"
      },
      "source": [
        "# Hier die komplette Rechnung fur alle y und alle 4 e-Terme:\n",
        "import numpy as np\n",
        "\n",
        "pi = np.pi\n",
        "for y in range (15) :\n",
        "  coeff = np.exp(-1j*3*pi/8 * y) + \\\n",
        "          np.exp(-1j*7*pi/8 * y) + \\\n",
        "          np.exp(-1j*11*pi/8* y) + \\\n",
        "          np.exp(-1j*15*pi/8* y)\n",
        "  if abs(coeff) < 1e-10: coeff= 0\n",
        "  print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (4+0j)\n",
            "1 0\n",
            "2 0\n",
            "3 0\n",
            "4 (-5.757077917265737e-15+4j)\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 (-4-1.1514155834531474e-14j)\n",
            "9 0\n",
            "10 0\n",
            "11 0\n",
            "12 (2.2600304269997962e-14-4j)\n",
            "13 0\n",
            "14 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXrcIqo14n0"
      },
      "source": [
        "**Step 5: Measure the |x> register**\n",
        "\n",
        "* You get either 0 or 4 or 8 or 12 with equal probability\n",
        "\n",
        "* Remaining steps are classical post-processing\n",
        "\n",
        "* You can already see the periodicity in the result: the difference is always 4\n",
        "\n",
        "* Analyse what happens for each outcome: **The measurement results peak near $j\\frac{N}{r}$ for same integer j $\\in Z$. And r is the period that we are looking for. N = $2^n$ Qubits!**\n",
        "\n",
        "  * if we measure |4>$_4$: $j\\frac{16}{r}$ = 4, true if j=1 and r=4\n",
        "\n",
        "  * there are multiple values that would work, but this is the lowest one\n",
        "\n",
        "* now check our protocoll for r=4:\n",
        "\n",
        "  * Is r even? yes!\n",
        "\n",
        "  * $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{4 / 2}(\\bmod 15)$ = 4\n",
        "\n",
        "  * x+1 = 5 and x-1 = 3\n",
        "\n",
        "* This looks good, now check:\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(5,15)=5$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "What do you do if r = 8 ?\n",
        "\n",
        "* |8>$_4$: $j\\frac{16}{r}$ = 8, true if j=1 and r=2 AND j=2 and r=4\n",
        "\n",
        "* if r=4 we are back in the case before\n",
        "\n",
        "* if r=2 then $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{2 / 2}(\\bmod 15)$ = 2, which brings x+1 = 3 and x-1 = 1\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(1,15)=1$\n",
        "\n",
        "* This leads you to a partial solution. Now you can back out the other solution, with checking 3 divides into 15\n",
        "\n",
        "* If we get r=0, then we need to do the experiment again\n",
        "\n",
        "Hier die Faktorisierungsergebnisse fur verschiedene QC-Ausgaben r. Mit r=0 geht es nicht, also kann man in 3 von 4 Faellen faktorisieren (und mit r=8 bekommt man eine partial solution, kann aber immer noch faktorisieren).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_097.png)\n",
        "\n",
        "Aus dem 2001 Paper von IBM, Faktorisierung von 15 auf einem Quantum Computer:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_096.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8beuwX8zVd_"
      },
      "source": [
        "**Appendix: What is the Gate structure in $U$?**\n",
        "\n",
        "* $a^{x_1}$, $a^{x_2}$, $a^{x_n}$ tells you this is a controlled operation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_098.png)\n",
        "\n",
        "* look now how the exponent doesn't contain $x_1$, $x_2$, .. $x_n$ anymore\n",
        "\n",
        "* this is done by implementing it by doing these controls\n",
        "\n",
        "* this is exactly like quantum phase estimation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_099.png)\n",
        "\n",
        "**Der linke Term stammt aus QPE, der rechte Term ist der Teil $U$ aus Shor's Algorithms:**\n",
        "\n",
        "> <font color=\"blue\">$U^{2^{x}}=a^{2^{x}}(\\bmod N)$</font>\n",
        "\n",
        "continue: https://youtu.be/IFmkzWF-S2k?t=1181"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0aBln0tpWG"
      },
      "source": [
        "###### *Grover Search*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgjkcQ3gZhD"
      },
      "source": [
        "> **[Grover‚Äôs algorithm](https://en.m.wikipedia.org/wiki/Grover's_algorithm) teaches us how we can search for an item in an unsorted list without needing to look at each item one by one but by looking at them all at once.**\n",
        "\n",
        "It accomplishes that using two techniques:\n",
        "\n",
        "  * First, it uses a quantum oracle to mark the searched state. \n",
        "  \n",
        "  * Second, it uses a diffuser that amplifies the amplitude of the marked state to increase its measurement probability.\n",
        "\n",
        "Grover's Algorithm : Suche in grossen Datenbanken (Squared speedup: get result in the square root of time that on classical computers)\n",
        "\n",
        "* Auf einem klassischen Computer ist der prinzipiell schnellstm√∂gliche Suchalgorithmus in einer unsortierten Datenbank die [lineare Suche](https://de.m.wikipedia.org/wiki/Lineare_Suche), die ${\\mathcal {O}}\\left(N\\right)$ Rechenschritte erfordert (Der Suchaufwand w√§chst linear mit der Anzahl der Elemente in der Liste.)\n",
        "\n",
        "* Die effizientere [Bin√§re Suche](https://de.m.wikipedia.org/wiki/Bin√§re_Suche) kann nur bei geordneten Listen benutzt werden. Die Bin√§re Suche ist deutlich schneller als die lineare Suche, welche allerdings den Vorteil hat, auch in unsortierten Feldern zu funktionieren. In Spezialf√§llen kann die [Interpolationssuche](https://de.m.wikipedia.org/wiki/Interpolationssuche) schneller sein als die bin√§re Suche.\n",
        "\n",
        "* Makes use of Amplitude Amplification, Quantum Walk & Quantum Counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBmtO9AqRfgC"
      },
      "source": [
        "> Grover's Algorithm uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states (=Phase Flip of Desired Outcome + Probability Amplitudes Inversion about the Mean to Amplify)\n",
        "\n",
        "Grover‚Äôs algorithm solves oracles that **add a negative phase to the solution states**. I.e. for any state |x‚ü© in the computational basis:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=\\left\\{\\begin{aligned}|x\\rangle & \\text { if } x \\neq \\omega \\\\-|x\\rangle & \\text { if } x=\\omega \\end{aligned}\\right.$\n",
        "\n",
        "We create a function $f$ that takes a proposed solution $x$, and returns \n",
        "\n",
        "* $f(x)=0$ if $x$ is not a solution ( $x \\neq \\omega)$ \n",
        "\n",
        "* $f(x)=1$ for a valid solution $(x=\\omega)$.\n",
        "\n",
        "The oracle can then be described as:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=(-1)^{f(x)}|x\\rangle$\n",
        "\n",
        "* you can see this is an Eigenvalue equation\n",
        "\n",
        "The oracle's matrix will be a diagonal matrix of the form:\n",
        "\n",
        "> $U_{\\omega}=\\left[\\begin{array}{cccc}(-1)^{f(0)} & 0 & \\cdots & 0 \\\\ 0 & (-1)^{f(1)} & \\cdots & 0 \\\\ \\vdots & 0 & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & (-1)^{f\\left(2^{n}-1\\right)}\\end{array}\\right]$\n",
        "\n",
        "*Source: https://qiskit.org/textbook/ch-algorithms/grover.html*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEw1xzY4XrR"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_105.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9OKn5QL4ZEt"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_106.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9O3gEWD4ayN"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_107.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJDLETO4cKX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_108.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrowEYzZ4Ptk"
      },
      "source": [
        "*Step 1: Hadamard-Operator for Superposition + Assign a Phase -1 to the desired outcome*\n",
        "\n",
        "Start with a balanced superposition, and assign a phase of -1 to the chosen ket, 111). Assigning -1 means applying a Pauli-Z-operator in the superposition to this one !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_102.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_101.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuU4yrjT4NB"
      },
      "source": [
        "**<font color=\"blue\">More about Amplitude Amplification**\n",
        "\n",
        "* [Amplitude amplification](https://en.m.wikipedia.org/wiki/Amplitude_amplification) is a technique in quantum computing which **generalizes the idea behind the Grover's search algorithm**, and gives rise to a family of quantum algorithms.\n",
        "\n",
        "* In a quantum computer, amplitude amplification can be used to **obtain a quadratic speedup over several classical algorithms**.\n",
        "\n",
        "1. If there are $G$ good entries in the database in total, then we can find them by initializing a quantum register $|\\psi\\rangle$ with $n$ qubits where $2^{n}=N$ into a uniform superposition of all the database elements $N$ such that\n",
        "\n",
        ">$| \\psi \\rangle=\\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1}|k\\rangle\n",
        "$\n",
        "\n",
        "2. and running the above algorithm. In this case the overlap of the initial state with the good subspace is equal to the square root of the frequency of the good entries in the database, $\\sin (\\theta)=|P| \\psi\\rangle \\mid=\\sqrt{G / N}$. If $\\sin (\\theta) \\ll 1$, \n",
        "\n",
        "3. we can\n",
        "approximate the number of required iterations as\n",
        "\n",
        ">$\n",
        "n=\\left\\lfloor\\frac{\\pi}{4 \\theta}\\right\\rfloor \\approx\\left\\lfloor\\frac{\\pi}{4 \\sin (\\theta)}\\right\\rfloor=\\left\\lfloor\\frac{\\pi}{4} \\sqrt{\\frac{N}{G}}\\right\\rfloor=O(\\sqrt{N})\n",
        "$\n",
        "\n",
        "Measuring the state will now give one of the good entries with high probability.\n",
        "\n",
        "Since each application of $S_{P}$ requires a single oracle query (assuming that the oracle is implemented as a quantum gate), we can find a good entry with just $O(\\sqrt{N})$ oracle queries, thus obtaining a quadratic speedup over the best possible classical algorithm. (The classical method for searching the database would be to perform the query for every $e \\in\\{0,1, \\ldots, N-1\\}$ until a solution is found, thus costing $O(N)$ queries.) Moreover, we can find all $G$ solutions using $O(\\sqrt{G N})$ queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoynUeh6XX0t"
      },
      "source": [
        "* Now, let‚Äôs say four qubits are enough and Mr. Grover is known as |0010‚ü©. The oracle uses the specific characteristic of this state to identifying it. That is the state has a |1‚ü© at the third position and |0‚ü© otherwise.\n",
        "\n",
        "* Since the quantum oracle takes all qubits as input, it can easily apply a transformation of this exact state. It doesn‚Äôt matter whether we use four qubits or 33. The oracle identifies Mr. Grover in a single turn.\n",
        "\n",
        "* The transformation the oracle applies to the searched state is an inversion of the amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_109.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_110.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkLvP9cX-_N"
      },
      "source": [
        "* In the representation above representation (the dark one) of the amplitudes, we can clearly see a difference between the searched state and all the other states. We could prematurely declare the search is over.\n",
        "\n",
        "* The only difference is in the sign of the amplitude. For the measurement probability results from the amplitude‚Äô absolute square, the sign does not matter at all.\n",
        "\n",
        "* The amplitude originates from the concept that every quantum entity may be described not only as a particle but also as a wave. The main characteristic of a wave is that it goes up and down as it moves. The amplitude is the distance between the center and the crest of the wave.\n",
        "\n",
        "* If we invert the amplitude of a wave at all positions, the result is the same wave shifted by half of its wavelength.\n",
        "\n",
        "* These two waves differ only in their relative position. This is the phase of the wave. For the outside world, the phase of a wave is not observable. Observed individually, the two waves appear identical. So, the problem is we can‚Äôt tell the difference between these two waves.\n",
        "\n",
        "> As a consequence, the system does not appear any different from the outside. Even though the oracle marked the searched state and it, therefore, differs from the other states, all states still have the same measurement probability.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_111.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XWXZNDvSt2X"
      },
      "source": [
        "*Step 2: Invert all probability amplitudes about the mean + Measure*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WSsCfvzaSx-"
      },
      "source": [
        "* We need to turn the difference into something measurable. We need to increase the measurement probability of the marked state. This is the task of the diffuser. The diffuser applies an inversion about the mean amplitude.\n",
        "\n",
        "* Let‚Äôs have a look at the average amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_112.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63cW7pbbQVL"
      },
      "source": [
        "* With four qubits, we have 16 different states. Each state has an amplitude of 1/sqrt(16)=1/4. In fact, each but one state ‚Äî the searched state has this amplitude. The searched state has an amplitude of ‚àí1/4. Thus, the average is (15‚àó1/4‚àí1/4)/16=0.21875.\n",
        "\n",
        "* The average is a little less than the amplitude of all states we did not mark. If we invert these amplitudes by this mean, they end up a little lower than the average at 0.1875.\n",
        "\n",
        "* For the amplitude of the marked state is negative, it is quite far away from the average. The inversion about the mean has a greater effect. It flips the amplitude from ‚àí0.25 by 2‚àó(0.25+0.21875) to 0.6875 (bzw: =0,21875-(-0,25-0,21875)).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_113.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZMnRRIbolE"
      },
      "source": [
        "* The inversion about the mean works well if we search for a single or a few negative amplitudes among many positive amplitudes. Then, this operation increases the negative amplitudes we know are the correct ones. And this operation decreases the positive amplitudes, we know are wrong.\n",
        "\n",
        "> This operation increases the negative amplitude by a large amount while decreasing the positive amplitudes by a small amount.\n",
        "\n",
        "* But the more states we have, the lower the overall effect will be. In our example, we calculated the new amplitude of the searched state as 0.6875. The corresponding measurement probability is 0.6875^2=0.47265625. Accordingly, we measure this system only about every other time in the state we are looking for. Otherwise, we measure it in any other case.\n",
        "\n",
        "* Of course, we could now measure the system many times and see our searched state as the most probable one. But running the algorithm so many times would give away any advantage we gained from not searching all the states.\n",
        "\n",
        "> **Instead, we repeat the algorithm. We use the same oracle to negate the amplitude of the searched state. Then we invert all the amplitudes around the mean, again**.\n",
        "\n",
        "However, we must not repeat this process too many times. There is an optimal number of times of repeating this process to get the greatest chance of measuring the correct answer. \n",
        "\n",
        "* The probability of obtaining the correct result grows until we reach about œÄ/4*sqrt(N) with N is the number of states of the quantum system. Beyond this number, the probability of measuring the correct result decreases again.\n",
        "\n",
        "* In our example with four qubits and N=16 states, the optimum number of iterations is 3.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_104.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuIq27h5ge43"
      },
      "source": [
        "https://towardsdatascience.com/towards-understanding-grovers-search-algorithm-2cdc4e885660"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWV-r4APgANP"
      },
      "source": [
        "*Grover Search: Simple Examples (Z-Gate and I-Gate as amplifiers. with H as Diffuser)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77PyMqZgnEg"
      },
      "source": [
        "**Example 1: Z Gate to switch from 0 to 1**\n",
        "\n",
        "* Let‚Äôs say the state |1‚ü© depicts the favorable state we want to find. Then, the oracle consists of the Z-gate that switches the amplitude when the corresponding qubit is in state |1‚ü©.\n",
        "\n",
        "* As a result, we see the amplitude changed for state |1‚ü©. The qubit is now in state |‚àí‚ü©. Its two states |0‚ü© and |1‚ü© are in two different phases, now.\n",
        "\n",
        "* In other words, we flipped the amplitude of state |1‚ü© from positive to negative.\n",
        "\n",
        "* Both states still have a measurement probability of 0.5. It is the task of the diffuser to magnify the amplitude to favor the searched state.\n",
        "\n",
        "* **The diffuser in a single-qubit circuit is quite simple. It is another H-gate**. This circuit results in state |1‚ü© with absolute certainty.\n",
        "\n",
        "We apply an important sequence on the qubit, the HZH-circuit. This circuit is known as an identity to the NOT-gate (X-gate) that turns state |0‚ü© into |1‚ü© and vice versa.\n",
        "\n",
        "The following equation proves this identity.\n",
        "\n",
        "> $H Z H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=X$\n",
        "\n",
        "Then, why would we use the HZH-sequence? If it is similar to the NOT-gate, why don‚Äôt we use that instead?\n",
        "\n",
        "<font color=\"red\">Simply put, the HZH-sequence is more flexible. It is the simplest form of Grover‚Äôs search algorithm.</font> It starts with all states being equal (the first H-gate). It applies an oracle (Z-gate). \n",
        "\n",
        "And, <font color=\"blue\">**it uses a diffuser that amplifies the amplitude of the selected state |1‚ü© (the second H-gate)**.\n",
        "\n",
        "> While we could rewrite these two circuits more succinctly, the circuit identities of HZH=X and HIH=I let us use the general structure of Grover‚Äôs algorithm. Simply by changing the oracle, we can mark and amplify different states. We don‚Äôt need to come up with a new algorithm for each possible state we want to select out of a list. But we only need to find an appropriate oracle. This ability comes in handy the more states our quantum system has.\n",
        "\n",
        "> The search for one of two possible states does not even deserve to be called a search. But the general structure of Grover‚Äôs algorithm is not different from this very simple example. **It uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bMTt-MVijqL"
      },
      "source": [
        "**Quantum Counting**\n",
        "\n",
        "> **In quantum counting, we simply use the quantum phase estimation algorithm to find an eigenvalue of a Grover search iteration.**\n",
        "\n",
        "* how many solutions exist?\n",
        "\n",
        "* does there any solution exist?\n",
        "\n",
        "\n",
        "The percentage number of solutions in our search space affects the difference between $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$. For example, if there are not many solutions, $|s\\rangle$ will be very close to $\\left|s^{\\prime}\\right\\rangle$ and $\\theta$ will be very small. It turns out that the eigenvalues of the Grover iterator are $e^{\\pm i \\theta}$, and **we can extract this using quantum phase estimation (QPE) to estimate the number of solutions $(M)$**.\n",
        "\n",
        "**First we want to get $\\theta$ from measured_int. (phase estimation)** \n",
        "\n",
        "* You will remember that $\\mathrm{QPE}$ gives us a measured value $=2^{n} \\phi$ from the eigenvalue $e^{2 \\pi i \\phi}$, \n",
        "\n",
        "* so to get $\\theta$ we need to do:\n",
        "\n",
        "> $\n",
        "\\theta=\\text { value } \\times \\frac{2 \\pi}{2^{t}}\n",
        "$\n",
        "\n",
        "**Second, we calculate the inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle:$**\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\cos \\frac{\\theta}{2}\n",
        "$\n",
        "\n",
        "* And that $|s\\rangle$ (a uniform superposition of computational basis states) can be written in terms of $|\\omega\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ as:\n",
        "\n",
        "> $\n",
        "|s\\rangle=\\sqrt{\\frac{M}{N}}|\\omega\\rangle+\\sqrt{\\frac{N-M}{N}}\\left|s^{\\prime}\\right\\rangle\n",
        "$\n",
        "\n",
        "* The inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ is:\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\sqrt{\\frac{N-M}{N}}=\\cos \\frac{\\theta}{2}$\n",
        "\n",
        "* From this, we can use some trigonometry and algebra to show:\n",
        "\n",
        "> $\n",
        "N \\sin ^{2} \\frac{\\theta}{2}=M\n",
        "$\n",
        "\n",
        "**Third, calculate number of solutions**\n",
        "\n",
        "* From the Grover's algorithm chapter, you will remember that a common way to create a diffusion operator, $U_{s}$, is actually to implement $-U_{s}$. \n",
        "\n",
        "* This implementation is used in the Grover iteration provided in this chapter. In a normal Grover search, this phase is global and can be ignored, but now we are controlling our Grover iterations, this phase does have an effect. \n",
        "\n",
        "* The result is that we have effectively searched for the states that are not solutions, and our quantum counting algorithm will tell us how m√¢ny states are not solutions. To fix this, we simply calculate \n",
        "\n",
        "> $N-M$\n",
        "\n",
        "The ability to perform quantum counting efficiently is needed in order to use Grover's search algorithm (because running Grover's search algorithm requires knowing how many solutions exist). Moreover, this algorithm solves the quantum existence problem (namely, deciding whether any solution exists) as a special case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kEC0TTtwdt"
      },
      "source": [
        "###### *Harrow-Hassidim-Lloyd Algorithm (HHL)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/using-quantum-algorithms-to-compute-discrete-logs-bad2c94f6df6\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/quantum-neural-networks-7b5bc469d984\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/hhl-solving-linear-systems-of-equations-with-quantum-computing-efb07eb32f74"
      ],
      "metadata": {
        "id": "If0SFdAZRoiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Solving a system of linear equations with a quantum computer (HHL)**\n",
        "\n",
        "* **Given a matrix $A \\in \\mathbb{C}^{N \\times N}$ and a vector $\\vec{b} \\in \\mathbb{C}^{N}$, find $\\vec{x} \\in \\mathbb{C}^{N}$ satisfying $A \\vec{x}=\\vec{b}$**\n",
        "\n",
        "* The spectrum of $A$ is given by: $A\\left|v_{j}\\right\\rangle=\\lambda_{j}\\left|v_{j}\\right\\rangle, 1 \\geq\\left|\\lambda_{j}\\right| \\geq 1 / \\kappa$\n",
        "\n",
        "\n",
        "**Objective: We want to solve a system of linear equations by finding $\\vec{x}$**\n",
        "\n",
        "* Familiar methods of solutions: Substitution method, Graphical method, Matrix method, Cramer's rule, Gaussian elimination\n",
        "\n",
        "* The classical algorithm returns the full solution, while the HHL can only approximate functions of the solution vector.\n",
        "\n",
        "\n",
        "> $A \\vec{x} = \\vec{b}$\n",
        "\n",
        "Classically you would take the inverse of $A$ (via spectral decomposition / eigendecomposition):\n",
        "\n",
        "> $\\vec{x} = A^{-1} \\vec{b}$\n",
        "\n",
        "The first step towards solving a system of linear equations with a quantum computer is to encode the problem in the quantum language. \n",
        "\n",
        "* By rescaling the system, we can assume $\\vec{b}$ and $\\vec{x}$ to be normalised and map them to the respective quantum states $|b\\rangle$ and $|x\\rangle$. \n",
        "\n",
        "* Usually the mapping used is such that $i^{\\text {th }}$ component of $\\vec{b}$ (resp. $\\vec{x}$ ) corresponds to the amplitude of the $i^{\\text {th }}$ basis state of the quantum state $|b\\rangle$ (resp. $|x\\rangle$ ). \n",
        "\n",
        "From now on, we will focus on the rescaled problem\n",
        "\n",
        "><font color=\"blue\">$A|x\\rangle=|b\\rangle\n",
        "$</font> $\\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        "><font color=\"blue\">$|x\\rangle=A^{-1}|b\\rangle$</font> $\\quad$ (the solution is: $|x\\rangle = \\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle$)\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        " \n",
        ">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        ">$\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$\n",
        "\n",
        "and the right hand side of the system can be written in the eigenbasis of $A$ as\n",
        "\n",
        ">$\n",
        "|b\\rangle=\\sum_{j=0}^{N-1} b_{j}\\left|u_{j}\\right\\rangle, \\quad b_{j} \\in \\mathbb{C}\n",
        "$\n",
        "\n",
        "It is useful to keep in mind that the goal of the HHL is to exit the algorithm with the readout register in the state\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle\n",
        "$\n",
        "\n",
        "Note that here we already have an implicit normalisation constant since we are talking about a quantum state."
      ],
      "metadata": {
        "id": "M6rk5CuVYtO6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij_dB1BOtFEY"
      },
      "source": [
        "**HHL-Algorithm**\n",
        "\n",
        "*Main Subroutines in HHL: Hamiltonian simulation, Phase estimation (newer: linear combination of unitaries) and (Variable-time) amplitude amplification*\n",
        "\n",
        "1. Prepare the initial state $|b\\rangle$. Note that $|b\\rangle=\\sum_{j} c_{j}\\left|v_{j}\\right\\rangle$.\n",
        "\n",
        "2. Use the so-called phase estimation algorithm to perform the map\n",
        "$|b\\rangle \\rightarrow \\sum_{j} c_{j}\\left|v_{j}\\right\\rangle\\left|\\tilde{\\lambda}_{j}\\right\\rangle$\n",
        "\n",
        "* $|\\tilde{\\lambda}_{j}\\rangle$ -> This register contains the eigenvalue estimates.\n",
        "\n",
        "3. Apply a one-qubit conditional rotation to perform the map\n",
        "$|0\\rangle \\rightarrow \\frac{1}{\\kappa \\tilde{\\lambda}_{j}}|0\\rangle+\\sqrt{1-\\frac{1}{\\kappa^{2} \\tilde{\\lambda}_{j}^{2}}}|1\\rangle$\n",
        "\n",
        "4. Undo step 2 - apply the inverse of phase estimation\n",
        "$\\sum_{j} \\frac{c_{j}}{\\kappa \\tilde{\\lambda}_{j}}\\left|v_{j}\\right\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle \\approx \\frac{1}{\\kappa A}|b\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle$\n",
        "\n",
        "5. Use amplitude amplification to get rid of the ‚Äûbad‚Äú part of the state with |1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YObfeUgOQfyO"
      },
      "source": [
        "**Applications of HHL**\n",
        "\n",
        "* Systems of linear equations arise naturally in many real-life applications in a wide range of areas, such as in the solution of Partial Differential Equations, the calibration of financial models, fluid simulation or numerical field calculation. \n",
        "\n",
        "* Used in many quantum machine learning algorithms as a building block\n",
        "\n",
        "\n",
        "* The quantum algorithm for linear systems of equations has been applied to a support vector machine, which is an optimized linear or non-linear binary classifier (https://arxiv.org/abs/1307.0471v2)\n",
        "\n",
        "* for Least-squares fitting (https://arxiv.org/abs/1204.5242)\n",
        "\n",
        "* for finite-element-methods (https://arxiv.org/abs/1512.05903) (but only for higher problems which include solutions with higher-order derivatives and large spatial dimensions. For example, problems in many-body dynamics require the solution of equations containing derivatives on orders scaling with the number of bodies, and some problems in computational finance, such as Black-Scholes models, require large spatial dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfhXMt-vq9J"
      },
      "source": [
        "**Promise**: \n",
        "\n",
        "* Solving 10,000 linear equation: a classical computer needs in best case 10,000 steps. HHL just 13. The [quantum algorithm for linear systems of equations](https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd: Provided the linear system is sparse and has a low condition number $\\kappa_{1}$ and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of $O\\left(\\log (N) \\kappa^{2}\\right)$, where $N$ is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in $O(N \\kappa)$ (or $O(N \\sqrt{\\kappa})$ for positive semidefinite matrices).\n",
        "\n",
        "* Unlike the classical solutions to the Deutsch-Jozsa and search problems, most of our classical methods for matrix manipulation do work in polynomial time. However, as data analysis becomes more and more powerful (and more and more demanding on today‚Äôs computers), the size of these matrices can make even polynomial time too long.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* solution vector is not yielded (rather it prepares a quantum state that is proportional to the solution): Actually reading out the solution vector would take O(N)time, so we can only maintain the logarithmic runtime by sampling the solution vector like ‚ü®x|M|x‚ü©, where M is a quantum-mechanical operator. Therefore, **HHL is useful mainly in applications where only samples from the solution vector are needed**. \n",
        "\n",
        "* Entries of matrix have to be sparse: Additionally, although HHL is exponentially faster than Conjugate Gradient in N, it is polynomially slower in s and ùúÖ, so HHL is restricted to only those matrices that are sparse and have low condition numbers.\n",
        "\n",
        "* Must satisfy robust invertibility (means that entries of matrix must all approx. of same size)\n",
        "\n",
        "* Preparation of input vector is complicated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-28aw9k0W5g"
      },
      "source": [
        "**Sources:**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution\n",
        "\n",
        "* [Peter Witteg: Quantum Machine Learning - 37 - Overview of the HHL Algorithm](https://m.youtube.com/watch?v=hQpdPM-6wtU)\n",
        "\n",
        "* https://qiskit.org/textbook/ch-applications/hhl_tutorial.html\n",
        "\n",
        "* https://github.com/quantumlib/Cirq/blob/master/examples/hhl.py\n",
        "\n",
        "* [Google Quantum: Quantum Algorithms for Systems of Linear Equations (Quantum Summer Symposium 2020)](https://m.youtube.com/watch?v=Xvp56xeNZo4)\n",
        "\n",
        "* https://www.quantamagazine.org/new-quantum-algorithms-finally-crack-nonlinear-equations-20210105/\n",
        "\n",
        "* [A New Approach to Multiplication Opens the Door to Better Quantum Computers](https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/)\n",
        "\n",
        "* https://www.quantamagazine.org/new-algorithm-breaks-speed-limit-for-solving-linear-equations-20210308/\n",
        "\n",
        "* https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/\n",
        "\n",
        "* https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/\n",
        "\n",
        "* https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical result\n",
        "from numpy import linalg as LA\n",
        "w, v = LA.eigh(A)\n",
        "w"
      ],
      "metadata": {
        "id": "rd6dnJ3G6OTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=wrong-or-nonexistent-copyright-notice\n",
        "\"\"\"Demonstrates the algorithm for solving linear systems by Harrow, Hassidim, Lloyd (HHL).\n",
        "\n",
        "The HHL algorithm solves a system of linear equations, specifically equations of the form Ax = b,\n",
        "where A is a Hermitian matrix, b is a known vector, and x is the unknown vector. To solve on a\n",
        "quantum system, b must be rescaled to have magnitude 1, and the equation becomes:\n",
        "\n",
        "|x> = A**-1 |b> / || A**-1 |b> ||\n",
        "\n",
        "The algorithm uses 3 sets of qubits: a single ancilla qubit, a register (to store eigenvalues of\n",
        "A), and memory qubits (to store |b> and |x>). The following are performed in order:\n",
        "1) Quantum phase estimation to extract eigenvalues of A\n",
        "2) Controlled rotations of ancilla qubit\n",
        "3) Uncomputation with inverse quantum phase estimation\n",
        "\n",
        "For details about the algorithm, please refer to papers in the REFERENCE section below. The\n",
        "following description uses variables defined in the HHL paper.\n",
        "\n",
        "This example is an implementation of the HHL algorithm for arbitrary 2x2 Hermitian matrices. The\n",
        "output of the algorithm are the expectation values of Pauli observables of |x>. Note that the\n",
        "accuracy of the result depends on the following factors:\n",
        "* Register size\n",
        "* Choice of parameters C and t\n",
        "\n",
        "The result is perfect if\n",
        "* Each eigenvalue of the matrix is in the form\n",
        "\n",
        "  2œÄ/t * k/N,\n",
        "\n",
        "  where 0‚â§k<N, and N=2^n, where n is the register size. In other words, k is a value that can be\n",
        "  represented exactly by the register.\n",
        "* C ‚â§ 2œÄ/t * 1/N, the smallest eigenvalue that can be stored in the circuit.\n",
        "\n",
        "The result is good if the register size is large enough such that for every pair of eigenvalues,\n",
        "the ratio can be approximated by a pair of possible register values. Let s be the scaling factor\n",
        "from possible register values to eigenvalues. One way to set t is\n",
        "\n",
        "t = 2œÄ/(sN)\n",
        "\n",
        "For arbitrary matrices, because properties of their eigenvalues are typically unknown, parameters C\n",
        "and t are fine-tuned based on their condition number.\n",
        "\n",
        "\n",
        "=== REFERENCE ===\n",
        "Harrow, Aram W. et al. Quantum algorithm for solving linear systems of\n",
        "equations (the HHL paper)\n",
        "https://arxiv.org/abs/0811.3171\n",
        "\n",
        "Coles, Eidenbenz et al. Quantum Algorithm Implementations for Beginners\n",
        "https://arxiv.org/abs/1804.03719\n",
        "\n",
        "=== CIRCUIT ===\n",
        "Example of circuit with 2 register qubits.\n",
        "\n",
        "(0, 0): ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄRy(Œ∏‚ÇÑ)‚îÄRy(Œ∏‚ÇÅ)‚îÄRy(Œ∏‚ÇÇ)‚îÄRy(Œ∏‚ÇÉ)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄM‚îÄ‚îÄ\n",
        "                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îê\n",
        "(1, 0): ‚îÄH‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ      ‚îÇ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÇ   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "           ‚îÇ         ‚îÇQFT^-1‚îÇ    ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ ‚îÇQFT‚îÇ         ‚îÇ\n",
        "(2, 0): ‚îÄH‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÇ      ‚îÇ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ@‚îÄ‚îÇ   ‚îÇ‚îÄ@‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄH‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "           ‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îî‚îÄ‚îÄ‚îÄ‚îò ‚îÇ       ‚îÇ\n",
        "(3, 0): ‚îÄ‚îÄ‚îÄe^iAt‚îÄe^2iAt‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄe^-2iAt‚îÄe^-iAt‚îÄ\n",
        "\n",
        "Note: QFT in the above diagram omits swaps, which are included implicitly by\n",
        "reversing qubit order for phase kickbacks.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import sympy\n",
        "import cirq\n",
        "\n",
        "\n",
        "class PhaseEstimation(cirq.Gate):\n",
        "    \"\"\"A gate for Quantum Phase Estimation.\n",
        "\n",
        "    The last qubit stores the eigenvector; all other qubits store the estimated phase,\n",
        "    in big-endian.\n",
        "\n",
        "    Args:\n",
        "        num_qubits: The number of qubits of the unitary.\n",
        "        unitary: The unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        yield cirq.H.on_each(*qubits[:-1])\n",
        "        yield PhaseKickback(self.num_qubits(), self.U)(*qubits)\n",
        "        yield cirq.qft(*qubits[:-1], without_reverse=True) ** -1\n",
        "\n",
        "\n",
        "class HamiltonianSimulation(cirq.EigenGate):\n",
        "    \"\"\"A gate that represents e^iAt.\n",
        "\n",
        "    This EigenGate + np.linalg.eigh() implementation is used here purely for demonstrative\n",
        "    purposes. If a large matrix is used, the circuit should implement actual Hamiltonian\n",
        "    simulation, by using the linear operators framework in Cirq, for example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, A, t, exponent=1.0):\n",
        "        cirq.EigenGate.__init__(self, exponent=exponent)\n",
        "        self.A = A\n",
        "        self.t = t\n",
        "        ws, vs = np.linalg.eigh(A)\n",
        "        self.eigen_components = []\n",
        "        for w, v in zip(ws, vs.T):\n",
        "            theta = w * t / math.pi\n",
        "            P = np.outer(v, np.conj(v))\n",
        "            self.eigen_components.append((theta, P))\n",
        "\n",
        "    def _num_qubits_(self) -> int:\n",
        "        return 1\n",
        "\n",
        "    def _with_exponent(self, exponent):\n",
        "        return HamiltonianSimulation(self.A, self.t, exponent)\n",
        "\n",
        "    def _eigen_components(self):\n",
        "        return self.eigen_components\n",
        "\n",
        "\n",
        "class PhaseKickback(cirq.Gate):\n",
        "    \"\"\"A gate for the phase kickback stage of Quantum Phase Estimation.\n",
        "\n",
        "    It consists of a series of controlled e^iAt gates with the memory qubit as the target and\n",
        "    each register qubit as the control, raised to the power of 2 based on the qubit index.\n",
        "    unitary is the unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        super(PhaseKickback, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        memory = qubits.pop()\n",
        "        for i, qubit in enumerate(qubits):\n",
        "            yield cirq.ControlledGate(self.U ** (2**i))(qubit, memory)\n",
        "\n",
        "\n",
        "class EigenRotation(cirq.Gate):\n",
        "    \"\"\"Perform a rotation on an ancilla equivalent to division by eigenvalues of a matrix.\n",
        "\n",
        "    EigenRotation performs the set of rotation on the ancilla qubit equivalent to division on the\n",
        "    memory register by each eigenvalue of the matrix. The last qubit is the ancilla qubit; all\n",
        "    remaining qubits are the register, assumed to be big-endian.\n",
        "\n",
        "    It consists of a controlled ancilla qubit rotation for each possible value that can be\n",
        "    represented by the register. Each rotation is a Ry gate where the angle is calculated from\n",
        "    the eigenvalue corresponding to the register value, up to a normalization factor C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, C, t):\n",
        "        super(EigenRotation, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.C = C\n",
        "        self.t = t\n",
        "        self.N = 2 ** (num_qubits - 1)\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        for k in range(self.N):\n",
        "            kGate = self._ancilla_rotation(k)\n",
        "\n",
        "            # xor's 1 bits correspond to X gate positions.\n",
        "            xor = k ^ (k - 1)\n",
        "\n",
        "            for q in qubits[-2::-1]:\n",
        "                # Place X gates\n",
        "                if xor % 2 == 1:\n",
        "                    yield cirq.X(q)\n",
        "                xor >>= 1\n",
        "\n",
        "                # Build controlled ancilla rotation\n",
        "                kGate = cirq.ControlledGate(kGate)\n",
        "\n",
        "            yield kGate(*qubits)\n",
        "\n",
        "    def _ancilla_rotation(self, k):\n",
        "        if k == 0:\n",
        "            k = self.N\n",
        "        theta = 2 * math.asin(self.C * self.N * self.t / (2 * math.pi * k))\n",
        "        return cirq.ry(theta)\n",
        "\n",
        "\n",
        "def hhl_circuit(A, C, t, register_size, *input_prep_gates):\n",
        "    \"\"\"Constructs the HHL circuit.\n",
        "\n",
        "    Args:\n",
        "        A: The input Hermitian matrix.\n",
        "        C: Algorithm parameter, see above.\n",
        "        t: Algorithm parameter, see above.\n",
        "        register_size: The size of the eigenvalue register.\n",
        "        *input_prep_gates: A list of gates to be applied to |0> to generate the desired input\n",
        "            state |b>.\n",
        "\n",
        "    Returns:\n",
        "        The HHL circuit. The ancilla measurement has key 'a' and the memory measurement is in key\n",
        "        'm'.  There are two parameters in the circuit, `exponent` and `phase_exponent` corresponding\n",
        "        to a possible rotation  applied before the measurement on the memory with a\n",
        "        `cirq.PhasedXPowGate`.\n",
        "    \"\"\"\n",
        "\n",
        "    ancilla = cirq.LineQubit(0)\n",
        "    # to store eigenvalues of the matrix\n",
        "    register = [cirq.LineQubit(i + 1) for i in range(register_size)]\n",
        "    # to store input and output vectors\n",
        "    memory = cirq.LineQubit(register_size + 1)\n",
        "\n",
        "    c = cirq.Circuit()\n",
        "    hs = HamiltonianSimulation(A, t)\n",
        "    pe = PhaseEstimation(register_size + 1, hs)\n",
        "    c.append([gate(memory) for gate in input_prep_gates])\n",
        "    c.append(\n",
        "        [\n",
        "            pe(*(register + [memory])),\n",
        "            EigenRotation(register_size + 1, C, t)(*(register + [ancilla])),\n",
        "            pe(*(register + [memory])) ** -1,\n",
        "            cirq.measure(ancilla, key='a'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    c.append(\n",
        "        [\n",
        "            cirq.PhasedXPowGate(\n",
        "                exponent=sympy.Symbol('exponent'), phase_exponent=sympy.Symbol('phase_exponent')\n",
        "            )(memory),\n",
        "            cirq.measure(memory, key='m'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "def simulate(circuit):\n",
        "    simulator = cirq.Simulator()\n",
        "\n",
        "    # Cases for measuring X, Y, and Z (respectively) on the memory qubit.\n",
        "    params = [\n",
        "        {'exponent': 0.5, 'phase_exponent': -0.5},\n",
        "        {'exponent': 0.5, 'phase_exponent': 0},\n",
        "        {'exponent': 0, 'phase_exponent': 0},\n",
        "    ]\n",
        "\n",
        "    results = simulator.run_sweep(circuit, params, repetitions=5000)\n",
        "\n",
        "    for label, result in zip(('X', 'Y', 'Z'), list(results)):\n",
        "        # Only select cases where the ancilla is 1.\n",
        "        # TODO: optimize using amplitude amplification algorithm.\n",
        "        # Github issue: https://github.com/quantumlib/Cirq/issues/2216\n",
        "        expectation = 1 - 2 * np.mean(result.measurements['m'][result.measurements['a'] == 1])\n",
        "        print(f'{label} = {expectation}')\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main program loop.\n",
        "\n",
        "    Simulates HHL with matrix input, and outputs Pauli observables of the resulting qubit state |x>.\n",
        "    Expected observables are calculated from the expected solution |x>.\n",
        "    \"\"\"\n",
        "\n",
        "    # Eigendecomposition:\n",
        "    #   (4.537, [-0.971555, -0.0578339+0.229643j])\n",
        "    #   (0.349, [-0.236813, 0.237270-0.942137j])\n",
        "    # |b> = (0.64510-0.47848j, 0.35490-0.47848j)\n",
        "    # |x> = (-0.0662724-0.214548j, 0.784392-0.578192j)\n",
        "    A = np.array(\n",
        "        [\n",
        "            [4.30213466 - 6.01593490e-08j, 0.23531802 + 9.34386156e-01j],\n",
        "            [0.23531882 - 9.34388383e-01j, 0.58386534 + 6.01593489e-08j],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    t = 0.358166 * math.pi\n",
        "    register_size = 4\n",
        "    input_prep_gates = [cirq.rx(1.276359), cirq.rz(1.276359)]\n",
        "    expected = (0.144130, 0.413217, -0.899154)\n",
        "\n",
        "    # Set C to be the smallest eigenvalue that can be represented by the\n",
        "    # circuit.\n",
        "    C = 2 * math.pi / (2**register_size * t)\n",
        "\n",
        "    # Simulate circuit.\n",
        "    print(\"Expected observable outputs:\")\n",
        "    print(\"X =\", expected[0])\n",
        "    print(\"Y =\", expected[1])\n",
        "    print(\"Z =\", expected[2])\n",
        "    print(\"Actual: \")\n",
        "    simulate(hhl_circuit(A, C, t, register_size, *input_prep_gates))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "FWww_qXA6LeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Calculate complexity*"
      ],
      "metadata": {
        "id": "CtFnUPlheUKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import math\n",
        "import sympy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pu9OrL0VvvyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/space-and-time-complexity-in-computer-algorithms-a7fffe9e4683\n",
        "\n",
        "https://towardsdatascience.com/python-performance-and-gpus-1be860ffd58d\n",
        "\n",
        "https://urban-institute.medium.com/using-multiprocessing-to-make-python-code-faster-23ea5ef996ba\n",
        "\n",
        "> The VM used for Colaboratory appears to have 13GB RAM and 2 vCPU when checking using psutil (so a n1-highmem-2 instance)\n",
        "\n",
        "- 2-core Xeon 2.2GHz\n",
        "- 13GB RAM\n",
        "- 33GB HDD\n",
        "\n",
        "maximum lifetime of a VM is 12 hours. Idle VMs time out after 90m"
      ],
      "metadata": {
        "id": "DLubvtAOefxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display complexity of Fibonacci\n",
        "\n",
        "import time\n",
        "\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "\n",
        "# My input data for the function\n",
        "my_tuple = 5,10,15,20,25\n",
        "\n",
        "# Define empty bucket\n",
        "rows_list = []\n",
        "\n",
        "for n in my_tuple:\n",
        "\n",
        "    # Begin tracking CPU time\n",
        "    st = time.process_time(),\n",
        "\n",
        "    # Run algorithms\n",
        "    result = fibonacci(n),\n",
        "    #result = math.factorial(n)\n",
        "    #result = n**2\n",
        "\n",
        "    # Finish tracking CPU time\n",
        "    et = time.process_time(),\n",
        "\n",
        "    # Calculate performance (end - begin)\n",
        "    performance = [x - y for x, y in zip(list(et), list(st))]\n",
        "\n",
        "    # Collect results in a list\n",
        "    rows_list.append(performance)\n",
        "\n",
        "# Write results in a table\n",
        "df = pd.DataFrame(rows_list, index=my_tuple, columns=['CPU Performance in sec']) \n",
        "print(df)\n",
        "\n",
        "# Visualize results in a graph\n",
        "df = df.reset_index().rename(columns={\"index\": \"Datasize\"})\n",
        "df.plot(x ='Datasize', y='CPU Performance in sec', kind = 'bar')\t"
      ],
      "metadata": {
        "id": "AutzcBQJezFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.geeksforgeeks.org/running-python-script-on-gpu/\n",
        "\n",
        "* does it consider several CPUs? \n",
        "* how to check distribution of a process?\n",
        "* is it important to consider GPUs? \n",
        "\n",
        "**time.process_time()** function always returns the float value of time in seconds. \n",
        "* Return the value (in fractional seconds) of the sum of the system and user CPU time of the current process.\n",
        "* It does not include time elapsed during sleep. \n",
        "*The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid.\n",
        "\n",
        "Note: **process_time()** is very different from **pref_counter()**, as perf_counter() calculates the program time with sleep time and if any interrupt is there but process_counter only calculates the system and the CPU time during the process it not includes the sleep time.\n",
        "\n",
        "Advantages of **process_time()** :\n",
        "1. process_time() provides the system and user CPU time of the current process.\n",
        "2. We can calculate float and integer both values of time in seconds and nanoseconds.\n",
        "3. Used whenever there is a need to calculate the time taken by the CPU for the particular process."
      ],
      "metadata": {
        "id": "Nqs6OPRBe3KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**%time**\n",
        "\n",
        "* Time execution of a Python statement or expression.\n",
        "\n",
        "* The CPU and wall clock times are printed, and the value of the expression (if any) is returned. Note that under Win32, system time is always reported as 0, since it can not be measured.\n",
        "\n",
        "* This function can be used both as a line and cell magic:\n",
        "\n",
        "  * In line mode you can time a single-line statement (though multiple ones can be chained with using semicolons).\n",
        "\n",
        "  * In cell mode, you can time the cell body (a directly following statement raises an error).\n",
        "\n",
        "* This function provides very basic timing functionality. Use the timeit magic for more control over the measurement.\n",
        "\n",
        "https://ipython.readthedocs.io/en/stable/interactive/magics.html"
      ],
      "metadata": {
        "id": "-q2srP_AfBIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1217.png)\n"
      ],
      "metadata": {
        "id": "Lujstc4WfFpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways:\n",
        "\n",
        "* **time.process_time()** function with start and stop\n",
        "\n",
        "* **%time** magic function\n",
        "\n",
        "Sources: \n",
        "\n",
        "* https://www.geeksforgeeks.org/time-process_time-function-in-python/\n",
        "\n",
        "* https://www.askpython.com/python/examples/python-wait-for-a-specific-time\n",
        "\n",
        "* https://pynative.com/python-get-execution-time-of-program/"
      ],
      "metadata": {
        "id": "ZsxXpJeOfHtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gertingold.github.io/tools4scicomp/profiling.html\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Landauer-Prinzip\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Bremermann-Grenze\n",
        "\n",
        "https://www.hri.res.in/~qipa13/QIPA/docu/7.12.2013/morning%20session/2-pjoag-Quantum%20Algorithm%20for%20LP--2007.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2010.07852.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2203.15421.pdf\n",
        "\n",
        "https://en.wikipedia.org/wiki/Time_complexity\n",
        "\n",
        "https://en.wikipedia.org/wiki/Karp%27s_21_NP-complete_problems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Integer_programming\n",
        "\n",
        "https://en.wikipedia.org/wiki/COIN-OR\n",
        "\n",
        "Total CPU time vs Wallclock time:\n",
        "\n",
        "https://pythonspeed.com/articles/blocking-cpu-or-io/"
      ],
      "metadata": {
        "id": "3LpSP0JxfPi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Gates*"
      ],
      "metadata": {
        "id": "zpgWYl3LkxVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Qubits & Quantum Gates*"
      ],
      "metadata": {
        "id": "vSshMr2W7qVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gate Depth**\n",
        "\n",
        "Increasing qubit numbers on Quantum Hardware are good. However, an important question is how much gate depth will be allowed within the single qubit decoherence time window? I would like to know the state-of-the-art on this subject, in terms of noise models, analytical bounds or experimental findings. It will help assess the practical limitations on the Quantum circuits we design or Quantum Algorithms we formulate. From my end I found these two papers, it will be great if fellow LinkedIn friend can advise more apers on the state-of-the-art,\n",
        "\n",
        "1.https://https://Inkd.in/dTM3NJsU\n",
        "\n",
        "2.https://https://Inkd.in/dMCTcCzR"
      ],
      "metadata": {
        "id": "lNiIqfr1plUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/qiskit/the-atoms-of-computation-ae2b27799eaa"
      ],
      "metadata": {
        "id": "TUZ_lRaeftG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qutech.nl/wp-content/uploads/2018/02/t7-Luke-Schaeffer-tuesday_schaeffer.pdf"
      ],
      "metadata": {
        "id": "udmrmumE8x4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qubits and Quantum Gates**\n",
        "\n",
        "* a [quantum logic gate](https://en.m.wikipedia.org/wiki/Quantum_logic_gate) (or simply quantum gate) is a basic quantum circuit operating on a small number of qubits. They are the building blocks of quantum circuits, like classical logic gates are for conventional digital circuits.\n",
        "\n",
        "* Types of Qubits:\n",
        "\n",
        "  * **Unary**: one qubit (X, Y, Z, Hadamard and rotation gates)\n",
        "\n",
        "  * **Binary**: two qubits (CNOT can entangle two qubits, CZ gate, Swap gate)\n",
        "\n",
        "  * **Terniary**: three qubits (Fredkin = CSWAP gate, and Toffoli = CCNOT)\n",
        "\n",
        "* **Charge qubit & Transmon**\n",
        "\n",
        "  * In quantum computing, a [charge qubit](https://en.m.wikipedia.org/wiki/Charge_qubit) (also known as Cooper-pair box) is a qubit whose basis states are charge states (i.e. states which represent the presence or absence of excess Cooper pairs in the island)\n",
        "\n",
        "  * [transmon](https://en.m.wikipedia.org/wiki/Transmon) is a type of superconducting charge qubit that was designed to have reduced sensitivity to charge noise\n",
        "\n",
        "  * Google video superconductors: https://youtu.be/uPw9nkJAwDY\n",
        "\n",
        "  * Google Video: transmon vs fluxion: https://youtu.be/qsizrKrUZDg\n",
        "\n",
        "* **Fun Facts & important to know**\n",
        "\n",
        "  * all gates are reversable, because irreversable gates will destroy the entanglement and superposition\n",
        "\n",
        "  * a classical computer can theoretically simulate a quantum computer in any size, assuming they have enough bits, we can't solve something like the Halting problem with a quantum computer. It can solve a certain set of problems faster, but not everything faster.\n",
        "\n",
        "  * If we use four qubits, for example, then there are 2^4=16 different states. Starting from |0000‚ü©, to |0001‚ü©, ending at |1111‚ü©. Each qubit can have a specific meaning. We could interpret it as a letter. A letter that does not have 26 different options but only two, |0‚ü© and |1‚ü©. **With a sufficient number of qubits, we could represent all living humans. With 33 qubits, we can represent around 8.5 billion different states**. A phonebook of mankind. And we haven‚Äôt sorted it. https://javafxpert.github.io/grok-bloch/\n",
        "\n",
        "* **Essential Gates**\n",
        "\n",
        "  * the **minimal set** of gates is {T, H, CNOT} = Toffoli Gate\n",
        "\n",
        "  * Hadamard + Toffoli gates constitute a **universal quantum gate**.\n",
        "\n",
        "  * Toffoli cannot be constructed directly, builds up on others.\n",
        "\n",
        "  * T-gate is the **most expensive gate**"
      ],
      "metadata": {
        "id": "W12vgL1G6tG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Circuits (and Quantum Turing machines)**\n",
        "\n",
        "* A [quantum Turing machine (QTM)](https://en.m.wikipedia.org/wiki/Quantum_Turing_machine) or universal quantum computer is an abstract machine used to model the effects of a quantum computer.\n",
        "\n",
        "* It provides a simple model that captures all of the power of quantum computation‚Äîthat is, any quantum algorithm can be expressed formally as a particular quantum Turing machine.\n",
        "\n",
        "* However, the computationally equivalent [quantum circuit](https://en.m.wikipedia.org/wiki/Quantum_circuit) is a more common model.\n"
      ],
      "metadata": {
        "id": "TonYa1ZQxmVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Toffoli-Gate*"
      ],
      "metadata": {
        "id": "LAH8nB-d7vrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toffoli Gate**\n",
        "\n",
        "* Google search: toffoli gates in quantum algorithms\n",
        "\n",
        "* **The Toffoli can be constructed from single qubit gates and a few CNOTs; the minimal cir- cuit to implement the Toffoli involves six CNOTs and a number of Hadamard and T and T‚Ä† gates**\n",
        "\n",
        "* **The Toffoli gate is universal when combined with the single qubit Hadamard gate.**\n",
        "\n",
        "* Hence in principle the Toffoli gate itself is not needed, i.e. it is not a ‚Äúfundamental‚Äù gate; however, it is very useful, and ‚Äúexpensive‚Äù to implement with more fun- damental gates, so a hardware system that supports the Toffoli can afford some computational efficiency.\n",
        "\n",
        "* Source: An introduction to the surface code, Andrew N. Cleland, University of Chicago, Chicago IL 60637, USA\n",
        "\n",
        "* What is the use of Toffoli gate? **The Toffoli gate is a key component for many important quantum algorithms**, notably\n",
        "\n",
        "  * the Shor algorithm,39 quantum error correction,20 fault-tolerant computation40 and quantum arithmetic operations,18 and, together with the Hadamard gate, is universal for quantum computation.\n",
        "\n",
        "* https://www.nature.com/articles/npjqi201619\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/11915/are-toffoli-gates-actually-used-in-designing-quantum-circuits That said, recently there has been work assuming a Clifford+T gate set where the T gate is the most expensive, based on the surface code"
      ],
      "metadata": {
        "id": "oSFoh65_qNui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3 Qubits - Toffoli gate (CCNOT): Negate a target bit if both control qubits are 1*\n",
        "\n",
        "TOFFOLI gate can be used to simulate standard boolean operations. It negates a target bit if both control qubits are 1.\n",
        "\n",
        "> **The Toffoli gate is essentially the atom of mathematics. It is the simplest element, from which every other problem-solving technique can be compiled**.\n",
        "\n",
        "* The [Toffoli gate](https://en.m.wikipedia.org/wiki/Toffoli_gate), named after Tommaso Toffoli; also called CCNOT gate or Deutsch gate $D(\\pi / 2)$; is a 3-bit gate, which is universal for classical computation but not for quantum computation.\n",
        "\n",
        "* The quantum Toffoli gate is the same gate, defined for 3 qubits. If we limit ourselves to only accepting input qubits that are $|0\\rangle$ and $|1\\rangle$, then if the first two bits are in the state $|1\\rangle$ it applies a Pauli- $X$ (or NOT) on the third bit, else it does nothing.\n",
        "\n",
        "* It is an example of a controlled gate. Since it is the quantum analog of a classical gate, it is completely specified by its truth table.\n",
        "\n",
        "* **The Toffoli gate is universal when combined with the single qubit Hadamard gate. $^{[13]}$**"
      ],
      "metadata": {
        "id": "xiRdxCDp8Xfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: CNOT-Gate (Non-Clifford)*"
      ],
      "metadata": {
        "id": "13sqDDC5l_f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Controlled_NOT_gate"
      ],
      "metadata": {
        "id": "dwy8ZMnemMIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: T-Gate (Non-Clifford)*"
      ],
      "metadata": {
        "id": "jb3vfKj27tso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T-Gate**\n",
        "\n",
        "* T-gate is the most expensive gate\n",
        "\n",
        "* is not a Clifford gate"
      ],
      "metadata": {
        "id": "pShsyvvyo5a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*T Gate $\\frac{\\pi}{4}$ and R8 Gate $\\frac{\\pi}{8}$: Change Phase (Parametrizing it as continuous)*\n",
        "\n",
        "*Let's take as an example the T-gate, and use Quantum Phase Estimation to estimate its phase.*\n",
        "\n",
        "You will remember that the $T$-gate adds a phase of $e^{\\frac{i \\pi}{4}}$ to the state $|1\\rangle$ :\n",
        "\n",
        "$\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$\n",
        "\n",
        "Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$\n",
        "\n",
        "<font color=\"red\">We expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "Calculate the algreba of T-gate applied:\n",
        "\n",
        "$\\mathbf{T}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$\n",
        "\n",
        "First we have a qubit in state 0 and apply Hadamard:\n",
        "\n",
        "$\\left[\\begin{array}{l}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ = $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right]$ + $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right]$\n",
        "\n",
        "$\n",
        "H=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}\\langle 0|+\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}\\langle 1|\n",
        "$\n",
        "\n",
        "in Dirac notation. This corresponds to the transformation matrix\n",
        "\n",
        "> $\n",
        "H_{T}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "1 & -1\n",
        "\\end{array}\\right)\n",
        "$\n"
      ],
      "metadata": {
        "id": "h5msiZLrucIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Hadamard Gate (Clifford)*"
      ],
      "metadata": {
        "id": "mUfkh74aiXp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hadamard Gate (Superposition)*\n",
        "\n",
        "> $H=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**The Hadamard states ‚à£+‚ü© and ‚à£‚àí‚ü© are considered superposition states**\n",
        "\n",
        "because they are a combination of the two computational states:\n",
        "\n",
        "> $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "**Apply Hadamard gate on a qubit that is in the |0> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "**Now apply Hadamard gate on a qubit that is in the |1> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{-1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "Hence, in both cases (qubit |0> or qubit |1>) applying a Hadamard Gate gives an equal chance for the qubit to be 0 or 1' when measured.\n",
        "\n",
        "Source: https://freecontent.manning.com/all-about-hadamard-gates/\n"
      ],
      "metadata": {
        "id": "_T8yeOt7mKxA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjeGeBvlLhN"
      },
      "source": [
        "**Herleitung Hadamard (Wichtig!)**\n",
        "\n",
        "For an equal (or uniform) superposition of the two computational states, we can set the two coefficients equal to each other:\n",
        "\n",
        "$a_{1}=a_{2}=a$\n",
        "\n",
        "The normalization condition for a well-behaved quantum state requires that the sum of the squared magnitudes of the coefficients be equal to one; this is sufficient to find\n",
        "a\n",
        "a for a uniform superposition:\n",
        "\n",
        "$|a|^{2}+|a|^{2}=1$\n",
        "\n",
        "$2|a|^{2}=1$\n",
        "\n",
        "$|a|^{2}=\\frac{1}{2}$\n",
        "\n",
        "$a=\\frac{1}{\\sqrt{2}}$\n",
        "\n",
        "In vector form, this state can represented as\n",
        "\n",
        "> $\\left[\\begin{array}{l}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ = $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right]$ + $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right]$\n",
        "\n",
        "This is what we can use now to understand Hadamard, where you want \"halfway\" a 50/50 chance of basis states 0 and 1 (Bloch sphere representation of superposition state):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_025.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlCP-muCpOWK"
      },
      "source": [
        "*Hadamard gate operations:*\n",
        "\n",
        "> $\\begin{aligned} H(|0\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle=:|+\\rangle \\\\ H(|1\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle=:|-\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)+\\frac{1}{2}(|0\\rangle-|1\\rangle)=|0\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)-\\frac{1}{2}(|0\\rangle-|1\\rangle)=|1\\rangle \\end{aligned}$\n",
        "\n",
        "One application of the Hadamard gate to either a 0 or 1 qubit will produce a quantum state that, if observed, **will be a 0 or 1 with equal probability** (as seen in the first two operations). This is exactly like flipping a fair coin in the standard probabilistic model of computation. However, if the Hadamard gate is applied twice in succession (as is effectively being done in the last two operations), then the final state is always the same as the initial state (because quantum operations are reversable, unlike operations on classical computers)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Clifford Gates*"
      ],
      "metadata": {
        "id": "R-pP8oZC7zxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)"
      ],
      "metadata": {
        "id": "liK-Ms2f3Xig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of Clifford gates: https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates"
      ],
      "metadata": {
        "id": "5TQhdrfQ9zL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [rotation operators](https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates) Rx(Œ∏), Ry(Œ∏), Rz(Œ∏), the [phase shift gate](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#Phase_shift_gates) P(œÜ)[c] and [CNOT](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#CNOT) form a widely used universal set of quantum gates.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_logic_gate"
      ],
      "metadata": {
        "id": "KsmHdaUjBc-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clifford Gates are not universal\n",
        "\n",
        "The Clifford group is generated by three gates, Hadamard, S and CNOT gates (arr all clifford gates themselves). Source: https://en.m.wikipedia.org/wiki/Clifford_gates\n",
        "\n",
        "\n",
        "A common universal gate set is the Clifford + T gate set, which is composed of the CNOT, H, S and T gates. The Clifford set alone is not universal and can be efficiently simulated classically by the Gottesman‚ÄìKnill theorem. https://en.m.wikipedia.org/wiki/Quantum_logic_gate"
      ],
      "metadata": {
        "id": "uaon_5lU-HUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group)."
      ],
      "metadata": {
        "id": "-bS6VsOA7ZUm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGvwll0SoLoA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_122.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Quantum_Logic_Gates.png/500px-Quantum_Logic_Gates.png)"
      ],
      "metadata": {
        "id": "eMSx6ntm6p8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-X Gate (Flip Computational States)**\n",
        "\n",
        "> $X=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\sigma_{x}=\\mathrm{NOT}$\n",
        "\n",
        "* The Pauli- $X$ gate is the quantum equivalent of the [NOT gate](https://en.m.wikipedia.org/wiki/Inverter_(logic_gate)) for classical computers (its main function is to invert the input signal applied) with respect to the standard basis $|0\\rangle,|1\\rangle$, which distinguishes the $z$ axis on the Bloch sphere. It is sometimes called a bit-flip as it maps $|0\\rangle$ to $|1\\rangle$ and $|1\\rangle$ to $|0\\rangle .$\n",
        "\n",
        "* The Pauli gates $(X, Y, Z)$ are the three Pauli matrices $\\left(\\sigma_{x}, \\sigma_{y}, \\sigma_{z}\\right)$ and act on a single qubit. The Pauli $X_{1} Y$ and $Z$ equate, respectively, to a rotation around the $x, y$ and $z$ axes of the Bloch sphere by $\\pi$ radians.\n",
        "\n",
        "* Nice visualisations: https://medium.com/analytics-vidhya/quantum-gates-7fe83817b684\n",
        "\n"
      ],
      "metadata": {
        "id": "zDXxSuLakwyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root of Pauli-X Gate**\n",
        "\n",
        "The square root of NOT gate (or square root of Pauli- $X, \\sqrt{X}$ ) acts on a single qubit. It maps the basis state $|0\\rangle$ to $\\frac{(1+i)|0\\rangle+(1-i)|1\\rangle}{2}$ and $|1\\rangle$ to $\\frac{(1-i)|0\\rangle+(1+i)|1\\rangle}{2} .$\n",
        "\n",
        "In matrix form it is given by\n",
        "\n",
        ">$\n",
        "\\sqrt{X}=\\sqrt{\\mathrm{NOT}}=\\frac{1}{2}\\left[\\begin{array}{cc}\n",
        "1+i & 1-i \\\\\n",
        "1-i & 1+i\n",
        "\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}\n",
        "e^{i \\pi / 4} & e^{-i \\pi / 4} \\\\\n",
        "e^{-i \\pi / 4} & e^{i \\pi / 4}\n",
        "\\end{array}\\right]$\n",
        "\n",
        "such that\n",
        "\n",
        ">$\n",
        "(\\sqrt{X})^{2}=(\\sqrt{\\mathrm{NOT}})^{2}=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right]=X\n",
        "$\n",
        "\n",
        "> *This operation represents a rotation of $\\pi / 2$ about $x$ -axis at the Bloch sphere*."
      ],
      "metadata": {
        "id": "PeMSkgSzlzsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Y Gate (Phase Flip between i and -i)**\n",
        "\n",
        "> $Y=\\sigma_{y}=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right]$\n",
        "\n",
        "* Similarly, the Pauli- $Y$ maps $|0\\rangle$ to $i|1\\rangle$ and $|1\\rangle$ to $-i|0\\rangle$ (NOT gate with i-multiple):\n"
      ],
      "metadata": {
        "id": "htDSfNdzJulw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Z Gate ($\\pi$ Flip Phase between +1 and -1)**\n",
        "\n",
        "> $Z=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]=\\sigma_{z}\\quad Z=|0\\rangle\\langle 0|-| 1\\rangle\\langle 1|$\n",
        "\n",
        "* Pauli Z gate is a phase flip gate that causes rotation around the z-axis by œÄ radians. Z flippt zwischen den Polen der X Achse (+ und -).\n",
        "\n",
        "* *Z Gate flippt zwischen den Hadamard gegen√ºberliegenden Richtungen auf der X-Achse. We change the phase (or sign) on a Qubit*:\n",
        "\n",
        "> $\\mathbf{Z}|0\\rangle=|0\\rangle$\n",
        "\n",
        "> $\\mathbf{Z}|1\\rangle=-|1\\rangle$\n",
        "\n",
        "* Since |0‚ü© and |1‚ü© lie on the z-axis, the Z-gate will not affect these states. To put it in other terms *|0‚ü© and |1‚ü© are the two eigenstates of the Z-gate*. On the other hand, it flips |+‚ü© to |-‚ü© and |-‚ü© to |+‚ü©.\n",
        "\n",
        "* Pauli $Z$ leaves the basis state $|0\\rangle$ unchanged and maps $|1\\rangle$ to $-|1\\rangle$. Due to this nature, it is sometimes called *phase-flip* (flips sign of second entangled state)\n",
        "\n",
        "* *The Z gate is like the X gate but in the hadamard basis, flipping states*"
      ],
      "metadata": {
        "id": "qj2bMNDwLoDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-S Gate: $\\frac{\\pi}{2}$ Flip Phase between Z and Y (with $\\mu$=i and $\\nu$=-i)**\n",
        "\n",
        "> ${S}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & \\sqrt{-1}\\end{array}\\right]=\\sqrt{\\mathbf{Z}}$\n",
        "\n",
        "* The phase gate $\\mathbf{Z}$ transforms $|+\\rangle$ to $|-\\rangle$, and we can find the gate that does \"half of\" this transformation by finding the square root of the matrix $\\mathbf{Z}$\n",
        "\n",
        "* fintuning Z gate in der Hadamard basis: S-gate is die H√§lfte vom Z-Gate, und R-Gate ein viertel vom Z-Gate\n",
        "\n",
        "* Use the matrix $\\mathbf{S}$ to find the state $|\\mu\\rangle$ which is \"halfway\" between $|+\\rangle$ and $|-\\rangle$ :\n",
        "\n",
        "> $\\mathbf{S}|+\\rangle=|\\mu\\rangle =\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "If you begin with $|+\\rangle$ and apply the $\\mathbf{S}$ gate three times in a row, you find a new state, $|\\nu\\rangle$ which appears to mirror the complex state $|\\mu\\rangle$ :\n",
        "\n",
        "> $|\\mu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "> $|\\nu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-i|1\\rangle) .$"
      ],
      "metadata": {
        "id": "Frcfbnlgr6d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already uncovered the gates we need to perform any rotations around the $x$ - and $z$-axes, which can bring us from an initialized qubit to any other quantum state.\n",
        "\n",
        "Unfortunately, this result comes with a pretty serious caveat: all of the gates we have used so far are discrete. They perform rotations around the Bloch sphere, but since they rotate in discrete hops (of $\\pi / 2$ or $\\pi$ ), they are unable to reach most of the intermediate states on the surface of the sphere.\n",
        "\n",
        "One solution to this problem is to define smaller and smaller rotations around these axes. The gate $\\mathbf{S}$ is equal to $\\sqrt{\\mathbf{Z}}$ and halves its rotation angle from $\\pi$ to $\\pi / 2$. Even greater division of these gates is possible, and some of them have\n",
        "common names:\n",
        "\n",
        "$\\mathbf{Z}=\\mathbf{Z}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]$ Rotation: $\\pi$\n",
        "\n",
        "$\\mathbf{S}=\\sqrt[2]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]$ Rotation: $\\pi / 2$\n",
        "\n",
        "\n",
        "$\\mathbf{T}=\\sqrt[4]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$ Rotation: $\\pi / 4$ not clifford\n",
        "\n",
        "$\\mathbf{R 8}=\\sqrt[8]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 8}\\end{array}\\right] \\quad$ Rotation: $\\pi / 8 .$ not clifford"
      ],
      "metadata": {
        "id": "6jspQ2kndBZg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjqz8Hkol68T"
      },
      "source": [
        "**Swap Gate**\n",
        "\n",
        "*2 Qubits - Swap Gate (& Swap Square root): Swap two Qubits (like in Quantum Fourier Transform)*\n",
        "\n",
        "The swap gate **swaps two qubits**.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\text { SWAP }=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzlOABhWmMms"
      },
      "source": [
        "**Square root of swap gate**\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate performs half-way of a two-qubit swap.\n",
        "\n",
        "It is universal such that any many-qubit gate can be constructed from only $\\sqrt{\\text { SWAP }}$ and single qubit gates.\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate is not, however maximally entangling; more than one application of it is required to produce a Bell state from product states.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\sqrt{\\mathrm{SWAP}}=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & \\frac{1}{2}(1+i) & \\frac{1}{2}(1-i) & 0 \\\\\n",
        "0 & \\frac{1}{2}(1-i) & \\frac{1}{2}(1+i) & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "This gate arises naturally in systems that exploit exchange interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Non-Clifford Gates*"
      ],
      "metadata": {
        "id": "Rjqwr4miC9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relative Phase Gates**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates\n",
        "\n",
        "includes t gate"
      ],
      "metadata": {
        "id": "ksB-lSPRDB_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rotation Operator Gates**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates\n",
        "\n",
        "non clifford"
      ],
      "metadata": {
        "id": "Q971EvdWC1KC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DomGPGBSmzGZ"
      },
      "source": [
        "**Non-Clifford swap gates**\n",
        "\n",
        "*3 Qubits - Fredkin Gate (CSWAP or CS): Swap identity gate for qubit 2+3, if qubit 1 is in state 1*\n",
        "\n",
        "The Fredkin gate is a universal reversible 3-bit gate that swaps the last two bits if the first bit is 1; a controlled-swap operation.\n",
        "\n",
        "The [Fredkin gate](https://en.m.wikipedia.org/wiki/Fredkin_gate) (also CSWAP or CS gate), named after Edward Fredkin, is a 3-bit gate that performs a controlled swap. It is universal for classical computation.\n",
        "\n",
        "It has the useful property that the numbers of 0s and 1s are conserved throughout, which in the billiard ball model means the same number of balls are output as input.\n",
        "\n",
        "The basic Fredkin gate[1] is a controlled swap gate that maps three inputs (C, I1, I2) onto three outputs (C, O1, O2). The C input is mapped directly to the C output.\n",
        "\n",
        "If C = 0, no swap is performed; I1 maps to O1, and I2 maps to O2.\n",
        "\n",
        "Otherwise, the two outputs are swapped so that I1 maps to O2, and I2 maps to O1.\n",
        "\n",
        "https://www.science.org/doi/10.1126/sciadv.1501531\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Block Encoding*"
      ],
      "metadata": {
        "id": "bOsPQjh-8FP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Encoding**\n",
        "\n",
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "\n",
        "block encodings (unitary, signal proicesing, transformation)\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "h7nV48-uvauA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Error Correction*"
      ],
      "metadata": {
        "id": "OWqmJDTmXzvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) (very good!)\n",
        "\n",
        "Video: [Gottesman 1](https://youtu.be/ltJ1jXQeDl8) and [Gottesman 2](https://youtu.be/cUqys29d0YA)"
      ],
      "metadata": {
        "id": "udDc1Y1NqwMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Circuit Depth*"
      ],
      "metadata": {
        "id": "3Ifz4t5vA6Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/new-algorithm-closes-quantum-supremacy-window-20230109/\n",
        "\n",
        "If you imagine continually increasing the number of qubits as complexity theorists do, and you also want to account for errors, you need to decide whether you‚Äôre also going to keep adding more layers of gates ‚Äî increasing the circuit depth, as researchers say. Suppose you keep the circuit depth constant at, say, a relatively shallow three layers, as you increase the number of qubits. You won‚Äôt get much entanglement, and the output will still be amenable to classical simulation. On the other hand, if you increase the circuit depth to keep up with the growing number of qubits, the cumulative effects of gate errors will wash out the entanglement, and the output will again become easy to simulate classically."
      ],
      "metadata": {
        "id": "kgZP7ruQA8s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Introduction*"
      ],
      "metadata": {
        "id": "rMqIXS4-cxRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) very good\n",
        "Video: [The superconducting transmon qubit](https://youtu.be/dKTNBN99xLw)\n",
        "Video: [The transmon qubit](https://youtu.be/cb_f9KpYipk)\n",
        "Video: [Making quantum error correction practical](https://youtu.be/YPFpll1NFQc)\n",
        "Video: [Steven Girvin](https://youtu.be/nhUKHf-GN_Y)\n",
        "Video: [Quantum Industry Talks](https://youtu.be/eyICn3KCUPI)\n",
        "Video: [Qiskit QEC](https://youtu.be/ZY8PddknCos), [Qiskit](https://youtu.be/SHr3uSv9Bts), [qiskit](https://youtu.be/96a0G4G5ZH8)"
      ],
      "metadata": {
        "id": "68dkfSxfVj8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Concepts:*** *Ancilla qubits, surface code, distance code, X (bit flip) and Z (phase flip) error, threshold theory, required fault-tolerance, logical vs physical qubit*\n",
        "\n",
        "*Tasks: Error detection, error mitigation, error correction, error suppression*\n",
        "\n",
        "Video [Progress Towards Quantum Error Correction with the Surface Code | Qiskit Seminar Series](https://www.youtube.com/watch?v=si5a9RJP01A)"
      ],
      "metadata": {
        "id": "uRkoUVMsuEg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use more than one qubit to represent a state, use neighboring qubit check (so you don't measure the exact state which would collapse the quantum state). You can decode with it and see that the error was on the last qubit. Then you can correct the physical qubit to get back the correct logical qubit state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1278.png)"
      ],
      "metadata": {
        "id": "l3Ja_nTmmvIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[An Introduction to Quantum Error Correction and Fault-Tolerant Quantum Computation](https://arxiv.org/pdf/0904.2557.pdf)"
      ],
      "metadata": {
        "id": "2OdDTy_mW_Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For what do you need abelian groups in quantum computing?\n",
        "\n",
        "In quantum computing, Abelian groups are used to describe the symmetry of a quantum system. Symmetry is a fundamental concept in quantum mechanics, and it refers to the idea that a physical system will remain unchanged under certain transformations. For example, the symmetry of a quantum system may be described by a group of rotations, translations, or reflections. Abelian groups are used to describe the symmetry of a quantum system because they have the useful property of being commutative, meaning that the order in which the transformations are applied does not affect the outcome. This property makes it possible to use Abelian groups to describe the symmetries of a quantum system in a way that is mathematically tractable and easy to work with."
      ],
      "metadata": {
        "id": "4KQRd-aYkmU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error detection and correction**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Error_detection_and_correction\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Repetition_code"
      ],
      "metadata": {
        "id": "mGJ-sWNFvF47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faut-Tolerant QC (Quantum Error Correction)**\n",
        "\n",
        "* Bit flip (from 0 to 1) or dephasing (from superposition to exact state)\n",
        "\n",
        "* Challenge: we need to keep that states correct without looking at them (because then WE dephase them)\n",
        "\n",
        "* We need a method to **build relatively noiseless qubits (logical qubits)out of many noisy ones (physical qubits)**. This is quantum error correction.\n",
        "\n",
        "* Solution: one way (repetition encoding), sit our qubits on a line. We then go along and ask every pair of next-door-neighbours whether they agree or disagree with each other. This tells us nothing about whether they are 0 or 1. But repetition encoding, which protects against bit flip errors so well, actually makes dephasing more likely!\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_error_correction\n",
        "\n",
        "\n",
        "* Examples of QEC: **repetition code** (simplest QEC) and **surface code** (and color codes?)\n",
        "\n",
        "* techniques: syndrome measurements, decoding, logical operations\n",
        "\n",
        "* https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/\n",
        "\n",
        "\n",
        "* [An introduction to Fault-tolerant Quantum Computing](https://arxiv.org/abs/1508.03695)\n",
        "\n",
        "http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html\n",
        "\n",
        "* [INTRODUCTION TO\n",
        "QUANTUM ERROR\n",
        "CORRECTION](https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2327/files/2019/11/QECIntro.pdf)"
      ],
      "metadata": {
        "id": "B-hRX5qBqHdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Error Mitigation**\n",
        "\n",
        "* quantum error correction is long term goal, meanwhile we try to mitigate it\n",
        "\n",
        "* Error mitigation techniques: statistical corrections (on histogram for example)\n",
        "\n",
        "\t* https://qiskit.org/textbook/ch-quantum-hardware/measurement-error-mitigation.html\n",
        "\n",
        "\t* https://arxiv.org/abs/2005.10189"
      ],
      "metadata": {
        "id": "jX7JYnTLqNo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fidelity (Error Probability)*"
      ],
      "metadata": {
        "id": "f0O6pdoUYhec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity**: For Shor's algorithm with estimated 10^9 physical gates required, the error should be less than 10^-9, ideally 10^-10. We are still several orders of magnitude away from that accuracy / faut-tolerance.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1279.png)"
      ],
      "metadata": {
        "id": "EM0u6HMroENg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many applications call for error rates in the 10‚àí15 regime [2‚Äì9], but state-of-the-art quantum platforms typically have physical error rates near 10‚àí3\n",
        "https://arxiv.org/pdf/2102.06132.pdf"
      ],
      "metadata": {
        "id": "FcbSD88fkhOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run quantum algorithms perfectly we need error probability of 1 in a billion or 1 in a trillion - but we are at 1 in a thousand\n",
        "\n",
        "Video: [Suppressing quantum errors by scaling a surface code logical qubit](https://www.youtube.com/watch?v=dVkLNwSTBU0)"
      ],
      "metadata": {
        "id": "pQbRXxY779rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Fidelity of quantum states**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Fidelity_of_quantum_states\n",
        "\n",
        "* Noise transforms pure states into mixed states.\n",
        "\n",
        "  * There are also simpler ones: Fidelity between two pure states\n",
        "\n",
        "  * And there are also more complex ones: Fidelity between two mixed states\n",
        "\n",
        "* fidelity is generally defined as the quantity:\n",
        "\n",
        "> ${\\displaystyle F(\\rho ,\\sigma )=\\left(\\operatorname {tr} {\\sqrt {{\\sqrt {\\rho }}\\sigma {\\sqrt {\\rho }}}}\\right)^{2}}$\n",
        "\n",
        "* most useless state is fidelity 0,5. because fidelity = 0 means orthogonal, and =1 means exactly the same.\n",
        "\n",
        "* Video: [Fidelity](https://www.youtube.com/watch?v=GWi_HIVz2B4)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1275.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1276.png)"
      ],
      "metadata": {
        "id": "GV4K-Cl0qIIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Physical and Logical Qubits*"
      ],
      "metadata": {
        "id": "MkJxJltjAV7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Physical_and_logical_qubits"
      ],
      "metadata": {
        "id": "ioWe4Z-6sBxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Threshold Theory & Code Distance*"
      ],
      "metadata": {
        "id": "dpys_ybfYkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum **Threshold theory** ensure that there is a limit that helps to get error under control even with larger numbers of physical qubits. The **code distance** is then a result of the max error rate and represents the number of physical qubits for one state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1280.png)"
      ],
      "metadata": {
        "id": "RuL0PiU_okd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Quantum Threshold Theorem**\n",
        "\n",
        "* Challenge: quantum computer will not be able to perform gate operations perfectly, some small constant error is inevitable\n",
        "\n",
        "* [quantum threshold theorem](https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem) (or quantum fault-tolerance theorem) states that a quantum computer\n",
        "  * **with a physical error rate below a certain threshold** can,\n",
        "  * **through application of quantum error correction schemes**,\n",
        "  * suppress the logical error rate to arbitrarily low levels.\n",
        "\n",
        "* This shows that quantum computers can be made fault-tolerant, as an analogue to von Neumann's threshold theorem for classical computation\n",
        "\n",
        "* The formal statement of the threshold theorem depends on the types of error correction codes and error model being considered.\n",
        "\n",
        "* for any particular error model (such as having each gate fail with independent probability p), use **error correcting codes** to build better gates out of existing gates.\n",
        "\n",
        "  * Though these \"better gates\" are larger, and so are more prone to errors within them, their error-correction properties mean that they have a lower chance of failing than the original gate (provided p is a small-enough constant).\n",
        "\n",
        "  * Then, one can use these better gates to recursively create even better gates, until one has gates with the desired failure probability, which can be used for the desired quantum circuit.\n",
        "\n",
        "* Current estimates put the threshold for the [surface code](https://en.m.wikipedia.org/wiki/Toric_code) (here: Toric code) on the order of 1%, though estimates range widely and are difficult to calculate due to the exponential difficulty of simulating large quantum systems.\n",
        "\n",
        "* At a 0.1% probability of a [depolarizing](https://en.m.wikipedia.org/wiki/Depolarization) error, the surface code would require approximately 1,000-10,000 physical qubits per logical data qubit, though more pathological error types could change this figure drastically.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1xEbTDdNwar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Distance Code / Code Distance**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory\n",
        "\n",
        "\n",
        "* Over all 25 cycles of error correction, the distance-5 code realises lower logi- cal error probabilities pL than the average of the subset distance-3 codes - [Paper](https://arxiv.org/pdf/2207.06431.pdf)\n",
        "\n",
        "* the distance is the shortest path in a certain \"space of errors\" which maps between two orthogonal quantum states that are in the code.\n",
        "\n",
        "* The natural space of errors is that of single qubit errors of the form ùúéùëã, ùúéùëå or ùúéùëß, in the case where the Hilbert space is that of ùëõ qubits.\n",
        "\n",
        "* So you can think of distance as the shortest path to get from one state to another by operations on single qubits, applied one at a time sequentially.\n",
        "\n",
        "* [Source](https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory)"
      ],
      "metadata": {
        "id": "ieotWtPN7sz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Types of Errors (Bit flip, phase flip, total loss)*"
      ],
      "metadata": {
        "id": "GYcJxiyZYuYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of errors that you want to detect: **Bit-flip** (represented with Pauli X gate) and **Phase-flip** (represented with Pauli-Z gate). But 1D string physical qubits cannot protect from bit and phase flip at the same time. You need a 2D string.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1281.png)"
      ],
      "metadata": {
        "id": "In3mQfhOp0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene: Qubits k√∂nnen auch ganz verloren gehen**\n",
        "\n",
        "* Inzwischen k√∂nnen Quantencomputer mit einer gewissen Anzahl von Rechenfehlern, wie zum Beispiel Bitflip- oder Phasenflip-Fehlern, umgehen. Zus√§tzlich zu diesen Fehlern k√∂nnen jedoch auch Qubits ganz aus dem Quantenregister verloren gehen.\n",
        "\n",
        "* Je nach Art des Quantencomputers kann dies auf den tats√§chlichen Verlust von Teilchen wie Atomen oder Ionen zur√ºckzuf√ºhren sein, oder darauf, dass Quantenteilchen beispielsweise in unerw√ºnschte Energiezust√§nde √ºbergehen, welche nicht mehr als Qubit erkannt werden. Wenn ein Qubit verloren geht, wird die Information in den verbleibenden Qubits unlesbar und ungesch√ºtzt. F√ºr das Ergebnis der Berechnung kann dieser Prozess zu einem potentiell verheerenden Fehler werden.\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neue_Methode_sch√ºtzt_Quantencomputer_vor_Ausf√§llen.html\n",
        "\n",
        "Resolving catastrophic error bursts from cosmic rays in large arrays of superconducting qubits.\n",
        "\n",
        "https://arxiv.org/abs/2104.05219\n",
        "\n",
        "https://physicsworld.com/a/cosmic-ray-threat-to-quantum-computing-greater-than-previously-thought/"
      ],
      "metadata": {
        "id": "0N8AlBcAz4Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Error Syndrome Measurement*"
      ],
      "metadata": {
        "id": "hZTQLaKGel6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syndrome: what qubit the error is on\n",
        "\n",
        "Error syndrome measurement:\n",
        "\n",
        "https://www.quora.com/Error-Correcting-Codes-What-is-a-syndrome:\n",
        "\n",
        "* The syndrome measurement provides information about the error that has happened, but not about the information that is stored in the logical qubit‚Äîas otherwise the measurement would destroy any quantum superposition of this logical qubit with other qubits in the quantum computer, which would prevent it from being used to convey quantum information. (https://en.m.wikipedia.org/wiki/Quantum_error_correction)\n",
        "\n",
        "* It is the result of multiplying a parity check matrix times a vector. By convention, codewords of a code have syndrome zero, so that by linearity of the code, the syndrome of a word is the syndrome of the \"error\" vector. Typically from the syndrome you would either try to determine whether there was an error (is the syndrome nonzero?) and recover the error from it, so that in turn you can recover the data from the received word.\n",
        "\n",
        "syndrome. = error?\n",
        "\n",
        "here slide 4: https://people.engr.tamu.edu/andreas-klappenecker/689/stabilizer.pdf"
      ],
      "metadata": {
        "id": "fYAPsGZXAPsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *2D Surface Code*"
      ],
      "metadata": {
        "id": "KSaDCK89Y3_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D Surface code** protects from X error and Z error (X and Z - that's why 2 D). Errors typically arise only locally. The gate structure needs to fit the physical geometry of the quantum processor. Error per gate should be 0,5%, but overall threshold depends on case.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1282.png)"
      ],
      "metadata": {
        "id": "jQZePon1slGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code $\\rightarrow$ Surface Code $\\rightarrow$ Toric Code**\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/2106/what-is-the-surface-code-in-the-context-of-quantum-error-correction\n",
        "\n",
        "* **Surface codes**: family of quantum error correcting codes defined on a 2D lattice of qubits.\n",
        "\n",
        "* Each code has [stabilizers](https://en.m.wikipedia.org/wiki/Stabilizer_code) that are defined equivalently in the bulk, but differ from one another in their boundary conditions.\n",
        "\n",
        "* The members of the surface code family are sometimes also described by more specific names:\n",
        "\n",
        "  * The [toric code](https://en.m.wikipedia.org/wiki/Toric_code) is a surface code with periodic boundary conditions,\n",
        "\n",
        "  * the planar code is one defined on a plane, etc.\n",
        "\n",
        "* How many qubits are in a surface code? - While the surface code requires four-qubit measurements to encode a single logical qubit, we introduce families of quantum error correcting codes that use only three-qubit measurements. [Paper](https://www.ucl.ac.uk/quantum/news/2021/aug/subsystem-codes-outperform-surface-code)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [Topological quantum memory (paper)](https://arxiv.org/abs/quant-ph/0110143)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation (paper)](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [My blog series introducing surface codes](http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html)\n",
        "\n",
        "* The surface codes can also be generalized to qudits. For more on that, [see here (Fault-tolerant quantum computation by anyons)](https://arxiv.org/abs/quant-ph/9707021)"
      ],
      "metadata": {
        "id": "Ur4bkT3KooVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Toric Code**\n",
        "\n",
        "* For the toric code we don‚Äôt put our qubits in a line, we put them in a grid pattern.\n",
        "\n",
        "* Video: [INTRODUCTION TO TOPOLOGICAL ORDER, DEMONSTRATION VIA THE TORIC CODE](https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s)\n",
        "\n",
        "* https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code-part-2.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/8-toric-code-part-3.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/9-toric-code-part-4.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/10-toric-code-part-5.html"
      ],
      "metadata": {
        "id": "zpXlgK2jyNPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Parity Check Measurement, Stabilizer Code, Repition Code & Ancilla Qubit*"
      ],
      "metadata": {
        "id": "v41iaGUgY8hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stabilizer code with parity check circuits (for Z and X errors)"
      ],
      "metadata": {
        "id": "4c1Ooe-Odl3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parity measurement**: is it even (correct) or odd (error)? You will have a square with one dimension for X error and another dimension for Z error measurement. You have an **ancilla qubit** to make the measurements with C-Z-gate and C-X gate (but you first put it into an equal superposition).\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1283.png)"
      ],
      "metadata": {
        "id": "iCtG8eoVtaGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You put both X and Z together and get a 2D lattice of surface code with a determined code distance. We have now data qubits, X ancilla qubits and Z ancilla qubits.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1284.png)"
      ],
      "metadata": {
        "id": "ujX9f7Wevt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code & Ancilla qubits**\n",
        "\n",
        "* A [stabilizer](https://en.m.wikipedia.org/wiki/Stabilizer_code) quantum error-correcting code appends [ancilla qubits](https://en.m.wikipedia.org/wiki/Ancilla_bit) to qubits that we want to protect."
      ],
      "metadata": {
        "id": "ldWaXAIIRrHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code**\n",
        "\n",
        "**A stabilizer quantum error-correcting code appends ancilla qubits to qubits that we want to protect**. A unitary encoding circuit rotates the global state into a subspace of a larger Hilbert space. This highly entangled, encoded state corrects for local noisy errors.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Stabilizer_code\n",
        "\n",
        "In quantum computing, **a stabilizer code is a type of error-correcting cod**e that is used to protect quantum information from the effects of noise and decoherence. These codes are based on the concept of stabilizer operators, which are a special type of operator that can be used to detect and correct errors in a quantum system. The basic idea behind stabilizer codes is to encode the quantum information using a set of stabilizer operators, such that any errors that occur in the system can be detected and corrected by measuring the values of these operators. GPT\n",
        "\n",
        "\n",
        "Many quantum error correction schemes can be classified as stabilizer codes, where a single bit of **quantum information is encoded in the joint state of many physical qubits**, which we refer to as data qubits. Interspersed among the data qubits are **measure qubits**, which periodically measure the parity of chosen combinations of data qubits. https://arxiv.org/pdf/2102.06132.pdf\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s\n",
        "\n",
        "https://leftasexercise.com/2019/01/28/basics-of-quantum-error-correction/\n",
        "\n",
        "https://leftasexercise.com/2019/02/04/q-fault-tolerant-quantum-computing/\n",
        "\n",
        "https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "https://leftasexercise.com/2019/04/08/quantum-error-correction-the-surface-code/\n",
        "\n",
        "https://leftasexercise.com/2019/02/11/quantum-error-correction-with-stabilizer-codes/\n",
        "\n",
        "https://leftasexercise.com/2018/09/10/quantum-computing-an-overview/"
      ],
      "metadata": {
        "id": "2ns0241plUDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Repetition Codes**\n",
        "\n",
        "* look at majority of bits\n",
        "\n",
        "* https://qiskit.org/textbook/ch-quantum-hardware/error-correction-repetition-code.html\n",
        "\n",
        "* repetition code: redundancy (repetition) is a way to make sure the message gets delivered (i.e. with majority voting, for d repetition: $P=\\sum_{n=0}^{[ a / 2]}\\left(\\begin{array}{l}d \\\\ n\\end{array}\\right) p^{n}(1-p)^{d-n} \\sim\\left(\\frac{p}{(1-p)}\\right)^{[ d / 2]}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_100.png)"
      ],
      "metadata": {
        "id": "zEwG8U1SwWhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Magic States*"
      ],
      "metadata": {
        "id": "kSmK9R6Hbm2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic States**\n",
        "\n",
        "* [Magic states](https://en.wikipedia.org/wiki/Magic_state_distillation#Magic_states) are certain states that have very nice properties with respect to fault-tolerant quantum computation.\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/13629/what-are-magic-states"
      ],
      "metadata": {
        "id": "2oUvm0fxqESw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Distance 2 Qubit Surface Code (Gate Circuit)***"
      ],
      "metadata": {
        "id": "cNo9Qsp7ZTQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go down from a seven qubit surface code to a simpler two qubit surface code. Smallest meaningful is a 2x2 lattice. But it's just an error detection code, because it's too small to do error correction.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1286.png)"
      ],
      "metadata": {
        "id": "d503fihiwLGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green line is a flux line**. We put a magnetic flux through the script loop of each qubit to put a a specific frequency where we want it to be. **Pink line is a charge line** that is used for single qubit gates. All the rest (red, blue, purple box) are part of the readout. It's very important to have a good readout - we need to measure the ancilla qubits during the operation to see if there wasn't an error. You see that sort of resonator over each Qubit (die dinger die aussehen wir alte Heizungskoerper) - this is very standard, there is a harmonic oscillator.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1287.png)"
      ],
      "metadata": {
        "id": "ugRZQcwN0eoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1288.png)"
      ],
      "metadata": {
        "id": "Mbmct0-22-2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1289.png)"
      ],
      "metadata": {
        "id": "sddTSG2S7J3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***7 Qubit Surface Code (Gate Circuit)***"
      ],
      "metadata": {
        "id": "QHclLheAYhtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a 7 qubit gate circuit. You can't correct the error, but you can detect it. At the end we verify by measuring the actual state of each qubit.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1290.png)\n"
      ],
      "metadata": {
        "id": "eqXaY73s5YSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run multiple measurements in (20) microseconds for Z and X operator:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1291.png)"
      ],
      "metadata": {
        "id": "ywM2-Zqb6dQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D = 3 Surface Code Stabilizer Gate Sequence**\n",
        "\n",
        "* Red: data qubits\n",
        "* Blue: X-type ancilla qubit\n",
        "* Green Z-type ancilla qubit\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1300.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1301.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1302.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1303.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1304.png)\n"
      ],
      "metadata": {
        "id": "5sejATAN9-FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1305.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1306.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1307.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1308.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1L2hW2GU_SsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *ZZ-Crosstalks*"
      ],
      "metadata": {
        "id": "Hz-PMZ5rZvs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZZ: Residual ZZ coupling, circular ZZ coupling, ZZ crosstalk**\n",
        "\n",
        "*  Noise is a significant obstacle to quantum computing, and ùëç ùëç cross- talk is one of the most destructive types of noise affecting supercon- ducting qubits. Previous approaches to suppressing ùëçùëç crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to sup- press ùëçùëç crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved. https://arxiv.org/pdf/2202.07628.pdf\n",
        "\n",
        "* In superconductors a destructive type of noise known as ùëçùëç crosstalk. This refers to an always-on ùúéùëß ‚äó ùúéùëß inter- action between qubits connected by couplings, which originates from the interaction between the computational and non-computational energy levels of qubits.\n",
        "\n",
        "* different types of crosstalks:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1295.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1296.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1297.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1298.png)\n",
        "\n",
        "\n",
        "*Source: https://www.youtube.com/watch?v=si5a9RJP01A&t=3645s*\n",
        "\n",
        "* second graph: top red is perfect readout, and black light is for max 10% readout\n",
        "* We need very good readout and very small ZZ error\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1294.png)"
      ],
      "metadata": {
        "id": "y9wvhg9DyWQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Future of Quantum Error Correction*"
      ],
      "metadata": {
        "id": "0PltPkkfZ1Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future of quantum error correction**\n",
        "* we still have problems like leakage etc\n",
        "  * When they ask about thresholds, they are also derived from ideal models. Things like circular ZZ coupling can make it much harder.\n",
        "  * If you add leakage to your CZ gates, you could take a surface code that would sort of there was zero leakage below the physical error rate. But if you add leakage to that 0.1 percent degrees (the red line), which is a second. When you do the decoding (the green line), suddenly you are not below the threshold anymore\n",
        "* Also, if we want distance n=17 (mentioned in the beginning) you have an insane amount of data coming out of the device:\n",
        "  * we need ($n^2 -1$) physical qubits for error correction. So if one readout is 1 bit, we need 288 bits per microsecond ($\\mu$s) per qubit).\n",
        "  * This amounts to 288 Gbits per s for 1000 logical qubits\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1292.png)"
      ],
      "metadata": {
        "id": "yps3FbFbuHOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color Code**\n",
        "\n",
        "* [Fault-tolerant quantum computing with color codes](https://arxiv.org/abs/1108.5738)\n",
        "\n",
        "* https://physics.stackexchange.com/questions/169176/quantum-error-correction-surface-code-vs-color-code\n",
        "\n",
        "The color code and surface code are very similar. They are stabilizer codes composed of qubits arranged in two dimensions, requiring only geometrically local stabilizer measurements.\n",
        "\n",
        "From the theory point of view, the codes are very similar. In fact, with collaborators we have proven that the color code is equivalent to a surface code (paper) up to a geometrically local unitary (one which only makes nearby qubits interact). One can think by analogy of the surface code* as a napkin with two rough and two smooth sides and the color code as folding this napkin along its diagonal. Because in the folded napkin, there are new things that are now close, it is possible to do more logical gates \"transversally\". This is good because it keeps errors from propagating and is relatively easy. However, the color code needs more qubits to interact in each stabilizer so ends up leading to a lower noise threshold. So one can say that although very similar, each code has its advantages and disadvantages.\n",
        "\n",
        "At this point, only very small versions of either of these codes are being demonstrated. The Rainer Blatt group demonstrated the smallest possible color-code which also uses 7 qubits (this instance is also referred to as the Steane code). However, the underlying geometry in which the qubits are laid out in the Blatt setup is a linear chain of ions, so I would say that this is not the natural setting to extend to larger and larger system sizes.\n",
        "\n",
        "**The superconducting qubit people (Martinis, IBM, DiCarlo, ...) on the other hand, are concentrating more on surface codes**. While in principle, their architecture should allow them to go full fledge 2D, for now, they are having the classical logic come in from the sides, which is something that needs to change.\n",
        "\n",
        "*There is actually an ambiguity as to what to call surface codes, but I will refer to the quantum double of Z2 with rough and smooth boundaries defined by Bravyi and Kitaev (paper)."
      ],
      "metadata": {
        "id": "9tWyVLKvxLE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Complexity Theory*"
      ],
      "metadata": {
        "id": "PkHbbQ0YHcmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/mip-re-6e903720c82f\n",
        "\n",
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/bqnp-the-quantum-analogue-of-np-486ed2469c1d"
      ],
      "metadata": {
        "id": "Zz1aptP1Rgnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Many Body problem as the basic challenge**\n",
        "\n",
        "\n",
        "* It is widely believed that it is exponentially difficult for classical computers to simulate quantum systems.\n",
        "\n",
        "* This problem is known as the [quantum many body problem](https://en.m.wikipedia.org/wiki/Many-body_problem).\n",
        "\n",
        "* However, quantum computers can simulate many (though not all) Hamiltonians in [polynomial time with bounded errors](https://en.m.wikipedia.org/wiki/BQP) (BQP), which is one of the main appeals of quantum computing.\n",
        "\n",
        "* This is applicable to chemical simulations, drug discovery, energy production, [climate modeling](https://en.m.wikipedia.org/wiki/Climate_model) and fertilizer production (e.g. [FeMoco](https://en.m.wikipedia.org/wiki/FeMoco)) as well. Because of this, quantum computers may be better than classical computers at aiding design of further quantum computers.\n",
        "\n",
        "*Source: https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem#Notes*\n",
        "\n",
        "\n",
        "* [Many-body problem](https://en.m.wikipedia.org/wiki/Many-body_problem) article with examples and aproaches from below\n",
        "\n",
        "* Examples:\n",
        "\n",
        "  * Condensed matter physics (solid-state physics, nanoscience, superconductivity)\n",
        "  * Bose‚ÄìEinstein condensation and Superfluids\n",
        "  * Quantum chemistry (computational chemistry, molecular physics)\n",
        "  * Atomic physics\n",
        "  * Molecular physics\n",
        "  * Nuclear physics (Nuclear structure, nuclear reactions, nuclear matter)\n",
        "  * Quantum chromodynamics (Lattice QCD, hadron spectroscopy, QCD matter, quark‚Äìgluon plasma)\n",
        "\n",
        "* Approaches:\n",
        "\n",
        "  * Mean-field theory and extensions (e.g. Hartree‚ÄìFock, Random phase approximation)\n",
        "  * Dynamical mean field theory\n",
        "  * Many-body perturbation theory and Green's function-based methods\n",
        "  * Configuration interaction\n",
        "  * Coupled cluster\n",
        "  * Various Monte-Carlo approaches\n",
        "  * Density functional theory\n",
        "  * Lattice gauge theory\n",
        "  * Matrix product state\n",
        "  * Neural network quantum states\n",
        "  * Numerical renormalization group\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rM91mt-54rmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems, that Quantum Computers can solve or not**\n",
        "\n",
        "* *See: [Quantum complexity theory](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory), [Quantum_supremacy](https://en.m.wikipedia.org/wiki/Quantum_supremacy), [Quantum algorithms](https://en.m.wikipedia.org/wiki/Quantum_algorithm), [Hamiltonian simulation](https://en.m.wikipedia.org/wiki/Hamiltonian_simulation), [Quantum information science](https://en.m.wikipedia.org/wiki/Quantum_information_science)*\n",
        "\n",
        "* Iterative improvements in classical algorithm have so far not paid off to transform a hard problem to an easy one. This is where quantum computing comes in: an ‚Äúeasy‚Äù problem that can be solved using a quantum computer in polynomial time is class BQP (Bounded-error Quantum Polynomial time), and a hard problem which can only be verified in polynomial time is class QMA (the playfully named Quantum Merlin Arthur, https://en.m.wikipedia.org/wiki/QMA). The hope in the field is that there is some overlap between the space of NP problems and BQP problems: that by leveraging quantum resources like superposition and entanglement, a hard problem can be transformed into an easy one.\n",
        "\n",
        "*Problems, that Quantum Computers can definitely solve efficiently*\n",
        "* [BQP](https://de.m.wikipedia.org/wiki/BQP) and [engl BQP](https://en.m.wikipedia.org/wiki/BQP), that are not in [BPP](https://de.m.wikipedia.org/wiki/BPP_(Komplexit√§tsklasse)) on a classical computer, like [Factorization](https://de.m.wikipedia.org/wiki/Faktorisierung) with [Shor's algorithm](https://de.m.wikipedia.org/wiki/Shor-Algorithmus)\n",
        "  * Currently, we know that $P \\subseteq B Q P$ (i.e. anything you can do with a classic computer you can do with a quantum computer). We don't know, strictly speaking, if that actually is a strict inequality! But many people will be very surprised if it turns out to not be one - not to mention the huge implications it would have. People think that $B Q P \\neq N P$ but that neither $B Q P \\subset N P$ nor $N P \\subset B Q P$ - but proving this is very hard and has not been done either way. [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "\n",
        "  * [Boson sampling](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Boson_sampling) under low enough error (quantum supremacy, but is it BQP class??)\n",
        "\n",
        "  * BQP = P - Dequantized algorithms for such problems ‚Äì high-rank matrix inversion, for example ‚Äì would imply that classical computers can efficiently simulate quantum computers, i.e., BQP = P, which is not currently considered to be likely. https://arxiv.org/abs/1905.10415\n",
        "\n",
        "  * https://www.quora.com/What-is-the-relationship-between-BQP-and-NP-1\n",
        "\n",
        "  * [Sampling the output distribution of random quantum circuits\n",
        "  ](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Sampling_the_output_distribution_of_random_quantum_circuits) - Google experiment (quantum supremacy, but is it BQP class??)\n",
        "\n",
        "* [QMA](https://en.m.wikipedia.org/wiki/QMA): many interesting classes are contained in QMA, such as P, BQP and NP, all problems in those classes are also in QMA. However, there are problems that are in QMA but not known to be in NP or BQP. A [list of known QMA-complete problems](https://arxiv.org/abs/1212.6312)\n",
        "  * Quantum circuit/channel property verification (V)\n",
        "  * Hamiltonian ground state estimation (H), icl. Quantum k-SAT (S)\n",
        "  * Density matrix consistency (C)\n",
        "\n",
        "* [QIP](https://en.m.wikipedia.org/wiki/QIP_(complexity))\n",
        "\n",
        "* **Contrary to decision problems that require yes or no answers, sampling problems ask for samples from probability distributions.** [Source](https://en.m.wikipedia.org/wiki/Quantum_supremacy), https://arxiv.org/abs/1702.03061\n",
        "\n",
        "* Further topics\n",
        "  * [How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html)\n",
        "  * a quantum computer can run quantum algorithms! It should be noted that we only know about a handful of quantum algorithms that perform exponentially faster than their best-known classical counterparts - and Grover is not one of them.\n",
        "  * if you solve any NP-complete problem, all other NP problems come as a 'freebie' (not just the NP-complete ones). In that sense, it would be a huge milestone. It is widely believed that quantum computers cannot solve NP-complete problems, but it has never been proven [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "\n",
        "*Problems, that Quantum Computers can maybe solve efficiently*\n",
        "* Electronic structure\n",
        "* P450, FeMoco (most definitely) and [quantum simulation in physics and chemistry](https://en.m.wikipedia.org/wiki/Quantum_simulator#Solving_physics_problems)\n",
        "\n",
        "*Problems, that Quantum Computers can not solve efficiently*\n",
        "* [np-complete](https://en.m.wikipedia.org/wiki/NP-completeness) (=the hardest of the problems to which solutions can be verified quickly), like [Halting problem](https://en.m.wikipedia.org/wiki/Halting_problem) or [3SAT](https://en.m.wikipedia.org/wiki/Boolean_satisfiability_problem) (except, one manages to create a reduction of Grovers algorithm on this NP-Complete algorithm)\n",
        "* [np-hard](https://en.m.wikipedia.org/wiki/NP-hardness), like [travelling salesman](https://en.m.wikipedia.org/wiki/Travelling_salesman_problem) (only annealing can approach better than classical computers) [Sign Problem](https://en.m.wikipedia.org/wiki/Numerical_sign_problem) is NP hard. https://arxiv.org/pdf/1805.08219.pdf\n",
        "* Big question in computer science: [P versus NP problem](https://en.m.wikipedia.org/wiki/P_versus_NP_problem), P vs NP videos: https://youtu.be/EHp4FPyajKQ and https://youtu.be/YX40hbAHx3s\n"
      ],
      "metadata": {
        "id": "p8UyNFNKszF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complexity Classes & Algorithmic Efficiency (Runtime)**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1274.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1273.png)\n",
        "\n",
        "\n",
        "* [Examples of runtime with big O](https://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly), and see examples of [Computational complexity of mathematical operations](https://en.m.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra), [Brilliant: Complexity Theory](https://brilliant.org/wiki/complexity-theory/), [Complexity class](https://en.m.wikipedia.org/wiki/Complexity_class), [Computational Complexity Theory](https://en.m.wikipedia.org/wiki/Computational_complexity_theory), [Quantum complexity theory](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory), [Quantum supremacy](https://en.m.wikipedia.org/wiki/Quantum_supremacy), [PH complexity](https://en.m.wikipedia.org/wiki/PH_(complexity)), [Finally, a Problem That Only Quantum Computers Will Ever Be Able to Solve](https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/),\n",
        "* [Theory of computation](https://en.m.wikipedia.org/wiki/Theory_of_computation), [Big O notation (Landau)](https://en.m.wikipedia.org/wiki/Big_O_notation): Big (O) worste case / Big Œ© (Omega) best case / Big Œ∏ (Theta) notation (Asymptotic Analysis of Algorithms) Omega=(O)\n",
        "* [Time complexity](https://en.m.wikipedia.org/wiki/Time_complexity), Space Complexity = Auxiliary Space + Space used for input values, [Space time tradeoff](https://en.m.wikipedia.org/wiki/Space‚Äìtime_tradeoff), [Garbage collection](https://de.m.wikipedia.org/wiki/Garbage_Collection), [Time and Space Complexity Analysis of Algorithm](https://afteracademy.com/blog/time-and-space-complexity-analysis-of-algorithm), [How to compute Time Complexity or Order of Growth of any program](https://www.rookieslab.com/posts/how-to-compute-time-complexity-order-of-growth-of-any-program)\n"
      ],
      "metadata": {
        "id": "xLp3ADPXlXzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/FP_(Komplexit√§tsklasse)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Permanente\n",
        "\n",
        "Hartree‚ÄìFock method"
      ],
      "metadata": {
        "id": "CUkHL9bqzGKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hidden subgroup problem**\n",
        "\n",
        "The dihedral hidden subgroup problem https://arxiv.org/abs/2106.09907\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem"
      ],
      "metadata": {
        "id": "IolmW-jt0zX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum-inspired Algorithms**\n",
        "\n",
        "* quantum computers still exhibit an exponential speedup over all known classical algorithms for sparse, full-rank matrix problems, including the quantum Fourier transform, eigenvector and eigenvalue analysis, linear systems, and others https://arxiv.org/abs/1905.10415\n",
        "* Quantum algorithms for linear algebra are a flagship application of quantum computing, partic- ularly due to their relevance in machine learning. These algorithms typically [scale polylogarithmically](https://en.wikipedia.org/wiki/Polylogarithmic_function) with dimension, which, at the time they were reported, implied an asymptotic exponential speedup compared to state-of-the-art classical methods. For this reason, significant interest has been generated in the dequantization approach that led to breakthrough quantum- inspired classical algorithms for linear algebra problems with sublinear complexity https://arxiv.org/abs/1905.10415\n",
        "\n",
        "* https://crypto.stackexchange.com/questions/48638/whats-the-difference-between-polylogarithmic-and-logarithmic\n",
        "\n",
        "* For example, matrix chain ordering can be solved in polylogarithmic time on a Parallel Random Access Machine."
      ],
      "metadata": {
        "id": "aDu7a9ro6-P_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGOLSoAweEmL"
      },
      "source": [
        "**Example: Efficiency of Matrix Multiplication**\n",
        "\n",
        "* Matrix Multiplication was known to have cubic time complexity $O(n^3)$. Whether you manipulate the equations or take the matrix route, you‚Äôll end up performing the same total number of computational steps to solve the problem. That number is the cube of the number of variables in the system ($n^3$). In case we have three variables, so it takes 33, or 27, computational steps.\n",
        "\n",
        "  * Achtung: researchers measure the speed of matrix multiplication purely in terms of the number of multiplications required\n",
        "\n",
        "  * Generally, the number of additions is equal to the number of entries in the matrix, so four for the two-by-two matrices and 16 for the four-by-four matrices.\n",
        "\n",
        "  * Source: [Matrix Multiplication Inches Closer to Mythic Goal](https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/)\n",
        "\n",
        "* New research in matrix multiplication algorithms shows $O(n^{2.3728596})$. Via simple guessing and them use symmetry to compute just half ('Because the entries in the matrix are random, and coordination happens between them, the matrix itself ends up with certain symmetries. Those symmetries enable computational shortcuts. Just like with any highly symmetric object, you only need to know what one part of it looks like in order to deduce the whole.')\n",
        "\n",
        "  * [Wiki: Matrix Multiplication Algorithm](https://en.m.wikipedia.org/wiki/Matrix_multiplication_algorithm)\n",
        "\n",
        "  * [Wiki: Computational complexity of matrix multiplication](https://en.m.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication)\n",
        "\n",
        "  * [Quanta: New Algorithm Breaks Speed Limit for Solving Linear Equations](https://www.quantamagazine.org/new-algorithm-breaks-speed-limit-for-solving-linear-equations-20210308/)\n",
        "\n",
        "  * [Quanta: Matrix Multiplication Inches Closer to Mythic Goal](https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/)\n",
        "\n",
        "* Simple number multiplication: The traditional grade-school method for matrix multiplication requires $n^2$ steps (linear problem), where n is the number of digits of the numbers you‚Äôre multiplying. So three-digit numbers require nine multiplications, while 100-digit numbers require 10,000 multiplications.\n",
        "\n",
        "  * 1960: Karatsuba‚Äôs method made it possible to multiply numbers using only $n^{1.58}$ single-digit multiplications.\n",
        "\n",
        "  * 1971: Arnold Sch√∂nhage and Volker Strassen published a method capable of multiplying large numbers in n √ó log n √ó log(log n) multiplicative steps, where log n is the logarithm of n. For two 1-billion-digit numbers, Karatsuba‚Äôs method would require about 165 trillion additional steps. **It introduced the use of a technique from the field of signal processing called a fast Fourier transform.**\n",
        "\n",
        "  * 2007: Martin F√ºrer beat it and the floodgates opened. Over the past decade, mathematicians have found successively faster multiplication algorithms, each of which has inched closer to n √ó log n\n",
        "\n",
        "  * 2019: Harvey and van der Hoeven got to n √ó log n (via improved version of the fast Fourier transform and replace even more multiplications with additions and subtractions)\n",
        "\n",
        "  * Further improvements possible? Maybe. But: the design of computer hardware has changed. Two decades ago, computers performed addition much faster than multiplication. The speed gap between multiplication and addition has narrowed considerably over the past 20 years to the point where multiplication can be even faster than addition in some chip architectures. With some hardware, ‚Äúyou could actually do addition faster by telling the computer to do a multiplication problem, which is just insane,‚Äù Harvey said.\n",
        "\n",
        "  * [A New Approach to Multiplication Opens the Door to Better Quantum Computers](https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/))\n",
        "\n",
        "  * [Mathematicians Discover the Perfect Way to Multiply](https://www.quantamagazine.org/mathematicians-discover-the-perfect-way-to-multiply-20190411/)\n",
        "\n",
        "* Addition takes only 2n steps\n",
        "\n",
        "* Tang‚Äôs algorithm ran in polylogarithmic time ‚Äî meaning the computational time scaled with the logarithm of characteristics like the number of users and products in the data set ‚Äî and was exponentially faster than any previously known classical algorithm.\n",
        "\n",
        "  * [Major Quantum Computing Advance Made Obsolete by Teenager](https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic Speedups**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Counting_problem_(complexity)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Polynomialzeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Komplexit√§tstheorie\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Speedup-Theorem\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Nondeterministic_Turing_machine\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Pushdown_automaton\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Quadratische_Optimierung\n",
        "\n",
        "Lineare Programmierung: immer quadratisch. Jedes lineare Optimierungsproblem ist ein quadratisches Optimierungsproblem. https://de.m.wikipedia.org/wiki/Quadratische_Optimierung\n",
        "\n",
        "Nichtlineare programmierung: ein teil ist quadratisch - Quadratic programming is a type of nonlinear programming. (https://en.m.wikipedia.org/wiki/Quadratic_programming)\n",
        "\n",
        "Conversely, any such constrained least squares program can be equivalently framed as a Quadratic programming - https://en.m.wikipedia.org/wiki/Quadratic_programming\n",
        "\n",
        "The use of a quadratic loss function is common, for example when using least squares techniques.  - https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_optimization_algorithms\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Optimization_problem\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Convex_optimization\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Mathematical_optimization"
      ],
      "metadata": {
        "id": "DBjGdloKeEmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it‚Äôs broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can‚Äôt express it easily n^80, you can‚Äôt write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don‚Äôt have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don‚Äôt need any edges, but n^1,5 you can‚Äôt have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "yloFCdAueEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Hardware*"
      ],
      "metadata": {
        "id": "oQZJ3rU97I-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Superconductivity*"
      ],
      "metadata": {
        "id": "-BAUgIGmirEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video [Map of Superconductivity](https://youtu.be/bD2M7P6dTVA)\n"
      ],
      "metadata": {
        "id": "YYP9SNpu4Uck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyon (Quasi Particles)**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Anyon\n",
        "\n",
        "* PBS Video on Quasiparticles: https://youtu.be/le_ORQZzkmE"
      ],
      "metadata": {
        "id": "gy8QFKOMICxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Jellium\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Cooper-Paar\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Bose‚ÄìEinstein_condensate\n",
        "\n",
        "https://opg.optica.org/oe/fulltext.cfm?uri=oe-2-8-299&id=63264\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Phonon\n",
        "\n",
        "\n",
        "Part of this correlation is the formation of pairs of electrons called Cooper pairs. According to Josephson, under certain circumstances these Cooper pairs move from one superconductor to the other across the thin insulating layer. Such motion of pairs of electrons constitutes the Josephson current, and the process by which the pairs cross the insulating layer is called Josephson tunneling.\n",
        "\n",
        "https://www.britannica.com/science/Josephson-effect\n",
        "\n",
        "\n",
        "Meissner effect\n",
        "\n",
        "Meissner effect, the expulsion of a magnetic field from the interior of a material that is in the process of becoming a superconductor, that is, losing its resistance to the flow of electrical currents when cooled below a certain temperature, called the transition temperature, usually close to absolute zero. The Meissner effect, a property of all superconductors, was discovered by the German physicists W. Meissner and R. Ochsenfeld in 1933.\n",
        "\n",
        "https://slideplayer.com/slide/5010186/\n",
        "\n",
        "Squid"
      ],
      "metadata": {
        "id": "I2GgHQvDATOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Josephson Junction*"
      ],
      "metadata": {
        "id": "siEtU4-r3-Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Superconducting integrated circuits**\n",
        "\n",
        "- Conductance is not constant but varies with how much current is flowing, making it an unharmonic oscillator\n",
        "- Potential of the conductor is a cosine\n",
        "- Low energy excitations are pairs of electrons slashing back and forth between the two antenna pads\n",
        "- From ground state to first excited state: 5 gigahertz\n",
        "- From first excited state to second excited state transition: 4.9 Ghz (due to flattened curve of cosine)\n",
        "\n",
        "More details: https://www.youtube.com/watch?v=uD69GCYF9Zg&t=2023s\n"
      ],
      "metadata": {
        "id": "yx8LP-FXATe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "√úbergangsdipolmoment (Transition dipole moment)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/√úbergangsdipolmoment"
      ],
      "metadata": {
        "id": "MTADPYzI5eaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Transmon Qubit*"
      ],
      "metadata": {
        "id": "PdiySKKbZ63A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transmon and Fluxonium Qubit**\n",
        "\n",
        "* Better hardware to protect against noise orders of magnitude better (to get down to 10^-5 instead of 10^-9\n",
        "* they can show cherence times above 1 millisecond, they had single qubit errors of 0.9999 (only 4x) with Fluxonium\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1293.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1299.png)\n",
        "\n",
        "*Source: https://theorie.physik.uni-konstanz.de/burkard/sites/default/files/images/Seminar_3_TrFl.pdf*\n",
        "\n",
        "\n",
        "Video: [Google Keynote: Superconducting qubits for quantum computation: transmon vs fluxonium](https://www.youtube.com/watch?v=qsizrKrUZDg)"
      ],
      "metadata": {
        "id": "We68Qy_9vgEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Appendix*"
      ],
      "metadata": {
        "id": "HN2IgU6_4bCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bsiegelwax.medium.com/i-love-neutral-atoms-47dd41b7a8d5"
      ],
      "metadata": {
        "id": "WoGx5IijM-q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Hardware**\n",
        "\n",
        "* [superconducting qubits](https://en.m.wikipedia.org/wiki/Superconducting_quantum_computing)\n",
        "\n",
        "  * [Building a quantum computer with superconducting qubits](https://www.youtube.com/watch?v=uPw9nkJAwDY)\n",
        "\n",
        "* [trapped ions](https://en.m.wikipedia.org/wiki/Trapped_ion_quantum_computer)\n",
        "\n",
        "* [liquid and solid state nuclear magnetic resonance](https://en.m.wikipedia.org/wiki/Nuclear_magnetic_resonance_quantum_computer)\n",
        "\n",
        "* [optical cluster states](https://en.m.wikipedia.org/wiki/One-way_quantum_computer)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1272.JPG)"
      ],
      "metadata": {
        "id": "puysuv7M8d-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hardware**\n",
        "\n",
        "\n",
        "[Simulations Using a Quantum Computer Show the Technology‚Äôs Current Limits](https://physics.aps.org/articles/v15/175)\n",
        "\n",
        "https://www.techexplorist.com/introducing-unimon-new-superconducting-qubit-quantum-computers/54880/?amp\n",
        "\n",
        "Rydbergs atoms and measuring time: https://www.sciencealert.com/scientists-just-discovered-an-entirely-new-way-of-measuring-time\n",
        "\n",
        "https://www.scinexx.de/news/physik/ein-moebiusband-aus-licht/\n",
        "\n",
        "https://phys.org/news/2022-11-erbium-atoms-silicon-prime-candidate.html\n",
        "\n",
        "https://medium.com/pasqal-io/why-analog-neutral-atoms-quantum-computing-is-the-most-promising-direction-for-early-quantum-77b462cefee0\n",
        "\n",
        "\n",
        "https://phys.org/news/2022-11-quantum-component-graphene.amp\n",
        "\n",
        "https://phys.org/news/2022-10-universal-parity-quantum-architecture-limitations.amp\n",
        "\n",
        "https://www.golem.de/news/quantencomputer-silizium-chip-liest-quantenpunkte-in-rekordzeit-2211-169385.amp.html\n",
        "\n",
        "https://aws.amazon.com/de/blogs/quantum-computing/an-illustrated-introduction-to-quantum-networks-and-quantum-repeaters/\n",
        "\n",
        "https://scitechdaily.com/100-times-longer-than-previous-benchmarks-a-quantum-breakthrough/amp/\n",
        "\n",
        "https://scitechdaily.com/physicists-create-first-quasiparticle-bose-einstein-condensate-the-mysterious-fifth-state-of-matter/amp/\n",
        "\n",
        "https://www.eetimes.eu/quantum-computers-a-technology-assessment/\n",
        "\n",
        "https://medium.com/qiskit/using-quantum-computers-to-tackle-complex-chemistry-simulations-with-quantum-embedding-7b7e4306b676"
      ],
      "metadata": {
        "id": "HMxnncRIeKR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Appendix*"
      ],
      "metadata": {
        "id": "5eNnEuegu2nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *DiVincenzo's criteria*"
      ],
      "metadata": {
        "id": "EBe-6i6B2drL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DiVincenzo's criteria**\n",
        "\n",
        "[DiVincenzo's criteria](https://en.m.wikipedia.org/wiki/DiVincenzo%27s_criteria) are conditions necessary for constructing a quantum computer, conditions proposed in 2000 by the theoretical physicist David P. DiVincenzo.\n",
        "\n",
        "**1. A scalable physical system with well-characterized qubits.**\n",
        "\n",
        "**2. The ability to initialize the state of the qubits to a simple fiducial state, such as 000...).**\n",
        "\n",
        "**3. Long relevant decoherence times, much longer than the gate operation time.**\n",
        "\n",
        "**4. A \"universal\" set of quantum gates** (that approximate any unitary operation - a unitary transformation preserves the inner product, which is a property of the Hilbert space)\n",
        "\n",
        "**5. A qubit-specific measurement capability** (ability to measure individual qubits)\n",
        "\n",
        "* Trapped Ion and superconducting qubits do really well on all five criteria\n",
        "\n",
        "6. *The ability to interconvert stationary and flying qubits.*\n",
        "\n",
        "7. *The ability to faithfully transmit flying qubits between specified locations.*\n",
        "\n",
        "* The DiVincenzo criteria consist of seven conditions an experimental setup must satisfy to successfully implement quantum algorithms such as Grover's search algorithm or Shor factorization. \n",
        "\n",
        "* The first five conditions regard quantum computation itself. Two additional conditions regard implementing quantum communication, such as that used in quantum key distribution. One can demonstrate that DiVincenzo's criteria are satisfied by a classical computer.\n",
        "\n",
        "* Comparing the ability of classical and quantum regimes to satisfy the criteria highlights both the complications that arise in dealing with quantum systems and the source of the quantum speed up.\n",
        "\n",
        "*Universal quantum computing and quantum annealer: not all criteria match the quantum annealers*\n",
        "\n",
        "**Definition of Quantum Computing**\n",
        "\n",
        "[Quantum computing](https://en.m.wikipedia.org/wiki/Quantum_computing) is a type of computation whose operations can harness the phenomena of quantum mechanics, such as superposition, interference, and entanglement to perform computation. Devices that perform quantum computations are known as quantum computers.\n",
        "\n",
        "*Harnessing effects of quantum mechanics: by this definition also quantum annealers are quyantum computers.*"
      ],
      "metadata": {
        "id": "rOFal-YerD7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Effects in Everyday Life*"
      ],
      "metadata": {
        "id": "KftCv8qy8vrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Quantum Effects in Biology**\n",
        "\n",
        "*Casimir effect from quantum (zero point) energy* [Source](https://www.youtube.com/watch?v=Rh898Yr5YZ8)\n",
        "\n",
        "  * Geckos are climbing with use of Van der Vals forces - essentially same things as Casimir force: Gekko's feet with Setae (microscopic hairs)**\n",
        "\n",
        "  * These hairs split into millions of spatula shaped ends that are around point 2 micrometers in diameter\n",
        "\n",
        "  * when a gecko presses its cute little feet onto any surface a Fraction of these hairs are close enough to the surface so that Casimir forces come into effect\n",
        "\n",
        "  * Gecko is literally manipulate quantum vacuum energy to climb walls\n",
        "\n",
        "*Quantum Tunneling* [Source](https://youtu.be/1zDzbXbt7Hc?t=537)\n",
        "\n",
        "  * Microchips: Quantum tunneling of electrons through barrier of less than 1 nanometer. affects minimum size of electronic components in microchips\n",
        "\n",
        "  * *Electrons in the process of photosynthesis via quantum tunneling (Hypothesis)**\n",
        "\n",
        "  * Quantum tunneling maybe responsbile for aging and cancer (Hypothesis)\n",
        "\n",
        "* Laser: in fiber optic cables https://youtu.be/Gbfj_FlEjI4"
      ],
      "metadata": {
        "id": "7K1G_ZESuwi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Sensing*"
      ],
      "metadata": {
        "id": "xioS_hE_0hkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Sensing**\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neuartiger_Quantenzustand_durch_exotisches_Wechselspiel_der_Elektronen.html\n",
        "\n",
        "https://singularityhub.com/2022/10/31/new-3d-quantum-accelerometer-leaves-classical-sensors-in-the-dust/\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Beschleunigungssensor"
      ],
      "metadata": {
        "id": "xw801dHeA42F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum sensing technology is capable of creating new measurement principles or sitting on top of conventional measurement technology and upgrading them (either as an component or theoretical concept).\n",
        "This paper written by Mr. Reichert et. al. in pj Quantur Information is a prime example of such improvements!"
      ],
      "metadata": {
        "id": "NhWE7u-MucGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Applications*"
      ],
      "metadata": {
        "id": "5iVvtH_jA15X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Problem**\n",
        "\n",
        "https://arxiv.org/abs/2111.03011\n",
        "\n",
        "https://en.wikipedia.org/wiki/Boson_sampling"
      ],
      "metadata": {
        "id": "remd7NZ3sK0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Machine Learning**\n",
        "\n",
        "https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/Studien/QML/Quantum_Machine_Learning.pdf\n",
        "\n",
        "https://www.nature.com/articles/s41467-022-31679-5\n",
        "\n",
        "https://arxiv.org/abs/2208.06198"
      ],
      "metadata": {
        "id": "RqyhU5Lu-Wnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Data Encoding (Embedding)*"
      ],
      "metadata": {
        "id": "0nJWHdt9gbt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "xYRmG4v9vwCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10> \n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "Fxjw-h8ox4Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "F2UqpWiN0Dv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features \n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value \n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "GbtZMigG2mij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "T8NOeUKI5dOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "BtmFtmBU6JFh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-1lpMAgXzD9"
      },
      "source": [
        "### <font color=\"blue\">**Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Linear Algebra*"
      ],
      "metadata": {
        "id": "04vVZZU1Q4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Linear algebra for Quantum Mechanics](https://www.youtube.com/watch?v=FF05fXg03A0)"
      ],
      "metadata": {
        "id": "ihAL0jVFhV5o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8I3t063LsOf"
      },
      "source": [
        "###### *Eigenwerte*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQKdBUmBB1w"
      },
      "source": [
        "**Kriterien fur die Existenz von Eigenwerten**\n",
        "\n",
        "(*Wenn eine dieser Aussagen wahr ist, dann alle. Und wenn eine falsch, dann sind alle falsch*)\n",
        "\n",
        "1. $\\operatorname{rg}(B) < n$\n",
        "\n",
        "2. $\\operatorname{det}(B) = 0$ -> dieses Kriterium zu prufen ist am einfachsten und daher am haufigsten!\n",
        "\n",
        "3. $B^{-1}$ existiert nicht (nicht invertierbar)\n",
        "\n",
        "4. $B \\vec{X}$ = 0 hat mehr als nur die Losung $\\vec{x}$ = 0\n",
        "\n",
        "5. $\\lambda$ = 0 ist ein Eigenwert von $B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RIOxr9LuLh"
      },
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1192.png)\n",
        "\n",
        "*Calculating Eigenvalues via determinant: The tweaked transformation squishes space into a lower dimension (Daher muss rang < n sein)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rayleigh-Quotient & Satz von Courant-Fischer*"
      ],
      "metadata": {
        "id": "30ncah90297J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), auch Rayleigh-Koeffizient genannt, ist ein Objekt aus der linearen Algebra.\n",
        "\n",
        "Der Rayleigh-Quotient wird insbesondere zur numerischen Berechnung von Eigenwerten einer quadratischen Matrix $A$ verwendet.\n",
        "\n",
        "Sei $A \\in {\\mathbb K}^{n \\times n}$ eine reelle symmetrische oder komplexe hermitesche Matrix und $x\\in {\\mathbb {K} }^{n}$ mit $x\\neq 0$ ein Vektor, dann ist der Rayleigh-Quotient von $A$ zum Vektor $x$ definiert durch:\n",
        "\n",
        "> $R_{A}(x)={\\frac  {x^{*}Ax}{x^{*}x}}$\n",
        "\n",
        "Der Rayleigh-Quotient hat eine enge Beziehung zu den Eigenwerten von $A$. Ist $v$ ein Eigenvektor der Matrix $A$ und $\\lambda$  der zugeh√∂rige Eigenwert, dann gilt:\n",
        "\n",
        "> $R_{A}(v)={\\frac  {v^{*}Av}{v^{*}v}}={\\frac  {v^{*}\\lambda v}{v^{*}v}}=\\lambda$\n",
        "\n",
        "Durch den Rayleigh-Quotienten wird also jeder Eigenvektor von $A$ auf den dazugeh√∂rigen Eigenwert $\\lambda$ abgebildet. Diese Eigenschaft wird unter anderem in der numerischen Berechnung von Eigenwerten benutzt.\n",
        "\n",
        "Insbesondere gilt f√ºr eine symmetrische oder hermitesche Matrix $A$ mit dem kleinsten Eigenwert $\\lambda _{{{\\rm {min}}}}$ und dem gr√∂√üten Eigenwert $\\lambda _{{{\\rm {max}}}}$ nach dem [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) (Der Satz von Courant-Fischer stellt die Eigenwerte einer symmetrischen oder hermiteschen Matrix als minimale beziehungsweise maximale Rayleigh-Quotienten):\n",
        "\n",
        "> $\\lambda _{{{\\rm {min}}}}\\leq R_{A}(x)\\leq \\lambda _{{{\\rm {max}}}}$\n",
        "\n",
        "Die Berechnung des kleinsten bzw. gr√∂√üten Eigenwerts ist damit √§quivalent zum Auffinden des Minimums bzw. Maximums des Rayleigh-Quotienten. Das l√§sst sich unter geeigneten Voraussetzungen auch noch auf den unendlichdimensionalen Fall verallgemeinern und ist als Rayleigh-Ritz-Prinzip bekannt.\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem Ellipsoid (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "q7aw0uQL3AnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kernel*"
      ],
      "metadata": {
        "id": "YfOrrAOFvF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel = Nullspace**\n",
        "\n",
        "The [kernel of this linear map (linear algebra)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) is the set of solutions to the equation Ax = 0, where 0 is understood as the zero vector."
      ],
      "metadata": {
        "id": "nHdImLnovH1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In algebra, the [kernel (algebra)](https://en.m.wikipedia.org/wiki/Kernel_(algebra)) of a homomorphism (function that preserves the structure) is generally the inverse image of 0.\n",
        "\n",
        "* The kernel of a homomorphism is reduced to 0 (or 1) if and only if the homomorphism is injective, that is if the inverse image of every element consists of a single element (jedes element im ziel hat nur ein element im ursprung, es kann aber mehr elemente im ziel ohne ein element im ursprung geben).\n",
        "\n",
        "* This means that the kernel can be viewed as a measure of the degree to which the homomorphism fails to be injective.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/320px-KerIm_2015Joz_L2.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "hLbt81t-PVPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "der Kern einer linearen Abbildung f genau dann nichttrivial ist, wenn eine linear unabh√§ngige Menge (bzw. genauer Familie) S existiert sodass f(S) linear abh√§ngig ist. Durch √úbergang zu den Negationen erhalten wir dann die √§quivalente Aussage:\n",
        "\n",
        "- kerf = {0}\n",
        "- ...genau dann, wenn gilt...\n",
        "- f√ºr alle linear unabh√§ngigen S ist auch f(S) linear unabh√§ngig.\n",
        "\n",
        "https://www.youtube.com/watch?v=GNf3StvaiFA&list=WL&index=9"
      ],
      "metadata": {
        "id": "UbcK-n4ZJ5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Diagonalisierbarkeit*"
      ],
      "metadata": {
        "id": "-wUJznaApPv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-5FB4sQ_XI0"
      },
      "source": [
        "**[Diagonalisierbarkeit](https://en.m.wikipedia.org/wiki/Diagonalizable_matrix) von Matrizen**:\n",
        "\n",
        "* **Eigenwerte liegen auf der Diagonalen**. Dabei: Spur = Summer aller Eigenwerte. Determinante = Produkt aller Eigenwerte = 0.\n",
        "\n",
        "* Square matrix $A$ into **invertible matrix** $P$ and **diagonal matrix** $D$ such that  ${\\displaystyle P^{-1}AP=D}$. Approach: [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix).\n",
        "\n",
        "> $P^{-1} A P=\\left[\\begin{array}{cccc}\n",
        "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right]$\n",
        "\n",
        "* Ein rotierender K√∂rper ohne √§u√üere Kr√§fte verbleibt in seiner Bewegung, wenn er um seine Symmetrieachse rotiert. Wenn eine Basis aus Eigenvektoren existiert, so ist die Darstellungsmatrix bez√ºglich dieser Basis eine Diagonalmatrix\n",
        "\n",
        "* *The diagonalization of a symmetric matrix can be interpreted as a rotation of the axes to align them with the eigenvectors:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/4/4e/Diagonalization_as_rotation.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Determinant (Permanent, Immanant)*"
      ],
      "metadata": {
        "id": "yIoPrdtMpRd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Determinante](https://de.m.wikipedia.org/wiki/Determinante)**:\n",
        "\n",
        "* [Determinante](https://de.m.wikipedia.org/wiki/Determinante) (nur fur quadratischen Matrix - bei $n \\cdot m$ Matrizen nutzt man zB SVD)\n",
        "\n",
        "  * gibt an, wie sich Fl√§che bzw. Volumen durch linearen Abbildung √§ndert. det(A) = 4 $\\rightarrow$ Matrix vervierfacht Fl√§cheninhalt.\n",
        "\n",
        "  * gibt an, ob lineares Gleichungssystem l√∂sbar ist (L√∂sung dann mit der Cramerschen Regel)\n",
        "\n",
        "  * Determinante ist Produkt aller Eigenwerte: $\\prod_{i=1}^{n} \\lambda_{i}=\\operatorname{det}(A) = 0$\n",
        "\n",
        "* Berechnung: $\\operatorname{det}(A)$: $\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right)\n",
        "$ $\\rightarrow$ $\n",
        "\\operatorname{det} A=\\left|\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$. *Die 2x2-Determinante **ist gleich dem orientierten Fl√§cheninhalt** des von ihren Spaltenvektoren aufgespannten Parallelogramms:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Area_parallellogram_as_determinant.svg/220px-Area_parallellogram_as_determinant.svg.png)\n",
        "\n",
        "* [Entwicklungssatz](https://de.m.wikipedia.org/wiki/Determinante#Laplacescher_Entwicklungssatz) oder  [Leibniz-Formel](https://de.m.wikipedia.org/wiki/Determinante#Leibniz-Formel) (geschlossene Form, theoretisch). [Gau√ü-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren) in Dreiecksform - Determinante ist Produkt der [Hauptdiagonale](https://de.m.wikipedia.org/wiki/Hauptdiagonale). Computeralgorithmus: [LU-Zerlegung](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren#LR-Zerlegung).\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) (=has the eigenvalues as roots, and it has the determinant and the trace / sum o fEigenvalues of the matrix among its coefficients):\n",
        "\n",
        "> <font color=\"red\">$f(\\lambda) = \\operatorname{det}(A) = 0$</font> $\\quad (= \\prod_{i=1}^{n} \\lambda_{i})$\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, <font color=\"red\">**the solutions of $f(\\lambda) = 0$** (determinant is equal to zero, Eigenvalues are \"coefficients in characteristic polynomial\" (it's trace to be precise))</font> are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "J1Ir4QyrBark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Permanent_(mathematics)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Immanant"
      ],
      "metadata": {
        "id": "U3D3fAGop0ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Characteristics polynomial (Fundamental theorem of algebra)*"
      ],
      "metadata": {
        "id": "xemut5KHpjBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Characteristics polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) = has the Eigenvalues as [roots](https://en.m.wikipedia.org/wiki/Zero_of_a_function) (zero of a function)\n",
        "\n",
        "Root (Zero of a function): The function f attains the value of 0 at x, or equivalently, x is the solution to the equation f(x) = 0. A root of a polynomial is a zero of the corresponding polynomial function. The [fundamental theorem of algebra](https://en.m.wikipedia.org/wiki/Fundamental_theorem_of_algebra) shows that any non-zero polynomial has a number of roots at most equal to its degree.\n",
        "\n",
        "For example, the polynomial f of degree two, defined by $f(x)=x^{2}-5x+6$ has the two roots (or zeros) that are $\\lambda_1=2$ and $\\lambda_2=3$:\n",
        "\n",
        "> ${\\displaystyle f(2)=2^{2}-5\\times 2+6=0{\\text{ and }}f(3)=3^{2}-5\\times 3+6=0.}$\n",
        "\n",
        "<font color=\"red\">Zero of a function is important because every equation in the unknown x may be rewritten as $f(x)=0$ by regrouping all the terms in the left-hand side. Hence the study of zeros of functions is exactly the same as the study of solutions of equations.</font>\n",
        "\n",
        "Siehe [Nullstellen](https://de.m.wikipedia.org/wiki/Nullstelle) sind bei einer Funktion diejenigen Werte der Ausgangsmenge (des Definitionsbereichs D), bei denen das im Rahmen der Abbildung zugeordnete Element der Zielmenge (des Wertebereichs W) die Null ist (${\\displaystyle 0\\in W}$). Nullstellen von Polynomfunktionen werden auch als Wurzeln bezeichnet.\n",
        "\n",
        "[Kern](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) is L√∂sungsmenge der [homogenen linearen Gleichung](https://de.m.wikipedia.org/wiki/Lineare_Gleichung) f(x)=0 und wird hier auch Nullraum genannt (denjenigen Vektoren in V, die auf den Nullvektor in W abgebildet werden)\n",
        "\n",
        "Example: A graph of the function $\\cos (x)$ for $x$ in $[-2 \\pi, 2 \\pi]$, with zeros at $-\\frac{3 \\pi}{2},-\\frac{\\pi}{2}, \\frac{\\pi}{2}$, and $\\frac{3 \\pi}{2}$, marked in red.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/X-intercepts.svg/480px-X-intercepts.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "0jYqsd3QptJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Invertible Matrix*"
      ],
      "metadata": {
        "id": "cLeG1PswpSaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Invertible Matrix**\n",
        "\n",
        "* ($B^{-1}$ existiert nicht (Matrix nicht invertierbar) $\\rightarrow$ dann existieren Eigenwerte)\n",
        "\n",
        "* [Invertible matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix) is used in methods to solve systems of linear equations and **help to get Eigenvalues**\n",
        "\n",
        "> Apply Eigendecomposition to matrix (or other method) $\\rightarrow$ Get matrix inverse (with Eigenvectors and a diagonal with Eigenvalues) $\\rightarrow$ Solve systems of linear equations\n",
        "\n",
        "* You can also use pseudo-inverse [Moore-Penrose](https://en.m.wikipedia.org/wiki/Moore‚ÄìPenrose_inverse) with [matrix solution](https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution) to solve systems of linear equations or in [curve fitting](https://de.wikipedia.org/wiki/Ausgleichungsrechnung) (like regression or ML). Methods: QR, Cholesky, Rank decomposition, SVD. 'The pseudoinverse provides a [Linear Least Squares](https://en.m.wikipedia.org/wiki/Linear_least_squares) solution to a system of linear equations.'\n",
        "\n",
        "  * Exkurs: [Linear least squares (LLS)](https://en.wikipedia.org/wiki/Linear_least_squares) for solving systems of linear equations: is the least squares approximation of linear functions to data (linear regression). **Numerical methods include inverting the matrix of the normal equations and orthogonal decomposition methods**.\n",
        "\n",
        "  * **Overdetermined case**: A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution. See under [Applications](https://en.m.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Applications)\n",
        "\n",
        "  * **Underdetermined case**: Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. [Underdetermined system](https://en.m.wikipedia.org/wiki/Underdetermined_system): there are **fewer equations than unknowns**. Has an infinite number of solutions, if any. In optimization problems that are subject to linear equality constraints, only one of the solutions is relevant, namely the one giving the **highest or lowest value of an objective function**. The use of the **(Moore Pensore) pseudoinverse is to find the minimum (Euclidean) norm solution** to a system of linear equations with multiple solutions. Can be computed using the singular value decomposition. *Example: The solution set for two equations in three variables is, in general, a line ([Source](https://en.m.wikipedia.org/wiki/System_of_linear_equations#General_behavior)):*\n",
        "\n",
        "  ![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Intersecting_Planes_2.svg/240px-Intersecting_Planes_2.svg.png)\n",
        "\n",
        "* [Methods to compute inverse of matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion): Gaussian elimination, Newton's method, Cayley‚ÄìHamilton method, Eigendecomposition, Cholesky decomposition, Analytic solution (Cramer's rule), Blockwise inversion.\n",
        "\n",
        "* Example: If matrix A can be [eigendecomposed](https://en.m.wikipedia.org/wiki/Invertible_matrix), and if none of its eigenvalues are zero, then A is invertible and its inverse is given by\n",
        "\n",
        ">${\\displaystyle \\mathbf {A} ^{-1}=\\mathbf {Q} \\mathbf {\\Lambda } ^{-1}\\mathbf {Q} ^{-1}}$\n",
        "\n",
        "* where $\\mathbf {Q}$  is the square ($N√óN$) matrix whose i-th column is the **eigenvector** $q_{i}$ of $\\mathbf {A}$\n",
        "* ${\\displaystyle \\mathbf {\\Lambda } }$ is the diagonal matrix whose diagonal elements are the corresponding **eigenvalues** ${\\displaystyle \\Lambda _{ii}=\\lambda _{i}}$."
      ],
      "metadata": {
        "id": "49Uc9N-uV6_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rank*"
      ],
      "metadata": {
        "id": "919bw_THpTYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a matrix**\n",
        "\n",
        "\n",
        "The rank of a matrix is the number of linearly independent vectors (columns).\n",
        "\n",
        "So **low rank is a low number** of linearly independent vectors. A low rank matrix can be this:\n",
        "\n",
        "1\n",
        "\n",
        "2\n",
        "\n",
        "3\n",
        "\n",
        "\n",
        "* a low rank matrix (whether approximation or not) is simply a matrix for which the number of linearly independent row or columns is much smaller than the actual number of rows or columns.\n",
        "\n",
        "* Viewed as a linear transformation, the span of its range is small or the span of its null space is large.\n",
        "\n",
        "* Quantum computers give exponential speedups for high rank matrices"
      ],
      "metadata": {
        "id": "t9uYK8aS8akn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Sparse Matrix vs Low Rank Matrix**\n",
        "\n",
        "Sparse matrices and low-rank matrices are two very different types of objects. The matrix\n",
        "\n",
        "$\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4\\end{array}\\right]$\n",
        "\n",
        "\n",
        "is sparse (meaning it has a lot of zero entries) but not low-rank (as a matter of fact it‚Äôs full rank). On the other hand, the matrix\n",
        "\n",
        "$\\left[\\begin{array}{cccc}1 & 2 & 3 & 4 \\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 9 & 12 \\\\ 4 & 8 & 12 & 16\\end{array}\\right]$\n",
        "\n",
        "is low-rank but not sparse!\n",
        "\n",
        "There's however a connection to be made between the two concepts: **a low-rank matrix has a sparse set of singular values**.\n",
        "\n",
        "Take the following singular value decomposition for a general matrix $\\mathrm{X}$\n",
        "\n",
        "> $\n",
        "\\mathrm{X}=\\mathrm{USV}^T\n",
        "$\n",
        "\n",
        "Then the number of non-zero entries in (the diagonal of) $S$ is precisely equal to the rank of $\\mathbf{X}$. Thus, a sparse $\\mathbf{S}$ leads to a low-rank $\\mathbf{X}$ and vice-versa.\n"
      ],
      "metadata": {
        "id": "4eqnSMcl83mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Algebraic and geometric multiplicity of eigenvalues*"
      ],
      "metadata": {
        "id": "h0hcBLukpUcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic and geometric multiplicity of eigenvalues**\n",
        "\n",
        "> Two or more distinct eigenvalues = algebraic multiplicity\n",
        "\n",
        "> By how many linearly independent vectors is the Eigenspace of $\\lambda_1$ (which is an Eigenvalue with multiplicity 1 or more) formed? = geometric multiplicity\n",
        "\n",
        "> Geometric multiplicity is max equal or less than its algebraic multiplicity\n",
        "\n",
        "> When the geometric multiplicity of a repeated eigenvalue is **strictly less** than its algebraic multiplicity, then that eigenvalue is said to be **defective**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multiplicity_(mathematics)#Multiplicity_of_a_root_of_a_polynomial\n",
        "\n",
        "*algebraic multiplicity*\n",
        "\n",
        "The **algebraic multiplicity** of an eigenvalue is the number of times it appears as a root of the characteristic polynomial (i.e., the polynomial whose roots are the eigenvalues of a matrix).\n",
        "\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its characteristic polynomial is:\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, the solutions of $f(\\lambda) = 0$  are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Thus, $A$ has one repeated eigenvalue whose algebraic multiplicity is\n",
        "\n",
        ">$\n",
        "\\mu\\left(\\lambda_1\\right)=\\mu\\left(\\lambda_2\\right)=2\n",
        "$\n",
        "\n",
        "*geometric multiplicity*\n",
        "\n",
        "The **geometric multiplicity** of an eigenvalue is the dimension of the linear space of its associated eigenvectors (i.e., its eigenspace).\n",
        "\n",
        "If the Eigenspace of $\\lambda_1$ is generated only by a single vector, it has dimension 1. As a consequence, the geometric multiplicity of $\\lambda_1$ is 1, less than its algebraic multiplicity, which is equal to 2.\n",
        "\n",
        "See complete example in [this pdf document](https://raw.githubusercontent.com/deltorobarba/repo/master/multiplicity.pdf)."
      ],
      "metadata": {
        "id": "VR06jupDflnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Condition Numbers*"
      ],
      "metadata": {
        "id": "0p0KFlAypVZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Condition Numbers**\n",
        "\n",
        "https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/"
      ],
      "metadata": {
        "id": "0JY7J0768m4Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwJ8QbNvEbX"
      },
      "source": [
        "###### *Get Eigenvalues: **Algorithms** (Power & Inverse Iteration, Charakteristisches Polynom) & **Matrix Decomposition** (Eigendecomposition/Spectrum, Schur, SVD)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenvalue algorithm](https://en.m.wikipedia.org/wiki/Eigenvalue_algorithm) - matrices are diagonalized numerically using computer software. See [List of Eigenvalue algorithms](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Eigenvalue_algorithms)."
      ],
      "metadata": {
        "id": "cjzaJcIsQrtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbDi3hDN8p4"
      },
      "source": [
        "**Small Matrices: Ermittlung der Eigenwerte der Matrix $A$ mit Determinante und charakteristischem Polynom**\n",
        "\n",
        "In practice, eigenvalues of large matrices are not computed using the characteristic polynomial [Source](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Numerical_computations)\n",
        "\n",
        "> $A=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)$\n",
        "\n",
        "**Step 1: Bilde mit der Einheitsmatrix $E_{n}$ die Matrix $\\left(A-\\lambda E_{n}\\right)$**\n",
        "\n",
        "> $\\left(A-\\lambda E_{n}\\right)=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)-\\left(\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{array}\\right)=\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "**Step 2: Berechne die Determinante von $\\operatorname{det}\\left(A-\\lambda E_{n}\\right) = \\chi_{A}(\\lambda)$ $\\rightarrow$ [charakteristisches Polynom](https://de.m.wikipedia.org/wiki/Charakteristisches_Polynom)**\n",
        "\n",
        "> $\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=\\operatorname{det}\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "$=(2-\\lambda)^{2} \\cdot(3-\\lambda)+2+2-2 \\cdot(2-\\lambda)-2 \\cdot(2-\\lambda)-(3-\\lambda)$\n",
        "$=-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5$ $\\quad$ (= Polynom)\n",
        "\n",
        "**Step 3: Bestimme die Nullstellen des charakteristischen Polynoms, weil das sind die Eigenwerte der Matrix $A$ (und Determinante wird Null)**\n",
        "\n",
        "> $(A-\\lambda E_{n}) \\cdot v=0$\n",
        "\n",
        "* Durch Ausprobieren: erste Nullstelle $\\lambda_{1}=1$.\n",
        "\n",
        "* Klammern wir dann den Faktor $(\\lambda-1)$ aus, erhalten wir:\n",
        "$-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5=(\\lambda-1) \\cdot\\left(-\\lambda^{2}+6 \\lambda-5\\right)\n",
        "$.\n",
        "\n",
        "* Anwendung der [Mitternachtsformel](https://de.m.wikipedia.org/wiki/Quadratische_Gleichung#L√∂sungsformel_f√ºr_die_allgemeine_quadratische_Gleichung_(a-b-c-Formel)): $\n",
        "\\lambda_{2,3}=\\frac{-6 \\pm \\sqrt{36-20}}{-2}=3 \\mp 2$\n",
        "\n",
        "* Somit lauten die drei Eigenwerte der Matrix $\\lambda_{1}=\\lambda_{2}=1$ sowie $\\lambda_{3}=5$\n",
        "\n",
        "*Gibt es eine Zahl $\\lambda$ und einen Vektor $v$, sodass dieser durch Multiplikation mit der Matrix $\\left(A-\\lambda E_{n}\\right)$ auf den Nullvektor abgebildet wird, so ist diese Matrix nicht von vollem Rang. Das bedeutet, dass ihre Determinante Null ist. Vektoren wie $v$ und $w$ sind linear abh√§ngig.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooWX-rgS12w"
      },
      "source": [
        "**Eigendecomposition (spectral decomposition)**\n",
        "\n",
        "* [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or sometimes spectral decomposition, see [spectral theorem](https://en.m.wikipedia.org/wiki/Spectral_theorem), is the factorization of a matrix into a canonical form (Normalform) - the matrix is represented in terms of its eigenvalues and eigenvectors. Matrix must be diagonalizable. See also [Summary of Eigendecomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition#Decompositions_based_on_eigenvalues_and_related_concepts).\n",
        "\n",
        "> ${\\displaystyle A=VDV^{-1}}A=VDV^{{-1}}$\n",
        "\n",
        "* $D$ = eigenvalues of $A$ (diagonal)\n",
        "* $V$ = eigenvectors of $A$\n",
        "\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur decomposition**\n",
        "\n",
        "* [Schur decomposition](https://en.m.wikipedia.org/wiki/Schur_decomposition) is a [matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition).\n",
        "\n",
        "* Take complex square matrix and **get an upper triangular matrix whose diagonal elements are the eigenvalues** of the original matrix."
      ],
      "metadata": {
        "id": "wqlosIz9egUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singular Value Decomposition**\n",
        "\n",
        "* [Singular Value Decomposition](https://de.m.wikipedia.org/wiki/Singul√§rwertzerlegung) is used in calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.\n",
        "\n",
        "* SVD can also be used in least squares linear regression, image compression, and denoising data. Siehe auch [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm)\n"
      ],
      "metadata": {
        "id": "lCrTpBOyevh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Decomposition** (Gaussian Elimination, Cholesky, QR, LU)*"
      ],
      "metadata": {
        "id": "0ChC-FyHZ41O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical: [Gaussian Elimination](https://en.m.wikipedia.org/wiki/Gaussian_elimination) (row reduction). See [List of numerical algorithms to solve systems of linear equation](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Solving_systems_of_linear_equations)"
      ],
      "metadata": {
        "id": "KzcuDJOLhCGd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8EITyoKDXA"
      },
      "source": [
        "**Cholesky Decomposition** (Hermitian / squared)\n",
        "\n",
        "* **alternative to Eigendecomposition** to get matrix inverse for solving linear equations\n",
        "\n",
        "* [Cholesky decomposition](https://en.m.wikipedia.org/wiki/Cholesky_decomposition) of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,\n",
        "\n",
        "* useful for efficient numerical solutions, e.g., Monte Carlo simulations (solving linear least squares for linear regression, as well as simulation and optimization methods)\n",
        "\n",
        "* When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.\n",
        "\n",
        "> $A = LL^T$\n",
        "\n",
        "* $L$ is the lower triangular matrix and $L^T$ is the transpose of L. Decompose as the product of upper triangular matrix $U$ is also possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NVXqSfFlBt"
      },
      "source": [
        "**QR Decomposition**\n",
        "\n",
        "* We can use QR decomposition to [find the determinant of a square matrix](https://en.m.wikipedia.org/wiki/QR_decomposition#Connection_to_a_determinant_or_a_product_of_eigenvalues)\n",
        "\n",
        "* The [QR decomposition](https://en.m.wikipedia.org/wiki/QR_decomposition) is for m x n matrices (not limited to square matrices) and decomposes a matrix into $Q$ (orthogonal $Q^T Q = I$ or unitary $Q \\cdot Q = I$, size m x m) and $R$ (upper triangle matrix with the size m x n) components.\n",
        "\n",
        "> $A = Q R$\n",
        "\n",
        "* Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, **although is <u>not</u> limited to square matrices**.\n",
        "\n",
        "* There are several methods for actually computing the QR decomposition, such as by [Householder transformations](https://de.m.wikipedia.org/wiki/Householdertransformation), [Givens rotations](https://de.m.wikipedia.org/wiki/Givens-Rotation) and  [Gram-Schmidtsch's orthogonalization method](https://de.m.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDrmirVPSXe"
      },
      "source": [
        "**LU Decomposition**\n",
        "\n",
        "* The [LU decomposition](https://en.m.wikipedia.org/wiki/LU_decomposition) is often used to simplify the **solving of systems of linear equations**, such as **finding the coefficients in a linear regression**, as well as in **calculating the determinant and inverse** of a matrix.\n",
        "\n",
        "* Lower‚Äìupper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix.\n",
        "\n",
        "* The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian elimination.\n",
        "\n",
        "* Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix.\n",
        "\n",
        "The **LU decomposition is for square matrices** and decomposes a matrix into L and U components. Let A be a square matrix. An LU factorization refers to the factorization of A, with proper row and/or column orderings or permutations, into two factors ‚Äì a **lower triangular matrix L** and an **upper triangular matrix U**:\n",
        "\n",
        "> A = L U\n",
        "\n",
        "* The LU decomposition is found using an <u>iterative numerical process</u> and **can fail for those matrices that cannot be decomposed or decomposed easily**.\n",
        "\n",
        "* In the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero. For example, for a 3 √ó 3 matrix A, its LU decomposition looks like this:\n",
        "\n",
        "> $\\left[\\begin{array}{lll}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
        "u_{11} & u_{12} & u_{13} \\\\\n",
        "0 & u_{22} & u_{23} \\\\\n",
        "0 & 0 & u_{33}\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Type** (Squared & Non-Squared)*"
      ],
      "metadata": {
        "id": "PREcYcRRPEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Solving Systems of Linear Equations: <u>Squared Matrix</u> (for HHL) - Eigendecomposition to get Pseudo-inverse**\n",
        "\n",
        "> https://towardsdatascience.com/from-eigendecomposition-to-determinant-fundamental-mathematics-for-machine-learning-with-1b6b449a82c6\n",
        "\n",
        "\n",
        "<font color=\"red\">**If A is squared is a matrix (and has [full rank](https://de.m.wikipedia.org/wiki/Rang_(Mathematik))) in a linear system of equations: Getting the matrix inverse via Eigendecomposition (pseudoinverse Moore-Penrose provides a least squares solution to a system of linear equations.')**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "then you can take the [inverse](https://de.m.wikipedia.org/wiki/Inverse_Matrix) if A to solve for x:\n",
        "\n",
        "> $x = A^{-1} b$\n",
        "\n",
        "<font color=\"red\">*This part is important for HHL:*\n",
        "\n",
        "* $\\hat{A} = \\hat{A}^{\\dagger}$ $\\quad$ - Hermitian operators are [Self-adjoint operators](https://en.m.wikipedia.org/wiki/Self-adjoint_operator)\n",
        "\n",
        "* Adjungierte Matrix = transponiert + complex konjugiert (Vorzeichen umgekehrt). Hermetian: selbstadjunktiert = symmetrisch\n",
        "\n",
        "* Following from this, in bra-ket notation: $\n",
        "\\left\\langle\\phi_{i}|\\hat{A}| \\phi_{j}\\right\\rangle=\\left\\langle\\phi_{j}|\\hat{A}| \\phi_{i}\\right\\rangle^{*}\n",
        "$\n",
        "\n",
        "\n",
        "<font color=\"red\">*Since $A$ is Hermitian, it has a spectral decomposition : $\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$*\n",
        "\n",
        "<font color=\"red\">*You need the Eigendecomposition (spectral decomposition) to get the inverse of a matrix (=here unitary and hence normal)*\n",
        "\n",
        "Getting the [Matrix inverse via eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition): If a matrix $\\mathbf{A}$ can be eigendecomposed and if none of its eigenvalues are zero, then $\\mathbf{A}$ is invertible and its inverse is given by\n",
        "\n",
        ">$\n",
        "\\mathbf{A}^{-1}=\\mathbf{Q}^{-1} \\mathbf{\\Lambda}^{-1} \\mathbf{Q}\n",
        "$\n",
        "\n",
        "If $\\mathbf{A}$ is a symmetric matrix, since $\\mathbf{Q}$ is formed from the eigenvectors of $\\mathbf{A}, \\mathbf{Q}$ is guaranteed to be an orthogonal matrix, therefore $\\mathbf{Q}^{-1}=\\mathbf{Q}^{\\mathrm{T}}$. Furthermore, because $\\mathbf{\\Lambda}$ is a diagonal matrix, its inverse is easy to calculate:\n",
        "\n",
        ">$\n",
        "\\left[\\Lambda^{-1}\\right]_{i i}=\\frac{1}{\\lambda_{i}}\n",
        "$\n",
        "\n",
        "**Example - the matrix:**\n",
        "\n",
        "$A=\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]$\n",
        "\n",
        "has the eigenvectors:\n",
        "\n",
        "$\\mathbf{u}_{1}=\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{1}=4$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\mathbf{u}_{2}=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{2}=-1$\n",
        "\n",
        "We can verify (as illustrated in Figure 1) that only the length of $\\mathbf{u}_{1}$ and $\\mathbf{u}_{2}$ is changed when one of these two vectors is multiplied by the matrix $\\mathbf{A}$ :\n",
        "\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=4\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 8\\end{array}\\right]$\n",
        "\n",
        "and\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=-1\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{r}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "For most applications we normalize the eigenvectors (i.e. transform them such that their length is equal to one):\n",
        "\n",
        "$\n",
        "\\mathbf{u}^{\\top} \\mathbf{u}=1 \\text {. }\n",
        "$\n",
        "\n",
        "For the previous example we obtain:\n",
        "\n",
        "$\n",
        "\\mathbf{u}_{1}=\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Exkurs: wie man einen Vektor normiert:\n",
        "\n",
        "1) Betrag von $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ist gleich $\\sqrt{3^{2}+2^{2}}=3.6055$ Vektor normieren, also mit 1/Betrag malnehmen:\n",
        "\n",
        "2) $\n",
        "\\frac{1}{3.6055} \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right)= 0.27735 \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right) =\\left(\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\n",
        "We can check that:\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "3.3284 \\\\\n",
        "2.2188\n",
        "\\end{array}\\right]=4\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "and\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
        ".7071 \\\\\n",
        "-.7071\n",
        "\\end{array}\\right]=-1\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5z-Ez-u4RWhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Solving Systems of Linear Equations: <u>Non-Squared Matrix</u> - Singular value decomposition (SVD) to get Pseudo-inverse**\n",
        "\n",
        "<font color=\"red\">**If a matrix A is not squared: singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.**\n",
        "\n",
        "> $A x = b$ $\\quad$ ($A$ is not regular)\n",
        "\n",
        "You multiply both sides by the $A^{T}$, which is the transpose of A:\n",
        "\n",
        "> $A^{T} A x = A^{T} b$\n",
        "\n",
        "Then move $A^{T} A$ on right side (by taking their inverse). This is not the original x anymore, but the [least squares](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate#Lineare_Modellfunktion) $\\hat{x}$\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "And that term <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> is know as (Moore‚ÄìPenrose) pseudo-inverse <font color=\"blue\">$A^{+}$</font>:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$A^{+}$</font> $b$\n",
        "\n",
        "And a pseudo-inverse is nothing else than our least-squares solutions. See more details under [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)\n",
        "\n",
        "*In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.*\n",
        "\n",
        "* non squared matrix\n",
        "\n",
        "* to solve you need the inverse, but normal eigendecomposition doesn work\n",
        "\n",
        "> you need to compute mooore penrose pseudo-inverse, and here you need to apply singular value decomposition to get eigendecomposition\n",
        "\n",
        "* SVD is a type of [Matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition), specicially one based on eigenvalue concepts. and SVD is a full rank decomposition (besides broader [Rank factorization methods](https://en.m.wikipedia.org/wiki/Rank_factorization))\n",
        "\n",
        "* matrix decompositions in general are used to take a large matrix apart (i.e. factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U) so the new matrices require fewer additions and multiplications to solve, compared with the original system $A\\mathbf {x} =\\mathbf {b}$\n",
        "\n",
        "<font color=\"red\">**Example:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right] x=\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "* We can immediately see that matrix A is of rank 2 because the first two rows are multiples of each other (2 and -2 and -2 and 2), just the last row numbers are not multiples of each other (5 and 3).\n",
        "\n",
        "* Now we can apply the pseudo-inverse to find the least-squares solution:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "> $\\hat{x}=$ <font color=\"blue\">$\\left(\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]$</font>$\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "> $\\hat{x}=$$\\left[\\begin{array}{l}-4 \\\\ -2\\end{array}\\right]$\n",
        "\n",
        "\n",
        "*Inserting the least-squares $\\hat{x}$ into the original equation:*\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\left[\\begin{array}{l}-4 \\\\ -3\\end{array}\\right]$ = $\\left[\\begin{array}{c}2 \\cdot(-4)+(-2) \\cdot(-3) \\\\ (-2)\\cdot (-4)+2 \\cdot (-3) \\\\ 5\\cdot (-4)+3 \\cdot (-3)\\end{array}\\right]$ = $\\left[\\begin{array}{c}-2 \\\\ 2 \\\\ -29\\end{array}\\right]$ $\\approx$ $\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$"
      ],
      "metadata": {
        "id": "g9VGZpsMZff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum-inspired Algorithms*"
      ],
      "metadata": {
        "id": "0-Lu47J5MYMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.hpcwire.com/2022/11/03/quantum-annealing-pioneer-d-wave-introduces-expanded-hybrid-solver/"
      ],
      "metadata": {
        "id": "c6wJklgHkzBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewin Tang: [A quantum-inspired classical algorithm for recommendation systems](https://arxiv.org/abs/1807.04271)\n",
        "\n",
        "https://www.youtube.com/watch?v=P2Bucnq_ap0"
      ],
      "metadata": {
        "id": "dCqBW6oqOTUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1905.10415 with https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf\n",
        "\n",
        "* There are two ways to compute the average of many values. A first method is to add all of them together and divide the result by the total number of values. An alternative approach is to select a few values at random, then compute their average using the first method. Of course, in the second case we get only an approximation of the true average, but in many cases that is good enough and can be done in significantly less time.\n",
        "\n",
        "* **This is the strategy of quantum-inspired algorithms: the coefficients of the solution vector are inner products between vectors, which can be expressed as the average of a collection of numbers that depend on the entries of the vectors**. Instead of directly computing this average (which takes linear time) we select a few of the values at random and compute their average instead.\n",
        "\n",
        "* The question is, how many values do we need to sample to get a good approximation? It turns out that **the number of samples grows with the precision of the approximation, the rank, and the condition number of the input matrix.**\n",
        "\n",
        "* In practice, this estimation method is only faster than a direct calculation of the coefficients if (i) precision, rank, and condition number are small, and (ii) the input matrix has a colossal dimension.\n",
        "\n",
        "* (..)\n",
        "\n",
        "* In practice, it pays off to be slightly more sophisticated: we keep each number with likelihood proportional to its value, such that large numbers are kept with high probability and small numbers with low probability. This is the basis of a technique called [rejection sampling](https://en.m.wikipedia.org/wiki/Rejection_sampling), **which is employed in quantum-inspired algorithms to sample from the solution vectors**. Once the approximate singular value decomposition has been obtained and the coefficients have been estimated, it is possible to query any entry of the solution vector in sublinear time.\n",
        "\n",
        "https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n",
        "\n",
        "https://cstheory.stackexchange.com/questions/42338/list-of-quantum-inspired-algorithms\n",
        "\n",
        "https://github.com/XanaduAI/quantum-inspired-algorithms"
      ],
      "metadata": {
        "id": "-wa6Ap4QMXJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulated annealing (inspired) vs quantum annealing (with quantum tunneling):\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/quantum/optimization-overview-introduction\n"
      ],
      "metadata": {
        "id": "4E5NYRMXNrHa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDNzWru2eKpK"
      },
      "source": [
        "##### <font color=\"blue\">*Homology, Groups & Graphs*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Simplicial Homology*"
      ],
      "metadata": {
        "id": "uOwaDiWZDTxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantum-journal.org/views/qv-2023-01-26-70/"
      ],
      "metadata": {
        "id": "pRRqjqbIK3yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1416.png)\n"
      ],
      "metadata": {
        "id": "0l5UtGJTKpJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1417.png)\n"
      ],
      "metadata": {
        "id": "U13kdlWrKrds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1418.png)\n"
      ],
      "metadata": {
        "id": "wVu6RaMlKsPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1419.png)\n"
      ],
      "metadata": {
        "id": "ud88megeKtzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1420.png)\n"
      ],
      "metadata": {
        "id": "_hlWriNLKu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1421.png)\n"
      ],
      "metadata": {
        "id": "2b-HFVA1KwWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1422.png)\n"
      ],
      "metadata": {
        "id": "9m7jXEzgKxIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1423.png)\n"
      ],
      "metadata": {
        "id": "_mR6uRUtKx_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Free abelian groups: https://youtu.be/xPeeFp_Hd3A\n",
        "\n",
        "simplicial complexes: https://youtu.be/Uq4dTjHfLpI\n",
        "\n",
        "computing homology groups: https://youtu.be/YNBi4Ix3cY0\n",
        "\n",
        "computing more homology groups: https://youtu.be/l7QWg0UzBRA\n",
        "\n",
        "betti numbers: https://youtu.be/NgrIPPqYKjQ\n",
        "\n",
        "persistent homology: https://youtu.be/ktKCzMmDXDk\n",
        "\n",
        "first isomorphism theorem: https://youtu.be/2kmIHyD8zTk\n",
        "\n",
        "isomorphic graphs: https://youtu.be/EwV4Puk2coU\n",
        "\n",
        "cohomoly and forms: https://youtu.be/2ptFnIj71SM\n",
        "\n",
        "differential forms: https://youtu.be/CYz_s82JnY8\n",
        "\n",
        "covectors and one-forms: https://youtu.be/ziD8ewQjaf4"
      ],
      "metadata": {
        "id": "gNk02DYZ_HXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Homology explained via Graph and Group Theory**\n",
        "\n",
        "* Find the nullspace of the matrix, then we can find all the cycles (min 29)\n",
        "\n",
        "* The number of vectors in the spanning set of the null space of the reduced matrix representation would be the number of generating cycles\n",
        "\n",
        "* A [free abelian group](https://en.m.wikipedia.org/wiki/Free_abelian_group) is an abelian group (commutative) with a basis.\n",
        "\n",
        "Source: [An introduction to homology | Algebraic Topology | NJ Wildberger](https://www.youtube.com/watch?v=ShWdSNJeuOg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1391.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1392.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1394.png)\n",
        "\n",
        "Now adding a higher dimensional disc:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1395.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1396.png)\n",
        "\n",
        "Continue: https://www.youtube.com/watch?v=2wn10l9qbJI&list=PL6763F57A61FE6FE8&index=36&t=1362s\n",
        "\n",
        "Playlist: https://www.youtube.com/playlist?list=PL6763F57A61FE6FE8"
      ],
      "metadata": {
        "id": "QdlLIADIS-QI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeX5czO1VjRi"
      },
      "source": [
        "**Simplicial Homology (Application of Quotient Groups)**\n",
        "\n",
        "> <font color=\"blue\">**Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence**\n",
        "\n",
        "[Simplicial Homology: On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://m0nads.wordpress.com/tag/persistent-homology/)\n",
        "\n",
        "*Since Bn is a subgroup of Zn, we may form the quotient group Hn = Zn/Bn -> so Modulo (=Restwerte) ist dann die Dimension der L√∂cher (=invarianten)*\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_07.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_10.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_08.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_09.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_06.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.intmath.com/differential-equations/3-integrable-combinations.php"
      ],
      "metadata": {
        "id": "xWw52dxv_93x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple examples of homology:\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Graph_homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology"
      ],
      "metadata": {
        "id": "iP7S89gV1zb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In biology, [homology](https://en.m.wikipedia.org/wiki/Homology_(biology)) is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles **are all derived from the same ancestral tetrapod structure**.\n",
        "\n",
        "https://de.wiktionary.org/wiki/homolog\n",
        "\n",
        "Biologie: **von einer gemeinsamen Urform ableitbar**.\n",
        "\n",
        "Worttrennung: ho¬∑mo¬∑log, keine Steigerung\n",
        "\n",
        "Beispiel: Der Fl√ºgelbug des Vogels ist unserem Handgelenk homolog.\n",
        "\n",
        "https://de.wiktionary.org/wiki/Homologie\n",
        "\n",
        "Biologie: √úbereinstimmung von biologischen Strukturen (Organe, Makromolek√ºle et cetera) oder Verhaltensanteilen hinsichtlich ihrer Entwicklungsgeschichte und nicht in Hinsicht auf ihre Funktion"
      ],
      "metadata": {
        "id": "XvGbLiHsv5na"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FhQOJufFMx0"
      },
      "source": [
        "**Homology**\n",
        "\n",
        "* [Homology](https://en.m.wikipedia.org/wiki/Homology_(mathematics)) is a general way of **associating a sequence of algebraic objects, such as abelian groups (or modules), to other mathematical objects such as topological spaces.**\n",
        "\n",
        "* **Homology Groups**: Homology groups were originally defined in algebraic topology. Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry. See [Construction of homology groups](https://en.m.wikipedia.org/wiki/Homology_(mathematics)#Construction_of_homology_groups)\n",
        "\n",
        "* In homology we want to Linearize the equivalence relation! Homology and cohomology are linear theories (easier to compute, and methods of linear algebra applicable) (though in the process you loose a bit information\n",
        "\n",
        "\n",
        "* Homology counts components, holds, voids etc.\n",
        "\n",
        "* In topology we define something called homology for simplicial complexes.\n",
        "\n",
        "* **Homology of a simplicial complex is computable via linear algebra**.\n",
        "\n",
        "* Die Homologie ist ein mathematischer Ansatz, die Existenz von L√∂chern zu formalisieren.\n",
        "\n",
        "* Gewisse ‚Äûsehr feine‚Äú L√∂cher sind f√ºr die Homologie unsichtbar; hier kann u. U. auf die schwerer zu bestimmenden Homotopiegruppen zur√ºckgegriffen werden.\n",
        "\n",
        "1. define set of vertices V0 (=vector space V0 with basis given by the set of vertices) and set of edges V1\n",
        "\n",
        "2. linearize equivalence relation: instead of looking at ‚Äûtail of edge is equivalent to head of edge‚Äú, we consider the difference between these two\n",
        "\n",
        "  * **e $\\mapsto$ h(e) - t(e)** is the linear combination of two vertices, and hence an element of e zero! e wird abgebildet auf dem Element (h(e) - t(e))\n",
        "  * **h(e) $\\equiv$ t(e) mod (im d)** das heisst: h(e) ist identisch zu t(e) modulo dem Bild von d\n",
        "\n",
        "* The linear map d : V1 -> V0 sends an edge e to (h(e) - t(e))\n",
        "* e is an edge which is a basis element inside V1\n",
        "* So the image of d (difference) is just the supspace of V0 generated by these elements\n",
        "* Generate [equivalence relation](https://en.m.wikipedia.org/wiki/Equivalence_relation) (Homomorphiesatz?): impose reflexivity, symmetry and transivity.\n",
        "* So: to get the image of d you look at the subspace generated by e |‚Äî> h(e) - t(e)\n",
        "    * Closure under addition more or less corresponds to transitivity,\n",
        "    * closure under negation corresponds to symmetry, and\n",
        "    * closure under zero corresponds to reflexivity.\n",
        "    * So this precisely linearizes the equivalence relation from before.\n",
        "* Now we have one linear map. But we need two linear maps which composes 0 to get homology or cohomology. There are two ways you can get a composite to get 0 very easily.\n",
        "    * Either start from zero and map to V1 and then go to V0, or\n",
        "    * start at V1 and use d to get to V0 and then go to 0 with the zero map there.\n",
        "* Modular the image of this map which is zero\n",
        "\n",
        "* https://ncatlab.org/nlab/show/homology\n",
        "\n",
        "**Homological Algebra**\n",
        "\n",
        "* [Homological algebra](https://en.m.wikipedia.org/wiki/Homological_algebra) is the study of homological functors and the intricate algebraic structures that they entail; its development was closely intertwined with the emergence of category theory. A central concept is that of chain complexes, which can be studied through both their homology and cohomology.\n",
        "\n",
        "*A diagram used in the snake lemma, a basic result in homological algebra:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Snake_lemma_origin.svg/375px-Snake_lemma_origin.svg.png)\n",
        "\n",
        "\n",
        "* Why using Homological Algebra: **translate a problem of interest into a sequence of ‚Äûhigher‚Äú (=homological algebra) linear algebra problems**\n",
        "\n",
        "* homological algebra is the **study of homological functors** and the intricate algebraic structures that they entail.\n",
        "\n",
        "* One quite useful and ubiquitous concept in mathematics is that of **chain complexes**, which can be studied through both their homology and cohomology.\n",
        "\n",
        "* Homological algebra affords the means to **extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces**, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.\n",
        "\n",
        "**Simplicial Homology**\n",
        "\n",
        "* Die [Simpliziale Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie) ist in der Algebraischen Topologie, einem Teilgebiet der Mathematik, eine Methode, die einem beliebigen Simplizialkomplex **eine Folge [abelscher Gruppen](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe) zuordnet**.\n",
        "\n",
        "* Anschaulich gesprochen z√§hlt sie die L√∂cher unterschiedlicher Dimension des zugrunde liegenden Raumes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph homology**\n",
        "\n",
        "In algebraic topology and graph theory, [graph homology](https://en.m.wikipedia.org/wiki/Graph_homology) describes the homology groups of a graph, where the graph is considered as a topological space. It formalizes the idea of the number of \"holes\" in the graph. It is a special case of a simplicial homology, as a graph is a special case of a simplicial complex. Since a finite graph is a 1-complex (i.e., its 'faces' are the vertices - which are 0-dimensional, and the edges - which are 1-dimensional), the only non-trivial homology groups are the 0-th group and the 1-th group.\n",
        "\n",
        "The 1st homology group\n",
        "\n",
        "Similarly, $\\partial _{1}$ maps any cycle in C1 to the zero element of C0. In other words, the set of cycles in C1 **generates the null space (the kernel)** of $\\partial _{1}$.\n",
        "\n",
        "https://www.youtube.com/watch?v=ShWdSNJeuOg&t=240s"
      ],
      "metadata": {
        "id": "nKnPM0x-6BG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Graph Theory**\n",
        "\n",
        "In mathematics, [topological graph theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory) is a branch of graph theory. It studies the embedding of graphs in surfaces, spatial embeddings of graphs, and graphs as topological spaces.[1] It also studies immersions of graphs.\n",
        "\n",
        "To an undirected graph we may associate an abstract simplicial complex C with a single-element set per vertex and a two-element set per edge.[2] The geometric realization |C| of the complex consists of a copy of the unit interval [0,1] per edge, with the endpoints of these intervals glued together at vertices. In this view, embeddings of graphs into a surface or as subdivisions of other graphs are both instances of topological embedding, homeomorphism of graphs is just the specialization of topological homeomorphism, the notion of a connected graph coincides with topological connectedness, and a connected graph is a tree if and only if its fundamental group is trivial.\n",
        "\n",
        "Other simplicial complexes associated with graphs include the **Whitney complex or clique complex**, with a set per clique of the graph, and the matching complex, with a set per matching of the graph (equivalently, the clique complex of the complement of the line graph). The matching complex of a complete bipartite graph is called a chessboard complex, as it can be also described as the complex of sets of nonattacking rooks on a chessboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvWc8p5p4Qlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph (topology)**\n",
        "\n",
        "In topology, a branch of mathematics, a [graph is a topological space](https://en.m.wikipedia.org/wiki/Graph_(topology)) which arises from a usual graph\n",
        "G\n",
        "=\n",
        "(\n",
        "E\n",
        ",\n",
        "V\n",
        ")\n",
        "{\\displaystyle G=(E,V)} by replacing vertices by points and each edge\n",
        "e\n",
        "=\n",
        "x\n",
        "y\n",
        "‚àà\n",
        "E\n",
        "{\\displaystyle e=xy\\in E} by a copy of the unit interval\n",
        "I\n",
        "=\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "{\\displaystyle I=[0,1]}, where\n",
        "0\n",
        "{\\displaystyle 0} is identified with the point associated to\n",
        "x\n",
        "x and\n",
        "1\n",
        "1 with the point associated to\n",
        "y\n",
        "y. That is, as topological spaces, graphs are exactly the simplicial 1-complexes and also exactly the one-dimensional **CW complexes**."
      ],
      "metadata": {
        "id": "40IB_N2Q6Mk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **path** is a walk with no repeated vertices\n",
        "\n",
        "A closed path is a **cycle**."
      ],
      "metadata": {
        "id": "kSH33jQVs3bM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLoLSdtySVs"
      },
      "source": [
        "**Cycles (ker), boundaries (img) and chains**\n",
        "\n",
        "* Cycles (and its Kern): A cycle is a closed submanifold,\n",
        "\n",
        "* boundary is a cycle which is also the boundary of a submanifold.\n",
        "\n",
        "* Boundary operator on [chains](https://en.m.wikipedia.org/wiki/Chain_(algebraic_topology)) (and its image): The boundary of a chain is the linear combination of boundaries of the simplices in the chain. The boundary of a k-chain is a (k‚àí1)-chain. Note that the boundary of a simplex is not a simplex, but a chain with coefficients 1 or ‚àí1 ‚Äì thus chains are the closure of simplices under the boundary operator.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Chainline.svg/320px-Chainline.svg.png)\n",
        "\n",
        "* *The boundary of a polygonal curve is a linear combination of its nodes; in this case, some linear combination of A1 through A6. Assuming the segments all are oriented left-to-right (in increasing order from Ak to Ak+1), the boundary is A6 ‚àí A1.*\n",
        "\n",
        "* If the 1 -chain $c=t_{1}+t_{2}+t_{3}$ is a path from point $v_{1}$ to point $v_{4},$ where $t_{1}=\\left[v_{1}, v_{2}\\right], t_{2}=\\left[v_{2}, v_{3}\\right]$ and $t_{3}=\\left[v_{3}, v_{4}\\right]$ are its\n",
        "constituent 1 -simplices, then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\partial_{1} c &=\\partial_{1}\\left(t_{1}+t_{2}+t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(t_{1}\\right)+\\partial_{1}\\left(t_{2}\\right)+\\partial_{1}\\left(t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(\\left[v_{1}, v_{2}\\right]\\right)+\\partial_{1}\\left(\\left[v_{2}, v_{3}\\right]\\right)+\\partial_{1}\\left(\\left[v_{3}, v_{4}\\right]\\right) \\\\\n",
        "&=\\left(\\left[v_{2}\\right]-\\left[v_{1}\\right]\\right)+\\left(\\left[v_{3}\\right]-\\left[v_{2}\\right]\\right)+\\left(\\left[v_{4}\\right]-\\left[v_{3}\\right]\\right) \\\\\n",
        "&=\\left[v_{4}\\right]-\\left[v_{1}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Closed_polygonal_line.svg/320px-Closed_polygonal_line.svg.png)\n",
        "\n",
        "* *A closed polygonal curve, assuming consistent orientation, has null boundary. (deswegen f√ºhrt der boundary operator Œ¥1 alle Werte immer in Null, zumindest bei geschlossenen objekten)*\n",
        "\n",
        "* Example 2: The boundary of the triangle is a formal sum of its edges with signs arranged to make the traversal of the boundary counterclockwise.\n",
        "\n",
        "   * Cycle: **A chain is called a cycle when its boundary is zero**.\n",
        "\n",
        "   * Boundary: A chain that is the boundary of another chain is called a boundary.\n",
        "\n",
        "   * **Boundaries are cycles**, so chains form a chain complex, whose homology groups (cycles modulo boundaries) are called simplicial homology groups.\n",
        "\n",
        "* Example 3: A 0-cycle is a linear combination of points such that the sum of all the coefficients is 0. Thus, the 0-homology group measures the number of path connected components of the space.\n",
        "\n",
        "* Example 4: The plane punctured at the origin has nontrivial 1-homology group **since the unit circle is a cycle, but not a boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazhAwJAFmL-"
      },
      "source": [
        "**Chain Complex**\n",
        "\n",
        "A chain complex\n",
        "(\n",
        "A\n",
        "‚àô\n",
        ",\n",
        "d\n",
        "‚àô\n",
        ")\n",
        "(A_{\\bullet },d_{\\bullet }) is a sequence of abelian groups or modules ..., A0, A1, A2, A3, A4, ... connected by homomorphisms (called boundary operators or differentials)\n",
        "\n",
        "> **[Chain Complex](https://en.m.wikipedia.org/wiki/Chain_complex) is a sequence of homomorphism of abelian groups**\n",
        "\n",
        "A chain complex $V$. is a sequence $\\left\\{V_{n}\\right\\}_{n \\in \\mathbb{Z}}$ of abelian groups or modules (for instance yector spaces) or similar equipped with linear maps $\\left\\{d_{n}: V_{n+1} \\rightarrow V_{n}\\right\\}$ such that $d^{2}=0,$ i.e. the composite of two consecutive such maps is the zero morphism $d_{n} \\circ d_{n+1}=0$\n",
        "\n",
        "* https://ncatlab.org/nlab/show/chain+complex\n",
        "\n",
        "* https://ncatlab.org/nlab/show/zero+morphism\n",
        "\n",
        "* https://m0nads.wordpress.com/tag/persistent-homology/\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Simplicial_homology_-_exactness_of_boundary_maps.svg/384px-Simplicial_homology_-_exactness_of_boundary_maps.svg.png)\n",
        "\n",
        "* The boundary of a boundary of a 2-simplex (left) and the boundary of a 1-chain (right) are taken.\n",
        "\n",
        "* Both are 0, being sums in which both the positive and negative of a 0-simplex occur once. The boundary of a boundary is always 0.\n",
        "\n",
        "* A nontrivial cycle is something that closes up like the boundary of a simplex, in that its boundary sums to 0, but which isn't actually the boundary of a simplex or chain.\n",
        "\n",
        "* Because trivial 1-cycles are equivalent to 0 in H1, the 1-cycle at right-middle is homologous to its sum with the boundary of the 2-simplex at left.\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_04.png)\n",
        "\n",
        "\n",
        "[Vergleich mit paper von Krishna 2017]\n",
        "\n",
        "5 kongruent 11 und 17 etc. modulo 3, weil (3 * 1) + 2 = 5, und (3 * 3) + 2 = 11. Reste m√ºssen identisch sein.\n",
        "* **Zp (Kern)**: 5, 11, 17 etc. (alle Objekte aus einer Filtration zB in Cp, die kongruent zueinander sind, **weil deren Differenz ein ganzzahliges Vielfaches von Bp (modulo) ist)**.\n",
        "* **Hp (Hom)**: 2 (= das Loch)\n",
        "* **Bp (Img)**: 3 (modulo)\n",
        "- drei Basiselemente: 0, 1 und 2 (?)\n",
        "\n",
        "Before:\n",
        "\n",
        "5 kongruent 11 modulo 3, weil 3 * 1 + 2 = 5, und 3 * 3 + 2 + 11, Reste m√ºssen identisch sein, sowie 17 etc.\n",
        "\n",
        "- drei Basiselemente: 0, 1 und 2\n",
        "- 3 ist wie Bp, also Image (element des vektorraums, und ist linear, weil zB multiplizier mit Skala bleibt im vektorraum)\n",
        "- Rest ware Hp, also 2 (das Loch)\n",
        "- 5 und 11 und 17 sind Zp (Kern)\n",
        "- Berate alle Objekte aus der Filtration\n",
        "- Ein element aus Zp was plus ein Element aus Bp\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDP2ORM8EGQ"
      },
      "source": [
        "**Exact Sequences**\n",
        "\n",
        "An [exact sequence](https://en.m.wikipedia.org/wiki/Exact_sequence) is a sequence of morphisms between objects (for example, groups, rings, modules, and, more generally, objects of an abelian category) such that the image of one morphism equals the kernel of the next.\n",
        "\n",
        "In the context of group theory, a sequence\n",
        "\n",
        "> $G_{0} \\stackrel{f_{1}}{\\longrightarrow} G_{1} \\stackrel{f_{2}}{\\longrightarrow} G_{2} \\stackrel{f_{3}}{\\longrightarrow} \\cdots \\stackrel{f_{n}}{\\longrightarrow} G_{n}$\n",
        "\n",
        "of groups and group homomorphisms is called exact if the image of each homomorphism is equal to the kernel of the next:\n",
        "\n",
        "> $\\operatorname{im}\\left(f_{k}\\right)=\\operatorname{ker}\\left(f_{k+1}\\right)$\n",
        "\n",
        "The sequence of groups and homomorphisms may be either finite or infinite.\n",
        "\n",
        "> **Every exact sequence is a [chain complex](https://en.m.wikipedia.org/wiki/Chain_complex)**\n",
        "\n",
        "**Vergleich mit exact sequence: im‚Å°( f k ) = ker( f k + 1 ) bzw. Zp = Bp mit Hp = 0**\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Illustration_of_an_Exact_Sequence_of_Groups.svg/640px-Illustration_of_an_Exact_Sequence_of_Groups.svg.png)\n",
        "\n",
        "![ccc](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/640px-KerIm_2015Joz_L2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti Numbers (Betti Sequence)**\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Topologische_Invariante\n",
        "\n",
        "* The formal definition of homology uses the language of group theory. (The equivalence class of loops surrounding a hole have a group structure.) Persistent homology examines these homological features from a multiscale perspective.\n",
        "\n",
        "* Persistent homology is a powerful tool to compute, study and encode efficiently multiscale topological features of nested families of simplicial complexes and topological spaces.\n",
        "\n",
        "* It does not only provide efficient algorithms to compute the Betti numbers of each complex in the considered families, as required for homology inference in the previous section, but also encodes the evolution of the homology groups of the nested complexes across the scales.\n",
        "Im Bereich der algebraischen Topologie sind die Homologien beziehungsweise die **Homologiegruppen Invarianten eines topologischen Raums**.\n",
        "\n",
        "* **Simplicial homology groups and Betti numbers are topological invariants**: if K,K‚Ä≤ are two simplicial complexes whose geometric realizations are homotopy equivalent, then their homology groups are isomorphic and their Betti numbers are the same.\n",
        "\n",
        "* Persistent Homology, a recent breakthrough idea, extends Homology theory to work across a range of parameterized Simplicial Complexes, like the one arising from a point cloud, instead of just a single, isolated complex.\n",
        "It looks for topological invariants across various scales of a topological manifold.\n",
        "\n",
        "* ‚ÄûBut there is a problem. Betti numbers are computationally demanding to calculate, ‚Äúquickly overwhelming even the most powerful classical computers, even for not-so-large data sets,‚Äù\n",
        "\n",
        "* https://www.technologyreview.com/s/610138/a-small-scale-demonstration-shows-how-quantum-computing-could-revolutionize-data-analysis/\n",
        "\n",
        "* In algebraic topology, the Betti numbers are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* **The nth Betti number represents the rank of the nth homology group, denoted Hn, which tells us the maximum amount of cuts that must be made before separating a surface into two pieces or 0-cycles, 1-cycles, etc.[1] These numbers are used today in fields such as simplicial homology, computer science, digital images, etc**.\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Betti_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Homology_(mathematics)\n",
        "\n",
        "* Betti numbers is a compact method to present this Homology Groups by investigating the properties of the topological spaces.\n",
        "\n",
        "* It distinguishes topological spaces according to the connectivity of n-dimensional simplicial complexes. The nth Betti number represents the rank of the nth homology group, denoted as Hn.\n",
        "\n",
        "* Informally, the nth Betti number refers to the number of n-dimensional holes on a topological surface.\n",
        "\n",
        "* Figure 9 shows that the first three Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2- dimensional simplicial complexes: Œ≤0 is the number of connected components Œ≤1 is the number of holes(one-dimensional) and Œ≤2 is the number of two-dimensional \"voids\".\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_437.png)\n",
        "\n",
        "Die Bettizahlen geben an, wie viele k-dimensionale nicht zusammenh√§ngende Fl√§chen der entsprechende topologische Raum hat. Die ersten drei Bettizahlen besagen anschaulich also:\n",
        "\n",
        "* b0 ist die Anzahl der Wegzusammenhangskomponenten* (connected components)\n",
        "\n",
        "* b1 ist die Anzahl der ‚Äûzweidimensionalen L√∂cher‚Äú.\n",
        "\n",
        "* b2 ist die Anzahl der dreidimensionalen Hohlr√§ume.\n",
        "\n",
        "Der unten abgebildete Torus (gemeint ist Oberfl√§che) besteht aus einer Zusammenhangskomponente, hat zwei ‚Äûzweidimensionale L√∂cher‚Äú, zum einen das in der Mitte, zum andern das im Inneren des Torus, und hat einen dreidimensionalen Hohlraum. Die Bettizahlen des Torus sind daher 1, 2, 1, die weiteren Bettizahlen sind 0.\n",
        "\n",
        "Ist der zu betrachtende topologische Raum jedoch keine orientierbare kompakte Mannigfaltigkeit, so versagt diese Anschauung allerdings schon.\n"
      ],
      "metadata": {
        "id": "8v0mv3YqBQGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti number & Rank of a group**\n",
        "\n",
        "* In algebraic topology, the [Betti numbers](https://en.m.wikipedia.org/wiki/Betti_number) are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* The nth Betti number represents the [rank (of a group)](https://en.m.wikipedia.org/wiki/Rank_of_a_group) of the nth [homology group](https://en.m.wikipedia.org/wiki/Homology_(mathematics)), denoted Hn, **which tells us the maximum number of cuts that can be made before separating a surface into two pieces or 0-cycles, 1-cycles**, etc.\n",
        "\n",
        "* The first few Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2-dimensional simplicial complexes:\n",
        "\n",
        "  * b0 is the number of (connected) components;\n",
        "  * b1 is the number of one-dimensional or \"circular\" holes;\n",
        "  * b2 is the number of two-dimensional \"voids\" or \"cavities\".\n",
        "\n",
        "* for example, a torus has one connected surface component so b0 = 1, two \"circular\" holes (one equatorial and one meridional) so b1 = 2, and a single cavity enclosed within the surface so b2 = 1.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/5/54/Torus_cycles.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvR1-z919rUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oP2QkFIRXLX"
      },
      "source": [
        "**Homology Group & Betti Number**\n",
        "\n",
        "Let $\\sigma=\\left(v_{0}, \\ldots, v_{k}\\right)$ be an oriented $k$ -simplex, viewed as a basis element of $C_{k}$. The boundary operator\n",
        "\n",
        "$\n",
        "\\partial_{k}: C_{k} \\rightarrow C_{k-1}\n",
        "$\n",
        "\n",
        "is the homomorphism defined by:\n",
        "\n",
        "$\n",
        "\\partial_{k}(\\sigma)=\\sum_{i=0}^{k}(-1)^{i}\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "where the oriented simplex\n",
        "\n",
        "$\n",
        "\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "is the $I^{\\text {th }}$ face of $\\sigma,$ obtained by deleting its $i^{\\text {th }}$ vertex.\n",
        "In $C_{k},$ elements of the subgroup\n",
        "\n",
        "$\n",
        "Z_{k}:=\\operatorname{ker} \\partial_{k}\n",
        "$\n",
        "\n",
        "are referred to as cycles, and the subgroup\n",
        "\n",
        "$\n",
        "B_{k}:=\\operatorname{im} \\partial_{k+1}\n",
        "$\n",
        "\n",
        "is said to consist of boundaries.\n",
        "\n",
        "The $k^{\\text {th }}$ homology group $H_{k}$ of $S$ is defined to be the [quotient abelian group](https://en.m.wikipedia.org/wiki/Quotient_group)\n",
        "\n",
        "$\n",
        "H_{k}(S)=Z_{k} / B_{k}\n",
        "$\n",
        "\n",
        "* It follows that the **homology group $H_{k}(S)$ is nonzero exactly when there are $k-$\n",
        "cycles on $S$ which are not boundaries**. In a sense, this means that there are $k$ -\n",
        "dimensional holes in the complex.\n",
        "\n",
        "* For example, consider the complex $S$ obtained by gluing two triangles (with no interior) along one edge, shown in the image. The edges of each triangle can be oriented so as to form a cycle. These two cycles are by construction not boundaries (since every 2 -chain is zero).\n",
        "\n",
        "* One can compute that the homology group $\\mathrm{H}_{1}(\\mathrm{S})$ is isomorphic to $\\mathrm{Z}^{2}$, with a basis given by the two cycles mentioned. This makes precise the informal idea that $S$ has two \"1-\n",
        "dimensional holes\".\n",
        "\n",
        "* Holes can be of different dimensions. The rank of the $k$ th homology group, the\n",
        "number\n",
        "\n",
        "$\n",
        "\\beta_{k}=\\operatorname{rank}\\left(H_{k}(S)\\right)\n",
        "$\n",
        "\n",
        "**is called the $k$ th Betti number of $S$. It gives a measure of the number of $k$ - dimensional holes in $S$.**\n",
        "\n",
        "**A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is \"not there\".**\n",
        "\n",
        "* An homology class Hn (which represents a hole) is an [equivalence class](https://en.m.wikipedia.org/wiki/Equivalence_class) of\n",
        "    * [cycles](https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles) Ker(Œ¥) modulo boundaries Im(Œ¥) bzw.\n",
        "    * h(e) $\\equiv$ t(e) mod (im d)\n",
        "\n",
        "* Ker(Œ¥) kann beschrieben werden: h(e) $\\equiv$ t(e) bzw. e $\\mapsto$ h(e) - t(e) or: ‚àÇn‚àí1 ‚ó¶ ‚àÇn = 0\n",
        "\n",
        "* This is the linear combination of two vertices, and hence an element of e zero\n",
        "\n",
        "* Get the image of d bzw. Im(Œ¥) when you look at the subspace generated by e $\\mapsto$ h(e) - t(e)\n",
        "\n",
        "*  Im(Œ¥): boundaries?\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles\n",
        "\n",
        "https://ncatlab.org/nlab/show/boundary+of+a+simplex\n",
        "\n",
        "https://ncatlab.org/nlab/show/chain+map\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental group**\n",
        "\n",
        "In the mathematical field of algebraic topology, the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) of a topological space is the group of the equivalence classes under homotopy of the loops contained in the space. It records information about the basic shape, or holes, of the topological space.\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))\n",
        "\n",
        "If $X$ is a graph and ${\\displaystyle T\\subseteq X}$ a maximal tree, then the fundamental group $\\pi _{1}(X)$ equals the free group generated by elements ${\\displaystyle (f_{\\alpha })_{\\alpha \\in A}}$, where the ${\\displaystyle \\{f_{\\alpha }\\}}$ correspond bijectively to the edges of ${\\displaystyle X\\setminus T}$; in fact, $X$ is homotopy equivalent to a wedge sum of circles."
      ],
      "metadata": {
        "id": "7WtsJdDE9oVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Laplacian*"
      ],
      "metadata": {
        "id": "f4lHqJyMa24i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=M9F7zT9Gg-k\n",
        "\n",
        "https://www.youtube.com/watch?v=skaXrTtQyJA\n",
        "\n",
        "https://www.youtube.com/watch?v=2h12m-3zQ0M\n",
        "\n",
        "https://www.youtube.com/watch?v=-Afa1WI3iug"
      ],
      "metadata": {
        "id": "X13DUDyLt9db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Graph Theory**\n",
        "\n",
        "* In [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory), the [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) of a [graph](https://en.m.wikipedia.org/wiki/Graph_(discrete_mathematics)) is the characteristic polynomial of its [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix).\n",
        "\n",
        "* Spectral graph theory is the study of the properties of a graph in relationship to the **characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph**, such as its adjacency matrix or [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix) = Edge Matrix - Adjacency Matrix\n",
        "\n",
        "* One of the reasons that the eigenvalues of matrices have meaning is that they arise as the solution to natural optimization problems\n",
        "\n",
        "> Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen. (The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**.)\n",
        "\n",
        "\n",
        "\n",
        "http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf\n",
        "\n",
        "https://www.cs.cmu.edu/~venkatg/teaching/15252-sp20/notes/Spectral-graph-theory.pdf"
      ],
      "metadata": {
        "id": "Zxze3ZZQpmmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian matrix:**\n",
        "\n",
        "> $L(G) = D - A = B^T B \\quad$ for some matrix $B$\n",
        "\n",
        "* $L$ = [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix)\n",
        "* $D$ = [degree matrix](https://en.m.wikipedia.org/wiki/Degree_matrix), entries contain degrees of each vertex\n",
        "* $A$ = [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix): 1 if edge, 0 if no edge\n",
        "* $B$ = the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "**Description of Laplacian matrix**\n",
        "* Laplacian matrix is an operator mapping from graph to the real numbers\n",
        "* Diagonals have degrees of vertices\n",
        "* Off-diagonal contain edges marked as -1\n",
        "* Contains all information about graph (you can recreate the graph just from laplacian).\n",
        "* And you can nicely apply linear algebra to analyse the graph\n",
        "\n",
        "**Properties of Laplacian matrix:**\n",
        "1. L(G) is symmetric\n",
        "2. L(G) has:\n",
        "  * real-valued, non-negative eigenvalues and\n",
        "  * real-valued, orthogonal eigenvectors\n",
        "* Is always positive-semidefinite matrix\n",
        "* $\\lambda$ = 0 is always an Eigenvalue with Eigenvector 1 (because all rows add to zero and multiplied by vector with 1‚Äòs leads to zero)\n"
      ],
      "metadata": {
        "id": "yk02m201n6n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of calculating Eigenvalues and Eigenvectors of Laplacian matrix**\n",
        "* second smallest Eigenvalue (**spectral gap**) is a solution (i.e. minimal energy) for many problems, if computable.\n",
        "* You can use the spectrum of the Laplacian matrix to solve\n",
        "  * **optimization problems** (i.e. shortest path, Hamiltonain circuit), like traveling salesman problem\n",
        "  * **graph partitioning problem** (optimal cut). reveal sparse connections and where there are more clusters, find bottlenecks (in social media it relates to communities, in networks it's a critical point\n",
        "  * both problems are np-hard\n",
        "* In physics: **Fundamental modes (harmonics)** are given by the Eigenvectors of the graph laplacian [Spectral Partitioning Part 3 Algebraic Connectivity](https://www.youtube.com/watch?v=Vng9lkibGEE)"
      ],
      "metadata": {
        "id": "9MumAIvqp8cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 1: Graph laplacian spectrum, meaning it's eigenvalues, tells us something about the underlying connectivity of the graph**\n",
        "* = Graph has k connected components if and only if the k-smallest Eigenvalues are identically zero: $\\lambda_0$ = $\\lambda_1$ = .. $\\lambda_{k-1}$ = 0 (Fiedler 1973).\n",
        "* Spectrum of L(G) $\\rightleftharpoons$ Connectivity of G\n",
        "* = The number of (connected) components in the graph is the dimension of the nullspace (=kernel / set of solutions) of the Laplacian and the algebraic multiplicity (=how many times the same Eigenvalue) of the 0 eigenvalue.\n",
        "* Laplacian: its Eigenvectors, which do not rotate, you can see an ellipse like section of space they draw.\n",
        "* **Spectrograph theory asked questions about this ellipse to determine graph behavior, what the fastest way from dot-to-dot is**, how many pathways there are dot to dot [Source](https://www.youtube.com/watch?v=njatNunHC_o)\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 √ó 3)-Matrix √ºber Extrempunkte auf einem **Ellipsoid** (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ √ºber bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)"
      ],
      "metadata": {
        "id": "cZ4ROEDbn9zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 2: $L(G)$ contains $B^T B$ for some matrix $B$**\n",
        "* the ‚Äûdirected‚Äú node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "> $\\min _{y \\in \\Re^{\\prime \\prime}} f(y)=\\sum_{(i, j) \\in E^{\\prime}}\\left(y_i-y_j\\right)^2=y^T L y$\n",
        "\n",
        "* [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient) with Rayleigh theorem:\n",
        "  * $\\lambda_2=\\min f(y)$ : The minimum value of $f(y)$ is given by the $2^{\\text {nd }}$ smallest eigenvalue $\\lambda_2$ of the Laplacian matrix $\\boldsymbol{L}$\n",
        "  * $\\mathrm{x}=\\arg \\min _{\\mathrm{y}} \\boldsymbol{f}(\\boldsymbol{y})$ : The optimal solution for $y$ is given by the corresponding eigenvector $\\boldsymbol{x}$, referred as the [Fiedler vector (Algebraic_connectivity)](https://en.m.wikipedia.org/wiki/Algebraic_connectivity)\n",
        "\n",
        "* see how to create Laplacian matrix from incidence matrix: [Spectral Partitioning, Part 1 The Graph Laplacian](https://www.youtube.com/watch?v=rVnOANM0oJE)\n",
        "* B captures relationship between different nodes and edges, rows are nodes and columns are edges of G (pair start-sink)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1390.png)\n",
        "\n",
        "Source: [(Lecture 14) Graph Laplacians](https://www.youtube.com/watch?v=M9F7zT9Gg-kI)\n",
        "\n",
        "**How can you use this property? - The number of cut edges equals $\\frac{1}{4} x^T L(G) x$ under specific constraints**.\n",
        "  * Means: if you want to minimize edge cuts, you should try minimizing this product (it‚Äôs a combinatorial optimization problem).\n",
        "  * Cut edges: find minimal number of connections, bottleneck, borders between two clusters.\n",
        "  * This problem is however np complete. You need to relax sum contraints.\n",
        "\n",
        "*If you relax it you can combine it with the [Courant-Fisher minimax theorem (Satz von Courant-Fischer)](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer):*\n",
        "* if we allowed to use any vector y, where y is normalized in a certain way, and it‚Äôs elements sum to 0, then the vector y that minimizes this quantity is actually q1.\n",
        "* And q1 is the Eigenvector corresponding to the second smallest Eigenvalue.\n",
        "* And in fact the minimum value simplifies to something that is proportional to that eigenvalue.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1388.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1387.png)"
      ],
      "metadata": {
        "id": "Nw6nUk4bb4ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application example: Problem statement for using the Laplacian matrix of a graph:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1384.png)\n",
        "\n",
        "Source: [Lecture 30 ‚Äî The Graph Laplacian Matrix (Advanced) | Stanford University](https://www.youtube.com/watch?v=FRZvgNvALJ4)\n",
        "\n",
        "\n",
        "Sum over all neighbouring edges:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1385.png)\n",
        "\n",
        "\n",
        "**Solution siehe** [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), um den zweitkleinsten Eigenwert der Laplacian matrix zu finden (spectral gap):\n",
        "\n",
        "> $(R_{A}(x)=) \\quad \\lambda_2 = {min \\frac {x^{T} M x}{x^{T}x}}$ with matrix $M$ being the Laplacian matrix $L$\n",
        "\n",
        "Meaning of (min $x^T L x$): I take the label of one endpoint edge, subtract the value of the other endpoint of an edge, then square it up, and sum this over all the edges. (see also in green on the bottom why quadratic forms are relevant here!)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1381.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1382.png)\n",
        "\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1383.png)\n",
        "\n",
        "Source: [Lecture 33 ‚Äî Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "> **Note that the largest eigenvalue of the adjacency matrix corresponds to the smallest eigenvalue of the Laplacian.** But: Where the smallest eigenvector of the Laplacian is a constant vector, the largest eigenvector of an adjacency matrix, called the Perron vector, need not be. The Perron-Frobenius theory tells us that the largest eigenvector of an adjacency matrix is non-negative, and that its value is an upper bound on the absolute value of the smallest eigenvalue. These are equal precisely when the graph is bipartite.\n"
      ],
      "metadata": {
        "id": "CYNJpwY7BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Graphs*"
      ],
      "metadata": {
        "id": "8yH2OFqH-37c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Quantum Application with Graphs: https://medium.com/quandela/exploring-graph-problems-with-single-photons-and-linear-optics-4f3d5848add8"
      ],
      "metadata": {
        "id": "Yye9sR8Zq-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph & Graph Theory**\n",
        "\n",
        "* In [discrete mathematics](https://en.m.wikipedia.org/wiki/Discrete_mathematics), and more specifically in [graph theory](https://en.m.wikipedia.org/wiki/Graph_theory), a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\".\n",
        "\n",
        "* The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called link or line).\n"
      ],
      "metadata": {
        "id": "b-7FPIb42kSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Graph Theory**\n",
        "\n",
        "* [Algebraic graph theory](https://en.m.wikipedia.org/wiki/Algebraic_graph_theory) is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches.\n",
        "\n",
        "* There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.\n",
        "\n",
        "* **Using linear algebra**:\n",
        "\n",
        "  * The first branch of algebraic graph theory involves the study of graphs in connection with linear algebra. Especially, it studies the [spectrum (Eigendecomposition of a matrix)](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the adjacency matrix, or the Laplacian matrix of a graph - this part of algebraic graph theory is also called [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory).\n",
        "\n",
        "  * **Several theorems relate properties of the spectrum to other [graph properties (invariant)](https://en.m.wikipedia.org/wiki/Graph_property).** As a simple example, a connected graph with diameter D will have at least D+1 distinct values in its spectrum. Aspects of graph spectra have been used in analysing the synchronizability of networks.\n",
        "\n",
        "* Video: [Daniel Spielman ‚ÄúMiracles of Algebraic Graph Theory‚Äù](https://www.youtube.com/watch?v=CDMQR422LGM)\n",
        "\n",
        "* Video: [The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry and Physics](https://www.youtube.com/watch?v=8XJes6XFjxM)"
      ],
      "metadata": {
        "id": "sg75zioGELCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Types of Graphs*"
      ],
      "metadata": {
        "id": "9ELeT7od--k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete graph**\n",
        "\n",
        "* In graph theory, a [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge.\n",
        "\n",
        "* A complete digraph is a directed graph in which every pair of distinct vertices is connected by a pair of unique edges (one in each direction).\n",
        "\n",
        "*K7, a complete graph with 7 vertices with $n$ vertices and ${\\displaystyle \\textstyle {\\frac {n(n-1)}{2}}}$ edges*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Complete_graph_K7.svg/245px-Complete_graph_K7.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "suoc9GoR3FCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete bipartite graph**\n",
        "\n",
        "* [Complete bipartite graph](https://en.m.wikipedia.org/wiki/Complete_bipartite_graph) (or biclique), a special [bipartite graph](https://en.m.wikipedia.org/wiki/Bipartite_graph) where every vertex on one side of the bipartition is connected to every vertex on the other side\n",
        "\n",
        "* A complete bipartite graph with m = 5 and n = 3:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Biclique_K_3_5.svg/320px-Biclique_K_3_5.svg.png)\n",
        "\n",
        "Bipartite Graph https://www.youtube.com/watch?v=HqlUbSA9cEY\n",
        "\n"
      ],
      "metadata": {
        "id": "p0YwqBrL3qCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directed vs Undirected Graphs**\n",
        "\n",
        "The applications for directed graphs are many and varied. They can be used to analyze electrical circuits, develop project schedules, find shortest routes, analyze social relationships, and construct models for the analysis and solution of many other problems.\n",
        "\n",
        "https://faculty.cs.niu.edu/~freedman/241/241notes/241graph.htm\n",
        "\n",
        "\n",
        "Undirected graphs are more restrictive kinds of graphs. They represent only whether or not a relationship exists between two vertices. They don't however represent a distinction between subject and object in that relationship. One type of graph can sometimes be used to approximate the other.\n",
        "\n",
        "https://www.baeldung.com/cs/graphs-directed-vs-undirected-graph"
      ],
      "metadata": {
        "id": "V0ExwcX52H9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hamiltonian path**\n",
        "\n",
        "A [Hamiltonian path](https://en.m.wikipedia.org/wiki/Hamiltonian_path) (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle around a network of six vertices:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/b/be/Hamiltonian.png)"
      ],
      "metadata": {
        "id": "E26jLergoS4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Graph Properties*"
      ],
      "metadata": {
        "id": "oAiyy65E-8Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Length** is the number of edges in a graph.\n",
        "\n",
        "A **closed walk** occurs when x = y.\n",
        "\n",
        "A **trail** is a walk with no repeated edges.\n",
        "\n",
        "A closed trail is a **circuit**.\n",
        "\n",
        "A **path** is a walk with no repeated vertices (A path is a sequence of distinct edges you can follow through the graph)\n",
        "\n",
        "A closed path is a **cycle** (and circuit)\n",
        "\n",
        "A **simple graph** is loop-free, undirected and has no multiple edges.\n",
        "\n",
        "[INTRODUCTION to GRAPH THEORY - DISCRETE MATHEMATICS](https://www.youtube.com/watch?v=HkNdNpKUByM)\n",
        "\n",
        "[Introduction to Graph Theory: A Computer Science Perspective](https://www.youtube.com/watch?v=LFKZLXVO-Dg)\n"
      ],
      "metadata": {
        "id": "yOjUf8ACsQ8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wichtige Zyklen und Pfade in der Graphentheorie**\n",
        "\n",
        "* [**Eulerian path** (Eulerkreisproblem)](https://de.m.wikipedia.org/wiki/Eulerkreisproblem) = every edge only once\n",
        "  * **Euler path** (every edge once only, vertices can be multiple: exists if either 0 or 2 odd degree vertices exist with all else even degree\n",
        "  * **Euler circuit** (more constraint): same as Euler path, but return to starting point (das Haus vom weihachtsmann). Use also Fleurys algorithm: works if all vertices have even degrees\n",
        "\n",
        "* [**Hamiltonian circuit / cycle** (Hamiltonian path problem)](https://de.m.wikipedia.org/wiki/Hamiltonkreisproblem): every vertex only once\n",
        "  * every vertex only once, but edges don‚Äòt matter how often. And return to starting point.\n",
        "  * **Minimum cost hamiltonian circuit = [traveling salesman problem](https://de.m.wikipedia.org/wiki/Problem_des_Handlungsreisenden).**. Is np-complete. Look for [shortest path](https://de.m.wikipedia.org/wiki/K√ºrzester_Pfad), siehe auch [Pathfinding](https://de.m.wikipedia.org/wiki/Pathfinding)\n",
        "  * https://www.quora.com/Why-are-quantum-computers-unable-to-solve-the-travelling-salesman-problem-in-polynomial-time: *Quantum computation offers speed improvements for some very specialized problems like integer factorization, but it isn‚Äôt known or expected to offer polynomial-time algorithms for generic NP-complete problems like the Traveling Salesman Problem. If it does then NP ‚äÜ BQP, and most experts don‚Äôt regard this as likely at all $^{*}$*.\n",
        "  * Heuristic algorithms as alternative to brute force:\n",
        "    * [Dijkstra algorithmus](https://de.m.wikipedia.org/wiki/Dijkstra-Algorithmus)\n",
        "    * Nearest neighbor algorithm is a heuristic, non optimal, but feasible - [greedy algorithm](https://de.m.wikipedia.org/wiki/Greedy-Algorithmus), cheapest route\n",
        "    * Repeated Nearest neighbor algorithm\n",
        "    * Sorted edges algorithm\n",
        "    * Kruskal algorithm (spanning tree) for minimum cost spanning tree (optimal and efficient). For example energy lines\n",
        "\n",
        "If you have a fully connected [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) and and want to compute unique Hamiltonain circuits:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1389.png)\n",
        "\n",
        "Source: [Graph theory full course for Beginners](https://www.youtube.com/watch?v=sWsXBY19o8I)\n",
        "\n",
        "$^{*}$ *Why can‚Äôt a quantum computer solve NP complete problems in a polynomial running time (relative to their input)?\n",
        "Quantum computers, like classical computers, are not currently believed to be able to solve NP-complete problems in polynomial time.\n",
        "\n",
        "Contrary to Kurt Behnke‚Äôs answer, this is not true by definition. It‚Äôs a profound open question in mathematics.\n",
        "\n",
        "As Vadim Yakovlevich correctly points out, it‚Äôs partly a matter of no one having found a polynomial-time quantum algorithm for NP-complete problems after decades of trying‚Äîjust like no one has found a polynomial-time classical algorithm.\n",
        "\n",
        "But it‚Äôs possible to say more than that. From reading popular articles, many people have the vague impression that a quantum computer could just try every possible solution in parallel. And if that‚Äôs all you know about them, then it‚Äôs indeed a mystery why they couldn‚Äôt solve NP-complete problems in polynomial time!\n",
        "\n",
        "The central difficulty is that, for a computer to be useful, at some point you need to measure it to observe an answer. And if you just measured an equal superposition over all possible answers, not having done anything else, the rules of QM dictate that you‚Äôll just see a random answer. And of course, if you‚Äôd just wanted a random answer, you could‚Äôve picked one yourself with a lot less trouble!\n",
        "\n",
        "So the name of the game, with every quantum algorithm, is somehow orchestrating a pattern of constructive and destructive quantum interference that boosts the probability of the correct answer (even though you yourself don‚Äôt know in advance which answer is the correct one!), and that does so efficiently. Famously, in 1994 Peter Shor showed how to do that for the problem of factoring integers, and a few related problems in number theory‚Äîbut he was able to do so only by exploiting extremely special structure in those problems.\n",
        "\n",
        "For more ‚Äúgeneric‚Äù search problems, like NP-complete problems, the best we generally know is Grover‚Äôs algorithm: a quantum algorithm that‚Äôs able to find the right answer (i.e., concentrate a large fraction of the amplitude on that answer) in roughly the square root of the number of steps that would be needed classically. So you get some speedup, but not an exponential one.\n",
        "\n",
        "But we also know that, if your search problem is a ‚Äúblack box‚Äù‚Äîi.e., you can just pick candidate solutions and evaluate if they‚Äôre correct, and you don‚Äôt know anything more about the problem‚Äôs structure‚Äîthen Grover‚Äôs algorithm is the best that even a quantum computer can do. This is the content of the so-called BBBV (Bennett, Bernstein, Brassard, Vazirani) Theorem, which has several proofs, all of them crucially relying on the linearity of unitary evolution. The BBBV Theorem gives a fundamental explanation for why any fast quantum algorithm for NP-complete problems would have to look very different from anything that we know today‚Äîmuch like a fast classical algorithm would have to*."
      ],
      "metadata": {
        "id": "6M6WUYqnfgAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph Properties (Invariants)**\n",
        "\n",
        "* A [graph property (invariant)](https://en.m.wikipedia.org/wiki/Graph_property) is a property of graphs that depends only on the abstract structure, not on graph representations such as particular labellings or drawings of the graph.\n",
        "\n",
        "* An example graph, with the properties of\n",
        "\n",
        "  * being [planar](https://en.m.wikipedia.org/wiki/Planar_graph) (it can be drawn on the plane in such a way that its edges intersect only at their endpoints / no edges cross each other)\n",
        "\n",
        "  * and being [connected](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)),\n",
        "\n",
        "  * and with order 6,\n",
        "\n",
        "  * size 7,\n",
        "\n",
        "  * [diameter (distance](https://en.m.wikipedia.org/wiki/Distance_(graph_theory)) 3 (number of edges in a shortest path, also called a graph geodesic, connecting them)\n",
        "\n",
        "  * [girth](https://en.m.wikipedia.org/wiki/Girth_(graph_theory)) 3 (girth of an undirected graph is the length of a shortest cycle contained in the graph)\n",
        "\n",
        "  * vertex [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) 1, and\n",
        "\n",
        "  * [degree sequence](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (3, 3, 3, 2, 2, 1):\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/333px-6n-graf.svg.png)"
      ],
      "metadata": {
        "id": "03Ad29FMyOx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph enumeration**\n",
        "\n",
        "* In combinatorics, an area of mathematics, [graph enumeration](https://en.m.wikipedia.org/wiki/Graph_enumeration) describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph.\n",
        "\n",
        "* These problems may be solved either exactly (as an [algebraic enumeration](https://en.m.wikipedia.org/wiki/Algebraic_enumeration) problem) or asymptotically.\n",
        "\n",
        "* The number of labeled n-vertex simple undirected graphs is $2^{\\frac{n(n‚Ää‚àí1)}{2}}$\n",
        "\n",
        "* The number of labeled n-vertex simple directed graphs is $2^{n(n‚Ää‚àí1)}$\n",
        "\n",
        "* **Number of connected labeled graphs with n nodes**: The number of distinct connected labeled graphs with n nodes is tabulated in the [On-Line Encyclopedia of Integer Sequences](https://en.m.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences) as sequence [A001187](https://oeis.org/A001187). The first few non-trivial terms are\n",
        "\n",
        "$\\begin{array}{|l|l|}\n",
        "\\hline {\\text { n }} & {\\text { graphs }} \\\\\n",
        "\\hline 1 & 1 \\\\\n",
        "\\hline 2 & 1 \\\\\n",
        "\\hline 3 & 4 \\\\\n",
        "\\hline 4 & 38 \\\\\n",
        "\\hline 5 & 728 \\\\\n",
        "\\hline6 & 26704 \\\\\n",
        "\\hline 7 & 1866256 \\\\\n",
        "\\hline 8 & 251548592 \\\\\n",
        "\\hline\n",
        "\\end{array}$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9u6fmPYO1EZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genus of a Graph**\n",
        "\n",
        "[genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (plural genera) has a few different, but closely related, meanings. Intuitively, the genus is the number of \"holes\" of a surface.[1] A sphere has genus 0, while a torus has genus 1.\n",
        "\n",
        "A genus-2 surface:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Double_torus_illustration.png/219px-Double_torus_illustration.png)\n",
        "\n",
        "**The genus of a graph is the minimal integer n such that the graph can be drawn without crossing itself on a sphere with n handles** (i.e. an oriented surface of the genus n). Thus, a planar graph has genus 0, because it can be drawn on a sphere without self-crossing.\n",
        "\n"
      ],
      "metadata": {
        "id": "4eb_PjW92TEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph embedding**\n",
        "\n",
        "In topological graph theory, an [embedding](https://en.m.wikipedia.org/wiki/Graph_embedding) (also spelled imbedding) of a graph G on a surface $\\Sigma$ is a representation of G on $\\Sigma$ in which points of $\\Sigma$ are associated with vertices and simple arcs (homeomorphic images of $[0,1])$ are associated with edges in such a way that:\n",
        "\n",
        "the endpoints of the arc associated with an edge $e$ are the points associated with the end vertices of e, no arcs include points associated with other vertices, two arcs never intersect at a point which is interior to either of the arcs.\n",
        "\n",
        "> Often, an embedding is regarded as an equivalence class (under homeomorphisms of $\\Sigma$) of representations of the kind just described."
      ],
      "metadata": {
        "id": "UiscJVPa4BY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spanning tree**\n",
        "\n",
        "A [spanning tree](https://en.m.wikipedia.org/wiki/Spanning_tree) T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/4x4_grid_spanning_tree.svg/252px-4x4_grid_spanning_tree.svg.png)\n",
        "\n",
        "A special kind of spanning tree, the Xuong tree, is used in topological graph theory to find graph embeddings with maximum [genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (is the number of \"holes\" of a surface)"
      ],
      "metadata": {
        "id": "Z72Y0JpX169N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of random walks on graphs**\n",
        "\n",
        "Cover times: A cover time is the number of steps needed for a random walk to visit all vertices in a given graph.\n",
        "\n",
        "http://uu.diva-portal.org/smash/get/diva2:319078/FULLTEXT01.pdf"
      ],
      "metadata": {
        "id": "ChVvi_4nAOHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of graphs and the Gaussian free field**\n",
        "\n",
        "Max point of a gaussian free field is the max number of time steps to visit each vertex in a graph\n",
        "\n",
        "https://tcsmath.wordpress.com/2010/12/09/open-question-cover-times-and-the-gaussian-free-field/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Gaussian_free_field\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/9/93/Discrete_Gaussian_free_field_on_60_x_60_square_grid.png)"
      ],
      "metadata": {
        "id": "6gIbaAJjACax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit rank**\n",
        "\n",
        "* [Circuit rank](https://en.m.wikipedia.org/wiki/Circuit_rank), cyclomatic number, cycle rank, or nullity of an undirected graph is the **minimum number of edges that must be removed from the graph to break all its cycles, making it into a tree or forest**.\n",
        "\n",
        "* It is equal to the number of independent cycles in the graph (the size of a cycle basis).\n",
        "\n",
        "* This graph has circuit rank r = 2 because it can be made into a tree by removing two edges, for instance the edges 1‚Äì2 and 2‚Äì3, but removing any one edge leaves a cycle in the graph:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/320px-6n-graf.svg.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wJ0FciA8LKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Degree**\n",
        "\n",
        "* Degree of a matrix = how many neighbours in each vertex?\n",
        "\n",
        "* [Degree](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (or valency) of a vertex of a graph is the number of edges that are incident to the vertex; in a multigraph, a loop contributes 2 to a vertex's degree, for the two ends of the edge.\n",
        "\n",
        "* The maximum degree of a graph G, denoted by $\\Delta (G)$, and the minimum degree of a graph, denoted by ${\\displaystyle \\delta (G)}$, are the maximum and minimum of its vertices' degrees. In the multigraph shown on the right, the maximum degree is 5 and the minimum degree is 0.\n",
        "\n",
        "* In a [regular graph](https://en.m.wikipedia.org/wiki/Regular_graph) (=each vertex has the same number of neighbors), every vertex has the same degree, and so we can speak of the degree of the graph.\n",
        "\n",
        "* A [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) (denoted $K_{n}$, where n is the number of vertices in the graph) is a special kind of regular graph where all vertices have the maximum possible degree, $n-1$.\n",
        "\n",
        "* Degree sum formula states that, given a graph $G=(V,E)$: ${\\displaystyle \\sum _{v\\in V}\\deg(v)=2|E|\\,}$ (Handshaking lemma)\n",
        "\n",
        "*Two non-isomorphic graphs with the same degree sequence (3, 2, 2, 2, 2, 1, 1, 1):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Conjugate-dessins.svg/240px-Conjugate-dessins.svg.png)"
      ],
      "metadata": {
        "id": "iMnZwQUq6F8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Connected) Component (isolated subgraph) = Betti number $B_0$**\n",
        "\n",
        "* The number of (connected) [components](https://en.m.wikipedia.org/wiki/Component_(graph_theory)) of a topological space is an important topological invariant, the zeroth Betti number $B_0$, and the number of components of a graph is an important graph invariant, and **in topological graph theory it can be interpreted as the zeroth Betti number of the graph**.\n",
        "\n",
        "* The number of components arises in other ways in graph theory as well: **In algebraic graph theory number of components equals the multiplicity of 0 as an eigenvalue of the Laplacian matrix of a finite graph**.\n",
        "\n",
        "* bzw: The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**. (Der gr√∂√üte Eigenwert eines k-regul√§ren Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen.) Taken from: https://de.m.wikipedia.org/wiki/Spektrum_(Graphentheorie)\n",
        "\n",
        "* Let G be a d-regular graph. The algebraic multiplicity of eigenvalue 0 for the Laplacian matrix is exactly 1 iff G is connected. https://math.uchicago.edu/~may/REU2013/REUPapers/Marsden.pdf\n",
        "\n",
        "* https://www.sciencedirect.com/science/article/pii/S0024379518300156"
      ],
      "metadata": {
        "id": "0tvO21EovW33"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgO45hovFfpL"
      },
      "source": [
        "**Connected Components**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Component_(graph_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Connected_space\n",
        "* How the graph can be used to capture connected component\n",
        "* Any graph gives us a topological space (like joining a square as a connected component)\n",
        "* Graph can capture connected components via putting an equivalence relation on the set of vertices.\n",
        "* **equivalence relation is generated by tail of an edge (vertex 1) is equivalent to a head of an edge (vertex 2) / between the vertices of an edge (which together forms a connected component)**.\n",
        "* To generate equivalence relation we have to make sure that we enforce reflexivity, symmetry and transivity.\n",
        "* You get equivalence classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a graph**\n",
        "\n",
        "* the [rank of an undirected graph](https://en.m.wikipedia.org/wiki/Rank_(graph_theory)) has two unrelated definitions. Let n equal the number of vertices of the graph.\n",
        "\n",
        "* In the [matrix theory](https://en.m.wikipedia.org/wiki/Matrix_(mathematics)) of graphs the rank r of an undirected graph is defined as the rank of its adjacency matrix. Analogously, the [nullity](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) of the graph is the nullity of its adjacency matrix, which equals n ‚àí r.\n",
        "\n",
        "* In the [matroid theory](https://en.m.wikipedia.org/wiki/Matroid) of graphs the rank of an undirected graph is defined as the number n ‚àí c, where c is the number of connected components of the graph.\n",
        "\n",
        "  * Equivalently, the rank of a graph is the rank of the oriented [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) associated with the graph.\n",
        "\n",
        "  * Analogously, the [nullity of the graph](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) is the [nullity (Kernel)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) of its oriented incidence matrix, given by the formula m ‚àí n + c, where n and c are as above and m is the number of edges in the graph.\n",
        "\n",
        "  * The nullity is equal to the first [Betti number](https://en.m.wikipedia.org/wiki/Betti_number) of the graph. The sum of the rank and the nullity is the number of edges.\n"
      ],
      "metadata": {
        "id": "Hju_JaTe8iLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity** (Menger's theorem, max-flow min-cut)\n",
        "\n",
        "* In mathematics and computer science, [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) is one of the basic concepts of graph theory: **it asks for the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more [isolated subgraphs (= Connected component)](https://en.m.wikipedia.org/wiki/Component_(graph_theory)).**\n",
        "\n",
        "* It is closely related to the theory of network flow problems. The connectivity of a graph is an important measure of its resilience as a network.\n",
        "\n",
        "* One of the most important facts about connectivity in graphs is [Menger's theorem](https://en.m.wikipedia.org/wiki/Menger%27s_theorem): for any two vertices u and v in a connected graph G, the numbers Œ∫(u, v) and Œª(u, v) can be determined efficiently using the [max-flow min-cut theorem](https://en.m.wikipedia.org/wiki/Max-flow_min-cut_theorem) algorithm.\n",
        "\n",
        "* The vertex- and edge-connectivities of a disconnected graph are both 0.\n",
        "\n",
        "* 1-connectedness is equivalent to connectedness for graphs of at least 2 vertices.\n",
        "\n",
        "* The [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) on n vertices has edge-connectivity equal to n ‚àí 1. Every other simple graph on n vertices has strictly smaller edge-connectivity.\n",
        "\n",
        "\n",
        "*This graph becomes disconnected when the right-most node in the gray area on the left is removed:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/389px-Network_Community_Structure.svg.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8L5-Pd_FzCPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique**\n",
        "\n",
        "* Eine [Clique](https://de.m.wikipedia.org/wiki/Clique_(Graphentheorie)) bezeichnet in der Graphentheorie **eine Teilmenge von Knoten in einem ungerichteten Graphen, bei der jedes Knotenpaar durch eine Kante verbunden ist**.\n",
        "\n",
        "* Zu entscheiden, ob ein Graph eine Clique einer bestimmten Mindestgr√∂√üe enth√§lt, wird [Cliquenproblem](https://de.m.wikipedia.org/wiki/Cliquenproblem) genannt und gilt, wie das Finden von gr√∂√üten Cliquen, als algorithmisch schwierig (NP-vollst√§ndig).\n",
        "\n",
        "* Das Finden einer Clique einer bestimmten Gr√∂√üe in einem Graphen ist ein [NP-vollst√§ndiges Problem](https://de.m.wikipedia.org/wiki/NP-Vollst√§ndigkeit) (= schwierigsten Problemen in der Klasse NP geh√∂rt, also sowohl in NP liegt als auch NP-schwer ist) und somit auch in der Informationstechnik ein relevantes Forschungs- und Anwendungsgebiet.\n",
        "\n",
        "*Ein Graph mit einer Clique der Gr√∂√üe 3:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/6n-graf-clique.svg/350px-6n-graf-clique.svg.png)\n"
      ],
      "metadata": {
        "id": "3CZpsY2coXHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique complex (Whitney complexes)**\n",
        "\n",
        "[Clique complexes](https://en.m.wikipedia.org/wiki/Clique_complex), independence complexes, flag complexes, Whitney complexes and conformal hypergraphs are closely related mathematical objects in graph theory and geometric topology that each describe the cliques (complete subgraphs) of an undirected graph.\n",
        "\n",
        "Siehe auch: [Topological Graph Theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory)"
      ],
      "metadata": {
        "id": "NENSroXr7Oh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CW Complex**\n",
        "\n",
        "A [CW complex](https://en.m.wikipedia.org/wiki/CW_complex) (also called cellular complex or cell complex) is a kind of a topological space that is particularly important in algebraic topology.[1] It was introduced by J. H. C. Whitehead[2] to meet the needs of homotopy theory. This class of spaces is broader and has some better categorical properties than simplicial complexes, but still retains a combinatorial nature that allows for computation (often with a much smaller complex). The C stands for \"closure-finite\", and the W for \"weak\" topology\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))"
      ],
      "metadata": {
        "id": "XdMET4Yk8PJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract simplicial complex**\n",
        "\n",
        "In combinatorics, an [abstract simplicial complex](https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex) (ASC), often called an abstract complex or just a complex, is a family of sets that is closed under taking subsets, i.e., every subset of a set in the family is also in the family. It is a purely combinatorial description of the geometric notion of a simplicial complex.[1]\n",
        "\n",
        "For example, in a 2-dimensional simplicial complex, the sets in the family are the triangles (sets of size 3), their edges (sets of size 2), and their vertices (sets of size 1).\n",
        "\n",
        "Geometric realization of a 3-dimensional abstract simplicial complex:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Simplicial_complex_example.svg/247px-Simplicial_complex_example.svg.png)\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Let G be an undirected graph. The [clique complex](https://en.m.wikipedia.org/wiki/Clique_complex) (flag complexes, Whitney complexes) of G is an ASC whose faces are all cliques (complete subgraphs) of G. The independence complex of G is an ASC whose faces are all independent sets of G (it is the clique complex of the complement graph of G). Clique complexes are the prototypical example of flag complexes. A flag complex is a complex K with the property that every set of elements that pairwise belong to faces of K is itself a face of K.\n",
        "\n",
        "* Let M be a metric space and Œ¥ a real number. The [Vietoris‚ÄìRips complex](https://en.m.wikipedia.org/wiki/Vietoris‚ÄìRips_complex) is an ASC whose faces are the finite subsets of M with diameter at most Œ¥. It has applications in homology theory, hyperbolic groups, image processing, and mobile ad hoc networking. It is another example of a flag complex (clique complex).\n",
        "\n"
      ],
      "metadata": {
        "id": "wAytmxq5-zzX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbqQuo7L2du"
      },
      "source": [
        "**Simplices**\n",
        "\n",
        "A [simplex](https://de.m.wikipedia.org/wiki/Simplex_(Mathematik)) consists of 3 components: vertices, edges and faces\n",
        "\n",
        "* 0-simplex: point\n",
        "* 1-simplex: edge (line)\n",
        "* 2-simplex: 3 connected points\n",
        "* 3-simplex: solid (3 dimensions)\n",
        "\n",
        "k-simplex is k-dimensional is formed using (k+1) vertices ('convex hull')\n",
        "\n",
        "* Complete graphs (every vertice is connected to all others) can be interpreted as simplices\n",
        "\n",
        "* you can interpret any graph as simplicial complex\n",
        "\n",
        "**Simplicial Complex**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex\n",
        "\n",
        "* Mit einem Simplizialkomplex k√∂nnen die entscheidenden Eigenschaften von triangulierbar topologischen R√§umen algebraisch charakterisiert werden k√∂nnen\n",
        "\n",
        "* Warum? Definition von Invarianten im topologischen Raum. Simplicial complexes can be seen as higher dimensional generalizations of neighboring graphs.\n",
        "\n",
        "* Wie? Untersuchung eines topologischen Raums durch Zusammenf√ºgen von Simplizes womit eine Menge im d-dimensionalen euklidischen Raum konstruiert wird, die [hom√∂omorph](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist zum gegebenen topologischen Raum.\n",
        "\n",
        "* Die ‚ÄûAnleitung zum Zusammenbau‚Äú der Simplizes, das hei√üt die Angaben dar√ºber, wie die Simplizes zusammengef√ºgt sind, wird dann in Form einer Sequenz von [Gruppenhomomorphismen](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus) rein algebraisch charakterisiert.\n",
        "\n",
        "* Cells can have various dimensions: vertices, edges, triangles, tetrahedra and their higher dimensional analogues. complexes reflect the correct topology of the data\n",
        "\n",
        "* If we glue many simplices together in such a way that the intersection is also a simplex (along an edge for example), we obtain a simplicial complex. If we see three points connected by edges that form a triangle, we fill in the triangle with a 2-dimensional face. Any four points that are all pairwise connected get filled in with a 3-simplex etc. The resulting simplicial complex is called (Vietoris) Rips complex.\n",
        "\n",
        "* The persistence diagram is not computed directly from LÙè∞ëŒµ. Instead, one forms an object called a Cech complex. Simplicial and cubical complexes are examples of cell complexes. Cech complex is an example of a simplicial complex. - in practice, often used Vietoris-Rips complex VŒµ - the persistent homology defined by VŒµ approximates the persistent homology defined by CŒµ.\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_03.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hSqsZ5LLdvz"
      },
      "source": [
        "**Triangulation**\n",
        "\n",
        "* In der Topologie ist eine [Triangulierung](https://en.m.wikipedia.org/wiki/Triangulation_(topology)) oder Triangulation eine **Zerlegung eines Raumes in Simplizes** (Dreiecke, Tetraeder oder deren h√∂her-dimensionale Verallgemeinerungen).\n",
        "\n",
        "* Mannigfaltigkeiten bis zur dritten Dimension sind stets triangulierbar.\n",
        "\n",
        "* Urspr√ºngliche Motivation f√ºr die Hauptvermutung war der Beweis der topologischen Invarianz kombinatorisch definierter Invarianten wie der [simplizialen Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie). Trotz des Scheiterns der Hauptvermutung lassen sich Fragen dieser Art oftmals mit dem [simplizialen Approximationssatz](https://de.m.wikipedia.org/wiki/Simpliziale_Approximation) beantworten.\n",
        "\n",
        "* Triangulation ist eine Zerlegung eines Raumes in Simplizes (Dreiecke, Tetraeder oder h√∂her-dimensionale Verallgemeinerungen) to tease out properties of manifolds\n",
        "\n",
        "* **A triangulation of a topological space X is a simplicial complex K, homeomorphic to X, together with a homeomorphism h: K ‚Üí X**\n",
        "\n",
        "* triangulation offers a concrete way of visualizing spaces that are difficult to see, and helps computing an invariant.\n",
        "\n",
        "* Ist gegeben durch einen (abstrakten) Simplizialkomplex K und Hom√∂omorphismus h : | K | ‚Üí X der geometrischen Realisierung | K | auf X.\n",
        "https://en.wikipedia.org/wiki/Triangulation_(topology)\n",
        "\n",
        "* a two-dimensional sphere (surface of a solid ball) can be approximated by gluing together two-dimensional triangles, and a three-dimensional sphere can be approximated by gluing together three-dimensional tetrahedra.\n",
        "\n",
        "* Triangles and tetrahedra are examples of more general shapes called simplices, which can be defined in any dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRA0yDZJGOjK"
      },
      "source": [
        "**[Filtration](https://de.m.wikipedia.org/wiki/Filtrierung_(Mathematik)) & Inclusion Maps**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtered_algebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtration_(mathematics)\n",
        "\n",
        "* **With increasing size of d, we are dealing with a sequence of simplicial complexes, each a sub-complex of the next. That is, a simplicial complex constructed from data for some small distance is a subset of the simplicial complex constructed for a larger distance**.\n",
        "\n",
        "* Equally there is an **[inclusion map](https://en.m.wikipedia.org/wiki/Inclusion_map) from each simplicial complex to the next**.\n",
        "\n",
        "* **This sequence of simplicial complexes, with inclusion maps, is called a filtration**.\n",
        "\n",
        "* When we apply homology to a filtration, we obtain an algebraic structure called persistent modul.\n",
        "\n",
        "* So if we want to compute **ith homology with coefficients from a field k**.\n",
        "\n",
        "* The **homology of any complex Cj is a vector space**, and the **inclusion maps between complexes induce linear maps between homology vector spaces**.\n",
        "\n",
        "* The direct sum of the homology vector spaces is an algebraic module - in fact a **graded module over the polynomial ring** k[x]. The variable x acts as a **shift map**, taking each homology generator to its image in the next vector space.\n",
        "\n",
        "* Furthermore, a structure theorem tells us that **a persistent module decomposes nicely into a direct sum of simple modules**, each corresponding to a bar in the barcode. This means: a barcode reall is an algebraic structure.\n",
        "\n",
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS9TpoyGEmjj"
      },
      "source": [
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)\n",
        "\n",
        "![cscsvs](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_02.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tropical Geometry*"
      ],
      "metadata": {
        "id": "lh3x3_1K_e1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Sturmfels - Tropical Geometry](https://youtu.be/TNwCzl02uck)"
      ],
      "metadata": {
        "id": "1gdFgCSJfTg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Geometry**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_geometry\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_variety\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/System_of_polynomial_equations\n",
        "\n",
        "Particle physics and cosmology are linked to combinatorics and algebraic geometry in an unexpected way. Novel geometric objects hint at new mathematical structures that challenge our current understanding of the laws of nature. The workshop ‚ÄûPositive Geometry\" on January 23 at the Campus of the Technische Universit√§t M√ºnchen will discuss the latest developments. The event is organized by Bernd Sturmfels, Director at the Max-Planck-Institut f√ºr Mathematik in den Naturwissenschaften, J√ºrgen Richter-Gebert, Chair for Geometry and Visualization at TUM, Johannes Henn, Director at MPP in collaboration with #SFB Transregio 109, Discretization in Geometry and Dynamics.\n",
        "More information: https://Inkd.in/eQVnYtyD\n",
        "particlephysics geometry cosmology"
      ],
      "metadata": {
        "id": "mFfIs1nSCsxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tropical Geometry**\n",
        "\n",
        "http://www.mittag-leffler.se/langa-program/tropical-geometry-amoebas-and-polytopes-0\n",
        "\n",
        "Video: [What is tropical mathematics?](https://youtu.be/DV-8kEn8udY)"
      ],
      "metadata": {
        "id": "hjvAO915_hkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Groups*"
      ],
      "metadata": {
        "id": "gS-QUpcFmXh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kongruenzklassen, √Ñquivalenzklassen & Restklassen (Modulorechnung)**\n",
        "\n",
        "* **Kongruenz**: [Kongruenz (Zahlentheorie)](https://de.m.wikipedia.org/wiki/Kongruenz_(Zahlentheorie))) Die Kongruenz zwischen zwei ganzen Zahlen ist in Bezug auf einen Teiler definiert. **Der Teiler hei√üt in diesem Zusammenhang Modul.**\n",
        "  * Gegeben sei ein Modul $m \\in \\mathbb{N}$. Zwei ganze Zahlen $a$ und $b$ hei√üen kongruent modulo $m$, **wenn die Division von $a$ und $b$ durch $m$ den gleichen Rest $r$ l√§sst**.\n",
        "  * Kongruenz ist eine Art Erweiterung der Modulorechnung: **Modulorechnung**: 17 mod 3 = 2.\n",
        "  * **Kongruenz: 11 $\\equiv$ 17 mod 3 (weil bei beiden der Rest 2 ist - beide sind in der der gleichen Restklasse)**\n",
        "  * Man kann alternativ zur [**Restklassenermittlung**](https://de.m.wikipedia.org/wiki/Restklasse) auch sagen, dass zwei Zahlen kongruent sind (modulo der nat√ºrlichen Zahl n), wenn ihre Differenz durch n teilbar ist. Hier: 17 - 11 = 6, ist teilbar durch 3.\n",
        "\n",
        "* [**√Ñquivalenz und √Ñquivalenzklassen** (Congruence Classes)](https://de.m.wikipedia.org/wiki/√Ñquivalenzrelation). √Ñquivalenz: Objekte, die sich in einem bestimmten Zusammenhang gleichen, als gleichwertig bzw. √§quivalent angesehen. The result of the modulo operation is an equivalence class (√Ñquivalenzklassen). Any member of the class may be chosen as representative.\n",
        "\n",
        "  * Von besonderem Interesse sind jedoch solche √Ñquivalenzrelationen $\\equiv$ , deren Quotientenabbildung $\\mathrm{q}_{\\mathrm{z}}: A \\rightarrow A / \\equiv, a \\mapsto[a]_{\\equiv}$ **mit der Struktur auf $A$ vertr√§glich bzw. ein Homomorphismus ist**, weil dann die von $\\mathrm{q}_{=}$ erzeugte Struktur auf der [Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) $A / \\equiv$ von der gleichen Art ist wie die von $A$. **Eine solche √Ñquivalenzrelation $\\equiv$ nennt man eine Kongruenzrelation auf der strukturierten Menge $A$.** <font color=\"blue\">Die Quotientengruppe G/N ist homomorph zur Gruppe G.</font>\n",
        "\n",
        "* **[Restklassen](https://de.wikipedia.org/wiki/Restklasse)** sind die √Ñquivalenzklassen in der Kongruenzrelation. Eine Zahl a modulo einer Zahl m die Menge aller Zahlen, die bei Division durch m denselben Rest lassen wie a. See also [Modulo Operation](https://en.m.wikipedia.org/wiki/Modulo_operation) (Restklassenrechnung).\n",
        "\n",
        "* In der Gruppentheorie werden √Ñquivalenzklassen als [Nebenklassen (Cosets)](https://de.m.wikipedia.org/wiki/Gruppentheorie#Nebenklassen) bezeichnet."
      ],
      "metadata": {
        "id": "0a8DDz9Vbn1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfs10URbe-RG"
      },
      "source": [
        "**Subgroups, Normal Subgroups & Cosets**\n",
        "\n",
        "* **Subgroups (Unterguppen):**\n",
        "\n",
        "  * Gegeben sei $\\mathbb{Z}$ die Gruppe der ganzen Zahlen mit der Addition als Gruppenoperation. Man kann fur diese Gruppe verschiedene (unendlich viele) **[Untergruppen](https://de.m.wikipedia.org/wiki/Untergruppe) (Subgroups)** bilden: 2$\\mathbb{Z}$, 3$\\mathbb{Z}$, 4$\\mathbb{Z}$, 5$\\mathbb{Z}$, 6$\\mathbb{Z}$, 7$\\mathbb{Z}$.\n",
        "\n",
        "  * Beispiel: Die ganzen Zahlen $\\mathbb {Z} $ sind bez√ºglich der Addition eine Untergruppe der rationalen Zahlen $\\mathbb {Q} $.\n",
        "\n",
        "  * Two standard subgroups / every group has at least 2 subgroups: the identity element {e} and the entire group G. These are technically **normal subgroups (Normalteiler)**.\n",
        "\n",
        "  * If a group has no other normal subgroups then these two, than it's called a **simple group**. A simple group does not have any factor (quotient) groups, but they are the building blocks of other groups.\n",
        "\n",
        "* **Normal Subgroups (Normalteiler)**:\n",
        "\n",
        "  * <font color=\"blue\">**Die √Ñquivalenzklasse mit dem Rest Null ist der Normalteiler**. Normalteiler sind spezielle Untergruppen, und ihre Bedeutung liegt vor allem darin, **dass sie genau die Kerne von Gruppenhomomorphismen sind** (=L√∂sungsmenge ist Null, also Null als Rest).</font> Normal Subgroups heissen auch \"invariant or self-conjugate subgroups\".\n",
        "\n",
        "  * Normal subgroups determine what kinds of homomorphisms are possible from a group $G$ to other groups $f : G -> H$.\n",
        "\n",
        "  * Trivial examples: Standard subgroups identity element {e} and the entire group G.\n",
        "\n",
        "  * Die Gruppe $\\mathbb{Z}$ ist **abelsch** (kommutativ) und somit ist jede Untergruppe auch ein **[Normalteiler](https://de.wikipedia.org/wiki/Normalteiler) bzw. [Normal Subgroup](https://en.wikipedia.org/wiki/Normal_subgroup)**.\n",
        "\n",
        "* **Nebenklassen (Cosets)**:\n",
        "\n",
        "  * Nebenklassen werden benutzt, um den [Satz von Lagrange](https://de.m.wikipedia.org/wiki/Satz_von_Lagrange) zu beweisen, um die Begriffe [Normal Subgroup (Normalteiler)](https://de.m.wikipedia.org/wiki/Normalteiler) und [Quotient Group (Faktorgruppe)](https://de.m.wikipedia.org/wiki/Faktorgruppe) zu erkl√§ren und um Gruppenoperationen zu studieren.\n",
        "\n",
        "  * In contrast to Subgroups, the Coset (Nebenklasse) are not closed under addition, have no inverse and don't contain the identity element\n",
        "\n",
        "  * Cosets are there to define how many (finite) possible subgroups exist in a group. There are **two types of cosets**: left cosets and right cosets.\n",
        "\n",
        "  * A subgroup $H$ of a group $G$ may be used to decompose the underlying set of $G$ into disjoint, equal-size subsets called [cosets](https://en.m.wikipedia.org/wiki/Coset)\n",
        "\n",
        "* <font color=\"blue\">**Example: Use Normal Subgroup 5$\\mathbb{Z}$ von $\\mathbb{Z}$ (integers mod 5) to divide a Group into Cosets. We get 5 sets of remainders (the congruence classes) which are the [Quotient Group (Faktorgruppe)](https://en.m.wikipedia.org/wiki/Quotient_group), a group with 5 elements: $\\mathbb{Z}$ mod 5 = {$\\overline{0}$, $\\overline{1}$, $\\overline{2}$, $\\overline{3}$, $\\overline{4}$}.**</font>\n",
        "\n",
        "    * 5$\\mathbb{Z}$ + Rest 0 = $\\overline{0} : \\{\\ldots-10,-5,0,5,10 \\ldots\\}$ **(Normal) Subgroup**, which is technically also a Coset 0+ 5 $\\mathbb{Z}$\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 1 = $\\overline{1} : \\{\\ldots-9,-4,1,6,11 \\ldots\\}$ **Coset 1+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 2 = $\\overline{2} : \\{\\ldots-8,-3,2,7,12 \\ldots\\}$ **Coset 2+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 3 = $\\overline{3} : \\{\\ldots-7,-2,3,8,13 \\ldots\\}$ **Coset 3+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 4 = $\\overline{4} : \\{\\ldots-6,-1,4,9,14 \\ldots\\}$ **Coset 4+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "* **Quotient Group (Faktorgruppe)**:\n",
        "\n",
        "  * Die Faktorgruppe oder Quotientengruppe wird mit $G / N$ bezeichnet und ist die Menge der Nebenklassen (Cosets). Aus einer Gruppe $G$ und jedem ihrer Normalteiler $N$ l√§sst sich eine Faktorgruppe $G/N$ bilden.\n",
        "\n",
        "    * Die Quotientengruppe unterteilt eine Menge in √Ñquivalenzklassen bzw. eine Gruppe in Restklassen / Nebenklassen. Diese Menge der Restklassen (√Ñquivalenzklassen) heisst **[Quotientenmenge](https://de.wikipedia.org/wiki/√Ñquivalenzrelation#Quotientenmenge_und_Partition) bzw. Faktormenge**.\n",
        "\n",
        "  * Diese [Quotientengruppen](https://de.wikipedia.org/wiki/Faktorgruppe) sind homomorphe Bilder von G, **und jedes homomorphe Bild von G ist zu einer solchen Quotientengruppe G/N isomorph**. See fundamental theorem on homomorphisms of groups as \"**Every homomorphic image of a group is isomorphic to a quotient group**\". [Source](https://en.m.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms)\n",
        "\n",
        "  * For finite groups you can find a chain of normal subgroups called a \"composition series\" which acts as a kind of 'prime factorization' of the group (1 ‚óÉ N1, ‚óÉ N2 ‚óÉ ... ‚óÉ Nr ‚óÉ G ). Normal subgroups can also be used to study fields (K√∂rper), i.e. in Galois theory (Field extension K / F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generators of a group*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0942.jpg)\n",
        "\n",
        "> **Each group SU(3) x SU(2) x U(1) leads to a symmetry resulting in a conservation law**"
      ],
      "metadata": {
        "id": "Pp3Z0FI7ipEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVfPVtLVGzQ"
      },
      "source": [
        "**Zusammenfassung algebraischer Strukturen mit Operationen**\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ is a Group\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ is a Ring\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ $√∑$ $\\quad$ is a Field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvvUdIJw3cu"
      },
      "source": [
        "A group is a set $G$ with an operation $*$ such that\n",
        "\n",
        "1. **Closure** [Abgeschlossenheit](https://de.m.wikipedia.org/wiki/Abgeschlossenheit_(algebraische_Struktur)): If $x$ and $y$ are in $G$ then $x * y$ is in $\\mathrm{G}$\n",
        "  * until here it is a [Magma (Groupoid)](https://de.m.wikipedia.org/wiki/Magma_(Mathematik))\n",
        "  * Beispiele (Magmen, die keine Halbgruppen sind):\n",
        "    * $(\\mathbb{Z},-):$ die ganzen Zahlen mit der Subtraktion\n",
        "    * (R $\\backslash\\{0\\}, /)$ : die reellen Zahlen ungleich 0 mit der Division\n",
        "    * Die nat√ºrlichen Zahlen mit der Exponentiation, also mit der Verkn√ºpfung $a * b=a^{b}$\n",
        "    * Die reellen Zahlen mit der Bildung des arithmetischen Mittels als Verkn√ºpfung\n",
        "\n",
        "2. **Associativity** [Assoziativit√§t](https://de.m.wikipedia.org/wiki/Assoziativgesetz): For all $x, y, z$ in $G$ $\\text { we have }(x * y) * z=x *(y * z)$\n",
        "  * until here it is a [Halbgruppe](https://de.m.wikipedia.org/wiki/Halbgruppe)\n",
        "  * Beispiel: Die Menge $\\mathbb  N$ $_0$ = {0, 1, 2 ..} der nat√ºrlichen Zahlen bildet mit der gew√∂hnlichen Addition eine kommutative und k√ºrzbare Halbgruppe ($\\mathbb  N$ $_0$,+), die keine Gruppe ist. Da hier die negativen Zahlen fehlen, also die ‚ÄûH√§lfte‚Äú der abelschen Gruppe ($\\mathbb Z,+$) der ganzen Zahlen, lag der Name Halbgruppe f√ºr diese mathematische Struktur nahe.\n",
        "\n",
        "3. **Identity Element** [Neutrales Element](https://en.m.wikipedia.org/wiki/Identity_element): There is an element $e$ in $G$ such that $e * x=x * e=x$ for all $x$ in $G$\n",
        "  * until here it is a [Monoid](https://de.m.wikipedia.org/wiki/Monoid)\n",
        "  * Beispiel: die nat√ºrlichen Zahlen mit der Addition und der Zahl 0 als neutralem Element.\n",
        "\n",
        "4. **Inverse Elements** [Inverse Elemente](https://de.m.wikipedia.org/wiki/Inverses_Element): For each element $x$ in $G,$ there is an element $x^{-1}$ such that $x * x^{-1}=x^{-1} * x=e$\n",
        "  * until here it is a [Group](https://de.m.wikipedia.org/wiki/Gruppe_(Mathematik))\n",
        "  * Beispiel: die Menge der ganzen Zahlen zusammen mit der Addition\n",
        "  * Ringe, K√∂rper (Field), Moduln und Vektorr√§ume sind Gruppen mit zus√§tzlichen Strukturen und Eigenschaften\n",
        "\n",
        "5. **Kommutative** [Kommutativit√§t](https://de.m.wikipedia.org/wiki/Kommutativgesetz): F√ºr alle $a, b \\in G$ gilt: $a * b=b * a$\n",
        "  * until here it is an [Abelian Group](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe)\n",
        "  * Beispiel:\n",
        "    * $(\\mathbb {Z} ,+)$ ist die wichtigste abelsche Gruppe. Dabei ist Z die Menge der ganzen Zahlen und + die gew√∂hnliche Addition.\n",
        "    * $(\\mathbb {Q}^{\\cdot} , \\cdot)$ ist eine abelsche Gruppe. Dabei ist $\\mathbb {Q}^{\\cdot}$ die Menge der rationalen Zahlen ohne die 0 und ‚ãÖ ist die gew√∂hnliche Multiplikation. Die Null muss hierbei ausgeschlossen werden, da sie kein inverses Element besitzt: ‚Äû1/0‚Äú ist nicht definiert\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "    * Die Menge der Drehungen in einer Ebene um einen Punkt bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Drehungen.\n",
        "    * Die Menge der Drehstreckungen in einer Ebene bilden eine abelsche Gruppe.\n",
        "    * Die Menge der endlichen Dezimalzahlen sind bez√ºglich der Multiplikation keine abelsche Gruppe. Zum Beispiel hat die Zahl 3 kein Inverses bez√ºglich der Multiplikation. $\\displaystyle {\\frac {1}{3}}$ l√§sst sich nicht als endlicher Dezimalbruch schreiben. Bez√ºglich der normalen Addition bilden die endlichen Dezimalbr√ºche eine abelsche Gruppe.\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verkn√ºpfung ist die Hintereinanderausf√ºhrung der Verschiebungen.\n",
        "  * [Free Abelian Group](https://de.m.wikipedia.org/wiki/Freie_abelsche_Gruppe): eine abelsche Gruppe, die als $\\mathbb {Z}$-Modul eine Basis hat. Ist G eine freie abelsche Gruppe ist, so wird man eine Basis w√§hlen und alle Elemente als Linearkombinationen von Elementen dieser Basis ausdr√ºcken. Allerdings sollte man betonen, dass es meist keine ausgezeichnete Basis geben wird.\n",
        "\n",
        "6. **Distributive law** [Distributivgesetz](https://de.m.wikipedia.org/wiki/Distributivgesetz): a(b + c) = ab + ac.\n",
        "  * Gilt zus√§tzlich Distributivgesetz gilt f√ºr Ring (da man hier zwei Operationen ben√∂tigt). Gruppen haben immer nur eine Operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/groups.png)"
      ],
      "metadata": {
        "id": "y12kc1i6O7GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Group Theory**\n",
        "\n",
        "* Solutions to polynomial equations, like find 2 roots of a quadratic equation. there is also q cubic formula and a quartic for degree 4 polynomial. Find formula to solve degree 5 polynomial - group theory showed it doesnt exist. Has to do with permutation group S5.\n",
        "\n",
        "* Connection to Physics - Noether's theorem: Conservation law - symmetry. Momentum - translation in space. Energy - translation in time.\n",
        "\n",
        "> [Researchers Use Group Theory to Speed Up Algorithms ‚Äî Introduction to Groups](https://www.youtube.com/watch?v=KufsL2VgELo&list=WL&index=3&t=1579s)"
      ],
      "metadata": {
        "id": "wwGvCcMgGFMM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCaLVo_nP2aQ"
      },
      "source": [
        "**Trivial Group (Zero Group)**\n",
        "\n",
        "Die [triviale Gruppe](https://de.m.wikipedia.org/wiki/Triviale_Gruppe) ist in der Gruppentheorie eine Gruppe, deren Tr√§germenge genau ein Element enth√§lt. Die triviale Gruppe ist bis auf Isomorphie eindeutig bestimmt. **Jede Gruppe enth√§lt die triviale Gruppe als Untergruppe**.\n",
        "\n",
        "Die triviale Gruppe $(\\{e\\}, *)$ ist eine Gruppe, die aus der einelementigen Menge $\\{e\\}$ besteht und versehen ist mit der einzig m√∂glichen Gruppenoperation\n",
        "\n",
        "$\n",
        "e * e=e\n",
        "$\n",
        "\n",
        "Das Element $e$ ist damit das **neutrale Element** der Gruppe.\n",
        "\n",
        "Alle trivialen Gruppen sind zueinander isomorph. Beispiele f√ºr triviale Gruppen sind:\n",
        "\n",
        "* die zyklische Gruppe $C_{1}$ vom Grad 1\n",
        "\n",
        "* die alternierende Gruppe $A_{2}$ vom Grad 2\n",
        "\n",
        "* die symmetrische Gruppe $S_{1}$ einer einelementigen Menge\n",
        "\n",
        "*Eigenschaften trivialer Gruppen*:\n",
        "\n",
        "* Da die Gruppenoperation $\\ast$ kommutativ ist, ist die triviale Gruppe eine abelsche Gruppe.\n",
        "\n",
        "* Die einzige Untergruppe der trivialen Gruppe ist die triviale Gruppe selbst.\n",
        "\n",
        "* Die triviale Gruppe wird von der leeren Menge erzeugt:\n",
        "$\\{e\\}=\\langle \\emptyset \\rangle$ . Hierbei ergibt das leere Produkt nach √ºblicher Konvention das neutrale Element.\n",
        "\n",
        "* Jede Gruppe enth√§lt die triviale Gruppe und sich selbst als (triviale) Normalteiler. **Die triviale Gruppe wird daher meistens nicht als einfache Gruppe angesehen**, die aus genau 2 Normalteilern besteht).\n",
        "\n",
        "* In der Kategorie der Gruppen Grp fungiert die triviale Gruppe als Nullobjekt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVFg6eAKyF0"
      },
      "source": [
        "**Simple Group**\n",
        "\n",
        "* [Simple Groups](https://en.wikipedia.org/wiki/Simple_group) bzw. [einfache Gruppen](https://de.wikipedia.org/wiki/Einfache_Gruppe_(Mathematik)) **are the fundamental building blocks of finite groups** (just like Prime numbers are fundamental building blocks in number theory)\n",
        "\n",
        "* Just as you can factor integers into prime numbers, you can break apart some groups into a direct product of simpler groups.\n",
        "\n",
        "*  **a simple group is a <u>nontrivial</u> group whose only normal subgroups are the trivial group and the group itself.**\n",
        "\n",
        "* **Jede Gruppe hat sich selbst und die nur das neutrale Element enthaltende Menge als Normalteiler.**\n",
        "\n",
        "Damit stellt sich die Frage, welche Gruppen keine weitere Normalteiler besitzen. Bei diesen handelt es sich per definitionem gerade um die einfachen Gruppen.\n",
        "\n",
        "  * Eine Gruppe $G$ heisst einfach, falls sie als Normalteiler nur $G$ und $\\{e\\}$ mit dem neutralen Element $e$ hat.\n",
        "  * Au√üerdem wird zusƒÉtzlich $G \\neq\\{e\\}$ gefordert, wonach man knapper sagen kann:\n",
        "  * **Eine Gruppe hei√üt einfach, wenn sie genau zwei Normalteiler besitzt.**\n",
        "\n",
        "* A group that is not simple can be broken into two smaller groups, namely a nontrivial [normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup) and the corresponding quotient group. This process can be repeated, and for finite groups one eventually arrives at uniquely determined simple groups, by the [Jordan‚ÄìH√∂lder theorem (Composition series)](https://en.wikipedia.org/wiki/Composition_series).\n",
        "\n",
        "* The complete classification of finite simple groups, completed in 2004, is a major milestone in the history of mathematics.\n",
        "\n",
        "**Seit 1982 sind die endlichen einfachen Gruppen vollst√§ndig klassifiziert, die Liste besteht aus**\n",
        "\n",
        "* den zyklischen Gruppen von Primzahlordnung,\n",
        "\n",
        "* den alternierenden Gruppen $A_{n}$ mit $n\\geq 5$,\n",
        "\n",
        "* den Gruppen vom Lie-Typ (16 jeweils unendliche Serien)\n",
        "\n",
        "* 26 sporadischen Gruppen (Es handelt sich um die endlichen einfachen Gruppen, die sich nicht in eine der (18) systematischen Familien mit unendlich vielen Mitgliedern (von endlichen einfachen Gruppen) einordnen lassen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSRFOXm2QgM"
      },
      "source": [
        "**Finite Groups**\n",
        "\n",
        "* Eine Gruppe ($G$,*) hei√üt [endliche Gruppe](https://de.m.wikipedia.org/wiki/Endliche_Gruppe), wenn $G$ eine endliche Menge ist, also eine endliche Anzahl von Elementen hat.\n",
        "\n",
        "* Die Annahme der Endlichkeit erm√∂glicht ein vereinfachtes Axiomensystem\n",
        "\n",
        "Ein Paar $(G, *)$ mit einer endlichen Menge $G$ und einer inneren zweistelligen Verkn√ºpfung $*: G \\times G \\rightarrow G$ hei√üt Gruppe, wenn folgende Axiome erf√ºllt sind:\n",
        "\n",
        "* Assoziativit√§t: F√ºr alle Gruppenelemente $a, b, c$ gilt $(a * b) * c=a *(b * c)$,\n",
        "\n",
        "* [K√ºrzungsregel](https://de.m.wikipedia.org/wiki/K√ºrzbarkeit): Aus $a * x=a * x^{\\prime}$ oder $x * a=x^{\\prime} * a$ folgt $x=x^{\\prime}$\n",
        "\n",
        "Aus der K√ºrzungsregel folgt, dass die Links- und Rechtsmultiplikationen $x \\mapsto a * x$ und $x \\mapsto x * a$\n",
        "injektiv sind, woraus wegen der Endlichkeit auch die Surjektivit√§t folgt. Daher gibt es ein $x$ mit\n",
        "$a * x=a,$ was zur Existenz des neutralen Elementes $e$ f√ºhrt, und dann ein $x$ mit $a * x=e$, was\n",
        "die Existenz der inversen Elemente zeigt.\n",
        "\n",
        "* The [List of small groups](https://en.wikipedia.org/wiki/List_of_small_groups) contains finite groups of small [order](https://en.wikipedia.org/wiki/Order_(group_theory)) [up to](https://en.wikipedia.org/wiki/Up_to) [group isomorphism](https://en.wikipedia.org/wiki/Group_isomorphism).\n",
        "\n",
        "* Die folgende Liste enth√§lt eine Auswahl [endlicher Gruppen kleiner Ordnung](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen).\n",
        "\n",
        "  * Diese Liste kann benutzt werden, um herauszufinden, zu welchen bekannten endlichen Gruppen eine Gruppe G isomorph ist.\n",
        "\n",
        "  * Als erstes bestimmt man die Ordnung von G und vergleicht sie mit den unten aufgelisteten Gruppen gleicher Ordnung.\n",
        "\n",
        "  * Ist bekannt, ob G abelsch (kommutativ) ist, so kann man einige Gruppen ausschlie√üen. Anschlie√üend vergleicht man die Ordnung einzelner Elemente von G mit den Elementen der aufgelisteten Gruppen, wodurch man G bis auf Isomorphie eindeutig bestimmen kann.\n",
        "\n",
        "In der nachfolgenden Liste werden folgende Bezeichnungen verwendet:\n",
        "\n",
        "- $\\mathbb{Z}_{n}$ ist die zyklische Gruppe der Ordnung $n$ (die auch als $C_{n}$ oder $\\mathbb{Z} / n \\mathbb{Z}$ geschrieben wird).\n",
        "\n",
        "- $D_{n}$ ist die Diedergruppe der Ordnung $2 n$.\n",
        "\n",
        "- $S_{n}$ ist die symmetrische Gruppe vom Grad $n$, mit $n !$ Permutationen von $n$ Elementen.\n",
        "\n",
        "- $A_{n}$ ist die alternierende Gruppe vom Grad $n$, mit $n ! / 2$ Permutationen von $n$ Elementen f√ºr $n \\geq 2$.\n",
        "\n",
        "- Dic $_{n}$ ist die dizyklische Gruppe der Ordnung $4 n$.\n",
        "\n",
        "- $V_{4}$ ist die [Klein'sche Vierergruppe](https://de.m.wikipedia.org/wiki/Kleinsche_Vierergruppe) der Ordnung $4 .$\n",
        "\n",
        "- $Q_{4 n}$ ist die Quaternionengruppe der Ordnung $4 n$ fur $n \\geq 2$.\n",
        "\n",
        "[Liste aller Gruppen bis Ordnung 20](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen#Liste_aller_Gruppen_bis_Ordnung_20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN_XWVc0s5p"
      },
      "source": [
        "**Finite Simple Groups**\n",
        "\n",
        "* [Endliche einfache Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe) gelten in der Gruppentheorie als die Bausteine der [endlichen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_Gruppe).\n",
        "\n",
        "* Die endlichen einfachen Gruppen spielen f√ºr die endlichen Gruppen eine √§hnliche Rolle wie die Primzahlen f√ºr die nat√ºrlichen Zahlen: Jede endliche Gruppe l√§sst sich in ihre einfachen Gruppen ‚Äûzerteilen‚Äú (f√ºr die Art der Eindeutigkeit siehe den Satz von Jordan-H√∂lder).\n",
        "\n",
        "* Die Rekonstruktion einer endlichen Gruppe aus diesen ihren ‚ÄûFaktoren‚Äú ist aber nicht eindeutig.\n",
        "\n",
        "* Es gibt jedoch keine ‚Äûnoch einfacheren Gruppen‚Äú, aus denen sich die endlichen einfachen Gruppen konstruieren lassen.\n",
        "\n",
        "Obwohl die endlichen einfachen Gruppen seit 1982 als vollst√§ndig klassifiziert galten, schlossen Mathematiker um Aschbacher die Klassifikation erst im Jahre 2002 mit einem 1200 Seiten langen Beweis ab:\n",
        "\n",
        "* Fast alle dieser Gruppen lassen sich einer von 18 Familien endlicher einfacher Gruppen zuordnen.\n",
        "\n",
        "* Es existieren 26 Ausnahmen. Diese Gruppen werden als **sporadische Gruppen** bezeichnet (Zu den sporadischen Gruppen z√§hlen die Conway-Gruppe, das Babymonster und die [**Monstergruppe**](\n",
        "https://de.m.wikipedia.org/wiki/Monstergruppe) (mit fast 1054 Elementen die gr√∂√üte sporadische Gruppe).\n",
        "\n",
        "* Die [sporadischen Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe) sind 26 spezielle Gruppen in der Gruppentheorie. Es handelt sich um die [endlichen einfachen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe), die sich nicht in eine der [(18) systematischen Familien mit unendlich vielen Mitgliedern](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Familien_endlicher_einfacher_Gruppen) (von endlichen einfachen Gruppen) einordnen lassen.\n",
        "\n",
        "> https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/\n",
        "\n",
        "\n",
        "*Klassifikation der endlichen einfachen Gruppe*\n",
        "\n",
        "Die endlichen einfachen Gruppen [lassen sich einteilen in](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Klassifikation) bzw [Classification of finite simple groups](https://en.m.wikipedia.org/wiki/Classification_of_finite_simple_groups), Every finite simple group is isomorphic to one of the following groups:\n",
        "\n",
        "* a member of one of three infinite classes of such, namely:\n",
        "\n",
        "  * (1) [zyklische Gruppen](https://de.m.wikipedia.org/wiki/Zyklische_Gruppe) von Primzahlordnung,\n",
        "\n",
        "  * (1) [alternierende Gruppen](https://de.m.wikipedia.org/wiki/Alternierende_Gruppe) $A_{n}$ mit $n>4$,\n",
        "\n",
        "  * (16) [Gruppen vom Lie-Typ](https://de.m.wikipedia.org/wiki/Gruppe_vom_Lie-Typ) √ºber einem [endlichen K√∂rper](https://de.m.wikipedia.org/wiki/Endlicher_K√∂rper) (16 jeweils unendliche Familien),\n",
        "\n",
        "* (26) one of 26 groups called the \"sporadic groups\" / [26 sporadische Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe).\n",
        "\n",
        "* (1) the [Tits group](https://en.m.wikipedia.org/wiki/Tits_group) (which is sometimes considered a 27th sporadic group)\n",
        "\n",
        "\n",
        "[2004: Classification of Quasithin group](https://en.m.wikipedia.org/wiki/Quasithin_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Structures (Moduln, Ring, Field)*"
      ],
      "metadata": {
        "id": "KHUTVWQOmCm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/%C3%9Cbersicht_K%C3%B6rper.svg/775px-%C3%9Cbersicht_K%C3%B6rper.svg.png)"
      ],
      "metadata": {
        "id": "MdAk53GrsPJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Modul](\n",
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)) ist ein n-dimensionaler Ring.**\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativit√§t)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where ‚Äúscalar multiplication‚Äù refers to multiplication by integer scalars.\n",
        "\n",
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$\n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$\n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ],
      "metadata": {
        "id": "xPGuj4OPrOPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Ring](https://en.m.wikipedia.org/wiki/Ring_theory) ist eine Menge R mit <u>zwei inneren bin√§ren Verkn√ºpfungen</u> ‚Äû+‚Äú und ‚Äû‚àô‚Äú, sodass gilt:**\n",
        "\n",
        "1. **Addition: (R, +) ist eine abelsche Gruppe**\n",
        "\n",
        "* Addition is associative and commutative;\n",
        "\n",
        "* There is an additive identity, zero;\n",
        "\n",
        "* Every element has an additive inverse;\n",
        "\n",
        "2. **Multiplikation: (R, ‚àô) ist eine Halbgruppe**, das bedeutet:\n",
        "\n",
        "* Halbgruppe in der Multiplikation im Ring: **nur nur die Assoziativit√§t, aber keine Inverse, neutrales element oder kommutativit√§t**)\n",
        "\n",
        "> **Das bedeutet: -> Sowohl Ringe als auch K√∂rper verlangen, dass bzgl. der Addition eine kommutative Gruppe vorliegt (abelsch!). Bei der Multiplikation erfolgt der √úbergang vom Ring zum K√∂rper durch die Versch√§rfung der Forderungen**\n",
        "\n",
        "* Unlike a field, a ring is not required to have multiplicative inverses, and the multiplication is not required to be commutative.\n",
        "\n",
        "> **A good example of a ring is the set of all n√ón matrices under the operations of matrix addition and matrix multiplication.** [Matrix-Multiplication is non-commutative!](https://en.m.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity)\n",
        "\n",
        "$\\mathbf{A B} \\neq \\mathbf{B A}$\n",
        "For example\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "but\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "> **The integers Z also form a ring under the operations of addition and multiplication.**\n",
        "\n",
        "3. **Die Distributivgesetze a*(b+c)=a*b+a*c und (a+b)*c = a*c+b*c sind f√ºr alle a,b,c Œµ $R$ erf√ºllt.**\n",
        "\n",
        "4. **Das neutrale Element 0 von (R, +) hei√üt Nullelement von R.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Ring_(Algebra)\n",
        "\n",
        "**Ein Ring hei√üt kommutativ**, falls er bez√ºglich der Multiplikation kommutativ ist (Ein Ring hei√üt kommutativ, falls er bez√ºglich der Multiplikation kommutativ ist, ansonsten spricht man von einem nicht-kommutativen Ring.)\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* 2√ó2 Real matrices.\n",
        "\n",
        "* Das wichtigste Beispiel eines Ringes sind die Integers / ist die Menge (Ùè∞Å$\\mathbb Z$,+,‚àô) der ganzen Zahlen mit der √ºblichen Addition und Multiplikation. Es handelt sich dabei um einen nullteilerfreien kommutativen Ring mit Einselement, also einen Integrit√§tsring.\n",
        "\n",
        "* the Integers modulo some Natural number greater than one;\n",
        "\n",
        "* Ebenso bildet ($\\mathbb Q$,+,‚àô) der rationalen Zahlen mit der √ºblichen Addition und Multiplikation einen Ring. Da in diesem Fall nicht nur ($\\mathbb Q$,+), sondern auch ($\\mathbb Q$ \\ {0},‚àô) eine abelsche Gruppe bildet, liegt sogar ein K√∂rper vor; es handelt sich dabei um den Quotientenk√∂rper des Integrit√§tsringes (Ùè∞Å$\\mathbb Z$,+,‚àô).\n",
        "\n",
        "* Kein Ring ist die Menge ($\\mathbb N$Ùè∞Ä,+,‚àô) der nat√ºrlichen Zahlen mit der √ºblichen Addition und Multiplikation, da die Addition √ºber den nat√ºrlichen Zahlen nicht invertierbar ist.\n",
        "\n",
        "https://www.quora.com/What-are-the-differences-between-rings-and-fields"
      ],
      "metadata": {
        "id": "usbC2QwNr7Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [K√∂rper (Field)](https://de.m.wikipedia.org/wiki/K%C3%B6rper_(Algebra)) ist eine spezielle Form von Ring**:\n",
        "\n",
        "> Man nennt die Elemente im K√∂rper nicht Vektoren, sondern Skalare. √úber dem Skalark√∂rper betrachtet man einen Vektorraum und dessen Elemente hei√üen Vektoren.\n",
        "\n",
        "* **A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)**\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, hei√üt ein K√∂rper, wenn in ihm jedes von Null verschiedene Element multiplikativ invertierbar ist.\n",
        "Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring K, in dem die Einheitengruppe K* gleich K \\ {0}, also maximal gro√ü, ist.\n",
        "\n",
        "* Ein kommutativer unit√§rer Ring, der nicht der Nullring ist, ist ein K√∂rper, wenn in ihm jedes von Null verschiedene Element ein Inverses bez√ºglich der Multiplikation besitzt. Anders formuliert, ist ein K√∂rper ein kommutativer unit√§rer Ring $K$, in dem die Einheitengruppe $K^{*}$ gleich $K \\backslash\\{0\\}$ ist.\n",
        "\n",
        "\n",
        "Ein Tripel (K,+,‚Ä¢), bestehend aus einer Menge K und zwei bin√§ren Verkn√ºpfungen ‚Äû+‚Äú und ‚Äû‚Ä¢‚Äú (die √ºblicherweise Addition und Multiplikation genannt werden), ist genau dann ein K√∂rper, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "* $(K,+)$ ist eine abelsche Gruppe (mit Neutralelement 0)\n",
        "\n",
        "* $(K \\backslash\\{0\\}, ‚Ä¢)$ ist eine abelsche Gruppe (mit Neutralelement 1)\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ und $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ (Distributivgesetz)\n",
        "\n",
        "Additive Eigenschaften:\n",
        "\n",
        "* $a+(b+c)=(a+b)+c$ (Assoziativgesetz)\n",
        "\n",
        "* $a+b=b+a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $0 \\in K$ mit $0+a=a$ (neutrales Element)\n",
        "\n",
        "* Zu jedem $a \\in K$ existiert das additive Inverse $(-a)$ mit $(-a)+a=0$\n",
        "\n",
        "Multiplikative Eigenschaften:\n",
        "\n",
        "* $\\cdot a \\cdot(b \\cdot c)=(a \\cdot b) \\cdot c$ (Assoziativgesetz)\n",
        "\n",
        "* $a \\cdot b=b \\cdot a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $1 \\in K$ mit $1 \\cdot a=a$ (neutrales Element), und es ist $1 \\neq 0$.\n",
        "\n",
        "* Zu jedem $a \\in K \\backslash\\{0\\}$ existiert das multiplikative Inverse $a^{-1}$ mit $a^{-1} \\cdot a=1$\n",
        "\n",
        "Zusammenspiel von additiver und multiplikativer Struktur:\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ (Links-Distributivgesetz)\n",
        "\n",
        "* Das Rechts-Distributivgesetz $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ folgt dann aus den √ºbrigen Eigenschaften:\n",
        "$(a+b) \\cdot c=c \\cdot(a+b)=c \\cdot a+c \\cdot b=a \\cdot c+b \\cdot c$\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* The most familiar form of algebra is the elementary algebra that you learned in high school, namely the algebra of the real numbers. From an abstract point of view, this is the algebra of fields.\n",
        "\n",
        "* Note that the axioms for a field are precisely the axioms for algebra on the real numbers. As a result, the real numbers R form a field under the usual operations of addition and multiplication. However, the real numbers are not the only possible field. Indeed, you are already familiar with a few other examples:\n",
        "\n",
        "* set of rational numbers under addition and multiplication. The rational numbers Q form a field under the usual operations of addition and multiplication. In particular, we can add or multiply two elements of Q to obtain another element of Q, and these operations obey all of the axioms listed above.\n",
        "\n",
        "* The complex numbers C form a field under the commonly defined operations of addition and multiplication. Complex numbers do obey all of the listed axioms for a field, which is why elementary algebra works as usual for complex numbers.\n",
        "\n",
        "* The Integers modulo a Prime number.\n",
        "\n",
        "*An example of a set of numbers that is **not a field** is the set of integers. It is an \"integral domain.\" It is not a field because it lacks multiplicative inverses. Without multiplicative inverses, division may be impossible.*\n",
        "\n",
        "* Both are algebraic objects with a notion of addition and multiplication, **but the multiplication in a field is more specialized**: it is necessarily commutative and every nonzero element has a multiplicative inverse.\n"
      ],
      "metadata": {
        "id": "j07gGLGUtEU6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxw9DSyjiah"
      },
      "source": [
        "**Siehe auch**\n",
        "\n",
        "* [Algebraische Struktur](https://de.m.wikipedia.org/wiki/Algebraische_Struktur)\n",
        "\n",
        "* [Outline_of_algebraic_structures](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures)\n",
        "\n",
        "* [Mathematische Strukturen](https://de.m.wikipedia.org/wiki/Mathematische_Struktur) (neben topologischen Strukturen, geometrischen Strukturen und Zahlbereichen)\n",
        "\n",
        "* [Geometrische Gruppentheorie](https://de.wikipedia.org/wiki/Geometrische_Gruppentheorie) (Gruppenoperationen auf Graphen und metrischen R√§umen, letztlich werden die Gruppen selbst zu solchen geometrischen Objekten)\n",
        "\n",
        "* [Verkn√ºpfungen](https://de.m.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik))\n",
        "\n",
        "* [Gruppenoperation](https://de.wikipedia.org/wiki/Gruppenoperation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Icosahedral Symmetry](https://youtu.be/B-WI8JZR140)"
      ],
      "metadata": {
        "id": "ibeXZKTToqNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Homomorphism*"
      ],
      "metadata": {
        "id": "iqHduf9WDZMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Homomorphiesatz (Isomorphie on quotient group)\n",
        "\n",
        "Video: [First Isomorphism Theorem for Groups](https://www.youtube.com/watch?v=JiS43Twomsk&list=WL&index=10)"
      ],
      "metadata": {
        "id": "M8aSZ3c541kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Persistent Homology](https://youtu.be/ktKCzMmDXDk)\n",
        "\n",
        "Video: [Chapter 6: Homomorphism and (first) isomorphism theorem | Essence of Group Theory](https://youtu.be/2kmIHyD8zTk)\n",
        "\n",
        "Kern is a normal subgroup (normalteiler) von H, weil Rest is 0. alles andere sind cosets (mit jeweils rest 1,2,3, etc nachdem was der normalteiler ist)"
      ],
      "metadata": {
        "id": "Z8eBrD-9qS_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Monad_(linear_algebra)"
      ],
      "metadata": {
        "id": "8QS1Q_s8JBFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cantorsparadise.com/an-intro-to-topology-9e0478313b63"
      ],
      "metadata": {
        "id": "6hBdWGAqr4lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence (simplicial homology). Ist das die normal subgroup (normal subgroup)??\n",
        "\n",
        "Der Kern von f ist stets ein Normalteiler von G und das Bild von f ist eine Untergruppe von H. Nach dem Homomorphiesatz ist die Faktorgruppe G/Kern(f) isomorph zu Bild (f)."
      ],
      "metadata": {
        "id": "R4CVKRmNV-E5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35B6lF4hR7zq"
      },
      "source": [
        "**Homomorphiesatz (Fundamental Theorem on Homomorphism)**\n",
        "\n",
        "* [Homomorphiesatz](https://de.m.wikipedia.org/wiki/Homomorphiesatz)\n",
        "\n",
        "* https://youtu.be/QA9rrDMlaHc (Homomorphiesatz mit Hasen und Jaegern)\n",
        "\n",
        "* https://youtu.be/390eRzVSC2k (Homomorphie mit Modulo und kommutativen Diagramm)\n",
        "\n",
        "**Homomorphiesatz (allgemein)**:\n",
        "\n",
        "* Aus einer Abbildung $f$ zwischen zwei Gruppen $G$ und $H$, die weder injektiv noch surjektiv ist, wollen wir eine bijektive Abbildung herleiten.\n",
        "\n",
        "* Schritt 1: Das macht man, indem man zuerst auf der rechten Seite alle Elemente ausschliesst, die nicht Teil der Zielmenge sind, und sich nur auf die 'getroffenen' Elemente fokussiert (The image of $f$ is hierbei a subgroup of $H$.)\n",
        "\n",
        "* Schritt 2: Jetzt hat man nur noch das Problem auf der linken Seite, dass mehrere Startelemente in $G$ auf ein und dasselbe Zielelement in $H$ verweisen. Man betrachtet die Startelemente dann einfach als identisch. Das macht man in dem man $G$ umwandelt in $G$ / Kern ($f$) (man bildet die Faktorgruppe, und spricht aus: modulo Kern von f. Der Quotientenvektorraum von $G$ nach kern von $f$.\n",
        "  * Das geht, weil $f$ ein Homomorphismus ist\n",
        "  * Das bedeutet, wenn zwei Elemente a und b im Start auf ein Element im Ziel verweisen, dann unterscheiden sie sich um ein Kernelement. Heisst, a minus b ist ein Element, das auf 0 geschickt wird, und damit ein Kernelement.\n",
        "  * Und wenn ich jetzt modulo des kerns rechne, dann tue ich so, als ob es die Differenz nicht gibt. Weil es bedeutet a =b. Der Quotientenraum ist ein kunstlich geschaffener Raum, wo a und b identisch gemacht wurden (kongruent).\n",
        "  * Remember aus Modulorechnung: Zwei Zahlen sind kongruent (modulo des Moduls m), wenn ihre Differenz durch m teilbar ist. Hier ist das Modulo der Kern. Also Rest muss 0 sein.\n",
        "  * Modulo n (Reste berechnen, hierbei 0): die Differenz zweier Elemente ist teilbar durch n. Die beiden Elemente sind dann kongruent (identisch).\n",
        "  * $G$ modulo Kern $f$ ist isomorph (identisch =bijektiv und homomorph)) zu Bild $f$, das in $H$ liegt.\n",
        "\n",
        "* **Der Kern von $f$ ist stets ein Normalteiler von $G$ und das Bild von $f$ ist eine Untergruppe von $H$. Nach dem Homomorphiesatz ist die Faktorgruppe $G / \\operatorname{Kern}(f)$ [isomorph (bijektiv)](https://de.m.wikipedia.org/wiki/Isomorphismus) zu Bild $(f)$.**\n",
        "\n",
        "**Bedingungen:**\n",
        "\n",
        "* Let $G$ and $H$ be two groups.\n",
        "* and let $f$ : $G \\rightarrow H$ be a [group homomorphism](https://de.wikipedia.org/wiki/Gruppenhomomorphismus).\n",
        "* and let $K$ be a normal subgroup (Normalteiler) in $G$ and $\\varphi$ the natural surjective homomorphism $G \\rightarrow G / K$ (where $G / K$ is a quotient group). Diese Faktorgruppen sind homomorphe Bilder von G und **jedes homomorphe Bild von G ist zu einer solchen Faktorgruppe G/K isomorph**.\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/homomorphy.jpg)\n",
        "\n",
        "**Then:**\n",
        "\n",
        "1. **Dann ist der Kern von $f$ ein Normalteiler von $G$.**\n",
        "  * Normalteiler sind die [Kerne](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) von Gruppenhomomorphismen, weshalb dann klar ist, dass umgekehrt der Kern von $f$ ein Normalteiler von $G$ ist.\n",
        "\n",
        "  * If $K$ is a **subset** of ker $(f)$ then there exists a unique homomorphism $h: G / K \\rightarrow H$ such that $f=h$ $\\varphi$. In other words, the natural projection $\\varphi$ is universal among homomorphisms on $G$ that map $K$ to the identity element.\n",
        "\n",
        "2. **und daher kann die Faktorgruppe $G /$ ker $f$ gebildet werden.**\n",
        "\n",
        "3. **Nach dem [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) ist diese Faktorgruppe $G /$ ker $f$ isomorph zum Bild von $f$, das eine Untergruppe von $H$ ist.**\n",
        "  * The image of $f$ is isomorphic to the quotient group $G /$ ker ($f$). And in particular, if $f$ is surjective then $H$ is isomorphic to $G$ / ker $(f)$. [Source](https://en.m.wikipedia.org/wiki/Isomorphism_theorems#First_Isomorphism_Theorem_4)\n",
        "  * The image of $f$ is hierbei a subgroup of $H$.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Group_homomorphism_ver.2.svg/500px-Group_homomorphism_ver.2.svg.png)\n",
        "\n",
        "*Image of a group homomorphism (h) from G (left) to H (right).*\n",
        "\n",
        "*The smaller oval inside H is the image of h. N is the kernel of h and aN is a coset of N.* [Source](https://en.m.wikipedia.org/wiki/Group_homomorphism)\n",
        "\n",
        "See also: https://mathepedia.de/Kern_und_Bild_Homomorphismus.html\n",
        "\n",
        "*Diagram of the fundamental theorem on homomorphisms where f is a homomorphism, N is a normal subgroup of G and e is the identity element of G.*\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Diagram_of_the_fundamental_theorem_on_homomorphisms.svg/440px-Diagram_of_the_fundamental_theorem_on_homomorphisms.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Category Theory*"
      ],
      "metadata": {
        "id": "jNQs-MXfBmVV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eLPSGAeDW0j"
      },
      "source": [
        "[Category](https://en.m.wikipedia.org/wiki/Category_(mathematics)) and [Category theory](https://en.m.wikipedia.org/wiki/Category_theory)\n",
        "\n",
        "* Eine Kategorie besteht aus Objekten und Morphismen. Man m√∂chte jedem Objekt in einer Kategorie ein Objekt in der anderen Kategorie zuordnen, und das gleiche mit den Morphismen zwischen den Objekten.\n",
        "\n",
        "* [Objects](https://ncatlab.org/nlab/show/object)\n",
        "\n",
        "* [Product und Coproduct](https://de.m.wikipedia.org/wiki/Produkt_und_Koprodukt)\n",
        "\n",
        "* [Anfangsobjekt (initiales), Endobjekt (terminales, finales) und Nullobjekt](https://de.m.wikipedia.org/wiki/Anfangsobjekt,_Endobjekt_und_Nullobjekt)\n",
        "\n",
        "* [Functor](\n",
        "https://ncatlab.org/nlab/show/functor): A homomorphism between categories is a functor. Zuordnung zwischen zwei Kategorien\n",
        "\n",
        "  * Funktoren werden auch Diagramme genannt (mitunter nur in bestimmten Kontexten), da sie eine formale Abstraktion [kommutativer Diagramme](https://de.m.wikipedia.org/wiki/Kommutatives_Diagramm) darstellen.\n",
        "\n",
        "  * functors must preserve [identity morphisms](https://en.m.wikipedia.org/wiki/Morphism#Definition) and [composition of morphisms](https://en.m.wikipedia.org/wiki/Function_composition)\n",
        "\n",
        "* [Natural Transformations](https://en.m.wikipedia.org/wiki/Natural_transformation) are Maps between functors (and functors are maps between categories)\n",
        "\n",
        "* [Duality](https://en.m.wikipedia.org/wiki/Dual_(category_theory)) is a correspondence between the properties of a category C and the dual properties of the opposite category C<sup>op</sup>.\n",
        "\n",
        "* Topos: Category theorists have proposed [topos theory](https://en.m.wikipedia.org/wiki/Topos) as an alternative to traditional [axiomatic set theory](https://de.m.wikipedia.org/wiki/Axiomatische_Mengenlehre). Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory.\n",
        "\n",
        "> [Outline of category theory](https://en.m.wikipedia.org/wiki/Outline_of_category_theory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlwPUfSMCiu"
      },
      "source": [
        "**Category Theory and Higher Category Theory**\n",
        "\n",
        "0) A category is a collection of objects and morphisms between those objects that satisfy some rules.\n",
        "\n",
        "1) A functor is a morphism in the category of categories.\n",
        "\n",
        "2) A natural transformation is a morphism in the category of functors.\n",
        "\n",
        "But they all stop right there. What about:\n",
        "\n",
        "3) the morphisms in the category of natural transformations?\n",
        "\n",
        "4) Or the \"morphisms in the category of the morphisms in the category of natural transformations\"\n",
        "\n",
        "*Definition*\n",
        "\n",
        "* [higher category theory](https://en.m.wikipedia.org/wiki/Higher_category_theory) bzw. [higher category theory](https://ncatlab.org/nlab/show/higher+category+theory) is the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities.\n",
        "\n",
        "* Higher category theory is often applied in algebraic topology (especially in homotopy theory), where one studies algebraic invariants of spaces, such as their fundamental weak ‚àû-groupoid.\n",
        "\n",
        "* *From 2-category to Higher Order Category*: The concept of 2-category generalizes further in higher category theory to n-categories, which have k-morphisms for all\n",
        "k\n",
        "‚â§\n",
        "n\n",
        ". The morphisms can be composed along the objects, while the 2-morphisms can be composed in two different directions: along objects ‚Äì called horizontal composition ‚Äì and along morphisms ‚Äì called vertical composition. The composition of morphisms is allowed to be associative only up to coherent associator 2-morphisms.\n",
        "\n",
        "* See also: [infinity-category](https://ncatlab.org/nlab/show/infinity-category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsmc7LDx_K-"
      },
      "source": [
        "*Morphismen (Linear Maps)*\n",
        "\n",
        "Ein [Morphismus](https://de.m.wikipedia.org/wiki/Morphismus) ist eine Funktion in Kategorientheorie. Man schreibt: $f\\colon X\\to Y$. Image Source: [Morphismen](https://youtu.be/0wKsFNLR15g)\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "* [Automorphismus](https://de.m.wikipedia.org/wiki/Automorphismus): bijektiv, Definitionsmenge = Zielmenge\n",
        "\n",
        "* [Isomorphismus](https://de.m.wikipedia.org/wiki/Isomorphismus): bijektiv, Definitionsmenge ‚â† Zielmenge\n",
        "\n",
        "* [Endomorphismus](https://de.m.wikipedia.org/wiki/Endomorphismus): Definitionsmenge = Zielmenge. aber Bildmenge umfasst nicht den ganzen Vektorraum (Ziele < Uspr√ºnge)\n",
        "\n",
        "* [Monomorphismus](https://de.m.wikipedia.org/wiki/Monomorphismus): injektiv (jedes Element des Usprungs hat ein exklusives Element)\n",
        "\n",
        "* [Epimorphismus](https://de.m.wikipedia.org/wiki/Epimorphismus): surjektiv (jedes Element im Ziel ist mind 1 mal getroffen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNkozbcHgYo"
      },
      "source": [
        "**Deep Dive: Homomorphismus**\n",
        "\n",
        "> Eine lineare Abbildung ist ein (Homo-)Morphismus zwischen Vektorr√§umen.\n",
        "\n",
        "*Unterschied zwischen Isomorphismus und Homomorphismus:*\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/isomorphismus.JPG)\n",
        "\n",
        "Ein [Vektorraum-Homomorphismus](https://de.m.wikipedia.org/wiki/Homomorphismus) (\"Lineare Abbildung\") ist eine Abbildung $\\varphi: V \\rightarrow W$ zwischen $K$ -Vektorr√§umen $V$ und $W$ (gemeinsamer Grundk√∂rper $K$) mit den folgenden Eigenschaften:\n",
        "\n",
        "\n",
        "1. **Additivit√§t**: $\\varphi(v+u)=\\varphi(v)+\\varphi(u)$ f√ºr alle $u$ und $v \\in V$\n",
        "\n",
        "2. **Homogenit√§t**: $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v) \\quad$ f√ºr alle $\\lambda \\in K$ und $v \\in V$ (Skalarmultiplikation)\n",
        "\n",
        "* Beispiel: Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt f√ºr die Multiplikation mit einem Skalar aus dem Grundk√∂rper.\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung unendlichdimensionaler Vektorr√§ume, die eine Topologie tragen, spricht man meist von linearen Operatoren statt von [linearen Abbildungen](https://de.m.wikipedia.org/wiki/Lineare_Abbildung).\n",
        "\n",
        "\n",
        "![xyz](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Reflection_of_a_triangle_about_the_y_axis.svg/320px-Reflection_of_a_triangle_about_the_y_axis.svg.png)\n",
        "\n",
        "*Achsenspiegelung als Beispiel einer linearen Abbildung*\n",
        "\n",
        "**Beispiele f√ºr Vektorraumhomomorphismus**\n",
        "\n",
        "* F√ºr $V=W=\\mathbb{R}$ hat jede lineare Abbildung die Gestalt $f(x)=m x$ mit $m \\in \\mathbb{R}$\n",
        "\n",
        "* Es sei $V=\\mathbb{R}^{n}$ und $W=\\mathbb{R}^{m}$. Dann wird f√ºr jede $m \\times n$ -Matrix $A$ mit Hilfe der Matrizenmultiplikation eine lineare Abbildung $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ durch\n",
        "\n",
        "$f(x)=A x=\\left(\\begin{array}{ccc}a_{11} & \\cdots & a_{1 n} \\\\ \\vdots & & \\vdots \\\\ a_{m 1} & \\cdots & a_{m n}\\end{array}\\right)\\left(\\begin{array}{c}x_{1} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right)$\n",
        "definiert.\n",
        "\n",
        "Jede lineare Abbildung von $\\mathbb{R}^{n}$ nach $\\mathbb{R}^{m}$ kann so dargestellt werden.\n",
        "\n",
        "Siehe auch: [Gruppenhomomorphismus](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Morphismen:**\n",
        "\n",
        "F√ºr manche Kategorien gibt es besondere Bezeichnungen f√ºr Morphismen.\n",
        "\n",
        "* Ein [Hom√∂omorphismus](https://de.m.wikipedia.org/wiki/Hom√∂omorphismus) ist ein Isomorphismus zwischen topologischen R√§umen. Sind (beispielsweise) die [Fundamentalgruppen](https://de.m.wikipedia.org/wiki/Fundamentalgruppe) zweier R√§ume isomorph, so sind die R√§ume hom√∂omorph.\n",
        "\n",
        "* Ein [Diffeomorphismus](https://de.m.wikipedia.org/wiki/Diffeomorphismus) ist ein Isomorphismus zwischen differenzierbaren Mannigfaltigkeiten.\n",
        "\n",
        "* Eine [Isometrie](https://de.m.wikipedia.org/wiki/Isometrie) ist ein Isomorphismus in der Kategorie der metrischen R√§umen mit den nichtexpansiven stetigen Abbildungen."
      ],
      "metadata": {
        "id": "p_vf2S1FOc2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *TDA*"
      ],
      "metadata": {
        "id": "Nror5UBfDJ3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAh4XJMHLDp9"
      },
      "source": [
        "**Persistent Barcode, Diagram & Landscape**\n",
        "\n",
        "* What is the ideal size of d? - Consider all distances d between d1 min (to connect two balls) and d2 max size (all are connected). Each hole appears at a particular value of d and disappears at anotjer value of d. We can represent the persistence of this hole as a pair (d1, d2).\n",
        "* Out of this distance we get a bar. Several holes result in a barcode. Short bars represent noise. Long bars are features.\n",
        "* Persistent barcodes are stable with respect to pertubations if data (Edelsbrunner 2007).\n",
        "* Barcode is computable via linear algebra. Runtime is O (n3), it‚Äòs cubic, where n is the number of simplices (Carlsson 2005).\n",
        "* A barcode is a visualization of an algebraic structure.\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_06.jpg)\n",
        "\n",
        "> **Homology of a simplicial complex is computable via linear algebra**\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_07.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pjdD_O2saB"
      },
      "source": [
        "**Topological Data Analysis + Machine Learning**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/tda_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Lie Algebra*"
      ],
      "metadata": {
        "id": "Q-zfzmbO4ZOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lie Groups Based Machine Learning\n",
        "- Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems\n",
        "https://lnkd.in/e-r-ChbX\n",
        "- Structure preserving deep learning\n",
        "https://lnkd.in/gPghU6x\n",
        "- Lie Group Cohomology and (Multi)Symplectic Integrators: New Geometric Tools for Lie Group Machine Learning Based on Souriau Geometric Statistical Mechanics\n",
        "https://lnkd.in/dCPaWBU\n",
        "\n",
        "more information at SEE GSI'23:\n",
        "https://gsi2023.org/\n"
      ],
      "metadata": {
        "id": "WQFsTGz04fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From: [QHack 2022: Marco Cerezo ‚ÄîBarren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)"
      ],
      "metadata": {
        "id": "imfDlpfHFb7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Theory**\n",
        "\n",
        "> ‚ÄúThe essential phenomenon of Lie theory is that one may associate in a natural way to a Lie group $\\mathcal{G}$ its Lie algebra $\\mathfrak{g}$. The Lie algebra $\\mathfrak{g}$ is first of all a vector space and secondly is endowed with a bilinear nonassociative product called the Lie bracket [...]. **Amazingly, the group $\\mathcal{G}$ is almost completely determined by $\\mathfrak{g}$ and its Lie bracket**. Thus for many purposes <font color=\"blue\">**one can replace $\\mathcal{G}$ with $\\mathfrak{g}$. Since $\\mathcal{G}$ is a complicated nonlinear object and $\\mathfrak{g}$ is just a vector space, it is usually vastly simpler to work with $\\mathfrak{g}$**</font>. [...] This is one source of the power of Lie theory.\" Stillwell: ‚Äúthe miracle of Lie theory‚Äù. (*https://arxiv.org/pdf/1812.01537.pdf*)\n",
        "\n",
        "* Article about [Lie-Gruppe](https://de.m.wikipedia.org/wiki/Lie-Gruppe)\n",
        "\n",
        "* See also [Lie algebra representation](https://en.m.wikipedia.org/wiki/Lie_algebra_representation) and [representation of a Lie group](https://en.m.wikipedia.org/wiki/Representation_of_a_Lie_group) (a linear action of a Lie group on a vector space). Representations play an important role in the study of continuous symmetry.\n",
        "\n",
        "* See this very good paper: [A micro Lie theory for state estimation in robotics](https://arxiv.org/abs/1812.01537)\n",
        "\n",
        "* Lie theory for the roboticist: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "* https://github.com/artivis/manif/blob/devel/paper/Lie_theory_cheat_sheet.pdf"
      ],
      "metadata": {
        "id": "NoSEZYMD4ijj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Group**\n",
        "\n",
        "* A [Lie group](https://en.wikipedia.org/wiki/Lie_group) is a group that is also a **differentiable manifold**. A manifold is a space that locally resembles Euclidean space, whereas groups define the abstract, generic concept of **multiplication and the taking of inverses (division)**. Combining these two ideas, one obtains a continuous group where points can be multiplied together, and their inverse can be taken.\n",
        "\n",
        "* Lie groups provide a natural model for the concept of **continuous symmetry**, a celebrated example of which is the rotational symmetry in three dimensions (given by the special orthogonal group ${\\text{SO}}(3)$).\n",
        "\n",
        "* Lie groups were first found by studying matrix subgroups\n",
        "$G$ contained in ${\\text{GL}}_{n}(\\mathbb {R} )$ or ${\\text{GL}}_{n}(\\mathbb {C} )$, the groups of $n\\times n$ invertible matrices over $\\mathbb {R}$  or $\\mathbb {C}$ (now called the classical groups).\n",
        "\n",
        "* Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations."
      ],
      "metadata": {
        "id": "1a43uSa04kxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of Lie groups**\n",
        "\n",
        "> [Table of some common Lie groups and their associated Lie algebras](https://en.m.wikipedia.org/wiki/Table_of_Lie_groups)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_01.png)\n",
        "\n",
        "* **Der Einheitskreis in der komplexen Zahlenebene**, d. h. die Menge $S^{1}=\\{z \\in \\mathbb{C}:|z|=1\\}$ der komplexen Zahlen vom Betrag 1, ist eine Untergruppe von $\\left(\\mathbb{C}^{*}, \\cdot\\right)$, die sogenannte **Kreisgruppe**: Das Produkt zweier Zahlen vom Betrag 1 hat wieder Betrag 1, ebenso das Inverse. Auch hier hat man eine $_{n}$ mit der Differentialrechnung vertr√§gliche Gruppenstruktur\", d. h. eine Lie-Gruppe.\n",
        "\n",
        "* **Die Menge $\\mathbb{C}^{*}=\\mathbb{C} \\backslash\\{0\\}$ der komplexen Zahlen ungleich 0 bildet mit der gew√∂hnlichen Multiplikation eine Gruppe $\\left(\\mathbb{C}^{*}, \\cdot\\right)$**. Die Multiplikation ist eine differenzierbare Abbildung $m: \\mathbb{C}^{*} \\times \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ definiert durch $m(x, y)=x y_{i}$ auch die durch $i(z)=z^{-1}=\\frac{1}{z}$ definierte Inversion $i: \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ ist differenzierbar. Die Gruppenstruktur der komplexen Ebene (bzgl. Multiplikation) ist also mit der Differentialrechnung vertr√§glich.\n",
        "\n",
        "* [Beispiele fur Lie-Gruppen](https://de.wikipedia.org/wiki/Lie-Gruppe#Beispiele) sind:  allgemeine lineare Gruppe,  Orthogonale Gruppe,  Unit√§re Gruppe & Spezielle Unit√§re Gruppe,  Affine Gruppe, [Poincar√©-Gruppe](https://en.m.wikipedia.org/wiki/Poincar√©_group), Galilei-Gruppe\n",
        "\n",
        "* **Simple Lie Groups**: a simple Lie group is a connected non-abelian Lie group G which does not have nontrivial connected normal subgroups. The list of simple Lie groups can be used to read off the list of simple Lie algebras and [Riemannian symmetric spaces](https://en.wikipedia.org/wiki/Symmetric_space). See also [Classification of semisimple Lie algebras](https://en.wikipedia.org/wiki/Dynkin_diagram#Classification_of_semisimple_Lie_algebras)\n"
      ],
      "metadata": {
        "id": "V9P7SfTo4m6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Algebra**\n",
        "\n",
        "* a [Lie algebra](https://en.m.wikipedia.org/wiki/Lie_algebra) **is a vector space $g$ together with an operation called the Lie bracket**, an alternating bilinear map $\\mathfrak{g} \\times \\mathfrak{g} \\rightarrow \\mathfrak{g},(x, y) \\mapsto[x, y]$, that satisfies the Jacobi identity.\n",
        "\n",
        "* The vector space $\\mathfrak{g}$ together with this operation is a non-associative algebra, meaning that the Lie bracket is not necessarily associative.\n",
        "\n",
        "> <font color=\"blue\">**Any Lie group gives rise to a Lie algebra, which is its tangent space at the identity.**\n",
        "\n",
        "* In physics, Lie groups appear as symmetry groups of physical systems, and their Lie algebras (tangent vectors near the identity) may be thought of as infinitesimal symmetry motions. Thus Lie algebras and their representations are used extensively in physics, notably in quantum mechanics and particle physics.\n",
        "\n",
        "* [Lie Algebra](https://de.m.wikipedia.org/wiki/Lie-Algebra) ist eine algebraische Struktur, die mit einer Lie-Klammer versehen ist, d. h. es existiert eine antisymmetrische Verkn√ºpfung, die die Jacobi-Identit√§t erf√ºllt.\n",
        "\n",
        "* Lie-Algebren werden haupts√§chlich zum Studium geometrischer Objekte wie Lie-Gruppen und differenzierbarer Mannigfaltigkeiten eingesetzt.\n",
        "\n",
        "* **Simple Lie Algebra**: a [simple Lie algebra](https://en.wikipedia.org/wiki/Simple_Lie_algebra) is a Lie algebra that is nonabelian and contains no nonzero proper ideals. The classification of real simple Lie algebras is one of major achievements of Wilhelm Killing and √âlie Cartan. A direct sum of simple Lie algebras is called a semisimple Lie algebra. A simple Lie group is a connected Lie group whose Lie algebra is simple.\n",
        "\n",
        "* **Semisimple Lie algebra**: a [Lie algebra is semisimple](https://en.wikipedia.org/wiki/Semisimple_Lie_algebra) if it is a direct sum of simple Lie algebras (non-abelian Lie algebras without any non-zero proper ideals)."
      ],
      "metadata": {
        "id": "xcVYeFvX4o4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie-Algebra der Lie-Gruppe**\n",
        "\n",
        "* Die Vektorfelder auf einer glatten Mannigfaltigkeit $M$ bilden mit der Lie-Klammer eine unendlich-dimensionale Lie-Algebra. Die zu einer Lie-Gruppe $G$ geh√∂rende Lie-Algebra $\\mathfrak{g}$ besteht aus dem Unterraum der [links-invarianten](https://de.m.wikipedia.org/wiki/Translationsinvarianz) Vektorfelder auf $G$.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum $T_{e} G$ am neutralen Element $e$ von $G$. Insbesondere gilt also $\\operatorname{dim} G=\\operatorname{dim} \\mathfrak{g}$. Bez√ºglich der LieKlammer $[\\cdot, \\cdot]$ ist der Vektorraum $\\mathfrak{g}$ abgeschlossen.\n",
        "\n",
        "* **Somit ist der Tangentialraum einer Lie-Gruppe $G$ am neutralen Element eine Lie-Algebra. Diese Lie-Algebra nennt man die Lie-Algebra der Lie-Gruppe $G$.**\n",
        "\n",
        "* Zu jeder Lie-Gruppe $G$ mit Lie-Algebra $\\mathfrak{g}$ gibt es eine **Exponentialabbildung exp (exponential map)**: $\\mathfrak{g} \\rightarrow G$. Diese Exponentialabbildung kann man definieren durch $\\exp (A)=\\Phi_{1}(e)$, wobei $\\Phi_{t}$ der Fluss des links-invarianten Vektorfelds $A$ und $e \\in G$ das neutrale Element ist. Falls $G$ eine abgeschlossene Untergruppe der $\\mathrm{GL}(n, \\mathbb{R})$ oder $\\mathrm{GL}(n, \\mathbb{C})$ ist, so ist die so definierte Exponentialabbildung identisch mit der Matrixexponentialfunktion.\n",
        "\n",
        "\n",
        "* Jedes Skalarprodukt auf $T_{e} G=\\mathfrak{g}$ definiert eine $G$ -links-invariante Riemannsche Metrik auf $G$. Im Spezialfall, dass diese Metrik zus√§tzlich auch\n",
        "rechtsinvariant ist, stimmt die Exponentialabbildung der Riemannschen\n",
        "Mannigfaltigkeit $G$ am Punkt $e$ mit der Lie-Gruppen-Exponentialabbildung\n",
        "√ºberein.\n",
        "\n",
        "* Mit der Lie-Gruppe $\\mathrm{SO}(n)$ ist eine Lie-Algebra $\\mathfrak{s o}(n)$ verkn√ºpft, ein Vektorraum mit einem bilinearen alternierenden Produkt (Lie-Klammer), wobei der Vektorraum bez√ºglich der Lie- Klammer abgeschlossen ist.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum am neutralen Element der $\\mathrm{SO}(n)$ (neutrales Element ist die Einheitsmatrix), sodass insbesondere $\\operatorname{dim}_\\mathfrak{s o}(n)=\\operatorname{dim} \\mathrm{SO}(n)$ gilt.\n",
        "\n",
        "* Die Lie-Algebra besteht aus allen schiefsymmetrischen $n \\times n$\n",
        "-Matrizen und ihre Basis sind die sog. Erzeugenden Generators).\n",
        "\n",
        "* Die Exponentialabbildung verkn√ºpft die Lie-Algebra mit der Lie-Gruppe:\n",
        "\n",
        "> $\\exp : \\mathfrak{s o}(n) \\rightarrow \\mathrm{SO}(n), J \\mapsto \\sum_{k=0}^{\\infty} \\frac{1}{k !} J^{k}$\n",
        "\n",
        "Exponentiation in quantum: $\\mathbb{G} = e^\\mathfrak{g}$"
      ],
      "metadata": {
        "id": "YiJEo1Y54q9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why using Lie Algebra?**\n",
        "\n",
        "* For example $\\mathcal{G}$ is all the operations on the surface of a ball (nonlinear): Lie Groups are Continuous Transformation Groups, like a 3D rotation vector on a curved surface. Very complicated to work with. (Lie Algebra SO(3)): $\n",
        "\\boldsymbol{W}^{\\wedge}=[\\boldsymbol{\\omega}]_{\\times}=\\left[\\begin{array}{ccc}\n",
        "0 & -\\omega_{z} & \\omega_{y} \\\\\n",
        "\\omega_{z} & 0 & -\\omega_{x} \\\\\n",
        "-\\omega_{y} & \\omega_{x} & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "* Meanwhile Lie Algebra $\\mathfrak{g}$ is at the origin of the tangent plane which is a linear vector space: Cartesian R3: $\n",
        "\\omega=\\left(\\omega_{x}, \\omega_{y}, \\omega_{z}\\right)\n",
        "$\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algebra\"** $\\rightarrow$ Exponential map translates between both\n",
        "\n",
        "**Lie Groups** were know as \"Continuous Transformation Groups\". **= a group that is also a smooth (differential) manifold**\n",
        "\n",
        "  * **a smooth manifold whose elements satisfy the group axioms**\n",
        "\n",
        "  * (so no singularities or breaks where differentiation or integration wouldn't work anymore)\n",
        "\n",
        "  * each point on a manifold represents one element of the Lie Group (i.e. 3d rotation matrix on the manifold)\n",
        "\n",
        "  * corresponding to it in the cartesian tangent space you can find a 3d rotation vector\n",
        "\n",
        "**Lie Algebra** is the in origin point on manifold (https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s)\n",
        "\n",
        "**Exponential Map** from (cartesian) tangent space to manifold (i.e 3 D surface):\n",
        "\n",
        "  * from tangent space a to manifold ('**exponential of a**'), like the exponential of a rotation vector (on tangent space) is the 3d rotation matrix (on manifold)\n",
        "\n",
        "  * and back: point on manifold x and its '**logarithm of x**' back on the tangent space\n",
        "\n",
        "  * it's an exact operation, and no approximation\n",
        "\n",
        "  * explanation and proof here: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_07.png)"
      ],
      "metadata": {
        "id": "9tPBGJIU4s4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie algebra is the origin point (= identity) on the tangent space!**\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algreba\"** **each time you take a derivative on the manifold, you are out of the manifold**, but you can stay on the tangent space for doing this operation (because there they are well defined)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_08.png)\n",
        "\n",
        "*https://arxiv.org/pdf/1812.01537.pdf*"
      ],
      "metadata": {
        "id": "YmuDaXN94zEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **antipodal point** to the origin: from origin point go half a turn around a 3d ball, you and up on the antipodal point, and there are many ways to go (any vector with length pie œÄ)\n",
        "\n",
        "* The tangent space will cover the manifold multiple times !!\n",
        "\n",
        "  * \"**First cover of the manifold by the tangent space**\": all points on the tangent space will end up in the antipodal point when applying any vector with length pie œÄ)\n",
        "\n",
        "  * there you can see relationship between Lie group and tangent space on manifold\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_02.png)"
      ],
      "metadata": {
        "id": "wiDgFHe2449f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following image: $R^{m}$ and $T_{E} M$ are the same vector space but with different representations\n",
        "\n",
        "> **Lie Algebra $T_{E} M \\sim R_{m} \\text { (Cartesian Tangent Space) and } w \\sim w^{\\wedge}$**\n",
        "\n",
        "* This means that $T_{E} M$ is isomorph to $R_{m}$\n",
        "\n",
        "* Since you can always go to an $R_{m}$ space, any Lie Group will have an cartesian $R_{m}$ tangent space, which is a vector\n",
        "\n",
        "* you can alweays go from one to the other give the isomorphism\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_03.png)"
      ],
      "metadata": {
        "id": "cwmJvdVO47i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SO(3): Lie group of rotation matrices in 3D**\n",
        "\n",
        "* you can write [w]<sub>x</sub> as a linear combination of 3 base matrices $E$<sub>x, y, z</sub>, which facilitates the calculation / it's easier than working directly with [w]<sub>x</sub>\n",
        "\n",
        "* and since $T_{E} M \\sim R_{m}$ sowie $w \\sim w^{\\wedge}$ (Isomorphism), it's also an allowed (exact) operation\n",
        "\n",
        "* Let's write a tangent vector as a regular cartesian vector with 3 coordinates\n",
        "\n",
        "> $[\\omega]_{\\times}=\\omega_{x} \\mathbf{E}_{x}+\\omega_{y} \\mathbf{E}_{y}+\\omega_{z} \\mathbf{E}_{z}$\n",
        "\n",
        "* The matrix product $[\\omega]_{\\times}$ is equivalent to cross product of vectors $\\omega =\\omega_{x} \\mathbf +\\omega_{y} \\mathbf +\\omega_{z}$, isomorph, two ways of representing the same elemtn of the tangent space\n",
        "\n",
        "* looking at the previous slide: cartesian R<sup>m</sup> is easier to work with than Lie Algebra T<sub>E</sub>M\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_04.png)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_05.png)\n",
        "\n",
        "**An elementary example is the space of three dimensional vectors $\\mathfrak{g}=\\mathbb{R}^{3}$** with the bracket operation defined by the cross product $[x, y]=x \\times y$. This is skew-symmetric since $x \\times y=-y \\times x$, and instead of associativity it satisfies the Jacobi identity:\n",
        "\n",
        "> $\n",
        "x \\times(y \\times z)=(x \\times y) \\times z+y \\times(x \\times z)\n",
        "$\n",
        "\n",
        "* **This is the Lie algebra of the Lie group of rotations of space**, and each vector $v \\in \\mathbb{R}^{3}$ may be pictured as an infinitesimal rotation around the axis $v$, with velocity equal to the magnitude of $v$.\n",
        "\n",
        "* The Lie bracket is a measure of the noncommutativity between two rotations: since a rotation commutes with itself, we have the alternating property $[x, x]=x \\times x=0$."
      ],
      "metadata": {
        "id": "ds8k2eji4-9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Spaces*"
      ],
      "metadata": {
        "id": "gmN2IqZ4FDsw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkLBUH1hb36"
      },
      "source": [
        "###### *Spaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqI_Wvvlu5y7"
      },
      "source": [
        "**What constitutes a 'space'?**\n",
        "\n",
        "* **A space is a [set](https://en.m.wikipedia.org/wiki/Set_(mathematics)) - (sometimes called a [universe - Grundmenge](https://en.m.wikipedia.org/wiki/Universe_(mathematics))) with some added structure.**\n",
        "\n",
        "* A space consists of selected **mathematical objects that are treated as points**, and selected **relationships between these points** (nature of the points can vary widely: for example, the **points can be elements of a set, functions on another space, or subspaces of another space.**)\n",
        "\n",
        "\n",
        "* **[Taxonomy of Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces)**: While each type of space has its own definition, the general idea of \"space\" evades formalization (modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, but it does not define the notion of \"space\" itself)\n",
        "\n",
        "Quelle: [Einordnung in die Hierarchie mathematischer Strukturen](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Einordnung_in_die_Hierarchie_mathematischer_Strukturen) sowie [Topologische R√§ume](https://de.m.wikipedia.org/wiki/Topologischer_Raum#Beispiele)\n",
        "\n",
        "Topologischer Raum |  | dazugeh√∂rige Struktur\n",
        "--- | --- | ---\n",
        "[Euklidischer Raum](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) | hat | Skalarprodukt\n",
        "[Normierter Raum](https://de.m.wikipedia.org/wiki/Normierter_Raum) | hat | Norm\n",
        "[Metrischer Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) | hat | Metrik\n",
        "[Uniformer Raum](https://de.m.wikipedia.org/wiki/Uniformer_Raum) | hat | Uniforme Struktur\n",
        "[Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum) | hat | Topologie\n",
        "\n",
        "![xxx](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Beziehungen_zwischen_mathematischen_R√§umen.svg/220px-Beziehungen_zwischen_mathematischen_R√§umen.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ostHcxEvNZ5r"
      },
      "source": [
        "![Normed Vector Space](https://upload.wikimedia.org/wikipedia/commons/7/74/Mathematical_Spaces.png)\n",
        "\n",
        "Quelle: [Mathematical Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYW5u_0Oi7l5"
      },
      "source": [
        "###### *Inner Product Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3TsP_aDsFR"
      },
      "source": [
        "**Inner Product**\n",
        "\n",
        "> * **Inner product $\\rightarrow$ measures distances, lengths, angles**\n",
        "\n",
        "* Das innere Produkt (Skalarprodukt / Produktpunkt called when applied to functions - the alternate name of inner product in linear algebra is 'dot product') stellt quasi eine Geometrie im Vektorraum her, wir k√∂nnen dadurch definieren, welche Vektoren orthogonal, und welche parallel zueinander sind.\n",
        "\n",
        "* **Das Skalarprodukt ben√∂tigt man**,\n",
        "  * um die Lange von Vektoren zu berechnen,\n",
        "  * den Winkel zwischen Vektoren zu berechnen (uber Cosinus von Alpha) und\n",
        "  * ob zwei Vektoren senkrecht zueinander stehen.\n",
        "\n",
        "* Das **Dot Product / Scalar Product / [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt)** (auch inneres Produkt oder Punktprodukt) ist eine [mathematische Verkn√ºpfung](https://de.wikipedia.org/wiki/Verkn√ºpfung_(Mathematik)), die zwei Vektoren eine Zahl (Skalar) zuordnet.\n",
        "\n",
        "* **Scalar vs Scalar Product**: A [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics)) is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector. The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically, the determinant can be viewed as the volume scaling factor of the linear transformation described by the matrix.\n",
        "\n",
        "* Geometrisch berechnet man das Skalarprodukt zweier Vektoren $\\vec{a}$ und $\\vec{b}$ nach der Formel:\n",
        "\n",
        "> $\n",
        "\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})\n",
        "$\n",
        "\n",
        "* Null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben.\n",
        "\n",
        "* **A common special case of the inner product is the scalar product or dot product, is written with a centered dot a ‚ãÖ b.**\n",
        "\n",
        "* In [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) the inner product is the dot product, also known as the scalar product. (They generalize Euclidean spaces to vector spaces of any (possibly infinite) dimension.)\n",
        "\n",
        "* Ist das Skalarprodukt von zwei Vektoren $\n",
        "\\vec{a} \\cdot \\vec{b}= 0$, dann folgt daraus, dass diese orthogonal zueinander stehen.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * A simple example is the real numbers $\\mathbb{R}$ with the standard multiplication as the inner product $\\langle x, y\\rangle:=x y$\n",
        "\n",
        "  * **Inner Product of Functions** says how similar two functions are (how much they align with each other). **Inner product of function are used a lot in Fourier Transform**\n",
        "\n",
        "    * i.e. if they are orthogonal, then zero. if they are very similar, then they have a large inner product\n",
        "\n",
        "    > $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "    * You can also take samples from both functions and calculate the inner product between both. Up to infinity, you get at the integral like written above (Riemann approximation of the continuuos integral above):\n",
        "\n",
        "    > $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcOpkYQjFKx"
      },
      "source": [
        "**Inner Product Space**\n",
        "\n",
        "* Der **[Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space)** (Pr√§hilbertraum bzw. Skalarprodukt) ist ein **Vektorraum**, auf dem ein **inneres Produkt definiert ist**.\n",
        "\n",
        "* An [inner product space](https://en.m.wikipedia.org/wiki/Inner_product_space) is a normed space, **where the norm of a vector is the square root of the inner product of the vector by itself**: $\\sqrt{\\vec{x} \\cdot \\vec{x}} = \\sqrt{{x}^{2}}$\n",
        "\n",
        "* [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) generalize **Euclidean spaces (in which the inner product is the dot product, also known as the scalar product**) to vector spaces of any (possibly infinite) dimension.\n",
        "\n",
        "An **inner product space** is a vector space $V$ over the field $\\mathbb{F}$ together with a map\n",
        "\n",
        "$\n",
        "\\langle\\cdot, \\cdot\\rangle: V \\times V \\rightarrow \\mathbb{F}\n",
        "$\n",
        "\n",
        "called an inner product that satisfies the following conditions $(1),(2),$ and $(3)$ for all vectors $x, y, z \\in V$ and all scalars $a \\in \\mathbb{F}:$ [see here](https://en.m.wikipedia.org/wiki/Inner_product_space)\n",
        "\n",
        "* In linear algebra, **an inner product space is a vector space with an additional structure called an inner product**. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Geometric interpretation of the angle between two vectors defined using an inner product.\n",
        "\n",
        "> **Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors**. They also provide the means of defining orthogonality between vectors (**zero inner product**).\n",
        "\n",
        "> An inner product **naturally induces an associated norm**, (|x| and |y| are the norms of x and y, in the picture) thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Normed Vector Space*"
      ],
      "metadata": {
        "id": "0JfDpLK4KBqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "\n",
        "classical probability theory: 1-Norm: ‚àë pi = ‚àë | p | = || p || 1 = 1\n",
        "\n",
        "quantum probabiliyt theory: 2-Norm: || | œà > || 2 = 1\n",
        "\n",
        "**Reeller Vektor**\n",
        "\n",
        "Die 1-, 2-, 3- und $\\infty$-Normen des reellen Vektors $x=(3,-2,6)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3|^3+|-2|^3+|6|^3}=\\sqrt[3]{251} \\approx 6,308 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Komplexer Vektor**\n",
        "\n",
        "Die 1-, 2-, 3-und $\\infty$-Normen des komplexen Vektors $x=(3-4 i,-2 i)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3-4 i|^3+|-2 i|^3}=\\sqrt[3]{5^3+2^3}=\\sqrt[3]{133} \\approx 5,104 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "1Ui-wrEg177C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhAD7kvd3uuL"
      },
      "source": [
        "> **Eine Norm gibt die <u>Gr√∂√üe / L√§nge (Betrag) eines Elements</u> in einem Vektorraum an**\n",
        "\n",
        "* Eine Metrik gibt hingegen die Distanz zwischen <u>zwei Vektoren (Punkten)</u> an. A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ (but Instead of distance between points, a norm gives us the length of a vector, as measured from the origin)\n",
        "\n",
        "* Eine **[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) ist eine Abbildung (Funktion)** $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$, welche einem Element von einem reellen oder komplexen Vektorraum eine **nicht-negative reelle Zahl** $\\mathbb{R}^{\\geq \\ 0 }$ zuordnet und folgende Eigenschaften besitzt (f√ºr alle $x, y$ aus dem $\\mathbb{K}$ Vektorraum und alle $\\lambda$ aus $\\mathbb{K}$):\n",
        "\n",
        "1. **[Definitheit](https://de.m.wikipedia.org/wiki/Definitheit)**:\n",
        "  * It is **nonnegative**, that is for every vector x, one has ‚Äñx‚Äñ ‚â• 0.\n",
        "  * It is **positive on nonzero vectors**, that is, ‚Äñx‚Äñ = 0 ‚ü∫ x = 0.\n",
        "\n",
        "2. **[Absolute Homogenit√§t](https://de.m.wikipedia.org/wiki/Homogene_Funktion)**: For every vector x, and every **scalar Œ±**, one has ‚Äñ Œ± x ‚Äñ = | Œ± | ‚Äñ x ‚Äñ.\n",
        "\n",
        "3. **[Subadditivit√§t, Dreiecksungleichung](https://de.m.wikipedia.org/wiki/Additive_Funktion#Sub-_und_Superadditivit√§t)**: for every vectors x and y, one has ‚Äñ x+y ‚Äñ ‚â§ ‚Äñ x ‚Äñ + ‚Äñ y ‚Äñ.\n",
        "\n",
        "* From Inner Products to Norms: Eine Norm kann (muss aber nicht) von einem [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (eine sogenannte ['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall is **the norm of a vector the square root of the inner product of the vector by itself**. A complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "\n",
        "* **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. If such a space is complete, we call it a [Banach space](https://de.m.wikipedia.org/wiki/Banachraum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygj95-xb7Eu"
      },
      "source": [
        "[**1. Normen auf endlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "\n",
        "* [**Zahlnorm**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): ein Beispiel ist die [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion).\n",
        "\n",
        "  * ist **induziert vom Standardskalarprodukt** (erf√ºllt die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t) zweier reeller bzw. komplexen Zahlen. Die Betragsnorm ist. der Betrag einer reellen Zahl $z \\in \\mathbb{R}$:\n",
        "  > $\n",
        "\\|z\\|=|z|=\\sqrt{z^{2}}=\\left\\{\\begin{array}{cl}\n",
        "z & \\text { f√ºr } z \\geq 0 \\\\\n",
        "-z & \\text { f√ºr } z < 0\n",
        "\\end {array}\\right.$\n",
        "\n",
        "* [**Matrixnorm**](https://de.m.wikipedia.org/wiki/Matrixnorm): Siehe [Norm -> Matrixnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Matrixnormen) sowie [https://de.m.wikipedia.org/wiki/Matrixnorm](Matrixnorm)\n",
        "  * [Nat√ºrliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): Eine nat√ºrliche Matrixnorm entspricht anschaulich dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor entsteht\n",
        "  * [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm):  Anschaulich entspricht die Spektralnorm damit dem gr√∂√ütm√∂glichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor der L√§nge Eins entsteht = Die Spektralnorm einer Matrix entspricht ihrem maximalen Singul√§rwert, also der Wurzel des gr√∂√üten Eigenwerts des Produkts der adjungierten (transponierten) Matrix mit dieser Matrix. Spektralnorm die von der euklidischen Norm abgeleitete nat√ºrliche Matrixnorm. Von: [Singul√§rwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "\n",
        "* [**Vektornormen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen): ein Beispiel sind die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm)\n",
        "\n",
        "  * Die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) sind eine Klasse von [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen), die f√ºr reelle Zahlen $p \\geq 1$ definiert sind.\n",
        "\n",
        "  * Die $p$ -Norm eines Vektors $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{K}^{n}$ mit $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{C}$ ist f√ºr reelles\n",
        "$1 \\leq p<\\infty$ definiert durch:\n",
        "\n",
        "  > **$\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$**\n",
        "\n",
        "  * p-Normen **erf√ºllen die Minkowski-Ungleichung sowie die H√∂lder-Ungleichung**. For all p ‚â• 1, the p-norms erf√ºllen die drei Normaxiome Definitheit, absolute Homogenit√§t und Subadditivit√§t.\n",
        "\n",
        "  * Die wichtigsten Vektornormen sind die [**Summennorm**](https://de.m.wikipedia.org/wiki/Summennorm) , Euklidische Norm und [**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**2. Normen auf unendlichdimensionalen Vektorr√§umen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorr√§umen)\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**[Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum)**\n",
        "\n",
        "* **The sequence space (Folgenraum) is a special case of the function space (Funktionenraum): $\\ell_{\\infty}=L_{\\infty}(\\mathbb{N})$ where the natural numbers are equipped with the counting measure.**\n",
        "\n",
        "* Die $\\ell^{p}$ -Normen sind die Verallgemeinerung der $p$ -Normen auf Folgenr√§ume, wobei lediglich die endliche Summe durch eine unendliche ersetzt wird. Die $\\ell^{p}$ -Norm einer in $p$ -ter Potenz betragsweise summierbaren Folge ist f√ºr reelles $1 \\leq p<\\infty$ dann definiert als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Versehen mit diesen Normen werden die $\\ell$ - R√§ume jeweils zu vollst√§ndigen normierten R√§umen. ${ }^{[6]}$ F√ºr den Grenzwert $p \\rightarrow \\infty$ ergibt sich der Raum der beschr√§nkten Folgen $\\ell^{\\infty}$ mit der Supremumsnorm.\n",
        "\n",
        "**[Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum**\n",
        "\n",
        "* Siehe auch L-p-Raum: https://de.m.wikipedia.org/wiki/Lp-Raum\n",
        "\n",
        "* Die $\\mathcal{L}^{p}$ -Normen einer in $p$ -ter Potenz **Lebesgue-integrierbaren Funktion** mit $1<p<\\infty$ sind in Analogie zu den $\\ell^{p}$ -Normen definiert als\n",
        "\n",
        "> $\n",
        "\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}\n",
        "$\n",
        "\n",
        "* **wobei die Summe durch ein Integral ersetzt wurde**. Ebenso wie bei der wesentlichen Supremumsnorm sind diese Narmen zun√§chst nur Halbnormen, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Ma√ü Null II von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man wieder die Menge der √Ñauivalenzklassen unnn Funktionen $[f] \\in L^{p}(\\Omega)$, die fast √ºberall gleich sind, und definiert auf diesen $L^{p}$ -R√§umen die $L^{p}$ -Normen durch\n",
        "$\\|[f]\\|_{L P(\\Omega)}=\\|f\\|_{\\mathcal{L}^{p}(\\Omega)}$\n"
      ],
      "metadata": {
        "id": "EC3JVQRj2H01"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27wE3ZTM_154"
      },
      "source": [
        "**Beispiele fur Normed Vector Spaces**\n",
        "\n",
        "* **A [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) or normed space <u>is a vector space</u> over the real or complex numbers, on which a norm is defined**.\n",
        "\n",
        "  * Ist $V$ ein [Vektorraum](https://de.m.wikipedia.org/wiki/Vektorraum) √ºber dem K√∂rper $\\mathbb{K}$ der reellen oder der komplexen Zahlen und $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$ eine Norm auf $V,$ dann nennt man das Paar $(V,\\|\\cdot\\|)$ einen **normierten Vektorraum**.\n",
        "\n",
        "  * **Any normed vector space is a metric space** by defining d(x, y) = ‚Äñ y - x ‚Äñ, see also metrics on vector spaces. (If such a space is complete, we call it a Banach space.)\n",
        "\n",
        "  * **Be careful**: A [vector space](https://en.m.wikipedia.org/wiki/Vector_space) is an [algebraic structure](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures), meanwhile a [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) is a type of [abstract (topological) space](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces) (a normed vector space is a vector space over the real or complex numbers, on which a norm is defined). (See also [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space)\n",
        "\n",
        "* **Banach Space**: **$\\mathbb{R}$<sup>n</sup> together with the p-norm is a [Banach space](https://de.wikipedia.org/wiki/Banachraum) = ein vollst√§ndiger normierter Vektorraum**. This Banach space is the Lp-space over Rn. Viele **Folgenr√§ume $\\ell$** oder **Funktionenr√§ume $L$** sind unendlichdimensionale Banachr√§ume. Function Spaces $L$ are a type of infinite vector space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces**, whose vectors are functions.\n",
        "\n",
        "* **Hilbert Space**: Ein Banachraum, dessen Norm durch ein Skalarprodukt induziert ist, hei√üt **[Hilbertraum](https://de.wikipedia.org/wiki/Hilbertraum)**. (z.B. p2-Norm Euklidische Norm, aber nicht p1-Summennorm). L√§sst man die Bedingung der Vollst√§ndigkeit fallen, spricht man von einem Pr√§hilbertraum).\n",
        "  * Die Struktur eines Hilbertraums ist eindeutig festgelegt durch seine Hilbertraumdimension. Diese kann eine beliebige Kardinalzahl sein. Ist die Dimension endlich und betrachtet man als K√∂rper die reellen Zahlen, so handelt es sich um einen euklidischen Raum.\n",
        "  * Hilbertr√§ume tragen durch ihr Skalarprodukt eine topologische Struktur. Dadurch sind hier im Gegensatz zu allgemeinen Vektorr√§umen Grenzwertprozesse m√∂glich.\n",
        "\n",
        "* **Hardy-Space**: Untersucht man statt der messbaren Funktionen nur die holomorphen beziehungsweise die harmonischen Funktionen auf Integrierbarkeit, so werden die entsprechenden $L^{p}$-R√§ume [Hardy-R√§ume](https://de.m.wikipedia.org/wiki/Hardy-Raum) genannt, [L_p Space-Hardy](https://de.m.wikipedia.org/wiki/Lp-Raum#Hardy-R%C3%A4ume)\n",
        "\n",
        "* **F-Space**: The **space Lp for 0 < p < 1 is an [F-space](https://en.m.wikipedia.org/wiki/F-space)**: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ‚â• 1. Some authors use the term [Fr√©chet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space) rather than F-space, but usually the term \"Fr√©chet space\" is reserved for locally convex F-spaces.\n",
        "\n",
        "* **Sobolev-Raum**: Ein [Sobolev-Raum](https://de.wikipedia.org/wiki/Sobolev-Raum), ist ein Funktionenraum von schwach differenzierbaren Funktionen, der zugleich ein Banachraum ist. Der Sobolev-Raum ist der Raum derjenigen reellwertigen Funktionen $u \\in L^{p}(\\Omega),$ deren gemischte partielle schwache Ableitungen bis zur Ordnung $k$ im Lebesgue-Raum $L^{p}(\\Omega)$ liegen.\n",
        "\n",
        "  * Aus der Variationsrechnung: Diese minimiert Funktionale √ºber Funktionen - Sobolev-R√§ume bilden dabei die Grundlage der L√∂sungstheorie partieller Differentialgleichungen.\n",
        "\n",
        "  * A Sobolev space is a vector space of functions equipped with a norm that is a **combination of Lp-norms of the function together with its derivatives up to a given order**.\n",
        "\n",
        "  * The derivatives are understood in a suitable weak sense to make the space complete, i.e. a Banach space.\n",
        "\n",
        "  * **A Sobolev space is a space of functions**\n",
        "\n",
        "    * **possessing sufficiently many derivatives** for some application domain, such as partial differential equations,\n",
        "\n",
        "    * and **equipped with a norm** that measures both the size and regularity of a function.\n",
        "\n",
        "    * Their importance comes from the fact that **weak solutions of some important partial differential equations exist in appropriate Sobolev spaces**, even when there are no strong solutions in spaces of continuous functions with the derivatives understood in the classical sense.\n",
        "\n",
        "  * Partielle Differentialgleichungen betrachtet man meistens auf Sobolew-R√§umen. In diesen R√§umen werden Funktionen, die bis auf Nullmengen √ºbereinstimmen, als gleich angesehen. Da der Rand eines Gebietes √ºblicherweise eine Nullmenge ist, ist der Begriff der Randbedingung problematisch. L√∂sungen f√ºr dieses Problem sind sobolewsche Einbettungss√§tze oder ‚Äì allgemeiner ‚Äì [**Spuroperatoren**](https://de.wikipedia.org/wiki/Sobolev-Raum#Spuroperator).\n",
        "\n",
        "* **$L^p$ (Lebesgue) Space** are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. **They are sometimes called Lebesgue spaces**. Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * [L<sup>p</sup>-Raum](https://de.m.wikipedia.org/wiki/Lp-Raum) sind spezielle R√§ume, die aus allen p-fach integrierbaren Funktionen bestehen. Das $p$ in der Bezeichnung ist ein reeller Parameter: F√ºr jede Zahl $0 < p \\leq \\infty$ ist ein $L^{p}$ -Raum definiert. Die Konvergenz in diesen R√§umen wird als Konvergenz im $p$ -ten Mittel bezeichnet.\n",
        "\n",
        "  * diese R√§ume werden √ºber das Lebesgue-Integral definiert\n",
        "\n",
        "  * Im Fall Banachraum-wertiger Funktionen bezeichnet man sie auch als Bochner-Lebesgue-R√§ume.\n",
        "\n",
        "  * [**Konvergenz_im_p-ten_Mittel**](https://de.m.wikipedia.org/wiki/Konvergenz_im_p-ten_Mittel)\n",
        "\n",
        "  * ${\\mathcal {L}}^{p}$ mit Halbnorm und ${\\mathcal {L}}^{p}$ mit Norm\n",
        "\n",
        "  * Hilbertraum ${\\mathcal {L}}^{2}$\n",
        "\n",
        "  * Der **normierte Vektorraum** $L^{p}$ ist [vollst√§ndig](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum) und damit ein [Banachraum](https://de.m.wikipedia.org/wiki/Banachraum), die Norm $\\|\\cdot\\|_{L} p$ wird **$L^{p}$ Norm** genannt.\n",
        "\n",
        "  * Auch wenn man von sogenannten $L^{p}$ -Funktionen spricht, handelt es sich dabei um die gesamte √Ñquivalenzklasse einer klassischen Funktion. Allerdings liegen im Falle des Lebesgue-Ma√ües auf dem $\\mathbb{R}^{n}$ zwei verschiedene stetige Funktionen nie in der gleichen √Ñquivalenzklasse, so dass der $L^{p}$-Begriff eine nat√ºrliche Erweiterung des Begriffs stetiger Funktionen darstellt.\n",
        "\n",
        "  * The [**Lp spaces**](https://de.m.wikipedia.org/wiki/Lp-Raum) are [function spaces](https://en.m.wikipedia.org/wiki/Function_space) defined using a natural **generalization of the p-norm for finite-dimensional vector spaces**. They are sometimes called **Lebesgue spaces**.\n",
        "\n",
        "  * A normed vector space is automatically a metric space, by defining the metric in terms of the norm in the natural way. But a metric space may have no algebraic (vector) structure ‚Äî i.e., it may not be a vector space ‚Äî so the concept of a **metric space is a generalization of the concept of a normed vector space**.\n",
        "\n",
        "  * Lp spaces form an important class of [Banach spaces](https://en.m.wikipedia.org/wiki/Banach_space) in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as [solutions to variational problems](https://en.m.wikipedia.org/wiki/Central_tendency#Solutions_to_variational_problems)\n",
        "\n",
        "  * An Lp space may be defined as a space of measurable functions for which the p-th power of the absolute value is Lebesgue integrable, where functions which agree almost everywhere are identified.\n",
        "\n",
        "  * More generally, let 1 ‚â§ p < ‚àû and (S, Œ£, Œº) be a [measure space](https://en.m.wikipedia.org/wiki/Measure_space). Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, that\n",
        "\n",
        "  * $\\|f\\|_{p} \\equiv\\left(\\int_{S}|f|^{p} \\mathrm{d} \\mu\\right)^{1 / p}<\\infty$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SpYIJ602L38"
      },
      "source": [
        "###### *Metric Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gcLfqnPX33L"
      },
      "source": [
        "> **Metric $\\rightarrow$ measures distances**\n",
        "\n",
        "* *Difference Metric to Norm: Instead of distance between points, a norm gives us the length of a vector, as measured from the origin*\n",
        "\n",
        "* **Eine Metrik definiert Abst√§nde zwischen Elementen des Vektorraumes.**\n",
        "\n",
        "* Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n",
        "\n",
        "* Eine Metrik (auch Abstandsfunktion) ist eine Funktion, die je zwei Elementen des Raums einen nicht negativen reellen Wert zuordnet, der als Abstand der beiden Elemente voneinander aufgefasst werden kann.\n",
        "\n",
        "* Sei M eine Menge. **<u>Eine Metrik ist eine Abbildung</u>** d: $M \\times M \\rightarrow \\mathbb{R}$ auf $M \\times M$ wenn folgende drei Axiome erf√ºllt sind:\n",
        "\n",
        "1. Beide zusammen bilde Positive Definitheit (**positive definiteness**):\n",
        "  * $d(x, y) \\geq 0$ (**non-negativity**) sowie\n",
        "  * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) f√ºr alle $x, y \\in M$.\n",
        "\n",
        "2. $d(x, y)=d(y, x)$ (**symmetry**) Symmetrie\n",
        "$d(x, y)=d(y, x) \\forall x, y \\in M$\n",
        "\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "\n",
        "**Metriken geben einem Raum eine globale und eine lokale mathematische Struktur**:\n",
        "  * Die globale Struktur kommt in **geometrischen Eigenschaften wie der Kongruenz** von Figuren zum Ausdruck.\n",
        "  * Die lokale metrische Struktur, also die Definition kleiner Abst√§nde, erm√∂glicht unter bestimmten zus√§tzlichen Voraussetzungen die Einf√ºhrung von **Differentialoperationen**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdrlGVRmlmx"
      },
      "source": [
        "**Aus Normen erzeugte Metriken**\n",
        "\n",
        "* **A norm induces a (distance) metric by the formula d (x,y) = ‚Äñ y-x ‚Äñ.**\n",
        "\n",
        "* Jede Norm auf einem Vektorraum induziert durch die Festlegung $d(x, y) \\equiv\\|x-y\\|$ eine Metrik. Somit ist jeder normierte Vektorraum (und erst recht jeder Innenproduktraum, Banachraum oder Hilbertraum) ein metrischer Raum.\n",
        "\n",
        "* **Aber Achtung**: nicht jede Metrik ist durch eine Norm induziert! Jede Norm induziert eine Metrik, aber nicht umgekehrt.\n",
        "\n",
        "* **Eine Metrik, die aus einer $p$ -Norm abgeleitet ist, hei√üt auch [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances)**. It is a metric in a normed vector space. p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold (which is possible, but then it's not a metric anymore). Wichtige Spezialf√§lle sind:\n",
        "  * [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "  * [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$\n",
        "  * [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "\n",
        "* Als eine [**Fr√©chet-Metrik**](https://de.m.wikipedia.org/wiki/Fr√©chet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. **Sie stellt eine Verbindung zwischen Metrik und Norm her.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUnISc3anLSR"
      },
      "source": [
        "**Nicht aus Normen erzeugte Metriken**\n",
        "\n",
        "* Auf jeder Menge l√§sst sich eine triviale Metrik, die sogenannte gleichm√§√üig diskrete Metrik (die sogar eine Ultrametrik ist) definieren: $d(x, y)=\\left\\{\\begin{array}{ll}0 & \\text { f√ºr } x=y \\\\ 1 & \\text { f√ºr } x \\neq y\\end{array}\\right.$\n",
        "\n",
        "* Im Allgemeinen **nicht durch eine Norm induziert ist die riemannsche Metrik**, die aus einer differenzierbaren Mannigfaltigkeit eine [riemannsche Mannigfaltigkeit](https://en.m.wikipedia.org/wiki/Riemannian_manifold) macht. (zB  Die k√ºrzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geod√§ten) sind nicht zwingend Geradenst√ºcke, sondern k√∂nnen gekr√ºmmte Kurven sein. **Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch gr√∂√üer (z. B. Kugel) oder kleiner (hyperbolische R√§ume) als 180¬∞ sein**.\n",
        "\n",
        "* Die [franz√∂sische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Franz√∂sische_Eisenbahnmetrik).\n",
        "\n",
        "* Die [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man k√∂nnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zur√ºck.\n",
        "\n",
        "* Der [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) ist eine Metrik auf dem Coderaum, die die Unterschiedlichkeit von (gleich langen) Zeichenketten angibt. Die [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) kann als Erweiterung des Hamming-Abstands angesehen werden. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro‚ÄìWinkler distance](https://en.m.wikipedia.org/wiki/Jaro‚ÄìWinkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "\n",
        "* Mehr Beispiele von nicht aus Normen erzeugten Metriken [hier](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-8s7XOx_Mxn"
      },
      "source": [
        "**Metric Space**\n",
        "\n",
        "* Unter einem [metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space) versteht man in der Mathematik eine Menge, auf der eine Metrik definiert ist.\n",
        "\n",
        "  * **Das Paar $(M, d)$ nennt man einen <u>[metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space)</u>**.\n",
        "\n",
        "  * Beispiel fur einen metrischen Raum: Die Menge der reellen Zahlen $\\mathbb{R}$ mit der Abstandsmetrik $d(x, y):=|x-y|$ bilden einen metrischen Raum.\n",
        "\n",
        "* Jeder metrische Raum ist ein [Hausdorff-Raum](https://de.m.wikipedia.org/wiki/Hausdorff-Raum).\n",
        "\n",
        "\n",
        "* **[Isometrie](https://de.m.wikipedia.org/wiki/Isometrie)**\n",
        "\n",
        "  * Isomorphismen zwischen metrischen R√§umen hei√üen Isometrien. Ein metrischer Raum hei√üt vollst√§ndig, falls alle Cauchy-Folgen konvergieren. Eine Isometrie ist eine Abbildung, die zwei metrische R√§ume aufeinander abbildet und dabei die Metrik ‚Äì also die Abst√§nde zwischen je zwei Punkten ‚Äì erh√§lt.\n",
        "\n",
        "  * Sind zwei metrische R√§ume $\\left(M_{1}, d_{1}\\right),\\left(M_{2}, d_{2}\\right)$ gegeben, und ist $f: M_{1} \\rightarrow M_{2}$ eine Abbildung mit der Eigenschaft\n",
        "\n",
        "  * $d_{2}(f(x), f(y))=d_{1}(x, y)$ f√ºr alle $x, y \\in M_{1}$\n",
        "\n",
        "  * dann hei√üt $f$ Isometrie von $M_{1}$ nach $M_{2}$. Eine solche Abbildung ist stets injektiv.\n",
        "\n",
        "  * Ist $f$ sogar bijektiv, dann hei√üt $f$ **isometrischer Isomorphismus**, und die R√§ume $M_{1}$ und $M_{2}$ hei√üen is isometrische Einbettung von $M_{1}$ in $M_{2}$. [Isometrische Isomorphie](https://de.m.wikipedia.org/wiki/Isometrische_Isomorphie) beschreibt in der Funktionalanalysis einen Zusammenhang zwischen zwei unterschiedlichen R√§umen, die geometrisch identisch sind.\n",
        "\n",
        "* [**Vollst√§ndiger Raum**](https://de.m.wikipedia.org/wiki/Vollst√§ndiger_Raum)\n",
        "\n",
        "  * Ein vollst√§ndiger Raum ist in der Analysis ein metrischer Raum, in dem jede Cauchy-Folge von Elementen des Raums auf eine Zahl (element) innerhalb desselben raumes konvergiert.\n",
        "\n",
        "  * ZB alle rationalen Zahlen sollten nach einsetzung in die cauchy-folge mit einer bestimmten metrik (zB betragsmetrik) auch wieder als ziel in rationalen zahlen muenden. tun sie das nicht, ist es ein unvollst√§ndiger Raum.\n",
        "\n",
        "  * Andererseits ist der Raum der rationalen Zahlen mit der Betragsmetrik nicht vollst√§ndig, weil etwa die Zahl 2‚Äì‚àö nicht rational ist, es jedoch Cauchy-Folgen rationaler Zahlen gibt, die bei Einbettung der rationalen Zahlen in die reellen Zahlen gegen 2‚Äì‚àö und somit gegen keine rationale Zahl konvergieren. Es ist aber stets m√∂glich, die L√∂cher auszuf√ºllen, also einen unvollst√§ndigen metrischen Raum zu vervollst√§ndigen. Im Fall der rationalen Zahlen erh√§lt man dadurch den Raum der reellen Zahlen.\n",
        "\n",
        "  * Die Menge Q der rationalen Zahlen ist mit der Betragsmetrik\n",
        "\n",
        "  * $\n",
        "d(x, y)=|x-y|\n",
        "$\n",
        "\n",
        "  * nicht vollst√§ndig, denn die Folge rationaler Zahlen $x_{1}=1, x_{n+1}=\\frac{x_{n}}{2}+\\frac{1}{x_{n}}$ ist eine Cauchy-Folge, deren Grenzwert (siehe Heron-Verfahren) die irrationale Zahl $\\sqrt{2}$ ist, die nicht in $\\mathbb{Q}$ liegt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exkurs: Divergence, Distance and Metric*"
      ],
      "metadata": {
        "id": "SZkKviQcpLi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ‚â• 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ‚â§ d(x, y) + d(y, z)     (subadditivity / triangle inequality).\n",
        "\n",
        "**Begriffsabgrenzungen**\n",
        "\n",
        "* **Divergence** fullfills property of positive definiteness (1 + 2)\n",
        "\n",
        "* **Distance** fullfills property of positive definiteness and symmetrie (1 + 2+ 3)\n",
        "\n",
        "* **Metric** fullfills property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3 + 4). H√§ufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion) bezeichnet. Metric Space: Together with the set, a metric makes up a metric space."
      ],
      "metadata": {
        "id": "EmfRw-AgpSZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergences in Machine Learning**\n",
        "\n",
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold.\n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback‚ÄìLeibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ],
      "metadata": {
        "id": "oryOJsv-pW2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Meaning of 'no symmetry' in divergences**\n",
        "\n",
        "*\n",
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)\n",
        "\n",
        "* The Kullback-Leibler divergence is not symmetric. Roughly speaking, it's because you should think of the two arguments of the KL divergence as different kinds of things: the first argument is empirical data, and the second argument is a model you're comparing the data to.\n",
        "\n",
        "* Take a bunch of independent random variables $X_{1}, \\ldots, X_{n}$ whose possible values lie in a finite set.\n",
        "\n",
        "* Say these variables are identically distributed, with $\\operatorname{Pr}\\left(X_{i}=x\\right)=p_{x}$. Let $F_{n, x}$ be the number of variables whose values are equal to $x$. The list $F_{n}$ is a random variable, often called the \"empirical frequency distribution\" of the $X_{i} .$ What does $F_{n}$ look like when $n$ is very large?\n",
        "\n",
        "* More specifically, let's try to estimate the probabilities of the possible values of $F_{n} .$ since the set of possible values is different for different $n$, take a sequence of frequency distributions $f_{1}, f_{2}, f_{3}, \\ldots$ approaching a fixed frequency distribution $f$. It turns out $^{* *}$ that\n",
        "\n",
        "> $\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\ln \\operatorname{Pr}\\left(F_{n}=f_{n}\\right)=-\\mathrm{KL}(f, p)$\n",
        "\n",
        "* In other words, the Kullback-Leibler divergence of $f$ from $p$ lets you estimate the probability of getting an empirical frequency distribution close to $f$ from a large number of independent random variables with distribution $p$.\n"
      ],
      "metadata": {
        "id": "vxh1T80ovldM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1l9_Erf5T5S"
      },
      "source": [
        "###### *Topological Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy5f4vGA-Llx"
      },
      "source": [
        "**General topology (point-set topology)**\n",
        "\n",
        "> Topology $\\rightarrow$ Lagebeziehungen wie ‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú werden verallgemeinert\n",
        "\n",
        "The fundamental concepts in point-set topology are continuity, compactness, and connectedness:\n",
        "\n",
        "* **Continuous functions**, intuitively, take nearby points to nearby points.\n",
        "\n",
        "* **Compact sets** are those that can be covered by finitely many sets of arbitrarily small size.\n",
        "\n",
        "* **Connected sets** are sets that cannot be divided into two pieces that are far apart.\n",
        "\n",
        "**Once a choice of open sets is made, the properties of continuity, connectedness, and compactness, which use notions of nearness, can be defined using these open sets.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Trennungsaxiom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNzMCPIBjYm"
      },
      "source": [
        "**Topological Vector Space**\n",
        "\n",
        "> **Man nennt $T$ eine Topologie auf $X$, und das Paar ($X$,$T$) einen topologischen Raum.**\n",
        "\n",
        "* a topological space ([Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum)) may be defined as a set of points, along with a set of **neighbourhoods** for each point, satisfying a set of **axioms** relating points and neighbourhoods.\n",
        "\n",
        "* The definition of a topological space **relies only upon set theory** and is the most general notion of a mathematical space that allows for the definition of concepts such as **[continuity](https://en.m.wikipedia.org/wiki/Continuous_function#Continuous_functions_between_topological_spaces), [connectedness](https://en.m.wikipedia.org/wiki/Connected_space), and [convergence](https://en.m.wikipedia.org/wiki/Limit_of_a_sequence)**.\n",
        "\n",
        "* **Other spaces, such as manifolds and metric spaces, are specializations of topological spaces with extra structures or constraints.**\n",
        "\n",
        "* Topological spaces are **studied in Point-Set Topology** (General Topology)\n",
        "\n",
        "* ein topologischer Raum ist ein elementarer Gegenstand der Topologie\n",
        "\n",
        "* Durch die Einf√ºhrung einer topologischen Struktur auf einer Menge lassen sich intuitive Lagebeziehungen wie **‚ÄûN√§he‚Äú und ‚ÄûStreben gegen‚Äú** aus dem [Anschauungsraum (Euklidischer Raum)](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) auf sehr viele und sehr allgemeine Strukturen √ºbertragen und mit pr√§ziser Bedeutung versehen.\n",
        "\n",
        "* Ein **[topologischer Vektorraum](https://de.m.wikipedia.org/wiki/Topologischer_Vektorraum)** / [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space) ist ein Vektorraum, auf dem neben seiner algebraischen auch noch eine damit vertr√§gliche topologische Struktur definiert ist.\n",
        "\n",
        "* Sei $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. Ein $\\mathbb{K}$ -Vektorraum $E$, der zugleich topologischer Raum ist, hei√üt topologischer Vektorraum, wenn folgende Vertr√§glichkeitsaxiome gelten:\n",
        "  * Die Vektoraddition $E \\times E \\rightarrow E$ ist stetig,\n",
        "  * Die Skalarmultiplikation $\\mathbb{K} \\times E \\rightarrow E$ ist stetig.\n",
        "\n",
        "* **Beispiele:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erkl√§rt, dass wir eine Menge $\\Omega$ C $\\mathbb{R}$ offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen l√§sst.\n",
        "\n",
        "* **Separation Axioms**: Topologische R√§ume k√∂nnen [klassifiziert werden nach Kolmogorov](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms).\n",
        "\n",
        "*What are some examples of topological spaces which are not a metric space?*\n",
        "\n",
        "A set with a single element  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  only has one topology, the discrete one (which in this case is also the indiscrete one‚Ä¶) So that‚Äôs not helpful.\n",
        "\n",
        "A set with two elements, however, is more interesting.  ùëã={‚àô,‚àò}\n",
        "X\n",
        "=\n",
        "{\n",
        "‚àô\n",
        ",\n",
        "‚àò\n",
        "}\n",
        "  can be topologized by having all subsets open (discrete), only the empty set and the whole space open (indiscrete), and also the topology in which  ‚àÖ,{‚àò}\n",
        "‚àÖ\n",
        ",\n",
        "{\n",
        "‚àò\n",
        "}\n",
        "  and  ùëã\n",
        "X\n",
        " are open (and the symmetric one with  {‚àô}\n",
        "{\n",
        "‚àô\n",
        "}\n",
        "  open).\n",
        "\n",
        "A metric on a two-element set forces us to declare that there‚Äôs a certain distance between the two points, rendering both singleton sets open (and closed). Indeed, a finite metric space has a discrete topology. But we just found three topologies on  ùëã\n",
        "X\n",
        "  which are not the discrete one, giving us three examples of a non-metrizable topological space.\n",
        "\n",
        "Note that the indiscrete topology on  ùëã\n",
        "X\n",
        "  can be regarded as coming from the pseudo-metric where the distance between the points is zero. However, the other two topologies, where one singleton set is open and the other is not, cannot be interpreted in this way. A metric is by definition symmetric, and these two topologies are not.\n",
        "\n",
        "*Source: [Quora](https://www.quora.com/What-are-some-examples-of-topological-spaces-which-are-not-a-metric-space)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mx_opXBBr-r"
      },
      "source": [
        "**Frechet-R√§ume**\n",
        "\n",
        "* [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Verallgemeinerungen des Banachraums und topologische Vektorraum mit speziellen Eigenschaften. Die Hauptvertreter von Fr√©chet-R√§umen sind Vektorr√§ume von glatten Funktionen. Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten.\n",
        "\n",
        "* Es handelt sich um einen **topologischen Vektorraum** mit speziellen Eigenschaften, die ihn als **Verallgemeinerung des Banachraums** charakterisieren.\n",
        "\n",
        "* Die Hauptvertreter von [Fr√©chet-Spaces](https://de.m.wikipedia.org/wiki/Fr√©chet-Raum) sind Vektorr√§ume von [glatten Funktionen](https://de.wikipedia.org/wiki/Glatte_Funktion).\n",
        "  * Eine glatte Funktion ist eine mathematische Funktion, die unendlich oft differenzierbar (insbesondere stetig) ist.\n",
        "\n",
        "* Diese R√§ume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bez√ºglich keiner Norm vollst√§ndig**, also keine Banachr√§ume. Man kann auf ihnen aber eine Topologie definieren, sodass viele S√§tze, die in Banachr√§umen gelten, ihre G√ºltigkeit behalten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUGnGb0RB0L-"
      },
      "source": [
        "**Uniform Spaces**\n",
        "\n",
        "* **[Uniforme R√§ume](https://de.m.wikipedia.org/wiki/Uniformer_Raum) erlauben es zwar nicht Abst√§nde einzuf√ºhren**, aber trotzdem Begriffe wie gleichm√§√üige Stetigkeit, Cauchy-Folgen, Vollst√§ndigkeit und Vervollst√§ndigung zu definieren. Jeder uniforme Raum ist auch ein topologischer Raum.\n",
        "\n",
        "* **Jeder topologische Vektorraum (egal ob metrisierbar oder nicht) ist auch ein uniformer Raum**. Allgemeiner ist jede kommutative topologische Gruppe ein uniformer Raum. Eine nichtkommutative topologische Gruppe tr√§gt jedoch zwei uniforme Strukturen, eine links-invariante und eine rechts-invariante. Topologische Vektorr√§ume sind in endlichen Dimensionen vollst√§ndig, in unendlichen Dimensionen im Allgemeinen aber nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_1$-Norm und $L^1$-Metrik*"
      ],
      "metadata": {
        "id": "6quxRLo3U9Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) im endlichdimensionalen Raum** (p=1, Lasso, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|$\n",
        "\n",
        "* zB Summennorm des reellen Vektors $x=(3,-2,6) \\in \\mathbb{R}^{3}$ ist $\\|x\\|_{1}=|3|+|-2|+|6|=11$\n",
        "\n",
        "* Von Summennorm abgeleitete Metrik ist [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik). Siehe auch [Taxicab_geometry](https://en.m.wikipedia.org/wiki/Taxicab_geometry).\n",
        "\n",
        "* Die Summennorm ist im Gegensatz zur euklidischen Norm (2-Norm) nicht von einem Skalarprodukt induziert.\n",
        "\n",
        "* Die Einheitssph√§re der reellen Summennorm ist ein Kreuzpolytop mit minimalem Volumen √ºber alle p-Normen. **Daher ergibt die Summennorm f√ºr einen gegebenen Vektor den gr√∂√üten Wert aller p-Normen**. (zB 3 + (-2) + 6 = 11 in Summennorm, aber = 7 in euklidischer Norm fur p=2)\n",
        "\n",
        "* Techniques which use an L1 penalty, like [LASSO](https://en.m.wikipedia.org/wiki/Lasso_(statistics)), encourage solutions where many parameters are zero\n",
        "\n",
        "\n",
        "*Summennorm in zwei Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Vector-1-Norm_qtl1.svg/316px-Vector-1-Norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "LdVhJHBAUSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summennorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "$\\ell^{1}-$ Norm (**Folgenraum**)\n",
        "\n",
        "* Die $\\ell^{1}$ -Norm ist die Verallgemeinerung der Summennorm auf den Folgenraum $\\ell^{1}$ der **betragsweise summierbaren Folgen** $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{N} .$\n",
        "\n",
        "* Hierbei wird lediglich **die endliche Summe durch eine unendliche ersetzt** und die $\\ell^{\\text {t }}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{1}}=\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|$\n",
        "\n",
        "$L^{1}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* Weiter kann die Summennorm auf den Funktionenraum $L^{1}(\\Omega)$ der auf einer Menge $\\Omega$ betragsweise integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{1}$ Norm einer **betragsweise Lebesgue-integrierbaren Funktion** $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}=\\int_{\\Omega}|f(x)| d x$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{1}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden.\n",
        "\n",
        "* Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{1}(\\Omega)$, die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{1}$ -Raum die $L^{1}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{1}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}$"
      ],
      "metadata": {
        "id": "MIi62WcSUgP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 - Manhattan Distance (Lasso)**\n",
        "\n",
        "* The Manhattan norm gives rise to the [Manhattan distance](https://de.m.wikipedia.org/wiki/Manhattan-Metrik), where the distance between any two points, or vectors, is the sum of the differences between corresponding coordinates.\n",
        "\n",
        "* **Die Manhattan-Metrik ist die von der Summennorm (1-Norm) eines Vektorraums erzeugte Metrik.**\n",
        "\n",
        "* ***Aber: Die Summennorm ist nicht von einem Skalarprodukt induziert.***\n",
        "\n",
        "* Die Manhattan-Metrik (auch Manhattan-Distanz, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz d zwischen zwei Punkten a und b als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird:\n",
        "\n",
        "> $d(a, b)=\\sum_{i}\\left|a_{i}-b_{i}\\right|$"
      ],
      "metadata": {
        "id": "n9vaUs0sXyOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_2$-Norm und $L^2$-Metrik*"
      ],
      "metadata": {
        "id": "32OR42wtUS3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidische Norm](https://de.m.wikipedia.org/wiki/Euklidische_Norm) im endlichdimensionalen Raum** (p=2, Ridge, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{2}= \\left(x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}\\right)^{1 / 2} = \\sqrt{\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}}$\n",
        "\n",
        "* Ziel: berechnen die L√§nge (Betrag) eines Vektors in der euklidischen Ebene.  The length of a vector $x = (x_1, x_2, ..., x_n)$ in the $n$-dimensional real vector space $\\mathbb{R}^n$ is usually given by the Euclidean norm $||x||_{2}$.\n",
        "\n",
        "* Die euklidische Norm ist eine von einem Skalarprodukt induzierte Norm (im Gegensatz zur p1 Summennorm)\n",
        "\n",
        "* Beispiel: Vektor ${\\vec {v}}$ mit Komponenten $x$, $y$ und $z$ in drei Dimensionen durch ${\\vec {v}}=(x,y,z)$ wird die L√§nge berechnet durch:\n",
        "\n",
        "> $|{\\vec {v}}|={\\sqrt {x^{2}+y^{2}+z^{2}}}$\n",
        "\n",
        "* Die Euclidean Norm besitzt als eine von einem Skalarprodukt [induzierte Norm](https://de.m.wikipedia.org/wiki/Skalarproduktnorm) **neben den [drei Normaxiomen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Definition) eine Reihe weiterer Eigenschaften**:\n",
        "\n",
        "  * die G√ºltigkeit der [Cauchy-Schwarz-Ungleichung](https://de.m.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung)\n",
        "\n",
        "  * der [Parallelogrammgleichung](https://de.m.wikipedia.org/wiki/Parallelogrammgleichung)\n",
        "\n",
        "  * sowie eine Invarianz unter unit√§ren Transformationen (Die euklidische Norm √§ndert sich also unter unit√§ren Transformationen nicht. F√ºr reelle Vektoren sind solche Transformationen beispielsweise Drehungen des Vektors um den Nullpunkt. Diese Eigenschaft wird zum Beispiel bei der numerischen L√∂sung linearer Ausgleichsprobleme √ºber die **Methode der kleinsten Quadrate mittels QR-Zerlegungen genutzt**.)\n",
        "\n",
        "* F√ºr orthogonale Vektoren erf√ºllt die euklidische Norm selbst eine allgemeinere Form des Satzes des Pythagoras.\n",
        "\n",
        "* Sieht man eine Matrix mit reellen oder komplexen Eintr√§gen als entsprechend langen Vektor an, so kann die euklidische Norm auch f√ºr Matrizen definiert werden und hei√üt dann [**Frobeniusnorm**](https://de.m.wikipedia.org/wiki/Frobeniusnorm). Die euklidische Norm kann auch auf unendlichdimensionale Vektorr√§ume √ºber den reellen oder komplexen Zahlen verallgemeinert werden und hat dann zum Teil eigene Namen.\n",
        "\n",
        "*Euklidische Norm in zwei reellen Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Vector-2-Norm_qtl1.svg/316px-Vector-2-Norm_qtl1.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "p1rB9cfOJadt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euklidische Norm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "* Als [Folgennorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum): Die $\\ell^{2}-$ Norm im Folgenraum ist die Verallgemeinerung der euklidischen Norm auf den [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum) $\\ell^{2}$ der quadratisch summierbaren Folgen $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{\\mathrm{N}} .$ Hierbei wird lediglich die endliche Summe durch eine unendliche ersetzt und die $\\ell^{2}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{2}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{2}\\right)^{1 / 2}$\n",
        "\n",
        "* Die ‚Ñì-p -R√§ume sind ein Spezialfall der allgemeineren Lp-R√§ume, wenn man das Z√§hlma√ü auf dem Raum N betrachtet.\n",
        "\n",
        "* Als [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im [Funktionenraum](https://de.m.wikipedia.org/wiki/Funktionenraum) $L^{2}(\\Omega)$-Norm der auf einer Menge $\\Omega$ quadratisch integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zun√§chst wird die $\\mathcal{L}^{2}$ Norm einer quadratisch Lebesgue-integrierbaren Funktion $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{2} d x\\right)^{1 / 2}$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{2}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zun√§chst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Ma√ü Null von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man die Menge der √Ñquivalenzklassen von Funktionen $[f] \\in L^{2}(\\Omega),$ die fast √ºberall gleich sind, und erh√§lt auf diesem $L^{2}$ -Raum die $L^{2}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{2}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}$\n",
        "\n",
        "* Der Raum $L^{2}(\\Omega)$ ist der [Hilbertraum fur L2](https://de.m.wikipedia.org/wiki/Lp-Raum#Der_Hilbertraum_L2) mit dem Skalarprodukt zweier Funktionen\n",
        "\n",
        "> $\\langle f, g\\rangle_{L_{2}(\\Omega)}=\\int_{\\Omega} \\overline{f(x)} \\cdot g(x) d x$"
      ],
      "metadata": {
        "id": "mXr0GKzWTwNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euclidian Distance](https://de.m.wikipedia.org/wiki/Euklidischer_Abstand) (Metrik) induziert aus der Euklidischen Norm**  (L2, Ridge)\n",
        "\n",
        "> $d_{2}:(x, y) \\mapsto\\|x-y\\|_{2}=\\sqrt{d_{\\mathrm{SSD}}}=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}$\n",
        "\n",
        "* Ziel: berechnen den Abstand zwischen zwei Vektoren in der euklidischen Ebene\n",
        "\n",
        "* Special case of the [Minkowski distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) with p=2\n",
        "\n",
        "* In Statistik siehe auch [Tikhonov Regularization (Ridge)](https://en.m.wikipedia.org/wiki/Tikhonov_regularization). Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small."
      ],
      "metadata": {
        "id": "3eQ4165xTtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidian (Metric) Space](https://en.m.wikipedia.org/wiki/Euclidean_space)**\n",
        "\n",
        "* Together with the [Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance) the Euclidean space is a metric space (x element R, d). http://theanalysisofdata.com/probability/B_4.html\n"
      ],
      "metadata": {
        "id": "kbrT-D7PTrew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Syw4dL96u31"
      },
      "source": [
        "###### *Special: Euclidean $p_{‚àû}$-Norm und $L^{‚àû}$-Metrik*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm) (p $\\rightarrow \\infty$): $\\rightarrow$\n",
        "\n",
        "> $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Sie ist ein Spezialfall der [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm).\n",
        "\n",
        "* Anschaulich gesprochen ist **der aus der Maximumsnorm abgeleitete Abstand immer dann relevant, wenn man sich in einem mehrdimensionalen Raum in alle Dimensionen gleichzeitig und unabh√§ngig voneinander gleich schnell bewegen kann**. (zB Rochade beim Schach)\n",
        "\n",
        "* Allgemeiner kann die Maximumsnorm benutzt werden, um zu bestimmen, wie schnell man sich in einem zwei- oder dreidimensionalen Raum bewegen kann, wenn angenommen wird, dass die Bewegungen in x-, y- (und z-)Richtung unabh√§ngig, gleichzeitig und mit gleicher Geschwindigkeit erfolgen.\n",
        "\n",
        "* Noch allgemeiner kann man ein System betrachten, dessen Zustand durch n unabh√§ngige Parameter bestimmt wird. An allen Parametern k√∂nnen gleichzeitig und ohne gegenseitige Beeinflussung √Ñnderungen vorgenommen werden. **Dann ‚Äûmisst‚Äú die Maximumsnorm in Rn die Zeit, die man ben√∂tigt, um das System von einem Zustand in einen anderen zu √ºberf√ºhren**. Voraussetzung hierf√ºr ist allerdings, dass man die Parameter so normiert hat, dass gleiche Abst√§nde zwischen den Werten auch gleichen √Ñnderungszeiten entsprechen. Andernfalls m√ºsste man eine gewichtete Version der Maximumsnorm verwenden, die die unterschiedlichen √Ñnderungsgeschwindigkeiten der Parameter ber√ºcksichtigt.\n",
        "\n",
        "* F√ºr einen Vektor $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{R}^{n}$ nennt man $\\|x\\|_{\\max }:=\\max \\left(\\left|x_{1}\\right|, \\ldots,\\left|x_{n}\\right|\\right)$ $\\rightarrow$ $\\|x\\|_{\\infty}=\\max _{i=1, \\ldots, n}\\left|x_{i}\\right|$ die Maximumsnorm von x.\n",
        "\n",
        "*√Ñquivalenz der euklidischen Norm (blau) und der Maximumsnorm (rot) in zwei Dimensionen:* [Source](https://de.m.wikipedia.org/wiki/√Ñquivalente_Normen)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Equiv_2-norm_max-norm_qtl1.svg/240px-Equiv_2-norm_max-norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "rpLxgxJgWGVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximumsnorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "* Im Gegensatz zur Maximumsnorm wird die Supremumsnorm $\\|f\\|_{\\text {sup }}:=\\sup _{t \\in X}|f(t)|$ nicht f√ºr stetige, sondern f√ºr beschr√§nkte Funktionen $f$ definiert.\n",
        "\n",
        "* In diesem Fall ist es nicht notwendig, dass $X$ kompakt ist; $X$ kann eine beliebige Menge sein.\n",
        "\n",
        "* **Da stetige Funktionen auf kompakten R√§umen beschr√§nkt sind, ist die Maximumsnorm ein Spezialfall der Supremumsnorm**.\n",
        "\n",
        "* Die Supremumsnorm (auch Unendlich-Norm genannt) ist in der Mathematik eine Norm auf dem Funktionenraum der beschr√§nkten Funktionen. Im einfachsten Fall einer reell- oder komplexwertigen beschr√§nkten Funktion ist die Supremumsnorm das Supremum der Betr√§ge der Funktionswerte. Allgemeiner betrachtet man Funktionen, deren Zielmenge ein normierter Raum ist, und die Supremumsnorm ist dann das Supremum der Normen der Funktionswerte.\n",
        "\n",
        "* **F√ºr stetige Funktionen auf einer kompakten Menge ist die Maximumsnorm ein wichtiger Spezialfall der Supremumsnorm.**\n",
        "\n",
        "*Die Supremumsnorm der reellen Arkustangens-Funktion ist œÄ/2. Auch wenn die Funktion diesen Wert betragsm√§√üig nirgendwo annimmt, so bildet er dennoch die kleinste obere Schranke.*\n",
        "\n",
        "![alternativer Text](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Graf_arctg.svg/260px-Graf_arctg.svg.png)\n",
        "\n",
        "\n",
        "Supremumsnorm vs Maximumsnorm:\n",
        "\n",
        "* So ist etwa die **Supremumsnorm** der linearen Funktion $f(x)=x$ in diesem Intervall gleich $1 .$ Die Funktion nimmt diesen Wert zwar innerhalb des Intervalls nicht an, kommt inm jedoch beliebig nahe.\n",
        "\n",
        "* W√§hlt man stattdessen das abgeschlossene Einheitsintervall $M=[0,1]$, dann wird der Wert 1 angenommen und die Supremumsnorm entspricht der **Maximumsnorm**.\n",
        "\n",
        "\n",
        "$L^{‚àû}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* L‚àû is a **function space** (Funktionenraum). Its elements are the essentially bounded measurable functions. More precisely, L‚àû is defined based on an underlying measure space, (S, Œ£, Œº). Start with the set of all measurable functions from S to R which are essentially bounded, i.e. bounded up to a set of measure zero. Two such functions are identified if they are equal almost everywhere. Denote the resulting set by L‚àû(S, Œº).\n",
        "\n",
        "\n",
        "* [Normen auf Operatoren](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_Operatoren)\n",
        "\n",
        "$\\ell^{‚àû}$ -Norm (**Folgenraum**)\n",
        "\n",
        "* The vector space ‚Ñì‚àû is a **sequence space** (Folgenraum) whose elements are the bounded sequences. The vector space operations, addition and scalar multiplication, are applied coordinate by coordinate.\n",
        "\n",
        "* $\\ell^{\\infty},$ the (real or complex) vector space of bounded sequences with the **[supremum norm](https://de.m.wikipedia.org/wiki/Supremumsnorm)**, and $L^{\\infty}=L^{\\infty}(X, \\Sigma, \\mu)$, the vector space of essentially bounded measurable functions with the **[essential supremum norm](https://de.m.wikipedia.org/wiki/Wesentliches_Supremum)**, are two closely related Banach spaces.\n",
        "\n",
        "* In fact the former is a special case of the latter. As a Banach space they are the continuous dual of the Banach spaces $\\ell_{1}$ of absolutely summable sequences, and $L^{1}=L^{1}(X, \\Sigma, \\mu)$ of absolutely integrable measurable functions (if the measure space fulfills the conditions of being localizable and therefore\n",
        "semifinite).\n",
        "\n",
        "* Pointwise multiplication gives them the structure of a Banach algebra, and in fact they are the standard examples of abelian Von Neumann algebras."
      ],
      "metadata": {
        "id": "jC0WO_F2VJou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L ‚àû - Chebyshev Distance**\n",
        "\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance) (or Tchebychev distance), maximum metric, or L‚àû metric is a metric defined on a vector space **where the distance between two vectors is the greatest of their differences** along any coordinate dimension.\n",
        "\n",
        "* The maximum norm gives rise to the **Chebyshev distance** or chessboard distance, the minimal number of moves a chess king would take to travel from x to y. The Chebyshev distance is the L‚àû-norm of the difference, a special case of the Minkowski distance where p goes to infinity. It is also known as Chessboard distance.\n",
        "\n",
        "> $d_{\\infty}:(x, y) \\mapsto\\|x-y\\|_{\\infty}=\\lim _{p \\rightarrow \\infty}\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{p}\\right)^{\\frac{1}{p}}=\\max _{i}\\left|x_{i}-y_{i}\\right|$"
      ],
      "metadata": {
        "id": "reDInvHxXs-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*(Vector) Norms for Regularization*"
      ],
      "metadata": {
        "id": "fk2nIlNKjMY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Regression*"
      ],
      "metadata": {
        "id": "F4uWOy1kiJIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms/notebook\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Residual_sum_of_squares"
      ],
      "metadata": {
        "id": "4x7F_UT4iLrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "h8VPlSUFiaHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "z7hExSBmilec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "2LHQ82bvinIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "CuQf_KldipMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "aPzEvAhTirT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "sCrxJgguiyCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "zym317e9i6qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "mTcFGaEmi8kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "fiBHTDOMi-iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "E7EvqMAjjAgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost, loss, risk or error function*"
      ],
      "metadata": {
        "id": "F1FnAXf0DeCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost, loss, risk or error function**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/cost.jpg)\n",
        "\n",
        "* The loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set.\n",
        "\n",
        "* Also: objective function, error, cost & loss function. A loss function measures the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).\n",
        "gradient of cost function tells each weight how to change to improve overall prediction\n",
        "MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.\n",
        "\n",
        "* We want to find the local minimum of the cost function\n",
        "\n",
        "*  Quadratic cost (mean squared error MSE):\n",
        "also maximum likelihood, and sum squared error.\n",
        "Most common. Used in regression.\n",
        "Mean squared error is appropriate to regression (line/curve fitting) where the goal is to minimize the mean squared error between the training set (points) and the fitted curve.\n",
        "\n",
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n",
        "\n",
        "* In most cases, our parametric model defines a distribution [‚Ä¶] and we simply use the **principle of maximum likelihood**. This means we use the cross-entropy between the training data and the model‚Äôs predictions as the cost function.\n",
        "\n",
        "* It is important, therefore, that the function faithfully represent our design goals. If we choose a poor error function and obtain unsatisfactory results, the fault is ours for badly specifying the goal of the search.\n",
        "\n",
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "* Maximum likelihood seeks to find the optimum values for the parameters by maximizing a likelihood function derived from the training data.\n",
        "\n",
        "* Given input, the model is trying to make predictions that **match the data distribution of the target variable**. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
        "\n",
        "* One way to interpret maximum likelihood estimation is to view it as **minimizing the dissimilarity** between the empirical distribution [‚Ä¶] defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. [‚Ä¶] **Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions**.\n",
        "\n",
        "* Under appropriate conditions, the maximum likelihood estimator has the **property of consistency** [‚Ä¶], meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter.\n",
        "\n",
        "* Under the framework maximum likelihood, the error between two probability distributions is measured using cross-entropy. Under maximum likelihood estimation, we would seek a set of model weights that minimize the difference between the model‚Äôs predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
        "\n",
        "When using the framework of maximum likelihood estimation, we will implement a cross-entropy loss function, which often in practice means:\n",
        "* a **cross-entropy** loss function for classification problems and\n",
        "* a **mean squared error** loss function for regression problems.\n",
        "\n",
        "* Under the framework of maximum likelihood estimation and assuming a **Gaussian distribution for the target variable**, mean squared error can be considered the cross-entropy between the distribution of the model predictions and the distribution of the target variable.\n",
        "\n",
        "* Many authors use the term ‚Äúcross-entropy‚Äù to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer.\n",
        "\n",
        "* Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
        "\n",
        "* For example, **mean squared error is the cross-entropy between the empirical distribution and a Gaussian model**\n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "* https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "* https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "* https://allenkunle.me/deriving-ml-cost-functions-part1\n",
        "\n",
        "\n",
        "* Properties of ideal Cost functions:\n",
        "  * smooth,\n",
        "  * continuous,\n",
        "  * symmetric (but i.e. Non-symmetric losses: e.g., for spam classification)\n",
        "  * differentiable\n",
        "\n",
        "**Similarity learning** is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, identity of indiscernibles, symmetry and subadditivity (or the triangle inequality). **In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.**\n",
        "\n",
        "> $\\min _{W}\\left\\{L(W):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(W ; x_{i}, y_{i}\\right)+\\lambda r(W)\\right\\}$\n",
        "\n",
        "**Similarity Learning & Distance Metric Learning**\n",
        "\n",
        "* √Ñhnlichkeitsma√üe werden f√ºr nominal oder ordinal skalierte Variablen genutzt\n",
        "\n",
        "* Distanzma√üe werden f√ºr metrisch skalierte Variablen (d. h. f√ºr Intervall- und Verh√§ltnisskala) genutzt.\n",
        "\n",
        "Complete list of [Loss / Cost Functions in TF](https://www.tensorflow.org/api_docs/python/tf/keras/losses/)"
      ],
      "metadata": {
        "id": "w3-c_-luhCKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Regression & Forecasting (mostly distance-based)*"
      ],
      "metadata": {
        "id": "DjYXE5LZhFXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions that belong to the category \"distance-based\" are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.\n",
        "\n",
        "> Great overview: http://juliaml.github.io/LossFunctions.jl/stable/losses/distance/\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/regression_loss.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm_tzCTnhHNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Linear) Least Squares**\n",
        "\n",
        "* Least Squares: Deren Parameter werden so bestimmt, dass die Summe der Abweichungsquadrate e der Beobachtungen y von den Werten der Funktion minimiert wird.\n",
        "\n",
        "* Da die Kleinste-Quadrate-Sch√§tzung die Residuenquadratsumme minimiert, ist es dasjenige Sch√§tzverfahren, welches das [Bestimmtheitsma√ü](https://de.wikipedia.org/wiki/Bestimmtheitsma√ü) maximiert.\n",
        "\n",
        "* Das Bestimmtheitsma√ü der Regression, auch empirisches Bestimmtheitsma√ü, ist eine dimensionslose Ma√üzahl die den Anteil der Variabilit√§t in den Messwerten der abh√§ngigen Variablen ausdr√ºckt, der durch das lineare Modell ‚Äûerkl√§rt‚Äú wird. Mithilfe dieser Definition k√∂nnen die Extremwerte f√ºr das Bestimmtheitsma√ü aufgezeigt werden. F√ºr das\n",
        "Bestimmtheitsma√ü gilt, dass es umso nƒÉher am Wert 1 ist, je kleiner die Residuenquadratsumme ist. Es wird maximal gleich 1 wenn $\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=0$ ist, also alle Residuen null sind. In diesem Fall ist die Anpassung an die Daten perfekt, was bedeutet, dass f√ºr jede Beobachtung $y_{i}=\\hat{y}_{i}$ ist.\n",
        "\n",
        "* [Least Squares](https://en.wikipedia.org/wiki/Least_squares) / [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) & [Linear Least Squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
      ],
      "metadata": {
        "id": "cNJH7IHehJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gauss‚ÄìMarkov theorem (BLUE)**\n",
        "\n",
        "*  states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, **if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero**.\n",
        "\n",
        "* stellt eine theoretische Rechtfertigung der Methode der kleinsten Quadrate dar\n",
        "\n",
        "* Der Satz besagt, dass in einem linearen Regressionsmodell, in dem die **St√∂rgr√∂√üen (error term) einen Erwartungswert von null und eine konstante Varianz haben sowie unkorreliert sind** (Annahmen des klassischen linearen Regressionsmodells), der Kleinste-Quadrate-Sch√§tzer ‚Äì vorausgesetzt er existiert ‚Äì ein bester linearer erwartungstreuer Sch√§tzer ist (englisch Best Linear Unbiased Estimator, kurz: BLUE).\n",
        "\n",
        "* Hierbei bedeutet der ‚Äûbeste‚Äú, dass er ‚Äì innerhalb der Klasse der linearen erwartungstreuen Sch√§tzer ‚Äì die ‚Äûkleinste‚Äú Kovarianzmatrix aufweist und somit minimalvariant ist. Die St√∂rgr√∂√üen m√ºssen nicht notwendigerweise normalverteilt sein. Sie m√ºssen im Fall der verallgemeinerten Kleinste-Quadrate-Sch√§tzung auch nicht unabh√§ngig und identisch verteilt sein.\n",
        "\n",
        "The Gauss-Markov assumptions concern the set of error random variables, $\\varepsilon_{i}:$\n",
        "\n",
        "1. They have mean zero: $\\mathrm{E}\\left[\\varepsilon_{i}\\right]=0$\n",
        "\n",
        "2. They are homoscedastic, that is all have the same finite variance: $\\operatorname{Var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}<\\infty$ for all $i$,\n",
        "3. Distinct error terms are uncorrelated: $\\operatorname{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0, \\forall i \\neq j$.\n",
        "\n",
        "A linear estimator of $\\beta_{j}$ is a linear combination $\\widehat{\\beta}_{j}=c_{1 j} y_{1}+\\cdots+c_{n j} y_{n}$\n",
        "\n",
        "* The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n",
        "\n",
        "* The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the [James‚ÄìStein estimator](https://en.wikipedia.org/wiki/James‚ÄìStein_estimator) (which also drops linearity), [ridge regression(Tikhonov_regularization)](https://en.wikipedia.org/wiki/Tikhonov_regularization), or simply any [degenerate estimator](https://en.wikipedia.org/wiki/Degenerate_distribution).\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem"
      ],
      "metadata": {
        "id": "wqLJa7izhK3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "* Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.\n",
        "\n",
        "* ‚ÄúOrdinary Least Squares‚Äù (OLS) method is used to find the best line intercept (b) and the slope (m). [in y = mx + b, m is the slope and b the intercept]\n",
        "\n",
        "\n",
        "> $m=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
        "\n",
        "> $b=\\bar{y}-m * \\bar{x}$\n",
        "\n",
        "* In other words ‚Üí with OLS Linear Regression the goal is to find the line (or hyperplane) that minimizes the vertical offsets. We define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
        "\n",
        "* OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
        "\n",
        "* The OLS method minimizes the sum of squared residuals, and leads to a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression) for the estimated value of the unknown parameter vector Œ≤.\n",
        "\n",
        "* It is important to point out though that OLS method will work for a univariate dataset (ie., single independent variables and single dependent variables). Multivariate dataset contains a single independent variables set and multiple dependent variables sets, requiring a machine learning algorithm called ‚ÄúGradient Descent‚Äù.\n",
        "\n",
        "* [Wiki](https://en.wikipedia.org/wiki/Ordinary_least_squares) & [Medium](https://medium.com/@jorgesleonel/linear-regression-307937441a8b)"
      ],
      "metadata": {
        "id": "YvnQ8gFphMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Least Squares (WLS)**\n",
        "\n",
        "* are used when heteroscedasticity is present in the error terms of the model.\n",
        "* https://en.wikipedia.org/wiki/Weighted_least_squares"
      ],
      "metadata": {
        "id": "0xWob81xhO3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized Least Squares (GLS)**\n",
        "\n",
        "* is an extension of the OLS method, that **allows efficient estimation of Œ≤ when either heteroscedasticity, or correlations, or both are present among the error terms of the model**, as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
        "\n",
        "* To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ith case is inversely proportional to var(Œµi). This special case of GLS is called \"weighted least squares\".\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Generalized_least_squares"
      ],
      "metadata": {
        "id": "L1U8vBsUhRcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE, SAE & SSE**\n",
        "\n",
        "**Sum of Errors (SE)** the difference in the predicted value and the actual value.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma(\\hat{Y}-Y)$\n",
        "\n",
        "Errors terms cancel each other out.\n",
        "\n",
        "**Sum of Absolute Errors (SAE)** takes the absolute values of the errors for all iterations.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma (|\\hat{Y}-Y|)$\n",
        "\n",
        "This loss function is not differentiable at 0.\n",
        "\n",
        "**Sum of Squared Errors (SSE)** is differentiable at all points and gives non-negative errors. But you could argue that why cannot we go for higher orders like 4th order or so. Then what if we consider to take 4th order loss function, which would look like:\n",
        "\n",
        "$\\mathbf{L}=\\left[\\Sigma(\\hat{Y}-Y)^{2}\\right]$\n",
        "\n",
        "The gradient of the loss function will vanish at minima & maxima. And the error will grow with the sample size.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/sumoferrors.png)\n",
        "\n",
        "* Minimizing Sum of Squared Errors / SSE ([wiki](https://de.m.wikipedia.org/wiki/Residuenquadratsumme) and [medium](https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511)).  We can think of the SSE loss as the (unscaled) variance of the model errors.\n",
        "* Therefore **minimizing the SEE loss is equivalent to minimizing the variance of the model residuals**. For this reason, the sum of squares loss is often referred to as the Residual Sum of Squares error (RSS) for linear models. We can think of minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.\n",
        "* Ideal when distribution of residuals in normal: the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss‚ÄìMarkov_theorem) states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the [best unbiased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) for the parameters .\n",
        "* Problem: Because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss\n"
      ],
      "metadata": {
        "id": "QegFiqDEhTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE (L2) & RMSE (Squared Euclidean Distance)**\n",
        "\n",
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis.\n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/3d-function-2.svg/566px-3d-function-2.svg.png)\n",
        "\n",
        "*A paraboloid, the graph of squared Euclidean distance from the origin*\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/3d-function-5.svg/566px-3d-function-5.svg.png)\n",
        "\n",
        "*A cone, the graph of Euclidean distance from the origin in the plane*"
      ],
      "metadata": {
        "id": "nPIA8NHThWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "$\\mathrm{MSE}={\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Mean Squared Error (L2 or Quadratic Loss). Error decreases as we increase our sample data as the distribution of our data becomes more and more narrower (referring to normal distribution). The more data we have, the less is the error.\n",
        "* Can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better. It is always non ‚Äì negative and values close to zero are better. The MSE is the second moment of the error (about the origin) and thus incorporates both the variance of the estimator and its bias.\n",
        "* Problem: Sensitive to outliers and the order of loss is more than that of the data. As my data is of order 1 and the loss function, MSE has an order of 2 (squared). So we cannot directly correlate data with the error.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate)\n",
        "\n",
        "**Mean Squared Logarithmic Error (MSLR)**\n",
        "\n",
        "* Mean Squared Logarithmic Error\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\n",
        "\n",
        "**RMSE** (Root-Mean-Square Error)\n",
        "\n",
        "$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Root-Mean-Square Error is the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
        "* The **RMSE is directly interpretable in terms of measurement units**, and so is a better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit. Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable\n",
        "* https://www.sciencedirect.com/science/article/pii/S096014811831231X\n",
        "* The **RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian**.\n",
        "https://www.geosci-model-dev-discuss.net/7/C473/2014/gmdd-7-C473-2014-supplement.pdf\n",
        "* When both metrics are calculated, the MAE tends to be much smaller than the RMSE because the RMSE penalizes large errors while the MAE gives the same weight to all errors.\n",
        "* They summarized that the **RMSE tends to become increasingly larger than the MAE** (but not necessarily in a monotonic fashion) as the distribution of error magnitudes becomes more variable. The RMSE tends to 1 grow larger than the MAE with n2 since its lower limit is fixed at the MAE and its upper 11 limit (n2 ¬∑ MAE) increases with n2 .\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Root-mean-square_deviation) & [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)"
      ],
      "metadata": {
        "id": "VTqsbCowhYav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE (L1) & MAPE**\n",
        "\n",
        "$\\mathrm{MAE}=\\frac{1}{n} \\sum_{j=1}^{n}\\left|y_{j}-\\hat{y}_{j}\\right|$\n",
        "\n",
        "* If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. MBE can convey useful information, but should be interpreted cautiously because positive and negative errors will cancel out.\n",
        "\n",
        "* Mean Absolute Error (L1 Loss)\n",
        "* Computes the mean of absolute difference between labels and predictions\n",
        "* measures the average magnitude of the errors in a set of predictions, without considering their direction. It‚Äôs the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "* On some regression problems, the **distribution of the target variable may be mostly Gaussian, but may have outliers**, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "* Metric can range from 0 to ‚àû and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Extremwerte als Ausrei√üer mit geringerem Einfluss auf das Modell ansehen: MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously receive unrealistically huge negative/positive values in our training environment, but not our testing environment).\n"
      ],
      "metadata": {
        "id": "eX6fU9qohaM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE vs MSE**\n",
        "\n",
        "* One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values.\n",
        "\n",
        "* This isn‚Äôt good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate.\n",
        "\n",
        "* The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/mae_vs_mse.PNG)"
      ],
      "metadata": {
        "id": "F71Ij_KchcFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "$\\mathrm{M}=\\frac{1}{n} \\sum_{t=1}^{n}\\left|\\frac{A_{t}-F_{t}}{A_{t}}\\right|$\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is a statistical measure of **how accurate a forecast** system is.\n",
        "\n",
        "* It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values. Where At is the actual value and Ft is the forecast value.\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is the most common measure used to forecast error, and works best if there are no extremes to the data (and no zeros).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "MZzrxESQheD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
        "\n",
        "* There are 3 different definitions of sMAPE. Two of them are below:\n",
        "\n",
        "$\\operatorname{SMAPE}=\\frac{100 \\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(\\left|A_{t}\\right|+\\left|F_{t}\\right|\\right) / 2}$\n",
        "\n",
        "* Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
        "\n",
        "* At is the actual value and Ft is the forecast value\n",
        "\n",
        "* The absolute‚ÄÖdifference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n",
        "\n",
        "* Armstrong's original definition is as follows:\n",
        "\n",
        "$\\mathrm{SMAPE (old)}=\\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(A_{t}+F_{t}\\right) / 2}$\n",
        "\n",
        "* The problem is that it can be negative (if ${\\displaystyle A_{t}+F_{t}<0}$) or even undefined (if ${\\displaystyle A_{t}+F_{t}=0}$). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator.\n",
        "\n",
        "* In contrast to the mean‚ÄÖabsolute‚ÄÖpercentage‚ÄÖerror, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator)\n",
        "\n",
        "* One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the second SMAPE formula:\n",
        "\n",
        "  * Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76%\n",
        "\n",
        "  * Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) & [Wiki2](https://wiki2.org/en/Symmetric_mean_absolute_percentage_error) & [other](https://www.brightworkresearch.com/the-problem-with-using-smape-for-forecast-error-measurement/)"
      ],
      "metadata": {
        "id": "QsaM9Z0whfuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean absolute scaled error (MASE)**\n",
        "\n",
        "* mean absolute scaled error (MASE) is a measure of the accuracy of forecasts.\n",
        "\n",
        "*  It is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast. It was proposed in 2005.\n",
        "\n",
        "* The mean absolute scaled error has the following desirable propertie: [Wiki](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error)"
      ],
      "metadata": {
        "id": "6HSFPTdVhhon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss (Smooth Mean Absolute Error)**\n",
        "\n",
        "* TLDR: will better find a minimum than L1, but less exposed to outliers than L2. However one has to tune the hyperparameter delta. The larger (3+), the more it is L2, the smaller (1), the more it is L1.\n",
        "\n",
        "* The Huber loss **combines the best properties of MSE and MAE** (Mean Absolute Error). It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter.\n",
        "\n",
        "* It's **less sensitive to outliers** in data than the squared error loss. It‚Äôs **also differentiable at 0**. It‚Äôs basically absolute error, which becomes quadratic when error is small.  How small that error has to be to make it quadratic depends on a hyperparameter ùõø.\n",
        "\n",
        "* Once differentiable.\n",
        "\n",
        "$L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
        "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
        "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
        "\\end{array}\\right.$\n",
        "\n",
        "* **Huber loss approaches MSE when ùõø ~ 0 and MAE when ùõø ~ ‚àû**\n",
        "\n",
        "* The choice of delta is critical because it determines what you‚Äôre willing to consider as an outlier. Residuals larger than delta are minimized with L1 (which is less sensitive to large outliers), while residuals smaller than delta are minimized ‚Äúappropriately‚Äù with L2.\n",
        "\n",
        "* One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n",
        "Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it‚Äôs more robust to outliers than MSE. Therefore, **it combines good properties from both MSE and MAE**.\n",
        "\n",
        "* However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Huber_loss) * [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
        "\n",
        "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
        "\n",
        "* The biggest problem with using MAE to train neural networks is the constant large gradient, which may cause the minimum point to be missed when the gradient descent is about to end. For MSE, the gradient will decrease as the loss decreases, making the result more accurate.\n",
        "\n",
        "* In this case, Huber loss is very useful. It will fall near the minimum value due to the decreasing gradient. It is more robust to outliers than MSE. Therefore, Huber loss combines the advantages of MSE and MAE. However, the problem with Huber loss is that we may need to constantly adjust the hyperparameters\n",
        "\n",
        "* https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "* When you compare this statement with the benefits and disbenefits of both the MAE and the MSE, you‚Äôll gain some insights about how to adapt this delta parameter:\n",
        "\n",
        "* **If your dataset contains large outliers**, it‚Äôs likely that your model will not be able to predict them correctly at once. In fact, it might take quite some time for it to recognize these, if it can do so at all. This results in large errors between predicted values and actual targets, because they‚Äôre outliers. Since MSE squares errors, large outliers will distort your loss value significantly. If outliers are present, you likely don‚Äôt want to use MSE. Huber loss will still be useful, but you‚Äôll have to use small values for ùõø.\n",
        "\n",
        "* If it does not contain many outliers, it‚Äôs likely that it will generate quite accurate predictions from the start ‚Äì or at least, from some epochs after starting the training process. In this case, you may observe that the errors are very small overall. Then, one can argue, it may be worthwhile to let the largest small errors contribute more significantly to the error than the smaller ones. In this case, MSE is actually useful; hence, with Huber loss, you‚Äôll likely want to use quite large values for ùõø.\n",
        "\n",
        "* If you don‚Äôt know, you can always start somewhere in between ‚Äì for example, in the plot above, ùõø = 1 represented MAE quite accurately, while ùõø = 3 tends to go towards MSE already. What if you used ùõø = 1.5 instead? You may benefit from both worlds.\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/\n",
        "\n",
        "* For target = 0, the loss increases when the error increases. However, the speed with which it increases depends on this ùõø value. In fact, Grover (2019) writes about this as follows: Huber loss approaches MAE when ùõø ~ 0 and MSE when ùõø ~ ‚àû (large numbers.)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/320px-Huber_loss.svg.png)\n",
        "\n",
        "*Huber loss (green,\n",
        "Œ¥\n",
        "=\n",
        "1) and squared error loss (blue) as a function of\n",
        "y\n",
        "‚àí\n",
        "f\n",
        "(\n",
        "x\n",
        ")*\n",
        "\n",
        "![huber](https://raw.githubusercontent.com/deltorobarba/repo/master/huberloss.jpg)"
      ],
      "metadata": {
        "id": "ZBE611qIhjXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-Cosh Loss**\n",
        "\n",
        "* TLDR: Similar to MAE, will not be affected by outliers. Log-Cosh has all the points of Huber loss, and no need to set hyperparameters. Compared with Huber, Log-Cosh derivation is more complicated, requires more computation, and is not used much in deep learning.\n",
        "\n",
        "* * Log-cosh is another function used in regression tasks that‚Äôs smoother than L2 (is smoothed towards large errors (presumably caused by outliers) so that the final error score isn‚Äôt impacted thoroughly.)\n",
        "* Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. ‚ÄúLog-cosh is the logarithm of the hyperbolic cosine of the prediction error.‚Äù (Grover, 2019). Oops, that‚Äôs not intuitive but nevertheless quite important ‚Äì this is the maths behind Logcosh loss:\n",
        "\n",
        "> $\\log \\cosh (t)=\\sum_{p \\in P} \\log (\\cosh (p-t))$\n",
        "\n",
        "* Similar to Huber Loss, but twice differentiable everywhere\n",
        "* [Wiki Hyperbolic Functions](https://en.m.wikipedia.org/wiki/Hyperbolic_functions), [TF Class](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh), [Machinecurve](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "* However, Log-Cosh is second-order differentiable everywhere, which is still very useful in some machine learning models. For example, XGBoost uses Newton's method to find the best advantage. Newton's method requires solving the second derivative (Hessian). Therefore, for machine learning frameworks such as XGBoost, the second order of the loss function is differentiable. But the Log-cosh loss is not perfect, and there are still some problems. For example, if the error is large, the first step and Hessian will become fixed, which leads to the lack of split points in XGBoost.\n",
        "\n",
        "https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "![logcosh](https://raw.githubusercontent.com/deltorobarba/repo/master/logcosh.jpeg)"
      ],
      "metadata": {
        "id": "vP9h6RbchlyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile Loss (Pinball Loss)**\n",
        "\n",
        "* TLDR: for Heteroskedastizit√§t, i.e. for risk management when variance changes. Quantile Loss, you can set different quantiles to control the proportion of overestimation and underestimation in loss.\n",
        "\n",
        "* estimates conditional ‚Äúquantile‚Äù of a response variable given certain values of predictor variables\n",
        "* is an extension of MAE (**when quantile is 50th percentile, it‚Äôs MAE**)\n",
        "* Im Gegensatz zur Kleinste-Quadrate-Sch√§tzung, die den Erwartungswert der Zielgr√∂√üe sch√§tzt, ist die Quantilsregression dazu geeignet, ihre Quantile zu sch√§tzen.\n",
        "* Fitting models for many percentiles, you can estimate the entire conditional distribution. Often, the answers to important questions are found by modeling percentiles in the tails of the distribution. For that reason **quantile regression provides critical insights in financial risk management & fraud detection**.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Quantilsregression), [TF Class](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/PinballLoss) & [TF Function](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/pinball_loss)\n",
        "\n",
        "* project where predictions were subject to high uncertainty. The client required for their decision to be driven by both the predicted machine learning output and a measure of the potential prediction error. The quantile regression loss function solves this and similar problems by replacing a single value prediction by prediction intervals.\n",
        "\n",
        "* The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
        "\n",
        "* For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.\n",
        "\n",
        "* The usual regression algorithm is to fit the expected or median training data, and the quantile loss function can be used to fit different quantiles of training data by giving different quantiles.\n",
        "\n",
        "![sdd](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss.jpg)\n",
        "\n",
        "* Set different quantiles to fit different straight lines: This function is a piecewise functio., Œ≥ is the quantile coefficient. y is the true value, f(x) is the predicted value. According to the size of the predicted value and the true value, there are two cases to consider.\n",
        "\n",
        "* y> f(x) For overestimation, the predicted value is greater than the true value;\n",
        "\n",
        "* y< f(x) to underestimate, the predicted value is smaller than the real value.\n",
        "\n",
        "* Use different pass coefficients to control the weight of overestimation and underestimation in the entire loss value.\n",
        "\n",
        "* Especially when Œ≥=0.5 When the quantile loss degenerates into the mean absolute error MAE, **MAE can also be regarded as a special case of quantile loss-median loss**. The picture below is taken with different median points [0.25,0.5,0.7] Obtaining different quantile loss function curves can also be seen as MAE at 0.5.\n",
        "\n",
        "![fgfgf](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss2.jpg)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Pinball_Loss_Function.svg/320px-Pinball_Loss_Function.svg.png)\n",
        "\n",
        "*Pinball-Verlustfunktion mit\n",
        "œÑ\n",
        "=0,9. F√ºr\n",
        "Œµ\n",
        "<\n",
        "0 betr√§gt der Fehler\n",
        "‚àí\n",
        "0\n",
        ",\n",
        "1\n",
        "Œµ, f√ºr\n",
        "Œµ\n",
        "‚â•\n",
        "0 betr√§gt er\n",
        "0\n",
        ",\n",
        "9\n",
        "Œµ.*\n",
        "\n",
        "* https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/"
      ],
      "metadata": {
        "id": "geqM6xx5hnqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson Loss**\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-103abfddc312\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459\n"
      ],
      "metadata": {
        "id": "-qbix6xwhpXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "UvgTnPqOhs3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "xZOPqB8Bhuad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "6CYGi1_hhwM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‚Äòsigmoid‚Äò activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "nhpMRXmPhyIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "> In machine learning, many optimization problems formulated as minimization problems wrt Kullback-Leibler divergence.\n",
        "interpreted as *information projections*, uniqueness projections proved by a generalization of the Pythagoras' theorem.\n",
        "PDF: https://Inkd.in/g9ETtTQp\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback‚ÄìLeibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen‚ÄìShannon divergence**\n",
        "\n",
        "* It is based on the Kullback‚ÄìLeibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen‚ÄìShannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erf√ºllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "iM9yRTVah0Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "_cMWr-Beh3wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "0kz2AVcBh5at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I‚Äôve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It‚Äôs used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That‚Äôs why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) ‚Äî a D-dimensional vector, transform this sample into a d-dimensional vector, where d ‚â™ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification ‚Äî it doesn‚Äôt impose any constraints on the distribution on the model‚Äôs inner representation of the given data ‚Äî i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag ‚Äî a Boolean label, specifying whether these two samples are ‚Äúsimilar‚Äù or ‚Äúdissimilar‚Äù. So the input must be not one, but 2 images.\n",
        "  * It penalizes ‚Äúsimilar‚Äù samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * ‚ÄúDissimilar‚Äù samples are penalized by being to close to each other, but in a somewhat different way ‚Äî Contrastive Loss introduces the concept of ‚Äúmargin‚Äù ‚Äî a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It‚Äôs a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it‚Äôs a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "R2dM3lxKh7QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "vlb98XbTh9Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a ‚ÄúClosed-set‚Äù task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case ‚Äî i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a ‚ÄúOpen-set‚Äù task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features ‚Äî features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning ‚Äî and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people‚Äôs faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "5yivnYJoh-uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Regularization*"
      ],
      "metadata": {
        "id": "G1TQJG1ufr-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*An L2-regularized version of the cost function used in SGD for NN [Source](https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db)*\n",
        "\n",
        "> $J_{\\text {regularited }}=\\underbrace{-\\frac{1}{m} \\sum_{i=1}^m\\left(y^{(i)} \\log \\left(a^{[L](i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-a^{[L](i)}\\right)\\right)}_{\\text {crossentropy cost }}+\\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum_l \\sum_l \\sum_j W_{i, j}^{[m 2}}_{\\text {L. reguatization cos }}$\n"
      ],
      "metadata": {
        "id": "OCuAdZfBf4BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "* Regularization is a technique for preventing a model from overfitting\n",
        "* Overfitting = a complicated model that gives worse predictions than a simpler model\n",
        "* Solution: e.g. preventing over-fitting by penalizing a model for having large weights (A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output)\n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called [weight regularization](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/).\n",
        "\n",
        "**Trivial Regularization Approaches**\n",
        "\n",
        "* Add more data\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data)\n",
        "* Use ensemble models\n",
        "\n",
        "**Particular Regularization Techniques**\n",
        "* Weight regularization\n",
        "* Vectornorm (L1, L2, or Elastic Net): Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.\n",
        "* Dropout\n",
        "* Jitter (add noise)\n",
        "* Batch size\n",
        "* Early stopping (this is not a formal regularization method, but can effectively limit overfitting).\n",
        "\n",
        "**Overfitting: Consider Variance-Bias-Tradeoff**: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)\n",
        "\n",
        "**Benefits of regularization from a mathematical optimization point of view**\n",
        "\n",
        "* Minimize a cost function. Neural networks are non-convex cost functions. Numerical optimization methods (gradient descent) can easily get stuck in local minima (stationary points)\n",
        "\n",
        "* Regularization can be used as a way of ‚Äöconvexifying‚Äò a non-convex cost function.\n",
        "\n",
        "* The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible.\n",
        "\n",
        "**Theoretical Foundation**\n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ],
      "metadata": {
        "id": "cjZzhH2ef5uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 (Lasso) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds ‚Äúabsolute value of magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
        "* Learn more on [Google Course](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda): Regularization for Simplicity: Lambda\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "\n",
        "$d_{1} \\equiv d_{\\mathrm{SAD}}:(x, y) \\mapsto\\|x-y\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|$\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature‚Äôs coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn‚Äôt easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**:\n",
        "  * if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero.\n",
        "  * very useful when you want to understand exactly which features are contributing to a decision.\n",
        "  * if you can ignore the ouliers in your dataset or you need them to be there.\n",
        "  * use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features¬† at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUERgs6Hf8C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 (Ridge) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds ‚Äúsquared magnitude‚Äù of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it‚Äôs important how lambda is chosen.\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply ‚Äúweight decay,‚Äù with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn‚Äôt give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge.\n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ],
      "metadata": {
        "id": "sfAtZ0ekf-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Analytical Equivalence**\n",
        "\n",
        "* Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "* https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n",
        "\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed Œª2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+Œª2)."
      ],
      "metadata": {
        "id": "Dnq1CBbfgAFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "\n",
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(k√ºnstliches_neuronales_Netz)\\\n",
        "\n",
        "**Is dropout outdated?**\n",
        "\n",
        "Neural Network: ¬†Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don‚Äôt Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance.\n",
        "\n",
        "**Use Dropout along with L1/L2 Regularization?**\n",
        "\n",
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ‚Ñì 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of ‚Äúswitching off‚Äù some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network‚Äôs over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can ‚Äúwork together‚Äù or rather end up ‚Äúfighting each other‚Äù. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014)"
      ],
      "metadata": {
        "id": "dC-z_kHhgB8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jitter (Noise)**\n",
        "\n",
        "* adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise"
      ],
      "metadata": {
        "id": "DW1FugwRgDxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization & Size**\n",
        "\n",
        "* https://towardsdatascience.com/understanding-batch-normalization-for-neural-networks-1cd269786fa6\n",
        "\n",
        "* Small batches can oÔ¨Äer a regularizing eÔ¨Äect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process.\n",
        "\n",
        "* Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)\n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        "* Batch normalization is another method to regularize a (convolutional) network.\n",
        "* On top of a regularizing effect, batch normalization also gives your convolutional network a resistance to vanishing gradient during training. This can decrease training time and result in better performance.\n",
        "* Batch Normalization Combats Vanishing Gradient\n",
        "* Batch normalization replaces¬†dropout.\n",
        "* Even if you don‚Äôt need to worry about overfitting there are many benefits to implementing batch normalization. Because of this, and its regularizing effect, batch normalization has largely replaced dropout in modern convolutional architectures.\n",
        "* ‚ÄúWe presented an algorithm for constructing, training, and performing inference with batch-normalized networks. The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.‚Äù -[Ioffe and Svegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "Why use batches?\n",
        "To avoid that small datasets increase overfitting to this datasets and worsen overall accuracy. But batch size shouldnt be too big either (computation time, speed of convergence of an algorithm)\n",
        "\n",
        "* Research 1: a low batch size means a very noisy gradient (because computed on a very small subset of the dataset), and a high learning rate means noisy steps.\n",
        "https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914\n",
        "\n",
        "* Research 2: How do you choose your batch size in deep learning/SGD?¬†- An interesting concept so-called \"generalization gap\": Train longer, generalize better: closing the generalization gap in large batch training of neural networks:\n",
        "https://arxiv.org/abs/1705.08741\n",
        "\n",
        "**Covariate Shift**\n",
        "\n",
        "pending...\n",
        "\n"
      ],
      "metadata": {
        "id": "PokQre2wgFWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Geometry**"
      ],
      "metadata": {
        "id": "emoYR7MX-Q2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Multilinear Algebra (Exterior / Grassmann)*"
      ],
      "metadata": {
        "id": "Nj_F9wX47fTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Algebra*"
      ],
      "metadata": {
        "id": "ZiAK4jblJhPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear algebra**\n",
        "\n",
        "* [Multilinear algebra](https://en.m.wikipedia.org/wiki/Multilinear_algebra) extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, **multilinear algebra builds on the concepts of**\n",
        "\n",
        "  * [Multivectors (p-vectors)](https://en.m.wikipedia.org/wiki/Multivector)\n",
        "\n",
        "  * [Exterior algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra) bzw. [Grassmann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra).\n",
        "\n",
        "* Fundamental objects of study in multilinear algebra are\n",
        "\n",
        "  * [Multilinear maps](https://en.m.wikipedia.org/wiki/Multilinear_map) (Multilineare_Abbildung: Abbildung, die f√ºr jedes ihrer Argumente linear ist)\n",
        "\n",
        "  * [Multilinear forms](https://en.m.wikipedia.org/wiki/Multilinear_form)\n",
        "\n",
        "* Abbildung von Modul in einen Ring (also Verallgemeinerung der K-Algebra von Vektorraum in den Korper, zB bei Integration oder Differential)\n",
        "\n",
        "  * Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "  * Jede lineare Abbildung ist eine 1-lineare Abbildung.\n",
        "\n",
        "  * Jede bilineare Abbildung ist eine 2-lineare Abbildung. (S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "    * Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen. Bei diesen ist der Wertebereich G mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch.)\n",
        "\n",
        "  * Sparprodukt ist eine 3-lineare Abbildung\n",
        "\n",
        "*  Multilinearform: wie linear- oder bilinearform, nur mehr argumente.\n"
      ],
      "metadata": {
        "id": "lKfW4ry1JlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exterior (Grassmann) Algebra*"
      ],
      "metadata": {
        "id": "Wkacn855ct_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gra√ümann-Algebra (Exterior Algebra)**\n",
        "\n",
        "Die Gra√ümann-Algebra $\\Lambda V$ eines reellen Vektorraumes $V$ ist die Clifford-Algebra $Cl(V,0)$ mit der trivialen quadratischen Form $Q=0$.\n",
        "\n",
        "Diese Beziehung ist unter anderem f√ºr die Quantisierung supersymmetrischer Feldtheorien wichtig.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Clifford-Algebra#Gra√ümann-Algebra\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) bzw. [Exterior Algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra): Algebra der Differentialformen.\n",
        "\n",
        "* Exterior Algebra eines Vektorraums V ist eine assoziative, schiefsymmetrisch-graduierte Algebra mit Einselement.\n",
        "\n",
        "* The exterior algebra provides an algebraic setting in which to answer geometric questions.\n",
        "\n",
        "* **Exterior algebra, geometric algebra, and clifford algebra are linear algebras** [Quora](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* Sie ist ‚Äì je nach Definition ‚Äì **Unteralgebra oder eine Faktoralgebra einer antisymmetrisierten** [**Tensoralgebra**](https://de.m.wikipedia.org/wiki/Tensoralgebra) von V und wird durch $\\Lambda V$ dargestellt.\n",
        "\n",
        "* Die Multiplikation wird als **√§u√üeres Produkt, Keilprodukt, Dachprodukt oder Wedgeprodukt** bezeichnet. Ein Spezialfall dieses Produkts ist mit dem Kreuzprodukt verwandt.\n",
        "\n",
        "* Anwendung: linearen Algebra (Theorie der Determinanten), Differentialgeometrie (Algebra der Differentialformen)"
      ],
      "metadata": {
        "id": "PaJipKZ_JnRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Vectors ($k$-blades)*"
      ],
      "metadata": {
        "id": "WqR3v9EaJrvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exterior Product (Wedge product u $\\wedge$ v) in Exterior Algebra (Gra√ümann-Algebra)**\n",
        "\n",
        "* **The exterior product (also called the Wedge product u $\\wedge$ v) used to construct bivectors and multivectors (is multilinear)**\n",
        "\n",
        "* the [exterior product or wedge product of vectors](https://en.m.wikipedia.org/wiki/Exterior_algebra) is an algebraic construction used in geometry to study areas, volumes, and their higher-dimensional analogues.\n",
        "\n",
        "> **The exterior product (also called the wedge product) used to construct bivectors and multivectors (is multilinear)**: The wedge product of two vectors is another object commonly called a bivector, and these lie in their own vector space. [Quora](https://www.quora.com/Is-the-wedge-product-the-same-thing-as-the-general-outer-product-If-not-why-is-the-wedge-symbol-used-for-both)\n",
        "\n",
        "* The exterior product of two vectors $u$ and $v$, denoted by $u\\wedge v$, is called a [bivector](https://en.m.wikipedia.org/wiki/Bivector) and lives in a space called the exterior square, a vector space that is distinct from the original space of vectors.\n",
        "\n",
        "* The exterior product, commonly called the wedge product, acts on tangent vectors and is an important operation in differential geometry that generalizes the cross product of 3-vectors.\n",
        "\n",
        "* See also: https://towardsdatascience.com/exterior-product-ecd5836c28ab\n",
        "\n",
        "* Add-on Differentiation:\n",
        "\n",
        "  * The **exterior product** is related to the tensor product in that the exterior product of two forms (a form is a skew-symmetric tensor of type (0,ùëù)) is just the antisymmetrization of the tensor product.\n",
        "\n",
        "  * The **cross product** is a speciality of the three-dimensional space; here the space of 2-forms has the same dimension as the space of 1-forms; indeed, given a metric, the hodge star maps between them. Since the metric also allows to associate vectors and 1-forms, you can define the cross product of v and ùë§ by the following procedure: Determine the 1-forms corresponding to ùë£ and ùë§, calculate their exterior product (which is a 2-form), apply the Hodge star to the result (which, given that we are in three dimensions, again results in a 1-form), and finally determine the vector corresponding to that 1-form.\n",
        "\n",
        "  * [Cross product as an external product](https://en.m.wikipedia.org/wiki/Cross_product#Cross_product_as_an_external_product)\n",
        "\n",
        "  * https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "9yW5RW7IJtlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$k$-blade ($k$-vector)**\n",
        "\n",
        "* a [$k$-blade](https://en.m.wikipedia.org/wiki/Blade_(geometry)) or a simple $k$-vector is a generalization of the concept of scalars and vectors to include simple bivectors, trivectors, etc.\n",
        "\n",
        "* Specifically, a $k$ blade is a $k$-vector that can be expressed as the exterior product (informally wedge product) of 1-vectors, and is of grade $k$. In detail:\n",
        "\n",
        "  * A 0-blade is a scalar.\n",
        "\n",
        "  * A 1-blade is a vector. Every vector is simple.\n",
        "\n",
        "  * A 2-blade is a simple bivector. Sums of 2-blades are also bivectors, but not always simple. A 2-blade may be expressed as the wedge product of two vectors $a$ and $b$ : $a \\wedge b$.\n",
        "\n",
        "  * A 3-blade is a simple trivector, that is, it may be expressed as the wedge product of three vectors $a, b$, and $c$ : $a \\wedge b \\wedge c \\text {. }$\n",
        "\n",
        "  * In a vector space of dimension $n$, a blade of grade $n-1$ is called a [pseudovector](https://en.m.wikipedia.org/wiki/Pseudovector) or an [antivector](https://en.m.wikipedia.org/wiki/Antivector)"
      ],
      "metadata": {
        "id": "ffxYeT3eJvhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Antisymmetry**\n",
        "\n",
        "> $\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2}=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}$\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} &=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1} \\wedge \\mathbf{e}_{3} \\\\\n",
        "&=\\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} \\wedge \\mathbf{e}_{1} \\\\\n",
        "&=-\\mathbf{e}_{3} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra"
      ],
      "metadata": {
        "id": "2ouXP-bDJxWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivector (Clifford number)**\n",
        "\n",
        "* In multilinear algebra, a [multivector](https://en.m.wikipedia.org/wiki/Multivector), sometimes called **Clifford number**,  is an element of the exterior algebra $\\Lambda(V)$ of a vector space $V$.\n",
        "\n",
        "* The exterior product $\\wedge$ (Wedge product) used to construct [bivectors](https://en.m.wikipedia.org/wiki/Bivector) and [multivectors](https://en.m.wikipedia.org/wiki/Multivector) (is multilinear)\n",
        "\n",
        "* This algebra is graded, associative and alternating, and consists of linear combinations of simple $k$ -vectors, also known as decomposable $k$ -vectors  or [$k$-blades](https://en.m.wikipedia.org/wiki/Blade_(geometry)), of the form\n",
        "\n",
        "> $v_{1} \\wedge \\cdots \\wedge v_{k}$\n",
        "\n",
        "where $v_{1}, \\ldots, v_{k}$ are in $V$.\n",
        "\n",
        "* A $k$ -vector is such a linear combination that is homogeneous of degree $k$ (all terms are\n",
        "$k$ -blades for the same $k$ ). Depending on the authors, a \"multivector\" may be either a $k-$\n",
        "vector or any element of the exterior algebra (any linear combination of $k$ -blades with potentially differing values of $k$ ).\n",
        "\n",
        "* In differential geometry, a $k$ -vector is a vector in the exterior algebra of the tangent\n",
        "vector space; that is, it is an antisymmetric tensor obtained by taking linear\n",
        "combinations of the exterior product of $k$ tangent vectors, for some integer $k \\geq 0 .$\n",
        "\n",
        "* A differential $k$ -form is a $k$ -vector in the exterior algebra of the dual of the tangent space,\n",
        "which is also the dual of the exterior algebra of the tangent space.\n",
        "\n",
        "> **For $k=0,1,2$ and $3, k$ -vectors are often called respectively scalars, vectors, bivectors and trivectors; they are respectively dual to 0 -forms, 1 -forms, 2 -forms and 3 forms.**"
      ],
      "metadata": {
        "id": "JbnIY2DPJzO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation for the exterior product of n 1-forms (Œµ, Œ∑, œâ) to obtain an n-form (\"mesh\" of coordinate surfaces, here planes), for n = 1, 2, 3. The \"circulations\" show orientation*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0938.png)"
      ],
      "metadata": {
        "id": "yyX6pjvrJ1J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation of grade n elements in a real exterior algebra for n = 0 (signed point), 1 (directed line segment, or vector), 2 (oriented plane element), 3 (oriented volume). The exterior product of n vectors can be visualized as any n-dimensional shape (e.g. n-parallelotope, n-ellipsoid); with magnitude (hypervolume), and orientation defined by that of its (n ‚àí 1)-dimensional boundary and on which side the interior is.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0939.png)"
      ],
      "metadata": {
        "id": "vcYQLc-WJ3H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parallel plane segments with the same orientation and area corresponding to the same bivector a ‚àß b:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Wedge_product.JPG/471px-Wedge_product.JPG)"
      ],
      "metadata": {
        "id": "7-GwR_RyJ40H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Maps (Operators)*"
      ],
      "metadata": {
        "id": "JYooOdCtJ76C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "XG6PAvtPN-V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Eine [lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)* from vector space(s) to a vector space\n",
        "\n",
        "> $f\\colon V\\to V$\n",
        "\n",
        "* [Lineare Abbildung = Linear Maps](https://de.wikipedia.org/wiki/Lineare_Abbildung): Eine lineare Abbildung (auch lineare Transformation oder Vektorraumhomomorphismus $\\varphi: V \\longrightarrow \\omega$ genannt) ist in der linearen Algebra ein wichtiger Typ von **Abbildung zwischen zwei Vektorr√§umen √ºber demselben K√∂rper**.\n",
        "\n",
        "* Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt f√ºr die Multiplikation mit einem Skalar aus dem Grundk√∂rper.\n",
        "\n",
        "Algebraische Eigenschaften:\n",
        "\n",
        "> $\\varphi\\left(v_{1}+v_{2}\\right)=\\varphi\\left(v_{c}\\right)+\\varphi\\left(v_{2}\\right)$\n",
        "\n",
        "> $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v)$\n",
        "\n",
        "* Beispiel: matrix multiplication\n",
        "\n",
        "* Eine lineare Abbildung $f\\colon V\\to V$ (also ein Endomorphismus) eines endlichdimensionalen Vektorraumes $V$ ist bereits invertierbar, wenn sie injektiv oder surjektiv ist.\n",
        "\n",
        "* Dies ist wiederum genau dann der Fall, wenn ihre Determinante ungleich null ist.\n",
        "\n",
        "* Hieraus folgt, dass die Eigenwerte eines Endomorphismus genau die Nullstellen seines charakteristischen Polynoms sind.\n",
        "\n",
        "* Eine weitere wichtige Aussage √ºber das charakteristische Polynom ist der Satz von Cayley-Hamilton.\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$. <font color=\"red\">Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)</font>\n",
        "\n",
        "* [LINEARE ABBILDUNG / Homomorphismus einfach erkl√§rt](https://www.youtube.com/watch?v=KK_fHodz-lQ)\n",
        "\n",
        "* Lineare Abbildungen als Tensoren:\n",
        "\n",
        "  * **Case 1: Linear Maps for Transformations of vectors within one basis (with linear maps)**: rank 1 tensor (1,0)-tensor, 1 axe\n",
        "\n",
        "  * **Case 2: Linear Maps for Transformations of vectors across two basis (with linear maps + forward/backward)**: Linear Transformation as a (1,1)-Tensor - rank 2 tensor, which usually is represented as a matrix (e.g. 2 axes). For example changing the linear map $L$ in one basis (i.e. Euclidean basis) and rotate basis where we define a new linear map $\\widetilde{L}$. We're not just multiplying by the inverse transform (contravariant), nor just the forward transform (covariant), we're doing both, which hints that this is a (1,1)-tensor!\n",
        "\n",
        "Bildung des Vektorraums $L(V,W)$:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/4/49/Vektorraum_linearer_Abbildungen.svg)"
      ],
      "metadata": {
        "id": "XR5ccHQcMM3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Besondere lineare Abbildungen:**](https://de.wikipedia.org/wiki/Lineare_Abbildung#Besondere_lineare_Abbildungen)\n",
        "\n",
        "* **Monomorphismus**\n",
        "Ein Monomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die injektiv ist. Dies trifft genau dann zu, wenn die Spaltenvektoren der Darstellungsmatrix linear unabh√§ngig sind.\n",
        "\n",
        "* **Epimorphismus**\n",
        "Ein Epimorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die surjektiv ist. Das ist genau dann der Fall, wenn der Rang der Darstellungsmatrix gleich der Dimension von $W$ ist.\n",
        "\n",
        "\n",
        "* **Isomorphismus**\n",
        "Ein Isomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung $f: V \\rightarrow W$, die bijektiv ist. Das ist genau der Fall, wenn die Darstellungsmatrix regul√§r ist. Die beiden R√§ume $V$ und $W$ bezeichnet man dann als isomorph.\n",
        "\n",
        "* **Endomorphismus**\n",
        "Ein Endomorphismus zwischen Vektorr√§umen ist eine lineare Abbildung, bei der die R√§ume $V$ und $W$ gleich sind: $f: V \\rightarrow V$. Die Darstellungsmatrix dieser Abbildung ist eine quadratische Matrix.\n",
        "\n",
        "* **Automorphismus**\n",
        "Ein Automorphismus zwischen Vektorr√§umen ist eine bijektive lineare Abbildung, bei der die R√§ume $V$ und $W$ gleich sind. Er ist also sowohl ein Isomorphismus als auch ein Endomorphismus. Die Darstellungsmatrix dieser\n",
        "Abbildung ist eine regul√§re Matrix."
      ],
      "metadata": {
        "id": "xEceSoEzMS6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lineare Operatoren (unendlichdimensional)**\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung [unendlichdimensionaler Vektorr√§ume](https://de.m.wikipedia.org/wiki/Lineare_Abbildung#Lineare_Abbildungen_zwischen_unendlichdimensionalen_Vektorr√§umen), die eine Topologie tragen, spricht man meist von [linearen Operatoren](https://de.wikipedia.org/wiki/Linearer_Operator) statt von linearen Abbildungen. Formal gesehen sind die Begriffe gleichbedeutend.\n",
        "\n",
        "* Die Bedeutung linearer Operatoren besteht darin, dass sie die lineare Struktur des unterliegenden Raumes respektieren, d. h., sie sind Homomorphismen zwischen Vektorr√§umen [Source](https://de.wikipedia.org/wiki/Linearer_Operator)\n",
        "\n",
        "* Der Begriff linearer Operator wurde in der Funktionalanalysis (einem Teilgebiet der Mathematik) eingef√ºhrt und ist **synonym zum Begriff der linearen Abbildung**. Eine lineare Abbildung ist eine **strukturerhaltende Abbildung zwischen Vektorr√§umen √ºber einem gemeinsamen K√∂rper**.\n",
        "\n",
        "* Werden Vektorr√§ume √ºber dem K√∂rper der reellen oder komplexen Zahlen betrachtet und sind diese mit einer Topologie versehen (lokalkonvexe R√§ume, normierte R√§ume, Banachr√§ume), so spricht man vorzugsweise von linearen Operatoren.\n",
        "\n",
        "* **Anwendungen linearer Operatoren sind:**\n",
        "\n",
        "  * Die Beschreibung von Koordinatentransformationen im dreidimensionalen Euklidischen Raum (**Spiegelung, Drehung, Streckung**) und der [Lorentztransformation](https://de.wikipedia.org/wiki/Lorentz-Transformation) in der vierdimensionalen Raumzeit durch Matrizen.\n",
        "\n",
        "  * **Die Entwicklung von L√∂sungstheorien f√ºr Differential- und Integralgleichungen, siehe Sobolew-Raum und Distribution.**\n",
        "\n",
        "  * Die Darstellung von Observablen in der Quantenmechanik und die Beschreibung der Dynamik eines quantenmechanischen Systems durch seinen Hamilton-Operator H in der Schr√∂dingergleichung."
      ],
      "metadata": {
        "id": "xMl0YGFkMUnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Operatoralgebra*\n",
        "\n",
        "* Welche Homomorphismen von einer Banachalgebra in eine Operatoralgebra existieren, wird in der Darstellungstheorie untersucht. Ein besonderes Interesse gilt dabei Darstellungen auf Hilbertr√§umen, das hei√üt Homomorphismen in die Operatoralgebra √ºber einem Hilbertraum, was zu den Begriffen Von-Neumann-Algebra und C*-Algebra f√ºhrt.\n",
        "\n",
        "* Siehe also [Chemtext](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_%28Levitus%29/11%3A_Operators/11.02%3A_Operator_Algebra)\n",
        "\n",
        "* [Operatoralgebren](https://de.m.wikipedia.org/wiki/Operatoralgebra) sind  **Verallgemeinerungen der [Matrizenalgebren](http://www-hm.ma.tum.de/ws0607/ei1/folien/Folie_11_RechnenMitMatrizen.pdf)** der linearen Algebra\n",
        "\n",
        "* Sind $E, F, G$ normierte R√§ume und $A: E \\rightarrow F$ und $B: F \\rightarrow G$ stetige, lineare Operatoren, so ist auch deren Komposition ein stetiger, linearer Operator $B \\circ A: E \\rightarrow G,$ und f√ºr die Operatornormen gilt $\\|B \\circ A\\| \\leq\\|B\\| \\cdot\\|A\\| .$\n",
        "\n",
        "* Daher wird der Raum $L(E)$ der stetigen, linearen Operatoren von $E$ in sich mit der Komposition als Multiplikation zu einer normierten Algebra, die bei vollst√§ndiaem $E$ sogar eine Banachalgebra ist.\n",
        "\n",
        "* Diese Algebren und ihre Unteralgebren nennt man Operatoralgebren, wobei der Fall, dass $E$ ein Hilbertraum ist, besonders intensiv untersucht wird."
      ],
      "metadata": {
        "id": "z1znuc7zMWf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Beschr√§nkter Operator*](https://de.wikipedia.org/wiki/Beschr√§nkter_Operator)\n",
        "\n",
        "* Im Gegensatz zu endlichdimensionalen R√§umen, wo lineare Operatoren stets beschr√§nkt sind, tauchen bei unendlichdimensionalen R√§umen auch unbeschr√§nkte lineare Operatoren auf.\n",
        "\n",
        "* Ein linearer Operator ist genau dann beschr√§nkt, wenn er stetig ist, also eine der\n",
        "folgenden √§quivalenten Bedingungen erf√ºlit:\n",
        "\n",
        "  * falls $x_{n} \\rightarrow x,$ so gilt $T x_{n} \\rightarrow T x$ in der von der jeweiligen Norm induzierten\n",
        "Metrik,\n",
        "\n",
        "  * f√ºr alle $x_{0} \\in X$ und alle $\\epsilon>0$ gibt es ein $\\delta>0$ mit\n",
        "$\n",
        "\\left\\|x-x_{0}\\right\\|<\\delta \\Rightarrow\\left\\|T x-T x_{0}\\right\\|<\\epsilon\n",
        "$\n",
        "\n",
        "  * Urbilder offener Mengen sind offen.\n",
        "\n",
        "* Beschr√§nkte lineare Operatoren werden deshalb oft als **stetige lineare Operatoren** bezeichnet. Wenn die Linearit√§t vorausgesetzt wird, spricht man h√§ufig auch nur von stetigen Operatoren oder beschr√§nkten Operatoren. **Ist der Bildraum der Skalarenk√∂rper, sagt man Funktional statt Operator.**"
      ],
      "metadata": {
        "id": "GdGpj-kGMYfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Unbeschr√§nkte lineare Operatoren auf Hilbertr√§umen*:\n",
        "\n",
        "* l√§sst man oft auch Operatoren zu, deren Definitionsbereich (Dom√§ne) lediglich ein Unterraum des betrachteten Raumes ist, so l√§sst man als Definitionsbereich auch einen Pr√§hilbertraum als Teilraum eines Hilbertraums zu, pr√§ziser spricht man dann von dicht definierten unbeschr√§nkten linearen Operatoren (s. u.). Der Operator wird als **partielle Abbildung** aufgefasst.\n",
        "\n",
        "* **Differential- und Multiplikationsoperatoren sind i. A. unbeschr√§nkt.** (Viele Anwendungsbeispiele werden durch unbeschr√§nkte Operatoren beschrieben (z.B. Differentialoperatoren). W√§hrend beschr√§nkte Operatoren auf dem ganzen Hilbertraum H definiert sind, werden unbeschr√§nkte Operatoren immer auf einem Definitionsbereich D(T ) ‚äÇ H betrachtet.)\n",
        "\n",
        "* Die Darstellung von Observablen der Quantenmechanik erfordert unbeschr√§nkte lineare Operatoren, da die den Observablen zugeordneten Operatoren i. A. unbeschr√§nkt sind.\n",
        "\n",
        "* Der [Laplace-Operator](https://de.wikipedia.org/wiki/Laplace-Operator) = [linearen Differentialoperator](https://de.m.wikipedia.org/wiki/Differentialoperator#Linearer_Differentialoperator) $\\Delta \\colon D(\\Delta )\\to L^{2}(\\mathbb{R} ^{n})$ ist ein unbeschr√§nkter Operator."
      ],
      "metadata": {
        "id": "JEZDMINdMaCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Selbstadjungierte & symmetrische Operatoren*\n",
        "\n",
        "* Sei $H$ ein Hilbertraum. Ein linearer Operator $T: D(T) \\rightarrow H$ hei√üt symmetrisch bzw. selbstadjungiert, falls\n",
        "\n",
        "> $\\langle T y, x\\rangle=\\langle y, T x\\rangle$\n",
        "\n",
        "* Der [selbstadjungierte Operator](https://de.wikipedia.org/wiki/Selbstadjungierter_Operator) ist eine Verallgemeinerung der selbstadjungierten Matrix. Siehe auch [Adjungierter_Operator](https://de.m.wikipedia.org/wiki/Adjungierter_Operator)\n",
        "\n",
        "* ein [symmetrischer Operator](https://de.wikipedia.org/wiki/Symmetrischer_Operator) ist ein **linearer Operator** und wird in der Funktionalanalysis im Kontext **unbeschr√§nkter Operatoren** betrachtet\n",
        "\n",
        "* ein **beschr√§nkter symmetrischer Operator ist ein selbstadjungierter Operator**.\n",
        "\n",
        "* **Unterschiede:**\n",
        "\n",
        "  * Symmetrische Operatoren koennen auch nicht-reelle Eigenwerte haben (im Gegensatz zu den selbstadjungierten Operatoren)\n",
        "\n",
        "  * Bei symmetrischen Operatoren wird nicht gefordert, dass der Operator $T$ dicht definiert sein muss (im Gegensatz zum selbstadjungierten Operator)\n",
        "\n",
        "  * Ist $T$ dicht definiert (und damit der adjungierte Operator wohl definiert), so ist $T$ genau dann symmetrisch, wenn $T\\subseteq T^{*}$ gilt.\n",
        "\n",
        "  * Nur fur selbstadjungierten Operatoren kann eine Spektralzerlegung gezeigt werden\n",
        "\n",
        "  * F√ºr beschr√§nkte Operatoren fallen die Begriffe selbstadjungiert und symmetrisch zusammen (daher sind symmetrische, nicht selbstadjungierte Operatoren immer unbeschr√§nkt)"
      ],
      "metadata": {
        "id": "0eCn9if2MbwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **In mathematics, a bilinear operator is a generalized \"multiplication\" which satisfies the distributive law.**\n",
        "\n",
        "[*Bilinearen Abbildungen*](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung) *verallgemeinern die verschiedensten Begriffe von Produkten (im Sinne einer Multiplikation). Die Bilinearit√§t entspricht dem Distributivgesetz $a \\cdot (b + c) = a \\cdot b + a \\cdot c$ bei der normalen Multiplikation.*\n",
        "\n",
        "* Jede bilineare Abbildung (Map) ist eine 2-lineare Abbildung. (S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "* **Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen**. Bei diesen ist der Wertebereich G mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch.)\n",
        "\n",
        "For a formal definition, given three vector spaces V, W and X over the same base field F, a bilinear operator is a function\n",
        "\n",
        "$B : V √ó W ‚Üí X$\n",
        "\n",
        "> [*A bilinear map*](https://en.m.wikipedia.org/wiki/Bilinear_map) *is a function combining elements of two vector spaces to yield an element of a third vector space, and is linear in each of its arguments. Matrix multiplication is an example as visible in $B : V √ó W ‚Üí X$. In contrast: linear map war eine matrix als map von vector zu vector, das hier ist eine matrix als map zw matrix und matrix!*\n",
        "\n",
        "such that for any w in W the map\n",
        "\n",
        "$v \\mapsto B(v, w)$\n",
        "\n",
        "is a linear operator from V to X, and for any v in V the map\n",
        "\n",
        "$w \\mapsto B(v, w)$\n",
        "\n",
        "is a linear operator from W to X. In other words, if we hold the first entry of the bilinear operator fixed, while letting the second entry vary, the result is a linear operator, and similarly if we hold the second entry fixed.\n",
        "\n",
        "* If V = W and we have B(v,w)=B(w,v) for all v,w in V, then we say that B is symmetric.\n",
        "\n",
        "> **The case where X is a field F for $B : V √ó W ‚Üí X$, and we have a bilinear form, is particularly useful (see for example scalar product, inner product and quadratic form).**\n",
        "\n",
        "* The definition works without any changes if instead of vector spaces we use modules over a commutative ring R. It also can be easily generalized to n-ary functions, where the proper term is multilinear.\n",
        "\n",
        "* For the case of a non-commutative base ring R and a right module MR and a left module RN, we can define a bilinear operator B : M √ó N ‚Üí T, where T is a commutative group, such that for any n in N, m |-> B(m, n) is a group homomorphism, and for any m in M, n |-> B(m, n) is a group homomorphism, and which also satisfies\n",
        "\n",
        "$B(mr, n) = B(m, rn)$\n",
        "\n",
        "for all m in M, n in N and r in R.\n",
        "\n",
        "https://academickids.com/encyclopedia/index.php/Bilinear_operator\n",
        "\n",
        "* Bilinear = gemischtes Assoziativgesetz & Distributivgesetz\n",
        "\n",
        "* [Bilineare Abbildungen](https://de.m.wikipedia.org/wiki/Bilineare_Abbildung) verallgemeinern die verschiedensten Begriffe von Produkten (im Sinne einer Multiplikation).\n",
        "\n",
        "* Die Bilinearit√§t entspricht dem Distributivgesetz bei der normalen Multiplikation:\n",
        "\n",
        ">$\n",
        "a \\cdot(b+c)=a \\cdot b+a \\cdot c\n",
        "$\n",
        "\n",
        "* Beispiel:\n",
        "\n",
        "  * S√§mtliche gemeinhin √ºbliche Produkte sind bilineare Abbildungen: die Multiplikation in einem K√∂rper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen),\n",
        "\n",
        "  * aber auch das Vektor- oder Kreuzprodukt,\n",
        "\n",
        "  * und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n"
      ],
      "metadata": {
        "id": "JOtdUCczMd0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear Map**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multilinear_map\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Multilineare_Abbildung\n",
        "\n",
        "> Multilinear Map - from Tensors to Tensor Spaces (Product Spaces & Vector Spaces ‚äó)\n",
        "\n",
        "> Riemann Curvature Tensor is a multilinear map. Wie findet man heraus, ob eine Surface flach oder gekruemmt ist?\n",
        "\n",
        "**Multilinear Maps: So the strongest kind of linearity we could reasonably impose is that $m$ is linear in each coordinate when all else is fixed**.\n",
        "\n",
        "https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Matrix_multiplication\n"
      ],
      "metadata": {
        "id": "LhNgU-EEMp8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product Spaces $\\otimes$**\n",
        "\n",
        "[Eigenchris: Tensors for Beginners 15: Tensor Product Spaces](https://www.youtube.com/watch?v=M-OLmxuLdbU&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=18)\n",
        "\n",
        "Im mathematischen Teilgebiet der linearen Algebra und verwandter Gebiete wird durch die [multilineare Abbildung](https://de.m.wikipedia.org/wiki/Multilineare_Abbildung) der Begriff der linearen Abbildung verallgemeinert. Ein wichtiges Beispiel einer multilinearen Abbildung ist die Determinante.\n",
        "\n",
        "In linear algebra, a [multilinear map](https://en.m.wikipedia.org/wiki/Multilinear_map) is a function of several variables that is linear separately in each variable. More precisely, a multilinear map is a function\n",
        "\n",
        "> $\n",
        "f: V_{1} \\times \\cdots \\times V_{n} \\rightarrow W\n",
        "$\n",
        "\n",
        "where $V_{1}, \\ldots, V_{n}$ and $W$ are vector spaces (or modules over a commutative\n",
        "ring), with the following property: for each $i$, if all of the variables but $v_{i}$ are held constant, then $f\\left(v_{1}, \\ldots v_{i}, \\ldots v_{n}\\right)$ is a linear function of $v_{i} .$\n",
        "\n",
        "A multilinear map of one variable is a linear map, and of two variables is a bilinear map. More generally, a multilinear map of k variables is called a k-linear map. **If the codomain of a multilinear map is the field of scalars, it is called a multilinear form**. Multilinear maps and multilinear forms are fundamental objects of study in multilinear algebra.\n",
        "\n",
        "\n",
        "For multilinear maps used in cryptography, see [Cryptographic multilinear map](https://en.m.wikipedia.org/wiki/Cryptographic_multilinear_map)."
      ],
      "metadata": {
        "id": "33s1HgP6M5rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product Rules**\n",
        "\n",
        "**Tensor Product Operation Rules - which gives us a (Tensor) Vector Space**\n",
        "\n",
        "Tensor product: Combining tensors that follow these scaling and adding rules:\n",
        "\n",
        "> $n(\\vec{v} \\alpha)=(n \\vec{v}) \\alpha=\\vec{v}(n \\alpha)$ (*Note the scaling rule: scale only with one side, not both at the same time!*)\n",
        "\n",
        "> $\\vec{v} \\alpha+\\vec{v} \\beta=\\vec{v}(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\alpha+\\vec{w} \\alpha=(\\vec{v}+\\vec{w}) \\alpha$\n",
        "\n",
        "And written in proper tensor notation:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_63.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_64.png)\n"
      ],
      "metadata": {
        "id": "mxTsLRy3NHv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tensor Products as Multilinear Maps*\n",
        "\n",
        "**What do all these functions have in common?**\n",
        "\n",
        "* If you make all inputs constant except one, we can scale the input before or scale the output after and we get the same result\n",
        "\n",
        "* And also if we replace these inout vector components with a sum of two sets of vector components I can just distribute these out and get this sum here (red line under equation in image), so basically I can add the inputs or I can add the outputs.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_73.png)\n",
        "\n",
        "> **Multilinear Map: A function that is linear when all inputs except one are held constant** (they are linear in each input variable)\n",
        "\n",
        "* when we scale the input variable by $n$ (and all other are held constant) that's the same as scaling the ouput of the function by $n$\n",
        "\n",
        "* when we hold all input constant except one, and we do a sum in the input slot, that's the same thing as doing the sum of the outputs (in image below)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_71.png)\n",
        "\n",
        "And all tensors are **multilinear maps** which means they are functions that take some number of inputs and they are linear in each input variable while all other input variables are held constant (kind of ceteris paribus in economics!).\n",
        "\n",
        "**Tensor Product $\\otimes$**\n",
        "\n",
        "* The outer product of tensors is also referred to as their [tensor product](https://en.m.wikipedia.org/wiki/Tensor_product), and can be used to define the tensor algebra.  https://en.m.wikipedia.org/wiki/Outer_product\n",
        "\n",
        "> The tensor product can be considered as a generalization and abstraction of the outer product. https://en.m.wikipedia.org/wiki/Tensor_product\n",
        "\n",
        "* a collection of tensor products attached to another space is called a [tensor bundle](https://en.m.wikipedia.org/wiki/Tensor_field)\n",
        "\n",
        "* Siehe auch: https://www.math3ma.com/blog/the-tensor-product-demystified"
      ],
      "metadata": {
        "id": "-OjmCCbxNnzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Forms (Functionals)*"
      ],
      "metadata": {
        "id": "MMxCx0fZKDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Form (Lineare Algebra) - 1-Form**:\n",
        "\n",
        "* they take one vector as input to output a number (scalar):\n",
        "\n",
        "> $\\mathcal{B}: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * A [linear form](https://de.m.wikipedia.org/wiki/Linearform) is a function that takes one or more vectors as input and outputs a number (= lineare Abbildung von einem Vektorraum in den zugrundeliegenden K√∂rper)\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * Algebraische Regeln: Es sei $K$ ein K√∂rper und $V$ ein $K$ -Vektorraum. Eine Abbildung $f: V \\rightarrow K$ hei√üt Linearform, wenn f√ºr alle Vektoren $x, y \\in V$ und Skalare $\\alpha \\in K$ gilt:\n",
        "\n",
        "    * $f(x+y)=f(x)+f(y)$ (**Additivit√§t**);\n",
        "\n",
        "    * $f(\\alpha x)=\\alpha f(x)$ (**Homogenit√§t**).\n",
        "\n",
        "  * Linear Form = 1-Form = Functional (incl Distribution) = Covector = Dualvector = Differentialformen\n",
        "\n",
        "> **A column vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "x \\\\\n",
        "y\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>vector</u>.** i.e. Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state'\n",
        "\n",
        "> **A row vector $\\begin{equation}\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>covector (linear form, functional)</u>.** i.e. Bra $[b_{0} \\quad b_{1}]$\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti‚Äìisomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra‚Äìket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ‚ü®0‚à£1‚ü© und ‚ü®1‚à£0‚ü© ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ‚ü®0‚à£0‚ü© und ‚ü®1‚à£1‚ü© = 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "THMYOFHeKF__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector (Tensor Algebra)**: in der Physik verwendet man gerne die Sprache der Tensoralgebra:\n",
        "\n",
        "  * dann hei√üen die Elemente von $V$ **kontravariante Vektoren** und die von $V^{*}$ heissen **kovariante Vektoren oder auch [Kovektoren](https://en.m.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors)**\n",
        "\n",
        "  * Covectors are functions from vectors to real numbers. They take one vector and output a scalar: $\\alpha: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "  * This scalar can be a integrand (Wegintegral) or Differential, Temperature, Speed etc.(meanwhile a metric tensor is two-form because it takes 2 vectors to output one scalar, which is length or angle).\n",
        "\n",
        "  * They follow the linearity properties: $\\alpha(\\vec{v}+\\vec{w})=\\alpha(\\vec{v})+\\alpha(\\vec{w})$ and $\\alpha(n \\vec{v})=n \\alpha(\\vec{v})$\n",
        "\n",
        "  * Eine Linearform $f$ ist ein kovarianter Tensor erster Stufe; man nennt sie deshalb manchmal auch [1-Form (Pfaffsche Form)](https://de.m.wikipedia.org/wiki/Pfaffsche_Form). <font color=\"red\">Linear forms are (0,1) tensors</font> (so they transform using 1 covariant rule when we change coordinate systems)\n",
        "\n",
        "  * Die Abbildung $V \\times V^{*} \\rightarrow K,(x, f) \\mapsto\\langle x, f\\rangle:=f(x)$ ist eine nicht ausgeartete Bilinearform und hei√üt duale Paarung.\n",
        "\n",
        "  * All covectors can be written as the linear combination of the dual basis vectors!\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 4: What are Covectors?](https://www.youtube.com/watch?v=LNoQ_Q5JQMY)"
      ],
      "metadata": {
        "id": "4HJ0C217KH3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funktional (Functional Analysis)**:\n",
        "\n",
        "* Im Kontext der Funktionalanalysis, im topologischen $\\mathbb {R}$ - oder $\\mathbb {C}$-Vektorraums, sind die betrachteten Linearformen meistens [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale). Aber: 'Linear form' is a more modern and abstract concept of 'functional'\n",
        "\n",
        "* Sei $V$ ein $\\mathbb{K}$ -Vektorraum mit $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\} .$ Ein Funktional $T$ ist eine Abbildung $T: V \\rightarrow \\mathbb{K} .$\n",
        "\n",
        "* **Function:**\n",
        "$\\mathbb{R} \\rightarrow f(x) \\rightarrow \\mathbb{R}$. **Functional:**\n",
        "$f(x) \\rightarrow J[f(x)] \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* A typical example of a linear functional is integration [Source](https://en.m.wikipedia.org/wiki/Linear_form)\n",
        "\n",
        "* In Funktionalanalyse ist der untersuchte Vektorraum $V$ zumeist ein Funktionenraum, wo diesen durch Funktionale Skalare zugeordnet werden. Beispiel: [Lebesgue-Integral](https://de.m.wikipedia.org/wiki/Lebesgue-Integral).\n",
        "\n",
        "* Eine Funktion bildet Elemente eines K√∂rpers auf andere ab, zum Beispiel. $f: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$. Ein Funktional hingegen ist ein Operator, der Elemente eines Vektorraumes $V$ auf den [Skalark√∂rper](https://de.wikipedia.org/wiki/Skalar_(Mathematik)) $K$ abbildet, √ºber dem $V$ modelliert ist. Diese Elemente k√∂nnen zum Beispiel selber Funktionen sein (wenn der Vektorraum ein Funktionenraum ist)\n",
        "\n",
        "* Siehe auch [Funktionaldeterminante](https://de.wikipedia.org/wiki/Funktionaldeterminante) oder Jacobi-Determinante fur Koordinatentransformationen zB von kartesisches zu Polarkoordinaten in der mehrdimensionalen Integralrechnung, also der **Berechnung von Oberfl√§chen- und Volumenintegralen**  an.\n",
        "\n",
        "* Siehe auch [Functional integration](https://en.m.wikipedia.org/wiki/Functional_integration): Richard Feynman used functional integrals as the central idea in his sum over the histories formulation of quantum mechanics. This usage implies an integral taken over some function space."
      ],
      "metadata": {
        "id": "AdSJOOjuKJwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dualraum (Lineare Algebra)**:\n",
        "\n",
        "* https://www.quora.com/What-is-a-dual-vector-and-what-does-it-do\n",
        "\n",
        "* Die Menge aller Linearformen (= stetigen, linearen Abbildungen) √ºber einem gegebenen Vektorraum $V$ bildet dessen [Dualraum](https://de.wikipedia.org/wiki/Dualraum) $V^{*}$ und damit selbst wieder in nat√ºrlicher Weise einen $K$ -Vektorraum.\n",
        "\n",
        "* Zu einem Vektorraum $V$ √ºber einem K√∂rper $K$ bezeichnet $V^{*}$ den zu $V$ geh√∂rigen [Dualraum](https://de.m.wikipedia.org/wiki/Dualraum), das hei√üt die Menge aller linearen Abbildungen von $V$ nach $K$.\n",
        "\n",
        "* Die Menge aller Funktionale ist wiederum in nat√ºrlicher Form ein Vektorraum uber dem gleichen K√∂rper $\\mathbb{K}$, indem man f√ºr zwei Funktionale $f$ und $g$ √ºber $V$ die Addition und Skalarmultiplikation punktweise definiert, $d .$ h. $\n",
        "(f+g)(x):=f(x)+g(x) \\quad(\\lambda f)(x):=\\lambda(f(x)), x \\in V\n",
        "$. Der Vektorraum der linearen Funktionale auf dem Vektorraum $V$ wird der algebraische **Dualraum** genannt und oft mit $V^{*}$ bezeichnet.\n",
        "\n",
        "* Kovektoren als Linearformen von [Normalenvektoren](https://de.wikipedia.org/wiki/Normalenvektor) (das Ergebnis ist eine Zahl), ist sowas wie ein Zeilenvektor. Zeilenvektor (Kovektor, Linearform) c * Spaltenvektor (Normalenvektor) a = eine Zahl, zB 2\n",
        "\n",
        "* Aus Basis fur den Vektorraum V {e<sub>1</sub>, e<sub>2</sub>} kann man nun √ºberlegen, wie man daraus eine Basis fur den Dualraum V* bekommt {e<sup>1</sup>, e<sup>2</sup>}. Die Dualbasis soll jetzt einen Originalvektor aus V nehmen und eine reelle Zahl als Ergebnis liefern. Also zB fur Vektor im Originalvektorraum mit den Koordinaten von der Basis: a = 1,3 e<sub>1</sub> + 1,2 e<sub>2</sub> , dann sind 1,3 = e<sup>1</sup> und 1,2 = e<sup>2</sup>\n",
        "\n",
        "* Dualraum: https://youtu.be/2vvjrBbcTZU, [Video: What are Dual vectors](https://www.youtube.com/watch?v=T04Yq1D20AM)\n",
        "\n",
        "* [Video: An introduction to vectors and dual vectors](https://youtu.be/2MC4xMhscjQ)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_17.png)"
      ],
      "metadata": {
        "id": "Q3I9O2SVKLjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution (Partial Differential Equations) - Special Functional**:\n",
        "\n",
        "* Eine [Distribution](https://en.m.wikipedia.org/wiki/Distribution_(mathematics)) (english) bzw. [Distribution (deutsch)](https://de.m.wikipedia.org/wiki/Distribution_(Mathematik)) bzw. [mathepedia](https://mathepedia.de/Distributionen.html) bezeichnet eine **besondere Art eines Funktionals** (eine Verallgemeinerung)\n",
        "\n",
        "* But: **it allows only linear operations. Distributions cannot be multiplied** (except for very special cases). For example it is not meaningful to square the Dirac delta function.\n",
        "\n",
        "* Es gibt partielle Differentialgleichungen, die keine hinreichend oft differenzierbare oder gar keine klassischen L√∂sungen haben, aber L√∂sungen im distributionellen Sinn. Beispiel: 'Knicke' in Differentialgleichungen (zB Ableitung der [Heaviside Funktion](https://de.wikipedia.org/wiki/Heaviside-Funktion))\n",
        "\n",
        "* Nutzung der [Delta Funktion (Delta Distribution)](https://de.wikipedia.org/wiki/Delta-Distribution). Delta Funktion ist Null ausserhalb der kritischen Stelle Null. Aber: Delta ist keine Funktion im ublichen Sinn. Delta ist Null fast uberall (bez. des Lebesgue Masses). Das heisst jegliches Integral ist Null. Loesung: Delta wird als 'Distribution' definiert (oder auch: 'verallgemeinerte Funktion'). Funktionen fasst man jetzt als eine Dichte auf (zB Massedichte auf einem eindimensionalen Stab). Delta-Funktion ist dann eine singulare Dichte (Punktmasse). https://www.youtube.com/watch?v=c5WYrQK_7ls. **Delta-Distribution** ([Dirac-Funktion](https://de.m.wikipedia.org/wiki/Delta-Distribution)) ist eine spezielle irregul√§re Distribution mit kompaktem Tr√§ger. Sie hat in der Mathematik und Physik grundlegende Bedeutung. Ihr √ºbliches Formelsymbol ist Œ¥ (kleines Delta).\n",
        "\n",
        "* [Testfunktionen](https://de.m.wikipedia.org/wiki/Testfunktion):\n",
        "Formulier problem in variationell (sobolove r√§ume), und dann Eigenschaften von Testfunktionen ausnutzen. Als Testfunktionen bezeichnet man gewisse Typen von Funktionen, die in der Distributionentheorie eine wesentliche Rolle spielen. √úblicherweise **fasst man Testfunktionen eines bestimmten Typs zu einem Vektorraum** zusammen. **Die zugeh√∂rigen Distributionen sind dann lineare Funktionale auf diesen Vektorr√§umen**. Ihr Name r√ºhrt daher, dass man die Distributionen (im Sinne linearer Abbildungen) auf die Testfunktionen anwendet und dadurch testet. In der mathematischen Literatur werden h√§ufig der Raum der glatten Funktionen mit kompaktem Tr√§ger oder der [Schwarz Raum](https://de.m.wikipedia.org/wiki/Schwartz-Raum) als Testfunktionenraum bezeichnet.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3sfnX5WeKNY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bilinear Forms (=2-Forms)** takes two vectors as input to output a number (scalar): $k=2$,  $f:V\\times V\\to K$\n",
        "\n",
        "> $\\mathcal{B}: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* Algebraic rules - linearity properties:\n",
        "\n",
        "  * $a \\mathcal{B}(\\vec{v}, \\vec{w})=\\mathcal{B}(a \\vec{v}, \\vec{w})=\\mathcal{B}(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}+\\vec{u}, \\vec{w})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}, \\vec{w}+\\vec{t})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{v}, \\vec{t})$\n",
        "\n",
        "* [Bilinear Forms](https://en.m.wikipedia.org/wiki/Bilinear_form) sind ein Spezialfall der bilinearen Abbildungen (Wertebereich G ist mit dem Skalark√∂rper K der Vektorr√§ume E und F identisch). Winkel sind wichtiger Anwendungsfall dafur: man kann Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht.\n",
        "\n",
        "  * A familiar and important example of a (symmetric) bilinear form is the standard [inner product (dot product)](https://en.m.wikipedia.org/wiki/Dot_product) of vectors. Jedes Skalarprodukt ist wiederum eine spezielle Bilinearform (es gelten noch weitere Eigenschaften: symmetrisch <v,w> = <w,v>, und positiv definit). Genauso Integral.\n",
        "\n",
        "  * [Bilinearform](https://de.wikipedia.org/wiki/Bilinearform), Bilinearform: cross product of two vectors, normal and tangent, see [Frenet‚ÄìSerret_formulas](https://en.m.wikipedia.org/wiki/Frenet‚ÄìSerret_formulas).\n",
        "\n",
        "* Die [Sesquilinearform](https://en.m.wikipedia.org/wiki/Sesquilinear_form) ist eine Generalizations der Bilinear Form auf den Koerper der komplexen Zahlen\n",
        "\n",
        "* Der [Metric Tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) ist ein Spezialfall einer Bilinear Form. The metric tensor has 2 additional properties that other bilinear forms might not have: components are symmetric, output must be positive:\n",
        "\n",
        "  * Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  * Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Just like the metric tensor <font color=\"red\">bilinear forms are (0,2) tensors</font> (so they transform using 2 covariant rules when we change coordinate systems):\n",
        "\n",
        "  * $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "  * $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 10: Bilinear Forms](https://www.youtube.com/watch?v=jLiBCaBEB3o&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14) und das Video [Bilinearform einfach erkl√§rt :) | Math Intuition](https://www.youtube.com/watch?v=TjAFH6hWg1I)"
      ],
      "metadata": {
        "id": "J9EJ2VfeKO--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between linear and bilinear forms**\n",
        "\n",
        "* If look at the bilinear form rules and focus only on the rules that involve the first input and pretend the second input $\\vec{w}$ is fixed, $\\mathcal{B}$ looks very much like a covector or a linear form (scale input linearly and addition is linear):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_33.png)\n",
        "\n",
        "* Alternatively if we focus only on the rules that involve the second input and pretend the first input is fixed $\\vec{v}$ also looks like a covector or linear form:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_34.png)\n",
        "\n",
        "**So if we say that $\\mathcal{B}$ is a bilinear form, it's because it's a form where each individual input is linear while the other input is held constant!**"
      ],
      "metadata": {
        "id": "PleWkdx8KQx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) is a function that takes vectors as inputs and outputs a number:**\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) on a vector space $V$ over a field $K$ is a map\n",
        "$f: V^{k} \\rightarrow K$\n",
        "that is separately $K$ -linear in each of its $k$ arguments.\n",
        "\n",
        "* A multilinear $k$ -form on $V$ over $\\mathbf{R}$ is called a (**covariant**) **$k$ -tensor**, and the vector space is usually denoted $\\mathcal{T}^{k}(V)$ or $\\mathcal{L}^{k}(V) \\cdot$\n",
        "\n",
        "* <font color=\"red\">Multilinear forms are (0,k) tensors</font> (so they transform using k covariant rules when we change coordinate systems) (??)\n",
        "\n",
        "* Kovariante Tensoren (Covectors) sind Multilinearformen [Source](https://de.m.wikipedia.org/wiki/Multilinearform)\n",
        "\n",
        "* Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "* https://unapologetic.wordpress.com/2009/10/22/multilinear-functionals/\n",
        "\n",
        "* Understanding the definition of tensors as multilinear maps: https://math.stackexchange.com/questions/2138459/understanding-the-definition-of-tensors-as-multilinear-maps\n"
      ],
      "metadata": {
        "id": "5uK_H84HKSuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Differential Operator*"
      ],
      "metadata": {
        "id": "LUxoBU9TKYpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Differential operator](https://en.m.wikipedia.org/wiki/Differential_operator) is an operator defined as a function of the differentiation operator. Consider differentiation as an abstract operation that **accepts a function and returns another function**.\n",
        "\n",
        "*Differential- & Nabla-Operator*\n",
        "\n",
        "Der [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, **um kontextabh√§ngig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren**. Es ist zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei Differentialoperatoren [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) (Anwendung im [Gradientenverfahren](https://de.wikipedia.org/wiki/Gradientenverfahren) in der Numerik), [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes) oder [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes)\n",
        "\n",
        "https://www.youtube.com/watch?v=YW-bUVIOpB0&t=51s\n",
        "\n",
        "* **Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enth√§lt.)\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und St√§rke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzuflie√üen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "  * [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "* **Diese drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei r√§umlicher Drehung des urspr√ºnglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden. Diese Eigenschaft folgt aus den **koordinatenunabh√§ngigen** Definitionen.\n",
        "\n",
        "https://www.youtube.com/watch?v=rB83DpBJQsE\n",
        "\n",
        "*Integraloperator*\n",
        "\n",
        "* **Integral**: Der [**Volterraoperator**](https://de.wikipedia.org/wiki/Integraloperator#Volterraoperator) $\\int_{0}^{t}$ zur Bildung des [bestimmten Integrals](https://de.wikipedia.org/wiki/Integralrechnung) (ist ein Beispiel fur einen [Integraloperator](https://de.wikipedia.org/wiki/Integraloperator). Operatoren wie diese, die einer Funktion eine Zahl zuordnen, nennt man [Funktional](https://de.wikipedia.org/wiki/Funktional).\n",
        "\n",
        "  * Die Fourier-Transformation ${\\mathcal {F}}$ ist z.B. ein **linearer Operator** (siehe unten).\n",
        "\n",
        "* Ein [linearer Integraloperator](https://de.wikipedia.org/wiki/Integraloperator) ist ein mathematisches Objekt aus der Funktionalanalysis. Dieses Objekt ist ein linearer Operator, der mit einer bestimmten Integralschreibweise mit einem Integralkern dargestellt werden kann."
      ],
      "metadata": {
        "id": "Y0ofnEZLKsn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Differential Form*"
      ],
      "metadata": {
        "id": "vD3xBKSIK3Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**√Ñu√üere Ableitung** (exterior derivative)\n",
        "\n",
        "* Die [√§u√üere Ableitung](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) ist ein Operator, der einer k-Differentialform eine $(k+1)$-Differentialform zuordnet.\n",
        "\n",
        "* Betrachtet man sie auf der Menge der $0$-Differentialformen, also auf der Menge der glatten Funktionen, so entspricht die √§u√üere Ableitung der √ºblichen Ableitung f√ºr Funktionen.\n",
        "\n",
        "* Die √§u√üere Ableitung $\\mathrm{d} \\omega$ einer $k$ -Form $\\omega$ wird induktiv mithilfe der [Lie-Ableitung](https://de.m.wikipedia.org/wiki/Lie-Ableitung) (=die Ableitung eines Vektorfeldes oder allgemeiner eines Tensorfeldes entlang eines Vektorfeldes. ) und der Cartan-Formel\n",
        "\n",
        "> $\n",
        "\\mathcal{L}_{X}=i_{X} \\circ \\mathrm{d}+\\mathrm{d} \\circ i_{X}\n",
        "$\n",
        "\n",
        "* definiert; dabei ist $X$ ein Vektorfeld, $\\mathcal{L}_{X}$ die Lie-Ableitung und $i_{X}$ die Einsetzung von $X$."
      ],
      "metadata": {
        "id": "hwqLWAHcK7af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "uDFxcwyecPgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connection between Differential Forms and Differential Operators (grad, div, curl):**\n",
        "\n",
        "  * By taking the [exterior derivative](https://de.m.wikipedia.org/wiki/√Ñu√üere_Ableitung) of 0-forms, 1-forms and 2-forms we can express the three important operators of vector calculus - grad, curl and div - in the language of differential forms. The gradient of a sclara field f(x,y,z) is given by:\n",
        "\n",
        "  * $\\operatorname{grad} f=\\nabla f=\\frac{\\partial f}{\\partial x} \\hat{e}_{x}+\\frac{\\partial f}{\\partial y} \\hat{e}_{y}+\\frac{\\partial f}{\\partial z} \\hat{\\mathrm{e}}_{z}$\n",
        "\n",
        "  * Using the correspondance:\n",
        "\n",
        "  * $d x \\Leftrightarrow \\hat{\\mathbf{e}}_{x}, d y \\Leftrightarrow \\hat{\\mathbf{e}}_{y}, d z \\Leftrightarrow \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "  * this gradient vector field can be associated with the exterior derivative of the 0-form f(x,y,z), which is a 1-form\n",
        "\n",
        "  * $d f=\\frac{\\partial f}{\\partial x} d x+\\frac{\\partial f}{\\partial y} d y+\\frac{\\partial f}{\\partial z} d z$\n",
        "\n",
        "  * Grad operator: $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$"
      ],
      "metadata": {
        "id": "QmtG8d3bK9Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form (Differential Geometry)**:\n",
        "\n",
        "* **[Differential forms](https://en.m.wikipedia.org/wiki/Differential_form) are linear or, more generally, multilinear alternating functions of tangent vectors**. Eine Differentialform ordnet einem Punkt einer Mannigfaltigkeit eine **alternierende Multilinearform** auf dem zugeh√∂rigen Tangentialraum zu. Differentialformen sind dabei das bekannteste Beispiel von Schnitten.\n",
        "\n",
        "* The differential forms form an [alternating algebra](https://en.m.wikipedia.org/wiki/Alternating_algebra). This implies that:\n",
        "\n",
        "  > $dy\\wedge dx=-dx\\wedge dy$\n",
        "\n",
        "  > $dx\\wedge dx=0.$\n",
        "\n",
        "* This alternating property reflects the orientation of the domain of integration.\n",
        "\n",
        "* Differential forms provide a unified approach to define integrands over curves, surfaces, solids, and higher-dimensional manifolds. ([Differentialformen](https://de.m.wikipedia.org/wiki/Differentialform) erlauben eine koordinatenunabh√§ngige Integration auf allgemeinen orientierten differenzierbaren Mannigfaltigkeiten.)\n",
        "\n",
        "* **Differential forms are a special class of tensors where you can find derivatives even in the absence of connections or metrics**. You can think of differential forms as a generalization of single variable calculus that works on manifolds, as well as curves, solids, and surfaces.\n",
        "\n",
        "* [Gra√ümann-Algebra](https://de.m.wikipedia.org/wiki/Gra√ümann-Algebra) ist die Algebra der Differentialformen\n",
        "\n",
        "* Simple Example of a Differential Form: in the expression $\\int_{0}^{1}$ <font color=\"blue\">$ x^{2} d x$</font> the term $x^{2}$ is the **integrand** and <font color=\"blue\">$ x^{2} d x$</font> is the **differential form** [(Source)](https://www.calculushowto.com/differential-operator/). So for identification, differential forms can be generally recognised as things containing differentials such as $d x, d y$ and $d z$.\n",
        "\n",
        "  * [Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes) (Vektoranalysis): sehr grundlegenden Satz √ºber die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "  * [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) (sowie Koordinatentransformationen und Funktionaldeterminante in der Vektoranalysis): Aus mathematischer Sicht ist eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird Riemann'sche Volumenform genannt (**Hodge-Stern-Operator in der Differentialgeometrie**).\n",
        "\n",
        "* Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind. Covectors (functional): Differential Forms = Covector Fields. Video Eigenchris: [Differential Forms and Covectors](https://youtu.be/XGL-vpk-8dU)\n"
      ],
      "metadata": {
        "id": "dl0fYX1-K_B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)"
      ],
      "metadata": {
        "id": "HyzwIvWTLA31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Types of Differential Forms where the $\\wedge$ symbol denotes a type of multiplication called the wedge product (exterior product)**:\n",
        "\n",
        "* **0-form = Function**:\n",
        "\n",
        "  * is a special case and is simply a smooth function. A 0-forms eats a point and returns a number\n",
        "\n",
        "  * [Glatte Funktionen](https://de.m.wikipedia.org/wiki/Glatte_Funktion) sind 0-Formen. Is a skalar from a function.\n",
        "\n",
        "* **1-form = Linear Form** (Pfaffsche Formen). Eats a vector and returns a number. Is a Covector.\n",
        "\n",
        "  * [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind 1-Formen (= zB Wegintegral). [1-Formen](https://en.m.wikipedia.org/wiki/One-form) bilden die Grundlage f√ºr die Einf√ºhrung von Differentialformen.\n",
        "\n",
        "  * [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind die nat√ºrlichen Integranden f√ºr Wegintegrale. Kovektorfeld oder kurz 1-Form ein Objekt, das in gewisser Weise dual zu einem Vektorfeld ist.\n",
        "\n",
        "  * Es ist eine Differentialform vom Grad 1. A one-form on a differentiable manifold is a smooth section of the cotangent bundle (= differential form).\n",
        "\n",
        "  * One Form and Line Integral: the expression $f(x) d x$ from one-variable calculus is an example of a [$l-$ form](https://en.m.wikipedia.org/wiki/One-form) (Pfaff'sche Form), and can be integrated over an oriented interval $[a, b]$ in the domain of $f: \\int_{a}^{b} f(x) d x$. See Video: [Line Integrals in Differential Forms](https://youtu.be/dhZgGIYzPUU).\n",
        "\n",
        "  * [Video: How to visualise a one-form](https://www.youtube.com/watch?v=dxz9JZPewu8): family of surfaces that it pierces. Integrating a one-form, we are just counting the number of surfaces that the line actually passes through\n",
        "\n",
        "  * Each term contains one differential: $\\begin{array}{a}\n",
        "\\omega=2 x d x+3 y d y-d z, \\\\\n",
        "\\omega=x d y, \\\\\n",
        "\\omega=d x .\n",
        "\\end{array}$\n",
        "\n",
        "* **2-form = Bilinear Form (Surface Integral)**: eats two vectors and returns a number:\n",
        "\n",
        "  * $\\omega=8 d y \\wedge d z$\n",
        "\n",
        "  * Similarly, the expression $f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z$ is a 2 -form that has a [surface integral](https://en.m.wikipedia.org/wiki/Surface_integral) over an oriented surface $S$ :\n",
        "\n",
        "  * $\\int_{S}(f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z) .$\n",
        "\n",
        "  * The symbol $\\wedge$ denotes the exterior product, sometimes called the wedge product, of two differential forms.\n",
        "\n",
        "* **3 -form = Trilinear Form (Volume Integral)**: eats three vectors and returns a number:\n",
        "\n",
        "  * $\\omega=-7 z d x \\wedge d y \\wedge d z$\n",
        "\n",
        "  * Likewise, a $3-$ form $f(x, y, z) d x \\wedge d y \\wedge d z$ represents a [volume element](https://en.m.wikipedia.org/wiki/Volume_element) that can be integrated over an oriented region of space.\n",
        "\n",
        "* **$k$-form = Multilinear Form**: eats $k$-vectors and returns a number: $\\omega= .. $\n",
        "\n",
        "  * In general, a $k$-form is an object that may be integrated over a $k$-dimensional oriented manifold, and is homogeneous of degree $k$ in the coordinate differentials.\n",
        "\n",
        "  * On an $n$-dimensional manifold, the top-dimensional form $(n$-form $)$ is called a [volume form](https://en.m.wikipedia.org/wiki/Volume_form)."
      ],
      "metadata": {
        "id": "gtnhb-2MLClh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Siehe auch [Level Sets](https://en.wikipedia.org/wiki/Level_set) bzw. Niveaumenge** (= die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist.)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)"
      ],
      "metadata": {
        "id": "NwHEgDTLLEiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)"
      ],
      "metadata": {
        "id": "BjDe-v7oLGGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier mit Niveaumenge (level sets):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)"
      ],
      "metadata": {
        "id": "TCjQDbgwLKl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: If we think of x as a scalar field, it would look like this: it‚Äôs a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [**Level Set (Niveaumenge)**](https://de.wikipedia.org/wiki/Niveaumenge). Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)"
      ],
      "metadata": {
        "id": "CptmeMH_LVXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)"
      ],
      "metadata": {
        "id": "unJXl_UfLXfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing One-Forms:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_87.png)"
      ],
      "metadata": {
        "id": "vAo_tYsKLaHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_88.png)"
      ],
      "metadata": {
        "id": "08oxaMctLcFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition der Differentialform (Tangent Spaces)**\n",
        "\n",
        "Es sei $U$\n",
        "- eine offene Teilmenge des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Untermannigfaltigkeit des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Mannigfaltigkeit.\n",
        "\n",
        "In jedem dieser F√§lle gibt es:\n",
        "- den Begriff der differenzierbaren Funktion auf $U$; der Raum der beliebig oft differenzierbaren Funktionen auf $U$ werde mit $C^{\\infty}(U)$ bezeichnet;\n",
        "- den Begriff des Tangentialraums $\\mathrm{T}_{p} U$ an $U$ in einem Punkt $p \\in U$;\n",
        "- den Begriff der Richtungsableitung $\\frac{\\partial f}{\\partial X}$ f√ºr einen Tangentialvektor $X \\in \\mathrm{T}_{p} U$ und eine differenzierbare Funktion $f$;\n",
        "- den Begriff des differenzierbaren Vektorfeldes auf $U$; der Raum der Vektorfelder auf $U$ sei mit $\\Gamma(\\mathrm{T} U)$ bezeichnet.\n",
        "- Der Dualraum des Tangentialraums $\\mathrm{T}_{p} U$ wird als Kotangentialraum $\\mathrm{T}_{p}^{*} U$\n",
        "bezeichnet.\n",
        "\n",
        "**Definition: Eine Differentialform vom Grad $k$ auf $U$ oder kurz $k$ -Form $\\omega$ ist ein glatter Schnitt in der $k$ -ten √§u√üeren Potenz des Kotangentialb√ºndels von $U$**.\n",
        "\n",
        "* In symbolischer Schreibweise bedeutet dies $\\omega \\in \\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$, wobei $T^{*} U$ das Kotangentialb√ºndel von $U, \\Lambda^{k}\\left(T^{*} U\\right)$ die $k$ -te √§u√üere Potenz von $T^{*} U$ und $\\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$ somit die Menge der glatten Schnitte von $\\Lambda^{k}\\left(T^{*} U\\right)$ bezeichnet.\n",
        "\n",
        "* Dies bedeutet, **dass jedem Punkt $p \\in U$ eine alternierende Multilinearform $\\omega_{p}$ auf dem Tangentialraum $T_{p} U$ zugeordnet wird**; und zwar so, dass f√ºr $k$ glatte Vektorfelder $X_{1}, \\ldots, X_{k}$ die folgende Funktion glatt, also beliebig oft differenzierbar, ist:\n",
        "\n",
        "> $p \\mapsto \\omega_{p}\\left(\\left(X_{1}\\right)_{p}, \\ldots,\\left(X_{k}\\right)_{p}\\right) \\in \\mathbb{R}$\n",
        "\n",
        "* **Alternativ dazu kann man eine $k$ -Form $\\omega$ als eine alternierende, glatte multilineare Abbildung $\\omega:(\\Gamma T U)^{k} \\rightarrow C^{\\infty}(U)$ auffassen**.\n",
        "\n",
        "* Das bedeutet: $\\omega$ ordnet $k$ Vektorfeldern $X_{1}, \\ldots, X_{k}$ eine Funktion $\\omega\\left(X_{1}, \\ldots, X_{k}\\right)$ zu, sodass\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}+X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)=\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}, \\ldots, X_{k}\\right)+\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)$\n",
        "\n",
        "* $\\omega\\left(X_{1}, \\ldots, f \\cdot X_{i}, \\ldots, X_{k}\\right)=f \\cdot \\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$ f√ºr $f \\in C^{\\infty}(U), 1 \\leq i \\leq k$\n",
        "\n",
        "* und\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{j}, \\ldots, X_{k}\\right)=-\\omega\\left(X_{1}, \\ldots, X_{j}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$\n",
        "gilt.\n",
        "\n",
        "* Alternative unter R√ºckgriff auf Tensorfelder: **Eine $k$ -Form ist ein alternierendes, kovariantes Tensorfeld der Stufe $k$.**"
      ],
      "metadata": {
        "id": "-gGtBekDLeBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raum der Differentialformen**\n",
        "\n",
        "* Die Menge der $k$ -Formen auf $U$ bildet einen Vektorraum und wird mit $\\Omega^{k}(U)$ bezeichnet. Weiterhin setzt man\n",
        "\n",
        "> $\n",
        "\\Omega(U)=\\bigoplus_{k=1}^{\\infty} \\Omega^{k}(U) .\n",
        "$\n",
        "\n",
        "* F√ºr endlichdimensionale Mannigfaltigkeiten ist diese Summe endlich, da f√ºr $k>\\operatorname{dim} U$ der Vektorraum $\\Omega^{k}(U)$ der Nullvektorraum ist. Die Menge $\\Omega(U)$ ist eine Algebra mit dem √§u√üeren Produkt als Multiplikation und somit auch wieder ein Vektorraum. Aus topologischer Sicht ist dieser Raum auch eine [Garbe](https://de.m.wikipedia.org/wiki/Garbe_(Mathematik)).\n",
        "\n",
        "* Man kann $\\omega_{p}$ als Element der √§u√üeren Potenz $\\Lambda^{k}\\left(T_{p}^{*} U\\right)$ auffassen; infolgedessen definiert das √§u√üere Produkt (d. h. das Produkt $\\wedge$ in der √§u√üeren Algebra) Abbildungen\n",
        "\n",
        "> $\n",
        "\\Omega^{k}(U) \\times \\Omega^{\\ell}(U) \\rightarrow \\Omega^{k+\\ell}(U), \\quad(\\omega, \\eta) \\mapsto \\omega \\wedge \\eta\n",
        "$\n",
        "\n",
        "* wobei $\\omega \\wedge \\eta$ durch\n",
        "\n",
        "> $\n",
        "(\\omega \\wedge \\eta)_{p}=\\omega_{p} \\wedge \\eta_{p}\n",
        "$\n",
        "\n",
        "* punktweise definiert ist. Dieses Produkt ist graduiert-kommutativ, es gilt\n",
        "\n",
        "> $\n",
        "\\omega \\wedge \\eta=(-1)^{\\operatorname{deg} \\omega \\cdot \\operatorname{deg} \\eta} \\cdot \\eta \\wedge \\omega ;\n",
        "$\n",
        "\n",
        "* dabei bezeichnet $\\operatorname{deg} \\omega$ den Grad von $\\omega, d$. h.: Ist $\\omega$ eine $k$ -Form, so ist $\\operatorname{deg} \\omega=k$. Demnach ist das Produkt zweier Formen ungeraden Grades antikommutativ und in allen anderen Kombinationen kommutativ."
      ],
      "metadata": {
        "id": "S70lQDJOLgKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form <u>Field</u> (on the manifold)**\n",
        "\n",
        "* A vector field on a manifold gives a vector (actually, a tangent vector) at every point. So, for example, say a point $P$ on $M$, a region of $\\mathbb{R}^{3}$, has coordinates $x=3, y=5, z=1$. At $P$ the vector field\n",
        "\n",
        "> $\\mathbf{v}=7 x \\hat{\\mathbf{e}}_{x}+4 y \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "* gives the (tangent) vector\n",
        "\n",
        "> $\\begin{array}{c}\\mathbf{v}=(7 \\times 3) \\hat{\\mathbf{e}}_{x}+(4 \\times 5) \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z} \\\\\\mathbf{v}=21 \\hat{\\mathbf{e}}_{x}+20 \\hat{\\mathbf{e}}_{y}+2 \\hat{\\mathbf{e}}_{z}\\end{array}$\n",
        "\n",
        "* Similarly, a differential form on a manifold can be thought of as a differential form field, that gives a particular differential form at every point. For example, at the same point $P(3,5,1)$ on $M$ the above 1-form field, $\\omega=2 x d x+3 y d y-d z$, gives the particular 1-form\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\omega=(2 \\times 3) d x+(3 \\times 5) d y-d z \\\\\n",
        "\\omega=6 d x+15 d y-d z\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "* If the tangent vector and 1-form are associated with the same point, they can act on each other to give a number.\n",
        "\n",
        "* Following widely accepted practice, we won't use the term 'field', as in 'differential form field'. Instead, **we'll let 'differential form' refer to both a differential form at a point and a differential form field on the manifold.**\n",
        "\n",
        "* (A hint of what is to come: just as the vectors $\\hat{\\mathbf{e}}_{x}, \\hat{\\mathbf{e}}_{y}$ and $\\hat{\\mathbf{e}}_{z}$ form a set of basis vectors for $\\mathbb{R}^{3}$, the differentials $d x, d y$ and $d z$ form a set of basis 1 -forms for $\\mathbb{R}^{3}$.)"
      ],
      "metadata": {
        "id": "2TNAlnDuLh7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Clifford Algebra (Geometric)*"
      ],
      "metadata": {
        "id": "q09fihf25FPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Clifford Algebra*"
      ],
      "metadata": {
        "id": "HrtIDW9V0p96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Hierarchy of Clifford Algebra $C‚Ñì_{p,q}$](https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras)**\n",
        "\n",
        "*Clifford algebra over the reals is also called Geometric algebra*\n",
        "\n",
        "Algebra $\\rightarrow$ [Signature](https://en.m.wikipedia.org/wiki/Metric_signature) $\\rightarrow$ Equation\n",
        "\n",
        "${Cl}_{4,2} (\\mathbb {R})$ - [Conformal geometric algebra (CGA)](https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra) $\\rightarrow$ +,+,+,+,-,-, $\\rightarrow$ [Twistor](https://en.m.wikipedia.org/wiki/Twistor_space) $\\rightarrow$ [twistor theory](https://en.m.wikipedia.org/wiki/Twistor_theory)\n",
        "\n",
        "\n",
        "\n",
        "${Cl}_{1,3} (\\mathbb {R})$ - [Spacetime algebra\n",
        "](https://en.m.wikipedia.org/wiki/Spacetime_algebra) and ${Cl}_{1,3} (\\mathbb {C})$ - [Dirac algebra\n",
        "](https://en.m.wikipedia.org/wiki/Dirac_algebra) $\\rightarrow$ +,-,-,-, $\\rightarrow$ [Dirac equation](https://en.m.wikipedia.org/wiki/Dirac_equation) $\\rightarrow$ [relativistic spin-1/2](https://en.m.wikipedia.org/wiki/Relativistic_wave_equations#Spin_1.2F2) $\\rightarrow$ [Gamma matrices](https://en.m.wikipedia.org/wiki/Gamma_matrices)\n",
        "\n",
        "${Cl}_{3,0} (\\mathbb {R})$ - [Algebra of physical space (Pauli algebra)\n",
        "](https://en.m.wikipedia.org/wiki/Algebra_of_physical_space) $\\rightarrow$ +,+,+ $\\rightarrow$ [Pauli equation](https://en.m.wikipedia.org/wiki/Pauli_equation) $\\rightarrow$ [spin-1/2](https://en.m.wikipedia.org/wiki/Spin-1/2) $\\rightarrow$ [Pauli matrices](https://en.m.wikipedia.org/wiki/Pauli_matrices)\n",
        "\n",
        "${Cl}_{0,3} (\\mathbb {R})$ - [i.e. Quaternions](https://en.m.wikipedia.org/wiki/Clifford_algebra#Quaternions)\n",
        "\n",
        "\n",
        "${Cl}_{0,1} (\\mathbb {R})$ - [Clifford_algebra#Real_numbers](https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers) $\\rightarrow$ - $\\rightarrow$ [Schr√∂dinger equation](https://en.m.wikipedia.org/wiki/Schr%C3%B6dinger_equation) $\\rightarrow$ spin-0\n",
        "\n",
        "> *See also: For a complete classification of these algebras see [Classification of Clifford algebras](https://en.m.wikipedia.org/wiki/Classification_of_Clifford_algebras).*\n"
      ],
      "metadata": {
        "id": "77his5x9yBCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Clifford Algebra $Cl_{p,q} (\\mathbb{C})$ (Incl. Weyl Algebra)*\n",
        "\n",
        "Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)\n",
        "\n",
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "https://clifford.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_gates\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/representations-and-spinors/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Majorana_equation\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/pauli-and-dirac-matrices/\n"
      ],
      "metadata": {
        "id": "ginSvag_5QfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Properties**\n",
        "\n",
        "* In Geometric Algebra the **basis vectors for the space are typically real valued vectors**. Complex valued vectors have uses in GA (i.e. frequency domain representation of vectors in electrodynamics), but the underlying basis for the vector space is still real valued (i.e. span{ùêû1,ùêû2,ùêû3}.\n",
        "\n",
        "* Clifford algebras provide a further generalization, **allowing those basis vectors to reside in a complex vector space**, with suitable modifications of the vector product rules.\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra\n",
        "\n",
        "* Clifford Algebra focuses on abstract mathematical and algebraic properties\n",
        "\n",
        "* Geometric Algebra focuses on geometric and physical applications\n",
        "\n",
        "> A [Clifford algebra](https://en.m.wikipedia.org/wiki/Clifford_algebra) is an algebra generated by a vector space with a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form), and is a unital [associative algebra](https://en.m.wikipedia.org/wiki/Associative_algebra) (not commutative, but anti-commutative!).\n",
        "\n",
        "* The best known example of a quadratic form is the square of the amount of a vector: $|\\vec{v}|^{2}=x^{2}+y^{2}+z^{2}+\\ldots$ (like used in quantum mechanics on Bloch sphere)\n",
        "\n",
        "* As K-algebras, they generalize the real numbers, complex numbers, quaternions and several other hypercomplex number systems.\n",
        "\n",
        "> **The theory of Clifford algebras is intimately connected with the theory of [quadratic forms](https://en.m.wikipedia.org/wiki/Quadratic_form) and [orthogonal transformations](https://en.m.wikipedia.org/wiki/Orthogonal_group) (Orthogonal group, like SO(2), SO(3) and SO(4)).**\n",
        "\n",
        "* Clifford algebras have important applications in a variety of fields including geometry, theoretical physics and digital image processing.\n",
        "\n",
        "A Clifford algebra is a unital associative algebra that contains and is generated by a vector space $V$ over a field $K_{1}$ where $V$ is equipped with a quadratic form $Q: V \\rightarrow K$. The Clifford algebra $\\mathrm{Cl}(V, Q)$ is the \"freest\" algebra generated by $V$ subject to the condition\n",
        "\n",
        "> $v^{2}=Q(v) 1$ for all $v \\in V$\n",
        "\n",
        "where the product on the left is that of the algebra, and the 1 is its [multiplicative identity](https://en.m.wikipedia.org/wiki/Identity_element#Definitions) (neutral element).\n",
        "\n",
        "See also: https://en.m.wikipedia.org/wiki/Clifford_algebra#Universal_property_and_construction"
      ],
      "metadata": {
        "id": "f7RjlHat54zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Extensions & Generalizations*"
      ],
      "metadata": {
        "id": "FMIv268H0nv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extensions & Generalizations**\n",
        "\n",
        "More precisely, **Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra**, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "Weyl algebras and Clifford algebras admit a further structure of a [*-algebra](https://en.m.wikipedia.org/wiki/*-algebra), and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "Source: https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "[Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra), a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra) by a [symplectic form](https://en.m.wikipedia.org/wiki/Symplectic_vector_space)\n",
        "\n",
        "Clifford algebra, a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the exterior algebra by a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form).\n",
        "\n",
        "The Weyl algebra is also referred to as the symplectic Clifford algebra. Weyl algebras represent the same structure for symplectic bilinear forms that Clifford algebras represent for non-degenerate symmetric bilinear forms.\n",
        "\n",
        "*Clifford Algebra & Free Algebra*\n",
        "\n",
        "\n",
        "* for Clifford Algebra: The idea of being the \"freest\" or \"most general\" algebra subject to this identity can be formally expressed through the notion of a [universal property](https://en.m.wikipedia.org/wiki/Universal_property) (from category theory).\n",
        "\n",
        "* in the area of abstract algebra known as ring theory, a [free algebra](https://en.m.wikipedia.org/wiki/Free_algebra) is the noncommutative analogue of a polynomial ring since its elements may be described as \"polynomials\" with non-commuting variables. Likewise, the polynomial ring may be regarded as a free commutative algebra.\n",
        "\n",
        "*Clifford Algebra & Exterior Algebra*\n",
        "\n",
        "* Clifford Algebra as a quantization of the exterior algebra\n",
        "\n",
        "* Clifford algebras are closely related to exterior algebras. Indeed, if Q = 0 then the Clifford algebra Cl(V, Q) is just the exterior algebra ‚ãÄ(V). For nonzero Q there exists a canonical linear isomorphism between ‚ãÄ(V) and Cl(V, Q) whenever the ground field K does not have characteristic two.\n",
        "\n",
        "* **Clifford multiplication together with the distinguished subspace is strictly richer than the exterior product since it makes use of the extra information provided by Q.**\n",
        "\n",
        "* The Clifford algebra is a [filtered algebra](https://en.m.wikipedia.org/wiki/Filtered_algebra), the [associated graded algebra](https://en.m.wikipedia.org/wiki/Associated_graded_ring) is the exterior algebra.\n",
        "\n",
        "* More precisely, Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, **in the same way that the Weyl algebra is a quantization of the symmetric algebra.**\n",
        "\n",
        "* Weyl algebras and Clifford algebras admit a further structure of a *-algebra, and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Clifford_algebra#Relation_to_the_exterior_algebra\n",
        "\n",
        "*Clifford Algebra & Geometric Algebra*\n",
        "\n",
        "* Geometric Algebra is a special form of the more general Clifford algebra\n",
        "\n",
        "* Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.\n",
        "\n",
        "*Other notes*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "...the geometric algebra for this quadratic space is the Clifford algebra... [Source](https://en.m.wikipedia.org/wiki/Geometric_algebra)\n",
        "\n",
        "Clifford algebras were born of a synthesis of inner product spaces and Grassmann's exterior algebras, both of which have geometric applications.\n",
        "\n",
        "A Clifford algebra is constructed from an inner product space (ùëâ,ùëÑ) by generating an associative algebra (whose product is a descendant of the tensor product in the tensor algebra for ùëâ). These are compatible in a sense made clear in the Wiki.\n",
        "\n",
        "https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "P3d4_VAH6tFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Applications*"
      ],
      "metadata": {
        "id": "Ig6IVIFPww88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*Applications*\n",
        "\n",
        "* One of the principal applications of the exterior algebra is in differential geometry where it is used to define the bundle of differential forms on a smooth manifold. In the case of a (pseudo-)Riemannian manifold, the tangent spaces come equipped with a natural quadratic form induced by the metric. Thus, one can define a Clifford bundle in analogy with the exterior bundle. This has a number of important applications in Riemannian geometry. Perhaps more importantly is the link to a spin manifold, its associated spinor bundle and spinc manifolds.\n",
        "\n",
        "* More: https://en.m.wikipedia.org/wiki/Clifford_algebra#Applications\n",
        "\n",
        "*Examples of Clifford Algebras $C‚Ñì_{p,q}$*\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras\n",
        "\n",
        "Different focusea created based on use cases / application\n",
        "\n",
        "Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "See also hierrchy of clifford algebra: https://en.m.wikipedia.org/wiki/Basil_Hiley#\n",
        "\n"
      ],
      "metadata": {
        "id": "FHJ0usg67LAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Geometric Algebra*"
      ],
      "metadata": {
        "id": "YWqVgPOwPCIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=60z_hpEAtD8&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=19)\n",
        "\n",
        "Video: [Addendum to A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=0bOiy0HVMqA&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)\n",
        "\n",
        "Video: [From Zero to Geo Introduction (Geometric Algebra Series)](https://www.youtube.com/watch?v=2hBWCCAiCzQ&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)"
      ],
      "metadata": {
        "id": "P5GPkVzEPEgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometric Algebra $Cl_{p,q} (\\mathbb{R})$ or $G(M^n)$ (Clifford algebra over reals)\n",
        "\n",
        "> In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does [Source](https://en.m.wikipedia.org/wiki/Vector_calculus)\n",
        "\n",
        "> Geometric Algebra is the Clifford Algebra over the field of real numbers.\n",
        "\n",
        "[**Geometric Product**](https://en.m.wikipedia.org/wiki/Geometric_algebra) *Sum of inner product (dot) and outer (not exterior??) product (wedge). Is the geometric product of any two vectors a and b as the sum of a symmetric product and an antisymmetric product:*\n",
        "\n",
        "> $a b=\\frac{1}{2}(a b+b a)+\\frac{1}{2}(a b-b a) =a \\cdot b+a \\wedge b$\n",
        "\n",
        "> $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n"
      ],
      "metadata": {
        "id": "cHOe2QmAPN5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Geometric Product: Sum of inner product (dot) and outer product (wedge)**: $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n",
        "* Magnitude of u $\\wedge$ v (=bivector) is the area of a parallelogram - similar to cross product, but not restricted to R3 like [cross product](https://en.m.wikipedia.org/wiki/Cross_product) (including magnitude and orientation)\n",
        "\n",
        "* Geometric algebra (GA) is an extension or completion of vector algebra (VA)\n",
        "\n",
        "* **Geometric Algebra: special form of the more general Clifford algebra**\n",
        "\n",
        "* the [geometric algebra](https://en.m.wikipedia.org/wiki/Geometric_algebra) (GA) of a vector space with a quadratic form (usually the Euclidean metric or the Lorentz metric) is an algebra over a field, the Clifford algebra of a vector space with a quadratic form with its multiplication operation called the geometric product.\n",
        "\n",
        "> **The algebra elements are called multivectors, which contains both the scalars $F$ and the vector space $V$.**\n",
        "\n",
        "* Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "* Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis and differential geometry, **e.g. by using the Clifford algebra instead of differential forms**.\n",
        "\n",
        "* Geometric algebra has been advocated as the **preferred mathematical framework for physics**. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity. GA has also found use as a computational tool in computer graphics and robotics.\n"
      ],
      "metadata": {
        "id": "tE1L0QYwPPsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "AuG47_giPSx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences to other algebras**\n",
        "\n",
        "* **Geometric Algebra vs Clifford algebra**: [Source](https://www.quora.com/How-are-geometric-algebra-and-Clifford-algebra-different)\n",
        "\n",
        "  * **Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.**\n",
        "\n",
        "  * A Clifford algebra is a unital associative algebra that contains and is generated by a vector space V over a field K, where V is equipped with a quadratic form Q.\n",
        "\n",
        "  * A Geometric algebra is a Clifford algebra of a vector space over the field of real numbers endowed with a quadratic form.\n",
        "\n",
        "  * **So a Geometric Algebra is a special form of the more general Clifford algebra**. In particular it is a CA over the reals. Additionally rather than being just an abstract object it has specific geometric meaning. For instance the 3+1 dimensional spacetime algebra is a Geometric algebra.\n",
        "\n",
        "* **Geometric Algebra vs Tensor Algebra**: Every element of a geometric algebra can be identified with a tensor, but not every tensor can be identified with an element of a geometric algebra [Source](\n",
        "https://math.stackexchange.com/questions/725350/is-geometric-algebra-isomorphic-to-tensor-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Exterior Algebra**: Exterior algebra defines an antisymmetric wedge product. In an exterior algebra, one can add k-forms to other k-forms, but would not add forms of different rank. This restriction is relaxed in geometric algebra (GA). [Source](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Vector Algebra**: [Source](\n",
        "https://en.m.wikipedia.org/wiki/Comparison_of_vector_algebra_and_geometric_algebra):\n",
        "\n",
        "  * Geometric algebra is an extension of vector algebra, providing additional algebraic structures on vector spaces, with geometric interpretations. Vector algebra uses all dimensions and signatures, as does geometric algebra, notably 3+1 spacetime as well as 2 dimensions.\n",
        "\n",
        "  * For example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. **The torque or curl is then a normal vector field in this 3rd dimension**.\n",
        "\n",
        "  * By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product."
      ],
      "metadata": {
        "id": "HkxCUeosPWmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: The cross product in relation to the exterior product. In red are the orthogonal unit vector, and the \"parallel\" unit bivector.*\n",
        "\n",
        "* $\\mathbf u \\times \\mathbf v$ is perpendicular to the plane containing $\\mathbf {u}$ and $\\mathbf {v}$\n",
        "\n",
        "* $\\mathbf u \\wedge \\mathbf v$ is an oriented representation of the same plane.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Exterior_calc_cross_product.svg/260px-Exterior_calc_cross_product.svg.png)"
      ],
      "metadata": {
        "id": "GLlRNs2BPZpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "9tCNN2HDPcAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "example: **Conformal geometric algebra $C‚Ñì_{4,2}$ - Twistor equation**"
      ],
      "metadata": {
        "id": "nt89GGCOPeSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Paravector"
      ],
      "metadata": {
        "id": "85mklZLQPgRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versor"
      ],
      "metadata": {
        "id": "GHAYUzVUPiWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{4,2}(\\mathbb{R})$ - Conformal geometric algebra*"
      ],
      "metadata": {
        "id": "Ym5Ax5nCOtuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra"
      ],
      "metadata": {
        "id": "g1zDxtzDOvrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{C})$ - Dirac Algebra*"
      ],
      "metadata": {
        "id": "QBuL0NbxO6cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Dirac_algebra"
      ],
      "metadata": {
        "id": "js5NF8RU528q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{1,3} (\\mathbb{R})$ - Spacetime Algebra*"
      ],
      "metadata": {
        "id": "M5NxmpI85zgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Spacetime Algebra](https://www.youtube.com/watch?v=e7aIVSVc8cI&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=21)\n",
        "\n",
        "> Spacetime Algebra: Geometric Algebra + Special Relativity\n",
        "\n",
        "**Spacetime Algebra $C‚Ñì_{1,3}$ - Dirac equation**\n",
        "\n",
        "> Spacetime algebra concerns the Clifford algebra Cl1,3(R) of the four-dimensional [Minkowski spacetime](https://en.m.wikipedia.org/wiki/Minkowski_space)\n",
        "\n",
        "* In mathematical physics, [spacetime algebra (STA)](https://en.m.wikipedia.org/wiki/Spacetime_algebra) is a name for the Clifford algebra Cl1,3(R), or equivalently the geometric algebra G(M4).\n",
        "\n",
        "* According to David Hestenes, spacetime algebra can be particularly closely associated with the geometry of special relativity and relativistic spacetime.\n",
        "\n",
        "* It is a vector space that allows not only vectors, but also bivectors (directed quantities associated with particular planes, such as areas, or rotations) or blades (quantities associated with particular hyper-volumes) to be combined, as well as rotated, reflected, or [Lorentz boosted](https://en.m.wikipedia.org/wiki/Lorentz_transformation#boost).\n",
        "\n",
        "* It is also the natural parent algebra of spinors in special relativity."
      ],
      "metadata": {
        "id": "T8yu7WqUO26x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{3,0}(\\mathbb{R})$ - Algebra of Physical Space and Pauli Algebra*"
      ],
      "metadata": {
        "id": "ur1A1K3aOl3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebra of physical space $C‚Ñì_{3,0}$ - Pauli equation**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebra_of_physical_space\n",
        "\n",
        "used in special relativity, classical electrodynamics and relativistic quantum mechanics (Dirac equation in quantum field theory)\n",
        "\n",
        "Whereas the mathematicians do not give special attention to the case  ùëõ=2, the physicists, dealing with four-dimensional space-time, have every reason to do so, and it turns out to be most rewarding to develop procedures and proofs for the special case rather than refer to the general mathematical theorems. The technique for such a program has been developed some years ago [Source](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)"
      ],
      "metadata": {
        "id": "C_DZg7k4OpnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli Algebra $A_2$**\n",
        "\n",
        "See article [The Pauli Algebra - Mathematics LibreTexts](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)\n",
        "\n",
        "$\\mathcal{A}_{2}$ is called the Pauli algebra. The basis matrices are\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\sigma_{0}=I=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{1}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{2}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{3}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "The [Pauli matrices](https://de.m.wikipedia.org/wiki/Pauli-Matrizen) are the following four 2 √ó 2 matrices:\n",
        "\n",
        "> $\\sigma_{0}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right), \\sigma_{1}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right), \\sigma_{2}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right), \\sigma_{3}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)$\n",
        "\n",
        "\n",
        "* The three Pauli matrices satisfy the well known multiplication rules\n",
        "\n",
        "* All of the basis matrices are Hermitian, or self-adjoint\n",
        "\n",
        "**Pauli Matrices (Pauli Operators)**\n",
        "\n",
        "The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse), meaning that the square of a Pauli matrix is the identity matrix.\n",
        "\n",
        ">$\n",
        "I^{2}=X^{2}=Y^{2}=Z^{2}=-i X Y Z=I\n",
        "$\n",
        "\n",
        "The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example $Z X=i Y=-X Z$.\n",
        "\n",
        "*Anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. **Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments**. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation, such as addition.*\n",
        "\n",
        "Single spin one half particle, focus on spin degrees of freedom:\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad 1 \\downarrow=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$\n",
        "\n",
        "**X, Y and Z axis on Bloch sphere:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_077.PNG)\n",
        "\n",
        "Source: https://www.researchgate.net/figure/The-Bloch-sphere-representation-of-a-qubit-The-basis-states-are-located-at-the-north_fig2_284259345\n",
        "\n",
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Bloch_Sphere.svg/423px-Bloch_Sphere.svg.png)\n",
        "\n",
        "*Bloch sphere*"
      ],
      "metadata": {
        "id": "Vi6h1_4V3hsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C‚Ñì_{0,0}, C‚Ñì_{0,1}, C‚Ñì_{0,2}(\\mathbb{R})$*"
      ],
      "metadata": {
        "id": "SN4YJmrE6HF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few low-dimensional cases are:\n",
        "\n",
        "* Cl0,0(R) is naturally **isomorphic to R** since there are no nonzero vectors.\n",
        "* Cl0,1(R) is a two-dimensional algebra generated by e1 that squares to ‚àí1, and is algebra-isomorphic to C, the field of **complex numbers**.\n",
        "* Cl0,2(R) is a four-dimensional algebra spanned by {1, e1, e2, e1e2}. The latter three elements all square to ‚àí1 and anticommute, and so the algebra is isomorphic to the **quaternions** H.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers\n"
      ],
      "metadata": {
        "id": "nywgP0rp6VF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Tensor Algebra*"
      ],
      "metadata": {
        "id": "fVQl5cbp7iqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Analysis*"
      ],
      "metadata": {
        "id": "PsodM6QW8wsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Motivation:*** *In our subject of differential geometry, where you talk about manifolds, **one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning**. They are allowed to **undergo transformation**. And in order to handle this kind of situation, an important tool is the so-called **tensor analysis, or Ricci calculus**, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.*\n",
        "\n",
        "* Die [Tensoranalysis](https://de.wikipedia.org/wiki/Tensoranalysis) ist ein Teilgebiet der Differentialgeometrie beziehungsweise der Differentialtopologie.\n",
        "\n",
        "* In der Tensoranalysis wird **das Verhalten von geometrischen Differentialoperatoren auf Tensorfeldern untersucht**.\n",
        "\n",
        "> <font color=\"blue\">**Tensor Analysis ist eine Verallgemeinerung der Vektoranalysis**</font>\n",
        "\n",
        "* Zum Beispiel kann der Differentialoperator Rotation in diesem Kontext auf n Dimensionen verallgemeinert werden.\n",
        "\n",
        "* Zentrale Objekte der Tensoranalysis sind Tensorfelder. Es wird untersucht, wie Differentialoperatoren auf diesen Feldern wirken.\n",
        "\n",
        "* Vektoren und Matrizen, sofern sie geometrische oder physikalische Groessen reprasentieren, koennen unter dem begriff eines tensors subsumiert werden\n",
        "\n",
        "* **Tensorrechnung in 2 teilen:**\n",
        "\n",
        "  * Anschauungsraum fur ingenieure (**kartesische tensoren**).\n",
        "\n",
        "  * Fur **schiefwinklige (also affine) oder krummlinige Koordinaten** sind begriffe wie kovariant und kontravariant wichtig (zur arbeit mit metriken, deren skalarproduktauf einer nicht positiv definiten bilinearform beruht)."
      ],
      "metadata": {
        "id": "rx-E5d4r8zxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoralgebra**\n",
        "\n",
        "> **The [tensor algebra](https://en.m.wikipedia.org/wiki/Tensor_algebra) is important because many other algebras arise as quotient algebras of T(V)**. These include the [exterior algebra (Grassmann algebra of Differential Forms)](https://en.m.wikipedia.org/wiki/Exterior_algebra), the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra), the [Clifford algebras](https://en.m.wikipedia.org/wiki/Clifford_algebra), the [Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra) and [universal enveloping algebras](https://en.m.wikipedia.org/wiki/Universal_enveloping_algebra).\n",
        "\n",
        "* Die [Tensoralgebra](https://de.m.wikipedia.org/wiki/Tensoralgebra) ist ein mathematischer Begriff, der in vielen Bereichen der Mathematik wie der linearen Algebra, der Algebra, der Differentialgeometrie sowie in der Physik verwendet wird.\n",
        "\n",
        "* Sie fasst \"alle Tensoren\" √ºber einem Vektorraum in der Struktur einer graduierten Algebra zusammen.\n",
        "\n",
        "* Es sei $V$ ein Vektorraum √ºber einem K√∂rper $K$ oder allgemeiner ein Modul √ºber einem kommutativen Ring mit Einselement. Dann ist die Tensoralgebra (als Vektorraum) definiert durch die direkte Summe aller Tensorprodukte des Raums mit sich selbst.\n",
        "\n",
        ">$\n",
        "\\mathrm{T}(V):=\\bigoplus_{n \\geq 0} V^{\\otimes n}=K \\oplus V \\oplus(V \\otimes V) \\oplus(V \\otimes V \\otimes V) \\oplus \\ldots\n",
        "$\n",
        "\n",
        "* Mit der Multiplikation, die auf den homogenen Bestandteilen durch das Tensorprodukt gegeben ist, wird $\\mathrm{T}(V)$ zu einer $\\mathbb{N}_{0}$ -graduierten, unit√§ren, assoziativen Algebra.\n",
        "\n",
        "* Quotientenr√§ume der Tensoralgebra: Durch Herausteilen eines bestimmten Ideals kann man aus der Tensoralgebra beispielsweise die symmetrische Algebra, die √§u√üere Algebra oder die **Clifford-Algebra** gewinnen. Diese Algebren sind in der Differentialgeometrie von Bedeutung.\n",
        "\n"
      ],
      "metadata": {
        "id": "MERE084982Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Example for Tensors: Einstein Field Equations in General Relativity:**\n",
        "\n",
        "> $\n",
        "R_{\\mu \\nu}-\\frac{1}{2} R g_{\\mu \\nu}+\\Lambda {g_{\\mu v}}=\\frac{8 \\pi G}{c^{4}} T_{\\mu \\nu}\n",
        "$\n",
        "\n",
        "4x4 rank 2 metric tensor: ${g_{\\mu v}}$ (measure lengths and angles in the curved geometry of spacetime)\n",
        "\n",
        "* Gravitation als geometrische Eigenschaft der gekr√ºmmten vierdimensionalen Raumzeit.\n",
        "\n",
        "* Zur Beschreibung der **Raumzeit und ihrer Kr√ºmmung bedient man sich der Differentialgeometrie**, die die Euklidische Geometrie des uns vertrauten ‚Äûflachen‚Äú dreidimensionalen Raumes der klassischen Mechanik umfasst und erweitert.\n",
        "\n",
        "* Die Differentialgeometrie verwendet zur Beschreibung gekr√ºmmter R√§ume, wie der Raumzeit der ART, sogenannte **Mannigfaltigkeiten**. Wichtige **Eigenschaften werden mit sogenannten Tensoren beschrieben**, die Abbildungen auf der Mannigfaltigkeit darstellen.\n",
        "\n",
        "* Die gekr√ºmmte Raumzeit wird als [Lorentz-Mannigfaltigkeit](https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit) (Pseudo-riemannsche Mannigfaltigkeit) beschrieben.\n",
        "\n",
        "* Eine besondere Bedeutung kommt dem [**metrischen Tensor**](https://de.wikipedia.org/wiki/Metrischer_Tensor) zu. Wenn man in den metrischen Tensor zwei Vektorfelder einsetzt, erh√§lt man f√ºr jeden Punkt der Raumzeit eine reelle Zahl.\n",
        "\n",
        "  * In dieser Hinsicht kann man den metrischen Tensor als ein verallgemeinertes, punktabh√§ngiges Skalarprodukt f√ºr Vektoren der Raumzeit verstehen.\n",
        "\n",
        "  * Mit seiner Hilfe werden Abstand und Winkel definiert und er wird daher kurz als Metrik bezeichnet.\n",
        "\n",
        "* Ebenso bedeutend ist der [riemannsche Kr√ºmmungstensor](https://de.wikipedia.org/wiki/Riemannscher_Kr√ºmmungstensor) zur Beschreibung der Kr√ºmmung der Mannigfaltigkeit,\n",
        "\n",
        "  * der eine Kombination von ersten und zweiten Ableitungen des metrischen Tensors darstellt.\n",
        "\n",
        "  * Wenn ein beliebiger Tensor in irgendeinem Koordinatensystem in einem Punkt nicht null ist, kann man √ºberhaupt kein Koordinatensystem finden, sodass er in diesem Punkt null wird. Dies gilt dementsprechend auch f√ºr den Kr√ºmmungstensor.\n",
        "\n",
        "  * Umgekehrt ist der Kr√ºmmungstensor in allen Koordinatensystemen null, wenn er in einem Koordinatensystem null ist. Man wird also in jedem Koordinatensystem bez√ºglich der Frage, ob eine Mannigfaltigkeit an einem bestimmten Punkt gekr√ºmmt ist oder nicht, zum gleichen Ergebnis gelangen.\n",
        "\n",
        "* Die ma√ügebliche Gr√∂√üe zur Beschreibung von Energie und Impuls der Materie ist der [Energie-Impuls-Tensor](https://de.wikipedia.org/wiki/Energie-Impuls-Tensor). Dieser Tensor bestimmt die Kr√ºmmungseigenschaften der Raumzeit. Siehe auch [Vierertensor](https://de.wikipedia.org/wiki/Vierertensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "1xA1Fe1B83_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Example for Tensors: Quantum Mechanics & Quantum Computing**\n",
        "\n",
        "Quantum Superposition:\n",
        "\n",
        "> $\\vec{\\psi}=a \\vec{v}+b \\vec{w}$\n",
        "\n",
        "* linear combination, physical quantum states are just vectors, using linear combinations to give more complicated states\n",
        "\n",
        "\n",
        "Quantum Entanglement\n",
        "\n",
        "> $\\vec{\\psi} \\otimes \\vec{\\phi}$\n",
        "\n",
        "* two states are 'entangled' together means state these state vectors have been combined together using the 'tensor product' (circle X)\n",
        "\n",
        "* Takes the geometrical space where the first system lives and the second system and combines them together to create a more complicated geometrical space, and that's where the entangled system lives"
      ],
      "metadata": {
        "id": "thzMwgWj86A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu k√∂nnen, ist eine Basis n√∂tig. Im n-dimensionalen Raum besteht diese aus n linear unabh√§ngigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur f√ºr lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) m√∂glich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabh√§ngigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): F√ºr echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabh√§ngigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekr√ºmmt sind, sind die Koordinatenachsen nicht r√§umlich fest, wie es f√ºr kartesische Koordinaten gilt. Dies f√ºhrt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abh√§ngt ‚Äì im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenfl√§chen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgew√§hlten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "oZN7V4P688A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Tangent Space & Kotangentialraum**\n",
        "\n",
        "* **Tangential and Normal Components (Koordinaten)**: Given a vector at a point on a curve (just a vector, not a tangential vector!), that vector can be decomposed uniquely as a sum of two vectors,\n",
        "\n",
        "  * $\\mathbf{v}_{\\|}$ : one tangent to the curve, called the **tangential component** of the vector. $\\mathbf{v}_{\\perp}$  : another one perpendicular to the curve, called the **normal component** of the vector. Zusatzlich gibt es noch: The binormal unit vector B is defined as the cross product of T and N ([Source](https://en.m.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas))\n",
        "\n",
        "  * More formally, let $S$ be a surface, and $x$ be a point on the surface. Let $\\mathbf{v}$\n",
        "be a vector at $x$. Then one can write uniquely $\\mathbf{v}$ as a sum: $\\mathbf{v}=\\mathbf{v}_{\\|}+\\mathbf{v}_{\\perp}$. Similarly a vector at a point on a surface can be broken down the same way.\n",
        "\n",
        "  * More generally, given a submanifold $N$ of a manifold $M,$ and\n",
        "a vector in the tangent space to $M$ at a point of $N$, it can be\n",
        "decomposed into the component tangent to $N$ and the\n",
        "component normal to $N$.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Surface_normal_tangent.svg/248px-Surface_normal_tangent.svg.png)\n",
        "\n",
        "* **Calculate the components**: To calculate the tangential and normal components, consider a [unit normal](https://en.wikipedia.org/wiki/Normal_(geometry)) to the surface, that is, a [unit vector](https://en.wikipedia.org/wiki/Unit_vector) $\\hat{n}$ perpendicular to $S$ at $x$.\n",
        "\n",
        "  * Then, normal component: $\\mathbf{v}_{\\perp}=(\\mathbf{v} \\cdot \\hat{n}) \\hat{n}$ and thus tangential component: $\\mathbf{v}_{\\|}=\\mathbf{V}-\\mathbf{V}_{\\perp}$ where \".\" denotes the [dot product](https://en.wikipedia.org/wiki/Dot_product).\n",
        "\n",
        "  * Another formula for the tangential component is $\n",
        "\\mathbf{v}_{\\|}=-\\hat{n} \\times(\\hat{n} \\times \\mathbf{v})$ where \" $\\times$ \" denotes the [cross product](https://en.wikipedia.org/wiki/Cross_product).\n",
        "\n",
        "  * Note that these formulas do not depend on the particular unit normal $\\hat{n}$ used (there exist two unit normals to any surface at a given point, pointing in opposite directions, so one of the unit normals is the negative of the other one).\n",
        "\n",
        "* Siehe Artikel: [Tangential and normal components](https://en.wikipedia.org/wiki/Tangential_and_normal_components)\n",
        "\n",
        "* **Tangentialvektor**: Sei $\\gamma:(-\\varepsilon, \\varepsilon) \\rightarrow M$ eine differenzierbare Kurve mit $\\gamma(0)=x$ und dem **Kurvenparameter $t$** (siehe 'Parameterdarstellungen' oben), dann ist: $v=\\frac{d \\gamma}{d t}(0) \\in T_{x} M$ ein [Tangentialvektor](https://en.wikipedia.org/wiki/Tangent_vector). Die Tangentialvektoren in einem Punkt $x \\in M$ spannen einen Vektorraum auf, den Tangentialraum $T_{x} M$. Siehe auch [Tangentialb√ºndel](https://de.wikipedia.org/wiki/Tangentialb√ºndel).\n",
        "\n",
        "* **Tangent Space (Tangentialraum)** Ein [Tangentialraum](https://de.wikipedia.org/wiki/Tangentialraum) ist $T_{x} M$ ein Vektorraum, der eine differenzierbare Mannigfaltigkeit $M$ am Punkt $x$ linear approximiert.\n",
        "\n",
        "* **Normal (Vector) Space**: The normal vector space or normal space of a manifold at point P is the **set of vectors which are orthogonal to the tangent space at P**. Normal vectors are of special interest in the case of smooth curves and smooth surfaces.\n",
        "\n",
        "* **Kotangentialraum**: Der [Kotangentialraum](https://de.wikipedia.org/wiki/Kotangentialraum) ist der Dualraum des entsprechenden Tangentialraums. Er ist ein Vektorraum, der einem Punkt einer differenzierbaren Mannigfaltigkeit $M$ zugeordnet wird.\n",
        "\n",
        "  * Sei $M$ eine differenzierbare Mannigfaltigkeit und $T_{p} M$ ihr Tangentialraum am Punkt $p \\in M$. Dann ist der Kotangentialraum definiert als der Dualraum von $T_{p} M$.\n",
        "\n",
        "  * **Das hei√üt, der Kotangentialraum besteht aus allen Linearformen auf dem Tangentialraum $T_{p} M$**.\n",
        "\n",
        "  * In differential geometry, **one can attach to every point $x$ of a smooth (or differentiable) manifold, $\\mathcal{M},$ a vector space called the cotangent space at $x .$** Typically, the cotangent space, $T_{x}^{*} \\mathcal{M}$ is defined as the dual space of the tangent space at $x, T_{x} \\mathcal{M},$ although there are more direct definitions. The elements of the cotangent space are **called cotangent vectors or tangent covectors**.\n",
        "\n",
        "  * Let $\\mathcal{M}$ be a smooth manifold and let $x$ be a point in $\\mathcal{M}$. Let $T_{x} \\mathcal{M}$ be\n",
        "the tangent space at $x$. Then the cotangent space at $x$ is defined as the dual space of $T_{x} \\mathcal{M}$ : $\n",
        "T_{x}^{*} \\mathcal{M}=\\left(T_{x} \\mathcal{M}\\right)^{*}\n",
        "$\n",
        "\n",
        "  * Concretely, **elements of the cotangent space are linear functionals on $T_{x} \\mathcal{M}$**. That is, **every element $\\alpha \\in T_{x}^{*} \\mathcal{M}$ is a linear map** $\n",
        "\\alpha: T_{x} \\mathcal{M} \\rightarrow F\n",
        "$\n",
        "\n",
        "  * where $F$ is the underlying field of the vector space being considered, for example, the field of real numbers. **The elements of $T_{x}^{*} \\mathcal{M}$ are called cotangent vectors.**\n",
        "\n",
        "*Die [Hauptkr√ºmmungen](https://de.wikipedia.org/wiki/Hauptkr%C3%BCmmung) sind [Eigenwerte](https://de.wikipedia.org/wiki/Eigenwertproblem) der [Weingartenabbildung](https://de.wikipedia.org/wiki/Weingartenabbildung)*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/e/eb/Minimal_surface_curvature_planes-en.svg)"
      ],
      "metadata": {
        "id": "E84nx70p9Db6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Koordinatentransformation (Geometric transformation) & Basiswechsel**\n",
        "\n",
        "* [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation), wenn sich ein Problem in einem anderen Koordinatensystem leichter l√∂sen l√§sst, z. B. bei der Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten.\n",
        "\n",
        "* [Liste von Transformationen in der Mathematik](https://de.m.wikipedia.org/wiki/Liste_von_Transformationen_in_der_Mathematik) und [Intro: Koordinatentransformation](http://walter.bislins.ch/physik/index.asp?page=Koordinatentransformation). Beispiel: [Drehung](https://de.wikipedia.org/wiki/Drehung) (Rotation), [Skalierung](https://de.wikipedia.org/wiki/Skalar_(Mathematik)), [Scherung](https://de.wikipedia.org/wiki/Scherung_(Geometrie)) & [Verschiebung](https://de.wikipedia.org/wiki/Parallelverschiebung) (Translation). [Affine Transformationen](https://de.wikipedia.org/wiki/Affine_Abbildung) aus linearen Transformation und Translation. Translation ist ein Spezialfall einer affinen Transformation, bei der A die Einheitsmatrix ist.\n",
        "\n",
        "* In der Regel verwendet man spezielle Transformationen, bei denen diese Funktionen gewissen Einschr√§nkungen ‚Äì z. B. Differenzierbarkeit, Linearit√§t oder Formtreue ‚Äì unterliegen.\n",
        "\n",
        "* Bei **lineare Transformationen** ([Lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)) sind die neuen Koordinaten lineare Funktionen der urspr√ºnglichen. [Matrixmultiplikation](https://de.wikipedia.org/wiki/Matrizenmultiplikation) des alten Koordinatenvektors $\\vec{x} = (x_1, \\dots, x_n)$ mit der Matrix $A \\rightarrow {\\vec {x}}'=A{\\vec {x}}$.\n",
        "\n",
        "    * $x_{1}^{\\prime}=a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n}$\n",
        "    * $x_{2}^{\\prime}=a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n}$\n",
        "    * $\\cdots$\n",
        "    * $x_{n}^{\\prime}=a_{n 1} x_{1}+a_{n 2} x_{2}+\\cdots+a_{n n} x_{n} .$\n",
        "\n",
        "* **Ein Basiswechsel ist ein Spezialfall einer Koordinatentransformation**: [**Basiswechsel im Vektorraum (Transformationsmatrix)**](https://de.wikipedia.org/wiki/Basiswechsel_(Vektorraum)) (lineare Algebra) ist der √úbergang zwischen zwei verschiedenen Basen eines endlichdimensionalen Vektorraums √ºber einem K√∂rper $K$. Beispiele:\n",
        "\n",
        "  * [Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten](https://de.m.wikipedia.org/wiki/Koordinatentransformation#Beispiele)\n",
        "\n",
        "  * [Transformation von Differential-Operatoren](https://de.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Differentialen) (Jacobi-Matrix)\n",
        "\n",
        "  * [Transformation von Vektorfeldern](https://de.m.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Vektorfeldern_und_-Operatoren)\n",
        "\n",
        "  * [Lorentz-Transformationen](https://de.wikipedia.org/wiki/Lorentz-Transformation) zur Beschreibungen von Ph√§nomenen in verschiedenen Bezugssystemen (Spezielle Relativit√§tstheorie). Verbinden in vierdimensionaler Raumzeit die Zeit- und Ortskoordinaten, mit denen verschiedene Beobachter angeben, wann und wo Ereignisse stattfinden. Lorentz-Transformationen erhalten Abst√§nde in nichteuklidischer Raumzeit ([Minkowskiraum](https://de.wikipedia.org/wiki/Minkowski-Raum)), Winkel aber nicht, da der Minkowskiraum kein normierter Raum ist.\n",
        "\n",
        "  * [Galilei-Transformation](https://de.wikipedia.org/wiki/Galilei-Transformation): √Ñquivalent zu Lorentz-Transformationen im dreidimensionalen euklidischen Raum. Anwendbar, wenn sich Bezugssysteme durch geradlinig-gleichf√∂rmige Bewegung, Drehung und/oder eine Verschiebung in Raum oder Zeit unterscheiden. Alle Beobachtungen von Strecken, Winkeln und Zeitdifferenzen stimmen in beiden Bezugssystemen √ºberein; alle beobachteten Geschwindigkeiten unterscheiden sich um die konstante Relativgeschwindigkeit der beiden Bezugssysteme.\n",
        "\n",
        "  * [Eichtransformation](https://de.wikipedia.org/wiki/Eichtransformation) ver√§ndert die Eichfelder einer physikalischen Theorie (z. B. die elektromagnetischen Potentiale oder die potentielle Energie) dergestalt, dass die physikalisch wirksamen Felder (z. B. das elektromagnetische Feld oder ein Kraftfeld) und damit alle beobachtbaren Abl√§ufe dabei die gleichen bleiben, z. B. die Verschiebung des Nullpunkts der potentiellen Energie, die Wahl des Referenzpotentials bei der Messung elektrischer Spannungen, ein konstanter Phasenfaktor an der komplexen Wellenfunktion der Quantenmechanik.\n",
        "\n",
        "* [Youtube Video 1](https://www.youtube.com/watch?v=CR7e7Zc0QLg) und [Youtube Video 2](https://www.youtube.com/watch?v=FFVauAY_FMI)\n",
        "\n",
        "**Eigenchris Video series:**\n",
        "\n",
        "* [Eigenchris: Tensors For Beginners (-1): Motivation](https://www.youtube.com/watch?v=8ptMTLzV4-I&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1: Forward and Backward Transformations](https://www.youtube.com/watch?v=sdCmW5N1LW4&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=3)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1.5: Correction on Forward + Backward Transforms](https://www.youtube.com/watch?v=ipRrCPvftTk&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=4)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 2: Vector definition](https://www.youtube.com/watch?v=uPbBDToXjBw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=5)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 3: Vector Transformation Rules](https://www.youtube.com/watch?v=A1h_eucHFW4&t=177s)\n",
        "\n"
      ],
      "metadata": {
        "id": "k7tUmz4d9FUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ as Kronecker Product, of Tensors and of Tensor Spaces*"
      ],
      "metadata": {
        "id": "zTjh6Dkf9Imc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product** (generalization of the outer product)\n",
        "\n",
        "* the [tensor product V ‚äó W](https://en.m.wikipedia.org/wiki/Tensor_product) of two vector spaces V and W (over the same field) is a vector space which can be thought of as the space of all tensors that can be built from vectors from its constituent spaces using an additional operation which can be **considered as a generalization and abstraction of the outer product**.\n",
        "\n",
        "**Tensor Product: The 2 different tensor product use cases**\n",
        "\n",
        "* the \"little\" tensor product which combines individual tensors\n",
        "\n",
        "* the \"big\" tensor product which combines entire tensor vector spaces\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_62.png)\n",
        "\n",
        "* the Kronecker product, sometimes denoted by ‚äó,[1] is an operation on two matrices of arbitrary size resulting in a block matrix.\n",
        "* It is a generalization of the outer product (which is denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis.\n",
        "* The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n",
        "* The Kronecker product is a special case of the tensor product, so it is bilinear and associative"
      ],
      "metadata": {
        "id": "JRvzAr6v9RWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Tensor Product $\\otimes$ vs Kronecker Product $\\otimes$**\n",
        "\n",
        "* They are technically different things, but highly related to each other.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 13: Tensor Product vs Kronecker Product](https://www.youtube.com/watch?v=qp_zg_TD0qE&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=16)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_40.png)\n",
        "\n",
        "* **Here are examples on how the tensor products works:**\n",
        "\n",
        "* Basis for Vector Space $V$\n",
        "\n",
        "> $\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}} \\in V$\n",
        "\n",
        "Basis for Dual Space $V*$\n",
        "\n",
        "> $\\epsilon^{1}, \\epsilon^{2} \\in V^{*}$\n",
        "\n",
        "* And the covectors are linear functions that are defined by these rules here, where the covector $\\epsilon^{i}$ is acting on the vector $\\overrightarrow{e_{j}}$ gives the Kronecker delta $\\delta_{j}^{i}$ as the result (=we get 1 if i and j are the same and 0 if not).\n",
        "\n",
        "> $\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{j}^{i}=\\left\\{\\begin{array}{l}1, i=j \\\\ 0, i \\neq j\\end{array}\\right.$\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)\n",
        "\n",
        "**To demonstrate that this is a linear map, we can pass in an input vector $\\overrightarrow{v}$**\n",
        "\n",
        "To get the output we can just we pass $v$ to the covector $\\epsilon$ first\n",
        "\n",
        "> $\\left(\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}\\right)(\\vec{v})$\n",
        "\n",
        "Pass v to the covector:\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}(\\vec{v})\\right)$\n",
        "\n",
        "Then we expand v as a linear combination of the basis vectors\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "The we bring the components outside since covectors are linear functions (we can scale before or after):\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "And the covector acting on a vector becomes a Kronecker delta:\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "Then Kronecker index cancellation rule we can remove the $k$ indexes and get j:\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "So this is a function that takes a vector input and produces a vector output\n",
        "\n",
        "\n",
        "**Here are examples on how the Kronecker products works:**\n",
        "\n",
        "* Here you get a row of columns when you take the first array on the left and distribute every element to the second array on the right:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_46.png)\n",
        "\n",
        ".. and multiplying this with another column vector we get the same thing (here you get a column of rows of columns)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_47.png)\n",
        "\n",
        "\n",
        "**Summary**\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "* these coefficient are the entries of a matrix (on the tensor product side)\n",
        "\n",
        "* on the Kronecker delta side you can see this matrix !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_48.png)\n",
        "\n",
        "* Tensor product and Kronecker product are doing basically the same kind of thing\n",
        "\n",
        "* It‚Äôs just the tensor product is combining the abstract vector and the abstract covector in the land of algebraic symbols\n",
        "\n",
        "* And the Kronecker product is combining the the vector array and the covector array  in the land of arrays\n",
        "\n",
        "* But the components that we get from the tensor are just the components of the matrix that we get from Kronecker product\n",
        "\n",
        "* So they are sort of the same operation they just do the work in different contexts\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_49.png)"
      ],
      "metadata": {
        "id": "kz-n4Xvc9TFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 1*\n",
        "\n",
        "We can see a lot of similarities between:\n",
        "* the Kronecker product for matrices and\n",
        "* the tensor product for abstract Hilbert spaces.\n",
        "\n",
        "* If a vector can be expressed as the tensor product of two vectors, we call it a product vector, i.e., $|\\Psi\\rangle$ **is a product vector** if $|\\Psi\\rangle=|\\phi \\otimes \\chi\\rangle$ for some $|\\phi\\rangle$ and $|\\chi\\rangle$. Any nonproduct vector is called entangled. The entangled states of a composite system occupy a distinguished position in the interpretation of quantum mechanics.\n",
        "\n",
        "* We would like to define a <font color=\"blue\">**tensor product for abstract Hilbert spaces**</font>.\n",
        "\n",
        "* The main need for this comes from the necessity of <font color=\"blue\">**describing composite quantum systems**</font>.\n",
        "\n",
        "* If we have two different systems $\\mathrm{A}$ and $\\mathrm{B}$, then **we have separate Hilbert spaces**, say $\\mathcal{H}_{A}$ and $\\mathcal{H}_{B}$, for describing the quantum states of these systems.\n",
        "\n",
        "* Basically a vector $|\\alpha\\rangle_{A}$ in $\\mathcal{H}_{A}$ gives a state of $\\mathrm{A}$ and a vector $|\\beta\\rangle_{B}$ in $\\mathcal{H}_{B}$ gives a state of $\\mathrm{B}$.\n",
        "\n",
        "* In a similar way, we should <font color=\"blue\">**have an entirely different Hilbert space** $\\mathcal{H}_{A B}$ for describing the states of the composite system AB.</font>\n",
        "\n",
        "* In particular, we would like to have a vector in $\\mathcal{H}_{A B}$ that describes the state of $\\mathrm{AB}$ such that $\\mathrm{A}$ is in state $\\mid \\alpha \\rangle_{A}$ and $\\mathrm{B}$ is in state $|\\beta\\rangle_{B}$. We will denote this state as $|\\alpha\\rangle_{A} \\otimes|\\beta\\rangle_{B}$.\n",
        "\n",
        "* We will also want to have this product $\\otimes$ be such that superpositions of states in $\\mathrm{A}$ or in B could also be equivalently described as superpositions of states in $\\mathrm{AB} ;$ hence **distributivity**.\n",
        "\n",
        "http://www.physics.metu.edu.tr/~sturgut/p455/qm-math4.pdf"
      ],
      "metadata": {
        "id": "L_0hUsHx9Wtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 2*\n",
        "\n",
        "* **The two notions represent operations on different objects: Kronecker product on matrices; tensor product on linear maps between vector spaces**.\n",
        "\n",
        "* But there is a connection: Given two matrices, we can think of them as representing linear maps between vector spaces equipped with a chosen basis.\n",
        "\n",
        "* The Kronecker product of the two matrices then represents the tensor product of the two linear maps.\n",
        "\n",
        "* (This claim makes sense because the tensor product of two vector spaces with distinguished bases comes with a distinguish basis.)\n",
        "\n",
        "https://math.stackexchange.com/questions/203947/tensor-product-and-kronecker-product"
      ],
      "metadata": {
        "id": "o08kUOZC9Ym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 3*\n",
        "\n",
        "Sometimes the Kronecker product is also called direct product\n",
        "or tensor product.\n",
        "\n",
        "https://www.worldscientific.com/doi/pdf/10.1142/9789811202520_0002"
      ],
      "metadata": {
        "id": "SdLDqnuT9aid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 4*\n",
        "\n",
        "* |u> <v| is a way of writing the <font color=\"blue\">**tensor product of a vector and a dual vector**</font> (ie, an element of the Hilbert space and an element of its dual, which is usually casually identified with the Hilbert space using the inner product).\n",
        "\n",
        "* This is a linear operator on the Hilbert space, sending |w > to <v|w>|u>.\n",
        "\n",
        "* In general, the tensor product of a vector space and its dual is the space of (finite rank) linear operators on the vector space.\n",
        "\n",
        "* On the other hand, |u>|v> is an element of the tensor product of the vector space with itself, usually used in physics for describing a composite of two identical systems. Again, since there is an isomorphism between the vector space and its dual, there is one between the space of composite states and the space of linear operators. This is interesting, but I've never seen this put to good use.\n",
        "\n",
        "* Finally, **the Kronecker product is just a particular representation of the tensor product, convenient for dealing with tensor products of linear operators**.\n",
        "\n",
        "Source https://www.physicsforums.com/threads/difference-between-the-outer-product.236244/"
      ],
      "metadata": {
        "id": "Xim2BtEb9cXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 5*\n",
        "\n",
        "* the Kronecker product and tensor product essentially have the same mathematical \"actions\" (an expanded matrix with dimensions equal to the product of the two matrices), but the tensor product explicitly applies only to matrices representing linear maps.\n",
        "\n",
        "* Every matrix represents a linear map.\n",
        "\n",
        "* Linear maps are in some sense more general than matrices. They are also different (types of) objects, even though matrices can be used to represent _some_ linear maps. The matrix representation of a particular linear map depends on the choice of basis of the domain and target space, for example, whereas the linear map itself is invariant under such choices. On the other hand a matrix can also represent a bilinear form, not just a linear map. Consider reading Axler's _Linear Algebra Done Right_ for a good explanation of some of such subtleties.\n",
        "\n",
        "https://math.stackexchange.com//questions/203947"
      ],
      "metadata": {
        "id": "iGV-6fNH9eGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covariance (Covector) and Contravariance (Vector): Tensor Notation for Vectors & Covectors in Coordinate Systems*"
      ],
      "metadata": {
        "id": "icJDI9EQ9gc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of a Tensor**\n",
        "\n",
        "> **Tensor = an object that is invariant under a change of coordinates, and... has components that change in a special, predictable way under a change of coordinates**\n",
        "\n",
        "* the direction and the lengths of an object are invariant (=vector)\n",
        "\n",
        "> **The vector components are NOT invariant. They change  depending on the coordinate system they use.**\n",
        "\n",
        "* if you know how to switch between coordinate system, you should be able to know how to switch between vector components (forward and backward)\n",
        "\n",
        "> **Tensor: A collection of vectors and convectors combined together using the tensor product.** (abstract definition)\n",
        "\n",
        "> **Tensors as partial derivatives and gradients that transform with the Jacobian matrix.**\n",
        "\n",
        "* Tensoren sind Gr√∂ssen, mit deren Hilfe man Skalare, Vektoren und weitere Gr√∂ssen analoger Struktur in ein einheitliches Schema zur Beschreibung mathematischer und physikalischer Zusammenh√§nge einordnen kann.\n",
        "\n",
        "* Sie sind definiert durch ihre Transformationseigenschaften gegen√ºber orthogonalen Transformationen wie z.B. Drehungen. Es geht darum, was √§ndert sich, was √§ndert sich nicht, wenn man das Bezugssystem √§ndert?\n",
        "\n",
        "* Die Besonderheit von Tensorgleichungen ist, dass sie transformations-invariant sind. Wenn es gelingt, einen physikalischen Sachverhalt in Tensorschreibweise zu formulieren, so kann man wegen der speziellen Art, wie Tensoren transformieren, sicher sein, dass die Gleichungen in jedem beliebigen Koordinatensystem gelten.\n",
        "\n",
        "* **Rang (oder Stufe) eines Tensors und Indizes**: Tensoren haben **Indizes**. Die Anzahl der Indizes gibt den Rang oder die Stufe des Tensors an. Die Indizes laufen entsprechend der **Dimension** $D$ des Raumes √ºber $1 . . D$. Bei der 4-dimensionalen Raumzeit beginnt die Nummerierung bei $0,$ wobei der Index 0 die zeitliche Komponente betrifft. Beim Arbeiten mit den Tensoren muss die **Reihenfolge der Indices** immer klar sein. Das Element $t_{12}$ eines Tensors ist in der Regel vom Element $t_{21}$ verschieden.\n",
        "\n",
        "  * Der einfachste Tensor ist ein Tensor mit Rang 0 . Dabei handelt es sich einfach um einen **Skalar**. Ein Skalar hat eigentlich keine Komponenten, sondern ist nur ein einzelner Wert und ben√∂tigt somit keinen Index; daher der Rang $0 .$\n",
        "\n",
        "  * Ein Tensor mit nur einem Index nennt man auch **Vektor**. Man sagt, der Vektor ist ein Tensor mit dem Rang 1. Der Index hat so viele Werte, wie die Dimension des Vektors. Bei einem 3 -dimensionalen Vektor hat der Index also 3 Werte (z.B: 1,2,3$),$ weil der Vektor 3 Komponenten hat: $\\vec{V}=\\left(V^{1}, V^{2}, V^{3}\\right)$.\n",
        "\n",
        "  * Ein Tensor vom Rang 2 hat 2 Indizes und stellt eine quadratische **Matrix** dar usw.\n",
        "\n",
        "  * Jede Tensor-Komponente kann eine Funktion oder eine Zahl sein. In der AR (allgemeinen Relativitatstheorie) sind die Komponenten in der Regel Funktionen der Raumzeit.\n",
        "\n",
        "* **Dimension eines Tensors**: Ein Vektor oder Tensor ist ein Objekt, welches Komponenten hat. Die Anzahl der Komponenten eines Vektors enstpricht der Dimension D des Raumes. Ein 3-dimensionaler Vektor besteht somit aus 3 Komponenten. Ein 3-dimensionaler Tensor der Stufe 2 besteht aus 3x3 Komponenten usw.\n",
        "\n",
        "> **Typ eines Tensors**: Wenn **Rang, Dimension und Komponenten-Arten** (Anzahl Indizes oben und unten) von Tensoren √ºbereinstimmen, dann sagt man, die Tensoren sind vom selben Typ. Dies muss bei der Tensor-Arithmetik beachtet werden.\n",
        "\n",
        "* **Tensor-Arithmetik**: Weil Skalare und Vektoren Subklassen von Tensoren sind ist zu erwarten, dass Tensoren denselben bekannten Rechenregeln f√ºr Addition, Subtraktion, Multiplikation und Division folgen.  Dies stimmt meistens, jedoch mit einigen √Ñnderungen und Einschr√§nkungen. Tensoren zeigen zudem neue Eigenschaften, die es bei Skalaren und Vektoren nicht gibt. [Source](http://walter.bislins.ch/physik/index.asp?page=Tensor%2DArithmetik)\n",
        "\n",
        "* **Tensor vs. Matrix**: A tensor of rank n is a mathematical object that has n indices and m$^n$ components. **Matrix is a tensor rank 2, because you need 2 basis vectors. Tensors rank 3 are the boxes with several stacked matrices**.\n",
        "\n",
        "  * Matrix is just and array of numbers, meanwhile a tensor (like stress tensor) obeys specific transformation rules and has a rules physical meaning. We can use a matrix to represent a tensor, but a tensor has a deeper physical significance. Matrix: I can just write down the matrix and its elements, but don‚Äôt have to add anything else\n",
        "\n",
        "  * Tensor: I need to specify the coordinate system, the components, and the basis vectors that each of those components correspond to. A tensor requires more detailed specification than a matrix. And it has these transformation properties where it‚Äôs invariant under a change of coordinate system, it has physical significance.\n",
        "\n",
        "* See video: [Introduction to Tensors](https://www.youtube.com/watch?v=uaQeXi4E7gA&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=1&t=187s)"
      ],
      "metadata": {
        "id": "icw4h44B9uTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoren und Koordinatensysteme**\n",
        "\n",
        "* In conventional math syntax we make **use of covariant indexes when dealing with Cartesian coordinate systems** $(x_{1},x_{2},x_{3})$ frequently without realizing this is a limited use of tensor syntax as covariant indexed components.\n",
        "\n",
        "* [Curvilinear coordinate systems](https://en.wikipedia.org/wiki/Curvilinear_coordinates), such as **cylindrical or spherical coordinates**, are often used in physical and geometric problems. Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and **covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another**.\n",
        "\n",
        "* In der Anwendung steht das Tensorsymbol immer f√ºr eine bestimmte Bedeutung. Zum Beispiel den Ort eines Teilchens. Der Ort kann in verschiedenen Koordinatensystemen gemessen werden. Entsprechend haben die Komponenten des Tensors in jedem Koordinatensystem andere Werte. Doch der Tensor bleibt derselbe (zB L√§nge eines Vektors), egal in welchem Koordinatensystem er gemessen wird.\n",
        "\n",
        "* **Gradient Vector**: Tensor calculus provides a generalization to the gradient vector formula from standard calculus **that works in all coordinate systems**: $\\nabla F=\\nabla_{i} F \\vec{Z}^{i}$ where: $\\nabla_{i} F=\\frac{\\partial F}{\\partial Z^{i}}$. In contrast, for standard calculus, the gradient vector formula is dependent on the coordinate system in use (example: Cartesian gradient vector formula vs. the polar gradient vector formula vs. the spherical gradient vector formula, etc.). In standard calculus, each coordinate system has its own specific formula, unlike **tensor calculus that has only one gradient formula that is equivalent for all coordinate systems**. This is made possible by an understanding of the metric tensor that tensor calculus makes use of."
      ],
      "metadata": {
        "id": "XavAxkWY9wMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covariance and Contravariance**\n",
        "\n",
        "> In multilinear algebra and tensor analysis, [covariance and contravariance](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors) describe how the quantitative description of certain geometric or physical entities changes with a [change of basis](https://en.wikipedia.org/wiki/Change_of_basis).\n",
        "\n",
        "* **The reason of having covariant or contravariant tensors is because you want to represent the same thing in a different coordinate system. Such a new representation is achieved by a transformation using a set of partial derivatives. In tensor analysis, a good transformation is one that leaves invariant the quantity you are interested in.**\n",
        "\n",
        "* Man unterscheidet in der [Tensor notation](https://en.wikipedia.org/wiki/Tensor_calculus#Syntax) bei Tensoren zwei Arten von Komponenten (*Jeder Tensor kann sowohl in kontravarianten, als auch in kovarianten Komponenten dargestellt werden*):\n",
        "\n",
        "  * **contravariant** upper component index: $v^{i}$\n",
        "\n",
        "  * **covariant** lower component index: $v_{i}$\n",
        "\n",
        "* [**Mixed Tensors**](https://en.wikipedia.org/wiki/Mixed_tensor) with both covariant and contravariant (for Tensors rang 2 or higher): $T^{m}{ }_{n}$. Consider related tensors: $T_{\\alpha \\beta \\gamma}, T_{\\alpha \\beta}^{\\gamma}, T_{\\alpha}{ }^{\\beta}{ }_{\\gamma}, T_{\\alpha}{ }^{\\beta \\gamma}, T_{\\beta \\gamma}^{\\alpha}, T^{\\alpha}{ }_{\\beta}^{\\gamma}, T^{\\alpha \\beta}{ }_{\\gamma}, T^{\\alpha \\beta \\gamma}$. The first one is covariant, the last one contravariant, and the remaining ones mixed.\n",
        "\n",
        "  * A mixed tensor of type or valence $\\left(\\begin{array}{l}M \\\\ N\\end{array}\\right),$ also written \"type $(M, N)$\" which has $M$ contravariant indices and $N$ covariant indices can be defined as a linear function which maps an $(M+N)$ -tuple of $M$ one-forms and $N$ vectors to a scalar.\n",
        "\n",
        "  * a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the [tangent bundle](https://en.wikipedia.org/wiki/Tangent_bundle) as well as the [cotangent bundle](https://en.wikipedia.org/wiki/Cotangent_bundle)\n",
        "\n",
        "* **Pushforwards (covariant) & Pullbacks (contravariant)**:\n",
        "\n",
        "  * Vectors with upper-index objects have [pushforwards](https://de.m.wikipedia.org/wiki/Pushforward), which are covariant.\n",
        "\n",
        "  * Covectors with lower-index objects have [pullbacks](https://de.m.wikipedia.org/wiki/R√ºcktransport), which are contravariant.\n",
        "\n",
        "  * In category theory covariance and contravariance are properties of [functors](https://en.m.wikipedia.org/wiki/Functor). Sometimes, contravariant functors are called [\"cofunctors\"](https://en.m.wikipedia.org/wiki/Functor).\n",
        "\n",
        "* Der Wechsel von einem Tensor in kontravarianter Darstellung zu einem Tensor in kovarianter Darstellung, wird [**\"Index ziehen\" oder Index Manipulation**](http://walter.bislins.ch/physik/index.asp?page=Index%2DManipulation+per+Metrik%2DTensor) genannt. Die Umrechnung geht durch Multiplikation mit dem sog. **Metrischen Tensor**.\n",
        "\n",
        "  * A given **contravariant index of a tensor can be lowered using the [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor)** $g_{\\mu v}$ and a given covariant index can be raised using the inverse metric tensor $g^{\\mu v}$. Thus, $g_{\\mu v}$ could be called the index lowering operator and $g^{\\mu v}$ the index raising operator.\n",
        "\n",
        "* On a manifold, a tensor field will typically have multiple, upper and lower indices, where [Einstein notation](https://en.m.wikipedia.org/wiki/Einstein_notation) is widely used. * Tensors notation allows a vector $(\\vec{V})$ to be decomposed into an [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation) representing the [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) of a [basis vector](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) $\\left(\\vec{Z}_{i}\\right.$ or $\\left.\\vec{Z}^{i}\\right)$ with a component vector $\\left(V_{i}\\right.$ or $\\left.V^{i}\\right)$: $\\vec{V}=V^{i} \\vec{Z}_{i}=V_{i} \\vec{Z}^{i}$.\n",
        "\n",
        "* *Exkurs: In einem engeren Wortsinn bezeichnet kovariant in der mathematischen Physik Gr√∂√üen, **die wie Differentialformen transformieren**. Diese kovarianten Gr√∂√üen $P$ bilden einen Vektorraum $\\mathcal{V}$, auf dem eine Gruppe von linearen Transformationen wirkt. Die Menge der linearen Abbildungen der kovarianten Gr√∂√üen in die reellen Zahlen $Q: P \\mapsto Q(P) \\in \\mathbb{R}, \\quad Q(a P+b \\tilde{P})=a Q(P)+b Q(\\tilde{P})$ bildet den zu $\\mathcal{V}$ dualen Vektorraum $\\mathcal{V}^{*}$. Schreiben wir die transformierten, kovarianten Gr√∂√üen $P^{\\prime}$ mit einer Matrix $\\Lambda$ als $P^{\\prime}=\\Lambda P$ dann definiert $Q^{\\prime}\\left(P^{\\prime}\\right)=Q(P)$ das kontravariante oder kontragrediente Transformationsgesetz des Dualraumes $Q^{\\prime}=\\Lambda^{-1 \\mathrm{~T}} Q$. Wegen $\\left(\\Lambda_{2}\\right)^{-1 \\mathrm{~T}}\\left(\\Lambda_{1}\\right)^{-1 \\mathrm{~T}}=\\left(\\Lambda_{2} \\Lambda_{1}\\right)^{-1 \\mathrm{~T}}$ gen√ºgt die kontravariante Transformation derselben Gruppenverkn√ºpfung wie die kovariante Transformation.*\n",
        "\n",
        "* Quellen: [Ko- und kontravariante Darstellung](https://www.math.tugraz.at/~ganster/lv_vektoranalysis_ss_10/20_ko-_und_kontravariante_darstellung.pdf), [Kovarianz und Kontravarianz von Vektoren](http://walter.bislins.ch/physik/index.asp?page=Kovarianz+und+Kontravarianz+von+Vektoren), [Kovariante und Kontravariante Komponenten](http://walter.bislins.ch/physik/index.asp?page=Kovariante+und+Kontravariante+Komponenten). Siehe auch Video: [Tensorrechnung, 3.1 : Was bedeutet Kovariant und kontravariant?](https://www.youtube.com/watch?v=PNRoBOzije8) und [Kovarianz, Kontravarianz, Tensoren](https://av.tib.eu/media/19916)\n"
      ],
      "metadata": {
        "id": "g1CsjpEU90rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein beliebiger Vektor kann als Linearkombination entweder der kovarianten Basisvektoren oder der kontravarianten Basisvektoren dargestellt werden** (die Transformationen invers zueinander).\n",
        "\n",
        "* **Kovariante Basisvektoren** $\\vec{b}_{u}$ sind kombiniert mit **kontravariante Koordinaten** $a_{u_{i}}$. Heisst auch kontravarianter Vektor (besser: kontravarianter Koordinatenvektor) und sind Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "* **Kontravariante Basisvektoren** $\\vec{b}_{u_{i}}^{*}$ kombiniert mit **kovarianten Koordinaten** $a_{u_{i}}^{*}$. Heisst auch kovarianter Vektor bzw. Covector (Kovektoren als Linearformen von Normalenvektoren) und sind normal zu den Koordinatenfl√§chen.\n",
        "\n",
        "* **Vektordarstellungen**: $\\vec{a}=\\sum_{i=1}^{n} a_{u_{i}} \\vec{b}_{u_{i}}=\\sum_{i=1}^{n} a_{u_{i}}^{*} \\vec{b}_{u_{i}}^{*}$\n",
        "\n",
        "* Wenn $\\vec{g}_{i} \\cdot \\vec{g}^{j}=\\delta_{i}^{j}$ (**Kronecker Delta / Skalarprodukt ist Null**) dann heissen $\\left\\{\\overrightarrow{g_{i}}\\right\\}$ und $\\left\\{\\overrightarrow{g^{j}}\\right\\}$ **reziprok** zueinander. Die beiden Klassen von Basisvektoren sind dual bzw. reziprok zueinander (siehe [duale Basis](https://de.wikipedia.org/wiki/Duale_Basis)). Diese beiden Basen bezeichnet man als **holonome Basen**.\n",
        "\n",
        "* Diese kreuzweise Paarung (kontra-ko bzw. ko-kontra) sorgt daf√ºr, dass der Vektor $\\vec{a}$ unter Koordinatentransformation invariant ist (zB Geschwindigkeit eines Teilchens), da die Transformationen von Koordinaten und Basisvektoren invers zueinander sind und sich gegenseitig aufheben."
      ],
      "metadata": {
        "id": "51tAOvER93PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Vectors): Basiswechsel und Vector-(Coordinate)-Transformation (Contravariant)*"
      ],
      "metadata": {
        "id": "VkuVX0zf959A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vektorkoordinaten verhalten sich kontravariant**\n",
        "\n",
        "* **Problem**: For example, we consider the transformation from one coordinate system $x^{1}, \\ldots, x^{n}$ to another $x^{\\prime}, \\ldots, x^{\\prime n}$\n",
        "$x^{i}=f^{i}\\left(x^{\\prime}, x^{\\prime 2}, \\ldots, x^{\\prime n}\\right)$ where $f^{i}$ are certain functions.\n",
        "Take a look at a couple of specific quantities. How do we transform coordinates? The answer is \"**coordinate differentials**\":\n",
        "\n",
        "> $d x^{i}=\\frac{\\partial x^{i}}{\\partial x^{\\prime k}} d x^{\\prime k}$\n",
        "\n",
        "* **Darstellung**: $c^{1}$ (upper index, hochgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Vektorkoordinaten verhalten sich kontravariant. (wahrscheinlich: kovariante Basisvektoren (= \"kontravarianten Vektor\", weil Koordinaten kontravariant): Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da, wie oben beschrieben, die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "  * Every quantity which under a transformation of (vector) coordinates, transforms like the coordinate differentials is called a contravariant tensor.\n",
        "\n",
        "  * **Vectors exhibit a behavior of changing scale inversely to changes in scale to the reference axes are called contravariant** (see second example below). As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).\n",
        "\n",
        "* **Examples**: (of contravariant vectors / vectors with contravariant components)\n",
        "\n",
        "  * position of an object relative to an observer, or any derivative of position with respect to time: velocity, acceleration, jerk, displacement, momentum, force.\n",
        "\n",
        "  * For instance, by changing scale from meters to centimeters (that is, **dividing the scale of the reference axes by 100**), the components of a measured velocity vector are **multiplied by 100**.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * In physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem.\n",
        "\n",
        "  * A contravariant vector or tangent vector (often abbreviated simply as vector, such as a direction vector or velocity vector) has components that contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be contravariant.\n",
        "\n",
        "  * If the **reference axes** were rotated in one direction, the **component representation** of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way. In Einstein notation, contravariant components are denoted with upper indices as in $\\mathbf{v}=v^{i} \\mathbf{e}_{i}$ (note: implicit summation over index \"i\")\n",
        "\n",
        "https://math.stackexchange.com/questions/8170/intuitive-way-to-understand-covariance-and-contravariance-in-tensor-algebra\n",
        "\n",
        "*Kontravariantes Transformationsverhalten*\n",
        "\n",
        "* Grundfrage: Wie werden Normalenvektoren in den alten Basisvektoren zu Normalenvektoren in neuen Basivektoren?\n",
        "\n",
        "* Die normalen Vektoren gehen gegen die Basis, d.h. kontravariant\n",
        "\n",
        "* Gegeben einen Normalenvektor a aus dem Vektorraum V, und diesen zerlegen in die Basisvektoren in alter Basis und dann transformieren in neue Basis:\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_03.png)\n",
        "\n",
        "Nun Kovektor angewandt auf Summe von Zahlen mal Vektoren (Vektoren sind kontravariant):\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_04.png)\n",
        "\n",
        "> **Man nennt a einen \"kontravarianten\" Vektor, aber eigentlich muss man sagen, dass seine Komponenten kontravariant transformieren. Die eigentlichen Vektoren aus dem Originalvektorraum sind kontravariant.**\n",
        "\n",
        "\n",
        "* and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates\n",
        "\n",
        "* The inverse of a covariant transformation is a **contravariant transformation**.\n",
        "\n",
        "  * Whenever a vector should be invariant under a change of basis, that is to say **it should represent the same geometrical or physical object <u>having the same magnitude and direction as before</u>, its components must transform according to the contravariant rule**.\n",
        "\n",
        "  * Conventionally, indices identifying the components of a vector are placed as upper indices and so are all indices of entities that transform in the same way.\n",
        "\n",
        "* The sum over pairwise matching indices of a product with the same lower and upper indices are invariant under a transformation.\n",
        "\n",
        "* A vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector $\\mathbf{v}$ is given, say, in components $V^{\\prime}$ on a chosen basis $\\mathbf{e}_{i}$. On another basis, say $\\mathbf{e}^{\\prime}{ }_{j}$, the same vector $\\mathbf{v}$ has different components $v^{\\prime j}$ and\n",
        "\n",
        "> $\n",
        "\\mathbf{v}=\\sum_{i} v^{i} \\mathbf{e}_{i}=\\sum_{j} v^{\\prime j} \\mathbf{e}_{j}^{\\prime}\n",
        "$\n",
        "\n",
        "* As a vector, $\\mathbf{v}$ should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its \"real world\" direction and magnitude should appear the same regardless of the basis vectors.\n",
        "\n",
        "* If we perform a change of basis by transforming the vectors $\\mathbf{e}_{i}$ into the basis vectors $\\mathbf{e}_{j}$, we must also ensure that the components $v^{i}$ transform into the new components $v$ to compensate.\n",
        "\n",
        "* The needed transformation of $\\mathbf{v}$ is called the contravariant\n",
        "transformation rule."
      ],
      "metadata": {
        "id": "4bkcyHaG-NV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation: Forward and Backward Transform between Basis Vectors in 2 dimensions**\n",
        "\n",
        "*Forward: build the new basis vectors (not any vector!) from the old basis vectors*:\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "\\text { Old Basis: }\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\\\\n",
        "\\text { New Basis: }\\left\\{\\widetilde{e_{1}}, \\widetilde{e_{2}}\\right\\}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_08.png)\n",
        "\n",
        "* Forward transformation (create new basis vectors):\n",
        "\n",
        "$\n",
        "\\begin{array}{ll}\n",
        "\\widetilde{e_{1}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Which gives us for the image above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}=-1 / 2 \\overrightarrow{e_{1}}+1 / 4 \\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Result is in the forward matrix (Notice how the coefficient from the first equation end up in the first column (2 and 1), and how the coefficients from the second equation end up in the second column)\n",
        "\n",
        "$\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\overrightarrow{e_{1}}=1 / 4 \\widetilde{e_{1}}+(-1) \\widetilde{\\overrightarrow{e_{2}}} \\\\\n",
        "\\overrightarrow{e_{2}}=1 / 2 \\widetilde{e_{1}}+2 \\widetilde{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Which gives as the backward matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in 2 dimensions?**\n",
        "\n",
        "* How do forward and backward matrices relate to each other?\n",
        "\n",
        "* They are inverses, and matrix multiplication (dot product) leads to the identity matrix:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "F B &=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "(2*1/4)+(-1/2*-1) & (2*1/2)+(-1/2*2) \\\\\n",
        "(1*1/4)+(1/4*-1) & (1*1/2)+(1/4*2)\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "> This means: $\n",
        "B=F^{-1}\n",
        "$\n",
        "\n",
        "*Reminder: Matrix Multiplication Rules:*\n",
        "\n",
        "$\\begin{aligned}\n",
        "&=\\left[\\begin{array}{cc}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "e & f \\\\\n",
        "g & h\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "ae+bg & af+bh \\\\\n",
        "ce+dg & cf+dh\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Summary of two transformation matrices between old basis and new basis:**\n",
        "\n",
        "> Forward Matrix: $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> Backward Matrix: $\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_09.png)"
      ],
      "metadata": {
        "id": "3Xgqkmin-PcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and Backward Transform to n dimensions (Generalization)**\n",
        "\n",
        "First construct the new basis vectors from the old ones with right coefficients:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=F_{11} \\overrightarrow{e_{1}}+F_{21} \\overrightarrow{e_{2}}+\\cdots+F_{n 1} \\overrightarrow{e_{n}} \\\\\n",
        "\\widetilde{e_{2}}={F_{12} \\overrightarrow{e_{1}}+F_{22} \\overrightarrow{e_{2}}+\\cdots+F_{n 2} \\overrightarrow{e_{n}}}\\\\\n",
        "{\\ldots} \\\\\n",
        "\\widetilde{e_{n}}=F_{1 n} \\overrightarrow{e_{1}}+F_{2 n} \\overrightarrow{e_{2}}+\\cdots+F_{n n} \\overrightarrow{e_{n}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $F$ (notice again **how it's transposed to the coefficients above**!)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "F_{11} & F_{12} & \\ldots & F_{1 n} \\\\\n",
        "F_{21} & F_{22} & \\ldots & F_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "F_{n 1} & F_{n 2} & \\ldots & F_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "**How to facilitate it without writing all these equations above?**\n",
        "\n",
        "Let‚Äôs take an example: in the following equation taken from the table above (not the coefficient matrix, because there the values are transposed!):\n",
        "\n",
        "$F_{12}$ tells us how much of $\\overrightarrow{e_{1}}$ is in $\\widetilde{e_{2}}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{e_{2}}=F_{12} \\overrightarrow{e_{1}}\n",
        "\\end{equation}$\n",
        "\n",
        "**So what I want is $\\sum F_{k j}$ that tells us how much of $\\overrightarrow{e_{k}}$ makes up $\\widetilde{{e}_{j}}$**:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Exkurs: later we will use the Einstein notation and drop the summation sign to make it easier:**\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        " Now we do the same for the backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=B_{11} \\widetilde{e_{1}}+B_{21} \\widetilde{e_{2}}+\\cdots+B_{n 1} \\widetilde{e_{n}} \\\\\n",
        "\\overrightarrow{e_{2}} &=B_{1 2} \\widetilde{e_{1}}+B_{22} \\widetilde{e_{2}}+\\cdots+B_{n 2} \\widetilde{e_{n}} \\\\\n",
        "{\\ldots} \\\\\n",
        "\\overrightarrow{e_{n}} & =B_{1 n} \\widetilde{e_{1}}+B_{2 n} \\widetilde{e_{2}}+\\cdots+B_{n n} \\widetilde{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $B$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "B_{11} & B_{12} & \\ldots & B_{1 n} \\\\\n",
        "B_{21} & B_{22} & \\ldots & B_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "B_{n 1} & B_{n 2} & \\ldots & B_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "Summarize the backward transformation as well:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Now both together forward and backward transformation:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "* The index that we do the summation over (here: k = 1 and j = 1 under the summation sign) is the first letter of the forward or backward transform and **corresponds to the index of the basis vector that we‚Äôre transforming**.\n",
        "\n",
        "* The second index of the transform (j and i at F, B and e) **corresponds to the output basis vector**\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in n dimensions?**\n",
        "\n",
        "How are they related to each other? (once again proving)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Replacing with following at the last term above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\stackrel{\\sim}{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "And you get:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging summation signs:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k}\\left(\\sum_{j} F_{k j} B_{j i}\\right) \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "So now we build the old basis vectors $\\overrightarrow{e_{i}}$ using the summation of the old basis vectors $\\overrightarrow{e_{k}}$\n",
        "\n",
        "Now the following middle part should be an identity matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ = $I$\n",
        "\n",
        "so that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Which means then that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=\\overrightarrow{e_{1}} \\\\\n",
        "\\overrightarrow{e_{2}} &=\\overrightarrow{e_{2}} \\\\\n",
        "& \\vdots \\\\\n",
        "\\overrightarrow{e_{n}} &=\\overrightarrow{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "In order to achieve that we want for this part the following conditions:\n",
        "\n",
        "for this $\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ we want:\n",
        "\n",
        "* it is 1 when i = k\n",
        "\n",
        "* it is 0 when i ‚â† k\n",
        "\n",
        "because that will give us the identity matrix (because 1 is where row is equal to the columns, so i = k, and all else 0):\n",
        "\n",
        "$\\begin{equation}\n",
        "I_{n}=\\left[\\begin{array}{ccccc}\n",
        "1 & 0 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 0 & 1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\cdots & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "So for this operation we have the Kronecker Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\sum_{j} F_{k j} B_{j i}=\\delta_{i k}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=k \\\\\n",
        "0 \\text { if } i \\neq k\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n"
      ],
      "metadata": {
        "id": "7HbfIgCM-RXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Transformation Rules (Contravariant)**\n",
        "\n",
        "* **How can you transform from one basis to the other using the vector components (and not the basis vectors)?**\n",
        "\n",
        "* Now one could apply the forward matrix to the vector components of the old basis $\\overrightarrow{e_{i}}$ and should get the vector components of the new basis $\\widetilde{e_{i}}$ with\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{\\overrightarrow{e_{i}} (old vector components)} \\quad\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]_{\\widetilde{e_{i}} (new  vector components)}\n",
        "\\end{equation}$\n",
        "\n",
        "* We do Forward Transform in Basis Vectors and Backward Transform in Vector Components to get from old to new vector basis!\n",
        "\n",
        "* Achtung: In this case (=for vector components) you need to apply the backward transformation to get from the old components to the new components!.\n",
        "**The reason is: for basis vectors, forward brings us from old to new, and backward from new to old.** But with vector components it‚Äôs the opposite.\n",
        "This will be important for understanding covariance & contravariance!\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "  * So, we take the vector components of the old basis: $\\begin{equation}\n",
        "B\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "  * Then we multiply it with our backward transformation matrix: $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "0.25 & 0.5 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "  * Making a few transformations: $\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "{\\left[\\begin{array}{l}\n",
        "0.25(1)+0.5(1.5) \\\\\n",
        "(-1)(1)+2(1.5)\n",
        "\\end{array}\\right]} \\\\\n",
        "{\\left[\\begin{array}{c}\n",
        "0.25+0.75 \\\\\n",
        "-1+3\n",
        "\\end{array}\\right]}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "  * And the result are the correct vector components of the new basis: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> **Be aware of difference between changes with basis vectors (forward from old to new, backward from new to old) and vector components (forward from new to old, backward from old to new):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_10.png)\n",
        "\n",
        "**Why does this make perfect sense?**\n",
        "\n",
        "* Look at this example: the vector components on the left (original) are [1, 1],\n",
        "\n",
        "* and the size of the new basis vectors $\\widetilde{e_{1}}$ is just double of the old one: $\\begin{equation}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}\n",
        "\\end{equation}$ as well as $\\begin{equation}\n",
        "\\widetilde{e_{2}}=2 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$.\n",
        "\n",
        "* This corresponds to a forward transformation matrix for the basis vector $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ to get the new basis vector\n",
        "\n",
        "* the vector components of the vector $\\vec{v}$ are then in the new coordinate system half: [0.5, 0.5], which makes sense.\n",
        "\n",
        "* **This is because (as said in the beginning) the <u>length and the direction of a vector should never change</u> (it's invariant!), but the vector component can change and in this case they change opposite to the change of the basis vectors to keep the vectors stable as they were in both bcoordinate systems!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_11.png)\n",
        "\n",
        "> **Learning: Vector components behave the opposite way that basis vectors do !**\n",
        "\n",
        "* Similar, **if you rotate the basis vector clockwise, the vector components of the vector $\\vec{v}$ rotate anti-clockwise**.\n",
        "\n",
        "* But remember: $\\vec{v}$ has not moved, just its components changed opposite to the basis vector components!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_12.png)"
      ],
      "metadata": {
        "id": "hPW8cblf-TWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So we have two ways of writing a vector $\\vec{v}$:**\n",
        "\n",
        "  * **a linear combination of the old basis vectors with the coefficient being the old components,**\n",
        "\n",
        "  * **or we can write it as a linear combination of the new basis vectors with the coefficient being the new components**.\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=v_{1} \\overrightarrow{e_{1}}+v_{2} \\overrightarrow{e_{2}}+\\cdots+v_{n} \\overrightarrow{e_{n}}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "as well as:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\widetilde{v_{1}} \\widetilde{{e_{1}}}+\\widetilde{v_{2}} \\widetilde{e_{2}}+\\cdots+\\widetilde{v_{n}} \\widetilde{e_{n}}=\\sum_{j=1}^{n} \\widetilde{v_{j}} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "So both if these summations are equal to $\\vec{v}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "Let‚Äôs bring out the forward and backward transformations at this point.\n",
        "\n",
        "* Forward: $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{i=1}^{n} F_{i j} \\overrightarrow{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward: $\\begin{equation}\n",
        "\\overrightarrow{e_{j}}=\\sum_{i=1}^{n} B_{i j} \\stackrel{\\sim}{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "If we take the old basis and the old components, what we can do is we can use the backward transformation B to replace the old basis vectors\n",
        "\n",
        "> $\\overrightarrow{e_{j}}$ with the new basis vectors from B: $\\begin{equation}\n",
        "=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{j=1}^{n} v_{j}\\left(\\sum_{i=1}^{n} B_{i j} \\widetilde{{e}_{i}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging the summation we get this:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right) \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Now we have $\\vec{v}$ written as a summation of the new basis vectors**.\n",
        "\n",
        "Nut now, let‚Äôs have a look at the middle part of the equation above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "If we compare that to our summation in the step before when we wrote $\\vec{v}$ as a summation of new basis vectors:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "the coefficients have to be the new components $\\widetilde{v}_{i}$. And indeed $\\widetilde{v}_{i}$ equals:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{v}_{i}=\\sum_{j=1}^{n} B_{i j} v_{j}\n",
        "\\end{equation}$\n",
        "\n",
        "we actually used the backward transformation! So, **this is the proof that to move from the old components to the new components we actually used the backwards transformation!**\n",
        "\n",
        "**Summary**: We know how basis vectors transform and how vector components transform (opposite direction).\n",
        "\n",
        "* Now because vector components behave contrary to the basis vectors we say that vector components are contra variant!\n",
        "\n",
        "* And we see later that vectors are contra variant tensors!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_13.png)\n",
        "\n",
        "**This means also we make a small but important change:**\n",
        "\n",
        "Here you can see we wrote the vector $\\vec{v}$ as a linear combination of the old and the new basis:\n",
        "\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v_{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "But since the vector components $v_{i}$ and $\\widetilde{v}_{i}$ behave contra variant we put the index up $v^{i}$ and $\\widetilde{v}^{i}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v^{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}^{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "t8PMvuxn-Vdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Linear Forms): Basiswechsel und Covector-(Coordinate)-Transformation (Covariant)*"
      ],
      "metadata": {
        "id": "Eg2UlbUr-YCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Rules:**\n",
        "\n",
        "* Vector Space $V$ to get from old to new: **Forward Transform in Basis Vectors (covariant) and Backward Transform in Vector Components (contravariant)**\n",
        "\n",
        "* Vector Space $V*$ to get from old to new: **Backward Transform in Basis Vectors (contravariant) and Forward Transform in Vector Components (covariant)**\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "\\overrightarrow{e_{i}} \\text{Old Basis V with Vectors} \\quad \\vec{v} &  \\widetilde{e_{i}} \\text{New Basis V with Vectors} \\quad \\widetilde{v}\\\\\n",
        "\\overrightarrow{\\epsilon_{i}} \\text{Old Dual Basis V* with Covectors} \\quad \\vec{\\alpha} & \\widetilde{\\epsilon_{i}} \\text{New Dual Basis V* with Covectors} \\quad \\widetilde{\\alpha}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "s9ikXvt0-iN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* covectors / linear forms, like scalars (for example differentials), change with basis (=covariant)\n",
        "\n",
        "* change of coordinate system does not change scalar (like temperature), hence linear forms =scalars) bechave covariant with basis (meanwhile direction in vectors change contravariant to keep the original position of a vector)\n",
        "\n",
        "* **All covectors can be written as the linear combination of the dual basis vectors!**\n",
        "\n",
        "* **Covector components can be obtained by counting how many covector lines that the basis vector pierces**\n",
        "\n",
        "* **Covector components transform in the opposite way that vector components do**\n",
        "\n",
        "* Videos:\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 5: Covector Components ](https://www.youtube.com/watch?v=rG2q77qunSw&t=10s)\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 6: Covector Transformation Rules](https://www.youtube.com/watch?v=d5da-mcVJ20&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=9)\n",
        "\n",
        "\n",
        "**Fun facts**\n",
        "\n",
        "* Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!** We need Epsilon $\\epsilon^{n}$ as dual basis.\n",
        "\n",
        "* **Basis Vector in $V$ - Upper-left**: we started with the transformation rules for basis vectors = covariant: from old to new with the forward transform.\n",
        "\n",
        "* **Vector Components in $V$ - Bottom-left**: The transformation rules for vector components are the opposite compared to the basis vectors (from old to new with the backward transform). So they are contra variant.\n",
        "\n",
        "* **Basis Covector in $V*$ - Upper-right (Dual Space)**: Transformation rules for basis covectors are also opposite compared to basis vectors. So basis covectors also transform by the contra variant rule.\n",
        "\n",
        "\n",
        "* **Covector Components in $V*$ - Bottom-right (Dual Space)**: Covector components transform in the same way that basis vectors do = This means that covectors transform covariantly. (and contravariant to vectors components)\n",
        "\n",
        "> **Wie man sieht: man konstruiert (neue) basisvektoren aus (alten) basisvektoren und (neue) vektorkomponenten aus (alten) vektorkomponenten**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_18.png)"
      ],
      "metadata": {
        "id": "WATiQxEx-kZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does it mean when we say the covector has components?**\n",
        "\n",
        "* Look at this example: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}} \\text { = } 2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$\n",
        "\n",
        "* So when we write a **column** vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* What we mean is: This vector is given by the **linear combination of** $2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}$\n",
        "\n",
        "> **It tells you how much of each basis vector is needed to make the vector**\n",
        "\n",
        "**So what exactly are we measuring with when we write [2 1] ?** - **2 of what and 1 of what?**\n",
        "\n",
        "* Remember: Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!**\n",
        "\n",
        "\n",
        "**Epsilon $\\epsilon^{n}$ as dual basis**\n",
        "\n",
        "Take the basis $\\begin{equation}\n",
        "\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\text { for } V \\text { . }\n",
        "\\end{equation}$\n",
        "\n",
        "We introduce two special covectors: $\\epsilon^{1},\\epsilon^{2}: V \\rightarrow \\mathbb{R}$, which are both functions from vectors to numbers (notice how the labels 1 and 2 are now above instead of below).\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0 \\\\\n",
        "\\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)=0 & \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=1\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "(Hint: kovariant basis from $V$ with kontravariant basis from $V*$ equals the identity)\n",
        "\n",
        "this means we can use the Kronecker-Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{i j}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=j \\\\\n",
        "0 \\text { if } i \\neq j\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n",
        "\n",
        "**So what does this epsilon $\\epsilon$ mean?**\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{1}(\\vec{v})=\\epsilon^{1}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "We can rewrite this like this (this addition and scaling can be brought outside the function since it's a linear combination):\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} \\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "From the previous part we know that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "So we can rewrite the equation above as:\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} * 1 +v^{2} * 0\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{1} = \\epsilon^{1}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "And the same goes for $\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})=\\epsilon^{2}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)=v^{1} \\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=v^{2}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{2} = \\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "> The epsilons are projecting out vector components\n",
        "\n",
        "*Directions of epsilon basis vectors*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_15.png)\n",
        "\n",
        "See image below: Write a general covector alpha $\\alpha$ (which can be any covector of our choice) as a linear combination of the epsilon covectors.\n",
        "\n",
        "**This means that epsilon covectors form a basis for the set of all covectors!**\n",
        "\n",
        "> **And for that reason we call these epsilons the dual basis, because they are basis for the dual space $V*$.**\n",
        "\n",
        "* **We can't just flip column vectors on their side to get the row vectors! This works only in an orthonormal basis**.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_16.png)"
      ],
      "metadata": {
        "id": "1Fkz_0Ra-mfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Die Koeffizienten der Linearformen verhalten sich kovariant**\n",
        "\n",
        "* **Problem**: How do we transform some scalar $\\Phi$ ? The answer is \"**derivatives of a scalar**\":\n",
        "\n",
        "> $\\frac{\\partial \\Phi}{\\partial x^{i}}=\\frac{\\partial \\Phi}{\\partial x^{\\prime k}} \\frac{\\partial x^{\\prime k}}{\\partial x^{i}}$\n",
        "\n",
        "* **Darstellung**: $x_{1}$ (lower index, tiefgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Koeffizienten der Linearformen (=zB Skalare) verhalten sich kovariant. (wahrscheinlich: kontravariante Basisvektoren (= \"kovarianten Vektor\", weil Koordinaten kovariant): Normal zu den Koordinatenfl√§chen)\n",
        "\n",
        "  * [Skalar](https://de.m.wikipedia.org/wiki/Skalar_(Mathematik)): In der Physik werden Skalare verwendet zur Beschreibung physikalischer Gr√∂√üen, die **richtungsunabh√§ngig** sind. Beispiele f√ºr skalare physikalische Gr√∂√üen sind die **Masse eines K√∂rpers, seine Temperatur, seine Energie und auch seine Entfernung von einem anderen K√∂rper** (als Betrag der Differenz der Ortsvektoren). Anders gesagt: Eine skalare physikalische Gr√∂√üe √§ndert sich bei √Ñnderungen der Lage oder Orientierung nicht. Wird hingegen f√ºr die vollst√§ndige Beschreibung der Gr√∂√üe eine Richtung ben√∂tigt, wie bei der Kraft oder der Geschwindigkeit, so wird ein Vektor verwendet, bei Abh√§ngigkeit von mehreren Richtungen ein Tensor (genauer: Tensor 2. oder noch h√∂herer Stufe).\n",
        "\n",
        "  * Every quantity which under a coordinate transformation, transforms like the derivatives of a scalar is called a covariant tensor.\n",
        "\n",
        "  * Covectors (also called **dual vectors**) typically have units of the inverse of distance or the inverse of distance with other units. The components of covectors **change in the same way as changes to scale of the reference axes and consequently are called covariant**.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * Koeffizienten der [Linearformen](https://de.wikipedia.org/wiki/Linearform) (=zB Skalare)\n",
        "\n",
        "  * An example of a covector is the gradient, which has units of a spatial derivative, or distance<sup>‚àí1</sup>.\n",
        "\n",
        "  * Examples of covariant vectors generally appear when taking a gradient of a function.\n",
        "\n",
        "* **Beispiele fur Kovarianz [in der Physik](https://de.wikipedia.org/wiki/Kovarianz_(Physik))**\n",
        "\n",
        "  * Unter Galilei-Transformationen transformieren sich die Beschleunigung und die Kraft in den newtonschen Bewegungsgleichungen im gleichen Sinne wie die Ortsvektoren. Daher sind die Newtonschen Bewegungsgleichungen und damit die klassische Mechanik kovariant bzgl. der Gruppe der Galilei-Transformationen.\n",
        "\n",
        "  * Im gleichen Sinne sind die Einstein-Gleichungen der Gravitation in der allgemeinen Relativit√§tstheorie kovariant unter beliebigen (nichtlinearen glatten) Koordinatentransformationen.\n",
        "\n",
        "  * Ebenso ist die Dirac-Gleichung der Quantenelektrodynamik kovariant unter der Gruppe der linearen Lorentz-Transformationen.\n",
        "\n",
        "  * Die linke Seite der Klein-Gordon-Gleichung f√ºr ein Skalarfeld √§ndert sich unter Lorentz-Transformationen nicht, sie ist spezieller invariant oder skalar.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * A covariant vector or cotangent vector (often abbreviated as covector) has components that co-vary with a change of basis.\n",
        "\n",
        "  * That is, the **components must be transformed by the same matrix as the change of basis matrix**. The components of covectors (as opposed to those of vectors) are said to be covariant.  In Einstein notation, covariant components are denoted with lower indices as in $\\mathbf{e}_{i}(\\mathbf{v})=v_{i}$\n",
        "\n",
        "  * A covariant vector has **components (=coordinates) that change oppositely to the coordinates or, equivalently,\n",
        "transform like the reference axes**.\n",
        "\n",
        "  * For instance, the components of the gradient vector of a function $\\nabla f=\\frac{\\partial f}{\\partial x^{1}} \\widehat{x}^{1}+\\frac{\\partial f}{\\partial x^{2}} \\widehat{x}^{2}+\\frac{\\partial f}{\\partial x^{3}} \\widehat{x}^{3}$\n",
        "transform like the reference axes themselves.\n",
        "\n",
        "  * See also [Covariant Transformation](https://en.wikipedia.org/wiki/Covariant_transformation)\n",
        "\n",
        "\n",
        "Die Komponenten eines Kovektors, eines Vektors aus dem Dualraum, transformieren kovariant.\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_05.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "amwdLqv5-oX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The derivative of a function transforms covariantly!** (and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates)\n",
        "\n",
        "* In physics, a [covariant transformation](https://en.wikipedia.org/wiki/Covariant_transformation) is a rule that specifies how certain entities, such as vectors or tensors, change under a change of basis.\n",
        "\n",
        "* The transformation that describes the new basis vectors as a linear combination of the old basis vectors is defined as a covariant transformation.\n",
        "\n",
        "* Conventionally, indices identifying the basis vectors are placed as lower indices and so are all entities that transform in the same way.\n",
        "\n",
        "*Covariant Derivative*\n",
        "\n",
        "* the [covariant derivative](https://en.wikipedia.org/wiki/Covariant_derivative) is a way of specifying **a derivative along tangent vectors of a manifold**.\n",
        "\n",
        "* The name is motivated by the importance of changes of coordinate in physics: the covariant derivative transforms covariantly under a general coordinate transformation, that is, linearly via the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the transformation.\n",
        "\n",
        "* The covariant derivative is a generalization of the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative) / [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) from vector calculus (ps: Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale R√§ume ist das [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential).)\n",
        "\n",
        "* The covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule,\n",
        "\n",
        "  * $\\nabla_{\\mathbf{u}} \\mathbf{v},$ which takes as its inputs:\n",
        "\n",
        "  * (1) a vector, $\\mathbf{u},$ defined at a point $P$, and\n",
        "\n",
        "  * (2) a vector field, $\\mathbf{v}$, defined in a neighborhood of $P$\n",
        "\n",
        "  * The output is the vector $\\nabla_{\\mathbf{u}} \\mathbf{v}(P)$, also at the point $P$.\n",
        "\n",
        "*Covariant derivative and covariant transformation*\n",
        "\n",
        "* The primary difference from the usual directional derivative is that $\\nabla_{\\mathrm{u}} \\mathrm{v}$ must, in a certain precise sense, **be independent of the manner in which it is expressed in a coordinate system**.\n",
        "\n",
        "* A vector may be described as a list of numbers in terms of a basis, **but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis**.\n",
        "\n",
        "* This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation.\n",
        "\n",
        "> **The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: <u>the covariant derivative must change by a covariant transformation</u>.**\n",
        "\n",
        "*In the Euclidean Space*\n",
        "\n",
        "* In the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points. In such a system one translates one of the vectors to the origin of the other, keeping it parallel.\n",
        "\n",
        "* **With a Cartesian (fixed orthonormal) coordinate system \"keeping it parallel\" amounts to keeping the components constant**.\n",
        "\n",
        "* Euclidean space provides the simplest example: a covariant derivative which is obtained by taking the ordinary directional derivative of the components in the direction of the displacement vector between the two nearby points.\n",
        "\n",
        "*In other (more general) Spaces*\n",
        "\n",
        "* In the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in **polar coordinates** in a two dimensional Euclidean plane, **then it contains extra terms that describe how the coordinate grid itself \"rotates\"**.\n",
        "\n",
        "* **In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves**, etc.\n",
        "\n",
        "* **In this case \"keeping it parallel\" does NOT amount to keeping components constant under translation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "u2amkyDK-qT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/covariant_transformation.png)\n",
        "\n",
        "* In the shown example, a vector $\\mathbf{v}=\\sum_{i \\in\\{x, y\\}} v^{i} \\mathbf{e}_{i}=\\sum_{j \\in\\{r, \\phi\\}} v^{\\prime j} \\mathbf{e}_{j}^{\\prime} .$ is described by two different coordinate\n",
        "systems:\n",
        "  * a rectangular coordinate\n",
        "system (the black grid),\n",
        "  * and a radial\n",
        "coordinate system (the red grid).\n",
        "\n",
        "* Basis vectors have been chosen for both\n",
        "coordinate systems: $\\mathbf{e}_{x}$ and $\\mathbf{e}_{\\mathbf{y}}$ for the rectangular coordinate system, and $\\mathbf{e}_{\\mathrm{r}}$\n",
        "and $\\mathbf{e}_{\\phi}$ for the radial coordinate system.\n",
        "\n",
        "* The radial basis vectors $\\mathbf{e}_{\\mathrm{r}}$ and $\\mathbf{e}_{\\phi}$ appear\n",
        "rotated anticlockwise with respect to the\n",
        "rectangular basis vectors $\\mathbf{e}_{\\mathrm{x}}$ and $\\mathbf{e}_{\\mathrm{y}}$.\n",
        "\n",
        "> **The covariant transformation, performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors**.\n",
        "\n",
        "* The coordinates of $v$ must be transformed into the new coordinate system, **but the vector $v$ itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates**.\n",
        "\n",
        "* The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view $v$ from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors $\\mathbf{e}_{\\mathbf{r}}$ and $\\mathbf{e}_{\\Phi}$. compared to how it appeared relative to the rectangular basis vectors $\\mathbf{e}_{x}$ and $\\mathbf{e}_{y}$.\n",
        "\n",
        "* **Thus, the needed contravariant transformation to $v$ in this example is a clockwise rotation.**\n",
        "\n",
        "Example\n",
        "\n",
        "* Consider the example of moving along a curve $\\gamma(t)$ in the Euclidean plane. In polar coordinates, $\\gamma$ may be written in terms of its radial and angular coordinates by $\\gamma(t)=(r(t), \\theta(t))$.\n",
        "\n",
        "* A vector at a particular time $t$ (for instance, the acceleration of the curve) is expressed in terms of $\\left(\\mathbf{e}_{r}, \\mathbf{e}_{\\theta}\\right),$ where $\\mathbf{e}_{r}$ and $\\mathbf{e}_{\\theta}$ are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and [tangential components](https://en.wikipedia.org/wiki/Tangential_and_normal_components).\n",
        "\n",
        "* At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the [Christoffel symbols](https://en.wikipedia.org/wiki/Christoffel_symbols)) serve to express this change.\n",
        "\n",
        "*Another Example*\n",
        "\n",
        "* In a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.\n",
        "\n",
        "* A vector e on a globe on the equator at point Q is directed to the north. Suppose we parallel transport the vector first along the equator until at point P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q.\n",
        "\n",
        "* **Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector**; instead, it has another orientation. **This would not happen in Euclidean space and is caused by the curvature of the surface of the globe**. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature."
      ],
      "metadata": {
        "id": "mpNqLFw7-sbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Bilinear Forms) $\\mathcal{B}_{i i} \\epsilon^{i} \\epsilon^{j} \\rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$*"
      ],
      "metadata": {
        "id": "GCJMH1Oj-vBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor product $\\otimes$ for bilinear maps: Bilinear forms are linear combinations of covector-covector-pairs** (including the metric tensor)\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$\n",
        "\n",
        "* Generalization: Bilinear Forms as Covector-Covector-Pairs\n",
        "\n",
        "* Why not vector-vector-pairs or so? - Bilinear forms take two vector inputs and since covectors take one vector each, a pair of covectors would take two vector inputs.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 12: Bilinear Forms are Covector-Covector pairs](https://www.youtube.com/watch?v=uDRzJIaN2qw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=15)\n",
        "\n",
        "**Advantages of the tensor product for bilinear maps:**\n",
        "\n",
        "* We can write bilinear forms as linear combinations of covector covector pairs and this:\n",
        "\n",
        "  * immediately gives us the transformation rules $\\begin{aligned} \\widetilde{\\epsilon}^{i} &=B_{j}^{i} \\epsilon^{j} & \\widetilde{\\mathcal{B}_{i j}} &=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l} \\\\ \\epsilon^{i} &=F_{j}^{i} \\widetilde{\\epsilon}^{j} & \\mathcal{B}_{i j} &=B_{i}^{k} B_{j}^{l} \\widetilde{B_{k l}} \\end{aligned}$\n",
        "\n",
        "  * the component multiplication formula: $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\Rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$\n",
        "\n",
        "  * and the correct array shape\n",
        "\n",
        "* **Combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors (and not a row and a column vwector like for the Kronecker delta) associated with the covectors**\n",
        "\n",
        "* Meanwhile the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_43.png)\n"
      ],
      "metadata": {
        "id": "svrOb-ni_IT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "ZdLtfLao_KKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 1: write out any bilinear form as a linear combination of covector-covector pairs**\n",
        "\n",
        "*Look at our classic transformation rules*:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{i j}=B_{i}^{k} B_{j}^{l} \\widetilde{\\mathcal{B}_{k l}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{e_{j}}=F_{j}^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "> $\\overrightarrow{e_{j}}=B_{j}^{i} \\widetilde{e_{i}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{\\epsilon}^{i}=B_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\epsilon^{i}=F_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "If we can assume we can write out any bilinear form as a linear combination of covector-covector pairs **to get the transformation rule for these components** we just transform the basis covectors individually:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l} \\epsilon^{k} \\epsilon^{l}$\n",
        "\n",
        "Basis covectors are contra variant, so to build old from the new we use the forward transform $F$\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l}\\left(F_{i}^{k} \\widetilde{\\epsilon}^{i}\\right)\\left(F_{j}^{l} \\widetilde{\\epsilon}^{j}\\right)$\n",
        "\n",
        "and putting these in front gives us this:\n",
        "\n",
        "> $\\mathcal{B}=\\left(F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}\\right) \\widetilde{\\epsilon}^{i}\\widetilde{\\epsilon}^{j}$\n",
        "\n",
        "which as we can see is the correct one:\n",
        "\n",
        "> $\\widetilde{B_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$"
      ],
      "metadata": {
        "id": "-1G-vCul_L_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 2: We can also get the correct component multiplication formula when a bilinear form acts on two vector inputs:**\n",
        "\n",
        "Given:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "> $\\vec{w}=w^{l} \\overrightarrow{e_{l}}$\n",
        "\n",
        "**Proof:** replace components with equations above:\n",
        "\n",
        "> $s=\\mathcal{B}(\\vec{v}, \\vec{w})$\n",
        "\n",
        "Replace the bilinear form and the vectors with their linear combination expansions in some basis\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}, w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Now we pass each of these vector inputs to their corresponding covectors\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i}\\left(v^{k} \\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "\n",
        "$v$ and $w$ come out in front:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\epsilon^{i}\\left(\\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(\\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Replace ($\\overrightarrow{e_{k}}$) and ($\\overrightarrow{e_{l}}$) with Kronecker deltas\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\delta_{k}^{i} \\delta_{l}^{j}$\n",
        "\n",
        "Finally using the index cancellation rules for $k$ and $l$ we get the correct component multiplication formula ends up giving us a single number as a result:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{i} w^{j}$\n"
      ],
      "metadata": {
        "id": "GaERJjMl_Nua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 3**\n",
        "\n",
        "* Remember with linear maps, the new way of writing with the tensor product makes a lot more tensors (with a row of rows): Even though we had two vector inputs we needed to write one as a column and one as a row  flipped on its side to make the multiplication work correctly. Which is awkward, because vectors should always be written as columns and not as rows!\n",
        "\n",
        "* When we write the bilinear forms as a row of rows the matrix multiplication formula makes a lot more sense. We can write out both vectors as columns\n",
        "\n",
        "* **Above is the old way (row vector & column vector both to describe vectors, and the transition matrix in the middle) and on the bottom is the new better way with tensor products $\\otimes$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_42.png)"
      ],
      "metadata": {
        "id": "94JzCC8X_QsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Metric Tensor) $g(\\vec{\\color{blue}v}, \\vec{\\color{blue}w}) \\mapsto \\color{blue}{v^{i}} \\color{blue}{w^{j}} g_{i j}$ for Index Manipulation (contravariant $Z^{ij}$ and covariant $Z_{ij}$)*"
      ],
      "metadata": {
        "id": "YgcZdTDc_TCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor**\n",
        "\n",
        "* In differential geometry, one definition of a [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) is a type of function which takes as input a pair of tangent vectors $v$ and $w$ at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar $g(v, w)$ in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space.\n",
        "\n",
        "* Define length of and angle between tangent vectors when basis changes (same way like a dot product). Use Cases: How to measure the length of a vector when we change the vector (with a linear map) as well as when the basis changes?\n",
        "\n",
        "* Through integration, the metric tensor allows one to define and **compute the length of curves on the manifold** [Source](http://walter.bislins.ch/physik/index.asp?page=Metrik%2DTensor)\n",
        "\n",
        "**Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$. In a given basis we compute the output of a metric tensor using this formula**:\n",
        "\n",
        "> $g(\\vec{\\color{red}v}, \\vec{\\color{blue}w}) \\mapsto \\color{red}{v^{i}} \\color{blue}{w^{j}} g_{i j}$\n",
        "\n",
        "And this is the formula for computing the output of a metric tensor when it acts on two vectors\n",
        "\n",
        "> $\\left[\\begin{array}{ll}\\color{red}{v^{1}} & \\color{red}{v^{2}}\\end{array}\\right]\\left[\\begin{array}{ll}g_{11} & g_{12} \\\\ g_{21} & g_{22}\\end{array}\\right]\\left[\\begin{array}{l}\\color{blue}{w^{1}} \\\\ \\color{blue}{w^{2}}\\end{array}\\right]$\n",
        "\n",
        "**The algebraic properties of metric tensors are**:\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 9: The Metric Tensor](https://www.youtube.com/watch?v=C76lWSOTqnc&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=12)"
      ],
      "metadata": {
        "id": "_mR6eUfX_WoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)"
      ],
      "metadata": {
        "id": "L2D_3hgX_oLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). **Metric tensors are (0,2) tensors, because it transforms using tow covariant rules**:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "KPlaTJsa_qJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensors for measuring length or angles**\n",
        "\n",
        "* Is a tensor whose components in a given vector basis are given by the **dot products of the basis vectors**:\n",
        "\n",
        "  * <font color=\"blue\">$g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}$</font>\n",
        "\n",
        "* Since the dot products don't care about the order of the input $g_{i j}=g_{j i}$, which means the **tensor is symmetric** along the diagonal line (spur).\n",
        "\n",
        "  * $g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}=\\overrightarrow{e_{j}} \\cdot \\overrightarrow{e_{i}}=g_{j i}$\n",
        "\n",
        "* when we want to get an **angle**, we put both vectors in:\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{v}=\\|\\vec{v}\\|^{2}=$ <font color=\"blue\">$v^{i} v^{j} g_{i j}$</font>\n",
        "\n",
        "  * $\\vec{w} \\cdot \\vec{w}=\\|\\vec{w}\\|^{2}=$ <font color=\"blue\">$w^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* when we want to get a **lengths** we out the same vector twice in\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{w}=\\|\\vec{v}\\|\\|\\vec{w}\\| \\cos \\theta=$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* **In a given basis the Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$**:\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}) \\mapsto$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5NQ7BCXo_r5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric tensor $g$ and inverse metric tensor $\\mathfrak{g}$ for lowering & raising indexes (Contraction)**\n",
        "\n",
        "* In mathematics and mathematical physics, [raising and lowering indices](https://en.m.wikipedia.org/wiki/Raising_and_lowering_indices) are operations on tensors which change their type. Raising and lowering indices are a form of [index manipulation (Ricci calculus)](https://en.m.wikipedia.org/wiki/Ricci_calculus) in tensor expressions.\n",
        "\n",
        "* The metric tensor represents a matrix with scalar elements $\\left(g_{i j}\\right.$ or $\\mathfrak{g}^{i j}$ ) and is a tensor object which is used to raise or lower the index on another tensor object by an operation called [Tensor Contraction](https://en.m.wikipedia.org/wiki/Tensor_contraction), thus **allowing a covariant tensor to be converted to a contravariant tensor, and vice versa**.\n",
        "\n",
        "  * In multilinear algebra, a [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) is an operation on a tensor that arises from the natural pairing of a finite-dimensional vector space and its dual.\n",
        "\n",
        "  * In components, it is expressed as a sum of products of scalar components of the tensor(s) caused by applying the summation convention to a pair of dummy indices that are bound to each other in an expression.\n",
        "\n",
        "  * The contraction of a single mixed tensor occurs when a pair of literal indices (one a subscript, the other a superscript) of the tensor are set equal to each other and summed over. In the Einstein notation this summation is built into the notation. **The result is another tensor with order reduced by 2.**\n",
        "\n",
        "  * Es ist eine Verallgemeinerung der Spur einer linearen Abbildung auf Tensoren, die mindestens einfach kovariant und einfach kontravariant sind.\n",
        "\n",
        "* **For an orthonormal [Cartesian coordinate system](https://en.m.wikipedia.org/wiki/Cartesian_coordinate_system), the metric tensor is just the [kronecker delta](https://en.m.wikipedia.org/wiki/Kronecker_delta) $\\delta_{i j}$ or $\\delta^{i j},$ which is just a tensor equivalent of the identity matrix, and $\\delta_{i j}=\\delta^{i j}=\\delta_{j}^{i}$**. For measuring length of a vector: Pythagoras theorem is only valid in orthonormal bases!\n",
        "\n",
        "* Normally a metric tensor as a function from a pair of vectors to scalars $g: V x V \\rightarrow \\mathbb{R}$. But it can also be a function from a vector in $V$ to a covector in $V^{*}$ $g: V  \\rightarrow V^{*}$ using the metric tensor. Reverse direction: from a covector to its vector partner.\n",
        "\n",
        "* From the metric tensor we know its components have 2 lower indices, so it‚Äôs a member of $V^{*}$ tensor $V^{*}$ bzw. $g \\in V^{*} \\otimes V^{*}$. Now we introduce what's called the inverse metric tensor $\\mathfrak{g} \\in V \\otimes V$. The combination between both in a summation gives you the Kronecker delta: $\\mathfrak{g}^{k i} g_{i j}=\\delta_{j}^{k}$. This is how we define the inverse metric tensor.\n",
        "\n",
        "* The **ordinary metric tensor $g_{i j}$ is covariant** with $g_{i j}=\\vec{g}_{i} \\cdot \\vec{g}_{j}$. It **lowers indexes** and its components are covariant: $T_{i}=g_{i j} T^{j}$\n",
        "\n",
        "* The **inverse metric tensor $\\mathfrak{g}^{k i}$ is contravariant** with $\\mathfrak{g}^{i j}=\\vec{\\mathfrak{g}}^{i} \\cdot \\vec{\\mathfrak{g}}^{j}$. It **raises the indexes** and its components are contravariant (go in the other direction): $T^{i}=\\mathfrak{g}^{i j} T_{j}$.\n",
        "\n",
        "\n",
        "* Both metric tensors are related by the identity: $g_{i k} \\, \\mathfrak{g}^{j k}=\\delta_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_82.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "87h0yKnx_try"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sharp Operator $\\sharp$ and Flat Operator $\\flat$ to generalize raising and lowering indexes on the component of any tensor**\n",
        "\n",
        "* The ordinary metric tensor lowers indexes meanwhile the inverse metric tensor raises indexes.\n",
        "\n",
        "* But these raising and lowering operations don‚Äôt just apply to vector and covector components. We can also raise and lower the indexes on the components of other tensors\n",
        "\n",
        "* $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$ wird zu $\\rightarrow$ $\\flat \\vec{v}=v_{i} \\epsilon^{i}$. The flat operator (on the left side) lowers the indexes (on the right side) from $v^{i}$ to $v_{i}$. **Another way to think of it: the $\\flat$ operator is transforming a vector arrow into a covector stack** (like flattening the pointy arrow into a flat stack)\n",
        "\n",
        "* The covector alpha has components with downstairs indexes: $\\alpha=\\alpha_{i} \\epsilon^{i}$ and the vector alpha sharp has components with upstairs indexes $\\sharp \\alpha=\\alpha^{i} \\overrightarrow{e_{i}}$. The sharp operator is basically raising the index. Also the sharp operator is basically turning a flat covector stack into a sharp pointy arrow vector.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 16: Raising/Lowering Indexes (with motivation, sharp + flat operators)](https://www.youtube.com/watch?v=_z9R7OMpxhY)\n",
        "\n",
        "* See article [Sharp & Flat Operator - Isomorphismus](https://de.wikipedia.org/wiki/√Ñu√üere_Ableitung#Be-_und_Kreuz-_(Flat-_und_Sharp-)_Isomorphismus)\n",
        "\n",
        "* Take tensor $Q$ which is member of vector space $Q \\in V \\otimes V^{*} \\otimes V^{*}$ and has components $Q_{j k}^{I}$:  $Q=Q_{j k}^{i} \\vec{e}_{i} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "* If we multiply it but the inverse metric tensor and sum over $j$ like this $Q^{i}{ }_{j k} \\mathfrak{g}^{j x}$ we can raise the index upward: $=Q^{i x}{ }_{k}$ and we get this new tensor $Q^{\\prime}=Q_{k}^{i x} \\overrightarrow{e_{i} {e}_{x}} \\epsilon^{k}$\n",
        "\n",
        "* This is a member of the new vector space $Q^{\\prime} \\in V \\otimes \\color{red}{V} \\otimes V^{*}$ - we raised the middle index $Q \\in V \\otimes \\color{red}{V^{*}} \\otimes V^{*}$\n",
        "\n",
        "* All these vector spaces can be traveled between using the ordinary metric tensor to lower indexes (blue arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_84.png)\n",
        "\n",
        "* Or we can use the inverse metric tensor to raise indexes (red arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_85.png)"
      ],
      "metadata": {
        "id": "spzpC64M_vis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor as Special Bilinear Form**: Metric Tensor = \"First Fundamental Form\". The metric tensor is a special bilinear form. Generalisation via tensor product: Bilinear Forms as Covector-Covector-Pairs.\n",
        "\n",
        "* So what is the difference between metric tensor and bilinear form? The metric tensor is a bilinear form, but it's a very specific example of a bilinear form. The metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* **Videos**: https://youtu.be/Hf-BxbtCg_A and https://youtu.be/Dn0ZZRVuJcU"
      ],
      "metadata": {
        "id": "oyUSwoxm_xXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "g4uy9SPO_zMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector-Pairs (Linear Maps) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$ to map vector to vector within one basis*"
      ],
      "metadata": {
        "id": "V44oHA2a_1hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear maps**\n",
        "\n",
        "1. maps vectors to vectors $L: V \\mapsto W$, or $L: V \\mapsto V$\n",
        "\n",
        "2. Linearity (addition & scaling)\n",
        "\n",
        "* Both covectors and linear maps are functions. The only difference is that covectors output is a scalar, and linear maps output vectors.\n",
        "\n",
        "> **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* Linear maps are linear combinations of vector-covector-pairs $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ with the tensor product $\\overrightarrow{e_{i}} \\epsilon^{j}$ bzw. $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* **Vector changes, basis not: When we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**.\n",
        "\n",
        "  * While the output vector might be different than the input vector, we are still measuring the output vector using the same basis\n",
        "\n",
        "  * With forward and backward transform we modify the vector components when we change from one basis to another, or from one dual basis to another.\n",
        "\n",
        "  * With linear maps now we modify the vector components when we move a vector around within a given basis!\n",
        "\n",
        "* **Coordinate representations of linear maps end up being matrices!**. Column vectors are coordinate representation of vectors. Row vectors are coordinate representation of covectors.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 7: Linear Maps](https://www.youtube.com/watch?v=dtvM-CzNe50&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=10)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 11: Linear maps are Vector-Covector Pairs](https://www.youtube.com/watch?v=YK2zVcWpROA&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14)\n",
        "\n",
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). Metric tensors are (0,2) tensors, because it transforms using tow covariant rules:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "8erR4QCL_9Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_19.png)"
      ],
      "metadata": {
        "id": "kjVQbCBy__67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Case / Fun fact about how matrices transform vectors:**\n",
        "\n",
        "When you use the column vector (1,0) as an input vector, you get the first column of the matrix as the output (3, -1):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "-1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "When you use the column vector (0,1) as an input vector, you get the second column of the matrix as the output (-4, 2):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "-4 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "The column vectors 1, 0 and 0,1 are like copies of the basis vectors e1 and e2. Because: **Linear maps transform input vectors. But linear maps don't transform the basis!**\n",
        "\n",
        "**So when we transform vectors using a linear map, the basis isn‚Äôt changing, we aren‚Äôt moving the basis**. While the output vector might be different than the input vector, we are still measuring the output vector using the same basis."
      ],
      "metadata": {
        "id": "xEM1uOD7ACQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_20.png)"
      ],
      "metadata": {
        "id": "IFqMZilJAEFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maps - Case 1: Transformations of vectors within one basis (Linear Maps as Tensor Product $\\otimes$ (Vector-Covector-Pairs)**"
      ],
      "metadata": {
        "id": "pg7lwrp9AJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pure & Impure Matrices & how Vector-Covectors-Pairs help**\n",
        "\n",
        "* When we multiply a row vector and a column vector in this order we get a scalar: $\\begin{aligned} &\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right] \\end{aligned}$ $= (2)(3)+(1)(-4) = 6-4 =2$\n",
        "\n",
        "* But if we reverse the order we get a matrix (which is basically a linear map):  $\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right]\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]$ = $\\left[\\begin{array}{cc}6 & 3 \\\\ -8 & -4\\end{array}\\right]$\n",
        "\n",
        "* **Can we do this the other way around (get the row and column vector from the matrix)?** (Answer: not always so easy)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_36.png)\n",
        "\n",
        "* Here is the proof why it doesn't work for this matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_37.png)\n",
        "\n",
        "* There are some matrices that can be broken up into column vectors and row vectors, and others not. We call them pure and impure matrices!\n",
        "\n",
        "* Pure matrices are boring when they are used as linear maps, because all the output vectors exist along the same direction.\n",
        "\n",
        "* The reason behind is that the columns of the matrices are all scalable multiples of each other (as shown below). Output vectors point all to the same direction.\n",
        "\n",
        "* The set of transformations a pure matrix can do as linear map is really limited\n",
        "\n",
        "* Impure matrices as linear maps are more interesting because they can send basis vectors into different directions.\n",
        "\n",
        "* **Question: Since impure matrices are more interesting, how can we construct impure matrices using column vector, row vector and products**?\n",
        "\n",
        "* **Solution: We define four special vector-covector-pairs using the old e basis $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$  and the old epsilon dual basis $\\left\\{\\epsilon^{1}, \\epsilon^{2}\\right\\}$** (Hint: this will be the transition from 'classic' linear maps to vector-covector-tensor products with another way of writing them!)\n",
        "\n",
        "* For example the 1 on top left can be written using the column and row vector via $\\overrightarrow{e_{1}} \\epsilon_{1}$:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{1}$\n",
        "\n",
        "And we can do the same for all other 3 matrix components:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{1}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{2}$\n",
        "\n",
        "And you‚Äôll notice that these 4 matrices when taking in linear combination, so when we scale each matrix by a different amount and then add them together, we get any general 2x2 matrix $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$ that we like, just by picking the right scaling numbers a, b, c and d:\n",
        "\n",
        "> $a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "So this set of 4 products forms a basis for all matrices that are linear maps from the vector space $V \\mapsto V$ to itself\n",
        "\n",
        "> $\\left\\{\\overrightarrow{e_{1}} \\epsilon^{1}, \\overrightarrow{e_{1}} \\epsilon^{2}, \\overrightarrow{e_{2}} \\epsilon^{1}, \\overrightarrow{e_{2}} \\epsilon^{2}\\right\\}$ is a basis for $V \\rightarrow V$\n",
        "\n",
        "So any general linear map $L$ can be written as this linear combination if we pick the coefficients right:\n",
        "\n",
        "> $L=a \\overrightarrow{e_{1}} \\epsilon^{1}+b \\overrightarrow{e_{1}} \\epsilon^{2}+c \\overrightarrow{e_{2}} \\epsilon^{1}+d \\overrightarrow{e_{2}} \\epsilon^{2}$\n",
        "\n",
        "And we can summarize this using the Einstein notation, any linear map $L$ can be written using these components\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "> **This is a vector and a covector written together - which is a linear map**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_38.png)\n"
      ],
      "metadata": {
        "id": "adsf_EarAL1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Formula for matrix multiplication**\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]\\left[\\begin{array}{l} \\color{red}x \\\\ \\color{blue}y\\end{array}\\right]=\\left[\\begin{array}{l}a \\color{red}x+b \\color{blue}y \\\\ c \\color{red}x+d \\color{blue}y\\end{array}\\right]$\n",
        "\n",
        "This originates in the following abstract algebraic definition:\n",
        "\n",
        "$L: V \\rightarrow W$\n",
        "\n",
        "$L(\\vec{v}+\\vec{w})=L(\\vec{v})+L(\\vec{w})$\n",
        "\n",
        "$L(n \\vec{v})=n L(\\vec{v})$\n",
        "\n",
        "**How to turn the $\\vec{v}$ coefficients into the $\\vec{w}$ coefficients?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_21.png)\n",
        "\n",
        "\n",
        "**Proof: Derive the matrix multiplication formulas just from the abstract linearity properties of linear maps**\n",
        "\n",
        "If we have a linear map $L$ that transforms a vector v into another vector w (where w can be written as a linear combination of basis vectors)\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=w^{1} \\overrightarrow{e_{1}}+w^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "and we know how $L$ transforms basis vectors (via the L coefficients)\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{1}}\\right)=L_{1}^{1} \\overrightarrow{e_{1}}+L_{1}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{2}}\\right)=L_{2}^{1} \\overrightarrow{e_{1}}+L_{2}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "this means we can transform the v components into  the w components using the formulas below\n",
        "\n",
        "$w^{1}=L_{1}^{1} v^{1}+L_{2}^{1} v^{2}$\n",
        "\n",
        "$w^{2}=L_{1}^{2} v^{1}+L_{2}^{2} v^{2}$.\n",
        "\n",
        "And if we repeat this argument for any number of dimensions, so if we have a linear map $L$ in n-dimensions\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=\\sum_{i=1}^{n} w^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "We would get all the $L$ coefficients from this formula below\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{i}}\\right)=\\sum_{j=1}^{n} L_{i}^{j} \\overrightarrow{e_{j}}$\n",
        "\n",
        "we can transform the v components into the w components\n",
        "\n",
        "$w^{i}=\\sum_{j=1}^{n} L_{j}^{i} v^{j}$\n",
        "\n",
        "This is the standard matrix multiplication formula for multiplying matrices and vectors together.\n"
      ],
      "metadata": {
        "id": "whZ3s5JiAh4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit of seeing linear maps as vector-covector-pairs - which are tensor products $\\otimes$**\n",
        "\n",
        "*When we have a linear map acting on a vector, we can get the correct matrix vector component multiplication formula*\n",
        "\n",
        "If you think of some linear map $L$ as a linear combination of these basis linear maps\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "and we have also a vector $\\vec{v}$ which is a linear combination of these basis vectors\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "What would we get in $\\vec{w}$ with $L$ acts on the input $\\vec{v}$?\n",
        "\n",
        "> $\\vec{w}=L(\\vec{v})$\n",
        "\n",
        "To find out we substitute the components $L$ and $\\vec{v}$ accordingly:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "The $\\epsilon^{j}$ dual basis vector is now acting on the input vector! (and that's what covectors do, they act on vectors).\n",
        "\n",
        "So we use the linearity of $\\epsilon^{j}$ to take out the scaling coefficient $v^{k}$ and put it out in front:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "This leaves us with $\\epsilon^{j}$ acting on $\\left(\\overrightarrow{e_{k}}\\right)$. And by definition this is just a Kronecker delta:\n",
        "\n",
        ">  $\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$ = $\\delta_{k}^{j}$\n",
        "\n",
        "So we can replace this in the equation:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "And by the kronecker delta cancellation rule, we can cancel out the $k$'s and replace it at $v$ with $j$:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}} \\delta^{j}$\n",
        "\n",
        "And this is our output vector written as a linear combination of the $e$ basis vectors:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "And we get these coefficients:\n",
        "\n",
        "> $L_{j}^{i} v^{j}$\n",
        "\n",
        "from the standard matrix multiplication rule:\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\vec{w}=L(\\vec{v}) \\\\\n",
        "w^{i}=L_{j}^{i} v^{j}\n",
        "\\end{array}$\n",
        "\n",
        "So from our equation above:\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "**we can see that this product pair is really a linear map**:\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "It took an input vector, transformed it, and gave us an output vector.\n",
        "\n",
        "> **Proof: vector-covector-pairs are linear maps - which are tensor products $\\otimes$  !**\n",
        "\n",
        "* These are pure matrices, so you get the boring linear maps! To get the more interesting linear maps (the impure matrices!), we need to combine a bunch of pure linear maps together in linear combination, which helps us to get more interesting impure linear maps like $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] = a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]$\n",
        "\n",
        "* **And finally this vector-covector-pair is actually a tensor product**\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$ normally written like this: $\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_39.png)"
      ],
      "metadata": {
        "id": "bv6NmiXzAOBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two ways of multiplying a row vector with a column vector: classic way (left) and tensor product (right - much better)**\n",
        "\n",
        "\n",
        "* The $\\otimes$ operator tells us to take the array on the left and distribute it to each of the components inside the array on the right.\n",
        "\n",
        "* This means take the column vector on the left and distribute a copy to each element inside the second array. And you get a row of columns (bottom right), which is basically like a matrix.\n",
        "\n",
        "* **This means that linear maps are rows of columns!**\n",
        "\n",
        "* Picture below: On the left is the old way (row vector & column vector both to describe vectors) and on the right side is the new better way with tensor products $\\otimes$\n",
        "\n",
        "* the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "* (meanwhile combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors associated with the covectors)\n",
        "\n",
        "* **The tensor product for linear maps gave a great change of perspective**. This idea of distributing arrays into each other will turn out to be very useful later on!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_41.png)"
      ],
      "metadata": {
        "id": "bsHsugHiAQoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector Pairs (Linear Maps) + Forward and Backward Transforms (Vectors) $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$ to map vector to vector across two bases*"
      ],
      "metadata": {
        "id": "VcEin6DXApdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **We need transformation rules for linear maps when we are moving a vector AND going from one basis to another**, because linear maps are only working within one basis (move vectors around within one basis), we need to find the coefficients of the linear map in the new basis when we change basis.\n",
        "\n",
        "* Task: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "**How do we do this?**\n",
        "\n",
        "If you take vector components $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ from the vector $\\vec{v}$ in the original basis $\\overrightarrow{e_{i}}$\n",
        "\n",
        "1. **apply the linear map** $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$, you get the new vector components $L(\\vec{v})$ $=\\left[\\begin{array}{c}1 / 2 \\\\ 2\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ that is still **in the same original basis** $\\overrightarrow{e_{i}}$.\n",
        "\n",
        "2. then **apply the backward transform** with the matrix $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$, you get the new vector components for $\\vec{v}$ $\\left[\\begin{array}{l}3/4 \\\\ 1\\end{array}\\right]_{\\widetilde{e_{i}}}$ **in the new basis** ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "**We use the backward transform from old to new, since vector components behave contravariant !!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_22.png)"
      ],
      "metadata": {
        "id": "nrdE_XbCA5ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you get the new vector components $L(\\vec{v})$ in the new basis ${\\widetilde{e_{i}}}$?**\n",
        "\n",
        "* Answer: We need to find the coefficients of the linear map ${\\widetilde{L_{j}^{q}}}$ in the new basis ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "* Again: how do you get the new vector components $L(\\vec{v})$ (the transformed vector with the linear map $L$) in the new basis ${\\widetilde{e_{i}}}$, and not only the components of $\\vec{v}$ (the original vector) in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* Said differently: what are the components of the output vector in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* we cannot apply the linear map $L$ $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$ since it's only valid in the original basis\n",
        "\n",
        "* **We need to find a new matrix ${\\widetilde{L}}$ in the new basis ${\\widetilde{e_{i}}}$ that tells us how to build output vectors using the ${\\widetilde{e_{1}}}$ and ${\\widetilde{e_{2}}}$ basis vectors.**\n",
        "\n",
        "* This means we need to find the ${\\widetilde{L_{j}^{q}}}$ coefficients:\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{1}}}\\right)=\\widetilde{L_{1}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{1}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{2}}}\\right)=\\widetilde{L_{2}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{2}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "**How to get the components of the output vector in the new basis ${\\widetilde{e_{i}}}$:**\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\color{red}{\\widetilde{e_{i}}}\\right)$\n",
        "\n",
        "Let's first use the forward transform $\\widetilde{e_{i}}=\\sum_{j=1}^{n} F_{i}^{j} \\overrightarrow{e_{j}}$ to rewrite the new basis vectors in terms of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\sum_{j=1}^{n} F_{i}^{j} \\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use the linearity of $L$ to take the scale and sum coefficients outside the function:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use this definition $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)=\\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$ to write the output $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$ as linear combination of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} \\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Then we re-arrange the sums a bit:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Now we write the old basis in terms of the new basis vectors with $\\color{blue}{\\overrightarrow{e_{k}}}=\\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{\\overrightarrow{e_{l}}}}$ using the bckward transform:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "We re-arrange the sums again:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "* So on the left we have a linear combination of ${\\widetilde{e}}$ basis vectors with the summation index $q$.\n",
        "\n",
        "* So on the right we have a linear combination of ${\\widetilde{e}}$ basis vectors again but with the summation index $l$.\n",
        "\n",
        "* So we have a linear combination of ${\\widetilde{e}}$ basis vectors on both sides.\n",
        "\n",
        "But with different summation indexes. But the choice doesn't really matter. So we change all the $q$ with $l$:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\widetilde{L_{i}^{l}} \\color{red}{\\widetilde{e_{l}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "Now we see that the $\\widetilde{L_{i}^{l}}$ coefficients on the left side are equal to this part $\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} B_{k}^{l}$ in the middle of the right equation:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\color{pink}{\\widetilde{L_{i}^{l}}} \\widetilde{e_{l}}=\\sum_{l=1}^{n} \\color{pink}{\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}} \\widetilde{e_{l}}$\n",
        "\n",
        "**This is saying that to transform the matrix coordinates from the old basis to the new basis we multiply the old matrix $L_{j}^{k}$ by the backward transform $B$ on the left and by the forward transform $F$ on the right:**\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "**This is what we just did (explanation below):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)"
      ],
      "metadata": {
        "id": "l4kEzo39A7c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUMMARY: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)**\n",
        "\n",
        "* So, matrices, or linear maps, transform with both the forward transform and the backward transform. Here is why:\n",
        "\n",
        "* You have two ways to get the new basis vector component: you can use $\\widetilde{L}$ matrix n the bottom to go from left to right directly $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$\n",
        "\n",
        "* But it's the same thing as going the other way around:\n",
        "\n",
        "  1. To transform the new vector components into the old vector components you use the forward transform\n",
        "\n",
        "  2. And here to transform the components of the input vector into components of the output vector in the old basis, we just use the matrix $L$\n",
        "\n",
        "  3. And finally to get from the old vector components to the new vector components for the output vector we use the backward transform\n",
        "\n",
        "  4. Now we have the components of the new basis vector $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$ with which you can describe any vector in the new vector space.\n",
        "\n",
        "* So the idea of transforming matrix components using both the forward and backward transformations makes sense.\n",
        "\n",
        "**Let's check this on our example from above:**\n",
        "\n",
        "This is the equation again to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis):\n",
        "\n",
        "> $\\widetilde{L}_{i}^{l}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "* Backward Transform $B$: $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$\n",
        "\n",
        "* Linear Map $L$ in old basis ${\\overrightarrow{e_{j}}}$: $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\overrightarrow{e_{j}}}$\n",
        "\n",
        "* Forward Transform $F$: $\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "Now let's place them all in the equation to get the linear map in the new basis:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 8 & 1 \\\\ -1 / 2 & 4\\end{array}\\right]\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "We get this matrix the:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}5 / 4 & 3 / 16 \\\\ 3 & 5 / 4\\end{array}\\right]$\n",
        "\n",
        "And this matrix above tells us how to write the outputs of the linear map as linear combination of the new basis:\n",
        "\n",
        "> $L\\left(\\widetilde{e_{1}}\\right)=5 / 4 \\widetilde{e_{1}}+3 \\widetilde{e_{2}}$\n",
        "\n",
        "> $L\\left(\\widetilde{e_{2}}\\right)=3 / 16 \\widetilde{e_{1}}+5 / 4 \\widetilde{e_{2}}$\n",
        "\n",
        "**Results are correct, the new matrix / linear map in the new basis is converting the vector properly from its original position to the new position (measuring both at the new basis vectors):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_24.png)"
      ],
      "metadata": {
        "id": "qozTbjNIA9Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward & Backward Transform of Linear Maps between different basis**\n",
        "\n",
        "* A tensor is an object that is invariant under a change of coordinates, and **has components that change in a special, predictable way under a change of coordinates**. The way we transform them is by a applying a series of forward and backward transforms!\n",
        "\n",
        "**Forward Transform (going from old $T$ to new $\\widetilde{T}$)**\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **backward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **forward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Backward Transformation (going from new $\\widetilde{T}$ to old $T$)**\n",
        "\n",
        "> $T_{r s t \\ldots}^{i j k \\ldots}=\\left(F_{\\color{red}a}^{i} F_{\\color{red}b}^{j} F_{\\color{red}c}^{k} \\cdots\\right) \\widetilde{T_{\\color{blue}{x y z} \\ldots}^{\\color{red}{a b c} \\ldots}}\\left(B_{r}^{\\color{blue}x} B_{s}^{\\color{blue}y} B_{t}^{\\color{blue}z} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **forward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **backward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Illustration from the linear maps part that helps to understand the $FTB$ = $\\widetilde{T}$ (oben) bzw. $FLB$ = $\\widetilde{L}$ (unten) transform:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$ (forward transform)\n",
        "\n",
        "**How many contravariant and covariant rules we need to follow during a transformation?** - Core Question when it gets more complicated with more basis & dual basis !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_32.png)"
      ],
      "metadata": {
        "id": "5cOVS7o9BBX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "_AUtbuZDBDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Tensors (Any Vector-Covector combination) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$*"
      ],
      "metadata": {
        "id": "457rg_0-BFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Maps vs Bilinear Forms**\n",
        "\n",
        "* **Bilinear Forms: Linear combinations of covector-covector-pairs** $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$ (including metric tensor)\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 14: Tensors are general vector/covector combinations](https://www.youtube.com/watch?v=9R4vhqvE_jw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=17)\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)"
      ],
      "metadata": {
        "id": "m-CmxPR5BUhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take two new tensor**\n",
        "\n",
        "This is a (2,0) tensor\n",
        "\n",
        "> $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "This is a (1,2) tensor\n",
        "\n",
        "> $Q=Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "**Now we can ask the same questions:**\n",
        "\n",
        "1. What are the coordinate transform rules?\n",
        "2. What is the multiplication formula for $Q$ ($D$)?\n",
        "3. What are the array shapes?\n",
        "\n",
        "**1. How to $D$ and $Q$ under a change of basis?**\n",
        "\n",
        "* Transforming tensor components is not hard, long as you have the transformation rules for basis vectors and covectors (plug backward transform in here for example twice on the left side)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_50.png)\n",
        "\n",
        "**2. What is the formula for $Q$ acting on the input $D$ = $Q(D)$**\n",
        "\n",
        "* This is tricky because there is no one way to do that.\n",
        "\n",
        "* **The challenge is now that: As we make these tensors bigger and bigger with more and more covariant and contravariant parts, we end up with more and more ways to do the summations, and more and more ways to compute functions.**\n",
        "\n",
        "* Writing $D$ = $Q(D)$ is ambuguous as it doesn't tell us exactly what to do\n",
        "\n",
        "* so we need to write it out in the Einstein notation like on the left side, for example $Q_{j k}^{i} D^{j k}$\n",
        "\n",
        "* Some examples are:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_51.png)\n",
        "\n",
        "**3. What is the array shape?**\n",
        "\n",
        "* It's like the Kronecker product between two column vectors: like this part $\\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$  in this here: $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "* We can do this for the first example from above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_52.png)\n",
        "\n",
        "* For the other example it would look like this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_53.png)\n",
        "\n",
        "* One could use the 3d visualisation, but it's better in the matrix array way. Because looking at this you can see it‚Äôs a (1,2) tensor with one column aspect and two row aspects\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_54.png)"
      ],
      "metadata": {
        "id": "rXmd5aWCBWWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Array Multiplication**\n",
        "\n",
        "* Easy for smaller tensors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_55.png)\n",
        "\n",
        "* That's not so easier for larger tensors that have high type numbers, because there are several possible multiplication rules (as discussed earlier):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_56.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_57.png)\n",
        "\n",
        "* For much more complex tensors, it's the easiest to just stick with the Einstein notation and also not do the array representation.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_58.png)\n",
        "\n",
        "* So with high type tensors the abstract notation and the array notation have their limitations. When trying to express tensor multiplication formulas it‚Äôs usually easier just to stick with the Einstein component notation. For this reason a lot of sources just write tensors like this and leave out basis vector and covectors completely:\n",
        "\n",
        "> $Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ $\\mapsto$ $Q_{j k}^{i}$\n",
        "\n",
        "> $D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$ $\\mapsto$ $D=D^{a b}$\n",
        "\n",
        "* **But it's important to remember that tensor components always come from a choice of basis, and the same tensor can have different components if we choose to represent it in a different basis.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_59.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_60.png)"
      ],
      "metadata": {
        "id": "mGPcQ4POBYNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector Spaces (Hilbert Spaces)*"
      ],
      "metadata": {
        "id": "vq-kSppvBbU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "These vectors are element of the vector space ${V}$\n",
        "\n",
        "> $\\vec{v}, \\vec{w}, \\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}$ $\\in {V}$\n",
        "\n",
        "These covectors are element of the dual vector space ${V^{*}}$\n",
        "\n",
        "> $\\alpha, \\beta, \\epsilon^{1}, \\epsilon^{2}$ $\\in {V^{*}}$\n",
        "\n",
        "We can make following vector-covector-pairs:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "And we can add them and scale with them with the tensor product rules which forms a vector space:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "So we know that these must be vectors in a vectors space:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "But in which vector space do they live in? In this one:\n",
        "\n",
        "> $\\in V \\otimes V^{*}$\n",
        "\n",
        "**Now this is a new use case of $\\otimes$, because so far we used it for combining vectors or tensors. Here we use it to combine entire vector spaces.** (more about it below in a short separat chapter)"
      ],
      "metadata": {
        "id": "gr-jRSA3I5Ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V \\otimes V^{*}$?** (1,1)-tensors\n",
        "\n",
        "\n",
        "> **(1,1)-Tensors**: $L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\in V \\otimes V^{*}$\n",
        "\n",
        "Remember: vectors have an upstairs index (because they transform contravariant):\n",
        "\n",
        "> $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "and covectors habe a downstairs index (because they transform covariant):\n",
        "\n",
        "> $\\alpha=\\alpha_{i} \\epsilon^{i}$\n",
        "\n",
        "\n",
        "\n",
        "So if we do a summation with vector components like this over $j$, we end up with vector components as the output (because we have one upstairs $i$ index that's left):\n",
        "\n",
        "> $L_{j}^{i} v^{j}=w^{i}$\n",
        "\n",
        "In this case $L$ ist acting as a map from $V$ to $V$, or essentially a linear map.\n",
        "\n",
        "> $V \\mapsto V$\n",
        "\n",
        "> **This is a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a summation with covector components like this with sum over $i$, and our outout would have $j$ as the dowstairs index:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i}=\\beta_{j}$\n",
        "\n",
        "We would end up with covector components as the output. So covector in, covector out:\n",
        "\n",
        "> $V^{*} \\mapsto V^{*}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "Also we can provide $L$ with both vector components and covector components and we do two summations over $i$ and $j$, and that would give us a scalar as the output since there are no indices left to sum over.\n",
        "\n",
        "> $L_{j}^{i} v^{j} \\alpha_{i}=s$\n",
        "\n",
        "So in this case $L$ can be viewed as a function from a pair of vectors and covectors to scalars.\n",
        "\n",
        "> $V \\times V^{*} \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "And finally we can do the same thing but reverse the order of the inputs:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i} v^{j}=s$\n",
        "\n",
        "And in this case $L$ is a function from a covector-vector-pair to scalar:\n",
        "\n",
        "> $V^{*} \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "4bn8DJW0I9YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V^{*} \\otimes V^{*}$?** (0,2)-tensors\n",
        "\n",
        "**Now what if we use the tensor product to combine two covectors together?**\n",
        "\n",
        "Those covectors life in $V^{*}$:\n",
        "\n",
        "> $\\alpha, \\beta, \\gamma, \\delta, \\epsilon^{1}, \\epsilon^{2}$ $\\in V^{*}$\n",
        "\n",
        "So when we have covector-covector-pairs like following it turns out that they all life in the vector space **$V^{*} \\otimes V^{*}$**\n",
        "\n",
        "> $\\alpha \\beta, \\alpha \\gamma, \\delta \\beta, \\epsilon^{1} \\epsilon^{2}, \\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$ **$\\in V^{*} \\otimes V^{*}$**\n",
        "\n",
        "**So elements of the $V^{*} \\otimes V^{*}$are (0,2)-tensors**\n",
        "\n",
        "> $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\in V^{*} \\otimes V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "(Covector-covector-pairs and their linear combinations)\n",
        "\n",
        "If we take these $B$ components and do two summations with two sets of vector components (via $i$ and $j$), then we end up with a scalar:\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i} w^{j}=s$\n",
        "\n",
        "And this of course is a bilinear form (which takes a pair of vectors and outputs a scalar):\n",
        "\n",
        "> $V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a single summation over $i$ with a set of vector components and we‚Äôd be left with the index $j$ downstairs. So the output would be a set of covector components\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i}=\\alpha_{j}$\n",
        "\n",
        "So in this case $B$ is a map from vectors to covectors:\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "Also we could choose to do summation with vector components over the $j$ index and then end up with covector components $i$\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{j}=\\beta_{i}$\n",
        "\n",
        "This would be another map from $V$ to $V^{*}$, but it would be a different map than the previous one, because we‚Äôre doing the summation differently.\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "AZ-VNTnoI_JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create even larger vector spaces from basic building blocks $V$ and $V^{*}$**\n",
        "\n",
        "**So we can create larger and larger vector spaces out of the two basic building blocks $V$ and $V^{*}$**:\n",
        "\n",
        "* starting with two vector spaces $V$ and $V^{*}$ (basic building blocks)\n",
        "\n",
        "  * they contain tensors $v_{i}$ and $\\alpha_{j}$\n",
        "\n",
        "  * with vector components with upstairs index and covector components with downstairs index\n",
        "\n",
        "* we can combine these 2 vector spaces into new vector spaces using the tensor product $V \\otimes V$, $V \\otimes V^{*}$, $V^{*} \\otimes V$ and $V^{*} \\otimes V^{*}$\n",
        "\n",
        "  * and these vectors spaces have following vector components: $\\mathcal{A}^{i j}$, $L_{j}^{I}$, $L_{j}{ }^{i}$ and $\\mathcal{B}_{i j}$\n",
        "\n",
        "  * Indexes from $V$ go upstairs and index from $V^{*}$ go downstairs\n",
        "\n",
        "* And we can continue to make larger and larger vector spaces using the tensor product like $V \\otimes V \\otimes V$, $V^{*} \\otimes V \\otimes V$ etc\n",
        "\n",
        "  * And all these vector spaces contain tensors like $T^{i j k}$, $T_{i}^{j k}$, $T_{j}^{i k}$ etc.\n",
        "\n",
        "  * with components that have different combinations of upstairs and downstairs indexes depending on whether they are constructed using $V$ or $V^{*}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_65.png)"
      ],
      "metadata": {
        "id": "npgLhzSYJA3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So if we have some new tensor $T$ from a vector space we‚Äôve never seen before, we can easily get the correct component indexes just by looking at the vector spaces.**\n",
        "\n",
        "> $T \\in V^{*} \\otimes V \\otimes V^{*} \\otimes V^{*}$\n",
        "\n",
        "Looking at these vector spaces we see that the basis would be made up of a covector, a vector, a covector and another covector in combination\n",
        "\n",
        "> $T={\\epsilon}^{i}  \\overrightarrow{e_{j}} \\epsilon^{k} {\\epsilon}^{l}$\n",
        "\n",
        "And we get the components just by **placing the indexes in the opposite position that we see in the basis**, so that all the summations work out properly.\n",
        "\n",
        "**And now we can ask: How can this tensor $T$ act on other tensors?**\n",
        "\n",
        "> $T_{i}^{j}{ }_{k l}$\n",
        "\n",
        "examples of other tensors to be acted on:\n",
        "\n",
        "> $u^{c} \\quad \\begin{array}{cc}D^{f g} & \\beta_{s} \\\\ Q_{u v}^{t} & L_{y}^{x} & w^{b} & U^{m n o}\\end{array}$\n",
        "\n",
        "**Well, we can basically do any summations we like as long as the upstairs indexes are matched with downstairs indexes and downstairs indexes are matched with upstairs indexes.**\n",
        "\n",
        "We could do something like this with four summations and you can see that all the indexes are positioned properly:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensors_69.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_68.png)"
      ],
      "metadata": {
        "id": "HEU05oWyJCrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is how the result for each would look like** [see explanation](https://youtu.be/M-OLmxuLdbU?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&t=683)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_70.png)\n",
        "\n",
        "The tensors we get from these tensor product form new vector spaces:\n",
        "\n",
        "> $V \\otimes V$\n",
        "\n",
        "> $V \\otimes V^{*}$\n",
        "\n",
        "> $V^{*} \\otimes V$\n",
        "\n",
        "> $V^{*} \\otimes V^{*}$\n"
      ],
      "metadata": {
        "id": "LU6hlHpSJEt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Tensor Products $\\otimes$ in Quantum Mechanics*#"
      ],
      "metadata": {
        "id": "F4gfVMumBdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two systems being described as a joint system:**\n",
        "\n",
        "1. the structure of the two systems is pre- served:\n",
        "\n",
        "2. a measurement on one of the systems does not disturb the other one;\n",
        "\n",
        "3. maximal information obtained on both systems separately gives maximal information on the joint system.\n",
        "\n",
        "With these conditions we show, within the framework of the propositional system formalism, that\n",
        "\n",
        "* if the systems are classical the joint system is described by the cartesian product of the corresponding phase spaces, and\n",
        "\n",
        "* if the systems are quantal the joint system is described by the tensor product of the corresponding Hilbert spaces.\n",
        "\n",
        "[Source: Paper Aerts](https://raw.githubusercontent.com/deltorobarba/papers/master/aerts.pdf)\n",
        "\n",
        "**One should deal with at least two operator products:**\n",
        "\n",
        "* one is given by the composition of two operators defined on the same vector space = product yields an operator de!ned on the common vector space of the factor operators\n",
        "\n",
        "* the other is the direct or tensor product of operators = leads to an operator de!ned on a different vector space: the direct or tensor product of the vector spaces of the factor operators\n",
        "\n",
        "The Kronecker product and some of its physical applications (Francisco M Fern√°ndez)"
      ],
      "metadata": {
        "id": "fZWP-A5kPUTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider two distinguishable particles:\n",
        "\n",
        "* Particle 1: its quantum mechanics is described by a complex vector space V. It has associated operators T1, T2, ..\n",
        "\n",
        "* Particle 2: its quantum mechanics is described by a complex vector space W. It has associated operators S1, S2, ..\n",
        "\n",
        "*This list of operators for each particle may include some or many of the operators you are already familiar with: position, momentum, spin, Hamiltonians, projectors, etc.*\n",
        "\n",
        "**Once we have two particles, the two of them together form our system.**\n",
        "\n",
        "* We are after the description of quantum states of this two-particle system.\n",
        "\n",
        "* On first thought, we may think that any state of this system should be described by giving the state v ‚àà V of the first particle and the state w ‚àà W of the second particle.\n",
        "\n",
        "* This information could be represented by the ordered list (v, w) where the first item is the state of the first particle and the second item the state of the second particle. This is a state of the two-particle system, but it is far from being the general state of the two-particle system. It misses remarkable new possibilities, as we shall soon see.\n",
        "\n",
        "We thus introduce a new notation. Instead of representing the state of the two-particle system with particle one in $v$ and particle two in $w$ as $(v, w)$, **we will represent it as $v \\otimes w$**.\n",
        "\n",
        "* <font color=\"blue\">This element $v \\otimes w$ will be viewed as a vector in a new vector space $V \\otimes W$ that will carry the description of the quantum states of the system of two particles.</font>\n",
        "\n",
        "* <font color=\"blue\">This $\\otimes$ operation is called the \"tensor product.\" In this case we have two vector spaces over $\\mathbb{C}$ **and the tensor product $V \\otimes W$ is a new complex vector space**:</font>\n",
        "\n",
        "> $v \\otimes w \\in V \\otimes W \\quad$ when $\\quad v \\in V, w \\in W$\n",
        "\n",
        "$\\operatorname{In} v \\otimes w$ there is no multiplication to be carried out, we are just placing one vector to the left of $\\otimes$ and another to the right of $\\otimes$.\n",
        "\n",
        "We have only described some elements of $V \\otimes W$, not quite given its definition yet. We now explain two physically motivated rules that define the tensor product completely.\n"
      ],
      "metadata": {
        "id": "3aRUE2W-PW9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Rule 1</u>: If the vector representing the state of the first particle is scaled by a complex number this is equivalent to scaling the state of the two particles. The same for the second particle. So we declare:**</font>\n",
        "\n",
        "> <font color=\"blue\">$(a v) \\otimes w=v \\otimes(a w)=a(v \\otimes w), \\quad a \\in \\mathbb{C}$\n",
        "\n",
        "* like in tensor algebra the linear map bzw. bilinear map, where a factor is attached only to one of two, not both\n",
        "\n",
        "<font color=\"blue\">**<u>Rule 2</u>: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition. We thus demand distributive properties for the tensor product:**</font>\n",
        "\n",
        "> <font color=\"blue\">$\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "\n",
        "> <font color=\"blue\">$v \\otimes\\left(w_{1}+w_{2}\\right)=v \\otimes w_{1}+v \\otimes w_{2}$\n",
        "\n",
        "Example in Quantum Phase Estimation: We first perform a Hadamard gate on the first qubit to get the state and then distribute the superposition (omitted the normalization factor of 1/‚àö2 for clarity):\n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "\n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "\n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "> The tensor product $V \\otimes W$ is thus defined to be the vector space whose elements are **(complex) linear combinations** of elements of the form $v \\otimes w$, with $v \\in V, w \\in W$, with the above rules for manipulation. The tensor product $V \\otimes W$ is the complex vector space of states of the two-particle system!"
      ],
      "metadata": {
        "id": "2hHr5XVJPZLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment: Let $v_{1}, v_{2} \\in V$ and $w_{1}, w_{2} \\in W$. A vector in $V \\otimes W$ constructed by superposition is**\n",
        "\n",
        "> $\n",
        "\\alpha_{1}\\left(v_{1} \\otimes w_{1}\\right)+\\alpha_{2}\\left(v_{2} \\otimes w_{2}\\right) \\in V \\otimes W\n",
        "$\n",
        "\n",
        "<font color=\"blue\">This shows clearly that a general state of the two-particle system cannot be described by stating the state of the first particle and the state of the second particle</font>. The above superpositions give rise to entangled states. An entangled state of the two particles is one that, roughly, **cannot be disentangled into separate states of each of the particles**.\n",
        "\n",
        "*Explanation 1*\n",
        "\n",
        "* **The \"Kronecker product\", better known as the tensor product**, is the natural notion of a product for spaces of states, when these are considered properly:\n",
        "\n",
        "* **A space of states is not a Hilbert space $\\mathcal{H}$, but the projective Hilbert space $\\mathbb{P} \\mathcal{H}$ associated to it**. This is the statement that quantum states are rays in a Hilbert space.\n",
        "\n",
        "* <font color=\"blue\">Now, **why does the physical notion of combining the spaces of states of individual systems into a space of states of the combined system correspond to taking the tensor product?**</font>\n",
        "\n",
        "  * The reason is that <font color=\"blue\">**we want every action of an operator (which are linear maps) on the individual states to define an action on the combined state**</font>\n",
        "\n",
        "  * and the tensor product is exactly that, since, <font color=\"red\">for every pair of linear maps $T_{i}: \\mathcal{H}_{i} \\rightarrow \\mathcal{H}$ (which is a bilinear map $\\left(T_{1}, T_{2}\\right): \\mathcal{H}_{1} \\times \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$ ) there is a unique linear map $T_{1} \\otimes T_{2}: \\mathcal{H}_{1} \\otimes \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$</font>\n",
        "\n",
        "* Alternatively, <font color=\"blue\">**concentrating more on the projective nature of the spaces of states, we observe that $|\\psi\\rangle$ and $a|\\psi\\rangle$ are the same state for any $a \\in \\mathbb{C}$.**</font> Therefore, denoting the sought-for physical product by $\\otimes$ (i.e. not assuming it is the tensor product), we must demand that\n",
        "\n",
        "><font color=\"red\">$\n",
        "|\\psi\\rangle \\otimes|\\phi\\rangle=(a|\\psi\\rangle) \\otimes|\\phi\\rangle=a(|\\psi\\rangle \\otimes|\\phi\\rangle)\n",
        "$</font>\n",
        "\n",
        "* since the states produced by $|\\psi\\rangle$ and $a|\\psi\\rangle$ must yield the same state, i.e. map onto the same projective state. **This obviously fails for the cartesian product, since the pair $(a|\\psi\\rangle,|\\phi\\rangle)$ <u>is not a multiple</u> of the $\\operatorname{pair}(|\\psi\\rangle,|\\phi\\rangle)$, but it is true for the tensor product**.\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/148378/importance-of-kronecker-product-in-quantum-computation\n",
        "\n",
        "*Explanation 2*\n",
        "\n",
        "* In der Mathematik k√∂nnen mit Hilfe des Tensorprodukts (Kronecker-Produkts) von PauliMatrizen (mit Einheitsmatrix) die Darstellungen der h√∂heren Clifford-Algebren √ºber den reellen Zahlen aufgebaut werden.\n",
        "\n",
        "* Pauli-Matrizen k√∂nnen zur Darstellung von Hamilton-Operatoren und zur N√§herung der Exponentialfunktion solcher Operatoren verwendet werden. Sind $\\sigma_{0}, \\sigma_{1}, \\sigma_{2}, \\sigma_{3}$ die vier Pauli-Matrizen, so kann man mit Hilfe des Kronecker-Produkt h√∂herdimensionale Matrizen erzeugen.\n",
        "\n",
        "> $\n",
        "p:=\\sigma_{\\mu_{1}} \\otimes \\sigma_{\\mu_{2}} \\otimes \\ldots \\otimes \\sigma_{\\mu_{n}} \\quad ; \\quad \\mu_{1}, \\mu_{2}, \\ldots, \\mu_{n} \\in\\{0,1,2,3\\} \\quad ; \\quad n \\in \\mathbb{N}\n",
        "$\n",
        "\n",
        "* **Das Kronecker-Produkt von Pauli-Matrizen tritt bei der Beschreibung von Spin-1/2-Systemen auf, die aus mehreren Teilsystemen aufgebaut sind**.\n",
        "\n",
        "* Der Zusammenhang ist dadurch gegeben, dass **das Tensorprodukt zweier Operatoren in der zugeh√∂rigen Matrixdarstellung durch das Kronecker-Produkt der Matrizen gegeben ist** (siehe [Kronecker-Produkt#Zusammenhang mit Tensorprodukten](https://de.wikipedia.org/wiki/Kronecker-Produkt#Zusammenhang_mit_Tensorprodukten)).\n",
        "\n",
        "\n",
        "https://de.wikipedia.org/wiki/Pauli-Matrizen#Kronecker-Produkt_von_Pauli-Matrizen\n",
        "\n",
        "*Explanation 3*\n",
        "\n",
        "* The kronecker product in group theory is widely used, especially with [Wigner D-function](https://de.wikipedia.org/wiki/Wignersche_D-Matrix)\n",
        "\n",
        "* The main purpose of its use in physics is to get the higher dimensional vector space. For example, in atomic physics, when we want to calculate the eigenvalues and eigenvectors of a system of spins 1/2 or spin Hamiltonian.\n",
        "\n",
        "* We analytically or with the help of computer diagonalize spin Hamiltonian and find eigenvectors and eigenvalues with the kronecker product:\n",
        "\n",
        "  * By appling the kronecker product between differnt spins matrices e.g., two matrices (dimensions 2 √ó 2) of spins 1/2, we will get the matrix of dimensions (4 √ó 4).\n",
        "\n",
        "  * This method is very compact, which means we can use the computer to get the eigenvectors and eigenvalues of matrix after applying kronecker product for higher number of spins e.g., for the system of spins 1/2.\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104019\n",
        "\n",
        "*Explanation 4*\n",
        "\n",
        "In quantum theory the analog of a Cartesian product of classical phase spaces is a tensor product of Hilbert spaces.\n",
        "\n",
        "https://quantum.phys.cmu.edu/CQT/chaps/cqt06.pdf\n",
        "\n",
        "*Explanation 5*\n",
        "\n",
        "How do you describe the combined state of two qubits? **Remember that each qubit is a vector space, so they can't just be multiplied**. Instead, you use a tensor product, which is a related operation that creates a new vector space from individual vector spaces, and is represented by the $\\otimes$ symbol.\n",
        "\n",
        "For example, the tensor product of two qubit states $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ and $\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]$ is calculated\n",
        "\n",
        "> $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right] \\otimes\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]=\\left[\\begin{array}{l}a\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right] \\\\ b\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$\n",
        "\n",
        "The result is a four-dimensional matrix, with each element representing a probability.\n",
        "\n",
        "For example, $a c$ is the probability of the two qubits collapsing to 0 and $0, a d$ is the probability of 0 and 1, and so on.\n",
        "\n",
        "Just as a single qubit state $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ must meet the requirement that $|a|^{2}+|b|^{2}=1$ in order to represent a quantum state,\n",
        "\n",
        "a two-qubit state $\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$ must meet the requirement that $|a c|^{2}+|a d|^{2}+|b c|^{2}+|b d|^{2}=1$\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/quantum/overview-algebra-for-quantum-computing\n",
        "\n",
        "*Explanation 6*\n",
        "\n",
        "If we have a set, denoted by 'a', of possible outcomes from one event and a set of outcomes for another event, denoted by 'b', then the possible outcomes for the union event is always the tensor product a‚äób.\n",
        "\n",
        "https://www.quora.com/What-is-a-tensor-product-in-quantum-mechanics\n",
        "\n",
        "*Explanation 7*\n",
        "\n",
        "At some point in the history of quantum mechanics, it was accepted that a single particle is described by a wavefunction which is a function of the position of the particle r, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        ")\n",
        ".\n",
        "\n",
        "At some (possibly later) point it was also accepted that two particles are described by a wavefunction which is a function of the positions of each one of the particles, r1 and r2, denoted:\n",
        "\n",
        "> œà\n",
        "(\n",
        "r\n",
        "1\n",
        ",\n",
        "r\n",
        "2\n",
        ")\n",
        ".\n",
        "\n",
        "In other words, the Hilbert space describing the two-particle system is the tensor product of the Hilbert spaces describing the system of each particle.\n",
        "\n",
        "https://physics.stackexchange.com/questions/53039/when-and-how-did-the-idea-of-the-tensor-product-originate-in-the-history-quantum\n",
        "\n",
        "*Explanation 8*\n",
        "\n",
        "Tensor Products are used to describe systems consisting of multiple subsystems. Each subsystem is described by a vector in a vector space (Hilbert space). For example, let us have two systems I and $/ /$ with their corresponding Hilbert spaces $H_{1}$ and $H_{11}$. Thus, using the bra-ket notation, the vectors $\\left|\\Psi_{1}\\right\\rangle$ and $\\mid \\Psi_{I I}$ ) describe the states of system I and $\\|$ with the state of the total system given by the tensor product $\\left|\\psi_{i}\\right\\rangle \\otimes\\left|\\psi_{1 I}\\right\\rangle$.\n",
        "\n",
        "https://www.quantiki.org/wiki/tensor-product\n",
        "\n",
        "*Explanation 9*\n",
        "\n",
        "The Hilbert space of a composite system is the Hilbert space tensor product of the state spaces associated with the component systems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "Others:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_mechanics\n",
        "\n",
        "http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html\n",
        "\n",
        "https://docs.microsoft.com/de-de/azure/quantum/overview-algebra-for-quantum-computing"
      ],
      "metadata": {
        "id": "QWKNU3sBPbDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary: Tensor Transformations & Einstein Notation*"
      ],
      "metadata": {
        "id": "pBNDxf7EBkj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "JIDiucDuBmiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Tensor Transformation**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_overview.png)"
      ],
      "metadata": {
        "id": "yZth_WAwBoY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)"
      ],
      "metadata": {
        "id": "g_PC1EsqBqK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "UxjiWAuDBsC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vectors and covectors (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)"
      ],
      "metadata": {
        "id": "u4TZq3cvBtyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vector fields and covector fields / differential forms (not single vectors!) (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)"
      ],
      "metadata": {
        "id": "0umDhwJfBvmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einstein Notation**\n",
        "\n",
        "* $\\sum_{i=1}^{3} a_{i} x_{i}=a_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}$  is in this case the same as saying: $a_{i} x_{i}$\n",
        "\n",
        "* See video: [4 rules of Einstein notation](https://www.youtube.com/watch?v=CLrTj7D2fLM&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=3)\n",
        "\n",
        "* This derivation to transform matrix components is pretty complex and can be written in a much easier way:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_25.png)\n",
        "\n",
        "If we have the same index on and top and on the bottom we end up summing over that index letter and can drop the summation sign, in this case: $\\color{red}{\\sum_{k=1}^{n}}$:\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "**Because when we see an index repeated on the top and bottom we know that there is a summation that's going to happen.**\n",
        "\n",
        "Another examples:\n",
        "\n",
        "> $\\color{red}{\\sum_{q=1}^{n}} \\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "can be rewritten by dropping the summation sign because ${\\color{red}{q}}$ (to which the sum sign refers to) is on the top and bottom:\n",
        "\n",
        "> $\\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "And as for the total formula:\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}= \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=  B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_26.png)\n",
        "\n",
        "* According to the [Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation), when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set \\{1,2,3\\} ,\n",
        "\n",
        "> $\n",
        "y=\\sum_{i=1}^{3} c_{i} x^{i}=c_{1} x^{1}+c_{2} x^{2}+c_{3} x^{3}\n",
        "$\n",
        "\n",
        "is simplified by the convention to:\n",
        "\n",
        ">$\n",
        "y=c_{i} x^{i}\n",
        "$\n",
        "\n",
        "* The upper indices are not exponents but are indices of coordinates, coefficients or basis vectors.\n",
        "\n",
        "In [general relativity](https://en.wikipedia.org/wiki/General_relativity), a common convention is that\n",
        "\n",
        "* the Greek alphabet is used for space and time components, where indices take on values 0,1,2 ,\n",
        "or 3 (frequently used letters are $\\mu, v, \\ldots),$\n",
        "\n",
        "* the Latin alphabet is used for spatial components only, where indices take on values $1,2,$ or 3 (frequently used letters are $i, j, \\ldots)$\n",
        "\n",
        "*Die [Penrosesche graphische Notation](https://de.m.wikipedia.org/wiki/Penrosesche_graphische_Notation) ist eine alternative Schreibweise f√ºr die Darstellung von Tensoren!*"
      ],
      "metadata": {
        "id": "OCv-TJ6LBxzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Field*"
      ],
      "metadata": {
        "id": "pAHNvHgpB0r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorfeld**\n",
        "\n",
        "* Tensorfelder sind Funktionen, **die jedem Punkt einen Tensor zuordnen** (Tensor meint in diesem Fall ein rein algebraisches Objekt)\n",
        "\n",
        "* Tensorfelder werden auf ihre analytischen Eigenschaften untersucht (zB differenziert). Man erh√§lt durch Differenzieren eines Tensorfeldes wieder ein Tensorfeld. Tensorfelder sind besondere glatte Abbildungen, die in Tensorb√ºndel hinein abbilden (siehe unten).\n",
        "\n",
        "* Sei $M$ eine differenzierbare Mannigfaltigkeit. Ein [Tensorfeld](https://de.wikipedia.org/wiki/Tensorfeld) vom Typ (r,s) ist ein glatter [Schnitt](https://de.m.wikipedia.org/wiki/Schnitt_(Faserb√ºndel)) im Tensorb√ºndel $T_{s}^{r}(M)$.\n",
        "\n",
        "  * Ein Tensorfeld ist also ein glattes Feld $M \\rightarrow T_{s}^{r}(M),$ welches jedem Punkt der Mannigfaltigkeit einen (r,s)-Tensor zuordnet.\n",
        "\n",
        "  * Die Menge der Tensorfelder wird oft mit $\\Gamma^{\\infty}\\left(T_{s}^{r}(M)\\right)$ bezeichnet."
      ],
      "metadata": {
        "id": "TP8jXL78B271"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensordichte**\n",
        "\n",
        "* [Tensordichte](https://de.wikipedia.org/wiki/Tensordichte) ist die Quantit√§tsgr√∂√üe eines Tensorfeldes (Generalisierung)\n",
        "\n",
        "* die Tensordichte ist eine **Verallgemeinerung der Tensorfelder** in der Tensoranalysis\n",
        "\n",
        "* wurde eingef√ºhrt, um den ‚ÄûUnterschied zwischen Quantit√§t und Intensit√§t, soweit er physikalische Bedeutung hat‚Äú, zu erfassen: ‚Äûdie Tensoren sind die Intensit√§ts-, die Tensordichten die Quantit√§tsgr√∂√üen‚Äú.\n",
        "\n",
        "* eine **Tensordichte** ordnet einem Koordinatensystem ein Tensorfeld derart zu, dass es bei einem Koordinatenwechsel mit dem Absolutbetrag der Funktionaldeterminante multipliziert wird. Eine Tensordichte der Stufe null ist demnach eine skalare Dichte, deren Integral gem√§√ü dem Transformationssatz eine Invariante liefert."
      ],
      "metadata": {
        "id": "WCB2oE9SB4xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field*"
      ],
      "metadata": {
        "id": "r-rllUswCV7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis of Vector Fields**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Tensorfeld\n",
        "\n",
        "> Basis Vectors = Partial Derivatives (Jacobian)\n",
        "\n",
        "> **From Single Vectors to Vector Fields (Transformations)**\n",
        "\n",
        "Energy Momentum Tensor: https://youtu.be/ii7rffG0EwU\n",
        "\n",
        "A (basis) vector can be re-interpreted as a partial derivative (see image below):\n",
        "\n",
        "> $\\overrightarrow{e_{x}} \\equiv \\frac{\\partial \\vec{R}}{\\partial x}$\n",
        "\n",
        "https://www.youtube.com/watch?v=rr5qEb_kT6c&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=3\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_112.png)\n",
        "\n",
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=OMCguyCnTQk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=4\n",
        "\n",
        "* Now if you want to get the coefficients when your old basis in Cartesian and your new basis is Polar coordinates, then you have another forward map at every point in the polar coordinate system\n",
        "\n",
        "* if you now think of the basis vectors as partial derivatives, it makes things much easier. Use mutlivariable chain rules:\n",
        "\n",
        "  * the first old basis vector $\\overrightarrow{e_{x}}$ can be thought of as the partrial derivative in the x direction: $\\frac{\\partial \\vec{R}}{\\partial x}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * the second old basis vector $\\overrightarrow{e_{y}}$ can be thought of as the partrial derivative in the y direction: $\\frac{\\partial \\vec{R}}{\\partial y}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * same goes for the new basis vector (polar coordinates) as partial derivaties into r and $\\theta$ directions\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_90.png)\n",
        "\n",
        "The underlined part are the coefficients to move from one basis (cartesian) to another (polar) (Achtung: not normalised):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_91.png)\n",
        "\n",
        "The forward matrix = the Jacobian matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_92.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_95.png)\n",
        "\n",
        "And you can do the same the other way around to get the Backward transform, which is the inverse Jacobian:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_93.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_94.png)\n",
        "\n",
        "It total it looks likes this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_96.png)\n",
        "\n",
        "And we can store the (forward & backward) coefficients in matrix\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_97.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tYEF_rwU4Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Field Components**\n",
        "\n",
        "> Vectors Components = Derivatives in Vector Fields (vector fields of tangent vectors along curves)\n",
        "\n",
        "> Task: Figure out vector components in a new basis, **but instead of one vector, we consider vector fields**.\n",
        "\n",
        "We consider vectors along a curve (=tangent vectors)\n",
        "\n",
        "https://www.youtube.com/watch?v=9yOb9gHnLUk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=5\n",
        "\n",
        "\n",
        "The cases are both the same:\n",
        "\n",
        "* on we have a single vector in a new basis constructed from the basis vectors in the old basis and its components\n",
        "\n",
        "* on the bottom we have a whole vector field constructed from the old basis (using the chain rule!) and the vectir components ar derivates!\n",
        "\n",
        "* Framed in red is the vector we want to expand, in blue framed the basis vectors and in green framed the vector components\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_98.png)\n",
        "\n",
        "Example: in the cartesian coordinate system the basis vectors are every where the same. Just components are everywhere different. But it‚Äôs easy using the multivariable chain rule when you take the vector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_99.png)\n",
        "\n",
        "Other example iof tangents on a circle in a cartesian coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_100.png)\n",
        "\n",
        "\n",
        "Do these component make sense? Let's check with an example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_101.png)\n",
        "\n",
        "Also works in the polar coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_103.png)\n",
        "\n",
        "Checking if it's true, and it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMCdUWicVfKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contravariance of Vector Field Components**\n",
        "\n",
        "https://www.youtube.com/watch?v=zKuyaQ4JRs8&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=6&t=154s\n",
        "\n",
        "* Vector components are contravariant\n",
        "\n",
        "* We can follow the same reasoning for vector fields of tangent vectors along curves\n",
        "\n",
        "In this example, the polar components have two be equal to the Cartesian components multiplied by the backward transform. So we get this transformation formula for the components of the tangent vectors (the box on the bottom right):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "Partial derivative basis vectors transform one way and the vector component derivatives transform the other way, the vector components are contravariant. (Dervative coefficients are opposites) But don't memorize them! Simply use multivariable chain rules for the four formulas on the bottom and you get the transformation (chain rules over cartesian frames in purple, chain rule sover polar coordinates framed in green)::\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_105.png)\n",
        "\n",
        "Remember when we used for forward and backward transforms with single vectors the following formula:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_107.png)\n",
        "\n",
        "**This is similar for Vector Fields: basis vectors transform one way, and the vector components transform the other wa, using the Jacobian $J$ and the Jacobian inverse $J^-1$ as the forward and backward transforms:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_106.png)\n",
        "\n",
        "..and this is for the specific example we had for the circular curve with radius 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_108.png)\n",
        "\n",
        "**One important last thing: we remove the position vector $R$ and leave the <u>derivative operators = (basis) vector</u>, and they will be considered basis vectors from now on:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_109.png)\n",
        "\n",
        "And this makes sense: The partial derivative with respect to x points in the x direction (and so on):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_110.png)\n",
        "\n",
        "This is useful becasue position vectors $R$ rely on an origin, and as we will see later, on manifolds on curved surfaces we cannot rely on the existnce of an origin point:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_111.png)"
      ],
      "metadata": {
        "id": "J152JTPfWWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covector Field (1-form-Differential Form)*"
      ],
      "metadata": {
        "id": "8h7LKwE9CYwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector Field (= 1-form-Differential Form)**\n",
        "\n",
        "> <font color=\"blue\">**Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind**\n",
        "\n",
        "> **Covectors: Differential Forms = Covector Fields**\n",
        "\n",
        "Das Differential eines Skalarfeldes (differentialform) ist ein Covector field, weil differentiale von skalaren covectoren sind??\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_119.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8&t=201s\n",
        "\n",
        "Re-interprete $d$ from $df$ as being an operator that takes a scalar field and outputs a covector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_120.png)\n",
        "\n",
        "Example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)\n",
        "\n",
        "The way to get the covector fields is by tracing out the level sets of the scalar function: **Skalarfeld links mit Temperaturen, Kovektorfeld rechts mit den Konturen, wo √ºberall die gleichen Konturen existieren (√Ñquivalenzen)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)\n",
        "\n",
        "Another example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_124.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_122.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)\n",
        "\n",
        "Example: If we think of x as a scalar field, it would look like this: it‚Äôs a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [Level Set (Niveaumenge)](https://de.wikipedia.org/wiki/Niveaumenge): **die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist**\n",
        "\n",
        "Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)\n",
        "\n",
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)\n",
        "\n",
        "**How to calculate now? - How do covector fields like $df$ act on $\\vec{v}$ to give us output values?**\n",
        "\n",
        "> Count the number of tangent lines to the curve at p (where the vector originates at point p)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_130.png)\n"
      ],
      "metadata": {
        "id": "mjpXmcIJSASJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But what's the geometrical meaning of $df$ ($\\vec{v}$)?**\n",
        "\n",
        "On a map we can draw out curves of constant elevation (which is the same thing as level sets). We can think of this level set drawing of a mountain as a covector field associated with the mountain.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_131.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_132.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_133.png)\n",
        "\n",
        "> $d f(\\vec{v})$ tells us the rate of change of $f$ when moving at velocity $\\vec{v}$. **$d f(\\vec{v})$ is the directional derivative of $f$ in direction $\\vec{v}$**.\n",
        "\n",
        "**Covector Field Components: The following are the basis covectors**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_134.png)\n",
        "\n",
        "So just as we can expand individual covectors into linear combinations of dual basis vectors (and of course we get different components depending on which basis we use (all on top), we can also expand differential forms - also callee covector fields - into linear combinations of other covector fields where we get different components depending o which basis we use (on the bottom).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_135.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=r_20yXBdhJk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=9\n",
        "\n"
      ],
      "metadata": {
        "id": "05_SpyG3SEPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation Rules of Differential Forms (Covector Fields)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)\n",
        "\n",
        "Same logic applies for covector fields: if we for example want to build the Cartesian basis covector fields our of the polar basis covector fields (from new to old), we use the following coefficients, which are the entries of the Jacobian matrix (=forward transform, because covector basis components act contravariant).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_137.png)\n",
        "\n",
        "**Summary of all transformations (between 2 basis of covector fields, for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=4doR1XCXzKU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=10"
      ],
      "metadata": {
        "id": "FffzqkWvSGIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration with Differential Forms**\n",
        "\n",
        "* With the following interpretation of differential forms we can create the integration and it doesn't depend on coordinate systms at all\n",
        "\n",
        "* Also we **just need start point and end point** and count the number of pierced covector components instead of computing the integral at each point on the line\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_147.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_149.png)\n",
        "\n",
        "Since the covector fields and the paths are the same in both following cases, the result of the integral which is negative 4, is also the same in both cases:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_148.png)\n",
        "\n",
        "> **Covector fields are invariant of the choice of coordinates. Covector field components depend on the choice if coordinate.*And that's why when we change the variable in an integral we still get the same answer.**\n",
        "\n",
        "https://www.youtube.com/watch?v=kyzSofggsqg&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=11\n",
        "\n",
        "https://www.youtube.com/watch?v=PzrGGbX-_54&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=12"
      ],
      "metadata": {
        "id": "rAtqpzXZSH87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field vs Covector Field*"
      ],
      "metadata": {
        "id": "ihDto5NgCay8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gradient ‚àá vs ùëë operator (exterior derivative/differential)\n",
        "\n",
        "https://www.youtube.com/watch?v=nJpONHO_X5o&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=15\n",
        "\n",
        "https://www.youtube.com/watch?v=Do5vzLJRWRE&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=16\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_139.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_zLHtahCwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Extrinsic (Exterior) Geometry vs Intrinsic Geometry*"
      ],
      "metadata": {
        "id": "BJaouQGICdBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=VHkL5HpL0HY&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=7\n",
        "\n",
        "* Let's take a 2D surface like earth, and someone drives along it\n",
        "\n",
        "* we want to get the velocity of the car. We need position vectors, where we need an origin point (center of earth), then we get 2 vectors, take their limit to compute velocity\n",
        "\n",
        "* There are 2 issues with this:\n",
        "\n",
        "  1. Firstly the origin point that we've chosen doesn't live o the earth's 2D surface. We picked an origin point that was exterior to the surface.\n",
        "\n",
        "  2. Secondly the target velocity actually leaves the surface that we are studying and goes off into the outside space - **Remember here Lie algreba and Lie group**: the tangent is outside the 2D surface and the tangent point is the Lie algebra. All on the 2D surface however is part of the Lie group.\n",
        "\n",
        "* So here, both the origin point and the velocity vector are both defined using the 3 dimensional space, even though we are only studying 2 dimensional spherical surface\n",
        "\n",
        "* **This is why it's all called exterior geometrty and exterior product** - studying something outside\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_113.png)"
      ],
      "metadata": {
        "id": "zH_Dj6_LEh_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to find the distance between 2 points on a map of earth, it's called \"intrinsiv geometry\" when we are not allowed to leave the surface (and hence cannot draw a straight line, but need to find a geodesic on the flat map).\n",
        "\n",
        "And that gets even more complicated in General Relativity:\n",
        "\n",
        "1. 4D spacetime is curved space, so we cannot draw a straight line int he curved space. So that means we cant draw poisiton vectors.\n",
        "\n",
        "2. And we cannot pick an origin outside the 4D spacetime, because that would mean picking a point outside of the universe\n",
        "\n",
        "That's why its called intrinsic geometry: you need to stay inside! **But how do we study velocity if we are not allowed to use position vectors?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_114.png)"
      ],
      "metadata": {
        "id": "Y8Mr4AgTEjud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution: we cannot draw straight lines, but we can draw curved paths on a 2D on a map for exmaple**.\n",
        "\n",
        "> **So we can't use normal position vectors to talk about directions on a surface, but we can use derivative operators to talk about different directions:**\n",
        "\n",
        "* If have have some path that's traveling around in our curved space $(x(\\lambda), y(\\lambda))$, we can still consider the direction that the path is pointing in using the derivative with respect to the curve parameter $\\frac{d}{d \\lambda}$\n",
        "\n",
        "* And we can break this direction up into its x and y components using the multivariable chain rules that we have for derivative operators $\\frac{d}{d \\lambda}=\\frac{d x}{d \\lambda} \\frac{\\partial}{\\partial x}+\\frac{d y}{d \\lambda} \\frac{\\partial}{\\partial y}$\n",
        "\n",
        "* But Achtung: you shouldn't think of this direction vector as actually connecting the two points on the earth (origin and destination of yellow arrow). The derivative just gives the general direction that the curve is traveling in at a given point.\n",
        "\n",
        "* So derivatives with us a way to talk about directions on a curved surface in a way that is complete intrinsic to the surface and doesn't require an outside space in any way.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_115.png)"
      ],
      "metadata": {
        "id": "cabaMULeElaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same approach in 4 D space: we can't draw straight lines, but we can still draw curved paths. And that means we can take derivatives with respect to the paths parameters to get a sense of different directions.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_116.png)"
      ],
      "metadata": {
        "id": "PFhrRHMWEnRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> So the old notation uses actual vectors from the **vector spaces R2 and R3** to define directions. But the new notation use the **vector space of derivative operators**.\n",
        "\n",
        "> **The vector space of derivative operators is formally known the \"Tangent Vector Space\" and is denoted: $T_{p}M$**, which is the vector space of derivatives at some point p on a surface $M$. And keep in mind that the vectors on the left and the vectors on the right are in fact from different vector spaces!!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_117.png)\n",
        "\n",
        "And they do form a vector space because we can scale and add them linearly! So these partial derivatives are vectors in the tangent space $T_{p}M$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_118.png)"
      ],
      "metadata": {
        "id": "dvn3ZKheEpKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Metric Tensor Field*"
      ],
      "metadata": {
        "id": "QHq7RwnYCe8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Metric Tensor to compute arc length of a curve in Flat & Curved Space**\n",
        "\n",
        "**In Flat Space**\n",
        "\n",
        "So in summary the equation for calculating the arc length of a curve is an integral that depends on the magnitude of the curves tangent vectors $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|$ in this:\n",
        "\n",
        "> $\\operatorname{arclength}=\\int\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\| d \\lambda$\n",
        "\n",
        "And to calculate the squared magnitude of the tangent vectors we need to use the dot product\n",
        "\n",
        "> $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|^{2}=\\frac{d \\vec{R}}{d \\lambda} \\cdot \\frac{d \\vec{R}}{d \\lambda}$\n",
        "\n",
        "And we end up with this equation in Cartesian coordinates:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$\n",
        "\n",
        "And this equation in polar coordinates:\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p^{j}}\\right)$\n",
        "\n",
        "And the basis vector dot products $\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$ and $\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p j}\\right)$ give us the components of the metric tensor:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda} g_{i j}$\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda} \\widetilde{g_{i j}}$\n",
        "\n",
        "\n",
        "**So the key to getting the arc length of a curve is the tangent vector magnitude. And the key to getting the tangent vector magnitude is the metric tensor components $g_{i j}$ and $\\widetilde{g_{i j}}$**.\n",
        "\n",
        "And remember: the metric tensor is a (0,2) tensor because its components obey 2 covariant transformation laws:\n",
        "\n",
        "> $\\begin{aligned} \\widetilde{g_{i j}} &=\\frac{\\partial c^{k}}{\\partial p^{i}} \\frac{\\partial c^{l}}{\\partial p^{j}} g_{k l} \\\\ g_{k l} &=\\frac{\\partial p^{i}}{\\partial c^{k}} \\frac{\\partial p^{j}}{\\partial c^{l}} \\widetilde{g_{i j}} \\end{aligned}$\n",
        "\n",
        "In flat space there is only 1 metric tensor with which you can calculate the arc length of any curve as long as we can do this integral.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_150.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=BbQmTmSzUCI&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=13\n"
      ],
      "metadata": {
        "id": "L3N2cBc6FoEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Curved Space**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_142.png)\n",
        "\n",
        "In curved space: Vector field: vector changes from point to point. Covector fields have covectors that change from point to point. And now a metric tensor field involves a different metric tensor being placed everywhere in space that change from point to point.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_151.png)\n",
        "\n",
        "Every curved space has its own metric tensor field that gives you the rules for measuring distance on it:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_152.png)\n",
        "\n",
        "Metric tensors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_143.png)\n",
        "\n",
        "Simplify them:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_144.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_145.png)\n",
        "\n",
        "For the intrinsic view (left side only) we can remove $R$ vectors completely and treat derivative operators themselves as the vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_146.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=SmjbpIgVKFs&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=14"
      ],
      "metadata": {
        "id": "yS56QI6iGRym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdr6F1zuU4MG"
      },
      "source": [
        "##### <font color=\"blue\">*Variationsanalyse*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AR9PIXysIa4"
      },
      "source": [
        "###### *Variational Principle*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Mathematisch gesehen ist die Wirkung ein Funktional. W√§hrend Funktionen bestimmten Zahlen andere Zahlen zuordnen, ordnen Funktionale bestimmten Funktionen Zahlen zu.\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "7qhLvSkEBgaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXIblPI6_-oP"
      },
      "source": [
        "**Variationsrechnung**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Variationsrechnung\n",
        "\n",
        "* calculus og variation, khan academy https://youtube.com/playlist?list=PLdgVBOaXkb9CD8igcUr9Fmn5WXLpE8ZE_\n",
        "\n",
        "* **Find [stationary points](https://internal.ncl.ac.uk/ask/numeracy-maths-statistics/core-mathematics/calculus/stationary-points.html) (=derivative is zero, local minima or maxima) of a functional, like an integral I[f] (=here for example the path lenghts, or time spent travelling) is minimal between two points a and b.**\n",
        "\n",
        "  * A stationary point of a function $f(x)$ is a point where the derivative of $f(x)$ is equal to 0 .\n",
        "  * These points are called \"stationary\" because at these points the function is neither increasing nor decreasing.\n",
        "  * Graphically, this corresponds to points on the graph of $f(x)$ where the tangent to the curve is a horizontal line.\n",
        " * The stationary points of a function $y=f(x)$ are the solutions to $\n",
        "\\frac{d y}{d x}=0 $. This repeats in mathematical notation the definition given above: \"points where the gradient of the function is zero\".\n",
        "\n",
        "* **The integral is a functional (=function of functions), its stationary point is a fix point / minima of a functional (not function). Solve (usually differential) equations for stationary function f(x) (via calculus of variations)**\n",
        "\n",
        "* from regular calculus to calculus of variations: find stationary functions, not only stationary points, a function becomes a functional.\n",
        "\n",
        "* **Typical problem in variational calculus: find minimal path between points A and B, not necessarily a linear one (in physics for examples check Brachistochrone !)**.\n",
        "\n",
        "* Also consider that velocity depending on position changes the minimum paths or time to travel (later in vector analysis relevant for Kurvenintegral)\n",
        "\n",
        "In general, Calculus of variations seeks to find y = f(x) such that this integral:\n",
        "\n",
        "> $I[f]=\\int_{x_{1}}^{x_{2}} F\\left(x, y, \\frac{d y}{d x}\\right) d x$\n",
        "\n",
        "is stationary (ps: $\\frac{d y}{d x}$ = $y'$)\n",
        "\n",
        "1. Die [Variationsrechnung](https://de.wikipedia.org/wiki/Variationsrechnung) ist eine **Erweiterung der Funktionalanalysis und beschaeftigt sich mit <u>nichtlinearen Funktionalen</u>** (in der Funktionalanalysis sind es linear Funktionale)\n",
        "\n",
        "2. The [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations) is a field that **uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals**: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. <u>**Functions that maximize or minimize functionals may be found using the Euler‚ÄìLagrange equation of the calculus of variations.**</u>\n",
        "\n",
        "* In calculus of variations we are **NOT concerned with finding fix points of functions (like local maxima in a function), but rather fix points of functionals.**\n",
        "\n",
        "\n",
        "* dann f√ºhrt eine Variation der Wirkung: https://de.m.wikipedia.org/wiki/Feldtheorie_(Physik)#Formalismus\n",
        "\n",
        "* Beispiel: https://de.wikipedia.org/wiki/Fluiddynamik\n",
        "\n",
        "* Martin: formulier problem in variationelle formulierung (dann bist du in sobolove r√§ume), und dann Eigenschaften von Testfunktionen ausnutzen\n",
        "\n",
        "* **Variation der Elemente**: die [Variation der Elemente](https://de.wikipedia.org/wiki/Variation_der_Elemente) ist eine im 19. Jahrhundert entwickelte Methode zur genauen Bahnbestimmung von Himmelsk√∂rpern. Sie dient bis heute zur Modellierung von [Bahnst√∂rungen](https://de.wikipedia.org/wiki/Bahnst√∂rung).\n",
        "\n",
        "* **History of variational principles in physics**:\n",
        "https://en.m.wikipedia.org/wiki/History_of_variational_principles_in_physics\n",
        "\n",
        "* [G√¢teaux-Differential](https://de.wikipedia.org/wiki/G√¢teaux-Differential) ist eine **Verallgemeinerung des gew√∂hnlichen Differentiationsbegriffes** dar, indem es die Richtungsableitung auch in unendlichdimensionalen R√§umen definiert.\n",
        "\n",
        "* Variational method in quantum mechanics: In quantum mechanics, the [variational method](https://en.m.wikipedia.org/wiki/Variational_method_(quantum_mechanics)) is one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states. This allows calculating approximate wavefunctions such as molecular orbitals. The basis for this method is the variational principle.\n",
        "\n",
        "Die Variationsrechnung besch√§ftigt sich mit der Minimierung bzw. Maximierung von Funktionalen, die als Integral dargestellt werden k√∂nnen. Man k√∂nnte sie daher als ‚Äûnat√ºrliche‚Äú Methode zur L√∂sung physikalischer Probleme bezeichnen, da die Physik ja bekanntlich von Extremalprinzipen regiert wird (k√ºrzeste Bahn, kleinste Wirkung, Gesamtenergie, Hamilton-Funktion). Die ‚ÄûVariation‚Äú dieser Integralbeziehung bez√ºglich einer abh√§ngigen Gr√∂√üe (in der Physik z.B. der Bahnkurve im Zustandsraum) f√ºhrt auf eine Differentialgleichung, deren L√∂sung diesen Integralausdruck minimiert respektive maximiert.\n",
        "\n",
        "https://link.springer.com/chapter/10.1007/978-3-642-83621-3_7\n",
        "\n",
        "Der Name Variationsrechnung bezieht sich dabei auf die Technik der Variation der Argumente. Wesentliches Ziel der Variationsrechnung ist das Finden von Extrema (haufig unter Nebenbedingungen) f√ºr ein gegebenes Funktional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqXlKua2Mj8"
      },
      "source": [
        "**Variational Principle**\n",
        "\n",
        "* a [variational principle](https://en.wikipedia.org/wiki/Variational_principle) is one that enables a problem to be solved using calculus of variations, which concerns finding such functions which optimize the values of quantities that depend upon those functions.\n",
        "\n",
        "* For example, the problem of determining the shape of a hanging chain suspended at both ends‚Äîa catenary‚Äîcan be solved using variational calculus, and in this case, the variational principle is the following: The solution is a function that minimizes the gravitational potential energy of the chain.\n",
        "\n",
        "* Any physical law which can be expressed as a variational principle describes a **self-adjoint operator.** These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/1/10/Total_variation.gif)\n",
        "\n",
        "*As the green ball travels on the graph of the given function, the length of the path travelled by that ball's projection on the y-axis, shown as a red ball, is the total variation of the function.*\n",
        "\n",
        "* the [total variation](https://en.wikipedia.org/wiki/Total_variation) identifies several slightly different concepts, related to the (local or global) structure of the codomain of a function or a measure. For a real-valued continuous function f, defined on an interval [a, b] ‚äÇ ‚Ñù, its total variation on the interval of definition is a measure of the one-dimensional [arclength](https://en.wikipedia.org/wiki/Arc_length) of the curve with parametric equation x ‚Ü¶ f(x), for x ‚àà [a, b].\n",
        "\n",
        "* In der Variationsrechnung und der Theorie der stochastischen Prozesse ist die [Variation](https://de.wikipedia.org/wiki/Variation_(Mathematik)) (auch totale Variation genannt) einer Funktion **ein Ma√ü f√ºr das lokale Schwingungsverhalten der Funktion**.\n",
        "\n",
        "* Bei den stochastischen Prozessen ist die Variation von besonderer Bedeutung, da sie die Klasse der zeitstetigen Prozesse in zwei fundamental verschiedene Unterklassen unterteilt: jene mit endlicher und solche mit unendlicher Variation.\n",
        "\n",
        "Die [erste Variation](https://de.wikipedia.org/wiki/Erste_Variation) ist eine verallgemeinerte Richtungsableitung eines Funktionals. Ihre Eigenschaften sind in der angewandten Mathematik und der theoretischen Physik relevant. Die erste Variation spielt eine zentrale Rolle in der Variationsrechnung und wird in der analytischen Mechanik genutzt. Ein verwandtes Konzept ist die Funktionalableitung.\n",
        "\n",
        "In der Analysis ist eine Funktion von [beschr√§nkter Variation](https://de.wikipedia.org/wiki/Beschr√§nkte_Variation) (beschr√§nkter Schwankung), wenn ihre totale Variation (totale Schwankung) endlich ist, sie also in gewisser Weise nicht beliebig stark oszilliert. Diese Begriffe h√§ngen eng mit der Stetigkeit und der Integrierbarkeit von Funktionen zusammen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anwendungsgebiete**\n",
        "\n",
        "* Die Variationsrechnung ist die mathematische Grundlage aller physikalischen Extremalprinzipien und deshalb besonders in der theoretischen Physik wichtig, so etwa\n",
        "\n",
        "  * im Lagrange-Formalismus der klassischen Mechanik\n",
        "\n",
        "  * bzw. der Bahnbestimmung, in der Quantenmechanik in Anwendung des Prinzips der kleinsten Wirkung\n",
        "\n",
        "  * und in der statistischen Physik im Rahmen der Dichtefunktionaltheorie.\n",
        "\n",
        "  * In der Mathematik wurde die Variationsrechnung beispielsweise bei der riemannschen Behandlung des Dirichlet-Prinzips f√ºr harmonische Funktionen verwendet.\n",
        "\n",
        "  * Auch in der Steuerungs- und Regelungstheorie findet die Variationsrechnung Anwendung, wenn es um die Bestimmung von Optimalreglern geht.\n",
        "\n",
        "* Ein typisches Anwendungsbeispiel ist das Brachistochronenproblem: Auf welcher Kurve in einem Schwerefeld von einem Punkt A zu einem Punkt B, der unterhalb, aber nicht direkt unter A liegt, ben√∂tigt ein Objekt die geringste Zeit zum Durchlaufen der Kurve? Von allen Kurven zwischen A und B minimiert eine den Ausdruck, der die Zeit des Durchlaufens der Kurve beschreibt. Dieser Ausdruck ist ein Integral, das die unbekannte, gesuchte Funktion, die die Kurve von A nach B beschreibt, und deren Ableitungen enth√§lt."
      ],
      "metadata": {
        "id": "A4hJCXFKFMa9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkRUt1ihCd_"
      },
      "source": [
        "**Fundamentallemma der Variationsrechnung**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Fundamentallemma_der_Variationsrechnung\n",
        "\n",
        "**Fundamentalsatz der Variationsrechnung**\n",
        "\n",
        "* Fundamental Theorem of the Calculus of Variations - [Fundamentalsatz der Variationsrechnung](https://de.wikipedia.org/wiki/Fundamentalsatz_der_Variationsrechnung)\n",
        "\n",
        "* eng verwandt mit dem [weierstra√üschen Satz vom Minimum](https://de.wikipedia.org/wiki/Satz_vom_Minimum_und_Maximum)\n",
        "\n",
        "* Er behandelt die in der Variationsrechnung zentrale Frage, unter welchen Bedingungen reellwertige Funktionale ein Minimum annehmen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eloiR80j3LKg"
      },
      "source": [
        "###### *Brachistochrone & Tautochronie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNCf_416SQR"
      },
      "source": [
        "[**Brachistochrone Curve**](https://en.wikipedia.org/wiki/Brachistochrone_curve) (in rot): Der K√∂rper gleitet auf einer solchen Bahn schneller zum Ziel als auf jeder anderen Bahn, beispielsweise auf einer geradlinigen, obwohl diese k√ºrzer ist.\n",
        "\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/6/63/Brachistochrone.gif)\n",
        "\n",
        "* Brachistochrone: Path between 2 points $A$ and $B$ which minimizes the time taken by a particle falling from $A$ to $B$ under the influence of gravity.\n",
        "\n",
        "* Time = distance / speed. Goal: Mix of minimize distance and maximize speed\n",
        "\n",
        "* Johann I Bernoulli hat sich mit dem **Problem des schnellsten Falles** besch√§ftigt. Im Jahre 1696 fand er schlie√ülich die L√∂sung in der **Brachistochrone**. Heute sieht man dies oft als die **Geburtsstunde der Variationsrechnung**.\n",
        "\n",
        "* Video: [The Brachistochrone Problem and Solution | Calculus of Variations](https://www.youtube.com/watch?v=zYOAUG8PxyM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvO3MMS6JFg"
      },
      "source": [
        "**Tautochronie** der Brachistochrone ‚Äì von jedem Startpunkt auf der Kurve erreichen die Kugeln das ‚ÄûZiel‚Äú gleichzeitig.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/b/bd/Tautochrone_curve.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Bewegungsgleichungen (Newton)*"
      ],
      "metadata": {
        "id": "sprbeM9PikBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hintergrund: Bewegungsgleichungen**\n",
        "\n",
        "* Unter einer [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt.\n",
        "\n",
        "* In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung (=Beschleunigung / Acceleration)\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "> **Es gibt drei Ans√§tze f√ºr Bewegungsgleichungen: Newtonian Mechanics, Lagrangian and Hamiltonian. F√ºr letztere beiden gilt the Principle of Least Action.**"
      ],
      "metadata": {
        "id": "hXh4vP1EJIiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newtonsche Gesetze**\n",
        "\n",
        "Die [Newtonschen Gesetze](https://de.m.wikipedia.org/wiki/Newtonsche_Gesetze) (Fundamentalkonzept der klassischen Mechanik, Extremalprinzip des Wirkungsfunktionals) gelten als die Grundlage der klassischen Mechanik, auf der alle weiteren Modelle basieren. Zentrales Konzept dieser Formulierung ist die Einf√ºhrung von Kr√§ften, die eine Beschleunigung $\\ddot{\\vec{x}}$ einer Masse $m$ hervorrufen. Die Bewegungsgleichung dieser Masse wird bestimmt durch die √úberlagerung der Kr√§fte $\\vec{F}_{i}$, die auf die Masse wirken\n",
        "\n",
        "> $\n",
        "m \\ddot{\\vec{x}}=\\sum_{i=1}^{N} \\vec{F}_{i}\n",
        "$\n",
        "\n",
        "1. Ein kr√§ftefreier K√∂rper bleibt in Ruhe oder bewegt sich geradlinig mit konstanter Geschwindigkeit (siehe [Tr√§gheit](https://de.m.wikipedia.org/wiki/Tr√§gheit#Bedeutung_f√ºr_wichtige_Prinzipien_der_Mechanik))\n",
        "\n",
        "2. Kraft gleich Masse mal Beschleunigung. $(\\vec{F}=m \\cdot \\vec{a})$ $\\rightarrow$ Equation of Motion (2. Newtonsches Gesetz)\n",
        "\n",
        "3. Kraft gleich Gegenkraft: Eine Kraft von K√∂rper A auf K√∂rper B geht immer mit einer gleich gro√üen, aber entgegen gerichteten Kraft von K√∂rper B auf K√∂rper A einher.\n",
        "\n",
        ">$\n",
        "\\vec{F}_{A \\rightarrow B}=-\\vec{F}_{B \\rightarrow A}\n",
        "$\n",
        "\n",
        "**Bewegungsgleichungen**\n",
        "\n",
        "* eine [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) ist eine Gleichung, die die Entwicklung eines mechanischen Systems bei √§u√üeren Einfl√ºssen beschreibt\n",
        "\n",
        "* Unter einer Bewegungsgleichung versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die r√§umliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung √§u√üerer Einfl√ºsse vollst√§ndig beschreibt. In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung.\n",
        "\n",
        "* Diese Differentialgleichungen sind f√ºr viele Systeme nicht analytisch l√∂sbar, sodass man bei der L√∂sung geeignete N√§herungsverfahren anwenden muss.\n",
        "\n",
        "* L√∂sung: Die L√∂sung der Bewegungsgleichung ist die [Trajektorie](https://de.m.wikipedia.org/wiki/Trajektorie_(Physik)), auf der sich das System bewegt. Sie ist, abgesehen von einigen einfachen F√§llen (siehe Beispiele unten), meist nicht in analytisch geschlossener Form darstellbar und muss √ºber [numerische Methoden](https://de.m.wikipedia.org/wiki/Numerische_Mathematik) gewonnen werden. Dies ist z. B. zur Ermittlung der Trajektorien dreier Himmelsk√∂rper, die sich gegenseitig gravitativ anziehen, erforderlich (siehe [Dreik√∂rperproblem](https://de.m.wikipedia.org/wiki/Dreik%C3%B6rperproblem)). Zur L√∂sung eines N-Teilchensystems l√§sst sich die [discrete element method](https://de.m.wikipedia.org/wiki/Discrete_element_method) anwenden. In einfachen F√§llen wird die geschlossene L√∂sung als ‚ÄûBahngleichung‚Äú bezeichnet.\n",
        "\n",
        "**Newtonsche Axiome**\n",
        "\n",
        "Prinzipen: Zum Aufstellen von Bewegungsgleichungen in der klassischen Physik wird verwendet:\n",
        "\n",
        "* das 2. Newtonsche Gesetz,\n",
        "* der Lagrange-Formalismus oder\n",
        "* der Hamilton-Formalismus\n",
        "\n",
        "Darauf basierend ergibt sich die Bewegungsgleichung der Quantenmechanik, die Schr√∂dingergleichung.\n",
        "\n",
        "**In der Technischen Mechanik werden verwendet**:\n",
        "\n",
        "* das Prinzip der virtuellen Arbeit (D‚ÄôAlembertsches Prinzip)\n",
        "* das Prinzip der virtuellen Leistung (Prinzip von Jourdain)\n",
        "* das Prinzip des kleinsten Zwanges.\n",
        "\n"
      ],
      "metadata": {
        "id": "mi3ntZBhiiWf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYzlWwuA8hXl"
      },
      "source": [
        "###### *Principle of Least / Stationary Action (Lagrange-Formalismus)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange-Formalismus**\n",
        "\n",
        "Der [Lagrange-Formalismus](https://de.m.wikipedia.org/wiki/Lagrange-Formalismus) beschreibt die Gesetze der klassischen Mechanik durch die Lagrange-Funktion $L$, die f√ºr Systeme mit einem generalisierten Potential und holonomen [Zwangsbedingungen](https://de.m.wikipedia.org/wiki/Zwangsbedingung) als Differenz aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben ist:\n",
        "\n",
        ">$\n",
        "L=T-V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der Euler-Lagrange-Gleichungen, die die Ableitungen nach der Zeit $t$, den Geschwindigkeiten $\\dot{q}_{i}$ und den [generalisierten Koordinaten](https://de.m.wikipedia.org/wiki/Generalisierte_Koordinate) $q_{i}$ miteinander in Verbindung setzt:\n",
        "\n",
        ">$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial L}{\\partial \\dot{q}_{i}}=\\frac{\\partial L}{\\partial q_{i}}\n",
        "$\n",
        "\n",
        "> Action $S$ = Kinetic Energy - Potential Energy = $\\int (T - V) dt$ = $\\int (\\frac{1}{2} mv^2 - mgh) dt$\n",
        "\n",
        "$L$ is the Lagrangian of the particle: $L=E_{u}-U$.\n",
        "\n",
        "*Need to solve Lagrange equations to make $S$ stationary:*\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{1}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{1}}\\right)$\n",
        "\n",
        ">$\\frac{\\partial L}{\\partial q_{2}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{2}}\\right)$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{3}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{3}}\\right) \\quad \\begin{array}{c}\\\\ \\end{array}$\n",
        "\n",
        "We can use a general coordinate system:\n",
        "$\\\\ {\\left[q_{1}(t), q_{2}(t), q_{3}(t)\\right]}$\n",
        "\n",
        "*Lagrange-Gleichungen erster Art*\n",
        "\n",
        "* Mit den Lagrange-Gleichungen erster Art lassen sich die Zwangskr√§fte berechnen.\n",
        "\n",
        "* Wenn man annimmt, dass sich die √§u√üeren Kr√§fte aus einem Potential ableiten lassen, kann man die Bewegungsgleichung schreiben (Lagrange-Gleichung 1. Art):\n",
        "\n",
        "*Lagrange-Gleichungen zweiter Art*\n",
        "\n",
        "* Die Lagrange-Gleichungen zweiter Art ergeben sich als sogenannte Euler-Lagrange-Gleichungen eines Variationsproblems und liefern die Bewegungsgleichungen, wenn die Lagrange-Funktion gegeben ist.\n",
        "\n",
        "* Sie folgen aus der Variation des mit der Lagrange-Funktion gebildeten Wirkungsintegrals im Hamiltonschen Prinzip.\n",
        "\n",
        "Eingef√ºhrte Formulierung der klassischen Mechanik, in der die Dynamik eines Systems durch **eine einzige skalare Funktion, die Lagrange-Funktion**, beschrieben wird. Der Formalismus ist (im Gegensatz zu der newtonschen Mechanik, die a priori nur in Inertialsystemen gilt) auch in beschleunigten Bezugssystemen g√ºltig. Der Lagrange-Formalismus ist invariant gegen Koordinatentransformationen.\n",
        "\n",
        "Der [Langrange Formalismus](https://de.wikipedia.org/wiki/Lagrange-Formalismus) ist eine m√∂gliche (von vielen!) Formulierung der klassischen Mechanik. Hier wird die Dynamik eines Systems durch die Langrange-Funktion L(${\\boldsymbol{q}}$, $\\dot{\\boldsymbol{q}}$, $t$) beschrieben:\n",
        "\n",
        "* ${\\boldsymbol{q}}$ = (q1, q2, ..., qw) - allgemeine Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ = (d / dt) q ... ($\\dot{\\boldsymbol{q}}$1, $\\dot{\\boldsymbol{q}}$2.. $\\dot{\\boldsymbol{q}}$n) - Vektor der Wirkung (allgemeine Geschwindigkeiten)\n",
        "* $t$ - Zeit (Die explizite Zeitabh√§ngige ber√ºcksichtigt externe, zeitabh√§ngige Faktoren (z.B. Magnet-Felder etc). Wird auf Null gesetzt, denn bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "> $L_{x}(q, \\dot{q}, t)$ wird zu: $L_{x}(q, \\dot{q})$ = $T$<sub>kin</sub> - $V$<sub>pot</sub>"
      ],
      "metadata": {
        "id": "BR8DkOZUtAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Principle of Least Action or Stationary Action*\n",
        "\n",
        "> Action  ùëÜ  = Kinetic Energy - Potential Energy\n",
        "\n",
        "* Principle of Least Action = better: [Stationary Action](https://en.m.wikipedia.org/wiki/Stationary-action_principle) = Wirkung\n",
        "\n",
        "https://youtu.be/dPxhTiiq-1A\n",
        "\n",
        "[The Principle of Stationary Action](https://www.youtube.com/watch?v=M05ixbSOY80): If a particle/system $P$ travels from one point to another in the time interval $\\left[t_{1}, t_{2}\\right]$, the path the particle traverses is such that this function:\n",
        "\n",
        "> $\n",
        "S=\\int_{t_{1}}^{t_{2}} \\mathcal{L} \\text { dt}$\n",
        "\n",
        "is stationary.\n",
        "\n",
        "* Total energy = kinetic energy + potential energy.\n",
        "\n",
        "> **Kinetic - potential energy = Lagrangian $L$**\n",
        "\n",
        "> has no physical meaning !! It's a ver useful mathematical tool\n",
        "\n",
        "* Kinetic energy depends on velocity of a particle (detonated with a dot over x,y,z), potential energy depends on position of a particle x,y,z. Hence the Lagrangian of a particle depends on all of the positions and all of their time derivatives.\n",
        "\n",
        "* **Use and solve the three Lagrangian equations in order to determine the equation of motion of a particle (and Lagrange equations are equivalent to Newton's second law)**\n",
        "\n",
        "* And they can easily applied to other coordinate systems than cartesian (i.e cylindric, or spherical)\n",
        "\n",
        "* Also: **Lagrange equations are very similar to Euler-Lagrange Equations!**\n",
        "\n",
        "* Because the three Lagrange equations very strongly resemble the Euler-Lagrange-equation, there must be some functional that's being made stationary by Lagrange equations\n",
        "\n",
        "* **$S$ is the action integral (= a functional!)**\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_01.png)\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_02.png)"
      ],
      "metadata": {
        "id": "rfHQAhk-EqQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Euler-Lagrange Equation*"
      ],
      "metadata": {
        "id": "puX57AODxaUG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_F2rBw5bk4"
      },
      "source": [
        "**Euler-Lagrange Equation**\n",
        "\n",
        "*Why using Euler-Lagrange Equation?*\n",
        "\n",
        "1) lagrangian mechanies gives equation of motion without considering forces at all, only energy\n",
        "\n",
        "2) This is more convenient for complicated systems with multiple forces to be considered\n",
        "\n",
        "3) Great for dealing with multiple coordinate\n",
        "\n",
        "*How to get the Euler-Lagrange Equation?*\n",
        "\n",
        "* **Step 1: Lagrangian equation**: L = T (kinetic energy) - V (potential energy)\n",
        "\n",
        "  * T = $\\frac{1}{2}m \\dot x ^2 $\n",
        "\n",
        "    * with $\\dot x$ short for: $\\frac{dx}{dt}$\n",
        "\n",
        "  * V = $\\frac{1}{2}k x ^2 $\n",
        "\n",
        "  * so: L = $\\frac{1}{2}m \\dot x ^2 $ - $\\frac{1}{2}k x ^2 $ -> This is our Lagrangian for a specific system !\n",
        "\n",
        "* **Step 2: Now take Euler‚ÄìLagrange Equation**: $\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot q}\\right)=\\frac{\\partial L}{\\partial q}$\n",
        "\n",
        "  * we see it contains the Lagrangian\n",
        "\n",
        "  * it's consistent with Newtonian classical mechanics F=ma etc\n",
        "\n",
        "  * we can plug in our Lagrangian for a specific system (replace L with formula above). we get the equation of motion!\n",
        "\n",
        "* **Step 3: Equation of Motion**: $m \\ddot x = -kx$\n",
        "\n",
        "  * mass m of an object multipliplied by acceleration $\\ddot x$ (which is the second derivative of x, the whole left side is same as newton's: f=m*a)\n",
        "\n",
        "  * we are stating something about the forces acting on the system\n",
        "\n",
        "*More about Euler‚ÄìLagrange equation*\n",
        "\n",
        "* Langrangian = Kinetic Energy - Potential Engergy\n",
        "\n",
        "* then insert it into the **Euler-Langrange Equation**:\n",
        "\n",
        "> $\\frac{d}{d t} \\frac{\\partial L}{\\partial \\dot{\\theta}}=\\frac{\\partial L}{\\partial \\theta}$\n",
        "\n",
        "* the Euler-Langrange Equation is the condition of the action $S$ to be minimized, where action is integral of Lagrangian\n",
        "\n",
        "* Means: of all the possible paths a particle could follow, the actual path it chooses is the one that minimizes (or actually \"extremizes\") the action = **principle of least action**\n",
        "\n",
        "* with Lagrangian we don't need any vectors anymore like in Newtonian, we can sue whatever coordinates. Also it makes it easier to deal with constraints and understands symmetries\n",
        "\n",
        "* F = ma in the Euler Lagrange Equation gives us a single second-order differential equation\n",
        "\n",
        "The Euler‚ÄìLagrange equation is an equation satisfied by a function q of a real argument t, which is a stationary point of the functional:\n",
        "\n",
        "> $S(\\boldsymbol{q})=\\int_{a}^{b} L(t, \\boldsymbol{q}(t), \\dot{\\boldsymbol{q}}(t)) \\mathrm{d} t$\n",
        "\n",
        "* ${\\boldsymbol{q}}$ - Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ - Vektor der Wirkung\n",
        "* t - Zeit (wird auf Null gesetzt, den bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "The Euler‚ÄìLagrange equation, then, is given by\n",
        "\n",
        "> $L_{x}(t, q(t), \\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d} t} L_{v}(t, q(t), \\dot{q}(t))=0$\n",
        "\n",
        "* partial derivative of one dimension, then second dimension and then time\n",
        "\n",
        "* the [Euler equation](https://en.m.wikipedia.org/wiki/Euler‚ÄìLagrange_equation) is a **second-order partial differential** equation whose **solutions are the functions for which a given functional is stationary**.\n",
        "\n",
        "* Because **a differentiable functional is stationary at its local extrema**, the Euler‚ÄìLagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing or maximizing it.\n",
        "\n",
        "* This is analogous to [Fermat's theorem](https://en.m.wikipedia.org/wiki/Fermat%27s_theorem_(stationary_points)) in calculus, stating that at any point where a differentiable function attains a local extremum its derivative is zero.\n",
        "\n",
        "* In Lagrangian mechanics, according to [Hamilton's principle](https://en.m.wikipedia.org/wiki/Hamilton%27s_principle) of stationary action, the evolution of a physical system is described by the solutions to the Euler equation for the action of the system. In this context Euler equations are usually called Lagrange equations. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations.\n",
        "\n",
        "  * Hamilton's principle is William Rowan Hamilton's formulation of the [principle of stationary action](https://en.m.wikipedia.org/wiki/Principle_of_least_action) (also called: 'Principle of least action'). It states that the **dynamics of a physical system are determined by a variational problem for a functional based on a single function**, the Lagrangian, which may contain all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hamiltonian Function*"
      ],
      "metadata": {
        "id": "SqYBnfUAD1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamilton'sches Prinzip (Wirkungsfunktional)*\n",
        "\n",
        "> Das [Hamilton'sche Prinzip](https://de.wikipedia.org/wiki/Hamiltonsches_Prinzip) zeichnet tatsachlich durchlaufene Bahnen dadurch aus, **dass bei ihnen die Wirkung (=Funktional) S[q] (verglichen mit anderen Bahnen) ein Minimum annimmt (minimal variation ??)**.\n",
        "\n",
        "* Das Hamiltonsche Prinzip der Theoretischen Mechanik ist ein **Extremalprinzip**. Physikalische Felder und Teilchen nehmen danach f√ºr eine bestimmte Gr√∂√üe einen extremalen (d. h. gr√∂√üten oder kleinsten) Wert an. Diese Bewertung nennt man Wirkung (=Action), mathematisch ist die Wirkung ein Funktional, daher auch die Bezeichnung **Wirkungsfunktional**.\n",
        "\n",
        "* Die Wirkung erweist sich in vielen F√§llen nicht als minimal, sondern nur als **‚Äûstation√§r‚Äú** (d. h. extremal). Deshalb wird das Prinzip von manchen Lehrbuchautoren auch das Prinzip der **station√§ren Wirkung** genannt. Manche Autoren nennen das Hamiltonsche Prinzip auch **'Prinzip der kleinsten Wirkung'**, was jedoch ‚Äì wie oben ausgef√ºhrt ‚Äì nicht pr√§zise ist.\n",
        "\n",
        "* Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation:\n",
        "\n",
        "> $\\frac{\\delta \\mathcal{S}}{\\delta \\mathbf{q}(t)}=0$\n",
        "\n",
        "* That is, the system takes a path in configuration space for which the action is stationary, with fixed boundary conditions at the beginning and the end of the path.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Least_action_principle.svg/500px-Least_action_principle.svg.png)\n",
        "\n",
        "*As the system evolves, q traces a path through configuration space (only some are shown). The path taken by the system (red) has a stationary action (Œ¥S = 0) under small changes in the configuration of the system (Œ¥q).*"
      ],
      "metadata": {
        "id": "aH2WDPHcw04D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamiltonsche Mechanik & Hamilton-Funktion*\n",
        "\n",
        "> Die [**Hamilton-Funktion**](https://de.m.wikipedia.org/wiki/Hamilton-Funktion) eines Systems von Teilchen ist (,wenn keine rheonomen (d. h. zeitabh√§ngigen) Zwangsbedingungen vorliegen), **die Gesamtenergie als Funktion der Orte und Impulse der Teilchen** und gegebenenfalls der Zeit.\n",
        "\n",
        "* Total Energy = Kinetic Energy + Potential Energy\n",
        "\n",
        "* rewrite everything in terms of momentum, and we get the **Hamiltonian**:\n",
        "\n",
        "> $H=\\frac{p^{2}}{2 m l^{2}}-m g l \\cos \\theta$\n",
        "\n",
        "* the Hamiltonian gives us a pair of first-order differential equations for theta and pi\n",
        "\n",
        "* the generalized version is Momentum * Velocity - L (Lagrangian):\n",
        "\n",
        "> $H=P \\dot{\\theta}-L$\n",
        "\n",
        "* we get a new geometric perspective by connecting it with something known as \"flow phase space\": P (momentum) and Theta create a new vector space (pairs of them) called the \"phase space\", where I can see what the particle will do in the future. Energy is constant = particle travels along a line of constant energy\n",
        "\n",
        "Die [Hamiltonsche Mechanik](https://de.m.wikipedia.org/wiki/Hamiltonsche_Mechanik) ist die am st√§rksten verallgemeinerte Formulierung der klassischen Mechanik und Ausgangspunkt der Entwicklung neuerer Theorien und Modelle, wie der Quantenmechanik. Zentrale Gleichung dieser Formulierung ist die Hamilton-Funktion H. Sie ist folgenderma√üen definiert:\n",
        "\n",
        "> $\n",
        "H=\\sum_{i} \\dot{q}_{i} p_{i}-L(\\vec{q}, \\dot{\\vec{q}}, t)\n",
        "$\n",
        "\n",
        "Dabei sind $\\dot{q}_{i}$ die generalisierten Geschwindigkeiten und $p_{i}$ die generalisierten Impulse.\n",
        "\n",
        "> Die hamiltonschen Bewegungsgleichungen folgen aus dem hamiltonschen Prinzip der station√§ren Wirkung.\n",
        "\n",
        "Ist die potentielle Energie unabh√§ngig von der Geschwindigkeit und h√§ngen die TransformationsGleichungen, die die generalisierten Koordinaten definieren, nicht von der Zeit ab, ist die Hamilton-Funktion in der klassischen Mechanik durch die Summe aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben:\n",
        "\n",
        "> $\n",
        "H=T+V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der kanonischen Gleichungen:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\dot{q}_{i} &=\\frac{\\partial H}{\\partial p_{i}} \\\\\n",
        "\\dot{p}_{i} &=-\\frac{\\partial H}{\\partial q_{i}}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Mit dem Hamilton-Jacobi-Formalismus existiert eine modifizierte Form dieser Beschreibung, die die Hamilton-Funktion mit der Wirkung verkn√ºpft."
      ],
      "metadata": {
        "id": "ntdvyhHFuhn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operators in Classical Mechanics**\n",
        "\n",
        "[Operators in classical mechanics](https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics): In classical mechanics, the movement of a particle (or system of particles) is completely determined by the **Lagrangian** $L(q, \\dot{q}, t)$ or equivalently the **Hamiltonian** $H(q, p, t)$, a function of the generalized coordinates $q$, generalized velocities $\\dot{q}=\\mathrm{d} q / \\mathrm{d} t$ and its conjugate momenta:\n",
        "\n",
        ">$\n",
        "p=\\frac{\\partial L}{\\partial \\dot{q}}\n",
        "$\n",
        "\n",
        "If either $L$ or $H$ is independent of a generalized coordinate $q$, meaning the $L$ and $H$ do not change when $q$ is changed, which in turn means the dynamics of the particle are still the same even when q changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate $q$ is a symmetry). **Operators in classical mechanics are related to these symmetries.**\n",
        "\n",
        "More technically, when $H$ is invariant under the action of a certain group of transformations $G$ :\n",
        "\n",
        ">$\n",
        "S \\in G, H(S(q, p))=H(q, p)\n",
        "$\n",
        "\n",
        "the elements of $G$ are physical operators, which map physical states among themselves.\n",
        "\n",
        "**Connections to Quantum Mechanics**\n",
        "\n",
        "* Functions on phase phase in classical mechanics turn into operators in the quantum space of states in quantum mechanics\n",
        "\n",
        "* And if you know the state of a quantum system at time t0, the Schr√∂dinger equation says they at a later time t will be $|\\psi \\rangle$ $\\rightarrow$ $e^{-\\frac{i}{\\hbar} H t}|\\psi\\rangle$ acting on the state,\n",
        "\n",
        "* where $H$ is the operator version of the classical Hamiltonian function\n",
        "\n",
        "**Feynman Path Integral Formulation**\n",
        "\n",
        "> *Principle of Least Action in Quantum Mechanics (Path Integral Formulation)*\n",
        "\n",
        "The [path integral formulation](https://en.m.wikipedia.org/wiki/Path_integral_formulation) is a description in quantum mechanics that **generalizes the action principle of classical mechanics**. It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or functional integral, over an infinity of quantum-mechanically possible trajectories to compute a quantum amplitude.\n",
        "\n",
        "* Richard Feynman zeigte in den 1940ern, dass sich das Hamiltonsche Prinzip in der Quantenfeldtheorie gerade dadurch ergibt, dass alle m√∂glichen Pfade (auch die nicht zielgerichteten) zul√§ssig sind und aufintegriert werden. Dabei √ºberlagern sich Pfade mit extremaler Wirkung konstruktiv und davon abweichende destruktiv, so dass die Natur schlie√ülich zielgerichtet erscheint.\n",
        "\n",
        "* Principle of least action is equivalent to newtonian mechanics (one can derive the on from the other). And both is for large scale objects, **meanwhile the principle of least action is the large scale approximation of the feynman path integral on quantum objects!** source at Min 7:53 here: https://www.youtube.com/watch?v=dPxhTiiq-1A\n",
        "\n",
        "**Bewegungsgleichung der Allgemeinen Relativit√§tstheorie**\n",
        "\n",
        "Die [Bewegung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) eines K√∂rpers wird durch die Geod√§tengleichung der gekr√ºmmten Raumzeit beschrieben, sofern nur gravitative Kr√§fte auf ihn einwirken. Dann bewegt sich der K√∂rper entlang einer Geod√§ten der Raumzeit. Die Geod√§tengleichung lautet\n",
        "\n",
        "> $\n",
        "\\ddot{x}^{\\mu}+\\Gamma_{\\lambda \\nu}^{\\mu} \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=\\ddot{x}^{\\mu}+\\frac{g^{\\mu \\rho}}{2}\\left(\\partial_{\\lambda} g_{\\nu \\rho}+\\partial_{\\nu} g_{\\lambda \\rho}-\\partial_{\\rho} g_{\\lambda \\nu}\\right) \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=0\n",
        "$\n",
        "\n",
        "wobei $\\Gamma_{\\lambda \\nu}^{\\mu}$ ein Christoffelsymbol 2. Art ist, welches die Abh√§ngigkeit des metrischen Tensors vom Raumzeitpunkt (Ereignis), d. h. der Kr√ºmmung der Raumzeit, charakterisiert."
      ],
      "metadata": {
        "id": "anQlQ5k0TzaJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zER_lMlN46"
      },
      "source": [
        "###### *Special: Minimal Surfaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**minimal surface**\n",
        "\n",
        "A [minimal surface](https://en.m.wikipedia.org/wiki/Minimal_surface) is a surface that locally minimizes its area.\n",
        "\n",
        "* Intro Video: https://youtu.be/_t-3lCZXlPM\n",
        "\n",
        "* Siehe auch: https://www.chemie-schule.de/KnowHow/Oberfl√§chenspannung\n",
        "\n",
        "* This is equivalent to having **zero mean curvature = second derivative is always zero at any point**\n",
        "\n",
        "* **Minimal surfaces represent the lowest energy state!** A flat plane is minimalist surface area\n",
        "\n",
        "* Erwin Schr√∂dinger used Minimal Surfaces equations in 1926 to describe the quantum state of real phsysical systems\n",
        "\n",
        "* the apparent horizon of a back whole can be are always minimal surfaces\n",
        "\n",
        "* non trivial minimal surface: [Katenoid](https://en.m.wikipedia.org/wiki/Catenoid) and a [Helicoid (Wendelfl√§che)](https://de.m.wikipedia.org/wiki/Wendelfl√§che)\n",
        "\n",
        "* For a given constraint there may also exist several minimal surfaces with different areas (for example, see [minimal surface of revolution](https://en.m.wikipedia.org/wiki/Minimal_surface_of_revolution)): the standard definitions only relate to a local optimum, not a global optimum.\n",
        "\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Catenoid.svg/600px-Catenoid.svg.png)\n",
        "\n",
        "\n",
        "Minimal surface theory originates with Lagrange who in 1762 considered the variational problem of finding the surface z = z(x, y) of least area stretched across a given closed contour. He derived the Euler‚ÄìLagrange equation for the solution\n",
        "\n",
        "> $\\frac{d}{d x}\\left(\\frac{z_{x}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)+\\frac{d}{d y}\\left(\\frac{z_{y}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)=0$\n",
        "\n",
        "He did not succeed in finding any solution beyond the plane. In 1776 Jean Baptiste Marie Meusnier discovered that the helicoid and catenoid satisfy the equation and that the differential expression corresponds to twice the mean curvature of the surface, concluding that surfaces with zero mean curvature are area-minimizing.\n",
        "\n",
        "By expanding Lagrange's equation to\n",
        "\n",
        "> $\\left(1+z_{x}^{2}\\right) z_{y y}-2 z_{x} z_{y} z_{x y}+\\left(1+z_{y}^{2}\\right) z_{x x}=0$\n",
        "\n",
        "Gaspard Monge and Legendre in 1795 derived representation formulas for the solution surfaces. While these were successfully used by Heinrich Scherk in 1830 to derive his surfaces, they were generally regarded as practically unusable. Catalan proved in 1842/43 that the helicoid is the only ruled minimal surface.\n",
        "\n",
        "Eine Minimalfl√§che ist eine Fl√§che im Raum, die lokal minimalen Fl√§cheninhalt hat. Derartige Formen nehmen beispielsweise Seifenh√§ute an, wenn sie √ºber einen entsprechenden Rahmen (wie etwa einen Blasring) gespannt sind. In mathematischer Sprache sind Minimalfl√§chen die kritischen Punkte des Fl√§cheninhaltsfunktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\int \\sqrt{g(u)} \\mathrm{d}^{n} u$\n",
        "\n",
        "\n",
        "Hierbei sind die Gr√∂√üen $g(u):=\\operatorname{det}\\left(g_{i j}(u)\\right)_{i, j=1, \\ldots, n}$ und $g_{i j}(u)=\\left(\\frac{\\partial \\mathbf{x}}{\\partial u_{i}}\\right)^{T} \\frac{\\partial \\mathbf{x}}{\\partial u_{j}}$ f√ºr $i, j=1, \\ldots, n$  erkl√§rt (vgl. Hesse-Matrix).\n",
        "\n",
        "**Man beachte, dass eine Minimalfl√§che nicht notwendig minimalen Fl√§cheninhalt hat, sondern lediglich ein station√§rer Punkt des Fl√§cheninhaltsfunktionals ist.** Man kann zeigen, dass das Verschwinden der ersten Variation des Fl√§cheninhaltsfunktionals in zwei Raumdimensionen √§quivalent zum Verschwinden der mittleren Kr√ºmmung H ist, falls die betrachtete Mannigfaltigkeit hinreichend regul√§r ist.\n",
        "\n",
        "\n",
        "**Formulierung als Variationsproblem**\n",
        "\n",
        "Eine Fl√§che ist genau dann eine [Minimalfl√§che](https://de.m.wikipedia.org/wiki/Minimalfl√§che), wenn sie an jedem Punkt die mittlere Kr√ºmmung null hat. Damit stellt sich eine Minimalfl√§che als Spezialfall einer Fl√§che vorgeschriebener mittlerer Kr√ºmmung dar. Diese entziehen sich ebenfalls nicht der Variationsrechnung, sie sind Minima des Hildebrandtschen Funktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\iint\\left(\\left|\\mathbf{x}_{u} \\times \\mathbf{x}_{v}\\right|+2\\left(Q(\\mathbf{x}), \\mathbf{x}_{u}, \\mathbf{x}_{v}\\right)\\right) \\mathrm{d} u \\mathrm{~d} v$\n",
        "\n",
        "Die Eulerschen Gleichungen als notwendige Minimalit√§tsbedingungen dieses Funktionals sind das nach Franz Rellich benannte H-Fl√§chen-System\n",
        "\n",
        "Siehe auch: [Schwarz minimal surface](https://en.m.wikipedia.org/wiki/Schwarz_minimal_surface)\n",
        "\n",
        "$\n",
        "\\Delta \\mathbf{x}=2 H \\mathbf{x}_{u} \\times \\mathbf{x}_{v}, \\quad \\mathbf{x}_{u}^{2}-\\mathbf{x}_{v}^{2}=0=\\mathbf{x}_{u} \\mathbf{x}_{v}\n",
        "$\n",
        "\n",
        "Hierbei ist $H=\\operatorname{div} Q$ die mittlere Kr√ºmmung."
      ],
      "metadata": {
        "id": "D5Q9F5n4hpl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Analysis**"
      ],
      "metadata": {
        "id": "b8EVGNJmEfDo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRiQv6BoMmA"
      },
      "source": [
        "##### <font color=\"blue\">*Funtionalanalysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "ppApZ0cS7_F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green's Function**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Greensche_Funktion\n",
        "\n",
        "Video: [Green's functions: the genius way to solve DEs](https://youtu.be/ism2SfZgFJg)\n",
        "\n",
        "Video: [Green's Functions](https://youtu.be/-riPW1yt_fA)\n",
        "\n",
        "Video: [Introducing Green's Functions for Partial Differential Equations (PDEs)](https://youtu.be/xNqLZnM-PPY)\n",
        "\n",
        "Video: [Green's functions, Delta functions and distribution theory](https://youtu.be/AqfYSNsrnhI)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1334.jpg)\n"
      ],
      "metadata": {
        "id": "qvEaRxI18Q6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4I7IyMVeJte"
      },
      "source": [
        "**Schwache Ableitung & schwache L√∂sung**\n",
        "\n",
        "* Eine [schwache Ableitung](https://de.wikipedia.org/wiki/Schwache_Ableitung) bzw. [weak derivative](https://en.wikipedia.org/wiki/Weak_derivative) ist in der Funktionalanalysis, einem Teilgebiet der Mathematik, eine Erweiterung des Begriffs der gew√∂hnlichen (klassischen) Ableitung.\n",
        "\n",
        "* Er erm√∂glicht es, Funktionen eine Ableitung zuzuordnen, die nicht (stark bzw. im klassischen Sinne) differenzierbar sind.\n",
        "\n",
        "* Schwache Ableitungen spielen eine gro√üe Rolle in der Theorie der partiellen Differentialgleichungen. **R√§ume schwach differenzierbarer Funktionen sind die Sobolev-R√§ume**.\n",
        "\n",
        "* Ein noch allgemeinerer Begriff der Ableitung ist die Distributionenableitung.\n",
        "\n",
        "* In mathematics, a weak derivative is a generalization of the concept of the derivative of a function (strong derivative) **for functions not assumed differentiable, but only integrable**, i.e., to lie in the $L^{p}$ space $L^{1}([a, b])$. See distributions for a more general definition.\n",
        "\n",
        "**This concept gives rise to the definition of [weak solutions](https://en.wikipedia.org/wiki/Weak_solution) in Sobolev spaces, which are useful for problems of differential equations and in functional analysis.**\n",
        "\n",
        "The **absolute value function** u : [‚àí1, 1] ‚Üí [0, 1], u(t) = |t|, which is not differentiable at t = 0, has a weak derivative v known as the [**sign function**](https://de.wikipedia.org/wiki/Vorzeichenfunktion) given by\n",
        "\n",
        "![gg](https://mathepedia.de/img/Abs_x.png)\n",
        "\n",
        "*Signumfunktion als schwache Ableitung der Betragsfunktions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbR-3EnGQ33f"
      },
      "source": [
        "**Anfangswertprobleme (Cauchy-Problem)**\n",
        "\n",
        "* Beispielsweise werden alle schwingenden Pendel durch eine Differentialgleichung beschrieben (siehe: Pendelgleichung), und der generelle Bewegungsablauf folgt immer dem gleichen Prinzip. Der konkrete Bewegungsablauf ist jedoch durch die Rand- oder Anfangsbedingung(en) (wann wurde das Pendel angesto√üen, und wie weit) bestimmt. Die L√∂sbarkeit von Anfangswertproblemen bei gew√∂hnlichen Differentialgleichungen 1. Ordnung wird durch den Satz von Picard-Lindel√∂f beschrieben. https://mathepedia.de/Gewoehnliche_Differentialgleichungen.html\n",
        "\n",
        "* Wer zu einer Differentialgleichung eine [**Anfangsbedingung**](https://de.wikipedia.org/wiki/Anfangsbedingung) hinzuf√ºgt, stellt damit ein **Anfangswertproblem**. Eine besonders spannende Frage lautet dabei, wie eine Anfangsbedingung zu einer gegebenen Differentialgleichung beschaffen sein muss, damit das entstehende Anfangswertproblem **genau eine eindeutig bestimmte L√∂sung zul√§sst**.\n",
        "\n",
        "Der freie Fall (etwa eines Apfels vom Baum) wird beschrieben durch die Bewegungsgleichung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "y^{\\prime \\prime}(t) &=-g \\\\\n",
        "\\Rightarrow y^{\\prime}(t) &=-g \\cdot t+v_{0}\n",
        "\\end{aligned}$\n",
        "\n",
        "mit der Konstanten $g \\approx 9,81 \\mathrm{~m} / \\mathrm{s}^{2}$ (Erdbeschleunigung).\n",
        "Die L√∂sungsmenge dieser Differentialgleichung besteht zun√§chst aus allen Funktionen der Form\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=-\\frac{1}{2} g t^{2}+v_{0} \\cdot t+y_{0}\n",
        "$\n",
        "\n",
        "mit beliebigen Integrationskonstanten $y_{0}$ und $v_{0}$.\n",
        "Eine m√∂gliche Anfangsbedingung sagt z. B. aus, dass der Apfel zu Beginn der Bewegung an einem\n",
        "Ast in drei Metern H√∂he h√§ngt:\n",
        "\n",
        ">$\n",
        "y(0)=y_{0}=3 \\mathrm{~m}\n",
        "$\n",
        "\n",
        "und sich in Ruhe befindet:\n",
        "\n",
        ">$\n",
        "y^{\\prime}(0)=v_{0}=0 \\mathrm{~m} / \\mathrm{s}\n",
        "$\n",
        "\n",
        "Diese Anfangsbedingung zeichnet nun in der L√∂sungsmenge der Differentialgleichung die eine Funktion\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=3 \\mathrm{~m}-\\frac{1}{2} g t^{2}\n",
        "$\n",
        "\n",
        "als die eindeutig bestimmte L√∂sung des Anfangswertproblems aus (Loesung dann zB uber das [Runge-Kutta-Verfahren](https://de.wikipedia.org/wiki/Runge-Kutta-Verfahren) - Einschrittverfahren zur n√§herungsweisen L√∂sung von Anfangswertproblemen in der numerischen Mathematik)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfUxHFgakA7f"
      },
      "source": [
        "**Randwertproblem (Boundary value)**\n",
        "\n",
        "* Bei partiellen Differentialgleichungen, wenn also die gesuchte Funktion nicht nur von einer, sondern von mehreren Variablen abh√§ngt, werden oftmals [Randbedingungen](https://de.wikipedia.org/wiki/Randbedingung) an Stelle von Anfangsbedingungen verwendet. Manchmal wird dann der Spezialfall einer Randbedingung, deren Definitionsbereich eine Hyperebene im vollen Definitionsbereich der Differentialgleichung bildet, Anfangsbedingung genannt.\n",
        "\n",
        "* In der Betriebswirtschaftslehre und der Volkswirtschaftslehre entsprechen die Randbedingungen den kurzfristig oder gar nicht durch den Entscheidungstr√§ger beeinflussbaren Datenparametern wie beispielsweise die Umweltzust√§nde der Witterung oder der Gesetze.\n",
        "\n",
        "Sei die gegebene Differentialgleichung $y^{\\prime \\prime}(x)=-y(x)$. Die L√∂sungsmenge dieser Gleichung ist $a \\sin (x)+b \\cos (x)$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=1$ und $y(\\pi / 2)=0 \\Rightarrow$ Die L√∂sung ist $y=\\cos (x)$.\n",
        "\n",
        "* Periodische Randbedingung: Gesucht≈Çist die L√∂sung mit $y(0)=0$ und $y(\\pi)=0 \\Rightarrow$ Es gibt unendlich viele L√∂sungen der Form $a \\sin (x)$ mit beliebigem $a$.\n",
        "\n",
        "* Gesucht ist die L√∂sung mit $y(0)=0$ und $y(2 \\pi)=1 \\Rightarrow$ Es gibt keine L√∂sung.\n",
        "\n",
        "Arten von Randbedingungen (Es gibt unterschiedliche M√∂glichkeiten, auf dem Rand des betrachteten Gebietes Werte vorzuschreiben):\n",
        "\n",
        "* Werte der L√∂sung vorschreiben; im Fall einer auf dem Intervall $[a, b]$ definierten gew√∂hnlichen Differentialgleichung schreibt man also $y(a)$ und $y(b)$ vor und spricht\n",
        "dann von [**Dirichlet-Randbedingungen**](https://de.wikipedia.org/wiki/Dirichlet-Randbedingung).\n",
        "\n",
        "* Bedingungen an die Ableitungen stellen, also $y^{\\prime}(a)$ und $y^{\\prime}(b)$ vorgeben, dann spricht man [**von Neumann-Randbedingungen**](https://de.wikipedia.org/wiki/Neumann-Randbedingung) (bei gew√∂hnlichen Differentialgleichungen, wie oben ausgef√ºhrt, von Anfangsbedingungen).\n",
        "\n",
        "* Ein Spezialfall sind [**periodische Randbedingungen**](https://de.wikipedia.org/wiki/Periodische_Randbedingung), hier muss (im Beispiel einer auf dem Intervall $[a, b]$ betrachteten gew√∂hnlichen Differentialgleichung) gelten: $y(a)=y(b)$ bzw. $y^{\\prime}(a)=y^{\\prime}(b)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partielle Differentialgleichung**\n",
        "\n",
        "> https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419\n",
        "\n",
        "* **[Partielle Differentialgleichungen](\n",
        "https://de.wikipedia.org/wiki/Partielle_Differentialgleichung) werden in erster Linie durch Trennung der Variablen und sp√§tere Integration gel√∂st.**\n",
        "\n",
        "* Sobolevr√§ume sind ein grundlegendes Werkzeug bei der Behandlung von PDE's (rein und angewandt).\n",
        "\n",
        "* See also Variational Formulations - wichtig fur partielle Differentialgleichungen\n",
        "\n",
        "* [Euler-Gleichungen (Str√∂mungsmechanik)](https://de.wikipedia.org/wiki/Euler-Gleichungen_(Str√∂mungsmechanik)) bildet dann ein System von **nichtlinearen partiellen** Differentialgleichungen **erster Ordnung** Wird die Viskosit√§t vernachl√§ssigt ($\\eta =\\lambda =0$), so erh√§lt man die Euler-Gleichungen (hier f√ºr den kompressiblen Fall)\n",
        "\n",
        "> $\\rho \\frac{\\partial \\vec{v}}{\\partial t}+\\rho(\\vec{v} \\cdot \\nabla) \\vec{v}=-\\nabla p+\\vec{f}$\n",
        "\n",
        "* [**Navier-Stokes-Gleichungen**](https://de.wikipedia.org/wiki/Navier-Stokes-Gleichungen) bilden dann ein System von **nichtlinearen partiellen** Differentialgleichungen **zweiter Ordnung**\n",
        "\n",
        "  * Die Gleichungen sind eine **Erweiterung der Euler-Gleichungen** der Str√∂mungsmechanik **um Viskosit√§t beschreibende Terme** (Die Navier-Stokes-Gleichungen beinhalten die Euler-Gleichungen als den Sonderfall, in dem die innere Reibung (Viskosit√§t) und die W√§rmeleitung des Fluids vernachl√§ssigt werden.)\n",
        "\n",
        "  * Die Navier-Stokes-Gleichungen bilden das Verhalten von Wasser, Luft und √ñlen ab und werden daher in diskretisierter Form bei der Entwicklung von Fahrzeugen wie Autos und Flugzeugen angewendet.\n",
        "\n",
        "  * Dies geschieht in N√§herungsform, da keine exakten analytischen L√∂sungen f√ºr diese komplizierten Anwendungsf√§lle bekannt sind.\n",
        "\n",
        "  * Siehe auch https://de.wikipedia.org/wiki/Numerische_Str√∂mungsmechanik\n",
        "\n",
        "> $\\rho \\overrightarrow{\\vec{v}}=\\rho\\left(\\frac{\\partial \\vec{v}}{\\partial t}+(\\vec{v} \\cdot \\nabla) \\vec{v}\\right)=-\\nabla p+\\mu \\Delta \\vec{v}+(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\vec{v})+\\vec{f}$\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Potentialstr√∂mung\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Black-Scholes-Modell\n",
        "\n",
        "* Loesungsverfahren\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Finite-Elemente-Methode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Spektralmethode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Liste_numerischer_Verfahren\n",
        "\n",
        "  * https://en.wikipedia.org/wiki/Method_of_characteristics\n",
        "\n",
        "* Die Methode der Charakteristiken ist eine Methode zur L√∂sung partieller\n",
        "Differentialgleichungen (PDGL/PDE), die typischerweise erster Ordnung und quasilinear sind\n",
        "\n",
        "* Die grundlegende Idee besteht darin, **die PDE durch eine geeignete Koordinatentransformation auf ein System gew√∂hnlicher Differentialgleichungen auf bestimmten Hyperfl√§chen, sogenannten Charakteristiken, zur√ºckzuf√ºhren.**\n",
        "\n",
        "* Die PDE kann dann als Anfangswertproblem in dem neuen System mit Anfangswerten auf den die Charakteristik schneidenden Hyperfl√§chen gel√∂st werden. St√∂rungen breiten sich l√§ngs der Charakteristiken aus.\n",
        "\n",
        "* Charakteristiken spielen eine Rolle in der qualitativen Diskussion der L√∂sung bestimmter PDE und in der Frage, wann Anfangswertprobleme f√ºr diese PDE korrekt gestellt sind.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdOgQKlZAssy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving the heat equation:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}=\\alpha \\nabla^{2} T$\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$\n",
        "\n",
        "* can be solved with Fourier series\n",
        "\n",
        "* [DE2: But what is a partial differential equation?](https://www.youtube.com/watch?v=ly4S0oi3Yz8&list=PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6&index=2)\n",
        "\n",
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$"
      ],
      "metadata": {
        "id": "aO7NYOE50lp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_001.png)"
      ],
      "metadata": {
        "id": "RRBeNGa92iKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small change in time:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_002.png)"
      ],
      "metadata": {
        "id": "EQLycVW52oK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small step in space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_003.png)"
      ],
      "metadata": {
        "id": "RKaZoj0J2pCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What that actually encodes is that we look at the limit of that ratio (temperature/space or time change) for smaller and smaller nudges to the input rather than a specific finitely small value\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_004.png)"
      ],
      "metadata": {
        "id": "YE3q91CB3c03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$ tells us that\n",
        "\n",
        "* the way this function changes with respect to time $\\frac{\\partial T}{\\partial t}(x, t)$\n",
        "\n",
        "* depends on how it changes with respect to space $\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$.\n",
        "\n",
        "* <font color=\"blue\">More specifically it's proportional to the second partial derivative with respect to x.</font>\n",
        "\n",
        "* at a high level, the intuition is that at points where the temperature distribution curves, it tends to change more quickly in the direction of that curvature.\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_005.png)"
      ],
      "metadata": {
        "id": "3cBh_epu3xyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$\n",
        "\n",
        "*It checks how different is a point from the average of its neigbours.* (you use second derivative for it)\n",
        "\n",
        "$\\frac{dT_2}{dt}$ = $\\Delta T_2$ - $\\Delta T_1$ (difference of differences) = $\\Delta \\Delta T_1$ (second difference = second derivative)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_007.png)"
      ],
      "metadata": {
        "id": "w9EjvvKl_LWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "* imagine discrete points. and then check the average of the neighbouring points.\n",
        "\n",
        "* if a point is much higher temperature than its neighbours, it will decrease. If it's lower, it will increase.\n",
        "\n",
        "* the equation reflects this relationship between the points and their differences\n",
        "\n",
        "* and the \"second difference\" is for continuous cases the \"second derivative\":\n",
        "\n",
        "> $\\frac{d T_{2}}{d t}=\\frac{\\alpha}{2} \\Delta \\Delta T_{1}$ $\\rightarrow$ $\\frac{\\partial T}{\\partial t}=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}$\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.** (you use second derivative for it)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_006.png)\n",
        "\n",
        "For higher dimensions than 1 (like 3D) you use also the second derivative, but add the other dimensions as well, and the result is called nabla, the Laplacian:\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ],
      "metadata": {
        "id": "qyKPD8oU7rEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Fourier, Laplace & Taylor*"
      ],
      "metadata": {
        "id": "O8cFhBpbIFFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fourier Analysis*"
      ],
      "metadata": {
        "id": "7bWObSh9Mh0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Discrete Fourier Transform](https://youtu.be/yYEMxqreA10)"
      ],
      "metadata": {
        "id": "CJhXQ68Ua1uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The maths behind fourier transform](https://youtu.be/FOOQrrOo-II)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1343.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1344.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1346.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1345.jpg)"
      ],
      "metadata": {
        "id": "Clt-PAZNLm0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $f(x)$ is a periodic function with period $2 \\pi$. If we want to approximate this function with a trigonometric polynomial of degree $n$.\n",
        "\n",
        "**Step 1: Approximate periodic functions using infinite sums of sines and cosines:**\n",
        "\n",
        "> <font color=\"blue\">$F_n(x)=a_0+a_1 \\cos (x)+b_1 \\sin (x)+a_2 \\cos (2 x)+b_2 \\sin (2 x)+\\cdots+a_n \\cos (n x)+b_n \\sin (n x)$\n",
        "\n",
        "For $k \\geq 1$ the \"best\" coefficients to use are the following Fourier coefficients:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "&a_0=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(x) d x \\\\\n",
        "&a_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\cos (k x) d x \\\\\n",
        "&b_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\sin (k x) d x\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Step 2: Replace sin and cosine with $e$ using Eulers Formula:**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$\n",
        "\n",
        "\n",
        "we get this euqation:\n",
        "\n",
        "> <font color=\"blue\">$X_k=x_0 e^{-b_0 j}+x_1 e^{-b_1 j}+\\ldots+x_n e^{-b_{N-1} j}$</font>\n",
        "\n",
        "**Step 3: Taking the sum where where ${\\frac{2 \\pi k n}{N}} = b_n$** for the Discrete Fourier Transform (oft hat man nur einige Messpunkte / Samples aus Experiment oder Simulation)\n",
        "\n",
        "> <font color=\"blue\">$X_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-\\frac{j 2 \\pi k n}{N}}$</font>\n",
        "\n",
        "\n",
        "$\\omega=\\frac{2 \\pi n}{T}$ ist die Kreisfrequenz der diskreten Fourier Transform f√ºr nicht-periodische Funktionen, wodurch man auch schreiben kann:\n",
        "\n",
        "> <font color=\"blue\">$F(\\omega)_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-i \\omega k}$</font> $\\quad$ ([Source](https://de.m.wikipedia.org/wiki/Fourierreihe#Zusammenhang_mit_der_Fourier-Transformation_f√ºr_nicht-periodische_Funktionen))\n",
        "\n",
        "$\\omega_{k}=\\frac{k \\pi}{T}$ is the basic frequency and you can expand f(x) as a sum of sines and cosines that are also periodic in 2 T, and **higher and higher harmonics of those basic sines and cosines (and each with a contribution factor)**.\n",
        "\n",
        "$k = \\frac{2 \\pi}{wavelength}$, e.g. wavelength = $4 \\pi$ (2 complete turns within one period of time), then $k = \\frac{1}{2}$.\n",
        "\n",
        "**Step 4: Understand orthogonal projection**: Dabei ist folgendes ein Skalarprodukt / inner product / projection:\n",
        "\n",
        "> $\\langle x_{n}, e^{-i \\omega k}\\rangle$\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$\n",
        "\n",
        "**Step 5: Get Continous Fourier Transform:** For very large N (the resolution with which we can resolve different frequencies becomes infinitesimally small) the sum $\\sum$ is becoming a Riemann integral (continous case) -  If a Fourier series can approximate a function well on an intervall, can it do it also on the whole real line between -$\\infty$ and $\\infty$?\n",
        "\n",
        "> <font color=\"blue\">$X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$</font>\n",
        "\n",
        "where we calculate the coefficients $a_k$ and $b_k$ at each particular frequency with function $x(t)$ and **analyzing function (sinusoids) $e^{-j 2 \\pi F t} d t$**.\n",
        "\n",
        "  * you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "  * wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient\n",
        "\n",
        "  * and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "*Appendix: Instead of getting one complex coefficient per frequency you can also work with two real coefficients per frequency and end up with two integrals:*\n",
        "\n",
        "* one to correlate with signal with the cosine function:\n",
        "\n",
        "> $x_{a}(F)=\\int_{-\\infty}^{\\infty} x(t) \\cos 2 \\pi \\, Ft \\,d t$\n",
        "\n",
        "* and one to correlate the signal with the sine function:\n",
        "\n",
        "> $X_{b}(F)=\\int_{-\\infty}^{\\infty} x(t) \\sin 2 \\pi \\, Ft \\, d t$\n"
      ],
      "metadata": {
        "id": "xto8Ha2DK0Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Whereas a Taylor Series attempts to approximate a function locally about the point where the expansion is taken, a Fourier series attempts to approximate a periodic function over its entire domain. That is, a Tavlor series approximates a function point wise and a Fourier series approximates a function globally.*\n",
        "\n",
        "* [Steve Brunton: The Fourier Transform](https://www.youtube.com/watch?v=jVYs-GTqm5U)\n",
        "\n",
        "* [Looking Glass: Fourier Transform](https://youtu.be/Xxut2PN-V8Q)\n",
        "\n",
        "* https://sites.oxy.edu/ron/math/120/03/labs/lab8.PDF\n",
        "\n",
        "* https://math.stackexchange.com/questions/47430/is-fourier-series-an-inverse-of-taylor-series\n",
        "\n",
        "* http://dev.ipol.im/~coco/website/taylorfourier.html\n",
        "\n",
        "* https://math.stackexchange.com/questions/7301/connection-between-fourier-transform-and-taylor-series"
      ],
      "metadata": {
        "id": "Z6mGDVo2QU7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Start with approximation of f(x) between -$\\pi$ and $\\pi$ with some orthogonal base vectors made by sin and cos and their coefficients A and B calculated via the inner product with f(x) with each frequency of sin and cos (all are orthogonal base vectors)**\n",
        "\n",
        "**Approximate <u>periodic functions</u>: [Fourier Series](https://de.m.wikipedia.org/wiki/Fourierreihe)**\n",
        "\n",
        "You expand f(x) as a sum of sines and cosines (Grundt√∂ne) that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics (Obert√∂ne) of those basic sines and cosines.\n",
        "\n",
        "> <font color=\"blue\">$f(t)=$$\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left[a_{n} \\cdot \\cos \\left(n \\omega_{0} t\\right)+b_{n} \\sin \\left(n \\omega_{0} t\\right)\\right]$\n",
        "\n",
        "mit folgenden Termen:\n",
        "\n",
        "* <font color=\"blue\">$a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$</font>\n",
        "\n",
        "* <font color=\"blue\">$\\omega_{0}=\\frac{2 \\pi}{T}$</font> $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "* $T$ *ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "* <font color=\"blue\">$a_n$ und $b_n$</font>: siehe weiter unten unter 'orthogonale Projektion'\n",
        "\n",
        "\n",
        "We can approximate f(x) with an expansion of sine and cosines of higher and higher frquency (and shorter wavelength):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos (k x)+B_{k} \\sin (k x)\\right)$\n",
        "\n",
        "How do we calculate the coefficients? You calculate the (Hilbert space) inner product between f(x) and cos(kx) (=the particular k-th frequency cosine wave) bzw. sin(kx). I project f(x) into the cosine k-direction (and I need to normalize by this function):\n",
        "\n",
        "> $A_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos (k x) d x$ = $\\frac{1}{||cos(kx)||^2}$ $\\langle f(x), cos(k x)\\rangle$\n",
        "\n",
        "> $B_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin (k x) d x$ = $\\frac{1}{||sin(kx)||^2}$ $\\langle f(x), sin(k x)\\rangle$\n",
        "\n",
        "**Fourier transform is just another representation on another orthogonal basis vectors (sine and cosine with different frequencies).**\n"
      ],
      "metadata": {
        "id": "7NB7Ne-NuLnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Inner product / projection on orthogonal basis vectors to get coefficients**\n",
        "\n",
        "**<u>Orthogonal Projection (Inner Product)</u>: Calculate the coefficients $a_k$ and $b_k$ bzw. $e$ at each particular frequency**\n",
        "\n",
        "*Why do we use inner product / orthogonal projection? - If you`re familiar with calculating correlations, the Fourier transform is essentially the same:*\n",
        "\n",
        "* you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "* <font color=\"blue\">wherever the function and the analyzing function are similar, they¬¥ll multiply and sum to a large coefficient (=inner product large when vectors are not orthogonal)</font>\n",
        "\n",
        "* and wherever the function and the analyzing function are dissimilar, they¬¥ll multiply and sum to a small coefficient\n",
        "\n",
        "> <font color=\"blue\">$a_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"blue\">$b_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"orange\">$\\langle f(t) , \\cos (x) \\rangle$</font> = Inner Product / Projection of f(t) on cos(x) to get coefficient (how similar?)\n",
        "\n",
        "*Die Koeffizienten berechnen den Anteil den f(x) jeweils an der analysing function hat. Oben multipliziert man diesen Anteil dann mit der jeweiligen analysing function (sehr √§hnlich zu probabilities of states in quantum mechanics). Das ist eine orthogonale Projektion (inner product f(x) mit analysing function) am unteren Beispiel:*\n",
        "\n",
        "> $a_n$ = $\\frac{2}{2} \\int_{0}^{1}(1-t) \\cdot \\cos (n \\pi t) d t$\n",
        "\n",
        "$\\rightarrow$ dieser Teil ist das inner product / Projektion von $f(x) = 1-t$ auf die analysing function $\\cos (n \\pi t) d t$\n",
        "\n",
        "\n",
        "*For when you use $e$ instead of sin & cos: We want to know the $c_k$ coefficients by projecting f(x) onto orthogonal basis vectors sin and cos*\n",
        "\n",
        "<font color=\"black\">The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$\n",
        "\n",
        "> <font color=\"blue\">$c_{k}$$=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$</font>$=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle$\n",
        "\n",
        "\n",
        "* we want to calculate the coefficients $a_k$ and $b_k$ at each particular frequency (and each frequency is orthogonal to each other, both cos vs sine as well as cos k vs cos j)\n",
        "\n",
        "> <font color=\"blue\">$c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$ (inner product)\n",
        "\n",
        "\n",
        "**Most important is the inner product / projection on orthogonal basis vectors to get coefficients!!**\n",
        "\n",
        "The Fourier series definition is exactly the same as how we write a vector F in an orthogonal basis in $R^2$, in a 2 dimensional vector space.\n",
        "\n",
        "* I pick some basis (picture on top right) for example x and y,\n",
        "\n",
        "  * and then I take the inner product of F in the x-direction and then the inner product of F in the y-direction,\n",
        "\n",
        "  * and I take those coefficients, let's call it $a_1$ and $a_2$, and I take them and multiply them by the X unit vector and the Y unit vector, and I add them up\n",
        "\n",
        "* and in Fourier: sine and cosine functions are orthogonal, just like x and y are orthogonal vectors.\n",
        "\n",
        "  * And then i take my function f and project it on the sine and cosine to see how much of f is in this cosine direction and how much of f is in the sine direction\n",
        "\n",
        "  * from that I get my $A_k$-th coefficient and $B_k$-th coefficient, and then I multiply that by my cosine function (and sine function) and I add all of those up\n",
        "\n",
        "\n",
        "> $a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$\n",
        "\n",
        "> $a_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $b_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $\\omega_{0}=\\frac{2 \\pi}{T}$ $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "*T ist die Periode: wie viele Zeiteinheiten werden ben√∂tigt, um eine Periode der Funktion vollst√§ndig zu umlaufen?*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pY8yHtyJwMjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Finetune with approximation of f(x) to get between 0 and $L$, which is periodic in L and repeats beyond 0 and L**\n",
        "\n",
        "* we go now in the domain 0 to L (before above it was -$\\pi$ to $\\pi$)\n",
        "\n",
        "* now we make the sines and cosines periodic between 0 and L (=space of Lebesgue integrable functions)\n",
        "\n",
        "> $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "We the Fourier series for L-periodic functions (and not just 2 $\\pi$ like before):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos \\left(\\frac{2 \\pi k x}{L}\\right)+B_{k} \\sin \\left(\\frac{2 \\pi k x}{L}\\right)\\right)$\n",
        "\n",
        "How do we get A and B?\n",
        "\n",
        "* We take the inner product of f(x) with $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$ (und entsprechend auch fur sin)\n",
        "\n",
        "* this inner product is the projection of f(x) onto the orthogonal basis vector (here each k - $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$)\n",
        "\n",
        "* and then normalized by the norm of the cosine function, which in thhis case the norm squared is $\\frac{2}{L}$:\n",
        "\n",
        "> $A_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "> $B_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\sin \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "* these cosine and sine functions are an orthogonal basis for my function space (Hilbert space of functions f(x))\n",
        "\n",
        "* If I plugged in a cosine k and a sine j, these functions are orthogonal. Like if I plug in two cosines with different k's, their inner product is 0 (because they should be orthogonal). Umgekehrt: if I plug in cos kx and cos kx then I would get a non zero inner product.\n",
        "\n",
        "* btw: the approximation of f(x) between 0 and L will repeat infineily beyond 0 and l."
      ],
      "metadata": {
        "id": "52T4qM5-A2r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Introduce $e^{ikx}$ to combine sin and cos in one complex coefficient**\n",
        "\n",
        "**<u>Analysing Function</u>: Statt sin und cos kann man auch $e$ verwenden**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$ (Eulers formula)\n",
        "\n",
        "$e^{i k x}$ is the analyzing function made of sinusoids (eigentlich: $e^{-j 2 \\pi F t} d t$)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i x \\frac{k \\pi}{L}}$</font> (becoming a Riemann integral) with $\\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$</font> $= \\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "\n",
        "\n",
        "Anderes Beispiel mit einem Integral statt Summe geschrieben:\n",
        "\n",
        "> $X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$\n",
        "\n",
        "* basics: remember the Euler expansion:\n",
        "\n",
        "> $e^{i k x}=\\cos (k x)+i \\sin (k x)$\n",
        "\n",
        "* now we talk about the fourier series in terms of a complex basis:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$\n",
        "\n",
        "(you could expand that in sin and cos: $f(x)=\\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "See also [Inner Products in Hilbert Space](https://www.youtube.com/watch?v=g-eNeXlZKAQ&list=PLMrJAkhIeNNT_Xh3Oy0Y4LTj0Oxo8GqsC&index=4)\n",
        "\n",
        "* inner product of functions is consistent with definition of inner products of vectors\n",
        "\n",
        "* similar functions should have a large inner product\n",
        "\n",
        "* if I go to infinity with delta x (make it finer and finer), then the Riemann approximation $\\langle f,g \\rangle$ becomes the continuous integral formulation $\\int$\n",
        "\n",
        "You can expand f(x) as a sum of sines and cosines that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics of those basic sines and cosines.\n",
        "\n",
        "You can represent this as a complex Fourier series:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k \\pi \\frac{x}{L}} \\quad \\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency / Kreisfrequenz)\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$"
      ],
      "metadata": {
        "id": "dLbwYXpgvwx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√§herungsverfahren f√ºr periodische Funktionen: Fourier Series**\n",
        "\n",
        "* nutzt man weil zB manche periodische Funktionen sehr kompliziert sind, um sie mit einer Funktion f(x) zu beschreiben\n",
        "\n",
        "**N√§herungsverfahren f√ºr nicht-periodische Funktionen: Fourier Transform**\n",
        "\n",
        "**N√§herungsverfahren f√ºr Polynome: Taylor Polynom**\n",
        "\n",
        "https://medium.com/sho-jp/fourier-transform-101-part-1-b69ea3cb4837\n",
        "\n",
        "**Harmonic Analysis**\n",
        "\n",
        "Aus Sicht der [abstrakten harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) sind sowohl die Fourier-Reihen und die Fourier-Integrale als auch die Laplace-Transformation, die Mellin-Transformation oder auch die Hadamard-Transformation Spezialf√§lle einer **allgemeineren (Fourier-)Transformation**."
      ],
      "metadata": {
        "id": "MWiANZ7vSEOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "*We can clearly observe a peak value at 10 Hz with a magnitude of one while all other frequencies hover around zero. We can verify this from the original signal where there are 10 complete cycles in a second with an amplitude of one:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_49.png)\n",
        "\n",
        "*When a similar principle is applied to a more complicated time series as shown in the green plot below, we can deduce from its Fourier transform that the data comprises of 3 different elementary components with 3 different frequencies (2, 5 and 10 Hz) at 3 different amplitudes (0.5, 1 and 2):*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_50.png)\n",
        "\n",
        "Source: https://medium.com/@khairulomar/deconstructing-time-series-using-fourier-transform-e52dd535a44e"
      ],
      "metadata": {
        "id": "1q0EqJnVRvQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetrien**\n",
        "\n",
        "Die Kosinusfunktion ist achsensymmetrisch:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_51.png)\n",
        "\n",
        "In Worten: Ein x-Wert und der negative x-Wert haben denselben Kosinuswert. Als Formel: cos(‚àíùë•)=cosùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        ">$\\cos \\left(\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        ">$\\cos \\left(-\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        "Die Sinusfunktion ist punktsymmetrisch zum Koordinatenursprung. Stelle dir vor, wie du den rechten Arm des Graphen um (0|0) drehst.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_52.png)\n",
        "\n",
        "In Worten: sin(‚àíùë•)\n",
        "sin\n",
        "(\n",
        "-\n",
        "x\n",
        ")\n",
        " ist sinùë•\n",
        "sin\n",
        "x\n",
        " mit umgedrehtem Vorzeichen.\n",
        "Als Formel: sin(‚àíùë•)=‚àísinùë•\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "> $\\sin \\left(\\frac{\\pi}{4}\\right)=0,71$\n",
        "\n",
        "> $\\sin \\left(-\\frac{\\pi}{4}\\right)=-0,71$\n",
        "\n",
        "Source: https://www.kapiert.de/sinus-und-kosinusfunktionen-eigenschaften/\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax6bbzUGSd6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_53.png)\n",
        "\n",
        "*Source: [Mathe mit Nina](https://youtu.be/u6Fqi8596qA)*\n"
      ],
      "metadata": {
        "id": "bdefArXifISa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeO_i6Z9-rLD"
      },
      "source": [
        "###### *Laplace Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhmFbOnmaUS"
      },
      "source": [
        "**Fourier** $\\quad X(\\omega)=\\int_{-\\infty}^{\\infty} x(t) e^{-i \\omega t} d t$\n",
        "\n",
        "**Laplace** $\\quad X(s)=\\int_{0}^{\\infty} x(t) e^{-s t} d t$\n",
        "\n",
        "with s = ${\\alpha + i \\omega}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1193.png)"
      ],
      "metadata": {
        "id": "msM9RjFYPKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1194.png)"
      ],
      "metadata": {
        "id": "qke32YL1QCrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Taylor Series*"
      ],
      "metadata": {
        "id": "xDUD9L1G7HIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor Series Expansion to approximate $e^x$**\n",
        "\n",
        "> <font color=\"blue\">$e^{x} \\approx \\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !} \\approx 1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\ldots$\n",
        "\n",
        "*(N√§herungsverfahren f√ºr Polynome: Taylor Polynom)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1347.jpg)\n",
        "\n",
        "Funktionsvorschrift:\n",
        "\n",
        "$a_{i}=a_{1} \\cdot q^{i-1}$ bzw $a_{i}=a_{0} \\cdot q^{i}$ f√ºr Anfangsglied a1. Oder auch (andere Folge): $a_{i}=a_{0} \\cdot q^{i}$\n",
        "\n",
        "Rekursionsvorschrift (wie Fibonacci):\n",
        "\n",
        "$a_{i+1}=a_{i} \\cdot q$. Oder auch (andere Folge): $a_{i}=q \\cdot a_{i-1}$\n",
        "\n",
        "*The following code examples are taken from ['Python for Undergraduate Engineers'](https://pythonforundergradengineers.com/creating-taylor-series-functions-with-python.html)*"
      ],
      "metadata": {
        "id": "rkbWd2CM6u6w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3QOyKV6llLn"
      },
      "source": [
        "**Taylor Series Expansion to approximate $cos(x)$**\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x) \\approx \\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !} \\approx 1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\frac{x^{8}}{8 !}-\\frac{x^{10}}{10 !}+\\ldots$\n",
        "\n",
        "* We can code this formula into a function that contains a for loop. Note the variable x is the value we are trying to find the cosine of, the variable n is the number of terms in the Taylor Series, and the variable i is the loop index which is also the Taylor Series term number.\n",
        "* We are using a separate variable for the coefficient coef which is equal to (‚àí1)<sup>i</sup>, the numerator num which is equal to x<sup>2i</sup> and the denominator denom which is equal to (2i!). Breaking the Taylor Series formula into three parts can cut down on coding errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Beziehung zwischen Sinus, Kosinus und Exponentialfunktion**\n",
        "\n",
        "Die [trigonometrischen Funktionen](https://de.m.wikipedia.org/wiki/Trigonometrische_Funktion) sind eng mit der [Exponentialfunktion](https://de.m.wikipedia.org/wiki/Exponentialfunktion) verbunden, wie die [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel) zeigt:\n",
        "\n",
        "Eigentlich hat $e$ nichts mit periodischen Funktionen wie $sin$ oder $cos$ zu tun. Aber f√ºgt man $i$ in den exponent von $e$, dann wird daraus eine rotierende, periodische Funktion.*\n",
        "\n",
        "**Part I: Let`s get the Taylor expansions for three common functions**\n",
        "\n",
        "> $e^{x}=1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{x^{5}}{5 !}+\\cdots$\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x)=1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\cdots$\n",
        "\n",
        "> <font color=\"red\">$\\sin (x)=x-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !}-\\frac{x^{7}}{7 !}+\\cdots$\n",
        "\n",
        "* there is some relation between these three:\n",
        "\n",
        "  * all all built upon $\\frac{x^{n}}{n !}$\n",
        "\n",
        "  * cosinus has all even numbers, sinus all odd numbers, and exponential both\n",
        "\n",
        "  * Vorzeichen is alternating for cos and sin, but all posisitv for exponential\n",
        "\n",
        "**Part II: Add $i$ to relate all three functions**\n",
        "\n",
        "* You can't just add up cos and sin to get exp! You need to add the $i = \\sqrt{-1}$\n",
        "\n",
        "* We need to replace all $x$ with an $i x$ in the exp series\n",
        "\n",
        "> $e^{ix}=1+ix+\\frac{(ix)^{2}}{2 !}+\\frac{(ix)^{3}}{3 !}+\\frac{(ix)^{4}}{4 !}+\\frac{(ix)^{5}}{5 !}+\\cdots$\n",
        "\n",
        "* It introduces the following: $i=i \\quad i^{2}=-1 \\quad i^{3}=-i \\quad i^{4}=1$\n",
        "\n",
        "* this turns the exponential into the following:\n",
        "\n",
        "> $e^{i x}=1+i x-\\frac{x^{2}}{2 !}-\\frac{ix^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{i x^{5}}{5 !}+$\n",
        "\n",
        "* finally, let's separate the terms with $i$ (all odd) from those without $i$ (all even):\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$\\left[1-\\frac{x^{2}}{2!}+\\frac{x^{4}}{4!}-\\cdots \\right]$</font> + $i$ <font color=\"red\">$\\left[x-\\frac{x^{3}}{3!}+\\frac{x^{5}}{5!}-\\cdots\\right]$</font>\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$cos(x)$</font> + $i$ <font color=\"red\">$ \\, sin(x)$</font>\n",
        "\n",
        "*$sin(x)$ ist der imagin√§re Anteil, und $cos(x)$ ist der reale Anteil f√ºr $z=e^{i \\varphi}=x+i y$*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/400px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "\n",
        "**Part III: Let`s replace $x$ with $\\pi$**\n",
        "\n",
        "> $e^{i \\pi}=\\cos (\\pi)+i \\sin (\\pi)$\n",
        "\n",
        "* sin and cos repeat and one full period is $2 \\pi$ radians (=360¬∞)\n",
        "\n",
        "* $cos(\\pi)$ is half a rotation = -1 and $sin(\\pi)$ = 0, which means:\n",
        "\n",
        "> $e^{i \\pi}= -1$ $\\,$ and $\\,$ $e^{2\\pi i} = 1$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIb2bU82PFrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Number Theory*"
      ],
      "metadata": {
        "id": "Qa3-DIbUt0Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mathworld.wolfram.com/topics/NumberTheory.html"
      ],
      "metadata": {
        "id": "oTRwcbs24qtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Number theory Full Course [A to Z]](https://www.youtube.com/watch?v=19SW3P_PRHQ)"
      ],
      "metadata": {
        "id": "QQ94xvSShDVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [The Continuity of Splines](https://www.youtube.com/watch?v=jvPPXbo87ds)"
      ],
      "metadata": {
        "id": "1nsbz4yAiP6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zahlentheorie**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebraische_Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Diophantische_Gleichung: Polynomfunktion mit ganzzahligen Koeffizienten ist und nur ganzzahlige L√∂sungen gesucht werden"
      ],
      "metadata": {
        "id": "NGT5msJ1sIRa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g878nE6q7lX"
      },
      "source": [
        "**Zahlenarten**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Zahldarstellung\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/number_001.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carmichael-Zahl**\n",
        "\n",
        "Eine [Carmichael-Zahl](https://de.m.wikipedia.org/wiki/Carmichael-Zahl) ist eine nat√ºrliche Zahl mit besonderer Primfaktorzerlegung\n",
        "\n",
        "Eine Carmichael-Zahl ist stets ungerade und enth√§lt mindestens 3 verschiedene Primfaktoren. Die kleinsten Carmichael-Zahlen sind 561, 1105, 1729.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/carmichael-zahl/1414\n",
        "\n",
        "https://www.faz.net/aktuell/wissen/daniel-larsen-findet-einen-mathebeweis-zu-carmichael-zahlen-18583248.html"
      ],
      "metadata": {
        "id": "dImJp0SNWxsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Narcissistic Number**\n",
        "\n",
        "number theory, a narcissistic number is a number that can be expressed as the sum of its own digits raised to the power of the number of digits.\n",
        "\n",
        "> $153 = 1^3 + 5^3 + 3^3$\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Narcissistic_number"
      ],
      "metadata": {
        "id": "04WEUT_eCbW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex Numbers**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/155762/complex-numbers-in-quantum-mechanics-and-in-special-relativity\n",
        "\n",
        "* you can use split-complex numbers in relativity, but ironically complex numbers have proved more popular for this)."
      ],
      "metadata": {
        "id": "klTQgi0O_Vnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quaternions are useful. In particular for representation of rotations of 3-space. ‚Äì\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Hyperkomplexe_Zahlen.svg/519px-Hyperkomplexe_Zahlen.svg.png)"
      ],
      "metadata": {
        "id": "ueOfwEOht3VB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y_nhc_-u7P"
      },
      "source": [
        "**Hyperreelle Zahlen & Nichtstandardanalysis (Infinitesimalrechnung)**\n",
        "\n",
        "* There are also applications of nonstandard analysis to the theory of stochastic processes, particularly constructions of Brownian motion as random walks.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Nichtstandardanalysis\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Hyperreelle_Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Infinitesimalrechnung\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Surreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Hyperreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Infinitesimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2-azta81gIJ"
      },
      "source": [
        "**Hyperkomplexe Zahlen**\n",
        "\n",
        "* [Hyperkomplexe Zahlen](https://de.m.wikipedia.org/wiki/Hyperkomplexe_Zahl) sind Verallgemeinerungen der komplexen Zahlen.\n",
        "\n",
        "https://www.quantamagazine.org/the-octonion-math-that-could-underpin-physics-20180720/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNHMgPsrj4B"
      },
      "source": [
        "**Transcendental numbers:**\n",
        "\n",
        "* Zahlen, die L√∂sung einer algebraischen Gleichung: (Wurzel aus 2) - 2 = 0\n",
        "\n",
        "* Transzendente Zahlen (in der Menge der reellen Zahlen): Zahlen, die nicht L√∂sung einer algebraischen Gleichung sind: e oder pi\n",
        "\n",
        "> from: https://www.youtube.com/watch?v=P24tmohytXs\n",
        "\n",
        "* pie œÄ or Euler number\n",
        "\n",
        "* Never end after comma: 3.14159265358979323846....\n",
        "\n",
        "* Cannot be displayed as fraction\n",
        "\n",
        "* [Transzedente Zahl](https://de.m.wikipedia.org/wiki/Transzendente_Zahl) heisst eine reelle Zahl (oder allgemeiner eine komplexe Zahl), wenn sie nicht Nullstelle eines (vom Nullpolynom verschiedenen) Polynoms mit ganzzahligen Koeffizienten ist. Andernfalls handelt es sich um eine algebraische Zahl. Jede reelle transzendente Zahl ist √ºberdies irrational.\n",
        "\n",
        "* omnem rationem transcendunt, lat.: Sie sind jenseits aller Vernunft\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplex Numbers**\n",
        "\n",
        "* --> real tessarines\n",
        "\n",
        "* also: [split complex number](https://en.m.wikipedia.org/wiki/Split-complex_number), hyperbolic number, double number)\n",
        "\n",
        "* In algebra, a split complex number (or hyperbolic number, also perplex number, double number) has two real number components x and y, and is written $z = x + y‚Äâj$, where $j^2$ = 1. The conjugate of z is $z^{‚àó}$ = x ‚àí y j. Since j2 = 1, the product of a number z with its conjugate is $zz^{‚àó}$ = $x^2 ‚àí y^2$, an [isotropic quadratic form](https://en.m.wikipedia.org/wiki/Isotropic_quadratic_form), =N(z) = $x^2 ‚àí y^2$.\n",
        "\n",
        "* Very perplexing, right? They are defined as of form a+hb, where a and b are real numbers, h¬≤=1.\n",
        "\n",
        "  * But wait, that sounds too easy! Until you realize that h isn't +1 or -1. It is \"something else\".\n",
        "\n",
        "* Perplex numbers have many applications outside of algebra and geometry, such as in Quantum Mechanics and the Theory of Relativity.\n",
        "\n",
        "* Special Relativity:https://aapt.scitation.org/doi/10.1119/1.14605\n",
        "\n",
        "* Quantum Mechanics: https://www.intlpress.com/site/pub/files/_fulltext/journals/cis/2014/0014/0003/CIS-2014-0014-0003-a001.pdf"
      ],
      "metadata": {
        "id": "VKqM5OuK6jYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bicomplex number**\n",
        "\n",
        "* --> \"Tessarine\" redirects here. For real tessarines, see Split-complex number.\n",
        "\n",
        "* In abstract algebra, a bicomplex number is a pair (w, z) of complex numbers constructed by the Cayley‚ÄìDickson process that defines the bicomplex conjugat ${\\displaystyle (w,z)^{*}=(w,-z)}$, and the product of two bicomplex numbers as\n",
        "\n",
        "> {\\displaystyle (u,v)(w,z)=(uw-vz,uz+vw).}\n",
        "\n",
        "* Then the bicomplex norm is given by\n",
        "\n",
        "> ${\\displaystyle (w,z)^{*}(w,z)=(w,-z)(w,z)=(w^{2}+z^{2},0),}$\n",
        "\n",
        "* a quadratic form in the first component.\n",
        "\n",
        "* https://hsm.stackexchange.com/questions/12866/why-are-quaternions-more-popular-than-tessarines-despite-being-non-commutative\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1200.png)\n",
        "\n",
        "Images source:https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.527.356&rep=rep1&type=pdf\n"
      ],
      "metadata": {
        "id": "bZqRMeca9IqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVu9D8nrvsY"
      },
      "source": [
        "**p-adic numbers**\n",
        "\n",
        "* Size is different in p-adic numbers\n",
        "\n",
        "* they have nothing to do with the real number line: here two numbers are close, when their first several digits are the same\n",
        "\n",
        "* in p-adic absolute value, two numbers are close when their last digits are the same. so two numbers are close in this field, when they agree on\n",
        "\n",
        "https://youtu.be/3gyHKCDq1YA\n",
        "\n",
        "* infinite before comma: ....985356295141.3\n",
        "\n",
        "* [p-adische Zahl](https://de.m.wikipedia.org/wiki/P-adische_Zahl) ist eine Zahl, die sich in einer Potenzreihe zu einer Primzahl darstellen l√§sst\n",
        "\n",
        "* p-adic number systems emerge from modular arithmetic\n",
        "\n",
        "* https://www.quantamagazine.org/how-the-towering-p-adic-numbers-work-20201019/\n",
        "\n",
        "* https://www.quantamagazine.org/peter-scholze-and-the-future-of-arithmetic-geometry-20160628/\n",
        "\n",
        "* https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/\n",
        "\n",
        "* \"Das Dualsystem ist das Stellenwertsystem mit der Basis 2, liefert also die dyadische (2-adische) Darstellung von Zahlen (Dyadik) (gr. Œ¥œçŒø = zwei).\" [Source](https://de.m.wikipedia.org/wiki/Dualsystem#Grundrechenarten_im_Dualsystem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPr6YYL5n9sy"
      },
      "source": [
        "**Modulform**\n",
        "\n",
        "* used in stringtheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Modulform\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Ramanujan-Thetafunktion\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Thetafunktion\n",
        "\n",
        "* https://www.quantamagazine.org/moonshine-link-discovered-for-pariah-symmetries-20170922/\n",
        "\n",
        "* https://www.quantamagazine.org/universal-math-solutions-in-dimensions-8-and-24-20190513/\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Kaluza-Klein-Theorie"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infinity $\\infty$**\n",
        "\n",
        "* Kontinuumshypothese (Cantor) kann man nicht beweisen oder widerlegen (G√∂del). Die Mathematik kann funktionieren, wenn man sowohl davon ausgeht, dass die Kontiuumshypothese gilt, als auch, dass sie nicht gilt.\n",
        "\n",
        "* term of size (how many?) is: **Cardinality**\n",
        "\n",
        "* Rules for comparing cardinalities: injection, subjection, bijection\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_319.png)\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite\n",
        "\n",
        "* [How Big are All Infinities Combined? (Cantor's Paradox) | Infinite Series](https://www.youtube.com/watch?v=TbeA1rhV0D0&list=WL&index=12&t=86s)\n",
        "\n",
        "**Smallest Sizes of Infinity**\n",
        "\n",
        "* intuition is often misleading in mathematics!\n",
        "\n",
        "* There are infinitely many sizes of infinity\n",
        "\n",
        "* Smallest infinity: natural numbers (counting numbers). Here: even numbers and natural numbers are the same size!\n",
        "\n",
        "  * Natural numbers: $\\aleph$ \"Aleph-naught\" (the least infinity cardinality)\n",
        "\n",
        "* We need to find a way to tell which infinity is bigger without counting them.\n",
        "\n",
        "* You use something called Bijection: if you can pair up two sets, they are the same size\n",
        "\n",
        "\t* even numbers and natural numbers are the same size, because there is a bijection between the two sets! Each natural number is [aired with 2 times itself: 1 with 2, 2 with 4, 4 with 6 etc. Damit sind alles odd numbers out, but the set didn't get smaller\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_318.png)\n",
        "\n",
        "\t* the natural numbers are also the same size as the integers: Integers include all natural numbers + all the negative whole numbers. We can pair them up exactly, so they must be the same size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_317.png)\n",
        "\n",
        "* Above the natural numbers are the real numbers in terms of infinity size. After that the real numbers.\n",
        "\n",
        "\t* an interval on the real number line is also an infinity. And an interval on the real line between 0 and 5 has the same size as an interval between 0 and 10. They are ll as big as the real numbers!!\n",
        "\n",
        "\t* You can show that any interval is the same size as the entire real number line !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_316.png)\n",
        "\n",
        "* Cantor wondered if there is an infinity between the natural and the real numbers?\n",
        "\n",
        "\t* CONTINUUM HYPOTHESIS: there is no size of infinity between the natural numbers and the real numbers\n",
        "\n",
        "\t* Decades later, mathematicians found out that the Continuum hypothesis is independent of the Zermela-Fraenkel set theory with choice (ZFC) - the Continuum hypothesis can not be proved or disproved using the standard rules of mathematics\n",
        "\n",
        "* So in one model of the tower of infinities the real numbers sit directly above the natural numbers\n",
        "\n",
        "* But in other models there are many infinities in between\n",
        "\n",
        "* The rules of maths don't say that one tower is correct and the other wrong.\n",
        "\n",
        "> So it seems that mathematics seems to be surprisingly agnostics with regards to which hierarchy of infinities is correct\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite"
      ],
      "metadata": {
        "id": "rniZL8bpenwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFaA3R5k-Wi"
      },
      "source": [
        "**Primzahlen**\n",
        "\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Skewes-Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahlsatz\n",
        "\n",
        "*Riemannsche Vermutung (Primzahlverteilung & Zeta-Funktion)*\n",
        "\n",
        "* Die Verteilung der Primzahlen ist sehr merkw√ºrdig und damit interessant. So zeigt die Verteilung von Primzahlen in (relativ) kurzen Intervallen eine gewisse ‚ÄûZuf√§lligkeit‚Äú, w√§hrend andererseits beliebig lange Intervalle existieren, die keine Primzahl enthalten.\n",
        "\n",
        "* Bernhard Riemann setzte sich in seiner Arbeit ‚ÄûUeber die Anzahl der Primzahlen unter einer gegebenen Gr√∂sse‚Äú (1859) zum Ziel, die Verteilung der Primzahlen mit analytischen Methoden zu bestimmen, stie√ü dabei auf Riemannsche Œ∂-Funktion und formulierte die Riemannsche Vermutung. Basierend auf den Riemannschen Ideen gelang 1896 der Beweis des Primzahlsatzes, mit dem man f√ºr gro√üe Zahlen x mit immer gr√∂√üerer relativer Genauigkeit sagen kann, wieviele Primzahlen ‚â§ x es gibt.\n",
        "\n",
        "* Will man diese Anzahlen noch genauer wissen, so kommt man schnell in einen Bereich mathematischer Fragestellungen mit zahlreichen offenen Problemen, z. B. den Goldbach-Problemen oder Fragen √ºber Primzahlzwillinge.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/primzahlverteilung/8085\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Vermutung\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Zetafunktion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Music Theory*"
      ],
      "metadata": {
        "id": "fq5mO57kWnjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A beginner's guide to music theory](https://youtu.be/n2z02J4fJwg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1372.jpg)"
      ],
      "metadata": {
        "id": "enDRiPrwWj_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The mathematical problem with music, and how to solve it](https://youtu.be/nK2jYk37Rlg)\n",
        "\n",
        "* Frequency (in Hertz) = no. of vibrations per second. Typically between 50 to a few thousand Hertz. Higher frequency = higher pitch (i.e. 200. vs 1000)\n",
        "\n",
        "* all musical tones are multiples of pure tones (harmonics) = behave to the mathematical sine function. For example tone f consists of frequencies of several integer multiples of a pure tone f, 2f, 3f.. (first harmonic, second harmonic, third harmonic..).\n",
        "\n",
        "* Melodies = sequences of tones. Strictly speaking these two melodies have not even one frequency in common, but they sound similar. But both melodies have the **same ratios between the frequencies of their tones.** Frequencies are different, but ratios are the same.\n",
        "\n",
        "* Changing from melody 1 to melody 2 is called **transposition from one key to another**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1373.jpg)\n",
        "\n",
        "* Another important concept is the **Musical intervall**, which is the frequency ratio mentioned above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1374.jpg)\n",
        "\n",
        "* One important interval is called the octave, and it corresponds to a frequency ratio of 2:1. The Octave is very pleasent to the ear. **Octave equivalence**: two tones that are an Octave apart sound highly similar: hence have same name and belong to the same pitch class.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1375.jpg)\n",
        "\n",
        "* Another important is called the **Fifth** with ration 3 over 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1376.jpg)\n"
      ],
      "metadata": {
        "id": "A5ewjWi3Xe1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGNbw_qosx7"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionentheorie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-vaCcWrF4t"
      },
      "source": [
        "Die [Funktionentheorie](https://de.wikipedia.org/wiki/Funktionentheorie) befasst sich mit der Theorie differenzierbarer komplexwertiger Funktionen mit komplexen Variablen. Da insbesondere die Funktionentheorie einer komplexen Variablen reichlich Gebrauch von Methoden aus der reellen Analysis macht, nennt man das Teilgebiet auch komplexe Analysis.\n",
        "\n",
        "* F√ºr holomorphe Funktionen gilt, dass Real- und Imagin√§rteil [harmonische Funktionen](https://de.wikipedia.org/wiki/Harmonische_Funktion) sind, also die [Laplace-Gleichung](https://de.wikipedia.org/wiki/Laplace-Gleichung) erf√ºllen. Dies verkn√ºpft die Funktionentheorie mit den partiellen Differentialgleichungen, beide Gebiete haben sich regelm√§√üig gegenseitig beeinflusst.\n",
        "\n",
        "* Das Wegintegral einer holomorphen Funktion ist vom Weg unabh√§ngig. Dies war historisch das erste Beispiel einer Homotopieinvarianz. Aus diesem Aspekt der Funktionentheorie entstanden viele Ideen der algebraischen Topologie, beginnend mit Bernhard Riemann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmbwvHZAec-"
      },
      "source": [
        "**Komplexe Funktion**\n",
        "\n",
        "* Eine [komplexe Funktion](https://de.wikipedia.org/wiki/Funktionentheorie#Komplexe_Funktionen) ordnet einer komplexen Zahl eine weitere [komplexe Zahl](https://de.m.wikipedia.org/wiki/Komplexe_Zahl) zu\n",
        "\n",
        "* Da jede komplexe Zahl durch zwei reelle Zahlen in\n",
        "der Form $x+$ iy geschrieben werden kann, l√§sst sich eine allgemeine Form einer komplexen Funktion darstellen durch:\n",
        "\n",
        "> $\n",
        "x+i y \\mapsto f(x+i y)=u(x, y)+i v(x, y)\n",
        "$\n",
        "\n",
        "* Dabei sind $u(x, y)$ und $v(x, y)$ reelle Funktionen, die von zwei reellen Variablen $x$ und $y$ abh√§ngen.\n",
        "\n",
        "* $u(x, y)$ hei√üt der Realteil und $v(x, y)$ der Imagin√§rteil der Funktion.\n",
        "\n",
        "* **Insofern ist eine komplexe Funktion nichts anderes als eine Abbildung von $\\mathbb{R}^{2}$ nach $\\mathbb{R}^{2}$ (also eine Abbildung, die zwei reellen Zahlen wieder zwei reelle Zahlen zuordnet).**\n",
        "\n",
        "* **Tats√§chlich k√∂nnte man die Funktionentheorie auch mit Methoden der reellen Analysis aufbauen.**\n",
        "\n",
        "* Der Unterschied zur reellen Analysis wird erst deutlicher, wenn man **komplex-differenzierbare Funktionen** betrachtet und dabei die <u>**multiplikative Struktur des K√∂rpers der komplexen Zahlen**</u> ins Spiel bringt, die dem Vektorraum $\\mathbb{R}^{2}$ fehlt.\n",
        "\n",
        "* Wie auch bei reellwertigen und reellen Funktionen ist die Verwendung des Begriffes einer komplexen Funktion in der Literatur aber nicht eindeutig. Teilweise wird er synonym mit einer komplexwertigen Funktion verwendet, teilweise wird er auch nur f√ºr komplexwertige Funktionen einer komplexen Variablen verwendet, also Funktionen\n",
        "\n",
        "> $\n",
        "f: D \\rightarrow \\mathbb{C}\n",
        "$\n",
        "\n",
        "bei denen $D \\subseteq \\mathbb{C}$ ist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDS2jqpHCZov"
      },
      "source": [
        "**Komplexwertige Funktion**\n",
        "\n",
        "* Eine [komplexwertige Funktion](https://de.wikipedia.org/wiki/Komplexwertige_Funktion) ist eine Funktion, deren Funktionswerte komplexe Zahlen sind\n",
        "\n",
        "* Eng damit verwandt ist der Begriff der **komplexen Funktion**, der in der Literatur aber nicht eindeutig verwendet wird\n",
        "\n",
        "* Komplexwertige Funktionen werden in der Analysis und in der Funktionentheorie untersucht und haben vielf√§ltige Anwendungen wie zum Beispiel in der Physik und der Elektrotechnik, wo sie beispielsweise zur Beschreibung von Schwingungen dienen.\n",
        "\n",
        "* Eine komplexwertige Funktion ist eine Funktion\n",
        "$f: D \\rightarrow \\mathbb{C}$ bei der die Zielmenge die Menge der komplexen Zahlen ist.\n",
        "\n",
        "* **An die Definitionsmenge $D$ sind keine Anforderungen gestellt**.\n",
        "\n",
        "* **Aufgrund der Einbettung der reellen Zahlen in die komplexen Zahlen lassen sich alle reellwertigen Funktionen auch als komplexwertige Funktionen auffassen.**\n",
        "\n",
        "* Beispiel: Die Funktion $f: \\mathbb{R} \\rightarrow \\mathbb{C}$ definiert durch $f(x)= \\mathrm{e}^{\\mathrm{i} x}=\\cos (x)+\\mathrm{i} \\sin (x)$ ist eine komplexwertige Funktion einer reellen Variable, und zwar die [Eulersche Formel](https://de.wikipedia.org/wiki/Eulersche_Formel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpqu4CjYvL7"
      },
      "source": [
        "**Holomorphe Funktion**\n",
        "\n",
        "* Holomorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* [Holomorphe Funktionen](https://de.wikipedia.org/wiki/Holomorphe_Funktion) sind **an jedem Punkt komplex differenzierbar**.\n",
        "\n",
        "* Eine Funktion $f\\colon U\\to {\\mathbb  {C}}$ mit einer offenen Menge $ U\\subseteq \\mathbb {C}$ hei√üt holomorph, falls sie in jedem Punkt von $U$ komplex differenzierbar ist.\n",
        "\n",
        "* Holomorphie eine sehr starke Eigenschaft ist, die im Reellen kein Pendant besitzen (z.B. ist jede holomorphe Funktion beliebig oft (stetig) differenzierbar und l√§sst sich lokal in jedem Punkt in eine Potenzreihe entwickeln.)\n",
        "\n",
        "Es sei $U \\subseteq \\mathbb{C}$ eine offene Teilmenge der komplexen Ebene und $z_{0} \\in U$ ein Punkt dieser Teilmenge. Eine Funktion $f: U \\rightarrow \\mathbb{C}$ hei√üt komplex differenzierbar im Punkt $z_{0}$, falls der Grenzwert\n",
        "\n",
        ">$\n",
        "\\lim _{h \\rightarrow 0} \\frac{f\\left(z_{0}+h\\right)-f\\left(z_{0}\\right)}{h}\n",
        "$\n",
        "\n",
        "existiert. Man bezeichnet ihn dann als $f^{\\prime}\\left(z_{0}\\right)$.\n",
        "\n",
        "Die Funktion $f$ hei√üt holomorph im Punkt $z_{0}$, falls eine Umgebung von $z_{0}$ existiert, in der $f$ komplex differenzierbar ist. Ist $f$ auf ganz $U$ holomorph, so nennt man $f$ holomorph.\n",
        "\n",
        "**Ist weiter $U=\\mathbb{C},$ so nennt man $f$ eine <u>ganze Funktion</u>**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeegSuMPXcu"
      },
      "source": [
        "**Biholomorphe Funktionen**\n",
        "\n",
        "* Eine Funktion, die holomorph, bijektiv und deren Umkehrfunktion holomorph ist, nennt man [biholomorph](https://de.wikipedia.org/wiki/Biholomorphe_Abbildung).\n",
        "\n",
        "* Im Fall einer komplexen Ver√§nderlichen ist das √§quivalent dazu, dass die Abbildung bijektiv und konform ist.\n",
        "* Aus Sicht der Kategorientheorie ist eine biholomorphe Abbildung ein Isomorphismus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StjTRqJOsq4"
      },
      "source": [
        "**Cauchy-Riemannsche Differentialgleichungen**\n",
        "\n",
        "* Siehe [Cauchy-Riemannsche Differentialgleichungen](https://de.wikipedia.org/wiki/Cauchy-Riemannsche_partielle_Differentialgleichungen) - Mit einer Einleitung [hier](https://de.wikipedia.org/wiki/Holomorphe_Funktion#Cauchy-Riemannsche_Differentialgleichungen).\n",
        "\n",
        "* Zerlegt man eine Funktion $f(x+i y)=u(x, y)+i v(x, y)$ in ihren Real-und Imagin√§rteil mit reellen Funktionen $u, v,$ so hat die totale Ableitung $L$ als Darstellungsmatrix die **Jacobi-Matrix**\n",
        "\n",
        ">$\n",
        "\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
        "\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Folglich ist die Funktion $f$ genau dann komplex differenzierbar, wenn sie reell differenzierbar ist und f√ºr $u, v$ die Cauchy-Riemannschen Differentialgleichungen\n",
        "\n",
        ">$\n",
        "\\begin{array}{l}\n",
        "\\frac{\\partial u}{\\partial x}=\\frac{\\partial v}{\\partial y} \\\\\n",
        "\\frac{\\partial u}{\\partial y}=-\\frac{\\partial v}{\\partial x}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "erf√ºllt sind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNafAzsW_vP"
      },
      "source": [
        "**Analytische Funktion (=holomorph)**\n",
        "\n",
        "* Als [analytische Funktion](https://de.wikipedia.org/wiki/Analytische_Funktion) bezeichnet man eine Funktion, die lokal durch eine **konvergente Potenzreihe** gegeben ist.\n",
        "\n",
        "* Aufgrund der Unterschiede zwischen reeller und komplexer Analysis spricht man zur Verdeutlichung oft auch explizit von reell-analytischen oder komplex-analytischen Funktionen.\n",
        "\n",
        "* **Im Komplexen sind die Eigenschaften analytisch und holomorph √§quivalent.**\n",
        "\n",
        "* **Ist eine Funktion in der gesamten komplexen Ebene definiert und analytisch, nennt man sie ganz.**\n",
        "\n",
        "\n",
        "Es sei $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{K}=\\mathbb{C} .$ Es sei $D \\subseteq \\mathbb{K}$ eine offene Teilmenge. Eine Funktion $f: D \\rightarrow \\mathbb{K}$ hei√üt analytisch im Punkt $x_{0} \\in D,$ wenn es eine **Potenzreihe**\n",
        "\n",
        ">$\n",
        "\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "gibt, die auf einer Umgebung von $x_{0}$ gegen $f(x)$ konvergiert. Ist $f$ in jedem Punkt von $D$ analytisch, so hei√üt $f$ analytisch.\n",
        "\n",
        "Viele g√§ngige Funktionen der reellen Analysis wie beispielsweise Polynome, Exponential- und Logarithmusfunktionen, trigonometrische Funktionen und rationale Ausdr√ºcke in diesen Funktionen sind analytisch.\n",
        "\n",
        "Unter einer [Potenzreihe](https://de.wikipedia.org/wiki/Potenzreihe) $P(x)$ versteht man in der Analysis eine unendliche Reihe der Form\n",
        "\n",
        ">$\n",
        "P(x)=\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "mit einer beliebigen Folge $\\left(a_{n}\\right)_{n \\in \\mathbb{N}_{0}}$ reeller oder komplexer Zahlen\n",
        "dem Entwicklungspunkt $x_{0}$ der Potenzreihe.\n",
        "\n",
        "Potenzreihen spielen eine wichtige Rolle in der Funktionentheorie und **erlauben oft eine sinnvolle Fortsetzung reeller Funktionen in die komplexe Zahlenebene**. Insbesondere stellt sich die Frage, f√ºr welche reellen oder komplexen Zahlen eine Potenzreihe konvergiert. Diese Frage f√ºhrt zum Begriff des [Konvergenzradius](https://de.wikipedia.org/wiki/Konvergenzradius).\n",
        "\n",
        "* Jede Polynomfunktion l√§sst sich als Potenzreihe auffassen, bei der fast alle Koeffizienten $a_{n}$ gleich 0 sind.\n",
        "\n",
        "* Wichtige andere Beispiele sind **Taylorreihe** und **Maclaurinsche Reihe**.\n",
        "\n",
        "* Funktionen, die sich durch eine Potenzreihe darstellen lassen, werden auch [analytische Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) genannt.\n",
        "\n",
        "**Beispielhaft die Potenzreihendarstellung einiger bekannter Funktionen**:\n",
        "\n",
        "* **Exponentialfunktion**: $e^{x}=\\exp (x)=\\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !}=\\frac{x^{0}}{0 !}+\\frac{x^{1}}{1 !}+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\cdots$ f√ºr alle\n",
        "$x \\in \\mathbb{R},$ d. h., der Konvergenzradius ist unendlich.\n",
        "\n",
        "* **Sinus**: $\\sin (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n+1}}{(2 n+1) !}=\\frac{x}{1 !}-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !} \\mp \\cdots$\n",
        "\n",
        "* **Kosinus**: $\\cos (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !}=\\frac{x^{0}}{0 !}-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !} \\mp \\cdots$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GiFSM9LMFFp"
      },
      "source": [
        "**Ganze Funktion**\n",
        "\n",
        "* In der Funktionentheorie ist eine [ganze Funktion](https://de.wikipedia.org/wiki/Ganze_Funktion) eine Funktion, **die in der gesamten komplexen Zahleneben**e $\\mathbb {C}$  holomorph (also analytisch) ist.\n",
        "\n",
        "* an entire function, also called an integral function, is a complex-valued function that is holomorphic at all finite points over the whole complex plane.\n",
        "\n",
        "* **Every entire function f(z) can be represented as a power series**\n",
        "\n",
        "* Typische Beispiele ganzer Funktionen sind Polynome oder die **Exponentialfunktion** sowie Summen, Produkte und Verkn√ºpfungen davon, etwa die **trigonometrischen Funktionen** und die **Hyperbelfunktionen**.\n",
        "\n",
        "* Jede ganze Funktion kann als eine √ºberall konvergierende Potenzreihe um ein beliebiges Zentrum dargestellt werden. Weder der Logarithmus noch die Wurzelfunktion sind ganz.\n",
        "\n",
        "* Eine ganze Funktion kann eine **isolierte Singularit√§t**, insbesondere sogar eine wesentliche Singularit√§t im komplexen Punkt im Unendlichen (und nur da) besitzen.\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* der Kehrwert der Gammafunktion $1 / \\Gamma(z)$\n",
        "\n",
        "* die Fehlerfunktion $\\operatorname{erf}(z)$\n",
        "\n",
        "* der Integralsinus $\\operatorname{Si}(z)$\n",
        "\n",
        "* die Airy-Funktionen $\\operatorname{Ai}(z)$ und $\\operatorname{Bi}(z)$\n",
        "\n",
        "* die Fresnelschen Integrale $S(z)$ und $C(z)$\n",
        "\n",
        "* die Riemannsche Xi-Funktion $\\xi(z)$\n",
        "\n",
        "* die Besselfunktionen erster Art $J_{n}(z)$ f√ºr ganzzahlige $n$\n",
        "\n",
        "* die Struve-Funktionen $H_{n}(z)$ f√ºr ganzzahlige $n>-2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Era7KDwoxIS"
      },
      "source": [
        "**Laurent-Reihe**\n",
        "\n",
        "**Exkurs**: Die Laurent-Reihe ist eine **unendliche Reihe √§hnlich einer Potenzreihe**, aber zus√§tzlich **mit negativen Exponenten**. Allgemein hat eine Laurent-Reihe in $x$ mit Entwicklungspunkt $c$ diese Gestalt (Dabei sind die $a_{n}$ und $c$ meist komplexe Zahlen):\n",
        "\n",
        ">$\n",
        "f(x)=\\sum_{n=-\\infty}^{\\infty} a_{n}(x-c)^{n}\n",
        "$\n",
        "\n",
        "Es sei $D$ eine nichtleere offene Teilmenge der Menge $\\mathbb{C}$ der komplexen Zahlen und $P_{f}$ eine weitere Teilmenge von $\\mathbb{C}$, die nur aus isolierten Punkten besteht. Eine Funktion $f$ hei√üt meromorph, wenn sie f√ºr Werte aus $D \\backslash P_{f}$ definiert und holomorph ist und f√ºr Werte aus $P_{f}$ Pole hat. $P_{f}$ wird als Polstellenmenge von $f$ bezeichnet.\n",
        "\n",
        "* Zerlegung einer komplex differenzierbaren Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4Q4vb9RlIR"
      },
      "source": [
        "**Meromorphe Funktion**\n",
        "\n",
        "* Meromorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* F√ºr viele Fragestellungen der Funktionentheorie ist der **Begriff der holomorphen Funktion zu speziell**.\n",
        "\n",
        "* Dies liegt daran, dass der Kehrwert $\\frac{1}{f}$ einer holomorphen Funktion $f$ an einer Nullstelle von $f$ eine Definitionsl√ºcke hat und somit $\\frac{1}{f}$ dort auch **nicht komplex differenzierbar ist**.\n",
        "\n",
        "* Man f√ºhrt daher den allgemeineren Begriff der [meromorphen Funktion](https://de.wikipedia.org/wiki/Meromorphe_Funktion) ein, die auch **isolierte Polstellen** besitzen kann.\n",
        "\n",
        "* Meromorphe Funktionen lassen sich lokal als [Laurentreihen](https://de.wikipedia.org/wiki/Laurent-Reihe) mit abbrechendem Hauptteil darstellen. Ist $U$ ein Gebiet von $\\mathbb{C},$ so bildet die Menge der auf $U$ meromorphen Funktionen einen K√∂rper.\n",
        "\n",
        "* Alle holomorphen Funktionen sind auch meromorph, da ihre Polstellenmenge leer ist.\n",
        "\n",
        "Zum Beispiel ist die Gamma-Funktion meromorph, weil sie holomorph ist auf $\\mathbb{C}$, abgesehen von abzahlbar vielen nicht-hebbaren Singularitaeten (hierbei in allen negativen ganzen Zahlen, da der Definitionsbereich einer Gammafunktion $\\mathbb{C}$  \\ -$\\mathbb{N}$ <sub>0</sub> ist).\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* [Gamma Funktion](https://en.wikipedia.org/wiki/Gamma_function)\n",
        "\n",
        "* [Elliptische Funktion](https://de.wikipedia.org/wiki/Elliptische_Funktion)\n",
        "\n",
        "*Der Absolutwert der Gammafunktion geht nach Unendlich an den Polstellen (links). Rechts hat sie keine Polstellen und steigt nur schnell an.*\n",
        "\n",
        "*The [gamma function](https://en.wikipedia.org/wiki/Gamma_function) is meromorphic in the whole complex plane.*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/3/33/Gamma_abs_3D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjZAY8WRubkK"
      },
      "source": [
        "**Isolierte Singularit√§t**\n",
        "\n",
        "* [Isolierte Singularit√§ten](https://de.wikipedia.org/wiki/Isolierte_Singularit√§t) sind besondere [**isolierte Punkte**](https://de.wikipedia.org/wiki/Isolierter_Punkt) in der Quellmenge einer holomorphen Funktion.\n",
        "\n",
        "* Man unterscheidet bei isolierten Singularit√§ten zwischen **hebbaren Singularit√§ten**, **Polstellen** und **wesentlichen Singularit√§ten**.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{C}$ eine offene Teilmenge, $z_{0} \\in \\Omega$. Ferner sei $f: \\Omega \\backslash\\left\\{z_{0}\\right\\} \\rightarrow \\mathbb{C}$ eine holomorphe komplexwertige Funktion. Dann hei√üt $z_{0}$ isolierte Singularit√§t von $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAgJBUpT0Oo"
      },
      "source": [
        "**Klasse 1: Hebbare Singularit√§t (Definitionsl√ºcke)**\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [hebbare Singularit√§t](https://de.wikipedia.org/wiki/Definitionsl√ºcke), wenn $f$ auf $\\Omega$ holomorph fortsetzbar ist.\n",
        "\n",
        "* Hat Grenzwert in Form von einer Zahl bzw. nach dem riemannschen Hebbarkeitssatz, wenn $f$ in einer Umgebung von $z_{0}$ beschr√§nkt ist.\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z)=c$\n",
        "\n",
        "> $f(z)$ beschrankt in $U\\left(z_{0}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAm2tWA_vART"
      },
      "source": [
        "**Klasse 2: Polstelle**\n",
        "\n",
        "* im Prinzip das gleiche wie bei hebbaren Singularitaten, aber mit dem Unterschied, dass die Funktion nah an der Singularitat unbeschraenkt ist = also ins unendliche geht $\\lim _{z \\rightarrow z_{0}}|f(z)|=\\infty$\n",
        "\n",
        "Alternative Definition:\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z) \\cdot\\left(z-z_{0}\\right)^{k}=c \\neq 0(k \\geqslant 1)$\n",
        "\n",
        "Man multipliziert an den unendlichen Funktionswert immer oefter eine fast Null in Form des Polynoms $\\left(z-z_{0}\\right)^{k}$, bis man eine endliche komplexe Zahl als Grenzwert hat, und nicht mehr unendlich (aber nicht Null). Die Anzahl der Polynome k multipliziert mit dem Funktionswert ist der Grad / Ordnung der Polstelle.\n",
        "\n",
        "* Der Punkt $z_{0}$ hei√üt [Polstelle](https://de.wikipedia.org/wiki/Polstelle) oder $P o l$, wenn\n",
        "\n",
        "  * $z_{0}$ **keine hebbare Singularit√§t ist** und\n",
        "  * es eine nat√ºrliche $\\operatorname{Zahl} k$ gibt, sodass $\\left(z-z_{0}\\right)^{k} \\cdot f(z)$ **eine hebbare Singularit√§t bei $z_{0}$ hat**.\n",
        "\n",
        "* Ist das $k$ minimal gew√§hlt, dann sagt man $f$ habe in $z_{0}$ einen Pol $k$ -ter Ordnung.\n",
        "\n",
        "* Man bezeichnet eine einpunktige Definitionsl√ºcke einer Funktion als Polstelle oder auch k√ºrzer als Pol, wenn die Funktionswerte in jeder Umgebung des Punktes (betragsm√§√üig) beliebig gro√ü werden.\n",
        "\n",
        "* Damit geh√∂ren die Polstellen zu den isolierten Singularit√§ten.\n",
        "\n",
        "* Das Besondere an Polstellen ist, dass sich die Punkte in einer Umgebung **nicht chaotisch verhalten, sondern in einem gewissen Sinne gleichm√§√üig gegen unendlich streben**. Deshalb k√∂nnen dort Grenzwertbetrachtungen durchgef√ºhrt werden.\n",
        "\n",
        "* Generell spricht man nur bei [glatten](https://de.wikipedia.org/wiki/Glatte_Funktion) (stetig & differenzierbar) oder [analytischen Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) von Polen.\n",
        "\n",
        "*Fur reelle Funktionen: f(x)=1/x hat einen Pol erster Ordnung an der Stelle x=0**\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/9/90/GraphKehrwertfunktion.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNpaYyHVD4o"
      },
      "source": [
        "**Klasse 3: Wesentliche Singularit√§t**\n",
        "\n",
        "* nicht hebbar und keine Polstelle, dann hei√üt $z_{0}$ eine wesentliche Singularit√§t von $f$\n",
        "\n",
        "* zum Beispiel fur $f(z)=e^{-\\frac{1}{z}}, z_{0}=0$, eine Funktion die von links ins unendliche strebt, und von rechts nach Null\n",
        "  * keine hebbare Singularitaet, weil zwei verschiedene Grenzwerte existieren und eine davon nicht endlich ist.\n",
        "  * keine Polstelle, weil ein Grenzwert gleich Null ist (muss aber unbeschraenkt sein bei Polstellen)\n",
        "\n",
        "\n",
        "*Plot der Funktion $\\exp(1/z)$. Sie hat im Nullpunkt eine wesentliche Singularit√§t (Bildmitte). Der Farbton entspricht dem komplexen Argument des Funktionswertes, w√§hrend die Helligkeit seinen Betrag darstellt. Hier sieht man, dass sich die wesentliche Singularit√§t unterschiedlich verh√§lt, je nachdem, wie man sich ihr n√§hert (im Gegensatz dazu w√§re ein Pol gleichm√§√üig wei√ü).*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/0/0b/Essential_singularity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSHsrLNpJDv"
      },
      "source": [
        "**Cauchysche Integralformel**\n",
        "\n",
        "* Die [cauchysche Integralformel](https://de.wikipedia.org/wiki/Cauchysche_Integralformel) ist eine der fundamentalen Aussagen der Funktionentheorie, eines Teilgebietes der Mathematik.\n",
        "\n",
        "* Sie besagt in ihrer schw√§chsten Form, dass die Werte einer holomorphen Funktion $f$ im Inneren einer Kreisscheibe bereits durch ihre Werte auf dem Rand dieser Kreisscheibe bestimmt sind.\n",
        "\n",
        "* Eine starke Verallgemeinerung davon ist der Residuensatz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVLX7lrIqBtX"
      },
      "source": [
        "**Residuensatz**\n",
        "\n",
        "* Der [Residuensatz](https://de.wikipedia.org/wiki/Residuensatz) ist ein wichtiger Satz der Funktionentheorie. Er stellt eine **Verallgemeinerung des cauchyschen Integralsatzes und der cauchyschen Integralformel** dar. Seine Bedeutung liegt nicht nur in den weitreichenden Folgen innerhalb der Funktionentheorie, sondern auch in der Berechnung von Integralen √ºber reelle Funktionen.\n",
        "\n",
        "* Er besagt, dass das Kurvenintegral l√§ngs einer geschlossenen Kurve √ºber eine bis auf isolierte Singularit√§ten holomorphe Funktion lediglich vom Residuum in den Singularit√§ten im Innern der Kurve und der Umlaufzahl der Kurve um diese Singularit√§ten abh√§ngt. Anstelle eines Kurvenintegrals muss man also nur Residuen und Umlaufzahlen berechnen, was in vielen F√§llen einfacher ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivq0YtFjjOPz"
      },
      "source": [
        "**Cauchyscher Integralsatz**\n",
        "\n",
        "* Der [cauchysche Integralsatz](https://de.wikipedia.org/wiki/Cauchyscher_Integralsatz)  ist einer der wichtigsten S√§tze der Funktionentheorie.\n",
        "\n",
        "* Er handelt von Kurvenintegralen f√ºr holomorphe (auf einer offenen Menge komplex-differenzierbare) Funktionen.\n",
        "\n",
        "* Im Kern besagt er, dass zwei dieselben Punkte verbindende Wege das gleiche Wegintegral besitzen, falls die Funktion √ºberall zwischen den zwei Wegen holomorph ist.\n",
        "\n",
        "* Der Satz gewinnt seine Bedeutung unter anderem daraus, dass man ihn zum Beweis der cauchyschen Integralformel und des Residuensatzes benutzt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHwO7VW3o75X"
      },
      "source": [
        "##### <font color=\"blue\">*Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JijVDdYpeTw"
      },
      "source": [
        "###### *Chaos Theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJSt90Zne8M"
      },
      "source": [
        "**Chaos Theory**\n",
        "\n",
        "* Many phenomena in nature can be described by dynamical systems; [chaos theory](https://en.m.wikipedia.org/wiki/Chaos_theory) makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n",
        "\n",
        "* [Complexity theory](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory. Chaos theory is a method of qualitative and quantitative analysis to investigate the behavior of [dynamical systems](https://en.m.wikipedia.org/wiki/Dynamical_system) that cannot be explained and predicted by single data relationships, but must be explained and predicted by whole, continuous data relationships.\n",
        "\n",
        "* Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random.\n",
        "\n",
        "* The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things:\n",
        "\n",
        "  * how much uncertainty can be tolerated in the forecast (uncertainty in a forecast increases exponentially with elapsed time)\n",
        "  * how accurately its current state can be measured,\n",
        "  * and a time scale depending on the dynamics of the system, called the [Lyapunov time](https://en.m.wikipedia.org/wiki/Lyapunov_time) (chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years)\n",
        "\n",
        "* In practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Lyapunov exponent*"
      ],
      "metadata": {
        "id": "RTdYXBryoZqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lyapunov exponent**\n",
        "\n",
        "* the Lyapunov exponent or Lyapunov characteristic exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories.\n",
        "\n",
        "* Quantitatively, two trajectories in phase space with initial separation vector $\\delta \\mathbf {Z} _{0}$ diverge (provided that the divergence can be treated within the linearized approximation) at a rate given by\n",
        "\n",
        "> ${\\displaystyle |\\delta \\mathbf {Z} (t)|\\approx e^{\\lambda t}|\\delta \\mathbf {Z} _{0}|}$\n",
        "\n",
        "where $\\lambda$ is the Lyapunov exponent.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1197.png)"
      ],
      "metadata": {
        "id": "uoo8xNAEmiam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complex Systems*"
      ],
      "metadata": {
        "id": "fhdD4JnqZHKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on4GTn_0efWt"
      },
      "source": [
        "**Complex Systems**\n",
        "\n",
        "* [Complex Systems](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* [Chaos](https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory) is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "*Properties:*\n",
        "\n",
        "1. **Agentenbasiert**: Komplexe Systeme bestehen aus einzelnen Teilen, die miteinander in Wechselwirkung stehen (Molek√ºle, Individuen, Software-Agenten etc.).\n",
        "2. **Nichtlinearit√§t**: Kleine St√∂rungen des Systems oder minimale Unterschiede in den Anfangsbedingungen f√ºhren oft zu sehr unterschiedlichen Ergebnissen (Schmetterlingseffekt, Phasen√ºberg√§nge). Die Wirkzusammenh√§nge der Systemkomponenten sind im Allgemeinen nichtlinear.\n",
        "3. [**Emergenz**](https://de.wikipedia.org/wiki/Emergenz): Im Gegensatz zu lediglich komplizierten Systemen zeigen komplexe Systeme Emergenz. Entgegen einer verbreiteten Vereinfachung bedeutet Emergenz nicht, dass die Eigenschaften der emergierenden Systemebenen von den darunter liegenden Ebenen vollst√§ndig unabh√§ngig sind. Emergente Eigenschaften lassen sich jedoch auch nicht aus der isolierten Analyse des Verhaltens einzelner Systemkomponenten erkl√§ren und nur sehr begrenzt ableiten.\n",
        "4. **Wechselwirkung** (Interaktion): Die Wechselwirkungen zwischen den Teilen des Systems (Systemkomponenten) sind lokal, ihre Auswirkungen in der Regel global.\n",
        "5. **Offenes System**: Komplexe Systeme sind √ºblicherweise offene Systeme. Sie stehen also im Kontakt mit ihrer Umgebung und befinden sich fern vom thermodynamischen Gleichgewicht. Das bedeutet, dass sie von einem permanenten Durchfluss von Energie bzw. Materie abh√§ngen.\n",
        "6. **Selbstorganisation**: Dies erm√∂glicht die Bildung insgesamt stabiler Strukturen (Selbststabilisierung oder Hom√∂ostase), die ihrerseits das thermodynamische Ungleichgewicht aufrechterhalten. Sie sind dabei in der Lage, Informationen zu verarbeiten bzw. zu lernen.\n",
        "7. **Selbstregulation**: Dadurch k√∂nnen sie die F√§higkeit zur inneren Harmonisierung entwickeln. Sie sind also in der Lage, aufgrund der Informationen und derer Verarbeitung das innere Gleichgewicht und Balance zu verst√§rken.\n",
        "8. **Pfade**: Komplexe Systeme zeigen Pfadabh√§ngigkeit: Ihr zeitliches Verhalten ist nicht nur vom aktuellen Zustand, sondern auch von der Vorgeschichte des Systems abh√§ngig.\n",
        "9. **Attraktoren**: Die meisten komplexen Systeme weisen so genannte Attraktoren auf, d. h., dass das System unabh√§ngig von seinen Anfangsbedingungen bestimmte Zust√§nde oder Zustandsabfolgen anstrebt, wobei diese Zustandsabfolgen auch chaotisch sein k√∂nnen; dies sind die ‚Äûseltsamen Attraktoren‚Äú der Chaosforschung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YzAyotrq51A"
      },
      "source": [
        "* Complexity theory is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIwXOz85zxW_"
      },
      "source": [
        "###### *Fractal Dimensions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzaMSBJmGj_F"
      },
      "source": [
        "**Fractal Geometry**\n",
        "\n",
        "* a [fractal](https://en.m.wikipedia.org/wiki/Fractal) is a subset of Euclidean space with a fractal dimension that strictly exceeds its topological dimension. Fractals appear the same at different scales, as illustrated in successive magnifications of the Mandelbrot set.\n",
        "\n",
        "* Fractals exhibit similar patterns at increasingly smaller scales, a property called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge,it is called affine self-similar.\n",
        "\n",
        "* idealization is that everything is smooth (rebellion against calculus, differentiable, where assumption is things look smooth if you zoom in enough). Mandelbrot: nature is fractal (capture roughness)\n",
        "\n",
        "* self-similar shapes give a basis for modeling the regularity in some forms of roughness. but that doesn't mean all is only perfectly self-similar either!! (Perfect self similar are: Von Koch snowflake, Sierpensky triangle)\n",
        "\n",
        "> <font color=\"blue\">Fractal dimension: Sierpensky triangle is 1,585 dimensional, Von Koch snowflake is 1,262 dimensional, Britain coast line 1,21 dimensional, Norway: 1,52 dimensional, calm sea 2,05 dimensional, waves 2,3 dimensional</font>\n",
        "\n",
        "> **Fractals are shapes with a non-integer dimension, captures idea of roughness, but dimension can vary depending on how much you zoom in**. But a shape is considered a fractal only when the measures dimension stays approximately constant across multiple different scales.\n",
        "\n",
        "> Useful in security: **Is something fractal? Yes - then probably from nature. No - then probably man-made**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deterministic fractals**:\n",
        "  * [Julia set](https://de.m.wikipedia.org/wiki/Julia-Menge) - siehe auch [Newtonfraktal](https://de.m.wikipedia.org/wiki/Newtonfraktal). The Julia set and the Fatou set are two complementary sets (Julia \"laces\" and Fatou \"dusts\").\n",
        "  * [Mandelbrot set](https://en.m.wikipedia.org/wiki/Mandelbrot_set) (is kind of like a map of Julia sets)\n",
        "  * [Logistic map](https://de.m.wikipedia.org/wiki/Logistische_Gleichung) (Feigenbaum Attractor) with [Feigenbaum-Konstante](https://de.m.wikipedia.org/wiki/Feigenbaum-Konstante),\n",
        "  * Peano curve, Pentaflake, 3D Hilbert curve etc.\n",
        "  * A [fractal curve](https://en.m.wikipedia.org/wiki/Fractal_curve) is, loosely, a mathematical curve whose shape retains the same general pattern of irregularity, regardless of how high it is magnified, that is, its graph takes the form of a fractal. They do NOT have finite length. A famous example is the boundary of the Mandelbrot set. [Space-filling curves](https://en.m.wikipedia.org/wiki/Space-filling_curve) are special cases of fractal curves. Its range contains the entire 2-dimensional unit square (or more generally an n-dimensional unit hypercube). Space-filling curves in the 2-dimensional plane are sometimes called [Peano curve](https://en.m.wikipedia.org/wiki/Peano_curve). Peano curves are constructed by [Hilbert curves](https://en.m.wikipedia.org/wiki/Hilbert_curve) to form a single continuous loop over the entire sphere.\n",
        "\n",
        "\n",
        "**Random and natural fractals**:\n",
        "  * Zeros of a Wiener process,\n",
        "  * Brownian motion,\n",
        "  * [Coastline of Ireland, Great Britain or Norway](https://en.m.wikipedia.org/wiki/Coastline_paradox)  (Coastline paradox)\n",
        "  * von [Koch curve](https://de.m.wikipedia.org/wiki/Koch-Kurve) with random orientation\n",
        "  * The surface of Broccoli or human brain,\n",
        "  * Distribution of [galaxy clusters](https://en.m.wikipedia.org/wiki/Galaxy_cluster)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1196.png)\n",
        "\n",
        "*Zeros of a Wiener process:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/8/83/Wiener_process_set_of_zeros.gif)"
      ],
      "metadata": {
        "id": "PeB3l9rViknz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fractal Dimensions**: die [fraktale Dimension](https://de.m.wikipedia.org/wiki/Fraktale_Dimension) einer Menge ist eine Verallgemeinerung des Dimensionsbegriffs von geometrischen Objekten wie Kurven (eindimensional) und Fl√§chen (zweidimensional).\n",
        "\n",
        "  * [Hausdorff dimension](https://de.m.wikipedia.org/wiki/Hausdorff-Dimension):  Hausdorff dimension of a single point is zero, of a line segment is 1, of a square is 2, and of a cube is 3. That is, for sets of points that define a smooth shape or a shape that has a small number of corners‚Äîthe shapes of traditional geometry and science‚Äîthe Hausdorff dimension is an integer agreeing with the usual sense of dimension, also known as the [topological dimension (inductive dimension)](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "  * [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "  * [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension)\n",
        "  * [Box-counting dimension](https://en.m.wikipedia.org/wiki/Minkowski‚ÄìBouligand_dimension) (Minkowski‚ÄìBouligand dimension)\n",
        "  * [List of fractals by Hausdorff dimension](https://en.m.wikipedia.org/wiki/List_of_fractals_by_Hausdorff_dimension)\n",
        "\n",
        "*Example of Box Counting dimension:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Great_Britain_Box.svg/640px-Great_Britain_Box.svg.png)\n"
      ],
      "metadata": {
        "id": "b-iO6R8pfzwE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcbvY2o-JyH"
      },
      "source": [
        "###### *Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-OG2Rw8EzzH"
      },
      "source": [
        "**Dynamisches System**\n",
        "\n",
        "> **A dynamical system involves one or more variables that change over time according to autonomous differential equations.**\n",
        "\n",
        "> $\\dot{x}=$ rate of change of $x$ as time changes\n",
        "\n",
        "> $\\dot{y}=$ rate of change of $y$ as time changes\n",
        "\n",
        "They depend on t:\n",
        "\n",
        "$\\frac{d x}{d t}=$ rate of change of $x$ as time changes $\\frac{d y}{d t}=$ rate of change of $y$ as time changes\n",
        "\n",
        "but the differential equation that describe x dot and y dot don‚Äôt actually involve time t:\n",
        "\n",
        "> $\\dot{x}=-y-0.1 x$\n",
        "\n",
        "> $\\dot{y}=x-0.4 y$\n",
        "\n",
        "This makes them autonomous. Each combination of x and y to only corresponds to one combination of x dot and y dot. You can represent it in a dynamical system called the phase space. Each point in space is a unique state of the system. And has its own rate of change shown as a vector.\n",
        "\n",
        "> <font color=\"red\">Siehe auch: [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. STS is interesting because it bridges the two major parts of mathematical physics ‚Äì the [dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory) and [topological (quantum) field theories](https://en.m.wikipedia.org/wiki/Topological_quantum_field_theory). Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.</font>\n",
        "\n",
        "* [Dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory). Ein (deterministisches) [dynamisches System](https://de.m.wikipedia.org/wiki/Dynamisches_System) ist ein Modell eines zeitabh√§ngigen Prozesses, der homogen bez√ºglich der Zeit ist (Verlauf h√§ngt nur vom Anfangszustand, aber nicht von der Wahl des Anfangszeitpunkts)\n",
        "\n",
        "* Wichtige Fragestellungen: Langzeitverhalten (zum Beispiel [Stabilit√§t](https://de.wikipedia.org/wiki/Stabilit√§tstheorie), [Periodizit√§t](https://de.wikipedia.org/wiki/Periodische_Funktion), [Chaos](https://de.wikipedia.org/wiki/Chaosforschung) und [Ergodizit√§t](https://de.wikipedia.org/wiki/Ergodizit√§t)), die [Systemidentifikation](https://de.wikipedia.org/wiki/Systemidentifikation) und ihre [Regelung](https://de.wikipedia.org/wiki/Regelung_(Natur_und_Technik))\n",
        "\n",
        "* Formal betrachte man ein **dynamisches System** bestehend aus einem topologischen Raum $X$ und einer Transformation $f: \\mathcal{T} \\times X \\longrightarrow X$, wobei $\\mathcal{T}$ ein linear geordnetes Monoid ist wie $\\mathcal{T}=\\mathbb{N}, \\mathbb{Z},[0, \\infty[$ oder $\\mathbb{R}$ und $f$ normalerweise stetig oder mindestens messbar ist (oder mindestens wird verlangt, dass $f(t, \\cdot): X \\longrightarrow X$ stetig/messbar ist f√ºr jedes $t \\in \\mathcal{T}$ ) und erf√ºllt $f(t+s, x)=f(t, f(s, x))$ f√ºr alle ¬ªZeiten ¬´ $t, s \\in \\mathcal{T}$ und Punkte $x \\in X$.\n",
        "\n",
        "\n",
        "Ein dynamisches System ist ein Tripel $(T, X, \\Phi)$, bestehend aus\n",
        "\n",
        "* **Zeitraum**: einer Menge $T=\\mathbb{N}_{0}, \\mathbb{Z}, \\mathbb{R}_{0}^{+}$ oder $\\mathbb{R}$,\n",
        "* **Zustandsraum** (dem Phasenraum): einer nichtleeren Menge $X$,\n",
        "* **Operation** $\\Phi: T \\times X \\rightarrow X$ von $T$ auf $X,$\n",
        "\n",
        "so dass f√ºr alle Zust√§nde $x \\in X$ und alle Zeitpunkte $t, s \\in T$ gilt:\n",
        "\n",
        "1. **Identit√§tseigenschaft**: $\\Phi(0, x)=x$\n",
        "\n",
        "2. **Halbgruppeneigenschaft**: $\\Phi(s, \\Phi(t, x))=\\Phi(s+t, x)$\n",
        "\n",
        "* Wenn $T=\\mathbb{N}_{0}$ oder $T=\\mathbb{Z}$ ist, dann hei√üt $(T, X, \\Phi)$ **zeitdiskret** oder kurz diskret, und mit $T=\\mathbb{R}_{0}^{+}$ oder $T=\\mathbb{R}$ nennt man $(T, X, \\Phi)$ **zeitkontinuierlich** oder kontinuierlich.\n",
        "\n",
        "* Beispiele: [Exponentielles Wachstum und Federpendel](https://de.m.wikipedia.org/wiki/Dynamisches_System#Einf√ºhrende_Beispiele)\n",
        "\n",
        "  * das Str√∂mungsverhalten von Fl√ºssigkeiten und Gasen\n",
        "  * Bewegungen von Himmelsk√∂rpern unter gegenseitiger Beeinflussung durch die Gravitation\n",
        "  * Populationsgr√∂√üen von Lebewesen unter Ber√ºcksichtigung der R√§uber-Beute-Beziehung\n",
        "  * die Entwicklung wirtschaftlicher Kenngr√∂√üen unter Einfluss der Marktgesetze.\n",
        "\n",
        "* Das Langzeitverhalten eines dynamischen Systems l√§sst sich durch den globalen Attraktor beschreiben, da bei physikalischen oder technischen Systemen oft **Dissipation vorliegt, insbesondere Reibung**.\n",
        "\n",
        "* Die [Determiniertheit (als Systemeigenschaft)](https://de.wikipedia.org/wiki/Systemeigenschaften#Determiniertheit) ist der Grad der ‚ÄûVorbestimmtheit‚Äú des Systems: Ein System geht von einem Zustand Z1 in den Zustand Z2 √ºber: Z1 ‚Üí Z2. Bei deterministischen Systemen ist dieser √úbergang bestimmt (zwingend), bei stochastischen wahrscheinlich. Deterministische Systeme erlauben prinzipiell die Ableitung ihres Verhaltens aus einem vorherigen Zustand, stochastische Systeme nicht. **Aus der Komplexit√§t eines Systems l√§sst sich keine Aussage √ºber die Vorhersagbarkeit treffen**: Es gibt einfache deterministische Systeme, die chaotisch sind (z. B. Doppelpendel) und komplexe deterministische Systeme (Chloroplasten bei der Photosynthese).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYGXfTQpV2tS"
      },
      "source": [
        "**Phasenraum und Trajektorie**\n",
        "\n",
        "* Der [Phasenraum](https://de.wikipedia.org/wiki/Phasenraum) beschreibt die Menge aller m√∂glichen Zust√§nde eines dynamischen Systems. Ein Zustand wird durch einen Punkt im Phasenraum eindeutig abgebildet.\n",
        "\n",
        "* **Jeder Zustand ist ein Punkt im Phasenraum und wird durch beliebig viele [Zustandsgr√∂√üen](https://de.m.wikipedia.org/wiki/Zustandsgr√∂√üe) dargestellt, welche die Dimensionen des Phasenraums bilden.**\n",
        "\n",
        "* kontinuierliche Systeme werden durch Linien (Trajektorien) repr√§sentiert\n",
        "* diskrete Systeme werden durch Mengen isolierter Punkte repr√§sentiert.\n",
        "\n",
        "* In der Mechanik besteht er aus verallgemeinerten Koordinaten (Konfigurationsraum) und zugeh√∂rigen verallgemeinerten Geschwindigkeiten, siehe [Prinzip der virtuellen Leistung](https://de.m.wikipedia.org/wiki/Prinzip_der_virtuellen_Leistung)\n",
        "\n",
        "* Die zeitliche Entwicklung eines Punktes im Phasenraum wird durch **Differentialgleichungen** beschrieben und durch **Trajektorien** (Bahnkurven, Orbit) im Phasenraum dargestellt. Dies sind **Differentialgleichungen erster Ordnung in der Zeit** und durch einen Anfangspunkt eindeutig festgelegt (ist die Differentialgleichung zeitunabh√§ngig, sind dies **autonome Differentialgleichungen**). Dementsprechend kreuzen sich zwei Trajektorien im Phasenraum auch nicht, da an einem Kreuzungspunkt der weitere Verlauf nicht eindeutig ist. Geschlossene Kurven beschreiben oszillierende (periodische) Systeme.\n",
        "\n",
        "* *Konstruktion eines Phasen(raum)portr√§ts f√ºr ein [mathematisches Pendel](https://de.wikipedia.org/wiki/Mathematisches_Pendel)*\n",
        "\n",
        "![hh](https://upload.wikimedia.org/wikipedia/commons/c/cd/Pendulum_phase_portrait_illustration.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m066sar7fCUi"
      },
      "source": [
        "###### *Attractor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxw1WlN9hhlY"
      },
      "source": [
        "**Attraktor**\n",
        "\n",
        "* [Attraktor](https://de.wikipedia.org/wiki/Attraktor) ist ein Begriff aus der Theorie dynamischer Systeme und beschreibt **eine Untermenge eines Phasenraums** (d. h. eine gewisse Anzahl von Zust√§nden), auf die sich ein dynamisches System im Laufe der Zeit zubewegt und die unter der Dynamik dieses Systems nicht mehr verlassen wird.\n",
        "\n",
        "* Das hei√üt, eine Menge von Variablen n√§hert sich im Laufe der Zeit (asymptotisch) einem bestimmten Wert, einer Kurve oder etwas Komplexerem (also einer Region im n-dimensionalen Raum) und bleibt dann im weiteren Zeitverlauf in der N√§he dieses Attraktors.\n",
        "\n",
        "* Bekannte Beispiele sind der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor), der [R√∂ssler-Attraktor](https://de.wikipedia.org/wiki/R%C3%B6ssler-Attraktor) und die [Nullstellen](https://de.wikipedia.org/wiki/Nullstelle) einer differenzierbaren Funktion, welche Attraktoren des zugeh√∂rigen Newton-Verfahrens sind.\n",
        "\n",
        "> **Die Menge aller Punkte des Phasenraums, die unter der Dynamik demselben Attraktor zustreben, hei√üt Attraktions- oder Einzugsgebiet dieses Attraktors**.\n",
        "\n",
        "\n",
        "Unter einem Attraktor versteht man eine Teilmenge $A \\subseteq X$,\n",
        "die den folgenden Bedingungen gen√ºgt\n",
        "1. $A$ ist vorw√§rts invariant;\n",
        "2. Das Sammelbecken $B(A)$ ist eine Umgebung von $A$;\n",
        "3. $A$ ist eine minimale nicht leere Teilmenge von $X$ mit Bedingungen 1\n",
        "und 2 .\n",
        "\n",
        "Bedingung 1 erfordert eine gewisse Stabilit√§t des Attraktors. Daraus folgt offensichtlich, dass $A \\subseteq B(A)$. Anhand Bedingung 2 wird weiterhin verlangt, dass $A \\subseteq B(A)^{\\circ}$ und bedeutet u. a., jeder Punkt in einer gewissen N√§he von $A$ n√§here sich dem Attraktor beliebig. Manche Autoren lassen Bedingung 2 weg. Bedingung 3 erfordert, dass der Attraktor nicht in weitere Komponenten zerlegt werden kann (ansonsten\n",
        "w√§re bspw. der ganze Raum trivialerweise ein Attraktor).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cai3DPiwy4"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine ganzzahlige Dimension besitzen**\n",
        "\n",
        "*Bei der Untersuchung dynamischer Systeme interessiert man sich ‚Äì ausgehend von einem bestimmten [Anfangszustand (=Anfangsbedingung)](https://de.wikipedia.org/wiki/Anfangsbedingung) ‚Äì vor allem f√ºr das Verhalten f√ºr $t\\to \\infty$ . Der Grenzwert in diesem Fall wird als Attraktor bezeichnet. Typische und h√§ufige Beispiele von Attraktoren sind:*\n",
        "\n",
        "* **asymptotisch stabile Fixpunkte**: Das System n√§hert sich immer st√§rker einem bestimmten Endzustand an, in dem die Dynamik erliegt; ein statisches System entsteht. Typisches Beispiel ist ein ged√§mpftes Pendel, das sich dem Ruhezustand im tiefsten Punkt ann√§hert.\n",
        "\n",
        "* **(asymptotisch) stabile Grenzzyklen**: Der Endzustand ist die Abfolge gleicher Zust√§nde, die periodisch durchlaufen werden (periodische Orbits). Ein Beispiel daf√ºr ist die Simulation der R√§uber-Beute-Beziehung, die f√ºr bestimmte Parameter der R√ºckkoppelung auf ein periodisches Ansteigen und Sinken der Populationsgr√∂√üen hinausl√§uft.\n",
        "\n",
        "* F√ºr ein hybrides dynamisches System mit chaotischer Dynamik konnte im $\\mathbb {R} ^{n}$ die Oberfl√§che eines [n-Simplex](https://de.wikipedia.org/wiki/Simplex_(Mathematik)) als Attraktor identifiziert werden: (asymptotisch stabile) Grenztori: Treten mehrere miteinander inkommensurable Frequenzen auf, so ist die Trajektorie nicht geschlossen, und der Attraktor ist ein Grenztorus, der von der Trajektorie asymptotisch vollst√§ndig ausgef√ºllt wird. Die zu diesem Attraktor korrespondierende Zeitreihe ist quasiperiodisch, d. h., es gibt keine echte Periode, aber das Frequenzspektrum besteht aus scharfen Linien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMS8uZlNlFTx"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine fraktale Dimension besitzen**\n",
        "\n",
        "*Die Existenz von Attraktoren mit komplizierterer Struktur war zwar schon l√§nger bekannt, man betrachtete sie aber zun√§chst als instabile Sonderf√§lle, deren Auftreten nur bei bestimmter Wahl des Ausgangszustands und der Systemparameter beobachtet wird. Dies √§nderte sich mit der Definition eines neuen, speziellen Typs von Attraktor:*\n",
        "\n",
        "* [Seltsamer Attraktor](https://de.wikipedia.org/wiki/Seltsamer_Attraktor): In seinem Endzustand zeigt das System h√§ufig ein chaotisches Verhalten (es gibt jedoch auch Ausnahmen, z. B. quasiperiodisch angetriebene nichtlineare Systeme).  Ein seltsamer Attraktor ist ein Attraktor, also ein Ort im Phasenraum, der den Endzustand eines dynamischen Prozesses darstellt, **dessen fraktale Dimension nicht ganzzahlig und dessen Kolmogorov-Entropie echt positiv ist**. Es handelt sich damit um ein Fraktal, das nicht in geschlossener Form geometrisch beschrieben werden kann. Gelegentlich wird auch der Begriff chaotischer Attraktor bevorzugt, da die ‚ÄûSeltsamkeit‚Äú dieses Objekts sich mit den Mitteln der Chaostheorie erkl√§ren l√§sst. Der dynamische Prozess zeigt ein aperiodisches Verhalten.\n",
        "\n",
        "    * [Lorenz Attraktor](https://de.m.wikipedia.org/wiki/Lorenz-Attraktor)\n",
        "\n",
        "    * [R√∂ssler attractor](https://en.m.wikipedia.org/wiki/R√∂ssler_attractor)\n",
        "\n",
        "  * [Multiscroll attractor](https://en.m.wikipedia.org/wiki/Multiscroll_attractor)\n",
        "\n",
        "  * [H√©non map](https://en.m.wikipedia.org/wiki/H√©non_map)\n",
        "\n",
        "* Der seltsame Attraktor l√§sst sich **nicht in einer geschlossenen geometrischen Form** beschreiben und **besitzt keine ganzzahlige Dimension**. Attraktoren nichtlinearer dynamischer Systeme weisen dann eine **fraktale Struktur** auf.\n",
        "\n",
        "* Wichtiges Merkmal ist das chaotische Verhalten, d. h., jede noch so geringe √Ñnderung des Anfangszustands f√ºhrt im weiteren Verlauf zu signifikanten Zustands√§nderungen. Prominentestes Beispiel ist der **Lorenz-Attraktor, der bei der Modellierung von Luftstr√∂mungen in der Atmosph√§re entdeckt wurde**.\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Attractor#Fixed_point\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Limit_cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAvwOlbmQn8"
      },
      "source": [
        "**Special: Lorenz-Attraktor**\n",
        "\n",
        "Der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor) oder englisch: [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) ist der seltsame Attraktor eines Systems von drei gekoppelten, nichtlinearen **gew√∂hnlichen Differentialgleichungen**:\n",
        "\n",
        "$\\dot{X}=a(Y-X)$\n",
        "\n",
        "$\\dot{Y}=X(b-Z)-Y$\n",
        "\n",
        "$\\dot{Z}=X Y-c Z$\n",
        "\n",
        "* Formuliert wurde das System um 1963 von dem Meteorologen Edward N. Lorenz, der es als Idealisierung eines [hydrodynamischen Systems (Fluiddynamik)](https://de.wikipedia.org/wiki/Fluiddynamik) entwickelte. Basierend auf einer Arbeit von Barry Saltzman (1931‚Äì2001) ging es Lorenz dabei um eine Modellierung der Zust√§nde in der Erdatmosph√§re zum Zweck einer Langzeitvorhersage.\n",
        "\n",
        "* Allerdings betonte Lorenz, dass das von ihm entwickelte System allenfalls f√ºr sehr begrenzte Parameterbereiche von $a,b,c$ realistische Resultate liefert.\n",
        "\n",
        "* Die mathematische Beschreibung des Modells durch die Navier-Stokes-Gleichungen f√ºhrt √ºber verschiedene Vereinfachungen, beispielsweise endlich abgebrochene Reihendarstellungen, zu dem oben angegebenen Gleichungssystem.\n",
        "\n",
        "* Die numerische L√∂sung des Systems zeigt bei bestimmten Parameterwerten deterministisch chaotisches Verhalten, die Trajektorien folgen einem seltsamen Attraktor. Damit spielt der Lorenzattraktor f√ºr die mathematische Chaostheorie eine Rolle, denn die Gleichungen stellen wohl eines der einfachsten Systeme mit chaotischem Verhalten dar.\n",
        "\n",
        "* Die typische Parametereinstellung mit chaotischer L√∂sung lautet: $a=10,b=28$ und $c=8/3$, wobei\n",
        "  * $a$ mit der [Prandtl-Zahl](https://de.wikipedia.org/wiki/Prandtl-Zahl) (=dimensionslose Kennzahl von Fluiden, das hei√üt von Gasen oder tropfbaren Fl√ºssigkeiten. Sie ist definiert als Verh√§ltnis zwischen kinematischer Viskosit√§t und Temperaturleitf√§higkeit. Die Prandtl-Zahl stellt die Verkn√ºpfung des Geschwindigkeitfeldes mit dem Temperaturfeld eines Fluids dar.)\n",
        "  * $b$ mit der [Rayleigh-Zahl](https://de.wikipedia.org/wiki/Rayleigh-Zahl) (=eine dimensionslose Kennzahl, die den Charakter der W√§rme√ºbertragung innerhalb eines Fluids beschreibt) identifiziert werden kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xyzZzNOEhv"
      },
      "source": [
        "###### *Summary: Attractor, Trajectory in phase space, Lyapunov exponent (forecast error)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIujOlBtNqx-"
      },
      "source": [
        "For this specific system shown, the vector field looks like this, and let scatter a bunch of random points around to represent different possible states see how they evolve and move around: They all spiral towards the center.\n",
        "\n",
        "For this exampe the attraction is zero, and the basin is every point in space. Notice that at the origin x dot and y dot also equal zero.\n",
        "\n",
        "$\\dot{x}=-(0)-0.1(0)=0$\n",
        "\n",
        "$\\dot{y}=(0)-0.4(0)=0$\n",
        "\n",
        "The origin is in this case **fixed point (attractor)**, because every point in there will stay forever. It seem like an inevitability, hence determinism (as chaos is).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkfT8XP8QmJd"
      },
      "source": [
        "But there are also other types of attractors, like the **Van der Pol oscillator**:\n",
        "\n",
        "> $\\dot{x}=\\mu\\left(x-\\frac{1}{3} x^{3}-y\\right)$\n",
        "\n",
        "> $\\dot{y}=\\frac{1}{\\mu} x$\n",
        "\n",
        "It creates something called: \"**Limit Cycle Attractor**\":\n",
        "\n",
        "* it doesn't end up in a point\n",
        "\n",
        "* typically appear in physical systems with some sort of oscillation (electrical circuits, tectonic plates, etc).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luo2ipo2RaUH"
      },
      "source": [
        "**Strange Attractor** (has a fractal dimension)\n",
        "\n",
        "Lorenz Attractor: Lorenz system describes convection cycles in the atmoshphere. Very sensitive to changes in initial conditions already after a short time:\n",
        "\n",
        "> $\\dot{x}=\\sigma(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(\\rho-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\beta z$\n",
        "\n",
        "The Lorenz equations have a few parameters that can be tweaked to alter the behavior of a system. This is what is known as ‚Äòstrange attractor‚Äô:\n",
        "\n",
        "> $\\dot{x}=10(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(28-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\frac{8}{3} z$\n",
        "\n",
        "1. No point in the space is ever visited more than once by the same trajectory - it that happens, the trajectory would travel in a predictable loop.\n",
        "2. And no 2 trajectories will ever intersect. If that happened, they would merge into the same path, giving two different sets of initial conditions the same outcome.\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_03.png)\n",
        "\n",
        "A single trajectory will visit an infinite number of points in this limited space, and this limited space will have an infinite number of trajectories.\n",
        "\n",
        "* Trajectories are just curved, so they should be 1-dimensional..normally!\n",
        "* For the strange attractor: no matter how much you zoom in on this attractor, you can always find more and more trajectories everywhere.\n",
        "* That‚Äôs why this attractor is said to have a **non integer dimension** - it‚Äôs **made up of infinite long curves in a finite space, which are so detailed, that they start to partially fill up higher dimensions**. It‚Äôs not 1, 2 or 3 dimensional, its somewhere in between. (=detail at arbitrarily small scales)\n",
        "\n",
        "Conclusion: Lorenz attractor is a fractal space, and hence a strange attractor.\n",
        "\n",
        "Even is both points started at the same point, small initial differences can bring them to complete different paths:\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chros_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOAWTDsoV7-F"
      },
      "source": [
        "The difference in trajectories increases exponentially:\n",
        "\n",
        "**$\\lambda$ stands for Lyapunov exponent**\n",
        "\n",
        "$\\lambda$ > 0, trajectories will increase exponentially (= chaotic)\n",
        "\n",
        "$\\lambda$ = 0, distance will stay constant\n",
        "\n",
        "$\\lambda$ < 0, distance will converge to zero.\n",
        "\n",
        "*Unfortunately there is no way to find the Lyapunov exponent only by looking at the equations. It is measured by running the simulation, keeping track of mainy pairs of trajectories, and find the average rate of change in their distance. But it provides a simple metric to comunicate how chaotic a system is*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_06.png)\n",
        "\n",
        "**For the Lorenz attractor it is $\\lambda$ ‚âà 0.9**\n",
        "\n",
        "Used to find out duration in which the predictions are valid: \"Predictability Horizon\". We get this by re-arranging our previous equation $d_{t}=d_{0} e^{\\lambda t}$:\n",
        "\n",
        "> Predictability horizon $=\\frac{1}{\\lambda} \\ln \\frac{a}{d_{0}}$\n",
        "\n",
        "$d_{0}=$ initial error\n",
        "\n",
        "$a=$ maximum allowed error\n",
        "\n",
        "**For the Lorenz attractor, after 10 time steps, any error would have multiplied by 8,000 already.**\n",
        "\n",
        "Example what that means with this exponential divergence:\n",
        "\n",
        "We have a simulation that predicts where ocean currents flow, and you want to keep the error less than 1,000 km.\n",
        "* If you ran it twice, once an initial error of 1m and once the initial error was a million times smaller than one micrometer (0,000001m), how much longer  the simulation with the smaller error would stay below the margin of error.\n",
        "* The simulation would be valid 9 days instead of 3 days, but for an initial error that is a million times smaller !!\n",
        "\n",
        "Let‚Äôs write the expressions for the two predictability horizons, and put them in a fraction:\n",
        "\n",
        "> $\\frac{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{0.000001}\\right)}{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{1}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(\\frac{1}{\\lambda} \\ln 10^{9}\\right)}{\\left(\\frac{1}{\\lambda} \\ln 10^{3}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(9 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}{\\left(3 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}$\n",
        "\n",
        "> = $\\frac{9}{3}$.\n",
        "\n",
        "= It‚Äôs 3 times longer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Stochastic**"
      ],
      "metadata": {
        "id": "82Lb_gBqUzi8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1flpzoQFTVa"
      },
      "source": [
        "##### <font color=\"blue\">*Stochastic Analysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Analysis**\n",
        "\n",
        "* [Stochastic calculus](https://en.m.wikipedia.org/wiki/Stochastic_calculus) bzw. [Stochastische Analysis](https://de.m.wikipedia.org/wiki/Stochastische_Analysis) is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. This field is created and started by the Japanese mathematician Kiyoshi It√¥ during World War 2.\n",
        "\n",
        "* [Stochastische_Integration](https://de.m.wikipedia.org/wiki/Stochastische_Integration): Es sind stochastische Prozesse mit unendlicher Variation, insbesondere der Wiener-Prozess, als Integratoren zugelassen.\n",
        "\n",
        "* [Stochastischer Prozess](https://de.m.wikipedia.org/wiki/Stochastischer_Prozess): (auch Zufallsprozess) zeitlich geordnete, zuf√§llige Vorg√§nge. Theorie der stochastischen Prozesse ist Erweiterung der Wahrscheinlichkeitstheorie dar und bildet Grundlage f√ºr stochastische Analysis.\n",
        "\n",
        "* > Stochastic: Statistik + Probability Theory (incl stochastic / random processes)\n",
        "\n",
        "* Bei einem Sparguthaben entspr√§che dies dem **exponentiellen Wachstum** durch Zinseszins. Bei Aktien wird dieses Wachstumsgesetz hingegen **in der Realit√§t offenbar durch eine komplizierte Zufallsbewegung √ºberlagert**.\n",
        "\n",
        "* Bei zuf√§lligen St√∂rungen, die sich aus vielen kleinen Einzel√§nderungen zusammensetzen, **wird von einer Normalverteilung als einfachstem Modell ausgegangen**.\n",
        "\n",
        "* Au√üerdem zeigt sich, dass die Varianz der St√∂rungen proportional zum betrachteten Zeitraum $\\Delta t$ ist. Der Wiener-Prozess $W_t$ besitzt alle diese gew√ºnschten Eigenschaften, eignet sich also als ein Modell f√ºr die zeitliche Entwicklung der Zufallskomponente des Aktienkurses.\n"
      ],
      "metadata": {
        "id": "DC-KHFg7J17I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0iE2bRTLxY"
      },
      "source": [
        "**Stochastic Differential Equation ( ‚Üí  Black Scholes, Spontaneous Symmetry Breaking)**\n",
        "\n",
        "* Eine [stochastischen Differentialgleichung](https://de.wikipedia.org/wiki/Stochastische_Differentialgleichung) ist eine Verallgemeinerung des Begriffs der gew√∂hnlichen Differentialgleichung auf stochastische Prozesse.\n",
        "\n",
        "> Stochastische Differentialgleichungen werden in zahlreichen Anwendungen eingesetzt, **um zeitabh√§ngige Vorg√§nge zu modellieren**, die neben deterministischen Einfl√ºssen zus√§tzlich **stochastischen St√∂rfaktoren (Rauschen)** ausgesetzt sind.\n",
        "\n",
        "* Die formale Theorie der stochastischen Differentialgleichungen wurde erst in den 1940er Jahren durch den japanischen Mathematiker It≈ç Kiyoshi formuliert.\n",
        "\n",
        "* Gemeinsam mit der **stochastischen Integration** begr√ºndet die Theorie der **stochastischen Differentialgleichungen** die **stochastische Analysis**.\n",
        "\n",
        "* **Objective**: Genau wie bei deterministischen Funktionen m√∂chte man auch bei stochastischen Prozessen den Zusammenhang zwischen dem Wert der Funktion und ihrer momentanen √Ñnderung (ihrer Ableitung) in einer Gleichung formulieren.\n",
        "\n",
        "* **Challenge**: Was im einen Fall zu einer gew√∂hnlichen Differentialgleichung f√ºhrt, ist im anderen Fall problematisch, da viele stochastische Prozesse, wie beispielsweise **der Wiener-Prozess, nirgends differenzierbar sind**.\n",
        "\n",
        "* Loesung ist wie bei gewohnlichen Differentialgleichungen plus einen stochastischen Term\n",
        "\n",
        "*Beim Typus der stochastischen Differentialgleichungen treten in der Gleichung stochastische Prozesse auf. Eigentlich sind stochastische Differentialgleichungen keine Differentialgleichungen [im obigen Sinne](https://de.m.wikipedia.org/wiki/Differentialgleichung#Weitere_Typen), sondern lediglich gewisse Differentialrelationen, welche als Differentialgleichung interpretiert werden k√∂nnen.*\n",
        "\n",
        "*Beispiele fur stochastische Differentialgleichungen*\n",
        "\n",
        "* Die SDGL f√ºr die geometrische brownsche Bewegung lautet $\\mathrm{d} S_{t}=r S_{t} \\mathrm{~d} t+\\sigma S_{t} \\mathrm{~d} W_{t} .$ Sie wird beispielsweise im Black-Scholes-Modell zur Beschreibung von Aktienkursen verwendet.\n",
        "\n",
        "* Die SDGL f√ºr einen Ornstein-Uhlenbeck-Prozess ist $\\mathrm{d} X_{t}=\\theta\\left(\\mu-X_{t}\\right) \\mathrm{d} t+\\sigma \\mathrm{d} W_{t} .$ Sie wird unter anderem im Vasicek-Modell zur finanzmathematischen Modellierung von Zinss√§tzen √ºber den Momentanzins\n",
        "verwendet.\n",
        "\n",
        "* Die SDGL f√ºr den Wurzel-Diffusionsprozess nach William Feller lautet $\\mathrm{d} X_{t}=\\kappa\\left(\\theta-X_{t}\\right) \\mathrm{d} t+\\sigma \\sqrt{X_{t}} \\mathrm{~d} W_{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelei85Xp8rU"
      },
      "source": [
        "**Supersymmetric theory of stochastic dynamics & Spontaneous Symmetry breaking**\n",
        "\n",
        "* [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) **is an exact theory of stochastic (partial) differential equations (SDEs)**, the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Chaos_theory#Chaos_as_a_spontaneous_breakdown_of_topological_supersymmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpmflh50O1X"
      },
      "source": [
        "##### <font color=\"blue\">*Markov Process & Monte Carlo Simulation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_psDmmPW0P"
      },
      "source": [
        "A Markov chain is a **discrete-time stochastic process** that progresses from one state to another with certain probabilities that can be **represented by a graph and state transition matrix P** as indicated below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/markov.PNG\" alt=\"markov\">\n",
        "\n",
        "* Markov chains may be modeled by **finite state machines, and random walks**\n",
        "\n",
        "* A Stochastic (State Transition) Matrix describes a Markov chain X<sub>t</sub> over a [**finite state space (Probability space) S**](https://en.m.wikipedia.org/wiki/Probability_space) with cardinality S.\n",
        "\n",
        "  * Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties.\n",
        "\n",
        "  * Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* Let P be the transition matrix of Markov chain {X0, X1, ...}.\n",
        "\n",
        "  * **Reducibility**: a Markov chain is said to be irreducible if it is possible to get to any state from any state. In other words, a Markov chain is irreducible if there exists a chain of steps between any two states that has positive probability.\n",
        "\n",
        "  * **Periodicity**: a state in a Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. Thus, starting in state 'i', the chain can return to 'i' only at multiples of the period 'k', and k is the largest such integer. State 'i' is aperiodic if k = 1 and periodic if k > 1.\n",
        "\n",
        "  * **Transience and Recurrence**: A state 'i' is said to be transient if, given that we start in state 'i', there is a non-zero probability that we will never return to 'i'. State i is recurrent (or persistent) if it is not transient. A recurrent state is known as positive recurrent if it is expected to return within a finite number of steps and null recurrent otherwise. Transience and recurrence issues are central to the study of Markov chains and help describe the Markov chain's overall structure. The presence of many transient states may suggest that the Markov chain is absorbing, and a strong form of recurrence is necessary in an ergodic Markov chain.\n",
        "\n",
        "  * **Ergodicity**: a state 'i' is said to be ergodic if it is aperiodic and positive recurrent. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic. Ergodic Markov chains are, in some senses, the processes with the \"nicest\" behavior.\n",
        "\n",
        "  * **Absorbing State**: a state i is called absorbing if it is impossible to leave this state. Therefore, the state 'i' is absorbing if pii = 1 and pij = 0 for i ‚â† j. If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain. Absorbing states are crucial for the discussion of absorbing Markov chains. A common type of Markov chain with transient states is an absorbing one. An absorbing Markov chain is a Markov chain in which it is impossible to leave some states, and any state could (after some number of steps, with positive probability) reach such a state. It follows that all non-absorbing states in an absorbing Markov chain are transient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcP1_FkDDDOv"
      },
      "source": [
        "**Monte Carlo Simulation**\n",
        "\n",
        "* Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response.\n",
        "\n",
        "* Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on **repeated random sampling to obtain numerical results**. The underlying concept is to use randomness to solve problems that might be deterministic in principle.\n",
        "\n",
        "* Monte Carlo methods are useful for **simulating systems with many coupled degrees of freedom**, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean‚ÄìVlasov processes, kinetic models of gases). Other examples include **modeling phenomena with significant uncertainty in inputs** such as the calculation of risk in business\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6jdLMMCTSD"
      },
      "source": [
        "**Markov Chain Monte Carlo (MCMC)**\n",
        "\n",
        "* Markov Chain Monte Carlo refers to a **class of methods** for sampling from a probability distribution in order to **construct the most likely distribution**.\n",
        "\n",
        "> MCMC can be considered as a **random walk** that gradually converges to the true distribution.\n",
        "\n",
        "* We cannot directly calculate the (i.e. logistic) distribution, so instead we generate thousands of values ‚Äî called samples ‚Äî for the parameters of the function (alpha and beta) to **create an approximation of the distribution**.\n",
        "\n",
        "* The idea behind MCMC is that **as we generate more samples, our approximation gets closer and closer to the actual true distribution**.\n",
        "\n",
        "* Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. **Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chCbs1o_Iud"
      },
      "source": [
        "##### <font color=\"blue\">*White Noise*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0ShhpknOvG"
      },
      "source": [
        "\n",
        "**A white noise process has following conditions**\n",
        "\n",
        "* Mean (level) is zero (does not change over time - stationary process)\n",
        "* Variance is constant (does not change over time - stationary process)\n",
        "* Zero autocorrelation (values do not correlate with lag values)\n",
        "\n",
        "**White Noise: Independent & Identically Distributed**\n",
        "\n",
        "* Hence, in a time series is white noise if the variables are independent and identically distributed (IID) with a mean of zero.\n",
        "\n",
        "* The term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies\n",
        "\n",
        "* **Necessary Condition**: Independence: variables are statistically uncorrelated = their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element R·µ¢·µ¢ is the variance of component w·µ¢; and the correlation matrix must be the n by n identity matrix.\n",
        "\n",
        "* **Sufficient Condition**: every variable in w has a normal distribution with zero mean and the same variance, w is said to be a Gaussian white noise vector. In that case, the joint distribution of w is a multivariate normal distribution; the independence between the variables then implies that the distribution has spherical symmetry in n-dimensional space. Therefore, any orthogonal transformation of the vector will result in a Gaussian white random vector. In particular, under most types of discrete Fourier transform, such as FFT and Hartley, the transform W of w will be a Gaussian white noise vector, too; that is, the n Fourier coefficients of w will be independent Gaussian variables with zero mean and the same variance.\n",
        "\n",
        "* A random vector (that is, a partially indeterminate process that produces vectors of real numbers) is said to be a white noise vector or white random vector if its components each have a probability distribution with zero mean and finite variance, and are statistically independent: that is, their joint probability distribution must be the product of the distributions of the individual components.\n",
        "\n",
        "* First moment has to be zero and second moment has to be finite though. (iid ) White noise is always an independent process but reverse may not be true.\n",
        "\n",
        "* The technical definition of white noise is that it has equal intensity at all frequencies. This corresponds to a delta function autocorrelation. This is only possible if there is no correlation between any sequential values. So yes, the independence is true both backwards and forwards. Note that the actual distribution is irrelevant.\n",
        "\n",
        "\n",
        "\n",
        "**Types of White Noise Processes**\n",
        "* If the variables in the series are drawn from a Gaussian distribution, the series is called Gaussian white noise\n",
        "* There are also white noise processes, like Levy etc.\n",
        "\n",
        "**Relationship to Stochastic Processes**\n",
        "* White noise is the generalized mean-square derivative of the Wiener process or Brownian motion (so Wiener is an integrated White Noise)\n",
        "\n",
        "**White noise is an important concept in time series analysis and forecasting**\n",
        "\n",
        "* **Predictability**: If your time series is white noise, then, by definition, it is random. You cannot reasonably model it and make predictions.\n",
        "* **Model Diagnostics**: The statistics and diagnostic plots can be uses on time series to check if it is white noise. The series of errors from a time series forecast model should ideally be white noise. If the series of forecast errors are not white noise, it suggests improvements could be made to the predictive model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8idRoUCpsoKR"
      },
      "source": [
        "##### <font color=\"blue\">*Random Walk*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdW8o7tCtbPh"
      },
      "source": [
        "**Random Walk**\n",
        "\n",
        "* Random walk is another time series model where the current observation is equal to the previous observation with a random step up or down. Known as a stochastic or random process.\n",
        "\n",
        "> y<sub>(t)</sub> = B<sub>0</sub> + B<sub>1</sub> * X<sub>(t-1)</sub> + e<sub>(t)</sub>\n",
        "\n",
        "* A random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence.\n",
        "\n",
        "* The process used to generate the series forces dependence from one-time step to the next. This dependence provides some consistency from step-to-step rather than the large jumps that a series of independent, random numbers provides. It is this dependency that gives the process its name as a ‚Äúrandom walk‚Äù or a ‚Äúdrunkard‚Äôs walk‚Äù.\n",
        "\n",
        "> **A simple random walk is a martingale**\n",
        "\n",
        "* In higher dimensions, the set of randomly walked points has interesting geometric properties. In fact, one gets a discrete fractal, that is, a set which exhibits stochastic self-similarity on large scales.\n",
        "* Examples include the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler: all can be approximated by random walk models, even though they may not be truly random in reality\n",
        "* Random walks serve as a fundamental model for the recorded stochastic activity / stochastic processes.\n",
        "* As a more mathematical application, the value of œÄ can be approximated by the use of random walk in an agent-based modeling environment.\n",
        "\n",
        "* There are many types of time-dependent processes referred to as random walks - most often refers to a special category of Markov chains or Markov processes. A random walk on the integers (and the gambler's ruin problem) are examples of **Markov processes in discrete time**.\n",
        "\n",
        "* Specific cases or limits of random walks include the L√©vy flight and diffusion models such as Brownian motion. **A Wiener process (~ Brownian motion) is the integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments. A Wiener process is the scaling limit of random walk in dimension 1.\n",
        "\n",
        "* Random walks can take place on a variety of spaces: graphs, on the integers or real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* **Random Walk and Autocorrelation**\n",
        "\n",
        "  * We can calculate the correlation between each observation and the observations at previous time steps. Given the way that the random walk is constructed, we would expect a strong autocorrelation with the previous observation and a linear fall off from there with previous lag values.\n",
        "\n",
        "* **Stationarity**\n",
        "\n",
        "  * A stationary time series is one where the values are not a function of time. Given the way that the random walk is constructed and the results of reviewing the autocorrelation, we know that the observations in a random walk are dependent on time.\n",
        "  * The current observation is a random step from the previous observation. Therefore we can expect a random walk to be non-stationary. In fact, all random walk processes are non-stationary. Note that not all non-stationary time series are random walks.\n",
        "  * Additionally, a non-stationary time series does not have a consistent mean and/or variance over time. A review of the random walk line plot might suggest this to be the case. We can confirm this using a statistical significance test, specifically the Augmented Dickey-Fuller test.\n",
        "\n",
        "**IID**\n",
        "\n",
        "  * This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (‚Äúi.i.d.‚Äù).\n",
        "\n",
        "  * This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied. So, if you begin with a time series that wanders all over the map, but you find that its first difference looks like it is an i.i.d. sequence, then a random walk model is a potentially good candidate.\n",
        "\n",
        "* **Prediction**\n",
        "\n",
        "  * A random walk is unpredictable; it cannot reasonably be predicted. Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step. Simply because we know that the next time step will be a function of the prior time step.\n",
        "  * This is often called the naive forecast, or a persistence model. We can implement this in Python by first splitting the dataset into train and test sets, then using the persistence model to predict the outcome using a rolling forecast method. Once all predictions are collected for the test set, the mean squared error is calculated.\n",
        "\n",
        "* **Drift**\n",
        "\n",
        "  * A random walk model is said to have ‚Äúdrift‚Äù or ‚Äúno drift‚Äù according to whether the distribution of step sizes has a non-zero mean or a zero mean. At period n, the k-step-ahead forecast that the random walk model without drift gives for the variable Y is\n",
        "\n",
        "  > $\\hat{Y}_{n+k}=Y_{n}$\n",
        "\n",
        "  * In others words, it predicts that all future values will equal the last observed value. This doesn‚Äôt really mean you expect them to all be the same, but just that you think they are equally likely to be higher or lower, and you are staying on the fence as far as point predictions are concerned. If you extrapolate forecasts from the random walk model into the distant future, they will go off on a horizontal line, just like the forecasts of the mean model. So, qualitatively the long-term point forecasts of the random walk model look similar to those of the mean model, except that they are always ‚Äúre-anchored‚Äù on the last observed value rather than the mean.of the historical data.\n",
        "\n",
        "  * For the random-walk-with-drift model, the k-step-ahead forecast from period n is:\n",
        "\n",
        "  > $\\hat{\\mathrm{Y}}_{\\mathrm{n}+\\mathrm{k}}=\\mathrm{Y}_{\\mathrm{n}}+\\mathrm{k} \\hat{\\mathrm{d}}$\n",
        "\n",
        "  * where dÀÜ is the estimated drift, i.e., the average increase from one period to the next. So, the long-term forecasts from the random-walk-with-drift model look like a trend line with slope dÀÜ , but it is always re-anchored on the last observed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk3roPmAuCDi"
      },
      "source": [
        "**Applications of Random Walks**\n",
        "\n",
        "* In computer networks, random walks can model the number of transmission packets buffered at a server.\n",
        "* In population genetics, random walk describes the statistical properties of genetic drift.\n",
        "* In image segmentation, random walks are used to determine the labels (i.e., ‚Äúobject‚Äù or ‚Äúbackground‚Äù) to associate with each pixel.\n",
        "* In brain research, random walks and reinforced random walks are used to model cascades of neuron firing in the brain.\n",
        "Random walks have also been used to sample massive online graphs such as online social networks.\n",
        "\n",
        "* **Random Walks in Financial Time Series**\n",
        "\n",
        "  * **Is time series a random walk?**: Your time series may be a random walk. Some ways to check if your time series is a random walk are as follows:\n",
        "The time series shows a strong temporal dependence that decays linearly or in a similar pattern.\n",
        "  * The time series is non-stationary and making it stationary shows no obviously learnable structure in the data.\n",
        "The persistence model provides the best source of reliable predictions.\n",
        "  * This last point is key for time series forecasting. Baseline forecasts with the persistence model quickly flesh out whether you can do significantly better. If you can‚Äôt, you‚Äôre probably working with a random walk. Many time series are random walks, particularly those of security prices over time. The random walk hypothesis is a theory that stock market prices are a random walk and cannot be predicted.\n",
        "  * \"A random walk is one in which future steps or directions cannot be predicted on the basis of past history. When the term is applied to the stock market, it means that short-run changes in stock prices are unpredictable.\" - Page 26, A Random Walk down Wall Street: The Time-tested Strategy for Successful Investing. https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/\n",
        "\n",
        "* https://towardsdatascience.com/random-walks-with-python-8420981bc4bc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbYRNP_zXe4"
      },
      "source": [
        "##### <font color=\"blue\">*Wiener Process (Brownian Motion)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldqy2UtMpa8P"
      },
      "source": [
        "**Wiener Process (Brownian Motion)**\n",
        "\n",
        "* Video: Physics of Randomness: https://youtu.be/5jBVYvHeG2c\n",
        "\n",
        "* Brownian Motion: Flower particles move randomly because they are hit by particles that move. And particles move faster with more heat.\n",
        "\n",
        "* Same in astrophysics with stars under the gravitational influence of smaller stars, big stars move a bit randomly\n",
        "\n",
        "* Same with stock market where the price is influenced by many factors all the time (and hence the price moves randomly up and down)\n",
        "\n",
        "**The Wiener process is a real valued continuous-time (continuous state-space) stochastic process**\n",
        "\n",
        "* W<sub>0</sub> = 0 (P-almost certain)\n",
        "* The Wiener process has (stochastically) independent increments.\n",
        "* The increases are therefore stationary and normally distributed with the expected value zero and the variance t - s.\n",
        "* The individual paths are (P-) almost certainly continuous.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* In physics it is used to study Brownian motion, the diffusion of minute particles suspended in fluid, and other types of diffusion via the Fokker‚ÄìPlanck and Langevin equations.\n",
        "* It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman‚ÄìKac formula, a solution to the Schr√∂dinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology.\n",
        "* It is also prominent in the mathematical theory of finance, in particular the Black‚ÄìScholes option pricing model.\n",
        "\n",
        "**Properties of a Wiener Process**\n",
        "\n",
        "1. The Wiener process belongs to the family of **Markov processes** and there specifically to the class of **Levy processes**. It also fulfills the strong markov property. It is one of the best known L√©vy processes (**c√†dl√†g** stochastic processes with stationary independent increments).\n",
        "2. The Wiener Process is a **special Gaussian process** with an expected value function E(W<sub>t</sub>)  = 0 and and the covariance function Cov (W<sub>s</sub>, W<sub>t</sub>) = min (s,t)\n",
        "3. The Wiener process is a (continuous time) **martingale** (L√©vy characterisation: the Wiener process is an almost surely continuous martingale with W0 = 0 and quadratic variation [Wt, Wt] = t, which means that Wt2 ‚àí t is also a martingale).\n",
        "4. The Wiener process is a **Levy process** with steady paths and constant expectation 0.\n",
        "\n",
        "*Another characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent N(0, 1) random variables. This representation can be obtained using the Karhunen‚ÄìLo√®ve theorem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9PQvsR1bKfq"
      },
      "source": [
        "**Wiener process as a limit of random walk (& Differences between Wiener Process & Random Walk)**\n",
        "\n",
        "* https://www.quora.com/What-is-an-intuitive-explanation-of-a-Wiener-process?top_ans=3955819\n",
        "\n",
        "* A Wiener process (~ Brownian motion) is the **integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments.\n",
        "\n",
        "* Let X<sub>1</sub>, X<sub>2</sub>, X<sub>n</sub> be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1.  The central limit theorem asserts that W<sup>(n)</sup> (1) converges in distribution to a standard Gaussian random variable W(1) as n ‚Üí ‚àû.\n",
        "\n",
        "* [Donsker's theorem](https://en.m.wikipedia.org/wiki/Donsker%27s_theorem) asserts that as n ‚Üí ‚àû , W<sub>n</sub> approaches a Wiener process, which explains the ubiquity of Brownian motion. **Donsker's invariance principle** states that: As random variables taking values in the Skorokhod space D [0,1], the random function W<sup>(n)</sup> converges in distribution to a standard Brownian motion W := (W(t))<sub>t ‚àà [0,1]</sub> as n ‚Üí ‚àû.\n",
        "\n",
        "![Donsker's Invariance Principle](https://upload.wikimedia.org/wikipedia/commons/8/8c/Donskers_invariance_principle.gif)\n",
        "\n",
        "*Donsker's invariance principle for simple random walk on Z*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/wiener.jpg\" alt=\"wiener\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKFiwhd7X_5I"
      },
      "source": [
        "**Differences to other stochastic processes**\n",
        "\n",
        "**Random Walk**\n",
        "\n",
        "\n",
        "* **A Wiener process is the [scaling limit](https://en.m.wikipedia.org/wiki/Scaling_limit) of random walk in dimension 1**. The **convergence of a random walk toward the Wiener process is controlled by the central limit theorem**, and by **Donsker's theorem**. The **Green's function** of the diffusion equation that controls the Wiener process, suggests that, **after a large number of steps, the random walk converges toward a Wiener process**.\n",
        "\n",
        "* A random walk is a discrete fractal (a function with integer dimensions; 1, 2, ...), but a **Wiener process trajectory is a true fractal**, and there is a connection between the two (a Wiener process walk is a fractal of **Hausdorff dimension** 2).\n",
        "\n",
        "* Unlike the random walk, a **Wiener Process is scale invariant**. A Wiener process enjoys many symmetries random walk does not. For example, a **Wiener process walk is invariant to rotations, but the random walk is not**, since the underlying grid is not. This means that in many cases, problems on a random walk are easier to solve by translating them to a Wiener process, solving the problem there, and then translating back.\n",
        "\n",
        "**(Gaussian) White Noise**\n",
        "\n",
        "* The Wiener process is used to represent the integral (from time zero to time t) of a zero mean, unit variance, delta correlated **<u>Gaussian</u> white noise process**\n",
        "\n",
        "**Brownian Motion**\n",
        "\n",
        "* **\"Brownian motion\" is a phenomenon that can be modeled with a Wiener Process**, because a Wiener process is a stochastic process with similar behavior to Brownian motion, the physical phenomenon of a minute particle diffusing in a fluid.\n",
        "\n",
        "* The Brownian motion process (and the Poisson process in one dimension) are both examples of **Markov processes in continuous time**\n",
        "\n",
        "* It≈ç also paved the way for the Wiener process from physics to other sciences: the **stochastic differential equations** he set up made it possible to adapt the Brownian motion to more statistical problems.\n",
        "\n",
        "* The **geometric Brownian motion** derived from a stochastic differential equation solves the problem that the **Wiener process, regardless of its starting value, almost certainly reaches negative values over time, which is impossible for stocks**. Since the development of the famous **Black-Scholes model**, the geometric Brownian movement has been the standard.\n",
        "\n",
        "**Ornstein-Uhlenbeck-Process**\n",
        "\n",
        "* The problem raised by the **[non-rectifiable paths](https://en.m.wikipedia.org/wiki/Arc_length)** of the Wiener process in the modeling of Brownian paths leads to the Ornstein-Uhlenbeck process and also makes the need for a theory of stochastic integration and differentiation clear\n",
        "* here it is not the motion but the speed of the particle as one that is not rectifiable process derived from the Wiener process, from which one obtains rectifiable particle paths through integration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0K1gPOoznzE"
      },
      "source": [
        "##### <font color=\"blue\">*Gaussian Process*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoS5J618HxN"
      },
      "source": [
        "* A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. **every finite linear combination of them is normally distributed**.\n",
        "\n",
        "* The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
        "\n",
        "* Gaussian Processes are a class of stationary, zero-mean stochastic processes which are completely dependent on their autocovariance functions. This class of models can be used for both regression and classification tasks.\n",
        "\n",
        "* Gaussian Processes provide estimates about uncertainty, for example giving an estimate of how sure an algorithm is that an item belongs to a class or not.\n",
        "\n",
        "* In order to deal with situations which embed a certain degree of uncertainty is typically made use of probability distributions.\n",
        "\n",
        "* Gaussian processes can allow us to describe probability distributions of which we can later update the distribution using Bayes Rule once we gather new training data.\n",
        "\n",
        "* **Relation to other Stochastic Processes**\n",
        "\n",
        "  * A **Wiener process (~ Brownian motion)** is the integral of a white noise generalized Gaussian process. It is not stationary, but it has stationary increments.\n",
        "\n",
        "  * The **fractional Brownian motion** is a Gaussian process whose covariance function is a generalisation of that of the Wiener process.\n",
        "\n",
        "  * The **Ornstein‚ÄìUhlenbeck** process is a stationary Gaussian process.\n",
        "\n",
        "  * The **Brownian bridge** is (like the Ornstein‚ÄìUhlenbeck process) an example of a Gaussian process whose increments are not independent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOw3me6qbku"
      },
      "source": [
        "##### <font color=\"blue\">*Further Stochastic Processes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEr4Ub47xgN1"
      },
      "source": [
        "**Brownian Bridge**\n",
        "\n",
        "* A Brownian bridge is a **continuous Gaussian process** with X<sub>0</sub> = X<sub>1</sub> = 0, and with mean and covariance functions given in (c) and (d), respectively.\n",
        "\n",
        "* The **Brownian bridge is (like the Ornstein‚ÄìUhlenbeck process)** an example of a Gaussian process **whose increments are not independent**.\n",
        "\n",
        "* There are several ways of constructing a Brownian bridge from a standard Brownian motion [Source](https://www.randomservices.org/random/brown/Bridge.html)\n",
        "\n",
        "* **Applications: Path Simulation for Stock Shares**: The simple Monte Carlo method with Euler method supplemented by the Brownian bridge correction for the possibility of falling below or exceeding the barriers between discretization times.\n",
        "\n",
        "  * By merely discreetly viewing (simulating) the (log) share price, those paths can also lead to a positive final payment in which the share price between the selected times k delta t has exceeded the lower barrier or exceeded the upper barrier without this is noticed in the discretized model.\n",
        "\n",
        "  * To calculate the probability of such an unnoticed barrier violation, Brownian Bridge is used (with the help of the independence and stationarity of its growth).\n",
        "\n",
        "  * With the help of the statements about the Brown Bridge, one can formally. Specify the Monte Carlo algorithm that can be used to evaluate double barrier options without having to discretize the price path.\n",
        "\n",
        "* **Application: Bond Prices**\n",
        "\n",
        "  * Computation of bond prices in a structural default model with jumps with an unbiased Monte-Carlo simulation.\n",
        "\n",
        "  * The algorithm requires the evaluation of integrals with the density of the first-passage time of a Brownian bridge as the integrand. (Metwally and Atiya (2002) suggest an approximation of these integrals.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0CRhOchiHXb"
      },
      "source": [
        "**Ornstein-Uhlenbeck Process**\n",
        "\n",
        "> Simulating a stochastic differential equation\n",
        "\n",
        "* the Ornstein‚ÄìUhlenbeck process is a **stochastic process** with applications in financial mathematics and the physical sciences.\n",
        "\n",
        "* Its original application in physics was as a model for the **<u>velocity</u> of a massive Brownian particle under the influence of <u>friction</u>**. It is named after Leonard Ornstein and George Eugene Uhlenbeck.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is a **stationary Gauss‚ÄìMarkov process**, which means that it is a **Gaussian process, a Markov process, and is temporally homogeneous**. In fact, it is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables.\n",
        "\n",
        "* Over time, the process tends to **drift towards its mean function**: such a process is called **mean-reverting**.\n",
        "\n",
        "* The process can be considered to be a **modification of the random walk in continuous time, or Wiener process**, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the center.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can also be considered as the **continuous-time analogue of the discrete-time AR(1) process**.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process can be interpreted as a **scaling limit of a discrete process**, in the same way that Brownian motion is a scaling limit of random walks.\n",
        "\n",
        "* Generalization: It is possible to extend Ornstein‚ÄìUhlenbeck processes to processes where the background driving process is a L√©vy process (instead of a simple Brownian motion).\n",
        "\n",
        "* In addition, in finance, stochastic processes are used where the volatility increases for larger values of C.\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process (just like the Brownian Bridge) a is an example of a **Gaussian process whose increments are not independent**. Look for stock returns devoid of explanatory factors, and analyze the corresponding residuals as stochastic processes. (e.g. mean reverting?). Can residuals be fitted to (increments of) OU processes or other MR processes? If so, what is the typical correlation time-scale? Mean reversion days: how long does it take to converge (e.g. model distribution of days).\n",
        "\n",
        "**In Financial Mathematics**\n",
        "\n",
        "* The Ornstein‚ÄìUhlenbeck process is one of several approaches used to model (with modifications) interest rates, currency exchange rates, and commodity prices stochastically.\n",
        "\n",
        "* The parameter Œº (mu) represents the equilibrium or mean value supported by fundamentals; œÉ (signa) the degree of volatility around it caused by shocks, and Œ∏ (theta) the rate by which these shocks dissipate and the variable reverts towards the mean.\n",
        "\n",
        "* One application of the process is a trading strategy known as pairs trade.\n",
        "\n",
        "* Stationary and mean-reverting around mean=10 (red dotted line)\n",
        "* **In financial engineering: how long does it take in average to converge? mean-reversion is an investment opportunity!**\n",
        "* Now, we are going to take a look at the time evolution of the distribution of the process. To do this, we will simulate many independent realizations of the same process in a vectorized way. We define a vector X that will contain all realizations of the process at a given time (that is, we do not keep all realizations at all times in memory). This vector will be overwritten at every time step. We will show the estimated distribution (histograms) at several points in time:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqrw3_Nqev5"
      },
      "source": [
        "**Jump diffusion**\n",
        "\n",
        "* [Jump diffusion](https://en.m.wikipedia.org/wiki/Jump_diffusion) is a stochastic process that involves jumps and diffusion.\n",
        "\n",
        "* It has important applications in magnetic reconnection, coronal mass ejections, condensed matter physics, option pricing, and pattern theory and computational vision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQP6Jd68WsL"
      },
      "source": [
        "**L√©vy Process**\n",
        "\n",
        "* A L√©vy process is a stochastic process with **[independent, stationary increments](https://de.m.wikipedia.org/wiki/Prozess_mit_unabh√§ngigen_Zuw√§chsen)** (= the course of the future of the process is independent of the past): it represents the motion of a point whose successive displacements are random and independent, and statistically identical over different time intervals of the same length.\n",
        "\n",
        "* A L√©vy process may thus be viewed as the **continuous-time analog of a random walk**.\n",
        "\n",
        "* The most well known **examples of L√©vy processes are Wiener process (~ Brownian motion), and Poisson process**. Aside from Brownian motion with drift, all other proper (that is, not deterministic) L√©vy processes have discontinuous paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAvA4JovM_-"
      },
      "source": [
        "**Bernoulli Process**\n",
        "\n",
        "* The outcomes of a Bernoulli process will follow a [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution).\n",
        "\n",
        "* https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/\n",
        "\n",
        "* A Bernoulli process is a finite or infinite sequence of binary random variables, so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1.\n",
        "\n",
        "* The component Bernoulli variables X<sub>i</sub> are [identically distributed and independent](https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). Prosaically, a Bernoulli process is a repeated coin flipping, possibly with an unfair coin (but with consistent unfairness).\n",
        "\n",
        "* Every variable X<sub>i</sub> in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided dice); this generalization is known as the **Bernoulli scheme**.\n",
        "\n",
        "* The problem of determining the process, given only a limited sample of Bernoulli trials, may be called the problem of checking whether a coin is fair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPTaHcB54U"
      },
      "source": [
        "**Poisson (Point) Process**\n",
        "\n",
        "* Poisson point process is a type of random mathematical object that consists of points randomly located on a mathematical space\n",
        "\n",
        "* Its name derives from the fact that if a collection of random points in some space forms a Poisson process, then the number of points in a region of finite size is a random variable with a Poisson distribution.\n",
        "\n",
        "* The process was discovered independently and repeatedly in several settings, including experiments on radioactive decay, telephone call arrivals and insurance mathematics. The Poisson point process is often defined on the real line, where it can be considered as a stochastic process.\n",
        "\n",
        "* On the real line, the Poisson process is a type of **continuous-time Markov process** known as a birth-death process (with just births and zero deaths) and is called a pure or simple birth process.\n",
        "\n",
        "* In this setting, it is used, for example, in queueing theory to model random events, such as the arrival of customers at a store, phone calls at an exchange or occurrence of earthquakes, distributed in time. In the plane, the point process, also known as a spatial Poisson process, can represent the locations of scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest.\n",
        "\n",
        "* In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process.\n",
        "\n",
        "* Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena where there is sufficiently strong interaction between the points. This has inspired the proposal of other point processes, some of which are constructed with the Poisson point process, that seek to capture such interaction.\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-process-everything-you-need-to-know-322aa0ab9e9a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Measure & Probability Theory*"
      ],
      "metadata": {
        "id": "ECcHaTG2nKiM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4KDiI20GSS8"
      },
      "source": [
        "###### *Measure (Ma√ü)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FMu2VsVHXMJ"
      },
      "source": [
        "* Ein Ma√ü ist in der Mathematik **eine Funktion**, die geeigneten Teilmengen einer Grundmenge Zahlen zuordnet, die als ‚ÄûMa√ü‚Äú f√ºr die Gr√∂√üe dieser Mengen interpretiert werden k√∂nnen.\n",
        "\n",
        "* Es sei $\\mathcal{A}$ eine o-Algebra √ºber einer nicht-leeren Grundmenge $\\Omega .$ Eine Funktion $\\mu: \\mathcal{A} \\rightarrow[0, \\infty]$ hei√üt Ma√ü auf $\\mathcal{A}$, wenn die beiden folgenden Bedingungen erf√ºllt sind:\n",
        "\n",
        "> $\\mu(\\emptyset)=0$\n",
        "\n",
        "> Additivit√§t: F√ºr jede Folge $\\left(A_{n}\\right)_{n \\in \\mathbb{N}}$ paarweise disjunkter Mengen aus $\\mathcal{A}$ gilt $\\mu\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right)$\n",
        "\n",
        "* Ist die o-Algebra aus dem Zusammenhang klar, so spricht man auch von einem Ma√ü auf $\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tOKtjbyH7Ai"
      },
      "source": [
        "* Eine Teilmenge von $\\Omega$, die in $\\mathcal{A}$ liegt, wird messbar genannt. F√ºr solch ein $A \\in \\mathcal{A}$ hei√üt $\\mu(A)$ das Ma√ü der Menge $A$.\n",
        "\n",
        "* Das **Tripel $(\\Omega, \\mathcal{A}, \\mu)$ wird Ma√üraum** genannt.\n",
        "\n",
        "* Das Paar $(\\Omega, \\mathcal{A})$ bestehend aus der Grundmenge und der darauf definierten $\\sigma$-Algebra **hei√üt Messraum oder auch messbarer Raum**.\n",
        "\n",
        "* **Ein Ma√ü $\\mu$ ist also eine auf einem Messraum definierte nicht-negative [$\\sigma$ -additive](https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t) [Mengenfunktion](https://de.m.wikipedia.org/wiki/Mengenfunktion) mit $\\mu(\\emptyset)=0$.**\n",
        "\n",
        "* Das Ma√ü $\\mu$ hei√üt Wahrscheinlichkeitsma√ü (oder normiertes Ma√ü), wenn zus√§tzlich $\\mu(\\Omega)=1$ gilt. Ein Ma√üraum $(\\Omega, \\mathcal{A}, \\mu)$ mit einem Wahrscheinlichkeitsma√ü $\\mu$ ist ein Wahrscheinlichkeitsraum.\n",
        "\n",
        "* Ist allgemeiner $\\mu(\\Omega)<\\infty,$ so nennt man $\\mu$ ein [endliches Ma√ü](https://de.m.wikipedia.org/wiki/Endliches_Ma√ü). Existieren abz√§hlbar viele Mengen, deren Ma√üendlich ist und deren Vereinigung ganz $\\Omega$ ergibt, dann wird $\\mu$ ein [$\\sigma$ -endliches](https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit) (oder $\\sigma$ -finites) Ma√ü genannt. $^{[4]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGw0qfYwrCJ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Messbare_Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtT12NSGYu4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbegVOVJGqzz"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Œ£-Additivit√§t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1v7eQtS4gqu"
      },
      "source": [
        "**Z√§hlma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbVSW8Q6RdZ"
      },
      "source": [
        "$\\mu$ (A) :=\n",
        "* #A, wenn A endlich\n",
        "* ‚àû, sonst\n",
        "\n",
        "Rechenregeln in [0, ‚àû] :\n",
        "* x + ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* x * ‚àû := ‚àû f√ºr alle x ‚àà [0, ‚àû]\n",
        "* 0 * ‚àû := 0 (in der Ma√ütheorie sinnvoll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzLXsG6C4Lx"
      },
      "source": [
        "**Dirac Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpN2hkk7KTX"
      },
      "source": [
        "p ‚àà X (Punktma√ü, weil es einem Punkt ein Ma√ü zuordnet, und sonst Null).\n",
        "\n",
        "Ist p innerhalb eine Menge A als Teilmenge von X, dann ist Diracma√ü = 1, sonst 0.\n",
        "\n",
        "Œ¥<sub>p</sub> (A) :=\n",
        "* 1, wenn p ‚àà A\n",
        "* 0, wenn sonst.\n",
        "\n",
        "(Œ¥ = delta klein)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJL1d3ifDByw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diracma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkR6CvL_C0uu"
      },
      "source": [
        "**Lebesgue Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAnnK16Dbua"
      },
      "source": [
        "* Das Lebesgue-Ma√ü ist das **Ma√ü im euklidischen Raum**, das geometrischen Objekten ihren Inhalt (L√§nge, Fl√§cheninhalt, Volumen, ‚Ä¶) zuordnet.\n",
        "\n",
        "* Das Lebesgue-Borel-Ma√ü auf der Borel-$\\sigma$-Algebra $\\mathcal{B}\\left(\\mathbb{R}^{n}\\right)$ (auch als Borel-Lebesgue-Ma√ü oder nur Bore/-Ma√ü bezeichnet) ist das eindeutige Ma√ü $\\lambda$ mit der Eigenschaft, dass es $n$ -dimensionalen\n",
        "Hyperrechtecken ihr $n$ -dimensionales Volumen zuordnet:\n",
        "\n",
        ">$\\lambda\\left(\\left[a_{1}, b_{1}\\right] \\times \\cdots \\times\\left[a_{n}, b_{n}\\right]\\right)=\\left(b_{1}-a_{1}\\right) \\cdots \\cdots\\left(b_{n}-a_{n}\\right)$\n",
        "\n",
        "* Das hei√üt, **es ist das Ma√ü, das Intervallen inre L√§nge zuordnet (im Eindimensionalen), Rechtecken ihren Fl√§cheninhalt zuordnet (im Zweidimensionalen), Quadern ihr Volumen zuordnet (im Dreidimensionalen) usw.**\n",
        "\n",
        "* Durch diese Bedingung wird der Inhalt $\\lambda(B)$ beliebiger Borel-Mengen eindeutig festgelegt. Die BorelMengen werden auch Borel-messbar oder $B$ -messbar genannt. **Das Borel-Ma√ü ist bewegungsinvariant und normiert, aber nicht vollst√§ndig.**\n",
        "\n",
        "* Das Lebesgue-Ma√ü ist das Haar-Ma√ü **auf der lokalkompakten topologischen Gruppe** $\\mathbb {R} ^{n}$ mit der Addition, die Existenz folgt daher bereits aus der Existenz des Haarma√ües.\n",
        "\n",
        "* Insbesondere ist es translationsinvariant, das bedeutet, dass sich das Ma√ü einer Menge unter Translation nicht √§ndert. Zudem ist es invariant unter Spiegelungen und Drehungen, also sogar bewegungsinvariant. Das Lebesgue-Ma√ü ist œÉ-endlich und regul√§r."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XsGc9DFRb"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymG9QHcCF1Hg"
      },
      "source": [
        "**Haarsches Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLkzlmPEF8cn"
      },
      "source": [
        "* Das Haarsche Ma√ü wurde eingef√ºhrt, um Ergebnisse der **Ma√ütheorie in der Gruppentheorie anwendbar zu machen**.\n",
        "\n",
        "* Beispiel: Das Lebesgue-Ma√ü $B$ auf $\\mathbb{R}^{n}$ und $\\mathbb{C}^{n}$ ist das Haarsche Ma√ü auf den additiven Gruppen $\\left(\\mathbb{R}^{n},+\\right)$ bzw. $\\left(\\mathbb{C}^{n},+\\right)$.\n",
        "\n",
        "* Es ist eine Verallgemeinerung des Lebesgue-Ma√ües. Das Lebesgue-Ma√ü ist ein Ma√ü auf dem euklidischen Raum, das unter Translationen invariant ist.\n",
        "\n",
        "* Der euklidische Raum ist eine lokalkompakte topologische Gruppe bez√ºglich der Addition. Das Haarsche Ma√ü ist f√ºr jede lokalkompakte (im Folgenden immer als hausdorffsch vorauszusetzende) topologische Gruppe definierbar, insbesondere also f√ºr jede Lie-Gruppe.\n",
        "\n",
        "* Lokalkompakte Gruppen mit ihren Haarschen Ma√üen werden in der [harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) untersucht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzktY0hJF4LR"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Haarsches_Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw99f_qvDl6i"
      },
      "source": [
        "**Haussdorf Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2DTGGZGDv5H"
      },
      "source": [
        "* Das Hausdorff-Ma√ü ist eine Verallgemeinerung des Lebesgue-Ma√ües auf nicht notwendig ganzzahlige Dimensionen. Mit seiner Hilfe l√§sst sich die Hausdorff-Dimension definieren, ein Dimensionsbegriff, mit dem beispielsweise fraktale Mengen untersucht werden k√∂nnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9M76qlDrVX"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Hausdorff-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V27J3X8qM9cN"
      },
      "source": [
        "**Jordan Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6tM4coFNIxa"
      },
      "source": [
        "* Mit dem Jordan-Ma√ü kann man beschr√§nkten Teilmengen des $\\mathbb {R} ^{n}$ einen Inhalt zuordnen und erh√§lt einen Integralbegriff, der dem [riemannschen Integralbegriff](https://de.m.wikipedia.org/wiki/Riemannsches_Integral) analog ist.\n",
        "\n",
        "* Das Jordan-Ma√ü ist kein Ma√ü im Sinne der Ma√ütheorie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcW8Znq_NB8I"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Jordan-Ma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ef5-QUNE22"
      },
      "source": [
        "Siehe auch: https://de.m.wikipedia.org/wiki/Inhalt_(Ma√ütheorie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomYI_xzCurv"
      },
      "source": [
        "**Radon Ma√ü**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGB7cfQ9VeD"
      },
      "source": [
        "Es handelt sich um ein spezielles Ma√ü auf der Borelschen œÉ-Algebra eines Hausdorff-Raums mit bestimmten Regularit√§tseigenschaften. Der Begriff wird in der Fachliteratur jedoch nicht einheitlich verwendet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AnX69MC1wo"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Radonma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0PG2uLQIPM"
      },
      "source": [
        "**Measurable Space (Messbarer Raum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraYrSFNKL4d"
      },
      "source": [
        "Das Tupel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge (universe / Set) genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine $\\sigma$ -Algebra √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "Zusammen mit einem Ma√ü $\\mu$ wird aus einem messbaren Raum ein Ma√üraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrI52A0M-lGw"
      },
      "source": [
        "* **A measurable space or Borel space is a basic object in measure theory**. It consists of a set and a œÉ-algebra, which defines the subsets that will be measured.\n",
        "\n",
        "* Note that in contrast to a measure space, **no measure is needed for a measurable space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCr-8c-d62V"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZG-bj9KyHs"
      },
      "source": [
        "**Example**\n",
        "\n",
        "Look at the set\n",
        "\n",
        "X=\\{1,2,3\\}\n",
        "\n",
        "One possible $\\sigma$ -algebra would be\n",
        "\n",
        "$\\mathcal{A}_{1}=\\{X, \\emptyset\\}$\n",
        "\n",
        "Then $\\left(X, \\mathcal{A}_{1}\\right)$ is a measurable space.\n",
        "\n",
        "Another possible $\\sigma$ -algebra would be the power set on $X$ :\n",
        "\n",
        "$\\mathcal{A}_{2}=\\mathcal{P}(X)$\n",
        "\n",
        "With this, a second measurable space on the set $X$ is given by $\\left(X, \\mathcal{A}_{2}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kIK_3HLLTT"
      },
      "source": [
        "**Types**\n",
        "\n",
        "* If $X$ is finite or countable infinite, **the $\\sigma$ -algebra is most of the times the power set on $X$**, so $\\mathcal{A}=\\mathcal{P}(X) .$ This leads to the measurable space $(X, \\mathcal{P}(X))$\n",
        "\n",
        "* **If $X$ is a topological space, the $\\sigma$ -algebra is most commonly the Borel $\\sigma$ -algebra $\\mathcal{B}$**, so $\\mathcal{A}=\\mathcal{B}(X)$. This leads to the measurable space $(X, \\mathcal{B}(X))$ that is common for all topological spaces such as\n",
        "the real numbers $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHhdyPQ-hgE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measurable_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0juq9WKIedRw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clc0uHou3vFE"
      },
      "source": [
        "**Riemann vs Lebesgue Integral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpeX84I3z0Q"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Integralrechnung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8ajZ5T31sN"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ma√ütheorie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3G8b13_MBD3"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Satz_von_Fubini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGhitrlOQEHM"
      },
      "source": [
        "###### *Measure Space (Ma√üraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> One important example of a measure space is a probability space."
      ],
      "metadata": {
        "id": "pPIoNoM1u-dH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsroJGMFRuY"
      },
      "source": [
        "Ein Ma√üraum ist eine spezielle mathematische Struktur, die eine essentielle Rolle in der Ma√ütheorie und dem axiomatischen Aufbau der Stochastik spielt.\n",
        "\n",
        "Das Tripel $(\\Omega, \\mathcal{A}, \\mu)$ hei√üt Ma√üraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine [$\\sigma$ -Algebra](https://de.m.wikipedia.org/wiki/Œ£-Algebra) √ºber der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "* $\\mu$ ein [Ma√ü](https://de.m.wikipedia.org/wiki/Ma√ü_(Mathematik)) ist, das auf $\\mathcal{A}$ definiert ist.\n",
        "\n",
        "Alternativ kann man einen Ma√üraum auch als einen [Messraum](https://de.m.wikipedia.org/wiki/Messraum_(Mathematik)) $(\\Omega, \\mathcal{A})$ versehen mit einem Ma√ü $\\mu$ definieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqmDCJDWy4sq"
      },
      "source": [
        "A measure space is a triple $(X, \\mathcal{A}, \\mu),$ where\n",
        "\n",
        "* $X$ is a set\n",
        "\n",
        "* $\\mathcal{A}$ is a $\\sigma$ -algebra on the set $X$\n",
        "\n",
        "* $\\boldsymbol{\\mu}$ is a measure on $(X, \\mathcal{A})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34X5FmJ4d--B"
      },
      "source": [
        "**Messraum und Ma√üraum sind spezielle œÉ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDp68lYUypzs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measure_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIqej8jGuOt"
      },
      "source": [
        "**Beispiele**\n",
        "\n",
        "* Ein einfaches Beispiel f√ºr einen Ma√üraum sind die nat√ºrlichen Zahlen als Grundmenge $\\Omega=\\mathbb{N}$, als $\\sigma$ Algebra w√§hlt man die Potenzmenge $\\mathcal{A}=\\mathcal{P}(\\mathbb{N})$ und als Ma√ü das Diracma√ü auf der $1: \\mu=\\delta_{1}$\n",
        "\n",
        "* Ein bekannter Ma√üraum ist die Grundmenge $\\mathbb{R}$, versehen mit der borelschen $\\sigma$ -Algebra $\\mathcal{B}(\\mathbb{R})$ und dem Lebesgue-Ma√ü. **Dies ist der kanonische Ma√üraum in der Integrationstheorie.**\n",
        "\n",
        "* Die in der Wahrscheinlichkeitstheorie verwendeten **[Wahrscheinlichkeitsr√§ume](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum)** $(\\Omega, \\mathcal{A}, P)$ sind allesamt Ma√ür√§ume. Sie bestehen aus der Ergebnismenge $\\Omega$, der Ereignisalgebra $\\mathcal{A}$ und dem Wahrscheinlichkeitsma√ü $P$ (= synonym 'Wahrscheinlichkeitsverteilung' oder einfach 'Verteilung'). Mit den Eigenschaften: Die drei Forderungen Normiertheit, œÉ-Additivit√§t und Werte im Intervall zwischen 0 und 1 werden auch die Kolmogorow-Axiome genannt. ([Source](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTp6RhYL8Je"
      },
      "source": [
        "**Klassen von Ma√ür√§umen**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Œ£-Endlichkeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollst√§ndiges_Ma√ü\n",
        "\n",
        "und mehr.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKgmO9kQZPo"
      },
      "source": [
        "* Verzichtet man auf Abst√§nde und Winkel, beh√§lt jedoch das Volumen geometrischer K√∂rper bei, gelangt man in das Gebiet der Ma√ütheorie. (f√ºr ein wahrscheinlichkeitsma√ü volumen = 1)\n",
        "\n",
        "* Der Ma√ütheorie gelang es, den Begriff des Volumens (oder eines anderen Ma√ües) auf eine enorm gro√üe Klasse von Mengen auszudehnen, die sogenannten messbaren Mengen. In vielen F√§llen ist es jedoch unm√∂glich, allen Mengen ein Ma√ü zuzuordnen (siehe Ma√üproblem).\n",
        "Die messbaren Mengen bilden dabei eine œÉ-Algebra. Mit Hilfen von messbaren Mengen lassen sich messbare Funktionen zwischen Messr√§umen definieren.\n",
        "\n",
        "* Um einen topologischen Raum zu einem Messraum zu machen, muss man ihn mit einer œÉ-Algebra ausstatten. Die œÉ-Algebra der Borel-Mengen ist die verbreitetste, aber nicht die einzige Wahl.\n",
        "\n",
        "* Ein Ma√üraum ist ein Messraum, der mit einem Ma√ü versehen ist. Ein euklidischer Raum mit dem [Lebesgue-Ma√ü](https://de.m.wikipedia.org/wiki/Lebesgue-Ma√ü) ist beispielsweise ein Ma√üraum. In der Integrationstheorie werden Integrierbarkeit und Integrale messbarer Funktionen auf Ma√ür√§umen definiert. Mengen vom Ma√ü null werden Nullmengen genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0omcCEyljo"
      },
      "source": [
        "* A measure space is a basic object of measure theory, a branch of mathematics that studies generalized notions of volumes.\n",
        "\n",
        "* It contains an underlying set, the subsets of this set that are feasible for measuring (the œÉ-algebra) and the method that is used for measuring (the measure).\n",
        "\n",
        "* **One important example of a measure space is a probability space**.\n",
        "\n",
        "* A measurable space consists of the first two components without a specific measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZqzchtzaSz"
      },
      "source": [
        "A **complete measure** (or, more precisely, a complete measure space) is a measure space in which every subset of every null set is measurable (having measure zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtShSxHrzeGE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Complete_measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9JLE04AcXav"
      },
      "source": [
        "**Probability Space (Wahrscheinlichkeitsraum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2axZJitcXav"
      },
      "source": [
        "Gegeben sei\n",
        "\n",
        "* eine Menge $\\Omega$, der sogenannte **Ergebnisraum**,\n",
        "\n",
        "* eine o-Algebra $\\Sigma$ auf dieser Menge, das **Ereignissystem**.\n",
        "\n",
        "Dann hei√üt eine Abbildung (=Funktion)\n",
        "\n",
        ">$P: \\Sigma \\rightarrow[0,1]$\n",
        "\n",
        "mit den Eigenschaften\n",
        "\n",
        "*  Normiertheit: Es ist $P(\\Omega)=1$\n",
        "\n",
        "* $\\sigma$-Additivit√§t: F√ºr jede abz√§hlbare Folge von paarweise disjunkten Mengen $A_{1}, A_{2}, A_{3}, \\ldots$ aus $\\Sigma$ gilt $P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right)$\n",
        "\n",
        "ein Wahrscheinlichkeitsma√ü oder eine Wahrscheinlichkeitsverteilung.\n",
        "\n",
        "Die drei Forderungen Normiertheit, $\\sigma$-Additivit√§t und Werte im Intervall zwischen O und 1 werden auch die [Kolmogorow-Axiome](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitstheorie#Axiome_von_Kolmogorow) genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qf8jvmMcXaw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma√ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3sQ4P3KcXaw"
      },
      "source": [
        "* **One important example of a measure space is a probability space.**\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum, bei dem das Ma√ü des ganzen Raums gleich 1 ist.\n",
        "\n",
        "* In der Wahrscheinlichkeitstheorie werden f√ºr die verwendeten ma√ütheoretischen Begriffe meist eigene Bezeichnungen verwendet, die der Beschreibung von Zufallsexperimenten angepasst sind: Messbare Mengen werden Ereignisse und messbare Funktionen zwischen Wahrscheinlichkeitsr√§umen werden Zufallsvariable genannt; ihre Integrale sind Erwartungswerte.\n",
        "\n",
        "* Das Produkt einer endlichen oder unendlichen Familie von Wahrscheinlichkeitsr√§umen ist wieder ein Wahrscheinlichkeitsraum. Im Gegensatz dazu ist f√ºr allgemeine Ma√ür√§ume nur das Produkt endlich vieler R√§ume definiert. Dementsprechend gibt es zahlreiche unendlichdimensionale Wahrscheinlichkeitsma√üe, beispielsweise die Normalverteilung, aber kein unendlichdimensionales Lebesgue-Ma√ü.\n",
        "Diese R√§ume sind weniger geometrisch. Insbesondere l√§sst sich die Idee der Dimension, wie sie in der einen oder anderen Form auf alle anderen R√§ume anwendbar ist, nicht auf Messr√§ume, Ma√ür√§ume und Wahrscheinlichkeitsr√§ume anwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFqotKf4cXaw"
      },
      "source": [
        "* Es handelt sich um ein mathematisches Modell zur Beschreibung von Zufallsexperimenten. Hierbei werden die verschiedenen m√∂glichen Ausg√§nge des Experiments zu einer Menge zusammengefasst. Teilmengen dieser Ergebnismenge k√∂nnen dann unter bestimmten Voraussetzungen Zahlen zwischen 0 und 1 zugeordnet werden, die als Wahrscheinlichkeiten interpretiert werden.\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Ma√üraum (Œ©, Œ£, P) dessen Ma√ü P ein Wahrscheinlichkeitsma√ü ist. Im Einzelnen bedeutet das:\n",
        "\n",
        "* Œ© ist eine beliebige nichtleere Menge, genannt die Ergebnismenge. Ihre Elemente hei√üen Ergebnisse.\n",
        "\n",
        "* Œ£ (Sigma) ist eine œÉ-Algebra √ºber der Grundmenge Œ© (Omega), also eine Menge bestehend aus Teilmengen von Œ©, die Œ© enth√§lt und abgeschlossen gegen√ºber der Bildung von Komplementen und abz√§hlbaren Vereinigungen ist. Die Elemente von Œ£ hei√üen Ereignisse. Die œÉ-Algebra Œ£ selbst wird auch Ereignissystem oder Ereignisalgebra genannt.\n",
        "\n",
        "* P : Œ£ ‚Äì> [0,1] ist ein Wahrscheinlichkeitsma√ü, das hei√üt eine Mengenfunktion, die den Ereignissen Zahlen zuordnet, derart dass P(‚àÖ) = 0 ist, P (A1 ‚à™ A2 ‚à™ ‚Ä¶ ) = P(A1) + P(A2) + ‚Ä¶ f√ºr paarweise disjunkte (d. h. sich gegenseitig ausschlie√üende) Ereignisse A1, A2, ‚Ä¶ gilt (3. Kolmogorow-Axiom) und P(Œ©) = 1 ist (2. Kolmogorow-Axiom).\n",
        "\n",
        "* Der Messraum (Œ©, Œ£) wird auch Ereignisraum genannt. Ein Wahrscheinlichkeitsraum ist also ein Ereignisraum, auf dem zus√§tzlich ein Wahrscheinlichkeitsma√ü gegeben ist.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4A4TeH7SewU"
      },
      "source": [
        "###### *Measurable Space (Messraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXZ_kJWGTey"
      },
      "source": [
        "* [Messraum](https://en.wikipedia.org/wiki/Measurable_space) und Ma√üraum sind spezielle œÉ-Algebren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErN__bggDh9m"
      },
      "source": [
        "###### *Field of Sets (Algebra oder Mengensystem)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIdFJLmUG_C"
      },
      "source": [
        "**Mengensystem** oder **Mengenalgebra** oder **Fields of Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3axOdZgGNHd"
      },
      "source": [
        "* **A field of sets is a pair $\\langle X, \\mathcal{F}\\rangle$ where $X$ is a set and $\\mathcal{F}$ is an algebra over $X$**\n",
        "\n",
        "* i.e., a subset of the power set of $X$, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying $X \\in \\mathcal{F}$.\n",
        "\n",
        "* In other words, $\\mathcal{F}$ forms\n",
        "a subalgebra of the power set Boolean algebra of $X$ (with the same identity element $X \\in \\mathcal{F}$ ).\n",
        "(Many authors refer to $\\mathcal{F}$ itself as a field of sets.) Elements of $X$ are called points and those of $\\mathcal{F}$\n",
        "are called complexes and are said to be the admissible sets of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct9dwFuXEuFS"
      },
      "source": [
        "Œ© (Omega) sei eine beliebige (Grund-)Menge. Ein System $\\mathcal{A}$ (oder $\\mathcal{F}$) von Teilmengen von Œ© hei√üt eine Mengenalgebra oder Algebra √ºber Œ©, wenn folgende Eigenschaften erf√ºllt sind:\n",
        "\n",
        "1. $\\mathcal{A} \\neq \\emptyset$ ( $\\mathcal{A}$ ist nicht leer)\n",
        "\n",
        "2. $A, B \\in \\mathcal{A} \\Rightarrow A \\cup B \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Vereinigung)\n",
        "\n",
        "3. $A \\in \\mathcal{A} \\Rightarrow A^{\\mathrm{c}} \\in \\mathcal{A}$ (Stabilit√§t/Abgeschlossenheit bez√ºglich Komplementbildung $\\left.A^{c}=\\Omega \\backslash A\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ksvdizjJS-"
      },
      "source": [
        "* In der Mathematik ist (Mengen-)Algebra ein Grundbegriff der Ma√ütheorie. Er beschreibt ein nicht-leeres Mengensystem, das vereinigungs- und komplementstabil ist.\n",
        "\n",
        "* A field of sets is a **pair ‚ü®X,F‚ü©** where X is a set and F is an algebra over X i.e., a subset of the power set of X, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying X ‚àà F.\n",
        "\n",
        "* In other words, F forms a subalgebra of the power set Boolean algebra of X (with the same identity element X ‚àà F). (Many authors refer to F itself as a field of sets.)\n",
        "\n",
        "* **Elements of X are called points and those of F are called complexes and are said to be the admissible sets of X.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRO-7ajId2V"
      },
      "source": [
        "* For arbitrary set $Y$, its power set (Potenzmenge) $2^{Y}$ (or, somewhat pedantically, the pair $\\left\\langle Y, 2^{Y}\\right\\rangle$ of this set and its power set) is a field of sets.\n",
        "\n",
        "* If $Y$ is finite (namely, $n$ -element), then $2^{Y}$ is finite (namely, $2^{n}$ element).\n",
        "\n",
        "* It appears that every finite field of sets (it means, $\\langle X, \\mathcal{F}\\rangle$ with $\\mathcal{F}$ finite, while $X$ may be infinite) admits a representation of the form $\\left\\langle Y, 2^{Y}\\right\\rangle$ with finite $Y ;$ it means a function $f: X \\rightarrow Y$ that establishes a one-to-one correspondence between $\\mathcal{F}$ and $2^{Y}$ via inverse image:\n",
        "$S=f^{-1}[B]=\\{x \\in X \\mid f(x) \\in B\\}$ where $S \\in \\mathcal{F}$ and $B \\in 2^{Y}$ (that is, $B \\subset Y$ ).\n",
        "\n",
        "* One notable consequence: the number of complexes, if finite, is always of the form $2^{n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkRhwrZHSGp"
      },
      "source": [
        "**Beispiele f√ºr Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFXRKkuHYNd"
      },
      "source": [
        "* F√ºr jede beliebige Menge $\\Omega$ ist $\\{\\emptyset, \\Omega\\}$ die kleinste und die Potenzmenge $\\mathcal{P}(\\Omega)$ die gr√∂√ütm√∂gliche Mengenalgebra.\n",
        "* Jede $\\sigma$ -Algebra ist eine Mengenalgebra.\n",
        "* F√ºr jede Menge $\\Omega$ ist das Mengensystem $\\mathcal{A}=\\left\\{A \\subseteq \\Omega \\mid A \\text { endlich oder } A^{c} \\text { endlich }\\right\\}$ eine Mengenalgebra. Wenn $\\Omega$ unendich ist, dann ist $\\mathcal{A}$ keine $\\sigma$ -Algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded5mK7pcYoX"
      },
      "source": [
        "**Separative and compact fields of sets: towards Stone duality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7fxFxMgcbMU"
      },
      "source": [
        "* A field of sets is called **separative (or differentiated)** if and only if for every pair of distinct points there is a complex containing one and not the other.\n",
        "\n",
        "* A field of sets is called **compact** if and only if for every proper filter over X the intersection of all the complexes contained in the filter is non-empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL_pBvidGa5"
      },
      "source": [
        "Given a field of sets $\\mathbf{X}=\\langle X, \\mathcal{F}\\rangle$ the complexes form a base for a topology. We denote by $T(\\mathbf{X})$ the corresponding topological space, $\\langle X, \\mathcal{T}\\rangle$ where $\\mathcal{T}$ is the topology formed by taking arbitrary unions of complexes. Then\n",
        "\n",
        "1. $T(\\mathbf{X})$ is always a [zero-dimensional space](https://en.m.wikipedia.org/wiki/Zero-dimensional_space)\n",
        "\n",
        "2. $T(\\mathbf{X})$ is a [Hausdorff space](https://en.m.wikipedia.org/wiki/Hausdorff_space) if and only if $\\mathbf{X}$ is separative.\n",
        "\n",
        "3. $T(\\mathbf{X})$ is a compact space with compact open sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is compact.\n",
        "\n",
        "4. $T(\\mathbf{X})$ is a Boolean space with clopen sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is both separative and compact (in which case it is described as being descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I7YepgQlABC"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Field_of_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8T8hpoiWFR"
      },
      "source": [
        "###### *œÉ-algebra (Sigma Algebra)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpgaCc5SSQx"
      },
      "source": [
        "**œÉ-algebra (Sigma Algebra)**\n",
        "\n",
        "* https://mathepedia.de/Sigma-Algebren.html\n",
        "\n",
        "* Sei $\\Omega \\neq \\emptyset$ eine Menge, $\\mathfrak{P}(\\Omega)$ die Potenzmenge und $\\mathcal{F} \\subseteq \\mathfrak{P}(\\Omega)$ ein Mengensystem (=field of sets).\n",
        "\n",
        "Definition\n",
        "\n",
        "$\\mathcal{F}$ hei√üt Algebra, wenn folgende Eigenschaften gelten:\n",
        "\n",
        "(1) $\\quad \\emptyset \\in \\mathcal{F}$\n",
        "\n",
        "(2) $\\quad A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}$\n",
        "\n",
        "(3) $\\quad A, B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}$\n",
        "\n",
        "$\\mathcal{F}$ hei√üt $\\sigma$ -Algebra, wenn die Punkte (1) und (2) gelten\n",
        "und zus√§tzlich\n",
        "\n",
        "(4) $\\quad A_{1}, A_{2}, \\ldots \\in \\mathcal{F} \\Rightarrow \\bigcup A_{k} \\in \\mathcal{F}$\n",
        "\n",
        "gilt.\n",
        "\n",
        "Algebren sind bez√ºglich der endlichen Vereinigung abgeschlossene Mengensysteme und $\\sigma$ -Algebren sind bez√ºglich der abz√§hlbaren Vereinigung abgeschlossene\n",
        "Mengensysteme. Wegen (1) und (2) gilt stets $\\Omega \\in \\mathcal{F}$.\n",
        "\n",
        "* If an algebra over a **set is closed under countable unions** (hence also under countable intersections), it is called a **sigma algebra** and **the corresponding field of sets (Mengensystem) is called a measurable space**. The complexes of a measurable space are called measurable sets.\n",
        "\n",
        "* * **A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it.** (Alternative: $\\langle$ Œ© , $\\mathcal{F}$, $\\mu$ $\\rangle$)\n",
        "\n",
        "* If $\\mu$ is in fact a probability measure we speak of a probability space and call its underlying measurable space a sample space.\n",
        "\n",
        "* The points of a sample space are called samples and represent potential outcomes while the measurable sets (complexes) are called events and represent properties of outcomes for which we wish to assign probabilities. (Many use the term sample space simply for the underlying set of a probability space, particularly in the case where every subset is an event.)\n",
        "\n",
        "* Measure spaces and probability spaces play a foundational role in measure theory and probability theory respectively.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebra_(Mengensystem)\n",
        "\n",
        "**Pain Point**\n",
        "\n",
        "* How to define a messbare Menge?\n",
        "\n",
        "* Wenn wir eine Menge aus den reellen Zahlen haben und mochten eine Teilbereich [a bis b] messen, dann brauchen wir einen allgemeinen Massbegriff unabhangig von der konkreten Menge. Hier kommt Masstheorie und Sigma-Algebra.\n",
        "\n",
        "**Definition**\n",
        "\n",
        "* A œÉ-algebra defines the **set of events that can be measured**, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".\n",
        "\n",
        "* Exkurs: Ergebnis vs Ereignis. Die m√∂glichen Ausg√§nge eines Zufallsexperimentes nennt man **Ergebnisse** (zB auf einem W√ºfel die Zahlen 1,2,3..). Wenn man alle m√∂glichen Ergebnisse eines Zufallsexperimentes in einer Menge zusammenfasst, erh√§lt man die **Ergebnismenge**. Sie wird √ºblicherweise mit dem Symbol Œ© (sprich Omega) bezeichnet. Beim W√ºrfeln ist Œ©= {1; 2; 3; 4; 5; 6} die Ergebnismenge. Jede Zusammenfassung von einem oder mehreren Ergebnissen eines Zufallsexperimentes in einer Menge nennt man **Ereignis** (zB auf einem W√ºrfel die Menge an geraden Zahlen {2,4,6} und ungeraden Zahlen {1,3,5}.\n",
        "\n",
        "* Eine Sigma-Algebra F ist ein System, um alle m√∂glichen **Ereignisse** (nicht Ergebnisse!) eines Zufallsexperiment zu beschreiben. Ereignisse sind an sich selbst Mengen, die man wie jede Menge vereinigen oder schneiden bzw. auch das Komplement bilden kann um so das Gegenereignis zu erhalten. Fasst man hier alle m√∂glichen Kombinationen an Ereignissen in einer Menge zusammen, bekommt man eine Menge, die wiederum Mengen als Elemente enth√§lt - eine Menge von Mengen sozusagen. Oft sagt man dazu auch einfach Mengensystem. Welche Eigenschaften ein Mengensystem genau haben muss, damit es eine Sigma-Algebra ist steht weiter unten.\n",
        "\n",
        "* Beispiel: Gl√ºcksrad mit blau, rot und gr√ºn. Dann haben wir folglich drei Ergebnisse, die wir auch abk√ºrzen k√∂nnen: Œ©={B,R,G}. Generell kann man sich schon merken: Œ© und ‚àÖ sind immer Elemente einer Sigma-Algebra. Daher haben wir hier 8 m√∂gliche Teilmengen von Œ©, die wir als Ereignis betrachten k√∂nnen und demnach als Menge in der Sigma-Algebra zusammenfassen (Potenzmenge von Omega): F ={‚àÖ, {B}, {R}, {G}, {B,R}, {B,G}, {R,G}, {B,R,G}} ([Source](https://www.massmatics.de/merkzettel/#!876:Ereignisraum_&_Sigma-Algebra)).\n",
        "\n",
        "* Bei diskreten Ergebnismengen kann man f√ºr die Sigma-Algebra immer die Potenzmenge P(Œ©)nehmen und hat demnach dann stets diesen **Ereignisraum: (Œ©,P(Œ©))**\n",
        "\n",
        "* Und f√ºr die reellen Zahlen gibt es die sogenannte **Borelsche Sigma-Algebra B**, die man dann auch in der Regel benutzt. Ist die Ergebnismenge Œ© eine Teilmenge der reellen Zahlen (oder ‚Ñù selbst), so nehmen wir die Borelsche-Sigma B und der Ereignisraum lautet (Œ©,B).\n",
        "\n",
        "* Wenn wir eine Sigma Algebra A gegeben haben, dann heisst jede Teilmenge in diesem Mengensystem (jedes Element aus dieser Sigma Algebra A) eine messbare Teilmenge (=die Mengen die wir messen wollen).\n",
        "\n",
        "* **<u>Die Elemente der Sigma Algebra sind die messbaren Teilmengen von unserer Grundmenge X</u>** (Und messbar ist der wesentliche Begriff). Das ist zB die Menge an vergangenen Trading-Events am Finanzmarkt bis zum Zeitpunkt t.\n",
        "\n",
        "* Sigma Algebra ist ein **Mengensystem von einer Teilmenge einer gegebenen Grundmenge** = der Raum, **den wir beschreiben wollen** (mit drei Eigenschaften). Die Menge einer Sigma-Algebra nennt man ‚Äû**messbare Teilmengen**‚Äú.\n",
        "\n",
        "**Eigenschaften**\n",
        "\n",
        "* **A $\\subseteq$ P(X) (=Potenzmenge) heisst Sigma Algebra, wenn gilt** (Die Mengen, die in dieser Sigma Algebra liegen, das sind jene, die folgende drei Eigenschaften erf√ºllen, und sind die, die wir messen wollen (=diesen Mengen wollen wir ein Mass zuordnen). Potenzmenge selbst soll eine Sigma Algebra sein. Sollten gewissen Eigenschaften der Potenzmenge fordern). **<u>A collection of subsets</u> A is called a œÉ-algebra on a set X if the following properties are met:**\n",
        "\n",
        "1. **A contains X (the set itself)**: $\\quad \\phi, X \\in A$ (Leere Menge (sollte L√§nge oder Volumen Null haben) und ganze Grundmenge selbst haben wir im Mengensystem / sollen messbar sein. Das ist was Sigma Algebra sagt). **Œ© ‚àà F (Ergebnismenge muss enthalten sein)**\n",
        "\n",
        "2. **If A contains a subset S, then A also contains the complement of S**: $A \\in A \\Rightarrow A^{c}:=X \\backslash A \\in A$ (Irgendein Element in der Algebra: dann sollte auch dessen Komplement im Mengensystem enthalten sein.) Hiermit ist auch Regel 1 eingeschlossen! Deswegen liegt auch die leere Menge (Gegenereignis von Œ©) in F.\n",
        "\n",
        "3. **Consider a countable collection of subsets. If each subset is included in A, then A must also contain their reunion.**: $A_{i} \\in A$ fur i $\\in N \\Rightarrow \\bigcup_{i=1}^{\\infty} A_{i} \\in A$ ((Letzter Punkt macht das Sigma aus): Abz√§hlbarkeit, abz√§hlbare Summe (A i‚Äòs aus unseren Mengensystem A): wir haben endlich viele bzw. abz√§hlbar viele, dann k√∂nnen wir die Vereinigung bilden / abziehbare Vereinigung. Die abz√§hlbare Vereinigung soll wieder in der Sigma Algebra liegen = Wenn wir L√§ngen haben, dann sollten wir die auch addieren k√∂nnen, auch wenn sich die Addition bis unendlich streckt! (blick auf messbarkeit))\n",
        "\n",
        "Having defined such a œÉ-algebra A, we call **the elements of œÉ-algebra A measurable sets** and the couple (X, A) a measurable space. An arbitrary set X can be a member of a multitude of œÉ-algebras. We denote the set of all œÉ-algebras that contain X with M(X). The **intersection of all those œÉ-algebras is called the œÉ-algebra generated by X**.\n",
        "\n",
        "**A œÉ-algebra (also œÉ-field) on a set X is a collection Œ£ of subsets of X that includes X itself, is closed under complement, and is closed under countable unions**. The definition implies that it also includes the empty subset and that it is closed under countable intersections. The pair (X, Œ£) is called a measurable space or Borel space. A œÉ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of finitely many subsets, which is a weaker condition.\n",
        "\n",
        "**Borel‚Äòsche Sigma-Algebra**\n",
        "\n",
        "* T ist ein topologischer Raum (oder ein metrischer Raum im engeren Sinn.) und X eine Menge darin. ‚ÄûOffene Mengen‚Äú.\n",
        "\n",
        "* Die Borel‚Äôsche Sigma Algebra auf topologischen Raum X ist jene kleinste Sigma Algebra, die von den offenen Mengen erzeugt wird.\n",
        "\n",
        "* B(X) := (T)\n",
        "\n",
        "**Measurable function**\n",
        "\n",
        "* **A set is measurable when it‚Äôs included in a œÉ-algebra.**\n",
        "\n",
        "* We can also extend the ‚Äúmeasurable‚Äù attribute to functions. Here‚Äôs how:\n",
        "\n",
        "* Let‚Äôs consider (X, A) and (Y, B) two measurable spaces. A function f from A to B is called measurable if every set from B comes from applying f to a set from A. Formally, we say that for any element S of B, the pre-image of S under the function f is in A.\n",
        "\n",
        "\n",
        "**Application**\n",
        "\n",
        "* The main use of œÉ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a œÉ-algebra.\n",
        "\n",
        "* This concept is important in mathematical analysis as the **foundation for Lebesgue integration**, and in probability theory, where it is **interpreted as the collection of events which can be assigned probabilities**.\n",
        "\n",
        "* Also, **in probability, œÉ-algebras are pivotal in the definition of conditional expectation**.\n",
        "\n",
        "* In statistics, (sub) œÉ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "1. **Minimum**: Sigma Algebra A enth√§lt leere Menge und Grundmenge selbst (kleinste Sigma Algebra die m√∂glich ist): A = {ùúô,X}\n",
        "2. **Maximum**: Sigma Algebra enth√§lt die Potenzmenge (beinhaltet alle Teilmengen von X): A = P(X)\n",
        "\n",
        "* If {A1, A2, A3, ‚Ä¶} is a countable partition of X then the **collection of all unions of sets in the partition** (including the empty set) is a œÉ-algebra.\n",
        "\n",
        "* A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the œÉ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. \"coarsest\") œÉ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZiS2oORjId"
      },
      "source": [
        "###### *Measure-theoretic probability theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-XQOB0Rk43"
      },
      "source": [
        "**Measure-theoretic probability theory**\n",
        "\n",
        "* The raison d'√™tre of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.\n",
        "\n",
        "* Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density.\n",
        "\n",
        "* The modern approach to probability theory solves these problems using measure theory to define the probability space:\n",
        "\n",
        "Given any set $\\Omega$ (also called sample space) and a $\\sigma$ -algebra $\\mathcal{F}$ on it, a measure $P$ defined on $\\mathcal{F}$ is\n",
        "called a probability measure if $P(\\Omega)=1$\n",
        "\n",
        "If $\\mathcal{F}$ is the Borel $\\sigma$ -algebra on the set of real numbers, then there is a unique probability measure on\n",
        "$\\mathcal{F}$ for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf.\n",
        "\n",
        "This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.\n",
        "\n",
        "The probability of a set $E$ in the $\\sigma$ -algebra $\\mathcal{F}$ is defined as\n",
        "\n",
        "$P(E)=\\int_{\\omega \\in E} \\mu_{F}(d \\omega)$\n",
        "\n",
        "where the integration is with respect to the measure $\\mu_{F}$ induced by $F$\n",
        "\n",
        "Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside R<sup>n</sup>, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.\n",
        "\n",
        "When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure.\n",
        "\n",
        "* Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes.\n",
        "\n",
        "* Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure.\n",
        "\n",
        "* If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Probability_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S4z_HeLRbtc"
      },
      "source": [
        "###### *Filtrations*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdKxs43TtLO"
      },
      "source": [
        "**Filtrations**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Filtrierung_(Wahrscheinlichkeitstheorie)\n",
        "\n",
        "* In martingale theory and the theory of stochastic processes, a **filtration is an increasing sequence of œÉ-algebras on a measurable space**.\n",
        "\n",
        "* That is, given a measurable space $(\\Omega, \\mathcal{F}),$ a filtration is a sequence of $\\sigma$ -algebras $\\left\\{\\mathcal{F}_{t}\\right\\}_{t \\geq 0}$ with $\\mathcal{F}_{t} \\subseteq \\mathcal{F}$ where each $t$ is a non-negative real number and\n",
        "\n",
        "> $t_{1} \\leq t_{2} \\Longrightarrow \\mathcal{F}_{t_{1}} \\subseteq \\mathcal{F}_{t_{2}}$\n",
        "\n",
        "* The exact range of the \"times\" $t$ will usually depend on context: the set of values for $t$ might be discrete or continuous, bounded or unbounded. For example,\n",
        "\n",
        "> $t \\in\\{0,1, \\ldots, N\\}, \\mathbb{N}_{0},[0, T]$ or $[0,+\\infty)$\n",
        "\n",
        "* **A œÉ-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".**\n",
        "\n",
        "* **Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information**.\n",
        "\n",
        "* A typical example is in mathematical finance, where a filtration represents the information available up to and including each time t, and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n",
        "\n",
        "A Filtration is a growing sequence of sigma algebras\n",
        "\n",
        "> $\\mathcal{F}_{1} \\subseteq \\mathcal{F}_{2} \\ldots \\subseteq \\mathcal{F}_{n}$\n",
        "\n",
        "When talking of martingales we need to talk of conditional expectations, and in particular conditional expectations w.r.t œÉ algebra's. So whenever we write\n",
        "\n",
        "> $E\\left[Y_{n} \\mid X_{1}, X_{2}, \\ldots, X_{n}\\right]$\n",
        "\n",
        "which can be written as\n",
        "\n",
        "> $E\\left[Y_{n+1} \\mid \\mathcal{F}_{n}\\right]$\n",
        "\n",
        "where Fùëõ is a sigma algebra that makes random variables\n",
        "\n",
        "> $X_{1}, \\ldots, X_{n}$\n",
        "\n",
        "measurable. Finally a flitration F1,‚Ä¶Fn is simply an increasing sequence of sigma algebras. That is **we are conditioning on growing amounts of information**.\n",
        "\n",
        "> **Der Begriff der Filtrierung ist unerl√§sslich, um, ausgehend vom Begriff des stochastischen Prozesses, wichtige Begriffe wie Martingale oder Stoppzeiten einzuf√ºhren.**\n",
        "\n",
        "* Als Menge $T$ wird wie bei stochastischen Prozessen meist $\\mathbb{R}_{+}$ oder $\\mathbb{N}_{0}$ gew√§hlt und $t \\in T$ als Zeitpunkt interpretiert.\n",
        "\n",
        "* **$\\sigma$ -Algebren modellieren verf√ºgbare Information**. Die Mengen der $\\sigma$ -Algebra $\\mathcal{F}_{t}$ geben zu jedem Zeitpunkt $t$ an, wie viele Informationen zur Zeit bekannt sind. F√ºr jedes Ereignis $A \\subseteq \\Omega$ bedeutet $A \\in \\mathcal{F}_{t}$ √ºbersetzt, dass zum Zeitpunkt $t$ die Frage $,$ ist $\\omega \\in A ?^{\\prime \\prime}$ eindeutig mit $,$ ja\" oder $,$ nein\" beantwortet werden kann.\n",
        "\n",
        "* Dass die Filtrierung stets aufsteigend geordnet ist, bedeutet demnach, **dass eine einmal erlangte Information nicht mehr verloren geht.**\n",
        "\n",
        "* Ist ein stochastischer Prozess $\\left(X_{t}\\right)_{t \\in T}$ an eine Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ adaptiert, bedeutet dies also, dass der Verlauf der Funktion $s \\mapsto X_{s}(\\omega)$ im Intervall $[0, t]$ zum Zeitpunkt $t$ (f√ºr beliebiges, aber unbekanntes $\\omega \\in \\Omega$ und in Hinsicht auf die durch Ereignisse $A \\in \\mathcal{F}_{s}, s \\in[0, t]$ formulierbaren Fragen bekannt ist.\n",
        "\n",
        "* Der Begriff wird aufgrund seiner Bedeutung in den meisten fortgeschrittenen Lehrb√ºchern √ºber stochastische Prozesse definiert. In einigen Lehrb√ºchern, zum Beispiel im Buch Probability von Albert N. Schirjajew, wird der Begriff aus didaktischen Gr√ºnden zun√§chst umfassend f√ºr Prozesse mit diskreten\n",
        "Werten in diskreter Zeit eingef√ºhrt.\n",
        "\n",
        "**Filtration in Finance**\n",
        "\n",
        "* In a multiperiod market, information about the market scenario is revealed in stages.\n",
        "\n",
        "* Some events may be completely determined by the end of the first trading period, others by the end of the second, and others not until the termination of all trading.\n",
        "\n",
        "* This suggests the following classification of events: for each t ‚â§ T ,\n",
        "\n",
        "(1) Ft = {all events determined in the first t trading periods}.\n",
        "\n",
        "* The finite sequence (Ft)0‚â§t‚â§T is a filtration of the space Œ© of market scenarios.\n",
        "\n",
        "* In general, a filtration of a set Œ© (not necessarily finite) is defined to be a collection Ft, indexed by a time parameter t (time may be either discrete or continuous), such that\n",
        "\n",
        "(a) each Ft is a œÉ‚àíalgebra of subsets (events) of Œ©; and\n",
        "\n",
        "(b) if s<t then Fs ‚äÜFt.\n",
        "\n",
        "**Beispiel**\n",
        "\n",
        "* Betrachtet man als Beispiel einen Wahrscheinlichkeitsraum $(\\mathbb{Z}, \\mathcal{P}(\\mathbb{Z}), P)$ mit abz√§hlbarer Grundmenge $\\mathbb{Z}$ die standardm√§√üig mit der Potenzmenge als $\\sigma$ -Algebra ausgestattet ist, so w√§re eine m√∂gliche Filtrierung beispielsweise\n",
        "\n",
        "> $\\mathcal{F}_{n}:=\\sigma(\\mathcal{P}(\\{-n, \\ldots, n\\}))$\n",
        "\n",
        "* Sie modelliert die Informationen, dass man bis zum n-ten Zeitschritt sich bis zu n Schritte vom Ursprung entfernt hat und w√§re beispielsweise die passende Filtrierung f√ºr einen einfachen symmetrischen Random\n",
        "Walk.\n",
        "\n",
        "**Filtration and Stochastic Processes**\n",
        "\n",
        "*  https://almostsure.wordpress.com/2009/11/08/filtrations-and-adapted-processes/\n",
        "\n",
        "* In mathematics, a filtration $\\mathcal{F}$ is an indexed family $\\left(S_{i}\\right)_{i \\in I}$ of subobjects of a given algebraic structure $S,$ with the index $i$ running over some totally ordered index set $I$, subject to the condition\n",
        "that\n",
        "\n",
        "> if $i \\leq j$ in $I,$ then $S_{i} \\subset S_{j}$\n",
        "\n",
        "* If the index i is the time parameter of some stochastic process, then the filtration can be interpreted as **representing all historical but not future information available about the stochastic process**, with the algebraic structure S<sub>i</sub> gaining in complexity with time.\n",
        "\n",
        "* Hence, a process that is adapted to a filtration F, is also called **non-anticipating**, i.e. one that cannot see into the future.\n",
        "\n",
        "* Eine Filtrierung (auch Filtration, Filterung oder Filtern) ist in der Theorie der stochastischen Prozesse eine Familie von verschachtelten œÉ-Algebren. Sie modelliert die zu verschiedenen Zeitpunkten verf√ºgbaren Informationen zum Verlauf eines Zufallsprozesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "###### *Martingale*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFV1b0SVIDD"
      },
      "source": [
        "**Martingale**\n",
        "\n",
        "* Let (Œ©,F,P) be a probability space and (Ft)0‚â§t‚â§T or (Ft)0‚â§t<‚àû a filtration by sub- œÉ‚àíalgebras of F. An adapted sequence Xt of integrable random variables is defined to be a\n",
        "\n",
        "  * martingale if E(Xt+1|Ft) = Xt ‚àÄt (=for all t).\n",
        "  * submartingale if E(Xt+1|Ft) ‚â• Xt ‚àÄt.\n",
        "  * supermartingale if E(Xt+1|Ft) ‚â§ Xt ‚àÄt.\n",
        "\n",
        "> A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it. If $\\mu$ is in fact a probability measure we speak of a probability space and call its\n",
        "underlying measurable space a sample space. The points of a sample space are called samples\n",
        "and represent potential outcomes while the measurable sets (complexes) are called events and\n",
        "represent properties of outcomes for which we wish to assign probabilities. (Many use the term\n",
        "sample space simply for the underlying set of a probability space, particularly in the case where\n",
        "every subset is an event.) Measure spaces and probability spaces play a foundational role in\n",
        "measure theory and probability theory respectively.\n",
        "\n",
        "* In probability theory, a martingale is a sequence of random variables (for example **a stochastic process**) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value.\n",
        "\n",
        "* A **martingale is characterized by the fact that it is fair on average**. Martingales arise naturally from the modeling of fair gambling.(In a fair game of chance, **the expected value of each win is zero**)\n",
        "\n",
        "* The closely related to the martingales are the super martingales, which are stochastic processes with an average loss and submartingales, which are stochastic processes with an average gain.\n",
        "\n",
        "* The property of being a (sub- / super-) martingale does not belong to stochastic processes alone, but always to a stochastic process **in combination with filtration**. Therefore, the filtration should always be specified.\n",
        "\n",
        "A **basic definition** of a discrete-time martingale is a discrete-time stochastic process (i.e., a sequence of random variables) X1, X2, X3, ... that satisfies for any time n,\n",
        "\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|X_{n}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(X_{n+1} \\mid X_{1}, \\ldots, X_{n}\\right)=X_{n}\n",
        "\\end{array}$\n",
        "\n",
        "That is, the conditional expected value of the next observation, given all the past observations, is equal to the most recent observation.\n",
        "\n",
        "A **continuous-time martingale** with respect to the stochastic process X<sub>t</sub> is a stochastic process Y<sub>t</sub> such that for all t\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|Y_{t}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(Y_{t} \\mid\\left\\{X_{\\tau}, \\tau \\leq s\\right\\}\\right)=Y_{s} \\quad \\forall s \\leq t\n",
        "\\end{array}$\n",
        "\n",
        "This expresses the property that the conditional expectation of an observation at time t, given all the observations up to time s, is equal to the observation at time s (of course, provided that s ‚â§ t). Note that the second property implies that Yn is measurable with respect to X1 ‚Ä¶ Xn.\n",
        "\n",
        "In **full generality**, a stochastic process Y : T √ó Œ© ‚Üí S taking value in a [Banach space](https://en.m.wikipedia.org/wiki/Banach_space) S is a martingale with respect to a filtration Œ£‚àó and probability measure P if\n",
        "\n",
        "**Examples of martingales**\n",
        "\n",
        "* An unbiased random walk (in any number of dimensions)\n",
        "\n",
        "* A Wiener process Wt is a martingale, and for a Wiener process the processes W<sub>t</sub><sup>2</sup> - t and the geometric Brownian movement without drift are martingales.\n",
        "\n",
        "* Stopped Brownian motion, which can be used to model the trajectory of such games\n",
        "\n",
        "* A gambler's fortune (capital) is a martingale if all the betting games which the gambler plays are fair. To be more specific: suppose Xn is a gambler's fortune after n tosses of a fair coin, where the gambler wins USD 1 if the coin comes up heads and loses USD 1 if it comes up tails. The gambler's conditional expected fortune after the next trial, given the history, is equal to their present fortune. This sequence is thus a martingale.\n",
        "\n",
        "* If { Nt : t ‚â• 0 } is a Poisson process with intensity Œª, then the compensated Poisson process { Nt ‚àí Œªt : t ‚â• 0 } is a continuous-time martingale with [right-continuous/left-limit](https://en.m.wikipedia.org/wiki/Classification_of_discontinuities) sample paths.\n",
        "\n",
        "* ([Likelihood-ratio testing](https://en.m.wikipedia.org/wiki/Likelihood-ratio_test) in statistics) A random variable X is thought to be distributed according either to probability density f or to a different probability density g. A random sample X1, ..., Xn is taken. Let Yn be the \"likelihood ratio\":\n",
        "\n",
        "> $Y_{n}=\\prod_{i=1}^{n} \\frac{g\\left(X_{i}\\right)}{f\\left(X_{i}\\right)}$\n",
        "\n",
        "If X is actually distributed according to the density f rather than according to g, then { Yn : n = 1, 2, 3, ... } is a martingale with respect to { Xn : n = 1, 2, 3, ... }.\n",
        "\n",
        "* [**Stopped Brownian Motion**](https://en.m.wikipedia.org/wiki/Stopped_process#Brownian_motion): a stopped process is a stochastic process that is forced to assume the same value after a prescribed (possibly random) time.\n",
        "\n",
        "Martingale Property vs Markov Property\n",
        "\n",
        "* In order to formally define the concept of Brownian motion and utilise it as a basis for an asset price model, it is necessary to define the Markov and Martingale properties. These provide an intuition as to how an asset price will behave over time.\n",
        "\n",
        "* The **Markov property** states that a stochastic process essentially has \"no memory\". This means that the conditional probability distribution of the future states of the process are independent of any previous state, with the exception of the current state.\n",
        "\n",
        "* The **Martingale property** states that the future expectation of a stochastic process is equal to the current value, given all known information about the prior events.\n",
        "\n",
        "* https://www.quantstart.com/articles/The-Markov-and-Martingale-Properties/\n",
        "\n",
        "**Simulate Martingale Process**: Toss a coin. toss results (1=lose 0=win). The first step is to find the edges of the losing runs, (steps + edges). You then need to take the difference of the sizes of the steps and shove those values back into the original data. When you take a cumsum of toss2 it gives you the current length of your losing streak. Your bet is then 2 ** cumsum(toss2)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Adapted (Stochastic) Process*"
      ],
      "metadata": {
        "id": "wc37qhDWywkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPqovDyR3ll"
      },
      "source": [
        "[Adapted (Stochastic) Process](https://de.m.wikipedia.org/wiki/Adaptierter_stochastischer_Prozess)\n",
        "\n",
        "* for exmaple in Finance\n",
        "\n",
        "* The share prices of assets in a multiperiod market depend on market scenarios, but evolve in such a way that their values at any time t, being observable at time t, do not depend on the unobservable post-t futures of the scenarios.\n",
        "\n",
        "* Thus, the price process St of a traded asset is **adapted to the natural filtration** (Ft)0‚â§t‚â§T defined by (1).\n",
        "\n",
        "* In general, a sequence Xt of random variables is said to be **adapted to a filtration** (Ft)0‚â§t‚â§T if, for each t, the random variable Xt is **Ft‚àímeasurable**, that is, if all events of the form {œâ : Xt(œâ) ‚àà B}, where **B is a Borel** subset of the real numbers R, are members of the œÉ‚àíalgebra Ft."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stopping Time*"
      ],
      "metadata": {
        "id": "_j3LHUI5y6Co"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWnh7gmSp_P"
      },
      "source": [
        "**Stopping Time**\n",
        "\n",
        "* [Stoppzeit (Markov Moment)](\n",
        "https://de.m.wikipedia.org/wiki/Stoppzeit): In der Stochastik bezeichnet der Begriff der Stoppzeit eine spezielle Art von Zufallsvariablen, die auf filtrierten Wahrscheinlichkeitsr√§umen definiert werden.\n",
        "\n",
        "  * Stoppzeiten sind nicht nur von Bedeutung f√ºr die Theorie der stochastischen Prozesse (beispielsweise bei der Lokalisierung von Prozessklassen oder Untersuchungen von gestoppten Prozessen), sondern auch von praktischer Relevanz, etwa f√ºr das Problem des optimalen Aus√ºbungszeitpunkts f√ºr amerikanische Optionen.\n",
        "\n",
        "  * Eine Stoppzeit kann man als die Wartezeit interpretieren, die vergeht, bis ein bestimmtes zuf√§lliges Ereignis eintritt. Wenn wie √ºblich die Filtrierung die vorhandene Information zu verschiedenen Zeitpunkten angibt, bedeutet die obige Bedingung also, dass zu jeder Zeit bekannt sein soll, ob dieses Ereignis bereits eingetreten ist oder nicht.\n",
        "\n",
        "* Optional Stopping Theorem: Das [Optional Stopping Theorem](https://de.m.wikipedia.org/wiki/Optional_Stopping_Theorem) ist ein mathematischer Satz √ºber Martingale, eine spezielle Klasse von stochastischen Prozessen, und damit der Wahrscheinlichkeitstheorie zuzuordnen.\n",
        "\n",
        "* [Optional Sampling Theorem](https://de.m.wikipedia.org/wiki/Optional_Sampling_Theorem): Eine popul√§re Version dieses Theorems besagt, dass es bei einem fairen, sich wiederholenden Spiel keine Abbruchstrategie gibt, mit der man seinen Gesamtgewinn verbessern kann.\n",
        "\n",
        "* [Starke Markoweigenschaft](https://de.m.wikipedia.org/wiki/Starke_Markoweigenschaft)\n",
        "\n",
        "* Filtrierung von Stoppzeiten:\n",
        "\n",
        "  * Eine Stoppzeit $\\tau: \\Omega \\rightarrow[0, \\infty]$ bez√ºglich einer beliebigen Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in[0, \\infty)}$ erzeugt in Analogie zur nat√ºrlichen Filtrierung eine $\\sigma$ -Algebra, die sogenannte $\\sigma$ -Algebra der $\\tau$ -Vergangenheit\n",
        "\n",
        "  > $\\mathcal{F}_{\\tau}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t}\\right\\} \\text { mit } \\mathcal{F}_{\\infty}=\\sigma\\left(\\bigcup_{t \\in[0, \\infty)} \\mathcal{F}_{t}\\right)$\n",
        "\n",
        "  * Sei nun $\\left(\\tau_{j}\\right)_{j \\in J}$ eine geordnete Familie von Stoppzeiten mit $P\\left(\\tau_{i} \\leq \\tau_{j}\\right)=1$ f√ºr alle $i, j \\in J$ mit $i \\leq j$ dann ist die Familie $\\left(\\mathcal{F}_{\\tau_{j}}\\right)_{j \\in J}$ eine Filtrierung, diese ist beim Studium von Stoppzeiten stochastischer Prozesse von Bedeutung.\n",
        "\n",
        "  * In Analogie erzeugt man die rechtsstetige Version der Filtrierung $\\left(\\mathcal{F}_{\\tau_{j}+}\\right)_{j \\in J}$ wobei:\n",
        "\n",
        "  > $\\mathcal{F}_{r+}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t+}\\right\\} \\text { und } \\mathcal{F}_{t+}=\\bigcap_{u \\in(t, \\infty)} \\mathcal{F}_{u}$\n",
        "\n",
        "  * Es gilt immer $\\mathcal{F}_{\\tau} \\subseteq \\mathcal{F}_{r+}$\n",
        "\n",
        "* [Vorhersagbarer Prozess](https://de.m.wikipedia.org/wiki/Vorhersagbarer_Prozess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *œÉ-Algebra der œÑ-Vergangenheit*"
      ],
      "metadata": {
        "id": "qCTiyeYsy0Y0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhsdp5wBSiAq"
      },
      "source": [
        "**œÉ-Algebra der œÑ-Vergangenheit**\n",
        "\n",
        "* Die [œÉ-Algebra der œÑ-Vergangenheit](https://de.m.wikipedia.org/wiki/Œ£-Algebra_der_œÑ-Vergangenheit) ist ein **Mengensystem**, sowie ein von der Stoppzeit abgeleitetes Konzept\n",
        "\n",
        "* Die œÉ-Algebra der œÑ-Vergangenheit ist eine **spezielle œÉ-Algebra**, welche √ºber die Filtrierung und die Stoppzeit definiert wird. Sie findet beispielsweise Anwendung bei der Definition der starken Markow-Eigenschaft und dem Optional Sampling Theorem.\n",
        "\n",
        "* Sie entsteht durch Kombination einer Filtrierung mit einer Stoppzeit und findet meist Anwendung bei Aussagen √ºber gestoppte Prozesse, also stochastische Prozesse, die an einem zuf√§lligen Zeitpunkt angehalten werden. Zu diesen Aussagen geh√∂ren beispielsweise das Optional Stopping Theorem, das Optional Sampling Theorem und die Definition der starken Markow-Eigenschaft.\n",
        "\n",
        "* Gegeben sei ein Wahrscheinlichkeitsraum $(\\Omega, \\mathcal{A}, P)$ sowie eine Filtrierung $\\mathbb{F}=\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ bez√ºglich der Ober- $\\sigma$ -Algebra $\\mathcal{A}$ und eine Stoppzeit $\\tau$ bez√ºglich $\\mathbb{F}$. Dann hei√üt\n",
        "\n",
        "$\\mathcal{F}_{\\tau}=\\left\\{A \\in \\mathcal{A} \\mid A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t} \\text { f√ºr alle } t \\in T\\right\\}$\n",
        "\n",
        "die $\\sigma$ -Algebra der $\\tau$ -Vergangenheit."
      ]
    }
  ]
}