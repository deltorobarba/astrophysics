{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "activation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N7OFxbU41pw",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNuFUedQd_vs",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZpemZIdcay",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "* Use ReLU. Be careful with your learning rates\n",
        "* Try out Leaky ReLU / Maxout / ELU\n",
        "* Try out tanh but don’t expect much\n",
        "* Don’t use sigmoid\n",
        "\n",
        "\n",
        "**Activation Functions - Key Points**\n",
        "* Reside within neurons\n",
        "* Transform input values into acceptable and useful range\n",
        "* Allow pass-through of values which are useful in subsequent layers of neurons\n",
        "* Default hidden layer activation function is ReLU\n",
        "* Sigmoid only for binary classification output layer\n",
        "\n",
        "Additional Sources: [kdnuggets](https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html) & [exxactcorp](https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68EhvxK9Pet9",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dJRlYxPVGZ",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y64HtUX0gdQZ",
        "colab_type": "text"
      },
      "source": [
        "Source: [Stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0vpxOzOXKr",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyZPPEcYc0",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "$g(z)=\\frac{1}{1+e^{-z}}$\n",
        "\n",
        "<br> \n",
        "**Characteristics**\n",
        "* Logistic regression. Squashes numbers to range [0,1]\n",
        "* Sigmoid activation derived from mean field solution of Boltzmann machine\n",
        "* Softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
        "\n",
        "**Advantages**\n",
        "* Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.\n",
        "* The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
        "* Especially used for models where we have to predict the probability as an output.\n",
        "* Sigmoid works well for a classifier: approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence\n",
        "\n",
        "**Disadvantages**\n",
        "* Saturated neurons “kill” the gradients (look at x= -10, 0 and 10). Can cause the neural network to get stuck during training. If a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since neural networks usethe feed-forward activations to calculate parameter gradients, this can result in model parameters that are updated less regularly than we would like, and are thus “stuck” in their current state.\n",
        "* Sigmoid outputs are not zero-centered. Mean not at zero anymore. Consider what happens when the input to a neuron (x) is always positive. What can we say about the gradients on w? They are always all positive or all negative (and run zig-zag)! (this is also why you want zero-mean data!)\n",
        "* exp() is a bit compute expensive. Learning time longer.\n",
        "* The function is monotonic but function’s derivative is not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDAIwoT1dnWo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## tanh\n",
        "\n",
        " $g(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$\n",
        "\n",
        "* LeCun et al., 1991\n",
        "* Squashes numbers to range [-1,1]\n",
        "* zero centered (nice)\n",
        "* The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
        "* The function is differentiable.\n",
        "* The function is monotonic while its derivative is not monotonic.\n",
        "* The tanh function is mainly used classification between two classes.\n",
        "* still kills gradients when saturated\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O35eK5VlSY87",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ReLU\n",
        "\n",
        "$g(z)=\\max (0, z)$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Characteristics**\n",
        "* Krizhevsky et al., 2012\n",
        "* rectified linear units, faster and more efficient, since fewer neurons are activated (less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations). \n",
        "* No gradient vanishing problem, as Relu’s gradient is constant = 1. Sparsity: since output 0 for negative values of x! When W*x < 0, Relu gives 0, which means sparsity. Less calculation load. This may be least important. \n",
        "* However, ReLu may amplify the signal inside the network more than softmax and sigmoid. \n",
        "* But: dying ReLU problem for values zero and smaller: neurons will never reactivated. Solution: leaky ReLU, noisy ReLU (in RBMs) and ELU (exponential linear units)\n",
        "* ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.\n",
        "\n",
        "**Advantages**\n",
        "* Does not saturate (in +region)\n",
        "* Very computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
        "Actually more biologically plausible than sigmoid\n",
        "\n",
        "**Disadvantages**\n",
        "* Not zero-centered output\n",
        "* An annoyance: what is the gradient when x < 0? What happens when x = -10, 0 or 10?\n",
        "* People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWsQRCAfmaK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Leaky ReLU\n",
        "\n",
        "$\\begin{aligned}\n",
        "g(z) &=\\max (\\epsilon z, z) \\\\\n",
        "& \\text { with } \\epsilon \\ll 1\n",
        "\\end{aligned}$\n",
        "\n",
        "* Mass et al., 2013 and He et al., 2015\n",
        "* Does not saturate\n",
        "* Computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice! (e.g. 6x) will not “die”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUyiCkgf4Aw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ELU\n",
        "\n",
        "* Exponential Linear Units\n",
        "* Clevert et al., 2015\n",
        "* All benefits of ReLU\n",
        "* Closer to zero mean outputs\n",
        "* Negative saturation regime compared with Leaky ReLU adds some robustness to noise \n",
        "* But Computation requires exp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2ikh67gXRC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## SELU\n",
        "\n",
        "* scaled exponential linear units\n",
        "* instead of normalizing the output of the activation function — the activation function suggested (SELU — scaled exponential linear units) outputs normalized values. https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "* Background: batchnormalization for feedfirward networks: Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. (https://arxiv.org/abs/1502.03167)\n",
        "* Negative values sometimes: Scaling the function is the mechanism by which the authors accomplish the goal (of self-normalizing properties). As a byproduct, they sometimes output negative values, but there's no hidden meaning in it. It just makes the math work out. \n",
        "* **SELU vs RELU**: https://www.hardikp.com/2017/07/24/SELU-vs-RELU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GDi-lkbeMo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Softmax\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers.\n",
        "* usually used in the last layer\n",
        "* Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) \n",
        "* is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We use the (standard) Logistic Regression model in binary classification tasks. in softmax regression (SMR), we replace the sigmoid logistic function by the so-called€softmax function€φ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB13XKQhyz9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Maxout\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4-kecUPvDN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## List of Activation Functions\n",
        "\n",
        "Comparison of Activation Functions:\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x81WYFo2vr_S",
        "colab_type": "text"
      },
      "source": [
        "* Activation functions can be defined as [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
        "\n",
        "* Activation functions can be defined as [activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ckAo-8hCBH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCY-TGgqnOFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jNUhO2Og63T",
        "colab_type": "text"
      },
      "source": [
        "## Select Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unoQZNcWnTrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = 'relu'\n",
        "# activation = 'linear'\n",
        "# activation = 'sigmoid'\n",
        "# activation = 'tanh'\n",
        "# activation = 'softmax'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMwV7R8ohG3d",
        "colab_type": "text"
      },
      "source": [
        "## Build Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r87fTNFmjrhX",
        "colab_type": "code",
        "outputId": "2b3eb466-b9e6-411f-a9e7-4b96f14d0525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation=activation))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4966 - acc: 0.8213 - val_loss: 0.3971 - val_acc: 0.8539\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3808 - acc: 0.8604 - val_loss: 0.3767 - val_acc: 0.8629\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3502 - acc: 0.8718 - val_loss: 0.3798 - val_acc: 0.8625\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3276 - acc: 0.8800 - val_loss: 0.3706 - val_acc: 0.8668\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3113 - acc: 0.8842 - val_loss: 0.3541 - val_acc: 0.8725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f171012e978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}