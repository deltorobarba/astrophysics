{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0jCYoWZoHd2X",
        "9dfYtm7HHhpn",
        "OpcnLfYSHmlq",
        "zwF3TGA4HyGe",
        "yO5uuQMehFZD",
        "CXBpZ1M8zNi5",
        "cxTnwP8u5YD2",
        "oAf4sna4hQMg",
        "deNJWcC3hUSX",
        "jtmj4FOHq6ht",
        "ZrVKpOHIzWHG",
        "8UfyDobq5cXp",
        "dfT-0QZZm8VW",
        "gD8HZ1Q1giLt",
        "BmZFI8hdhy1u",
        "-59nC3Yih3Gm",
        "HN2Dn3F0iAzD",
        "TSRliOMMiHC-",
        "tvctBykzhmbB",
        "ZT8FPvZ3hi1N",
        "rYLc2b-qsm8N",
        "lydB1B0oq-VJ",
        "DhwL39dIoAR5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "# **Information Geometry (Entropy & Statistical Learning) NEW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8YntQ5cG4l-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfmBr0auIUJC"
      },
      "source": [
        "https://www.kaggle.com/debanga/statistical-distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-i-EOMX2yTa"
      },
      "source": [
        "https://www.inovex.de/blog/the-mystery-of-entropy-how-to-measure-unpredictability-in-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvYF3jYCGiRj"
      },
      "source": [
        "https://franknielsen.github.io\n",
        "\n",
        "\n",
        "http://yosinski.com/mlss12/MLSS-2012-Amari-Information-Geometry/\n",
        "\n",
        "\n",
        "https://www.frontiersin.org/articles/10.3389/fevo.2019.00447/full\n",
        "\n",
        "\n",
        "https://numerics.mathdotnet.com/Distance.html\n",
        "\n",
        "\n",
        "https://research.wmz.ninja/articles/2018/03/a-brief-list-of-statistical-divergences.html\n",
        "\n",
        "\n",
        "https://link.springer.com/article/10.1007/s00362-018-01082-8?shared-article-renderer\n",
        "\n",
        "\n",
        "https://gmarti.gitlab.io//qfin/2020/07/01/mutual-information-is-copula-entropy.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VefQJanNG6a6"
      },
      "source": [
        "## **Information Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE6jnCm3G85l"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Entropy_(information_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Information_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wnEN4OmHG9p"
      },
      "source": [
        "### **Information Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZDkuqffHJnr"
      },
      "source": [
        "#### **Information Gain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJkhaidZHLSG"
      },
      "source": [
        "**Mutual Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrBFWqRyHM1K"
      },
      "source": [
        "* Mutual Information is also known as information gain.\n",
        "\n",
        "* In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. \n",
        "\n",
        "* More specifically, it quantifies the \"amount of information\" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable. \n",
        "\n",
        "* The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected \"amount of information\" held in a random variable.\n",
        "\n",
        "* Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair (X, Y) is to the product of the marginal distributions of X and Y. MI is the expected value of the pointwise mutual information (PMI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPscRlapHOtm"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Mutual_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWUMDHP6HQyi"
      },
      "source": [
        "**Kullback–Leibler divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InmmPaXEHSoz"
      },
      "source": [
        "* the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
        "\n",
        "* Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. \n",
        "\n",
        "* In contrast to variation of information, **it is a distribution-wise asymmetric measure** and thus **does not qualify as a statistical metric of spread** - it also does not satisfy the triangle inequality.\n",
        "\n",
        "* In the simple case, a Kullback–Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zvq7jNZHXsF"
      },
      "source": [
        "#### **Variation of information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8l6m_UAHZBA"
      },
      "source": [
        "* In probability theory and information theory, the variation of information or shared information distance is a measure of the distance between two clusterings (partitions of elements). \n",
        "\n",
        "* It is closely related to mutual information; indeed, it is a simple linear expression involving the mutual information. \n",
        "\n",
        "* Unlike the mutual information, however, **the variation of information is a true metric**, in that it obeys the triangle inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMcAMeUGHa5V"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Variation_of_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jCYoWZoHd2X"
      },
      "source": [
        "### **Quantities of information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGtNC3UHHf5P"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Quantities_of_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dfYtm7HHhpn"
      },
      "source": [
        "#### **Information Content (Self Information)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2iCcCMmHjEv"
      },
      "source": [
        "* In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable. It can be thought of as an alternative way of expressing probability, much like odds or log-odds, but which has particular mathematical advantages in the setting of information theory.\n",
        "\n",
        "* The Shannon information can be interpreted as quantifying the level of \"surprise\" of a particular outcome. As it is such a basic quantity, it also appears in several other settings, such as the length of a message needed to transmit the event given an optimal source coding of the random variable.\n",
        "\n",
        "* The **Shannon information is closely related to information (theoretic) entropy**, which is the expected value of the self-information of a random variable, quantifying how surprising the random variable is \"on average.\" This is the average amount of self-information an observer would expect to gain about a random variable when measuring it.\n",
        "\n",
        "* The information content can be expressed in various units of information, of which the most common is the \"bit\" (sometimes also called the \"shannon\"), as explained below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tM6tWbNHk9P"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpcnLfYSHmlq"
      },
      "source": [
        "#### **Units of Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0zl_yCmHoGV"
      },
      "source": [
        "**shannon**\n",
        "\n",
        "* The shannon (symbol: Sh), more commonly known as the bit, is a unit of information and of entropy defined by IEC 80000-13. One shannon is the information content of an event occurring when its probability is ​1⁄2.\n",
        "\n",
        "* It is also the entropy of a system with two equally probable states. If a message is made of a sequence of a given number of bits, with all possible bit strings being equally likely, the message's information content expressed in shannons is equal to the number of bits in the sequence.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Shannon_(unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toe-HxUTHplE"
      },
      "source": [
        "**nat**\n",
        "\n",
        "* The natural unit of information (symbol: nat), sometimes also nit or nepit, is a unit of information or entropy, based on natural logarithms and powers of e, rather than the powers of 2 and base 2 logarithms, which define the bit. \n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Nat_(unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaZy4pU5HrKZ"
      },
      "source": [
        "**Hartley**\n",
        "\n",
        "* The hartley (symbol Hart), also called a ban, or a dit (short for decimal digit), is a logarithmic unit which measures information or entropy, based on base 10 logarithms and powers of 10, rather than the powers of 2 and base 2 logarithms which define the bit, or shannon. \n",
        "\n",
        "* One ban or hartley is the information content of an event if the probability of that event occurring is ​1⁄10. It is therefore equal to the information contained in one decimal digit (or dit), assuming a priori equiprobability of each possible value.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hartley_(unit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJLTT4PWIGJ6"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Jensen%27s_inequality\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Fisher_information#Matrix_form\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Information_content\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Probability_theory#Measure-theoretic_probability_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwF3TGA4HyGe"
      },
      "source": [
        "### **Cross Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJVQSAzHzfe"
      },
      "source": [
        "* the cross entropy between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEHec9phH1hb"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Cross_entropy\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Cross-entropy_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEZnguPZH4CX"
      },
      "source": [
        "**Conditional Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ekAmVTH5lJ"
      },
      "source": [
        "* the conditional entropy (or equivocation)\n",
        "quantifies the amount of information needed to describe the outcome of a random variable $Y$ given that the value of another\n",
        "random variable $X$ is known. Here, information is measured in shannons, nats, or hartleys. The entropy of $Y$ conditioned on $X$ is written as $\\mathrm{H}(Y \\mid X)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1gLjoOpH7Tb"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Conditional_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AcuGJjH9yd"
      },
      "source": [
        "**Joint Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Iko81KIAT_"
      },
      "source": [
        "* In information theory, joint entropy is a measure of the uncertainty associated with a set of variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai0GeirSIDkx"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Joint_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO5uuQMehFZD"
      },
      "source": [
        "### **Similarity & Distances**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXBpZ1M8zNi5"
      },
      "source": [
        "#### **Similarity Learning & Similarity Measures**\n",
        "\n",
        "*Ähnlichkeitsmaße werden für nominal oder ordinal skalierte Variablen genutzt*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxTnwP8u5YD2"
      },
      "source": [
        "##### **Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhCXyrkgLB0"
      },
      "source": [
        "In der Statistik, insbesondere der Multivariaten Statistik, interessiert man sich für die Messung der Ähnlichkeit zwischen verschiedenen Objekten und definiert dazu Ähnlichkeits- und Distanzmaße. **Es handelt sich dabei nicht um Maße im mathematischen Sinn**, der Begriff bezieht sich ausschließlich auf die Messung einer bestimmten Größe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez_pHYiXgPcu"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Ähnlichkeitsanalyse\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Distanzfunktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_3BJSrgnoi"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Distance_(graph_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAf4sna4hQMg"
      },
      "source": [
        "##### **Jaccard-Koeffizient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYu5h2eLhR-N"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Jaccard-Koeffizient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deNJWcC3hUSX"
      },
      "source": [
        "##### **Yules Index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGeN7197hXMA"
      },
      "source": [
        "Yules Index ist ein statistischer Messwert, der die Uniformität oder Diversität des Wortschatzes bestimmt. Er wurde vom schottischen Statistiker George Udny Yule entwickelt und misst die Wahrscheinlichkeit, mit der zwei zufällig ausgewählte Wörter eines Textes identisch sind – und zwar weitgehend unabhängig vom Umfang des Textes.Diesen Index hat Herdan aufgegriffen und weiterentwickelt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-YLY8Q4hZmk"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Yules_Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmj4FOHq6ht"
      },
      "source": [
        "##### **Pearson Korrelationskoeffizient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0A6wTO22MZ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Korrelationskoeffizient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrVKpOHIzWHG"
      },
      "source": [
        "#### **Distance Metric Learning & Distance Measures**\n",
        "\n",
        "*Distanzmaße werden für metrisch skalierte Variablen (d. h. für Intervall- und Verhältnisskala) genutzt.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UfyDobq5cXp"
      },
      "source": [
        "##### **Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AepjAni7x_g9"
      },
      "source": [
        "**Exkurs**: [Distance measures](https://en.m.wikipedia.org/wiki/Distance_measures_(cosmology)) in cosmology are complicated by the [expansion of the universe](https://en.m.wikipedia.org/wiki/Expansion_of_the_universe), and by effects described by the [theory of relativity](https://en.m.wikipedia.org/wiki/Theory_of_relativity) such as [length contraction](https://en.m.wikipedia.org/wiki/Length_contraction) of moving objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfT-0QZZm8VW"
      },
      "source": [
        "##### **Properties of a Divergence, Distance & Metric**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HScpxU3gCkB"
      },
      "source": [
        "1. $d(x, y) \\geq 0 \\quad$ (**non-negativity**)\n",
        "\n",
        "2. $d(x, y)=0$ if and only if $x=y$ (**identity of indiscernibles**. Note that condition 1 and 2 together produce **positive definiteness**)\n",
        "\n",
        "3. $d(x, y)=d(y, x)$ (**symmetry**)\n",
        "\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (**subadditivity / triangle inequality**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGoqrlNKgEo8"
      },
      "source": [
        "* **Divergence** fullfills property of positive definiteness (1 + 2)\n",
        "\n",
        "* **Distance** fullfills property of positive definiteness and symmetrie (1 + 2+ 3)\n",
        "\n",
        "* **Metric** fullfills property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3 + 4)\n",
        "\n",
        "* Metric Space: Together with the set, a metric makes up a metric space.\n",
        "\n",
        "* (*Jede Norm induziert eine Metrik, aber nicht jede Metrik wird durch eine Norm induziert*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuiVX3avwKmS"
      },
      "source": [
        "**Distance function vs. Distance Metric**\n",
        "\n",
        "• Distance Metric:\n",
        "▫ Satisfy non-negativity, symmetry and triangle\n",
        "inequation\n",
        "\n",
        "• Distance Function:\n",
        "▫ May not satisfy one or more requirements for\n",
        "distance metric\n",
        "▫ More general than distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLWj7je0wMiG"
      },
      "source": [
        "https://www.cs.utexas.edu/~grauman/courses/spring2008/slides/Learning_distance_functions.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8HZ1Q1giLt"
      },
      "source": [
        "##### **Divergence & 'Statistical Distance'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqH4VD9zgjtm"
      },
      "source": [
        "In statistics and information geometry, divergence or a contrast function is a function which establishes the **\"distance\" of one probability distribution to the other** on a statistical manifold. In statistics, probability theory, and information theory, **a statistical distance** quantifies the distance between two statistical objects, which can be\n",
        "\n",
        "* two random variables, or \n",
        "* two probability distributions or \n",
        "* two samples, or \n",
        "* the distance can be between an individual sample point and a population or \n",
        "* a wider sample of points.\n",
        "\n",
        "A distance between populations can be interpreted as **measuring the distance between two probability distributions** and hence they are essentially measures of distances between probability measures. \n",
        "\n",
        "* Where statistical distance measures relate to the differences between random variables, these may have statistical dependence, and hence these distances are not directly related to measures of distances between probability measures. \n",
        "\n",
        "* Again, a measure of distance between random variables may **relate to the extent of dependence between them, rather than to their individual values.**\n",
        "\n",
        "**<u>Many statistical distances are not metrics</u>** (and some types are regerred to as divergence), because they lack one or more properties of proper metrics. For example, \n",
        "\n",
        "* [pseudometrics](https://en.m.wikipedia.org/wiki/Pseudometric_space) violate the \"positive definiteness\" (alternatively, \"identity of indescernibles\") property (1 & 2 above); \n",
        "\n",
        "* [quasimetrics](https://en.m.wikipedia.org/wiki/Metric_(mathematics)#Quasimetrics) violate the symmetry property (3); and semimetrics violate the triangle inequality (4). \n",
        "\n",
        "* Statistical distances that satisfy (1) and (2) are referred to as divergences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2GffaHgpYZ"
      },
      "source": [
        "* In statistics and information geometry, there are many kinds of statistical distances, notably divergences, especially Bregman divergences and f-divergences. These include and generalize many of the notions of \"difference between two probability distributions\", and allow them to be studied geometrically, as statistical manifolds. \n",
        "\n",
        "* The **most elementary** is the **squared Euclidean distance**, which forms the basis of least squares; this is the most basic Bregman divergence (-> is this a metric then ???)\n",
        "\n",
        "* The **most important** in information theory is the relative entropy (**Kullback–Leibler divergence**), which allows one to analogously study maximum likelihood estimation geometrically; this is the most basic f-divergence, and is also a Bregman divergence (and is the only divergence that is both). \n",
        "\n",
        "* Statistical manifolds corresponding to Bregman divergences are flat manifolds in the corresponding geometry, allowing an analog of the Pythagorean theorem (which is traditionally true for squared Euclidean distance) to be used for linear inverse problems in inference by optimization theory.\n",
        "\n",
        "* Other important statistical distances include the Mahalanobis distance, the energy distance, and many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_6VnI9tgsJe"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_geometry\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Statistical_distance\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ok6ieBgYkv"
      },
      "source": [
        "**Exkurs: Meaning of 'no symmetry' in divergences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFglF2QTgaEc"
      },
      "source": [
        "* The Kullback-Leibler divergence is not symmetric. Roughly speaking, it's because you should think of the two arguments of the KL divergence as different kinds of things: the first argument is empirical data, and the second argument is a model you're comparing the data to. \n",
        "\n",
        "* Take a bunch of independent random variables $X_{1}, \\ldots, X_{n}$ whose possible values lie in a finite set.\n",
        "\n",
        "* Say these variables are identically distributed, with $\\operatorname{Pr}\\left(X_{i}=x\\right)=p_{x}$. Let $F_{n, x}$ be the number of variables whose values are equal to $x$. The list $F_{n}$ is a random variable, often called the \"empirical frequency distribution\" of the $X_{i} .$ What does $F_{n}$ look like when $n$ is very large?\n",
        "\n",
        "* More specifically, let's try to estimate the probabilities of the possible values of $F_{n} .$ since the set of possible values is different for different $n$, take a sequence of frequency distributions $f_{1}, f_{2}, f_{3}, \\ldots$ approaching a fixed frequency distribution $f$. It turns out $^{* *}$ that\n",
        "\n",
        "> $\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\ln \\operatorname{Pr}\\left(F_{n}=f_{n}\\right)=-\\mathrm{KL}(f, p)$ \n",
        "\n",
        "* In other words, the Kullback-Leibler divergence of $f$ from $p$ lets you estimate the probability of getting an empirical frequency distribution close to $f$ from a large number of independent random variables with distribution $p$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEE5LAAygcf-"
      },
      "source": [
        "Excellent article \"Information Theory, Relative Entropy and Statistics,\" by [François Bavaud](https://link.springer.com/chapter/10.1007/978-3-642-00659-3_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1geLcP8Pgut9"
      },
      "source": [
        "List of Distances Types\n",
        "\n",
        "'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance\n",
        "\n",
        "'canberra': hdbscan.dist_metrics.CanberraDistance\n",
        "\n",
        "'chebyshev': hdbscan.dist_metrics.ChebyshevDistance\n",
        "\n",
        "'cityblock': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        "'dice': hdbscan.dist_metrics.DiceDistance\n",
        "\n",
        "'euclidean': hdbscan.dist_metrics.EuclideanDistance\n",
        "\n",
        "'hamming': hdbscan.dist_metrics.HammingDistance\n",
        "\n",
        "'haversine': hdbscan.dist_metrics.HaversineDistance\n",
        "\n",
        "'infinity': hdbscan.dist_metrics.ChebyshevDistance\n",
        "\n",
        "'jaccard': hdbscan.dist_metrics.JaccardDistance\n",
        "\n",
        "'kulsinski': hdbscan.dist_metrics.KulsinskiDistance\n",
        "\n",
        "'l1': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        "'l2': hdbscan.dist_metrics.EuclideanDistance\n",
        "\n",
        "'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance\n",
        "\n",
        "'manhattan': hdbscan.dist_metrics.ManhattanDistance\n",
        "\n",
        "'matching': hdbscan.dist_metrics.MatchingDistance\n",
        "\n",
        "'minkowski': hdbscan.dist_metrics.MinkowskiDistance\n",
        "\n",
        "'p': hdbscan.dist_metrics.MinkowskiDistance\n",
        "\n",
        "'pyfunc': hdbscan.dist_metrics.PyFuncDistance\n",
        "\n",
        "'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance\n",
        "\n",
        "'russellrao': hdbscan.dist_metrics.RussellRaoDistance\n",
        "\n",
        "'seuclidean': hdbscan.dist_metrics.SEuclideanDistance\n",
        "\n",
        "'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance\n",
        "\n",
        "'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance\n",
        "\n",
        "'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance\n",
        "\n",
        "https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\n",
        "\n",
        "https://reference.wolfram.com/language/guide/DistanceAndSimilarityMeasures.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmZFI8hdhy1u"
      },
      "source": [
        "##### **f-Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9smhem9wh0x8"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "The Hellinger distance is a type of f-divergence\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59nC3Yih3Gm"
      },
      "source": [
        "##### **Bregman Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXaFiel6h7Fl"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Bregman_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnDfoUM_h5VG"
      },
      "source": [
        "The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvhq-zZ2h9Ae"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN2Dn3F0iAzD"
      },
      "source": [
        "##### **Kullback–Leibler divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OWsTyvziEce"
      },
      "source": [
        "The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSRliOMMiHC-"
      },
      "source": [
        "##### **Jensen–Shannon divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AgQmDiLiJW_"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Jensen–Shannon_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvctBykzhmbB"
      },
      "source": [
        "##### **Mahalanobis distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-TZ_CmyyVV-"
      },
      "source": [
        "* The Mahalanobis distance is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guFweB6AhoJs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYZzNeN0mJJ2"
      },
      "source": [
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution. \n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCC_7jqmo0rN"
      },
      "source": [
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/machinelearning/master/mahalanobis.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAydAR1XqBN_"
      },
      "source": [
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdi3fJK7pK-O"
      },
      "source": [
        "So computationally, how is Mahalanobis distance different from Euclidean distance?\n",
        "\n",
        "1. It transforms the columns into uncorrelated variables\n",
        "2. Scale the columns to make their variance equal to 1\n",
        "3. Finally, it calculates the Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzlk1JzZmQRz"
      },
      "source": [
        "https://www.machinelearningplus.com/statistics/mahalanobis-distance/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Jv2Z1mmUFw"
      },
      "source": [
        "The Mahalanobis distance has the following properties:\n",
        "\n",
        "* It accounts for the fact that the variances in each direction are different.\n",
        "\n",
        "* It accounts for the covariance between variables.\n",
        "\n",
        "* It reduces to the familiar Euclidean distance for uncorrelated variables with unit variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Hwd5cNpl_c"
      },
      "source": [
        "Distance in standard units\n",
        "\n",
        "In statistics, we sometimes measure \"nearness\" or \"farness\" in terms of the scale of the data. **Often \"scale\" means \"standard deviation.\"** For univariate data, we say that an observation that is one standard deviation from the mean is closer to the mean than an observation that is three standard deviations away. (You can also specify the distance between two observations by specifying how many standard deviations apart they are.)\n",
        "\n",
        "**For many distributions, such as the normal distribution, this choice of scale also makes a statement about probability**. Specifically, it is more likely to observe an observation that is about one standard deviation from the mean than it is to observe one that is several standard deviations away. Why? Because the probability density function is higher near the mean and nearly zero as you move many standard deviations away.\n",
        "\n",
        "**For normally distributed data, you can specify the distance from the mean by computing the so-called z-score**. For a value x, the z-score of x is the quantity z = (x-μ)/σ, where μ is the population mean and σ is the population standard deviation. This is a dimensionless quantity that you can interpret as the number of standard deviations that x is from the mean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjjqmwLmY0F"
      },
      "source": [
        "https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT8FPvZ3hi1N"
      },
      "source": [
        "##### **Bhattacharyya distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1YoJ42mx-xb"
      },
      "source": [
        "* In statistics, the Bhattacharyya distance measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations. \n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same. \n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AVM3T1Dhkld"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYLc2b-qsm8N"
      },
      "source": [
        "##### **Hellinger Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEGZl1OtyhBy"
      },
      "source": [
        "*  the Hellinger distance (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**. \n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erfüllt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owVHpnpSywyW"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Hellinger_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lydB1B0oq-VJ"
      },
      "source": [
        "##### **Euclidean L1 and Manhatten Distance L2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGE57oYk2smE"
      },
      "source": [
        "Sinde it fullfills all 4 properties, it is not only a distance but also a metric (see details below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhwL39dIoAR5"
      },
      "source": [
        "##### **Squared Euclidean Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mHVqq2toEeh"
      },
      "source": [
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis. \n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjoet-ldoMGv"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance"
      ]
    }
  ]
}