{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B1HrnrATe2Sr",
        "-YMl6Sm5fumW",
        "El_L8v73edyK",
        "ygm4xHq2ghfV",
        "hnk7cuh4bwKI",
        "JkzdtTBMfEkP",
        "6cvgIAIAakIE",
        "hx-l56cebNOh",
        "t5dBaUIwf5y5",
        "OagtrigB2Sc-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV",
        "colab_type": "text"
      },
      "source": [
        "# **Entropy (Information Theory)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U5_6xH7tAQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLNaWhBdgWg6",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Entropy_(information_theory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4fb_tuIgYN1",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1HrnrATe2Sr",
        "colab_type": "text"
      },
      "source": [
        "## **Information Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YMl6Sm5fumW",
        "colab_type": "text"
      },
      "source": [
        "#### **Information Gain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaX08S6JdvTy",
        "colab_type": "text"
      },
      "source": [
        "**Mutual Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VneFEFI8dyWq",
        "colab_type": "text"
      },
      "source": [
        "* Mutual Information is also known as information gain.\n",
        "\n",
        "* In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. \n",
        "\n",
        "* More specifically, it quantifies the \"amount of information\" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable. \n",
        "\n",
        "* The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected \"amount of information\" held in a random variable.\n",
        "\n",
        "* Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair (X, Y) is to the product of the marginal distributions of X and Y. MI is the expected value of the pointwise mutual information (PMI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iqUy1wHeEWd",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Mutual_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZvIenXQeNB9",
        "colab_type": "text"
      },
      "source": [
        "**Kullback–Leibler divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mD-J59MeRL1",
        "colab_type": "text"
      },
      "source": [
        "* the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
        "\n",
        "* Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. \n",
        "\n",
        "* In contrast to variation of information, **it is a distribution-wise asymmetric measure** and thus **does not qualify as a statistical metric of spread** - it also does not satisfy the triangle inequality.\n",
        "\n",
        "* In the simple case, a Kullback–Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_L8v73edyK",
        "colab_type": "text"
      },
      "source": [
        "#### **Variation of information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB3mPxJVfZ0g",
        "colab_type": "text"
      },
      "source": [
        "* In probability theory and information theory, the variation of information or shared information distance is a measure of the distance between two clusterings (partitions of elements). \n",
        "\n",
        "* It is closely related to mutual information; indeed, it is a simple linear expression involving the mutual information. \n",
        "\n",
        "* Unlike the mutual information, however, **the variation of information is a true metric**, in that it obeys the triangle inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78iSo_hbeg7e",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Variation_of_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_G15PsDdGKp",
        "colab_type": "text"
      },
      "source": [
        "## **Quantities of information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSxbNrvrdJxA",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Quantities_of_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygm4xHq2ghfV",
        "colab_type": "text"
      },
      "source": [
        "#### **Information Content (Self Information)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrTbr2Bbguy1",
        "colab_type": "text"
      },
      "source": [
        "* In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable. It can be thought of as an alternative way of expressing probability, much like odds or log-odds, but which has particular mathematical advantages in the setting of information theory.\n",
        "\n",
        "* The Shannon information can be interpreted as quantifying the level of \"surprise\" of a particular outcome. As it is such a basic quantity, it also appears in several other settings, such as the length of a message needed to transmit the event given an optimal source coding of the random variable.\n",
        "\n",
        "* The **Shannon information is closely related to information (theoretic) entropy**, which is the expected value of the self-information of a random variable, quantifying how surprising the random variable is \"on average.\" This is the average amount of self-information an observer would expect to gain about a random variable when measuring it.\n",
        "\n",
        "* The information content can be expressed in various units of information, of which the most common is the \"bit\" (sometimes also called the \"shannon\"), as explained below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQ3bmgxgmRt",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnk7cuh4bwKI",
        "colab_type": "text"
      },
      "source": [
        "#### **Units of Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEpiFZ-nb7nV",
        "colab_type": "text"
      },
      "source": [
        "**shannon**\n",
        "\n",
        "* The shannon (symbol: Sh), more commonly known as the bit, is a unit of information and of entropy defined by IEC 80000-13. One shannon is the information content of an event occurring when its probability is ​1⁄2.\n",
        "\n",
        "* It is also the entropy of a system with two equally probable states. If a message is made of a sequence of a given number of bits, with all possible bit strings being equally likely, the message's information content expressed in shannons is equal to the number of bits in the sequence.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Shannon_(unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGp45cZOcKO8",
        "colab_type": "text"
      },
      "source": [
        "**nat**\n",
        "\n",
        "* The natural unit of information (symbol: nat), sometimes also nit or nepit, is a unit of information or entropy, based on natural logarithms and powers of e, rather than the powers of 2 and base 2 logarithms, which define the bit. \n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Nat_(unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz0GiXDVcUhO",
        "colab_type": "text"
      },
      "source": [
        "**Hartley**\n",
        "\n",
        "* The hartley (symbol Hart), also called a ban, or a dit (short for decimal digit), is a logarithmic unit which measures information or entropy, based on base 10 logarithms and powers of 10, rather than the powers of 2 and base 2 logarithms which define the bit, or shannon. \n",
        "\n",
        "* One ban or hartley is the information content of an event if the probability of that event occurring is ​1⁄10. It is therefore equal to the information contained in one decimal digit (or dit), assuming a priori equiprobability of each possible value.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hartley_(unit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzdtTBMfEkP",
        "colab_type": "text"
      },
      "source": [
        "## **Measures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cvgIAIAakIE",
        "colab_type": "text"
      },
      "source": [
        "#### **Cross Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wih47Pj8asdd",
        "colab_type": "text"
      },
      "source": [
        "* the cross entropy between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRCMsyJapY4",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Cross_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cofUTyUta_rI",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Cross-entropy_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx-l56cebNOh",
        "colab_type": "text"
      },
      "source": [
        "#### **Conditional Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwMUyV8LbhtY",
        "colab_type": "text"
      },
      "source": [
        "* the conditional entropy (or equivocation)\n",
        "quantifies the amount of information needed to describe the outcome of a random variable $Y$ given that the value of another\n",
        "random variable $X$ is known. Here, information is measured in shannons, nats, or hartleys. The entropy of $Y$ conditioned on $X$ is written as $\\mathrm{H}(Y \\mid X)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW8TymssbQ8Q",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Conditional_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5dBaUIwf5y5",
        "colab_type": "text"
      },
      "source": [
        "#### **Joint Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "204tw2flgB6Q",
        "colab_type": "text"
      },
      "source": [
        "* In information theory, joint entropy is a measure of the uncertainty associated with a set of variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzin9ZvWf-WR",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Joint_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OagtrigB2Sc-",
        "colab_type": "text"
      },
      "source": [
        "#### **Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF2bXuy2l25",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Jensen%27s_inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhYmPAGW2oGk",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Fisher_information#Matrix_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7tjIIei22xi",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Information_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-viFWasmc1h6",
        "colab_type": "text"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Probability_theory#Measure-theoretic_probability_theory"
      ]
    }
  ]
}