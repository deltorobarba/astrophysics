{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "learningrate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/learningrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOEi6BGsJnlE",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkV8aXX_rpnk",
        "colab_type": "text"
      },
      "source": [
        "* being used in optimizers / cost function\n",
        "* Learning rate is an important component of backpropagation in a neural network:\n",
        "* New Value of weight a = Old value a - (learning rate * gradient ∂SSE/∂a) θ=θ−η⋅∇θJ(θ)\n",
        "\n",
        "**Ian Goodfellow answering to \"Why do not use the whole training set to compute the gradient?\" on Quora**\n",
        "\n",
        "* The size of the learning rate is limited mostly by factors like how curved the cost function is. You can think of gradient descent as making a linear approximation to the cost function, then moving downhill along that approximate cost. \n",
        "* If the cost function is highly non-linear (highly curved) then the approximation will not be very good for very far, so only small step sizes are safe. You can read more about this in Chapter 4 of the deep learning textbook, on numerical computation: http://www.deeplearningbook.org/contents/numerical.html\n",
        "* When you put m examples in a minibatch, you need to do O(m) computation and use O(m) memory, but you reduce the amount of uncertainty in the gradient by a factor of only O(sqrt(m)). In other words, there are diminishing marginal returns to putting more examples in the minibatch. You can read more about this in Chapter 8 of the deep learning textbook, on optimization algorithms for deep learning: http://www.deeplearningbook.org/contents/optimization.html\n",
        "* Also, if you think about it, even using the entire training set doesn’t really give you the true gradient. The true gradient would be the expected gradient with the expectation taken over all possible examples, weighted by the data generating distribution. Using the entire training set is just using a very large minibatch size, where the size of your minibatch is limited by the amount you spend on data collection, rather than the amount you spend on computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usQCqhUvyRE",
        "colab_type": "text"
      },
      "source": [
        "## Reasons for using a Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT1p3eJpoz95",
        "colab_type": "text"
      },
      "source": [
        "* Prevent overreaction which can cause loss increase\n",
        "* Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight. \n",
        "* However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight.\n",
        "* You can also think of a neural networks loss function as a surface, where each direction you can move in, represents the value of a weight. Gradient descent is like taking leaps in the current direction of the slope, and the learning rate is like the length of the leap you take.\n",
        "* Learning rate decay os alternative to momentum: when replacing gradient descent with SDG, we take smaller, noisier steps towards objective (minimum).\n",
        "* How small should steps be? Much research! Always: beneficial to make steps smaller and smaller; i.e. apply exponential decay to learning rate, others make smaller every time loss reaches a plateau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raFoZMJfnbTd",
        "colab_type": "text"
      },
      "source": [
        "![Learning Rate](https://raw.githubusercontent.com/deltorobarba/repo/master/F640CFA7-A209-4639-A1B0-E260375A5EEE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFiTZeDxvo9T",
        "colab_type": "text"
      },
      "source": [
        "## Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOl775W2r_wv",
        "colab_type": "text"
      },
      "source": [
        "* Which learning rate: If low, training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny. If learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n",
        "* A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n",
        "* Which Cost function: with SSE cost function the value of θF(Wj)/θWj gets larger and larger as we increase the size of the training dataset\n",
        "* Change learning rate during process? Upwards or downwards?\n",
        "* Learning rate schedules [11] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [10].\n",
        "* Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n",
        "* As full-scale hyperparameter optimization: Selecting a learning rate is a \"meta-problem\" (hyperparameter optimization). The best learning rate depends on the problem at hand, as well as on the architecture of the model being optimized, and even on the state of the model in the current optimization process! There are even software packages devoted to hyperparameter optimization such as spearmint and hyperopt (just a couple of examples, there are many others!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvKRdJUwv5pe",
        "colab_type": "text"
      },
      "source": [
        "## Cosine Annealing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9M10jfktFTw",
        "colab_type": "text"
      },
      "source": [
        "**Adapting learning rate in each iteration downwards (Annealing)**\n",
        "\n",
        "Optimize is the learning rate during training. conventional: decrease over time. There are Multiple ways: step-wise learning rate annealing when the loss stops improving, exponential learning rate decay, cosine annealing, etc. [Source](\n",
        "https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/)\n",
        "\n",
        "\n",
        "**Approach 2: Time-based learning rate schedule**\n",
        "* The learning rate for stochastic gradient descent has been set to a higher value of 0.1. \n",
        "* The model is trained for 50 epochs and the decay argument has been set to 0.002, calculated as 0.1/50. \n",
        "* Additionally, it can be a good idea to use momentum when using an adaptive learning rate. In this case we use a momentum value of 0.8.\n",
        "* Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine tuning later.\n",
        "* Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.\n",
        "* Experiment with different schedules. It will not be clear which learning rate schedule to use so try a few with different configuration options and see what works best on your problem. Also try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets\n",
        "\n",
        "**Approach 1: Drop-based learning rate schedule**\n",
        "* Often this method is implemented by dropping the learning rate by half every fixed number of epochs. \n",
        "* For example, we may have an initial learning rate of 0.1 and drop it by 0.5 every 10 epochs. The first 10 epochs of training would use a value of 0.1, in the next 10 epochs a learning rate of 0.05 would be used, and so on\n",
        "\n",
        "**Approach 3: Adapting learning rate in each iteration upwards**\n",
        "* Leslie N. Smith describes a powerful technique to select a range of learning rates for a neural network in section 3.3 of the 2015 paper [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/abs/1506.01186) . \n",
        "* The trick is to train a network starting from a low learning rate and increase the learning rate exponentially for every batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsikqbqXzDXm",
        "colab_type": "text"
      },
      "source": [
        "## Batch Size Incrase vs Learning Rate Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cm-LHkcy0sG",
        "colab_type": "text"
      },
      "source": [
        "**Don't Decay the Learning Rate, Increase the Batch Size**\n",
        "\n",
        "* Paper: https://arxiv.org/abs/1711.00489\n",
        "* It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. \n",
        "* It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate ϵ and scaling the batch size B∝ϵ. \n",
        "* Finally, one can increase the momentum coefficient m and scale B∝1/(1−m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1% validation accuracy in under 30 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHu0whFdwoH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmKZX1K4Jrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niyell7b3bLz",
        "colab_type": "code",
        "outputId": "aca16725-4f3e-4884-ed7d-1df7a6b7f1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYk9nB0c3fJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Select Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uOQFKMrZW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61kx4ugDrKfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKoJ94krWZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDEv6ZETl129",
        "colab_type": "code",
        "outputId": "691e0db9-250d-4ad6-d4f0-6de579bd88d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, \n",
        "                                    momentum=0.9, \n",
        "                                    nesterov=False),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.7493 - acc: 0.7522 - val_loss: 0.5664 - val_acc: 0.8069\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.5273 - acc: 0.8192 - val_loss: 0.5011 - val_acc: 0.8287\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4764 - acc: 0.8355 - val_loss: 0.4692 - val_acc: 0.8365\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4479 - acc: 0.8443 - val_loss: 0.4499 - val_acc: 0.8397\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4261 - acc: 0.8519 - val_loss: 0.4367 - val_acc: 0.8439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60609fa588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}