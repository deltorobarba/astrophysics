{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "learningrate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/learningrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOEi6BGsJnlE",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "* being used in optimizers / cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raFoZMJfnbTd",
        "colab_type": "text"
      },
      "source": [
        "![Learning Rate](https://raw.githubusercontent.com/deltorobarba/repo/master/F640CFA7-A209-4639-A1B0-E260375A5EEE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkV8aXX_rpnk",
        "colab_type": "text"
      },
      "source": [
        "Learning rate is an important component of backpropagation in a neural network:\n",
        "\n",
        "New Value of weight a = Old value a - (learning rate * gradient ∂SSE/∂a) θ=θ−η⋅∇θJ(θ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT1p3eJpoz95",
        "colab_type": "text"
      },
      "source": [
        "**Reason for Learning Rate**\n",
        "* Prevent overreaction which can cause loss increase\n",
        "* Neural networks are often trained by gradient descent on the weights. This means at each iteration we use backpropagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight. \n",
        "* However, if you actually try that, the weights will change far too much each iteration, which will make them “overcorrect” and the loss will actually increase/diverge. So in practice, people usually multiply each derivative by a small value called the “learning rate” before they subtract it from its corresponding weight.\n",
        "* You can also think of a neural networks loss function as a surface, where each direction you can move in, represents the value of a weight. Gradient descent is like taking leaps in the current direction of the slope, and the learning rate is like the length of the leap you take.\n",
        "* Learning rate decay os alternative to momentum: when replacing gradient descent with SDG, we take smaller, noisier steps towards objective (minimum).\n",
        "* How small should steps be? Much research! Always: beneficial to make steps smaller and smaller; i.e. apply exponential decay to learning rate, others make smaller every time loss reaches a plateau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOl775W2r_wv",
        "colab_type": "text"
      },
      "source": [
        "**Challenges** \n",
        "\n",
        "* Which learning rate: If low, training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny. If learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n",
        "* A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n",
        "* Which Cost function: with SSE cost function the value of θF(Wj)/θWj gets larger and larger as we increase the size of the training dataset\n",
        "* Change learning rate during process? Upwards or downwards?\n",
        "* Learning rate schedules [11] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [10].\n",
        "* Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n",
        "* As full-scale hyperparameter optimization: Selecting a learning rate is a \"meta-problem\" (hyperparameter optimization). The best learning rate depends on the problem at hand, as well as on the architecture of the model being optimized, and even on the state of the model in the current optimization process! There are even software packages devoted to hyperparameter optimization such as spearmint and hyperopt (just a couple of examples, there are many others!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHu0whFdwoH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmKZX1K4Jrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niyell7b3bLz",
        "colab_type": "code",
        "outputId": "aca16725-4f3e-4884-ed7d-1df7a6b7f1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYk9nB0c3fJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Select Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uOQFKMrZW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61kx4ugDrKfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKoJ94krWZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDEv6ZETl129",
        "colab_type": "code",
        "outputId": "6dc47548-7327-4fdf-887d-ef3e310d3934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, \n",
        "                                    momentum=0.9, \n",
        "                                    nesterov=False),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5294 - acc: 0.8113 - val_loss: 0.4408 - val_acc: 0.8413\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4001 - acc: 0.8546 - val_loss: 0.4292 - val_acc: 0.8457\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3665 - acc: 0.8669 - val_loss: 0.3779 - val_acc: 0.8635\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3427 - acc: 0.8739 - val_loss: 0.3670 - val_acc: 0.8684\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3272 - acc: 0.8803 - val_loss: 0.3588 - val_acc: 0.8684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6010511f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}