{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometry.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/geometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjNmcYpFuFTY",
        "colab_type": "text"
      },
      "source": [
        "# **Differential (Information) Geometry**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYViK39mt7Nj",
        "colab_type": "code",
        "outputId": "9297d7ea-dccc-483c-bb27-6ed9a1abb124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwyJNSuepvyl",
        "colab_type": "text"
      },
      "source": [
        "# **Distance & Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDaFyrlkAN03",
        "colab_type": "text"
      },
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ≥ 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ≤ d(x, y) + d(y, z)     (subadditivity / triangle inequality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCFr4OMNuoir",
        "colab_type": "text"
      },
      "source": [
        "**Distances**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQsGsly_w8h",
        "colab_type": "text"
      },
      "source": [
        "For continuous data:\n",
        "\n",
        "* Euclidean Distance\n",
        "* Manhattan Distance\n",
        "* Canberra Distance\n",
        "* Bray Curtis Distance\n",
        "* Cosine Distance\n",
        "* Correlation Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiPqyo5E8Uzq",
        "colab_type": "text"
      },
      "source": [
        "**Divergences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYq6ACsE8s7N",
        "colab_type": "text"
      },
      "source": [
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold. \n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback–Leibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMM3R7pCwCHG",
        "colab_type": "text"
      },
      "source": [
        "## **Find the similarity between two probability distributions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eozx-uGEwh_9",
        "colab_type": "text"
      },
      "source": [
        "Using Jensen Shannon Divergence to build a tool to find the distance between probability distributions using Python.\n",
        "\n",
        "I was on a mission to find a good measure of difference between two probability distributions. After doing a lot of research online, taking feedback from my colleagues, and validating various methods, I found one that does a really good job.\n",
        "\n",
        "My problem statement could be solved by calculating the statistical distance between the two probability distributions. To do this, I found out that Jensen Shannon Distance can be used.\n",
        "\n",
        "Jensen-Shannon Divergence (JSD)is a metric derived from another measure of statistical distance called the Kullback-Leiber Divergence(KLD). The reason why I couldn’t use the KLD is that it’s an asymmetrical function. Since there might have been a lot of distance calculations required, it posed a risk.\n",
        "\n",
        "JSD, on the other hand, is a symmetrical function and the square root of JSD gives the Jensen-Shannon Distance. A measure that we can use to find the similarity between the two probability distributions. 0 indicates that the two distributions are the same, and 1 would indicate that they are nowhere similar.\n",
        "\n",
        "Where P & Q are the two probability distribution, M = (P+Q)/2, and D(P ||M) is the KLD between P and M. Similarly D(Q||M) is the KLD between Q and M.\n",
        "Implementation in Python\n",
        "Now that we know the formula, it’s time to implement it. First of all, we need to calculate M and also, the KLD between P&M and Q&M.\n",
        "Scipy is a phenomenal Python Library for scientific computing and it has lots of statistical measures in-built. It turns out that the entropy measure in scipy is implemented using the KLD. Just what we want.\n",
        "\n",
        "(I found it to be quite simple to implement it with python and I got really good results when I tested it with a few distributions.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7Uutxrz9iN",
        "colab_type": "code",
        "outputId": "f87e74ca-7d4a-4099-af0f-eb57554c2f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        " # Create test data\n",
        "p = np.random.rayleigh(3,3)\n",
        "q = np.random.weibull(3,3)\n",
        "p, q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.84848271, 4.18906706, 5.61567569]),\n",
              " array([1.036487  , 0.9192782 , 0.63485667]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zufnQaip3Xgs",
        "colab_type": "code",
        "outputId": "9295beb0-3a93-4a6c-a200-880837d402ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
        "# Calculate the entropy of a distribution for given probability values\n",
        "entropy([p, q], base=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.51682914, 0.471327  , 0.32851553])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfittfZK1uDS",
        "colab_type": "code",
        "outputId": "45ca97b1-13b6-4dd6-b911-8a99baee6e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " m = (p + q) / 2\n",
        "\n",
        "    # compute Jensen Shannon Divergence\n",
        "divergence = (scipy.stats.entropy(p, m))\n",
        "\n",
        "divergence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([inf, inf, inf])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snhT2RwTwKq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create function to compute distance\n",
        "from scipy.stats import entropy\n",
        "def jensen_shannon_distance(p, q):\n",
        "    \"\"\"\n",
        "    method to compute the Jenson-Shannon Distance \n",
        "    between two probability distributions\n",
        "    \"\"\"\n",
        "\n",
        "    # convert the vectors into numpy arrays in case that they aren't\n",
        "    # p = np.array(p)\n",
        "    # q = np.array(q)\n",
        "\n",
        "    # calculate m\n",
        "    m = (p + q) / 2\n",
        "\n",
        "    # compute Jensen Shannon Divergence\n",
        "    divergence = (scipy.stats.entropy(p, m) + scipy.stats.entropy(q, m)) / 2\n",
        "\n",
        "    # compute the Jensen Shannon Distance\n",
        "    distance = np.sqrt(divergence)\n",
        "\n",
        "    return distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOLXS2P0gum",
        "colab_type": "code",
        "outputId": "a6fcaa25-2fec-4f17-bdb4-adc976bfd30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(jensen_shannon_distance(eins,zwei))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[inf inf inf]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R84pGVrPDRV",
        "colab_type": "text"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence"
      ]
    }
  ]
}