{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometry.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/geometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjNmcYpFuFTY",
        "colab_type": "text"
      },
      "source": [
        "# **Differential (Information) Geometry**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhp10G3Muto0",
        "colab_type": "text"
      },
      "source": [
        "## **Import Libraries & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYViK39mt7Nj",
        "colab_type": "code",
        "outputId": "adf9fe01-2044-4942-95fc-1661d96672c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!pip install livelossplot --quiet\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "# Populating the interactive namespace from numpy and matplotlib\n",
        "# %pylab inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import math \n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yact1eGKt9bi",
        "colab_type": "code",
        "outputId": "18702cca-195c-4726-dd15-8f359a696215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwyJNSuepvyl",
        "colab_type": "text"
      },
      "source": [
        "# **Distance & Divergence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R84pGVrPDRV",
        "colab_type": "text"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Gromov%E2%80%93Hausdorff_convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfbBc4K7pr1m",
        "colab_type": "text"
      },
      "source": [
        "# **Cost (Loss) Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crFQ0HvkFi--",
        "colab_type": "text"
      },
      "source": [
        "## **Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HipzMNFIFmwl",
        "colab_type": "text"
      },
      "source": [
        "## **Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L9Rza0hlnNE",
        "colab_type": "text"
      },
      "source": [
        "**Loss Minimization**\n",
        "\n",
        "$\\min _{W}\\left\\{L(W):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(W ; x_{i}, y_{i}\\right)+\\lambda r(W)\\right\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LliBhSvJlpaC",
        "colab_type": "text"
      },
      "source": [
        "**Mean Absolute Error**\n",
        "\n",
        "* Computes the mean of absolute difference between labels and predictions.\n",
        "\n",
        "**Kullback-Leibler divergence loss**\n",
        "\n",
        "* xxx\n",
        "\n",
        "**Sparse Categorical Crossentropy**\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgzQ1AKElykW",
        "colab_type": "text"
      },
      "source": [
        "## **Run an Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "346aSPWLl99u",
        "colab_type": "text"
      },
      "source": [
        "**Select Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHRINrb9l0wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = 'sparse_categorical_crossentropy'\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# loss = 'mae'\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02dLLEH3l86z",
        "colab_type": "text"
      },
      "source": [
        "**Define Model & Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMKezOIrl3hT",
        "colab_type": "code",
        "outputId": "da50b38a-61c3-4c55-dc57-200747d10b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', \n",
        "              loss=loss, \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test, y_test), \n",
        "#          callbacks=[PlotLossesKeras()]\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4953 - accuracy: 0.8217 - val_loss: 0.4219 - val_accuracy: 0.8443\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 5ms/step - loss: 0.3841 - accuracy: 0.8592 - val_loss: 0.3892 - val_accuracy: 0.8600\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3485 - accuracy: 0.8712 - val_loss: 0.3700 - val_accuracy: 0.8673\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3277 - accuracy: 0.8795 - val_loss: 0.3589 - val_accuracy: 0.8682\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3120 - accuracy: 0.8848 - val_loss: 0.3592 - val_accuracy: 0.8732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa432635748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsP-maXp4TL8",
        "colab_type": "text"
      },
      "source": [
        "# **Regularization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlaD0R2A_Xm_",
        "colab_type": "text"
      },
      "source": [
        "## **Overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kofQf1Yi_b6h",
        "colab_type": "text"
      },
      "source": [
        "* A fundamental problem in machine learning is the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations. Overfitting is when you have a complicated model that gives worse predictions than a simpler model.\n",
        "* Regularization is a technique for preventing a model from overfitting (e.g. preventing over-fitting by penalizing a model for having large weights)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEDOi0Mf_gkA",
        "colab_type": "text"
      },
      "source": [
        "**Regularization techniques against overfitting**\n",
        "\n",
        "* Add more data,\n",
        "* Vectornorm (L1, L2, Elastic Net) $^{1}$\n",
        "* Dropout, \n",
        "* Jitter (add noise),\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data), \n",
        "* Ensemble models, \n",
        "* Batch size (Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process) $^{2}$ \n",
        "* early stopping (this is not a formal regularization method, but can effectively limit overfitting). \n",
        "\n",
        "$^{1}$ *Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.*\n",
        "\n",
        "$^{2}$\n",
        "*Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUp8zJmV_k6G",
        "colab_type": "text"
      },
      "source": [
        "**Overfitting: Variance-Bias-Tradeoff**\n",
        "\n",
        "Generally, we refer to this model as having a large variance and a small bias. That is, the model is sensitive to the specific examples, the statistical noise, in the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_vXArve_o4V",
        "colab_type": "text"
      },
      "source": [
        "![Bias Variance Tradeoff](https://raw.githubusercontent.com/deltorobarba/repo/master/bias-and-variance.png)\n",
        "\n",
        "Source: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff\n",
        "](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttcXa-1n_uOR",
        "colab_type": "text"
      },
      "source": [
        "**Benefits of regularization from a mathematical optimization point of view**\n",
        "\n",
        "* Machine learning is an optimization problem, where we try to minimize a cost function to find optimal values for our model's parameter. Some machine learning models, like neural networks, have non-convex cost functions. Stationary points in these cost functions are problematic because numerical optimization schemes (like gradient descent) can easily get stuck, leading to poor results.\n",
        "* Regularization can be used as a way of ‚convexifying‘ a non-convex cost function. The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ-aw2an_xYB",
        "colab_type": "text"
      },
      "source": [
        "**Overfitting or Overtraining**\n",
        "\n",
        "* [Mehmet Suzen](https://www.linkedin.com/in/mehmetsuzen/): Regularisation does not prevent overfitting or even reduces. Regularisation originally developed for reducing ill-conditioning in inverse-problems. Regularisation, along with early-stopping, cross-validation and drop out, reduces and provides a reliable measure for generalisation error. Overfitting, on the other hand, is about the 'fit' , i.e., the model complexity. In deep nets, model complexity correlates with the full architecture and the activation functions. This is called 'overtraining' ([IEEE](https://ieeexplore.ieee.org/document/623200)).\n",
        "* Answer: if let it be, the learning process \"will tend to learn more and more complex functions as the number of iterations increases\". A model represented by a more complex function, thus having poor generalization, is an overfitting model. From the statement above, such a model can be prevented by stopping the learning early (among other techniques). Regularization is a process of applying those techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHw6PP5XFgXo",
        "colab_type": "text"
      },
      "source": [
        "## **Vectornorm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr-q4oEusOUa",
        "colab_type": "text"
      },
      "source": [
        "![Regularization Types](https://raw.githubusercontent.com/deltorobarba/repo/master/vectornorm.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5O1QrOXswPf",
        "colab_type": "text"
      },
      "source": [
        "Source: ['Getting started with Regression'](https://medium.com/@savannahar68/getting-started-with-regression-a39aca03b75f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75X3Z2teJNs-",
        "colab_type": "text"
      },
      "source": [
        "**Theoretical Foundation** \n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJBr4ZHljUM",
        "colab_type": "text"
      },
      "source": [
        "**Lambda Value (λ)**\n",
        "\n",
        "* Lambda is a regularization hyperparameter\n",
        "* Reasonable values of lambda range between 0 and 0.1\n",
        "* L2 weight regularization with very small regularization hyperparameters such as (e.g. 0.0005 or 5 x 10^−4) may be a good starting point\n",
        "* Learn more: [Google Course: Regularization for Simplicity: Lambda](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlN8ilZEqQj7",
        "colab_type": "text"
      },
      "source": [
        "**L1 Regularization**\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn’t easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**: if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero. very useful when you want to understand exactly which features are contributing to a decision. if you can ignore the ouliers in your dataset or you need them to be there. use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features  at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWJgeJqB4gy",
        "colab_type": "text"
      },
      "source": [
        "**Add L1 (Lasso) Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZuDNlbKlWu4",
        "colab_type": "text"
      },
      "source": [
        "**L2 Regularization**\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply “weight decay,” with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn’t give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge. \n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYzCd1FUBnjV",
        "colab_type": "text"
      },
      "source": [
        "**Add L2 (Ridge) Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8UW9XnleoV",
        "colab_type": "text"
      },
      "source": [
        "**Elastic Net**\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed λ2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+λ2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foOVciWvULS",
        "colab_type": "text"
      },
      "source": [
        "**Special: Analytical Equivalence**\n",
        "\n",
        "Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "\n",
        "https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knt6wCob4xfj",
        "colab_type": "text"
      },
      "source": [
        "**Weight Regularization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvnac8AA41ac",
        "colab_type": "text"
      },
      "source": [
        "* Weight regularization was borrowed from penalized regression models in statistics. Neural networks learn a set of weights that best map inputs to outputs. \n",
        "* A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data. \n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCXiItcr8fBF",
        "colab_type": "text"
      },
      "source": [
        "## **Dropout**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O51DzciO8gY4",
        "colab_type": "text"
      },
      "source": [
        "**What is it and how it works?**\n",
        "\n",
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(künstliches_neuronales_Netz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9kfZ8__8lq6",
        "colab_type": "text"
      },
      "source": [
        "**Is dropout outdated?**\n",
        "\n",
        "Neural Network:  Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don’t Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Zhb5RT8q-i",
        "colab_type": "text"
      },
      "source": [
        "**Use Dropout along with L1/L2 Regularization?**\n",
        "\n",
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ℓ 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of “switching off” some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network’s over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can “work together” or rather end up “fighting each other”. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "## **Run an Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gKj7gjwQtw",
        "colab_type": "text"
      },
      "source": [
        "**Kernel Regularizer**\n",
        "\n",
        "*Regularizer function applied to the kernel weights matrix.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRsFyXXiwXYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKn6U3WTwVRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yVhv_57wbN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJSTvHowfzn",
        "colab_type": "text"
      },
      "source": [
        "**Bias Regularizer**\n",
        "\n",
        "*Regularizer function applied to the bias vector*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7QXjYfzwsgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHyBz0fwwvU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NrlNShswyvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDXotzf7wk4o",
        "colab_type": "text"
      },
      "source": [
        "**Activity Regularizer**\n",
        "\n",
        "*Regularizer function applied to the output of the layer (its \"activation\")*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO2jgyv5w30S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=tf.keras.regularizers.l1(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXWwwJ5vw8HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=tf.keras.regularizers.l2(l=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFwslg_Bw_9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activity_regularizer=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhvypXgZ_-44",
        "colab_type": "text"
      },
      "source": [
        "**Select Dropout Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkuqSCGXADpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "**Define Model & Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDEv6ZETl129",
        "colab_type": "code",
        "outputId": "bb7974ae-da65-489a-fa8e-1200136fdc7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu', \n",
        "                                kernel_regularizer=kernel_regularizer, \n",
        "                                bias_regularizer=bias_regularizer, \n",
        "                                activity_regularizer=activity_regularizer))\n",
        "model.add(tf.keras.layers.Dropout(dropout))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test, y_test), \n",
        "#          callbacks=[PlotLossesKeras()]\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 1.4673 - accuracy: 0.7791 - val_loss: 0.9737 - val_accuracy: 0.7863\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8910 - accuracy: 0.8081 - val_loss: 0.8340 - val_accuracy: 0.8255\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8322 - accuracy: 0.8158 - val_loss: 0.7904 - val_accuracy: 0.8317\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8076 - accuracy: 0.8211 - val_loss: 0.7757 - val_accuracy: 0.8313\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7898 - accuracy: 0.8237 - val_loss: 0.7681 - val_accuracy: 0.8314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9af7584048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6vxhurCxHL",
        "colab_type": "text"
      },
      "source": [
        "# **Activation Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqYvBaH5CzyR",
        "colab_type": "text"
      },
      "source": [
        "## **Overview**\n",
        "\n",
        "![Activation Function](https://raw.githubusercontent.com/deltorobarba/repo/master/activation.png)\n",
        "\n",
        "* The activation function is the non-linear function that we apply over the output data coming out of a particular layer of neurons before it propagates as the input to the next layer.\n",
        "* Activation functions reside within neurons and transform input values into acceptable and useful range. They can introduce non-linearity to a network.\n",
        "* There are various kinds of activation functions and it has been found, empirically, that some of them works better for large datasets or particular problems than the others. \n",
        "* Neural networks extract hidden pattern from a dataset by observing given examples of known answers. Evidently, it does so by comparing its predictions to the ground truth (labeled images for example) and turning the parameters of the model. The difference between the prediction and the ground truth is called the ‘classification error’.\n",
        "* Parameters of a DL model consists of a set of weights connecting neurons across layers and bias terms which add to those layers. So, the ultimate goal is to set those weights to specific values which reduces the overall classification error. This is a minimization operation, and consequently, an optimization technique is needed.\n",
        "* The overall representation structure of a deep learning model is a highly complex nonlinear function and therefore, the optimizer is responsible for minimizing the error produced by the evaluation of this complex function. Therefore, standard optimization like linear programming does not work for DL models and innovative nonlinear optimization must be used.\n",
        "* These two components – **activation functions** and **nonlinear optimizers** – are at the core of every deep learning architecture. However, there is considerable variety in the specifics of these components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1XeaYadEqRK",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42XS4rQqEl6M",
        "colab_type": "text"
      },
      "source": [
        "**Necessary Characteristics of Activation Function**\n",
        "\n",
        "Activation functions must be:\n",
        "\n",
        "1. Non-constant (obvious)\n",
        "2. Bounded\n",
        "3. Monotonically increasing\n",
        "4. Continuous\n",
        "\n",
        "These are the conditions under which the universal approximation theorem holds. The universal approximation theorem proves that, under the above conditions, any continuous function of N-variables defined on a compact subset of R^N can be approximated by a three- layer (input, hidden layer, output) neural network with that activation function.\n",
        "\n",
        "The universal approximation theorem is certainly one of the most rigorous tenets of neural networks.\n",
        "\n",
        "Of course, if the prediction problem at hand does not deal with continuous variables or cannot be approximated by a problem that does, then the above is no longer valid and the choice of activation functions becomes more of a customized problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxM-2W5KEoJR",
        "colab_type": "text"
      },
      "source": [
        "**Selection Criteria**\n",
        "\n",
        "Activation layers are a type of hyperparameter, and you’ll need to experiment with all of them in order to find which works best for you. You can narrow your search by referring to prior work in the field for your particular problem. For example, it has already been shown that tanh activations work better for image classification while leaky ReLUs work better for temporal sequences such as video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXEabIWIEsx2",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_jDw6JnEw9B",
        "colab_type": "text"
      },
      "source": [
        "Sources: [Stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning) & [Deep Dive into Math Behind Deep Networks](https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvP81b2KC6py",
        "colab_type": "text"
      },
      "source": [
        "## **Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMZjfgrwEWaq",
        "colab_type": "text"
      },
      "source": [
        "**Sigmoid Family**\n",
        "\n",
        "Im Allgemeinen ist eine Sigmoidfunktion eine beschränkte und differenzierbare reelle Funktion mit einer durchweg positiven oder durchweg negativen ersten Ableitung und genau einem Wendepunkt.\n",
        "\n",
        "Außer der logistischen Funktion enthält die Menge der Sigmoidfunktionen den Arkustangens, den Tangens hyperbolicus und die Fehlerfunktion, die sämtlich transzendent sind, aber auch einfache algebraische Funktionen. \n",
        "\n",
        "Das Integral jeder stetigen, positiven Funktion mit einem „Berg“ (genauer: mit genau einem lokalen Maximum und keinem lokalen Minimum, z. B. die gaußsche Glockenkurve) ist ebenfalls eine Sigmoidfunktion. Daher sind viele kumulierte Verteilungsfunktionen sigmoidal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtoAvnU9EfEg",
        "colab_type": "text"
      },
      "source": [
        "![Sigmoidfunktionen](https://raw.githubusercontent.com/deltorobarba/repo/master/sigmoidfunktionen.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kNO9D9ELB_",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression ('Sigmoid')**\n",
        "\n",
        "$g(z)=\\frac{1}{1+e^{-z}}$\n",
        "\n",
        "<br> \n",
        "**Characteristics**\n",
        "* Logistic regression. Takes a real-valued number as an input and compresses all its outputs to the range of [0,1]. Sigmoid only for binary classification output layer.\n",
        "* Sigmoid activation derived from mean field solution of Boltzmann machine\n",
        "* Softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
        "\n",
        "**Advantages**\n",
        "* In the logistic function, a small change in the input only causes a small change in the output as opposed to the stepped output. Hence, the output is smoother than the step function output.\n",
        "* Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.\n",
        "* The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
        "* Especially used for models where we have to predict the probability as an output (of a binary problem).\n",
        "* Sigmoid works well for a classifier: approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence\n",
        "\n",
        "**Disadvantages**\n",
        "* exp() is a bit compute expensive. Learning time longer. Also other functions have been shown to produce the same performance with less iterations. Additionally: small local gradients can mute the gradient and disallow the forward propagation of a useful signal.\n",
        "* The sigmoid function is monotonic but function’s derivative is not: the tails of the first derivative of a Sigmoid are near zero (covariate shift), which lead to vanishing or exploding gradient. Incorrect weight initialization can lead to saturation, where most neurons of the network then become saturated and almost no learning will take place. Saturated neurons “kill” the gradients (look at x= -10, 0 and 10). Can cause the neural network to get stuck during training. If a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since neural networks usethe feed-forward activations to calculate parameter gradients, this can result in model parameters that are updated less regularly than we would like, and are thus “stuck” in their current state (this problem can be solved if we normalize the data in advance to be zero-centered as in batch/layer normalization).\n",
        "* Sigmoid outputs are not zero-centered. Neurons in later layers of processing in a neural net would be receiving data that is not zero-centered. If data coming into is always positive, the gradient on the weights 𝑤 will during backpropagation become either all be positive, or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. (However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem.)\n",
        "\n",
        "*Example: there are two parameter w1 and w 2; f the gradients of two dimensions are always of the same sign, it means we can only move roughly in the direction of northeast or southwest in the parameter space. If our goal happens to be in the northeast, we can only move in a zig-zagging fashion to get there, just like parallel parking in a narrow space.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7UIRXc7EQ2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={'figure.figsize':(7, 4)})\n",
        "\n",
        "# Increasing weight size or input scale will form function steeper\n",
        "weight_size = 1\n",
        "input_scale_upper = 10\n",
        "input_scale_lower = -10\n",
        "resolution = 100\n",
        "\n",
        "x = np.linspace(input_scale_lower, input_scale_upper, resolution) \n",
        "z = 1/(1 + np.exp(-(weight_size)*x)) \n",
        "  \n",
        "plt.plot(x, z), plt.xlabel(\"x\"), plt.ylabel(\"Sigmoid(X)\") \n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l94l0iDnDz2j",
        "colab_type": "text"
      },
      "source": [
        "**tanh ((hyperbolic tangent)**\n",
        "\n",
        "$g(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$\n",
        "\n",
        "* LeCun et al., 1991\n",
        "* The tanh function \"squashes\" values to the range -1 and 1. Output values are, therefore, centered around zero. Can be thought of as a scaled, or shifted, sigmoid, and is almost always preferable to the sigmoid function\n",
        "* Squashes numbers to range [-1,1]\n",
        "* zero centered (nice)\n",
        "* The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
        "* The function is differentiable.\n",
        "* The function is monotonic while its derivative is not monotonic.\n",
        "* The tanh function is mainly used classification between two classes.\n",
        "* still kills gradients when saturated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUUN_71gD5oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-10, 10, 100) \n",
        "z = (np.tanh(x)) \n",
        "  \n",
        "plt.plot(x, z), plt.xlabel(\"x\"), plt.ylabel(\"tanh\") \n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ocOJ2aDpLW",
        "colab_type": "text"
      },
      "source": [
        "**Softmax**\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers.\n",
        "* usually used in the last layer\n",
        "* Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) \n",
        "* is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We use the (standard) Logistic Regression model in binary classification tasks. in softmax regression (SMR), we replace the sigmoid logistic function by the so-called€softmax function€φ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB5f2FhkDr1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-5, 5, 0.1)\n",
        "plt.plot(np.exp(x) / np.sum(np.exp(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQlDy7wZDeny",
        "colab_type": "text"
      },
      "source": [
        "**Custom Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4zBuToxDgmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weibull - Cumulative Distribution Function\n",
        "sns.set(rc={'figure.figsize':(4, 4)})\n",
        "x = np.linspace(0, 25, 10000) \n",
        "shape = 5 # k\n",
        "scale = 1 # λ\n",
        "# Simple: z = 1 - (np.exp(-(x/λ)**k))\n",
        "z = scipy.stats.weibull_min.cdf(x, c=shape, scale=scale)\n",
        "\n",
        "plt.title('Weibull Distribution')\n",
        "plt.plot(x, z)\n",
        "plt.show(xlim(0,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoqFG0akDh-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rayleigh Distribution\n",
        "sns.set(rc={'figure.figsize':(4, 4)})\n",
        "x = np.linspace(0, 25, 10000) \n",
        "σ = 0.3\n",
        "z = 1 - (np.exp((-x**2)/(2*σ**2)))\n",
        "\n",
        "plt.plot(x, z)\n",
        "# plt.title('Rayleigh Distribution')\n",
        "plt.show(xlim(0,2)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH68QMWSDkK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution fitting against Rayleigh\n",
        "\n",
        "from scipy.stats import norm,rayleigh\n",
        "samp = rayleigh.rvs(loc=5,scale=2,size=150) # samples generation\n",
        "param = rayleigh.fit(samp) # distribution fitting\n",
        "\n",
        "x = linspace(5,13,100)\n",
        "# fitted distribution\n",
        "pdf_fitted = rayleigh.pdf(x,loc=param[0],scale=param[1])\n",
        "# original distribution\n",
        "pdf = rayleigh.pdf(x,loc=5,scale=2)\n",
        "\n",
        "title('Rayleigh distribution')\n",
        "plot(x,pdf_fitted,'r-',x,pdf,'b-')\n",
        "hist(samp,normed=1,alpha=.3)\n",
        "show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHA4RDyNECpd",
        "colab_type": "text"
      },
      "source": [
        "**Rectifier Functions Family**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
        "\n",
        "**Advantages**\n",
        "* Biological plausibility: One-sided, compared to the antisymmetry of tanh.\n",
        "* Sparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (having a non-zero output).\n",
        "* Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\n",
        "* Efficient computation: Only comparison, addition and multiplication.\n",
        "* Scale-invariant: \n",
        "\n",
        "\n",
        "**Disadvantages**\n",
        "* Non-differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\n",
        "* Not zero-centered.\n",
        "* Unbounded.\n",
        "* Dying ReLU problem: ReLU neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state and \"dies\". This is a form of the vanishing gradient problem. In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead, which assign a small positive slope for x < 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFqnnWMTDTnC",
        "colab_type": "text"
      },
      "source": [
        "**ReLU (Rectified Linear Unit)**\n",
        "\n",
        "$g(z)=\\max (0, z)$\n",
        "\n",
        "<br>\n",
        "\n",
        "Transformation leads positive values to be 1, and negative values to be zero. Shown to accelerate convergence of gradient descent compared to above functions. Can lead to neuron death, which can be combated using Leaky ReLU modification (see [1]). ReLU is has become the default activation function for hidden layers (see [3])\n",
        "\n",
        "**Characteristics**\n",
        "* Krizhevsky et al., 2012\n",
        "* rectified linear units, faster and more efficient, since fewer neurons are activated (less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations). \n",
        "* No gradient vanishing problem, as Relu’s gradient is constant = 1. Sparsity: since output 0 for negative values of x! When W*x < 0, Relu gives 0, which means sparsity. Less calculation load. This may be least important. \n",
        "* However, ReLu may amplify the signal inside the network more than softmax and sigmoid. \n",
        "* But: dying ReLU problem for values zero and smaller: neurons will never reactivated. Solution: leaky ReLU, noisy ReLU (in RBMs) and ELU (exponential linear units)\n",
        "* ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.\n",
        "\n",
        "**Advantages**\n",
        "* Does not saturate (in +region)\n",
        "* Very computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
        "Actually more biologically plausible than sigmoid\n",
        "\n",
        "**Disadvantages**\n",
        "* Not zero-centered output\n",
        "* An annoyance: what is the gradient when x < 0? What happens when x = -10, 0 or 10?\n",
        "* People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJn-LujVDaJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-10, 10, 100) \n",
        "z = (np.maximum(0, x))\n",
        "  \n",
        "plt.plot(x, z), plt.xlabel(\"x\"), plt.ylabel(\"ReLU\") \n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=1.0);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlhvbqgfDKF6",
        "colab_type": "text"
      },
      "source": [
        "**Leaky ReLU**\n",
        "\n",
        "$\\begin{aligned}\n",
        "g(z) &=\\max (\\epsilon z, z) \\\\\n",
        "& \\text { with } \\epsilon \\ll 1\n",
        "\\end{aligned}$\n",
        "\n",
        "* Mass et al., 2013 and He et al., 2015\n",
        "* Leaky ReLUs allow a small, positive gradient when the unit is not active\n",
        "* Does not saturate\n",
        "* Computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice! (e.g. 6x) will not “die”.z = np.arange(-55, 5, 1)\n",
        "plt.plot(np.maximum(0.01 * z, z))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBGoAOFUDN8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = np.arange(-55, 5, 1)\n",
        "plt.plot(np.maximum(0.01 * z, z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXS-DDJfDG9V",
        "colab_type": "text"
      },
      "source": [
        "**ELU**\n",
        "\n",
        "* Exponential Linear Units\n",
        "* Clevert et al., 2015\n",
        "* All benefits of ReLU\n",
        "* Closer to zero mean outputs\n",
        "* Negative saturation regime compared with Leaky ReLU adds some robustness to noise \n",
        "* But Computation requires exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqjjnsXVC_jF",
        "colab_type": "text"
      },
      "source": [
        "**Swish**\n",
        "\n",
        "* Google Brain 2017\n",
        "* Variant of ReLU\n",
        "\n",
        "https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7\n",
        "\n",
        "https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHvFihVSDDHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "plt.xlabel('x'), plt.ylabel('f(x)')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=1.0);\n",
        "beta = 1.0\n",
        "# plt.plot(x * (1 / (1 + np.exp(-x))))\n",
        "plt.plot(x * ((1 / (1 + np.exp(-x))) * (beta * x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otcAxDatC8EY",
        "colab_type": "text"
      },
      "source": [
        "**SeLU**\n",
        "\n",
        "* scaled exponential linear units\n",
        "* instead of normalizing the output of the activation function — the activation function suggested (SELU — scaled exponential linear units) outputs normalized values. https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "* Background: batchnormalization for feedfirward networks: Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. (https://arxiv.org/abs/1502.03167)\n",
        "* Negative values sometimes: Scaling the function is the mechanism by which the authors accomplish the goal (of self-normalizing properties). As a byproduct, they sometimes output negative values, but there's no hidden meaning in it. It just makes the math work out. \n",
        "* **SELU vs RELU**: https://www.hardikp.com/2017/07/24/SELU-vs-RELU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT9x2aVPE93J",
        "colab_type": "text"
      },
      "source": [
        "## **Run an Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ezor8BFAaX",
        "colab_type": "text"
      },
      "source": [
        "* activation function als Dense parameter. Activation layer als eigener layer.\n",
        "* keras.activation = functions\n",
        "* keras.layers = layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ek7hOGSFCmK",
        "colab_type": "text"
      },
      "source": [
        "**Select one activation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3tL9-qQFEiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = tf.keras.activations.tanh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zol8HBRrFF6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = tf.keras.activations.sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59uAyPyxFHTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = tf.keras.layers.ReLU(max_value=None,\n",
        "                                 negative_slope=0,\n",
        "                                 threshold=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di9tHn1eFJiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = tf.keras.layers.LeakyReLU(alpha=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrxLGckiFLbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = tf.keras.layers.Softmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfFfY5icFNBM",
        "colab_type": "text"
      },
      "source": [
        "**Alternatively Add Default Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOXFZMJ-FOqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = 'relu'\n",
        "# activation = 'linear'\n",
        "# activation = 'sigmoid'\n",
        "# activation = 'tanh'\n",
        "# activation = 'softmax'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiztpDMoFQK-",
        "colab_type": "text"
      },
      "source": [
        "**Define Model and Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FoYSx7IFSdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation=activation))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train, \n",
        "          y=y_train, \n",
        "          epochs=5, \n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}