{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lstm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "pa49bUnKyRgF",
        "first-twist",
        "3FWqa48wm7RE",
        "agdlQRdamBX1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# **RNN Multivariate Forecasting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU8C5qm_4vZb"
      },
      "source": [
        "This tutorial is an introduction to time series forecasting using Recurrent Neural Networks (RNNs). This is covered in two parts: first, you will forecast a univariate time series, then you will forecast a multivariate time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOzXNxQ7ZyMX"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rZnJaGTWQw0"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TokBlnUhWFw9"
      },
      "source": [
        "## Import Dataset\n",
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">[weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>.\n",
        "\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by Fran√ßois Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyv_i85IWInT"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6uGeeeWIkG"
      },
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdbOWXiTWM2T"
      },
      "source": [
        "Let's take a glance at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojHE-iCCWIhz"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfbpcV0MWQzl"
      },
      "source": [
        "As you can see above, an observation is recorded every 10 mintues. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations. \n",
        "\n",
        "Given a specific time, let's say you want to predict the temperature 6 hours in the future. In order to make this prediction, you choose to use 5 days of observations. Thus, you would create a window containing the last 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with.\n",
        "\n",
        "The function below returns the above described windows of time for the model to train on. The parameter `history_size` is the size of the past window of information. The `target_size` is how far in the future does the model need to learn to predict. The `target_size` is the label that needs to be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AoxQuTrWIbi"
      },
      "source": [
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoFJZmXBaxCc"
      },
      "source": [
        "In both the following tutorials, the first 300,000 rows of the data will be the training dataset, and there remaining will be the validation dataset. This amounts to ~2100 days worth of training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia-MPAHxbInX"
      },
      "source": [
        "TRAIN_SPLIT = 300000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EowWDtaNnH1y"
      },
      "source": [
        "Setting seed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x-GgENynHdx"
      },
      "source": [
        "tf.random.set_seed(13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlJYi3_HXcw8"
      },
      "source": [
        "## Forecast a multivariate time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxNZ2GM7DPm"
      },
      "source": [
        "The original dataset contains fourteen features. For simplicity, this section considers only three of the original fourteen. The features used are air temperature, atmospheric pressure, and air density. \n",
        "\n",
        "To use more features, add their names to this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DphrB7bxSNDd"
      },
      "source": [
        "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQUSiJfUpXJ"
      },
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['Date Time']\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfhTZi5r15R"
      },
      "source": [
        "Let's have a look at how each of these features vary across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdgC8zvGr21X"
      },
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqStgZ-O1b3_"
      },
      "source": [
        "As mentioned, the first step will be to standardize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VuNIwfHRHx"
      },
      "source": [
        "dataset = features.values\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJUeWDqploCt"
      },
      "source": [
        "dataset = (dataset-data_mean)/data_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyuGuJUgjUK3"
      },
      "source": [
        "#### Single step model\n",
        "In a single step setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as below, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-rVX4d3OF86"
      },
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i, step)\n",
        "    data.append(dataset[indices])\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWVGYwbN2ITI"
      },
      "source": [
        "In this tutorial, the network is shown data from the last five (5) days, i.e. 720 observations that are sampled every hour. The sampling is done every one hour since a drastic change is not expected within 60 minutes. Thus, 120 observation represent history of the last five days.  For the single step prediction model, the label for a datapoint is the temperature 12 hours into the future. In order to create a label for this, the temperature after 72(12*6) observations is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlhVGzPhmMYI"
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CamMObrwPhnp"
      },
      "source": [
        "Let's look at a single data-point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tVKm-ZIPls0"
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCWG4xgQ3O6E"
      },
      "source": [
        "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
        "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
        "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aWec9_nlxBl"
      },
      "source": [
        "single_step_model = tf.keras.models.Sequential()\n",
        "single_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                           input_shape=x_train_single.shape[-2:]))\n",
        "single_step_model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYhUfWjwOPFN"
      },
      "source": [
        "Let's check out a sample prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY7FodHVOPsH"
      },
      "source": [
        "for x, y in val_data_single.take(1):\n",
        "  print(single_step_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0jnt2l2mwkl"
      },
      "source": [
        "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            validation_data=val_data_single,\n",
        "                                            validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZAdeAnP5c72"
      },
      "source": [
        "def plot_train_history(history, title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lBKA-z5yYV"
      },
      "source": [
        "plot_train_history(single_step_history,\n",
        "                   'Single Step Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfjrGAlEUp7i"
      },
      "source": [
        "##### Predict a single step future\n",
        "Now that the model is trained, let's make a few sample predictions. The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1qmPLLVUpuN"
      },
      "source": [
        "for x, y in val_data_single.take(3):\n",
        "  plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
        "                    single_step_model.predict(x)[0]], 12,\n",
        "                   'Single Step Prediction')\n",
        "  plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GnE087bJYSu"
      },
      "source": [
        "#### Multi-Step model\n",
        "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predict a sequence of the future.\n",
        "\n",
        "For the multi-step model, the training data again consists of recordings over the past five days sampled every hour. However, here, the model needs to learn to predict the temperature for the next 12 hours. Since an obversation is taken every 10 minutes, the output is 72 predictions. For this task, the dataset needs to be prepared accordingly, thus the first step is just to create it again, but with a different target window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCk9fqyJZqX"
      },
      "source": [
        "future_target = 72\n",
        "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImXPwAGRtWy"
      },
      "source": [
        "Let's check out a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpWDcBkQRwS-"
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
        "print ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjR4PJArMOpA"
      },
      "source": [
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZcg8FWpSG8K"
      },
      "source": [
        "Plotting a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXKVbwBV7D3"
      },
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQKetflZRMF"
      },
      "source": [
        "In this plot and subsequent similar plots, the history and the future data are sampled every hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6G8bacQR4w2"
      },
      "source": [
        "for x, y in train_data_multi.take(1):\n",
        "  multi_step_plot(x[0], y[0], np.array([0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOjz8DzZ4HFS"
      },
      "source": [
        "Since the task here is a bit more complicated than the previous task, the model now consists of two LSTM layers. Finally, since 72 predictions are made, the dense layer outputs 72 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byAl0NKSNBP6"
      },
      "source": [
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=x_train_multi.shape[-2:]))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.Dense(72))\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvB7zBqVSMyl"
      },
      "source": [
        "Let's see how the model predicts before it trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13_ZWvB9SRlZ"
      },
      "source": [
        "for x, y in val_data_multi.take(1):\n",
        "  print (multi_step_model.predict(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uwOhXo3Oems"
      },
      "source": [
        "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfQoBjQ5l7U"
      },
      "source": [
        "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDg94-yq4pas"
      },
      "source": [
        "##### Predict a multi-step future\n",
        "Let's now have a look at how well your network has learnt to predict the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt22wq6fyIBU"
      },
      "source": [
        "for x, y in val_data_multi.take(3):\n",
        "  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOzaIRYBhqwg"
      },
      "source": [
        "## Next steps\n",
        "This tutorial was a quick introduction to time series forecasting using an RNN. You may now try to predict the stock market and become a billionaire.\n",
        "\n",
        "In addition, you may also write a generator to yield data (instead of the uni/multivariate_data function), which would be more memory efficient. You may also check out this [time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing) guide and use it in this tutorial.\n",
        "\n",
        "For further understanding, you may read Chapter 15 of [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), 2nd Edition and Chapter 6 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "first-twist"
      },
      "source": [
        "# **LSTM Univariate Forecasting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKYS0huhUn9L"
      },
      "source": [
        "https://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG9uBIZ-Um12"
      },
      "source": [
        "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cardiac-conversion"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "print('Tensorflow version %s' % tf.__version__)\n",
        "\n",
        "!pip install --quiet ipython-autotime pandas_gbq\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "superb-abraham"
      },
      "source": [
        "#### **Load Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-uWiif4TOnj"
      },
      "source": [
        "**Sunspot Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt6KnxyBWHnO"
      },
      "source": [
        "https://blogs.rstudio.com/ai/posts/2018-06-25-sunspots-lstm/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olGxzFAzTVcW"
      },
      "source": [
        "# https://www.kaggle.com/robervalt/sunspots\n",
        "# https://www.kaggle.com/leeminwoo/keras-bidirectonal-lstm\n",
        "url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/sunspots.csv.xls'\n",
        "data = pd.read_csv(url)\n",
        "data = data[['Date', 'Monthly Mean Total Sunspot Number']].rename(columns={\"Date\": \"date\", \"Monthly Mean Total Sunspot Number\": \"sunspots\"}).set_index('date')\n",
        "data.index = pd.to_datetime(data.index, utc=False)\n",
        "dataset = data.copy()\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC6pub6mRWKB"
      },
      "source": [
        "**Airline Passengers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTSJmbbCRYqq"
      },
      "source": [
        "#url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/airline-passengers.csv'\n",
        "#data = pd.read_csv(url)\n",
        "#data = data[['Month', 'Passengers']].rename(columns={\"Month\": \"date\", \"Passengers\": \"passengers\"}).set_index('date')\n",
        "#data.index = pd.to_datetime(data.index, utc=False)\n",
        "#dataset = data.copy()\n",
        "#dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6rl09rTL26"
      },
      "source": [
        "**Google Stock Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1j1iRkNPqO"
      },
      "source": [
        "# url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/google.csv'\n",
        "# data = pd.read_csv(url)\n",
        "# series = data[['Date', 'Close']].rename(columns={\"Date\": \"date\", \"Close\": \"values\"}).set_index('date')\n",
        "# series.index = pd.to_datetime(series.index, utc=False)\n",
        "# dataset = series.copy()\n",
        "# dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AFUiW7vMuSW"
      },
      "source": [
        "Diverse stock data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suYGWWf4Mlqi"
      },
      "source": [
        "#url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/timeseries.csv'\n",
        "#data = pd.read_csv(url)\n",
        "#series = data[['date', 'price1', 'price2']].rename(columns={\"price1\": \"google\", \"price2\": \"apple\"}).set_index('date')\n",
        "#series.index = pd.to_datetime(series.index, utc=False)\n",
        "#dataset = series.copy()\n",
        "#dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeENmezNTRWv"
      },
      "source": [
        "**Insolvenzverfahren**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ideuB7R3E9sS"
      },
      "source": [
        "# url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/sh_unmelt_bq_raw.csv'\n",
        "# sh_unmelt_bq_raw = pd.read_csv(url)\n",
        "# sh_unmelt_bq_raw = sh_unmelt_bq_raw.sort_values('datum')\n",
        "# dataset = sh_unmelt_bq_raw.copy()\n",
        "# dataset = dataset.drop(columns=['Bergbau', 'Dienstleistungen', 'Energieversorgung','Erziehung', 'Finanzleistungen', 'Gastgewerbe','IT', 'Manufaktur', 'Sonstiges','Sozialwesen', 'Verkehr', 'Wohnungswesen'])\n",
        "# dataset = dataset.set_index('datum')\n",
        "# dataset.index = pd.to_datetime(dataset.index, utc=False)\n",
        "# dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "german-hometown"
      },
      "source": [
        "#### **Set input data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "willing-yahoo"
      },
      "source": [
        "# Set data\n",
        "data = dataset.copy()\n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "marked-hormone"
      },
      "source": [
        "#### **Normalize input data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4BejHq6KB4P"
      },
      "source": [
        "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attempted-arnold"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "data = scaler.fit_transform(data)\n",
        "# data = data[:-1, :] # for multivariate datasets: remove last row due to nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excellent-blowing"
      },
      "source": [
        "print(data.shape)\n",
        "# print(dataset.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lesser-correlation"
      },
      "source": [
        "# Display dataset\n",
        "data [0:10] # Display first 10 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-occasions"
      },
      "source": [
        "#### **Train & Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romantic-particle"
      },
      "source": [
        "# Train & Test Split in % of complete dataset\n",
        "split = 0.9\n",
        "train_size = int(len(data) * split)\n",
        "test_size = len(data) - train_size\n",
        "train, test = data[0:train_size,:], data[train_size:len(data),:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numerical-walnut"
      },
      "source": [
        "train [0:10] # Display first 10 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "static-badge"
      },
      "source": [
        "test [0:10] # Display first 10 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "applied-parallel"
      },
      "source": [
        "Now train & test data have been separated. \n",
        "\n",
        "We need to divide by feature input (X) and prediction variable (Y).\n",
        "\n",
        "For both train and test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protected-criterion"
      },
      "source": [
        "#### **Window & Reshape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compliant-robert"
      },
      "source": [
        "window = 12 # Use (t), (t-1) and (t-2) as input variables X\n",
        "features = 1 # we have only one time series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgRWhoigSGda"
      },
      "source": [
        "https://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "limited-airport"
      },
      "source": [
        "# Convert an array of values into a dataset matrix\n",
        "def create_dataset(data, window=window):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(data)-window-1):\n",
        "        a = data[i:(i+window), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(data[i + window, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# X is input data = value (at t, and t-1, t-2, depending on window size)\n",
        "# Y is target data = value at t+1\n",
        "# Each is additionally divided into train and test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numerical-invention"
      },
      "source": [
        "# Reshape into X=t and Y=t+1 and get as arrays\n",
        "trainX, trainY = create_dataset(train, window)\n",
        "testX, testY = create_dataset(test, window)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "allied-pavilion"
      },
      "source": [
        "print(trainX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "endless-chile"
      },
      "source": [
        "print(trainY.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "separated-bankruptcy"
      },
      "source": [
        "# X input has 3 (= window size) values at each step\n",
        "trainX [0:10] # Display first 10 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "colored-organization"
      },
      "source": [
        "# Y output has here only one value (t+1) at each step\n",
        "trainY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blank-sheep"
      },
      "source": [
        "trainX.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polished-ballet"
      },
      "source": [
        "trainX.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "right-keyboard"
      },
      "source": [
        "#### **Reshape Data Format for LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZKqyE6MF75"
      },
      "source": [
        "https://www.kaggle.com/shivajbd/input-and-output-shape-in-lstm-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "changing-hardware"
      },
      "source": [
        "# Reshape Data Format for LSTM (simple model)\n",
        "\n",
        "# Data is in shape **[samples, features]**. LSTM expects **[samples, time steps, features]**\n",
        "# Reminder 2D array: shape (n,m), n [0] = rows (time steps), m [1] = column(s)\n",
        "# Shape: First Block: t, t+1, t+2. Second: t+1, t+2, t+3. Third: t+2, t+3, t+4.\n",
        "# Approach 1: Use past observations as separate input features =(trainX.shape[0], features, trainX.shape[1])\n",
        "\n",
        "# trainX = np.reshape(trainX, (trainX.shape[0], features, trainX.shape[1]))\n",
        "# testX = np.reshape(testX, (testX.shape[0], features, testX.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdtgTdNmNuK8"
      },
      "source": [
        "trainX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atmospheric-camel"
      },
      "source": [
        "# Approach 2: Use past observations as time steps of the one input feature\n",
        "\n",
        "trainX = tf.reshape(trainX, (trainX.shape[0], trainX.shape[1], features))\n",
        "testX = tf.reshape(testX, (testX.shape[0], testX.shape[1], features))\n",
        "trainX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlikely-brunswick"
      },
      "source": [
        "print(trainX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precious-jesus"
      },
      "source": [
        "trainX [0:10] # Display first 10 values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equal-intervention"
      },
      "source": [
        "print(testX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bddK1FOuJWue"
      },
      "source": [
        "#### **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enabling-lease"
      },
      "source": [
        "# Set Hyperparameter\n",
        "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "loss = 'mean_squared_error' #tf.keras.losses.LogCosh() or 'huber'\n",
        "activation = 'relu'\n",
        "dropout = 0.002\n",
        "epochs = 15\n",
        "verbose = 1\n",
        "units = 1 # LSTM output units\n",
        "repeat = 1\n",
        "batch_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virgin-license"
      },
      "source": [
        "model = keras.Sequential(name=\"my_sequential\")\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "\n",
        "# Add a LSTM layer with internal units.\n",
        "model.add(layers.LSTM(units, \n",
        "                      input_shape=(window, features),\n",
        "                      dropout = dropout,\n",
        "                      return_sequences=False, \n",
        "                      activation=activation, \n",
        "                      name=\"layer1\"))\n",
        "\n",
        "# Add a Dense layer with 1 unit (das aktivieren, falls LSTM output units > 1)\n",
        "# model.add(layers.Dense(1, activation = 'relu', name=\"layer2\"))\n",
        "\n",
        "# Call model on a test input\n",
        "trainY = model(trainX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silver-pledge"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "burning-simple"
      },
      "source": [
        "trainY.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breeding-injection"
      },
      "source": [
        "# model.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naked-journalism"
      },
      "source": [
        "# print(\"Number of weights after calling the model:\", len(model.weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "invisible-canon"
      },
      "source": [
        "model.compile(loss=loss, \n",
        "              optimizer=optimizer, \n",
        "              metrics=['mean_squared_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "changing-pennsylvania"
      },
      "source": [
        "# Prepare visualisation - Plot Losses Keras TF (simple)\n",
        "!pip install livelossplot --quiet\n",
        "from livelossplot import PlotLossesKerasTF\n",
        "\n",
        "# Select visualization method\n",
        "callbacks = [PlotLossesKerasTF()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "original-navigator"
      },
      "source": [
        "# Train model\n",
        "model.fit(trainX,\n",
        "          trainY, \n",
        "          epochs=epochs, \n",
        "          verbose=verbose, \n",
        "          shuffle=False, \n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mighty-italic"
      },
      "source": [
        "#### **Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "universal-surface"
      },
      "source": [
        "trainPredict = model.predict(trainX)\n",
        "model.reset_states()\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scientific-joint"
      },
      "source": [
        "#### **Predictions (Train & Test)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varied-gross"
      },
      "source": [
        "**Test Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medieval-accident"
      },
      "source": [
        "testPredict [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "above-burns"
      },
      "source": [
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testPredict [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convinced-experience"
      },
      "source": [
        "**Train Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "superior-aquatic"
      },
      "source": [
        "trainPredict [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accredited-parts"
      },
      "source": [
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainPredict [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exact-immigration"
      },
      "source": [
        "#### **Actual (Train & Test)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "designed-narrow"
      },
      "source": [
        "**Test Actual**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apparent-evanescence"
      },
      "source": [
        "testY [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sexual-edmonton"
      },
      "source": [
        "testY = scaler.inverse_transform([testY])\n",
        "testY [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessible-wings"
      },
      "source": [
        "**Train Actual**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjustable-boating"
      },
      "source": [
        "trainY [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenging-carnival"
      },
      "source": [
        "# Convert / Create a numpy ndarray from a tensor\n",
        "trainY = tf.make_tensor_proto(trainY)  # convert `tensor a` to a proto tensor\n",
        "trainY = tf.make_ndarray(trainY)\n",
        "trainY [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mathematical-species"
      },
      "source": [
        "trainY.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ordinary-defendant"
      },
      "source": [
        "# trainY = scaler.inverse_transform([trainY])\n",
        "trainY [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "occupational-myanmar"
      },
      "source": [
        "#### **Residuals**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infrared-bouquet"
      },
      "source": [
        "# Get residuals\n",
        "residuals = (testY[0] - testPredict[:,0])\n",
        "res = pd.DataFrame(data=residuals, columns=['residuals'])\n",
        "res.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adequate-sheep"
      },
      "source": [
        "# Plot Residuals\n",
        "sns.set(rc={'figure.figsize':(15, 8)})\n",
        "res.plot(title='Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surface-happening"
      },
      "source": [
        "#### **Calculate RMSE & Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_TYoqSNdTJ8"
      },
      "source": [
        "# Inverse original data\n",
        "data = scaler.inverse_transform(data)\n",
        "data [0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prostate-harvard"
      },
      "source": [
        "# Plot results\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[window:len(trainPredict)+window, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(dataset)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(window*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "sns.set(rc={'figure.figsize':(11, 5)})\n",
        "#plt.plot(scaler.inverse_transform(dataset))\n",
        "plt.plot(data)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsfjjVQ6c3jh"
      },
      "source": [
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idsbc79AdDnN"
      },
      "source": [
        "# trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "# print('Train Score: %.2f RMSE' % (trainScore))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu2RPOeSIfJ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FWqa48wm7RE"
      },
      "source": [
        "# **LSTM Multivariate Forecasting (Normalized)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clz5dotBohqM"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4AbwKM9G3Ji"
      },
      "source": [
        "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Og2pP-9G4CL"
      },
      "source": [
        "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IKxXEJJm3Hd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "import seaborn as sns\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import concatenate\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "print('Tensorflow version %s' % tf.__version__)\n",
        "\n",
        "!pip install --quiet ipython-autotime pandas_gbq\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5PDFB27fjf"
      },
      "source": [
        "## **Preprocess Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iS9fgg6okfr"
      },
      "source": [
        "#### **Import & Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNndQzIHnnnh"
      },
      "source": [
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "# load data\n",
        "def parse(x):\n",
        "\treturn datetime.strptime(x, '%Y %m %d %H')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/deltorobarba/repo/master/pollution.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
        "dataset.drop('No', axis=1, inplace=True)\n",
        "# manually specify column names\n",
        "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
        "dataset.index.name = 'date'\n",
        "# mark all NA values with 0\n",
        "dataset['pollution'].fillna(0, inplace=True)\n",
        "# drop the first 24 hours\n",
        "dataset = dataset[24:]\n",
        "# summarize first 5 rows\n",
        "print(dataset.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lfcRC7cqTxj"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/timeseries.csv'\n",
        "data = pd.read_csv(url)\n",
        "series = data[['date', 'price1', 'price2']].rename(columns={\"price1\": \"google\", \"price2\": \"apple\"}).set_index('date')\n",
        "series.index = pd.to_datetime(series.index, utc=False)\n",
        "dataset = series.copy()\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLLOE7avooVd"
      },
      "source": [
        "#### **Visualize Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwYHvVZYoQX2"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(10, 8), \"lines.linewidth\": 1.0})\n",
        "\n",
        "# load dataset\n",
        "values = dataset.values\n",
        "# specify columns to plot\n",
        "groups = [0, 1]\n",
        "i = 1\n",
        "# plot each column\n",
        "pyplot.figure()\n",
        "for group in groups:\n",
        "\tpyplot.subplot(len(groups), 1, i)\n",
        "\tpyplot.plot(values[:, group])\n",
        "\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
        "\ti += 1\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMcvVI2voyED"
      },
      "source": [
        "#### **LSTM Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL9ouXuFu5YB"
      },
      "source": [
        "windows = 3\n",
        "features = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYZJnFpwo1we"
      },
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_RcTBBAr5Z_"
      },
      "source": [
        "#dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNgh3AGr7K9"
      },
      "source": [
        "# ensure all data is float\n",
        "values = values.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tmdg5kir9Pw"
      },
      "source": [
        "# normalize features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c0r9o9-r-9q"
      },
      "source": [
        "# frame as supervised learning\n",
        "reframed = series_to_supervised(scaled, windows, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPEcgIjj-J1d"
      },
      "source": [
        "# Display data\n",
        "reframed [:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peUy104owszp"
      },
      "source": [
        "# Show last column (to be removed)\n",
        "num_rows, num_cols = reframed.shape\n",
        "n = print (num_cols)\n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjwBk1SksAjw"
      },
      "source": [
        "# drop columns we don't want to predict\n",
        "n = 7 # number of last column (from above) minus 1 (the index column)\n",
        "reframed.drop(reframed.columns[[n]], axis=1, inplace=True)\n",
        "print(reframed.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGXlL5dRpCNs"
      },
      "source": [
        "#### **Shape for LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "humf4h6zyKTw"
      },
      "source": [
        "# how many rows (time stamps)?\n",
        "reframed.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mmYnhfgEnku"
      },
      "source": [
        "values = reframed.values\n",
        "train_time = 2000\n",
        "train = values[:train_time, :]\n",
        "test = values[train_time:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWZMJsYCyAPp"
      },
      "source": [
        "# split into input and outputs\n",
        "n_obs = windows * features\n",
        "train_X, train_y = train[:, :n_obs], train[:, -features]\n",
        "test_X, test_y = test[:, :n_obs], test[:, -features]\n",
        "print(train_X.shape, len(train_X), train_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXh_ebEMx-qx"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], windows, features))\n",
        "test_X = test_X.reshape((test_X.shape[0], windows, features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO75KO7bFRMW"
      },
      "source": [
        "print(test_X.shape, len(test_X), test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhnLYVOd9_g7"
      },
      "source": [
        "test_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhp3-pUzDQL"
      },
      "source": [
        "## **Model Fitting & Forecasting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVyFo2CU4joq"
      },
      "source": [
        "#### **Create & Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtC5Z3D8uKtP"
      },
      "source": [
        "# design network\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(10))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mae', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h70lwqUVuN3i"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEgZ8-NEpK_T"
      },
      "source": [
        "# Fit model\n",
        "history = model.fit(train_X, train_y, \n",
        "                    epochs=20, \n",
        "                    batch_size=72, \n",
        "                    validation_data=(test_X, test_y), \n",
        "                    verbose=1, \n",
        "                    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7docl58R0PCm"
      },
      "source": [
        "# plot history\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQEAzIJ2rec"
      },
      "source": [
        "#### **Make a prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR3nLuWlzXQf"
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)\n",
        "test_X = test_X.reshape((test_X.shape[0], windows*features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUud1pR30rzm"
      },
      "source": [
        "# Shape: 6 windows (3 timesteps for 2 features)\n",
        "test_X.shape[0], test_X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG3LedRK4LGI"
      },
      "source": [
        "test_X [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxvYcPzN3ERu"
      },
      "source": [
        "# Shape: one timestep forecasted for one feature\n",
        "yhat.shape[0], yhat.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5gcnz0G4Dwm"
      },
      "source": [
        "yhat [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP-YXG7CpUCM"
      },
      "source": [
        "## **Postprocess Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu0sv43q2uwX"
      },
      "source": [
        "#### **Invert Scaling for Forecasted Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "929bPPIu9nNl"
      },
      "source": [
        "# remove all other features to align matrix structure\n",
        "inv_yhat = concatenate((yhat, test_X[:, -1:]), axis=1)\n",
        "inv_yhat [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Rh-nPU4wjO"
      },
      "source": [
        "inv_yhat.shape[0], inv_yhat.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDs6im4U4mTg"
      },
      "source": [
        "inv_yhat = scaler.inverse_transform(inv_yhat)[:, [1]]\n",
        "inv_yhat [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueyV1uY7CMi4"
      },
      "source": [
        "inv_yhat = inv_yhat[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJgxsIuwCN7G"
      },
      "source": [
        "inv_yhat [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8NPH8u2w4-"
      },
      "source": [
        "#### **Invert Scaling for Actual Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEm0Uto65Scd"
      },
      "source": [
        "# Display target data\n",
        "test_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSxtQp9O1c67"
      },
      "source": [
        "# Reshape target data to concatenate later\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "test_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zP_aiw6lwI"
      },
      "source": [
        "# Display input data\n",
        "test_X [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9csZywC515Hj"
      },
      "source": [
        "# Concatenate input & target data\n",
        "### This is purely for demonstration !!\n",
        "\n",
        "inv_y = concatenate((test_y, test_X[:, 0:]), axis=1)\n",
        "inv_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLDJmi0U9dje"
      },
      "source": [
        "# Concatenate input & target data\n",
        "inv_y = concatenate((test_y, test_X[:, -1:]), axis=1)\n",
        "inv_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxOe6ERv-EFZ"
      },
      "source": [
        "# testplot\n",
        "\n",
        "df = pd.DataFrame(data=inv_y)\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df.plot(title='series')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlE2PPCftlWq"
      },
      "source": [
        "# Invert Scaling\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytQ9gSo_HS9"
      },
      "source": [
        "# testplot\n",
        "\n",
        "df = pd.DataFrame(data=inv_y)\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df.plot(title='series')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRXKpcEhCWYq"
      },
      "source": [
        "# Reshape again (transpose) and leave target data only (first column)\n",
        "inv_y = inv_y[:,0]\n",
        "inv_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E9PE7QMt3tQ"
      },
      "source": [
        "## **Evaluate Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNET_CJGlZK"
      },
      "source": [
        "#### **Reshape Data and Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUopABfKBSYQ"
      },
      "source": [
        "df1 = inv_y.reshape((len(inv_y), 1))\n",
        "# df1 [0:5]\n",
        "df1 = pd.DataFrame(data=df1, columns=['Actual'])\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWwVfwrvBlyi"
      },
      "source": [
        "df2 = inv_yhat.reshape((len(inv_yhat), 1))\n",
        "# df2 [0:5]\n",
        "df2 = pd.DataFrame(data=df2, columns=['Forecast'])\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1CxZSqwEpSx"
      },
      "source": [
        "def residuals(actual, forecast):\n",
        "    # returns the difference between post and pre\n",
        "    return actual - forecast\n",
        "\n",
        "df = pd.concat([df1,df2], axis=1)\n",
        "df['Residuals'] = residuals(df['Actual'], df['Forecast'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s8au5iaA_0j"
      },
      "source": [
        "# Plot Actual vs Forecast Values\n",
        "# df = pd.DataFrame(data=df)\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df[['Actual', 'Forecast']].plot(title='Actual vs Forecast')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXJPZxLoGCqe"
      },
      "source": [
        "# Plot Residuals\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df[['Residuals']].plot(title='Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0t941Iz2zpR"
      },
      "source": [
        "#### **Calculate RMSE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNmqypy5pV48"
      },
      "source": [
        "# inv_y : Test Actual Target Data\n",
        "# inv_yhat : Test Forecasted Target Data\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.2f' % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7RYRD33UuK"
      },
      "source": [
        "#### **Residuals**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0hieeLrtriv"
      },
      "source": [
        "# Get residuals (normalized)\n",
        "residuals = (test_y[0] - yhat[:,0])\n",
        "res = pd.DataFrame(data=residuals, columns=['residuals'])\n",
        "res.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGmZWGOMt7dR"
      },
      "source": [
        "# Plot Residuals\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "res.plot(title='Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGsirhr33ob7"
      },
      "source": [
        "#### **Plot Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klPJb6MKuU5X"
      },
      "source": [
        "# Plot results\n",
        "\n",
        "# shift train predictions for plotting\n",
        "#trainPredictPlot = np.empty_like(dataset)\n",
        "#trainPredictPlot[:, :] = np.nan\n",
        "#trainPredictPlot[windows:len(yhat)+windows, :] = yhat\n",
        "\n",
        "# shift test predictions for plotting\n",
        "#testPredictPlot = np.empty_like(dataset)\n",
        "#testPredictPlot[:, :] = np.nan\n",
        "#testPredictPlot[len(test_X)+(windows*2)+1:len(dataset)-1, :] = test_X\n",
        "\n",
        "# plot baseline and predictions\n",
        "#sns.set(rc={'figure.figsize':(11, 5)})\n",
        "#plt.plot(scaler.inverse_transform(dataset))\n",
        "#plt.plot(dataset)\n",
        "#plt.plot(trainPredictPlot)\n",
        "#plt.plot(testPredictPlot)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agdlQRdamBX1"
      },
      "source": [
        "# **LSTM Multivariate Forecasting (Original)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vczg3CwmBX2"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACTB7mcmBX2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "import seaborn as sns\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import concatenate\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "print('Tensorflow version %s' % tf.__version__)\n",
        "\n",
        "!pip install --quiet ipython-autotime pandas_gbq\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_9pfdSbmBX3"
      },
      "source": [
        "## **Preprocess Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqp2j0vWmBX3"
      },
      "source": [
        "#### **Import & Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNsvMg9amBX3"
      },
      "source": [
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "# load data\n",
        "def parse(x):\n",
        "\treturn datetime.strptime(x, '%Y %m %d %H')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/deltorobarba/repo/master/pollution.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
        "dataset.drop('No', axis=1, inplace=True)\n",
        "# manually specify column names\n",
        "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
        "dataset.index.name = 'date'\n",
        "# mark all NA values with 0\n",
        "dataset['pollution'].fillna(0, inplace=True)\n",
        "# drop the first 24 hours\n",
        "dataset = dataset[24:]\n",
        "# summarize first 5 rows\n",
        "print(dataset.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHhP2LoumBX4"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/deltorobarba/repo/master/timeseries.csv'\n",
        "data = pd.read_csv(url)\n",
        "series = data[['date', 'price1', 'price2']].rename(columns={\"price1\": \"google\", \"price2\": \"apple\"}).set_index('date')\n",
        "series.index = pd.to_datetime(series.index, utc=False)\n",
        "dataset = series.copy()\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P_j7dHimBX4"
      },
      "source": [
        "#### **Visualize Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ifBn4UDmBX4"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(10, 8), \"lines.linewidth\": 1.0})\n",
        "\n",
        "# load dataset\n",
        "values = dataset.values\n",
        "# specify columns to plot\n",
        "groups = [0, 1]\n",
        "i = 1\n",
        "# plot each column\n",
        "pyplot.figure()\n",
        "for group in groups:\n",
        "\tpyplot.subplot(len(groups), 1, i)\n",
        "\tpyplot.plot(values[:, group])\n",
        "\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
        "\ti += 1\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07tK2PnAmBX4"
      },
      "source": [
        "#### **LSTM Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24PZzLbBmBX4"
      },
      "source": [
        "windows = 3\n",
        "features = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QHijQ46mBX4"
      },
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MEouyNSmBX5"
      },
      "source": [
        "#dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
        "values = dataset.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTYP9LGjmBX5"
      },
      "source": [
        "# ensure all data is float\n",
        "values = values.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZibD3FgmBX5"
      },
      "source": [
        "# remove this column if you normalize\n",
        "scaled = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59E9kalGmBX5"
      },
      "source": [
        "# frame as supervised learning\n",
        "reframed = series_to_supervised(scaled, windows, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyqd-d2NmBX6"
      },
      "source": [
        "# Display data\n",
        "reframed [:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG7nesIXmBX6"
      },
      "source": [
        "# Show last column (to be removed)\n",
        "num_rows, num_cols = reframed.shape\n",
        "n = print (num_cols)\n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJRof--9mBX6"
      },
      "source": [
        "# drop columns we don't want to predict\n",
        "n = 7 # number of last column (from above) minus 1 (the index column)\n",
        "reframed.drop(reframed.columns[[n]], axis=1, inplace=True)\n",
        "print(reframed.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZePo4QpmBX6"
      },
      "source": [
        "#### **Shape for LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16qws5RGmBX6"
      },
      "source": [
        "# how many rows (time stamps)?\n",
        "reframed.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrUkY1-tmBX6"
      },
      "source": [
        "values = reframed.values\n",
        "train_time = 2000\n",
        "train = values[:train_time, :]\n",
        "test = values[train_time:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOWb0jxsmBX7"
      },
      "source": [
        "# split into input and outputs\n",
        "n_obs = windows * features\n",
        "train_X, train_y = train[:, :n_obs], train[:, -features]\n",
        "test_X, test_y = test[:, :n_obs], test[:, -features]\n",
        "print(train_X.shape, len(train_X), train_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E3lYx9smBX7"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], windows, features))\n",
        "test_X = test_X.reshape((test_X.shape[0], windows, features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ov2D8fRmBX7"
      },
      "source": [
        "print(test_X.shape, len(test_X), test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Q51ZQYmBX7"
      },
      "source": [
        "test_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSzFaMbHmBX7"
      },
      "source": [
        "## **Model Fitting & Forecasting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9SZTE-SmBX7"
      },
      "source": [
        "#### **Create & Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj2j4HTkmBX8"
      },
      "source": [
        "# design network\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(10))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFFTL99omBX8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG2U85n0mBX8"
      },
      "source": [
        "# Fit model\n",
        "history = model.fit(train_X, train_y, \n",
        "                    epochs=60, \n",
        "                    batch_size=72, \n",
        "                    validation_data=(test_X, test_y), \n",
        "                    verbose=1, \n",
        "                    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8masor4mBX9"
      },
      "source": [
        "# plot history\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFL_HH5JmBX9"
      },
      "source": [
        "#### **Make a prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOg1jcTguvBw"
      },
      "source": [
        "hier muss ein datensatz mit einem timestep in die zukunft rein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2EAtDLOqFRI"
      },
      "source": [
        "# make a prediction\n",
        "yhat = model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCFmIuuImBX-"
      },
      "source": [
        "# Shape: one timestep forecasted for one feature\n",
        "yhat.shape[0], yhat.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju37De9qmBX-"
      },
      "source": [
        "yhat [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1w8p6MgtuH3"
      },
      "source": [
        "test_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL2ZkfZCuHc8"
      },
      "source": [
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "test_y [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0YSYPjVqUD0"
      },
      "source": [
        "**Reshape Test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A19D5vruK9a"
      },
      "source": [
        "dieser teil ist nutzlos!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiAp3v4UqMKz"
      },
      "source": [
        "# Shape: 2 features with each 3 timesteps in the past (windows)\n",
        "test_X [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvFaR_i1qC8a"
      },
      "source": [
        "test_X = test_X.reshape((test_X.shape[0], windows*features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyd-HVHamBX9"
      },
      "source": [
        "# Shape: 6 windows (3 timesteps for 2 features)\n",
        "test_X.shape[0], test_X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KeCHOTGmBX9"
      },
      "source": [
        "test_X [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quih6D8FmBX-"
      },
      "source": [
        "# remove unnecessary data\n",
        "test_X_reshape = test_X[:, -1:]\n",
        "test_X_reshape [0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkUiR7ocmBX-"
      },
      "source": [
        "#### **Evaluate on Unnormalized Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MRfPCUrmBX-"
      },
      "source": [
        "# Concatenate series\n",
        "#df1 = pd.DataFrame(data=test_X_reshape, columns=['Actual'])\n",
        "df1 = pd.DataFrame(data=test_y, columns=['Actual'])\n",
        "df2 = pd.DataFrame(data=yhat, columns=['Forecast'])\n",
        "\n",
        "def residuals(Actual, Forecast):\n",
        "    # returns the difference between post and pre\n",
        "    return Actual - Forecast\n",
        "\n",
        "df = pd.concat([df1,df2], axis=1)\n",
        "df['Residuals'] = residuals(df['Actual'], df['Forecast'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jub9SDbamBX_"
      },
      "source": [
        "# Plot Residuals\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df[['Residuals']].plot(title='Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M2tMt5MmBX_"
      },
      "source": [
        "# Plot Actual vs Forecast Values\n",
        "sns.set(rc={'figure.figsize':(10, 5)})\n",
        "df[['Actual', 'Forecast']].plot(title='Actual vs Forecast')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgdhBuskmBX_"
      },
      "source": [
        "rmse = sqrt(mean_squared_error(test_y, yhat))\n",
        "print('Test RMSE: %.2f' % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}