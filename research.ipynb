{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L_XbhEv0eDdg",
        "hNQBtp95w2Di",
        "DyGjb0ZUwy_5",
        "AhIupns8tUZr",
        "YY-9SFOGdbPy",
        "L_xE9c7WKnUU",
        "qMfQjLMqmzuw",
        "m_M1o-kVjMZ1",
        "02eilyDPlFh8",
        "Cmqnes9brFPk",
        "LOnEwXajSzfL",
        "ijtvzpIDxEzV",
        "ThL-AmiaxI8J",
        "8SsJmnB0xNNN",
        "l-DiG5X3xTfp",
        "oKvbzZ8cxOS1",
        "eLjOkdyQxLeb",
        "V36uKfvAxRCm",
        "hiezBxLBxVLy",
        "IJCykPfwxdQn",
        "zXCXyX9DxfdE",
        "OhIkej28gmaf",
        "AXGHc7oUfx_p",
        "A90mbGhcftHI",
        "GXXGNyygfl3B",
        "w9lDedY3fgnk",
        "2VU6BC9nYCGr",
        "a1VNgmJMpYSL",
        "fSOCv2LsgeIu",
        "dRx4Wa1NZjf_",
        "s9R2XXEqZeYH",
        "dDeahAqJZZUs",
        "dLcMEj-iZUWy",
        "nzm9UioSZQMr",
        "yIEGR7ZtZKfS",
        "RbnAnPCJZDh4",
        "SfaztpziY2sb",
        "egmKQRu3GHft",
        "tnFQtxBD9KEc",
        "_UQPJ8Oz9iXB",
        "JFVnpVvY9T1w",
        "QnYJ0XpX82sX",
        "mL9gnsMGYrOZ",
        "GUue56lG_YaO",
        "vUaRXgHIgW2X",
        "pqNjjLBMYen-",
        "Bs0vbHxkhFvU",
        "Apl61SPnakw3",
        "bIfbMtM1Y-7c",
        "ZVxXcD-mYYhA",
        "91Gi3EMaYlBw",
        "_ZLCspR7YR3z",
        "RcixBlEjhSch",
        "g9-cNnkyYxVk",
        "-91jSrDIX7jC",
        "uxq8EP-8YIIc",
        "U2lhywuKYNRw"
      ],
      "authorship_tag": "ABX9TyNC0KD+BBlfSwqivz+os66k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Research**"
      ],
      "metadata": {
        "id": "xEVSiuftty-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0000.png)"
      ],
      "metadata": {
        "id": "YiqcrmGetjeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**2024 study**"
      ],
      "metadata": {
        "id": "L_XbhEv0eDdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualtran\n",
        "https://github.com/quantumlib/Qualtran/blob/main/qualtran/surface_code/magic_state_factory.py\n",
        "\n",
        "https://github.com/quantumlib/Qualtran/tree/main/qualtran"
      ],
      "metadata": {
        "id": "zMmjG4At5vOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *2024 Study*"
      ],
      "metadata": {
        "id": "hNQBtp95w2Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Quantum hadamrd test](https://youtu.be/F98jpHBQPys?si=_pH2ozWfJjwL7Ap5)\n",
        "\n",
        "[Algebra of pauli matrices](https://youtu.be/Gj9iezP89Dk?si=w_6P8vH3ONLq1a8L)\n",
        "\n"
      ],
      "metadata": {
        "id": "4zPmw5k95FR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2024-02-magnesium-tantalum-material-qubits.amp"
      ],
      "metadata": {
        "id": "CeWSjnS5DjD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www-insidequantumtechnology-com.cdn.ampproject.org/c/s/www.insidequantumtechnology.com/news-archive/news-from-infleqtion-inertial-sensors-atomic-clocks-rf-receivers-oh-and-1600-qubits-by-brian-siegelwax/amp/"
      ],
      "metadata": {
        "id": "ObzMgs60-XB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://thequantuminsider.com/2024/02/07/quantum-matters-quantum-ai-early-days-for-a-killer-combination/\n",
        "\n",
        "https://arxiv.org/abs/2402.03871: Geometric quantum machine learning of BQP^A protocols and latent graph classifiers\n",
        "\n",
        "https://www.spektrum.de/news/revolutioniert-die-fusion-von-ki-und-quantenphysik-die-wissenschaft/2203074"
      ],
      "metadata": {
        "id": "UN-M-61Vhj1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Daniel Sank: The superconducting transmon qubit as a microwave resonator](https://www.youtube.com/watch?v=dKTNBN99xLw&list=WL&index=7&t=2329s)\n",
        "\n",
        "[Quantum Industry Talks: Quantum computing hardware](https://www.youtube.com/watch?v=eyICn3KCUPI&list=WL&index=6&t=2000s)"
      ],
      "metadata": {
        "id": "nKydZaFIbSqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Course 3: How to build a quantum network/ Hardware perspective](https://youtu.be/iGdbROJfmiw?si=v7Bi6FIfY01Dobrd)\n",
        "\n",
        "[Course 4: How to build a quantum network/ Theory perspective](https://youtu.be/pZJVU_60Gd8?si=LLIgpDalf4eLx_ps)"
      ],
      "metadata": {
        "id": "TYad4CPW-EfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hamiltonian simulation and Trotterization: https://vtomole.com/blog/2019/04/07/trotter"
      ],
      "metadata": {
        "id": "1RO_b38EmEgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.golem.de/news/militaer-erfolgreiche-demonstration-von-quanten-funkkommunikation-2401-181120.amp.html"
      ],
      "metadata": {
        "id": "q8fwKeBZlQ8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pending:**\n",
        "\n",
        "* [Robert Huang: Fundamental aspects of solving quantum problems with machine learning\n",
        "](https://www.youtube.com/watch?v=VazUK13iwpQ&list=WL&index=5&t=936s)\n",
        "\n",
        "* [Vedran Dunjko - Building the case for Quantum Machine Learning](https://www.youtube.com/watch?v=td3jfKxT5TI&list=WL&index=7&t=2795s)\n",
        "\n",
        "* [Vedran Dunjko - Exponential separations between classical and quantum learners - IPAM at UCLA](https://www.youtube.com/watch?v=IlEixl4mq0o&list=WL&index=5&t=163s)\n",
        "\n",
        "* [Markus Müller - Topological Quantum Error Correction: From Theoretical Concepts to Experiments](https://www.youtube.com/watch?v=tbrTOemjxow&t=11s)\n",
        "\n",
        "* [Xanadu PennyLane Quantum Computing Training - 2023 NUG Annual Meeting](https://www.youtube.com/watch?v=gJUBBZIq8zo&list=WL&index=4&t=4450s)\n",
        "\n",
        "*Applied Reviews*\n",
        "\n",
        "* [A Look Back at 2023 and Quantum Predictions for 2024](https://www.youtube.com/watch?v=ORhwzlDHdy8&list=PLBn8lN0DcvpmcGQ1H_9YMFPew1ZsP_8Sj&index=3&t=2742s)"
      ],
      "metadata": {
        "id": "UvrswVv2QT7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NISQ and LISQ**\n",
        "\n",
        "https://dabacon.org/pontiff/2024/01/03/acronyms-beyond-nisq/\n",
        "\n",
        "https://www.linkedin.com/pulse/bye-nisq-hello-lisq-simone-severini-ybkmc"
      ],
      "metadata": {
        "id": "OzwFAIepZQMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Encoding and QSVT**\n",
        "\n",
        "* [Pennylane: Intro to QSVT](https://pennylane.ai/qml/demos/tutorial_intro_qsvt/#transforming-matrices-encoded-in-matrices)\n",
        "\n"
      ],
      "metadata": {
        "id": "f-mVvct2q9g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimental advantage in learning with noisy Quantum memory - Quantum Summer Symposium 2021**\n",
        "\n",
        "* Video: [Experimental advantage in learning with noisy Quantum memory - Quantum Summer Symposium 2021](https://www.youtube.com/watch?v=GEgJkqQNwvQ&list=WL&index=1)\n",
        "\n",
        "*details used for QML book"
      ],
      "metadata": {
        "id": "OgCGvQeGkFtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x| x\\right\\rangle$\n",
        "\n",
        "The given expression $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x|x\\right\\rangle$ represents the **squared norm of the quantum state |x⟩**. The squared norm of a quantum state is a measure of its overall size or amplitude. It is calculated by taking the inner product of the state with itself.\n",
        "\n",
        "Let's break down the expression:\n",
        "\n",
        "* **⟨0|:** This denotes the inner product of the vacuum state |0⟩ with itself. The vacuum state is the state with no particles, and it has unit norm.\n",
        "\n",
        "* **U_0^{\\dagger}(x):** This is the conjugate transpose of the identity operator I0, which is a unitary operator that acts as the identity on the vacuum state. The conjugate transpose is a common operation in quantum mechanics, and it relates the action of an operator on a quantum state to its action on the dual state.\n",
        "\n",
        "* **U_0(x):** This is the identity operator I0. The identity operator is a unitary operator that does not change the state of a quantum system. It is the simplest and most fundamental unitary operator.\n",
        "\n",
        "* **|0⟩:** This is the vacuum state |0⟩. The vacuum state is the state with no particles, and it has unit norm.\n",
        "\n",
        "* **⟨x|x⟩:** This denotes the inner product of the state |x⟩ with itself. The inner product of two quantum states is a measure of their overlap. It is calculated by taking the dot product of their corresponding complex vectors.\n",
        "\n",
        "Putting everything together, the expression $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x|x\\right\\rangle$ represents the squared norm of the quantum state |x⟩. It is a scalar quantity that indicates the overall size or amplitude of the state.\n"
      ],
      "metadata": {
        "id": "Fkcx5erfdR5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 1**\n",
        "\n",
        "* Video: [Parameter--Shift Rule Derivation — Part 1 | PennyLane Tutorial](https://www.youtube.com/watch?v=crgmwXBogx4&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=5)\n",
        "* https://pennylane.ai/qml/glossary/parameter_shift/#gaussian-gate-example-\n",
        "* Also see: [Automatic Differentiation of Quantum Circuits](https://www.youtube.com/watch?v=McgBeSVIGus&t=1s)\n",
        "* Also see: [Parameter Shift Rule for calculation of Gradients in Quantum Variational Circuits](https://www.youtube.com/watch?v=yJriVOy5l_8&list=WL&index=1&t=6s)\n",
        "* Represent certain expectation values with Fourier series, important to derive some parameter shift rules in order to compute quantum gradients\n",
        "* Fourier representation is also useful to expressivity of certain quantum models\n",
        "* **We are computing the expectation value <font color=\"green\">$E(x)$</font> of operator <font color=\"red\">$B$</font> such that we have a quantum state $\\psi$ which is evolved by some unitary <font color=\"blue\">$U(x)$</font>**\n",
        "\n",
        "> <font color=\"green\">$E(x)$</font> $=\\langle\\psi|$ <font color=\"blue\">$U^{\\dagger}(x)$</font> <font color=\"red\">$B$</font> <font color=\"blue\">$U(x)$</font> $| \\psi\\rangle$\n",
        "\n",
        "This kind of expression is common in quantum computing and quantum mechanics, **where it's used to calculate the expected outcomes of various quantum operations**, which are critical for understanding how quantum algorithms will behave.\n",
        "\n",
        "**How to solve this? $E(x) =\\langle\\psi| U^{\\dagger}(x) B U(x) | \\psi\\rangle$**\n",
        "\n",
        "* Two main approaches to calculating the eigenvalues of a Hermitian operator B:\n",
        "  1. **Diagonalization:** This method involves finding an orthonormal basis of eigenvectors of B. The eigenvalues of B are then the corresponding eigenvalues of the Hermitian matrix representing B in this basis.\n",
        "  2. **Eigenvalue Solvers:** There are various numerical methods for calculating eigenvalues of Hermitian operators, such as the Rayleigh-Ritz quotient, the Lanczos algorithm, and the Arnoldi algorithm. These methods can be used to calculate eigenvalues for large Hermitian matrices that cannot be diagonalized directly.\n",
        "* In the case of $E(x) =\\langle\\psi| U^{\\dagger}(x) B U(x) | \\psi\\rangle$, the observable B is sandwiched between the unitary operator $U(x)$ and its conjugate transpose $U^{\\dagger}(x)$.\n",
        "* This means that $B$ can be expressed as a combination of powers of $U(x)$ and $U^{\\dagger}(x)$. **If we can find the eigenvalues of this combination of operators $U(x)$ and $U^{\\dagger}(x)$, then we can calculate the eigenvalues of $E(x)$**.\n",
        "  * One way to achieve this is to use a diagonalization method - accurate and but computational expensive. This involves finding a suitable basis for the Hilbert space of quantum states, and representing U(x) and U^{\\dagger}(x) in this basis. The eigenvalues of B can then be calculated by diagonalizing the matrix representing B in this basis.\n",
        "  * Another approach is to use an eigenvalue solver - efficient, but not super accurate. This involves choosing an appropriate numerical method, and applying it to the matrix representing B in the basis defined by U(x) and U^{\\dagger}(x). The eigenvalues of B can then be calculated by using the numerical method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_89dYYPMwz0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 2**\n",
        "\n",
        "(same video, continuation, [Parameter--Shift Rule Derivation — Part 1 | PennyLane Tutorial](https://www.youtube.com/watch?v=crgmwXBogx4&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=5)\n",
        ")\n",
        "\n",
        "* <font color=\"blue\">$U(x) = e^{ixG}$</font>\n",
        "* <font color=\"orange\">$G$</font> = hermitian operator <font color=\"orange\">$G^{\\dagger}$ = $G$</font> is called the **generator** of the operator $U$ (e.g. the generator of a Y rotation gate is the Pauli Y gate with a factor of $-\\frac{1}{2}$\n",
        "\n",
        "Eigenvalues and Eigenstates of <font color=\"orange\">$G$</font>, denoted by some $\\omega_j$. We create set out of these eigenvalues, ordered in non-descending order. j is from e, which is defined as set of positive integers from 1 to d\n",
        "\n",
        "> Eigenvalue: <font color=\"orange\">$\\left\\{\\omega_j\\right\\}_{j \\in[d]}$ $\\quad$ with $[d]:=\\{1, \\cdots, d\\}$\n",
        "\n",
        "> Eigenequation: <font color=\"orange\">$G\\left|\\varphi_j\\right\\rangle=\\omega_j\\left|\\varphi_j\\right\\rangle$\n",
        "\n",
        "Eigenvalues and Eigenstates of <font color=\"blue\">$U(x)$</font>:\n",
        "\n",
        "> Eigenvalue: <font color=\"blue\">$\\left\\{e^{i x \\omega_j}\\right\\}_{j \\in[d]}$ $\\quad$ with $[d]:=\\{1, \\cdots, d\\}$\n",
        "\n",
        "> Eigenstate: <font color=\"blue\">$|\\varphi_j\\rangle_{j \\in[d]}$\n",
        "\n",
        "> Eigenequation: <font color=\"blue\">$U(x)\\left|\\varphi_j\\right\\rangle=e^{i x \\omega_j}\\left|\\varphi_j\\right\\rangle$\n",
        "\n",
        "Two more definitions:\n",
        "\n",
        "> $\\psi_j := \\left\\langle\\varphi_j \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $b_{j k} := \\left\\langle\\varphi_j|B| \\varphi_k\\right\\rangle$ with $j, k \\in[d]$\n",
        "\n",
        "Now we can re-express the expectation value <font color=\"green\">$E(x)$</font> by expanding each of these terms in the multiplication in the Eigenbasis of the unitary <font color=\"blue\">$U(x)$</font>:\n",
        "\n",
        "> $\\begin{gathered}E(x)=\\sum_{j, k=1}^d \\psi_j^*\\left[e^{i x \\omega_j}\\right]^* b_{j k} e^{i x \\omega_k} \\psi_k \\\\ \\psi_j=\\left\\langle\\varphi_j \\mid \\psi\\right\\rangle \\\\ b_{j k}=\\left\\langle\\varphi_j|B| \\varphi_k\\right\\rangle\\end{gathered}$"
      ],
      "metadata": {
        "id": "156UXYtVzz68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 3**\n",
        "\n",
        "Video: [QHack 2021: Maria Schuld—Quantum Differentiable Programming](https://www.youtube.com/watch?v=cwiINWkMOvA)\n",
        "\n",
        "Are there QML problems that are proven to be superior to CML? - Yes, there are certain problems superior on QML, but none are useful. Useful QML problems with exponential separation to CML are still an open question.\n",
        "\n",
        "What model class is QML? How can it generalize? Let's describe what it is, before we look into concrete cases with only exponential speedups.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1685.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1686.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1687.png)\n"
      ],
      "metadata": {
        "id": "STeAmaKnzi8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Chemistry**\n",
        "* Video: [Introduction to Quantum Chemistry | PennyLane Tutorial](https://www.youtube.com/watch?v=khC0CCjxB7k&list=WL&index=11)\n",
        "* See also: [Aurora Clark (3/10/20): Topology in chemistry applications](https://www.youtube.com/watch?v=qSJeynJbmqU&list=WL&index=12&t=1252s)\n",
        "* Predict properties of materials\n",
        "* What chemical reactions can happen under certain circumstances\n",
        "* Molecular orbitals of electrons: how to represent them with qubits? With Jordan Wigner representation\n",
        "* Hamiltonian operator (all interactions and movements) important to know the energy levels occupied by the electrons in order to predict accurately the chemical properties of the molecules\n",
        "* Hamiltonian operator = all interactions and movements -> too complicated, so we approximate: Hartree Fock approximation\n",
        "    * Calculates molecular geometry (positions where the atomic nuclei are)\n",
        "    * Then calculate hamiltonian\n",
        "* **Jordan Wigner representation**\n",
        "  * https://learn.microsoft.com/en-us/azure/quantum/user-guide/libraries/chemistry/concepts/jordan-wigner\n",
        "  * https://en.m.wikipedia.org/wiki/Jordan–Wigner_transformation\n",
        "  * https://www.cond-mat.de/events/correl21/manuscripts/koch.pdf"
      ],
      "metadata": {
        "id": "Z6d3CddIw4Ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Quantum Algorithms**\n",
        "* Video: [Variational Quantum Algorithms](https://www.youtube.com/watch?v=YtepXvx5zdI&t=21s)\n",
        "* Contains **Great images of model types etc)**\n",
        "* https://pennylane.ai/qml/glossary/variational_circuit/\n",
        "* Also known as: parametrized quantum circuits, quantum neural networks\n",
        "* example: VQE (variational quantum eigensolver) for chemistry problems or QAOA for optimization and many more\n",
        "* Variational quantum circuits are already hybrid models"
      ],
      "metadata": {
        "id": "Ezjnw5L2w_ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Quantum-Classical Machine Learning**\n",
        "* Video: [Hybrid Quantum-Classical Machine Learning](https://www.youtube.com/watch?v=t9ytqPTij7k)\n",
        "* * Variational quantum circuits are already hybrid models\n",
        "* Backprop:\n",
        "  * treat the quantum circuit as a basic atomic individual step in the computation graph.  \n",
        "  * the expectation value of a quantum circuit is a differentiable function.\n",
        "  * So if we treat that single expectation value as a single step, we can use the parameter shift rule to determine the gradient of that same function.\n",
        "  * Means: we can feed that to the backprop algorithm."
      ],
      "metadata": {
        "id": "gD0Zvm95xCNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizing a quantum circuit with PennyLane | PennyLane Tutorial**\n",
        "* Quantum circuit is a function. Think of quantum circuit as cost function and measurement will be the cost -> useful for finding minimum in optimization or ground state in chemistry.\n",
        "* Video: [Optimizing a quantum circuit with PennyLane | PennyLane Tutorial](https://www.youtube.com/watch?v=42aa-Ve5WmI)\n",
        "* https://docs.pennylane.ai/en/stable/introduction/interfaces.html\n"
      ],
      "metadata": {
        "id": "xzAcWeX8xT3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beyond-classical computing, Sergio Boixo**\n",
        "* Video: [Beyond-classical computing, Sergio Boixo](https://www.youtube.com/watch?v=cGaULUQuu1A&list=WL&index=1&t=1147s)\n",
        "* Sample problem, approximate from a sample distribution\n",
        "* Experimental Random Circuit Sampling (RCS)"
      ],
      "metadata": {
        "id": "iBn3SNx_w9qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to solve the QUBO problem | PennyLane Tutorial**\n",
        "\n",
        "* Video: [How to solve the QUBO problem | PennyLane Tutorial](https://www.youtube.com/watch?v=LhbDMv3iA9s&t=212s)"
      ],
      "metadata": {
        "id": "M72_rlVX5fsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Haar Measure | PennyLane Tutorial**\n",
        "\n",
        "* Video: [The Haar Measure | PennyLane Tutorial](https://www.youtube.com/watch?v=d4tdGeqcEZs)\n"
      ],
      "metadata": {
        "id": "QkYxR2W-5p8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Physical simulation through a quantum computational lens, Jarrod McClean**\n",
        "\n",
        "* Video: [Physical simulation through a quantum computational lens, Jarrod McClean](https://www.youtube.com/watch?v=-8fYbDwLAug&list=WL&index=2&t=383s)\n",
        "\n",
        "* What we mean with quantum chemistry simulation: we want some amount of understanding if the system. Like learn interesting things, like will a system absorb light and why? How often will it show up?\n",
        "\n",
        "* The predictive power of (free) energies: Can all physically interesting questions be answered by some reduced model? Recent, but there are undecidable, there can't be an algorithm that determines the answer:\n",
        "  * Does a system thermalize?\n",
        "  * Does a system have an electronic gap?\n",
        "  * Will molecule X ever form from constituents Y?\n",
        "\n",
        "* qualitative changes that can't be predicted ahead of time: proteins, RNA/DNA\n",
        "\n",
        "* Physical undecidability - as the system evolves in time, there are sudden, qualitative changes that cannot be predicted in any way except evolving forward in time and seeing if it happened, and no answer in finite time can indicate if it will never happen (for all systems).\n",
        "\n",
        "> Undecidability formally broken by advice in some cases - and data is a restricted form of advice!\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1688.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1689.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5K4X-OB5RiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jarrod McClean - The role of data, precomputation, and communication in a quantum learning landscape**\n",
        "\n",
        "* Video: [Jarrod McClean - The role of data, precomputation, and communication in a quantum learning landscape](https://www.youtube.com/watch?v=dOUppTONVDM&list=WL&index=8&t=819s)\n",
        "\n",
        "* Quantum data is interesting for future discovery of the universe (recall the impact of CCD cameras on telescopes - see\n",
        "\"The Perfect Theory\"), but most data we work with today, even from quantum systems, seems classical.\n",
        "\n",
        "* There are a few pieces of evidence that QC might help for classical data (sampling hard distributions, learning problems based on discrete log, linear algebra routines, ...) but a lot of pieces of evidence that it will be hard to achieve in practice\n",
        "\n",
        "* Question - What is the full class of uniquely quantum pre-computations we can do to accelerate time to solution?\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1690.png)\n"
      ],
      "metadata": {
        "id": "pDVzhkBUBGja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Locally flat distributions*"
      ],
      "metadata": {
        "id": "DyGjb0ZUwy_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/s41598-023-43404-3\n",
        "\n",
        "we use a robust local minima problem to compare state-of-the-art local optimizers (SLSQP, COBYLA, L-BFGS-B and SPSA) against DE using the Variational Quantum Eigensolver algorithm."
      ],
      "metadata": {
        "id": "Wl6tjloxxBNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.quantum.ibm.com/api/qiskit/qiskit.algorithms.optimizers.SLSQP"
      ],
      "metadata": {
        "id": "Mv_PoowCww0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is so interesting aboiut Locally flat distributions and Paulis on quantum computers?**\n",
        "\n",
        "Here's what's interesting about locally flat distributions and Paulis (Pauli operators) in the context of quantum computers, and why they matter:\n",
        "\n",
        "**Locally Flat Distributions**\n",
        "\n",
        "* **Noise Representation:** Locally flat distributions are used to model a particular type of noise in quantum circuits. This noise has the property that small errors become much more likely than large errors. Locally flat distributions capture this behavior.\n",
        "* **Error Correction and Threshold Theorems:** Understanding the properties of locally flat distributions plays a role in the development of quantum error correction codes.  Importantly, they are connected to the idea of fault-tolerance thresholds – limits on error rates below which quantum computation can be made reliable.\n",
        "* **Simulation and Analysis:** Studying locally flat distributions helps researchers design and analyze quantum algorithms that are robust to this specific type of noise.\n",
        "\n",
        "**Paulis (Pauli Operators)**\n",
        "\n",
        "* **Fundamental Building Blocks:** The Pauli operators (X, Y, Z) together with the identity operator (I) form a basis for the space of operators on a single qubit.  This means any operation on a single qubit can be expressed as a combination of Paulis.\n",
        "* **Error Representation:** Many common quantum errors can be represented as combinations of Pauli operators acting on qubits. This makes them convenient for describing noise processes\n",
        "* **Qubit Control:**  Pauli operators are used as quantum gates to manipulate the state of qubits, forming the essential toolkit for quantum computation.\n",
        "\n",
        "**Where They Intersect**\n",
        "\n",
        "* **Describing Noise with Paulis:** Locally flat noise distributions can often be well-described in terms of probabilistic combinations of Pauli errors.  This connection helps analyze how such noise affects quantum circuits.\n",
        "* **Error Correction Tailored to Paulis:** Quantum error-correcting codes can be specifically designed to combat Pauli errors, which are a common type of noise. Understanding the relationship between Paulis and noise distributions helps in creating more effective codes.\n",
        "* **Efficient Simulation:** Certain simulation techniques, such as stabilizer circuit simulations, heavily exploit the structure of Pauli operators to efficiently simulate specific classes of quantum circuits.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Locally flat distributions and Paulis provide a framework to model, analyze, and mitigate the effects of noise in quantum computers. Understanding their connection enables the design of more resilient quantum algorithms and the development of robust error-correction schemes that are crucial for the advancement of practical quantum computing.\n",
        "\n",
        "**Let me know if you'd like to dive deeper into any of these aspects, such as quantum error correction codes, noise modeling, or stabilizer circuits!**\n"
      ],
      "metadata": {
        "id": "yB16-W-bwYg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is low degree truncation in quantum computing?**\n",
        "\n",
        "Here's an explanation of low-degree truncation in quantum computing, its uses, and its significance:\n",
        "\n",
        "**What is Low-Degree Truncation?**\n",
        "\n",
        "Low-degree truncation is a technique used in quantum computing to simplify calculations involving complex quantum systems. It involves approximating a system's Hamiltonian (the operator that describes its energy) by keeping only the terms with low-degree polynomials.\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "1. **Hamiltonian Representation:**  The Hamiltonian of a quantum system can often be expressed as a sum of terms involving products of Pauli operators (X, Y, Z).  Each term has a degree determined by the number of Pauli operators in the product (e.g., the term XY has degree 2).\n",
        "\n",
        "2. **Truncation:** In low-degree truncation, you keep only the terms in the Hamiltonian up to a certain degree and discard the higher-degree terms. This simplifies the representation and makes calculations more manageable.\n",
        "\n",
        "**Why Use Low-Degree Truncation**\n",
        "\n",
        "* **Computational Complexity:** Simulating complex quantum systems on classical computers can be extremely computationally expensive. The number of terms in the Hamiltonian grows exponentially with the size of the system. Low-degree truncation reduces this complexity.\n",
        "* **Focus on Essential Interactions:** In many systems, the essential physics is often captured by the low-order interactions. Truncating higher-order terms may introduce some error but often provides a good approximation while significantly simplifying the calculations.\n",
        "* **Resource Limitations:** Quantum computers themselves have a limited number of qubits and limited gate depth they can apply. Low-degree truncation can make problems more amenable to simulation on near-term quantum devices.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* **Quantum Chemistry Simulations:** Low-degree truncation is used in simulating the electronic structure of molecules. This helps understand chemical reactions and design new materials.\n",
        "* **Material Science:**  It's used in simulations of materials to predict their properties, aiding in the discovery of new materials with tailored characteristics.\n",
        "* **Quantum Optimization:**  Some quantum optimization algorithms are amenable to low-degree truncation, making it possible to approximate solutions to complex optimization problems.\n",
        "\n",
        "**Significance**\n",
        "\n",
        "Low-degree truncation is a powerful tool for making quantum simulations more tractable, especially when dealing with large or complex systems. However, there's a trade-off:\n",
        "\n",
        "* **Accuracy vs. Efficiency:**  Truncation introduces errors, so it's important to balance accuracy requirements with computational efficiency when choosing the degree of truncation.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Consider a Hamiltonian with terms like X, Y, Z, XY, XZ, YZ, XYZ.  A low-degree truncation at degree 2 would only keep the terms X, Y, Z, XY, XZ, and YZ, and discard the XYZ term.\n",
        "\n",
        "**Let me know if you would like to explore any specific application of low-degree truncation in more detail!**\n"
      ],
      "metadata": {
        "id": "FrqzVdYqwaWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the Depolarizing channel in each qubit?**\n",
        "\n",
        "Here's a breakdown of the depolarizing channel in quantum computing, including how it works and its significance:\n",
        "\n",
        "**What is the Depolarizing Channel?**\n",
        "\n",
        "* **Definition:** The depolarizing channel is a type of quantum channel (a mathematical model of how quantum states evolve) that introduces noise into a qubit (the basic unit of quantum information).  It does this by effectively shrinking the Bloch sphere representation of the qubit's state.\n",
        "* **Behavior:** With a probability `p`, the depolarizing channel replaces the input qubit with the completely mixed state (a state of maximum uncertainty). With the remaining probability `(1-p)`, the qubit is left unchanged.\n",
        "* **Effect:** The depolarizing channel introduces randomness, making it harder to reliably store and process quantum information.\n",
        "\n",
        "**Mathematical Representation**\n",
        "\n",
        "The depolarizing channel can be represented mathematically using the Kraus operator-sum representation:\n",
        "\n",
        "```\n",
        "ℰ(ρ) = (1-p) ρ + p/3 (XρX + YρY + ZρZ)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `ρ` is the input density matrix (representing the qubit's state).\n",
        "* `p` is the depolarization probability.\n",
        "* `X`, `Y`, and `Z` are the Pauli matrices (basic quantum gates).\n",
        "\n",
        "**Geometric Interpretation**\n",
        "\n",
        "The depolarizing channel can be visualized as shrinking the Bloch sphere (a geometric representation of a qubit's state):\n",
        "\n",
        "* The completely mixed state is represented by the center of the Bloch sphere.\n",
        "* The depolarizing channel moves the qubit's state closer to the center, shrinking its representation on the Bloch sphere.\n",
        "* The stronger the depolarization (higher `p`), the more the Bloch sphere shrinks.\n",
        "\n",
        "**Significance**\n",
        "\n",
        "* **Modeling Noise:** The depolarizing channel is a useful model for representing realistic noise in quantum systems.  Real-world qubits interact with their environment, leading to decoherence that is often well-approximated by a depolarizing channel.\n",
        "* **Error Correction:** Understanding the depolarizing channel is crucial for designing quantum error correction codes. These codes help protect quantum information from the effects of noise.\n",
        "* **Capacity Limits:** The depolarizing channel affects the capacity of a quantum communication channel, which limits the amount of information that can be reliably transmitted.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose you have a qubit in the pure state |0⟩. If it passes through a depolarizing channel with probability `p = 0.25`, the output state would be:\n",
        "\n",
        "```\n",
        "(1 - 0.25) |0⟩⟨0| + 0.25/3 (I/2) = 0.75 |0⟩⟨0| + 0.083 I\n",
        "```\n",
        "\n",
        "This is a mixed state, indicating a loss of quantum information.\n",
        "\n",
        "**Let me know if you would like a deeper dive into any of these aspects or have a specific application in mind!**\n"
      ],
      "metadata": {
        "id": "3J6s2aTawcbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Computational Learning*"
      ],
      "metadata": {
        "id": "AhIupns8tUZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Model vs Circuit Capacity (Complexity / Expressivity)*"
      ],
      "metadata": {
        "id": "YY-9SFOGdbPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model vs Circuit Capacity (Complexity / Expressivity)*\n",
        "\n",
        "- What is meant by: In neural networks the differentiation rules, that we use, scale linearly with the number of parameters. In variational circuits the differentiation rules, that we use, scale quadratically with the number of parameters.\n",
        "- Automatic differentiation is based on recycling values of gradients, so that not for every parameter they have to run the whole network again, forwards and backwards.\n",
        "- Challenge: Just guessing an ansatz (expressive)\n",
        "- Adding more layers = Increase the frequency of the cosine kernel?? - Min 27: [QML Meetup: Dr Maria Schuld, Taking stock of quantum machine learning - a critical perspective](https://youtu.be/8bfUMdj0-x4), then just repeating these layers of encoding would be better. In many cases making an embedding and then repeating it makes the model class richer.\n",
        "- Quantum advantage for learning is currently still ill-posed.\n",
        "- What is meant by quantum speedup? Different concepts. It’s always relative to something.\n",
        "- And what do you mean by ‘more powerful’: learning or speedup? Etc.\n",
        "\n",
        "> How can I prove that my ansatz is classically intractable? versus **What is an ansatz design that allows gradient-descent to scale as efficient as it does when training neural networks?**\n",
        "\n",
        "> How can I show that QML is more powerful? versus **How can I understand what QML is doing?**\n",
        "\n",
        "https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1621s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1609.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1610.png)"
      ],
      "metadata": {
        "id": "zrowK8QAUvAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Concept and Hypothesis*"
      ],
      "metadata": {
        "id": "L_xE9c7WKnUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Jens Eisert - Do quantum computers have application in machine learning & combinatorial optimization](https://www.youtube.com/watch?v=DgCNmk4kvVs&list=WL&index=11&t=827s)"
      ],
      "metadata": {
        "id": "IzhwAXtGdmvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis class = Function Family I'm considering for learning\n",
        "\n",
        "Concept class = true relationship between data and labels"
      ],
      "metadata": {
        "id": "uxUNTV6yhxzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept class**\n",
        "\n",
        "A concept class in computational learning theory is a set of target concepts or functions that a learning algorithm is trying to learn. For example, a concept class could be the set of all Boolean functions, or the set of all linear functions.\n",
        "\n",
        "Concept class refers to the set of all possible concepts or functions that the learner is trying to learn or approximate. It represents the underlying target concept that the learner is trying to capture.\n",
        "\n",
        "\n",
        "**Hypothesis**\n",
        "\n",
        "A hypothesis in computational learning theory is a candidate solution to a learning problem. For example, a hypothesis for learning a Boolean function could be a decision tree, or a neural network.\n",
        "\n",
        "Hypothesis is a subset of the concept class that the learner uses to approximate or represent the target concept. It's the set of functions or models that the learner considers as potential solutions to the learning problem.\n",
        "\n",
        "\n",
        "**Learner**\n",
        "\n",
        "A learner in computational learning theory is an algorithm that takes training examples as input and outputs a hypothesis. The goal of the learner is to find a hypothesis that is consistent with the training data and that will generalize well to new data.\n",
        "\n",
        "The learner is the algorithm or method used to search through the hypothesis space and select the best hypothesis that approximates the target concept based on available training data.\n",
        "\n",
        "\n",
        "Here are some specific examples of concept classes, hypotheses, and learners in computational learning theory:\n",
        "\n",
        "Example 1:\n",
        "\n",
        "* **Concept class:** The set of all Boolean functions\n",
        "* **Hypothesis:** A decision tree\n",
        "* **Learner:** The ID3 algorithm\n",
        "\n",
        "Example 2:\n",
        "\n",
        "* **Concept class:** The set of all linear functions\n",
        "* **Hypothesis:** A linear regression model\n",
        "* **Learner:** The least squares algorithm\n",
        "\n",
        "Example 3:\n",
        "\n",
        "* **Concept class:** The set of all images of cats\n",
        "* **Hypothesis:** A convolutional neural network\n",
        "* **Learner:** The Adam optimizer\n",
        "\n",
        "Other examples:\n",
        "\n",
        "* **Concept class:**\n",
        "  * Equivalently, a concept c is a subset of {0,1}n, namely {x : c(x) = 1}. Let C ⊆ {f : {0,1}n → {0,1}} be a concept class. This could for example be the class of functions computed by disjunctive normal form (DNF) formulas of a certain size, or Boolean circuits or decision trees of a certain depth. https://arxiv.org/pdf/1607.00932.pdf\n",
        "  * Set of all Boolean functions: If you are dealing with a problem where the underlying target concept or function you want to learn is inherently Boolean in nature (i.e., it maps inputs to binary outputs, such as 0 or 1), then Boolean functions can be the concept class.\n",
        "  * set of all linear functions: Linear functions can be considered the concept class when the underlying target concept is believed to be a linear relationship between inputs and outputs. In this case, the concept class consists of all possible linear functions that could describe the target concept. Example: If you are working on a regression problem where you want to predict a numerical value based on input features, and you believe that the relationship is linear, the concept class could be all possible linear functions.\n",
        "  * Unitary transformations\n",
        "  * Example: In a binary classification problem where you want to distinguish between spam and non-spam emails, the concept class could be the set of all possible decision boundaries that separate spam emails from non-spam emails in the feature space. Each decision boundary represents a different concept in the concept class.\n",
        "  * For agnostic learner:  In the case of binary classification (e.g., spam vs. non-spam email classification), the agnostic concept class might represent all possible ways to separate the two classes in the feature space without assuming any specific data distribution. It could include various decision boundaries, non-linear separations, or complex relationships between features and labels.\n",
        "  * Gibbs states, also known as Gibbs distributions or Gibbs measures, are mathematical constructs from statistical physics and probability theory. They are used to model the probability distributions of physical systems consisting of multiple interacting components (particles, spins, etc.). In the context of computational learning theory, Gibbs states can be considered as a concept class when they are used to model or represent the set of all possible probability distributions over a space of random variables. Concept classes represent the set of all possible underlying target concepts or functions that the learner is trying to learn or approximate. In this case, Gibbs states represent a class of possible probability distributions.\n",
        "* **Hypothesis:**\n",
        "  * Boolean functions: Boolean functions can also be used as the hypothesis set, representing the set of candidate models or classifiers that the learner considers as potential solutions to the problem. In this case, you are considering Boolean functions as candidate hypotheses for approximating the target concept. Example: In binary classification tasks, you might consider simple Boolean functions (e.g., \"If feature A is true and feature B is false, output 1; otherwise, output 0\") as potential hypotheses for classifying data points.\n",
        "  * Linear functions: Linear functions can also be used as the hypothesis set when you are considering linear models as potential hypotheses for approximating the target concept. In this case, you are using linear functions as candidate models to fit the data and learn the mapping between inputs and outputs. Example: Linear regression models, which use linear functions to model relationships between features and outcomes, can be considered as hypotheses in a regression problem.\n",
        "  * decision tree with regularization\n",
        "  * linear regression model with regularization\n",
        "  * neural network: As a hypothesis, a neural network can be used to represent a specific function or concept. For example, a neural network could be trained to represent the function that maps an image to a probability distribution over different categories. e.g.: convolutional neural network with regularization\n",
        "  * Example: In the context of linear classification, the hypothesis space could be the set of all possible linear classifiers (e.g., support vector machines, logistic regression). Each linear classifier within this space is a hypothesis used to approximate the target concept.\n",
        "  * For agnostic learner: The agnostic hypothesis set could include various types of models, such as decision trees, support vector machines, neural networks, or any other model that can be used for binary classification. These models are chosen based on their flexibility and general applicability, allowing the learner to explore a wide range of possibilities without being tied to specific distributional assumptions.\n",
        "* **Learner:**\n",
        "  * agnostic learner, and examples of agnostic learners:\n",
        "    * Support vector machines\n",
        "    * Random forests\n",
        "    * Neural networks: As a learner, a neural network can be used to learn a wide variety of functions and concepts. This is because neural networks are able to approximate any continuous function to arbitrary accuracy, given enough neurons and hidden layers. In practice, neural networks are typically used as learners. This is because neural networks are able to learn from data without being given any prior knowledge about the function or concept that they are trying to learn.\n",
        "    * A perceptron is a type of artificial neuron that is used in machine learning. It is the simplest type of artificial neuron and is the building block of more complex neural networks.\n",
        "  * proper learner\n",
        "  * online learner (spam filter, stock trading algorithm, recommendation system)\n",
        "  * active learner\n",
        "  * Example: For supervised learning tasks, a learner could be a decision tree algorithm, a neural network, or a k-nearest neighbors classifier. The learner's role is to use the training data to find the hypothesis (model) that minimizes a certain loss function or error measure.\n",
        "\n",
        "\n",
        "*It is important to note that a learning algorithm may use different hypotheses for different concept classes. For example, the ID3 algorithm can be used to learn decision trees for any concept class, but the least squares algorithm can only be used to learn linear functions.*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dpelTUvXhJ2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems / Use Cases (and representations of states and operations):**\n",
        "* **Hamiltonian learning**: is the process of learning the Hamiltonian of a quantum system. The Hamiltonian is a mathematical object that describes the energy of a quantum system. Knowing the Hamiltonian of a quantum system allows us to predict its behavior and to design experiments to control it.\n",
        "  * Designing quantum computers to perform specific tasks\n",
        "  * Developing new quantum algorithms\n",
        "  * Understanding the fundamental properties of quantum matter\n",
        "  * The main difference between Hamiltonian learning and learning unitary dynamics is that Hamiltonian learning is more general. By learning the Hamiltonian of a quantum system, we can learn its unitary evolution, as well as other properties of the system. However, learning the unitary evolution of a quantum system does not necessarily tell us anything about the Hamiltonian of the system.\n",
        "  * Another difference between Hamiltonian learning and learning unitary dynamics is that Hamiltonian learning is typically more difficult. This is because the Hamiltonian of a quantum system is typically a complex function of many parameters. In contrast, the unitary evolution of a quantum system can often be described by a simpler mathematical object, such as a matrix.\n",
        "  * Hamiltonian learning is a process of learning the Hamiltonian of a quantum system. The Hamiltonian of a quantum system is a mathematical object that describes the energy of the system. Knowing the Hamiltonian of a quantum system allows us to predict its behavior and to design experiments to control it.\n",
        "  * Hamiltonian learning can be used to learn a variety of quantum concepts and functions. For example, Hamiltonian learning can be used to learn the ground state of a quantum system, the energy levels of a quantum system, and the unitary evolution of a quantum system.\n",
        "* **Learning unitary dynamics**: is the process of learning the unitary evolution of a quantum system. The unitary evolution of a quantum system describes how the state of the system changes over time. Knowing the unitary evolution of a quantum system allows us to design experiments to perform specific tasks, such as quantum computing and quantum teleportation.\n",
        "  * Controlling quantum systems to perform specific tasks, such as quantum teleportation and quantum cryptography\n",
        "  * Developing new quantum measurement techniques\n",
        "  * Understanding the dynamics of complex quantum systems\n",
        "* **Learning unknown element U of the Clifford group on n qubits**\n",
        "  * learning a Clifford element U is a special case of both Hamiltonian learning and learning unitary dynamics.\n",
        "  * The Clifford group is a subset of the unitary group, which is the group of all unitary transformations on n qubits. This means that every Clifford element U can be represented by a unitary matrix. Therefore, learning a Clifford element U is equivalent to learning a unitary transformation.\n",
        "  * The Hamiltonian of a quantum system is a unitary operator. This means that the evolution of a quantum system over time can be described by a unitary transformation. Therefore, learning the Hamiltonian of a quantum system is equivalent to learning a unitary transformation.\n",
        "* **Gibbs state**\n",
        "  * Gibbs states are a type of probability distribution over the states of a physical system. They are typically used in statistical physics to model the behavior of systems at equilibrium.\n",
        "  * Gibbs states can be used to learn about the properties of a physical system. For example, we can use Gibbs states to learn about the energy levels of a system, or the phase transitions that the system can undergo.\n",
        "* **Tomography, both quantum state tomography (QST) and quantum shadow tomography (QST)**\n",
        "* **DNF formulas** themselves are not hypotheses or learners. They are simply a way of representing Boolean functions.\n",
        "* **Bell sampling**:\n",
        "  * Bell sampling is a technique for measuring the correlations between pairs of qubits. It can be used to learn about the properties of quantum states, but it is not a learner in the traditional sense.\n",
        "  * Bell sampling does not take training examples as input. Instead, it takes measurements of a quantum state as input.\n",
        "  * Bell sampling does not output a hypothesis. Instead, it outputs a set of correlation measurements.\n",
        "  * However, Bell sampling can be used as part of a learning algorithm. For example, Bell sampling can be used to learn the Hamiltonian of a quantum system.\n",
        "* **stabilizer state** can be a concept class or hypothesis:\n",
        "  * Stabilizer states as a concept class\n",
        "    * A stabilizer state is a quantum state that is invariant under the action of a set of Pauli operators. Pauli operators are unitary operators that act on individual qubits.\n",
        "    * Stabilizer states are a useful concept in quantum computing because they can be efficiently prepared and manipulated.\n",
        "    * Stabilizer states can be used to represent a wide variety of quantum concepts and functions. For example, stabilizer states can be used to represent Boolean functions, linear functions, and unitary transformations.\n",
        "    * Therefore, stabilizer states can be used to define a concept class in computational learning theory.\n",
        "  * Stabilizer states as a hypothesis\n",
        "    * A stabilizer state can also be used as a hypothesis for learning a quantum concept or function.\n",
        "    * For example, we can use a quantum algorithm to learn a stabilizer state that represents a Boolean function.\n",
        "    * Once we have learned the stabilizer state, we can use it to evaluate the Boolean function on any input.\n",
        "    * Therefore, stabilizer states can be used as hypotheses in computational learning theory.\n",
        "* **Thermal state learning**:\n",
        "  * Thermal state learning is a technique for learning the thermal state of a quantum system. The thermal state of a quantum system is the state that the system will eventually reach if it is allowed to interact with its environment for a long time.\n",
        "  * Thermal state learning can be used to learn about the properties of quantum systems. However, it is not a concept class, hypothesis, or learner in the traditional sense."
      ],
      "metadata": {
        "id": "eR530ZtNhisC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning classical functions (through quantum encoding)**\n",
        "\n",
        "\n",
        "* **Boolean functions**: [Boolean functions](https://de.m.wikipedia.org/wiki/Boolesche_Funktion): Fourier analysis of functions on the Boolean cube, [Boolesche_Algebra](https://de.m.wikipedia.org/wiki/Boolesche_Algebra), [Boolean_circuit](https://en.m.wikipedia.org/wiki/Boolean_circuit) and [Analysis_of_Boolean_functions](https://en.m.wikipedia.org/wiki/Analysis_of_Boolean_functions): A concept c : {0,1}$^n$ → {0,1} by its N-bit truth-table (with N = 2$^n$), hence C ⊆ {0,1}$^N$\n",
        "\n",
        "  * An **N-bit truth table is a truth table that has N rows**, where each row represents one possible combination of truth values for the N input variables. The output variable is listed in the last column of the truth table.\n",
        "  \n",
        "  * For example, a **2-bit truth table would have 2 rows**, representing the two possible combinations of truth values for the two input variables. The output variable would be listed in the third column.\n",
        "  \n",
        "  * Below is an example of a 2-bit truth table for the logical function AND.\n",
        "\n",
        "> ```\n",
        "Input 1 | Input 2  | Output\n",
        "------- | -------- | --------\n",
        "0       | 0        | 0\n",
        "0       | 1        | 0\n",
        "1       | 0        | 0\n",
        "1       | 1        | 1\n",
        "```\n",
        "\n",
        "\n",
        "* **Parities**: The concept classes consisting of parities\n",
        "\n",
        "* **DNF formulas** [(Disjunctive Normal Form)](https://de.m.wikipedia.org/wiki/Disjunktive_Normalform)\n",
        "\n",
        "  * it is not known whether DNF is PAC-learnable\n",
        "\n",
        "* **Juntas**: Special case of DNF: (log n) juntas.\n",
        "\n",
        "  * A k-junta is a Boolean function that depends on at most k variables. In other words, a k-junta is a function f : {0, 1}^n → {0, 1} such that there exists a set S ⊆ [n] of size |S| ≤ k such that f(x) = f(y) for all x, y ∈ {0, 1}^n such that x and y agree on all variables in S.\n",
        "\n",
        "  * K-juntas are a natural class of functions to study in computational learning theory because they are relatively simple to learn. In fact, it is known that k-juntas can be learned in time polynomial in n and k. In circuit complexity, k-juntas can be used to lower bound the complexity of certain problems. Here are some examples of k-juntas:\n",
        "\n",
        "    * The function f(x) = x1 AND x2 AND x3 is a 3-junta.\n",
        "    * The function f(x) = x1 OR x2 OR x3 is not a 3-junta, because it depends on all 3 variables.\n",
        "    * The function f(x) = ¬x1 is a 1-junta.\n",
        "\n",
        "* **Sparse functions**, can be learned under the uniform distribution in the QSQ model."
      ],
      "metadata": {
        "id": "bylhFiAhcFVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning physical quantum states (through quantum encoding)** with classical or quantum computers\n",
        "\n",
        "* **Quantum state identification problem**: *Fully learning arbitrary quantum states could require exponentially many copies of the unknown state. But: Look at physical subclasses of quantum states which can be learned using polynomially many copies. So what classes of quantum states are learnable efficiently and why some classes of states are hard to learn? (Source: Survey on the complexity of learning quantum states)*\n",
        "* Learning **Quantum k-uniform states**\n",
        "* Learning **Stabilizer states** (Clifford gates)\n",
        "* Learning **Gibbs states**\n",
        "* Learning **States from Clifford hierarchy** and **Distributions induced by Clifford circuits** (*can be learned in the QSQ framework, however, if we add a single T gate, then classical SQ learning the output distribution is as hard as learning parities with noise*)\n",
        "* Learning **Circuits with non-Clifford gates**\n",
        "* Learning **Phase states**\n",
        "* Learning **Matrix Product States**\n",
        "* Learning **Product States**\n",
        "\n"
      ],
      "metadata": {
        "id": "nSmcViuznOTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept Class and Hypothesis Class** - What can be learnt?\n",
        "\n",
        "The [Concept Class](https://en.m.wikipedia.org/wiki/Concept_class) (=limited by our use case) contains the true relationships (all possible things) we want to learn = Linear and non-linear functions for a spam filter.\n",
        "  * The \"true\" function or the actual relationship might be unknown to us. Our goal in learning is to approximate it.\n",
        "  * A concept class is a set of **all possible concepts that can be learned**. A concept is a set of inputs that produce the same output. For example, the concept class is the set of all possible images of cats and dogs. The hypothesis class is the set of all possible neural networks with 3 layers. The learner is the gradient descent algorithm.\n",
        "  * See also [Concept learning](https://en.m.wikipedia.org/wiki/Concept_learning)\n",
        "\n",
        "The [Hypothesis Class](https://en.m.wikipedia.org/wiki/Hypothesis_Theory) (=limited by our learner from space of concept classes, used to approximate a concept) (e.g. neural network or ID3 decision tree) : While the concept class denotes the true relationships, the hypothesis class is the set of functions that a learning algorithm considers when trying to produce a model from dat\n",
        "  * = Set of models we consider based on our chosen learning method (set of all possible ways we can represent the things we want to learn). Set of models or functions that our chosen learning algorithm can possibly produce = only linear functions (This might miss out on some true nonlinear relationships present in the concept class).\n",
        "  * goal of a learning algorithm (the learner) is to probably approximate some unknown target concept c ∈ C from random labeled examples (from \"Optimal Quantum Sample Complexity of Learning Algorithms\")\n",
        "  * An ideal situation is when our chosen hypothesis class contains the true function (from the concept class). However, this is not always the case, which is why choosing the right type of model (and thereby the right hypothesis class) is crucial in machine learning.\n",
        "  * Hypothesis Class: In classification in general, the hypothesis class is the **set of possible (classification) functions** you're considering; the **learning algorithm picks a function from the hypothesis class**. For a decision tree learner, the hypothesis class would just be the set of all possible decision trees. [Source](https://stats.stackexchange.com/questions/270324/what-is-a-hypothesis-class-in-svm)\n",
        "  * Example: **A quantum PAC learner is given copies of the quantum example state, performs a POVM (where each outcome of the POVM is associated with an hypothesis) and outputs the resulting hypothesis.** *from: Survey on the complexity of learning quantum states,page 20*\n",
        "\n",
        "**Differentiate:** [Induktive_Verzerrung](https://de.wikipedia.org/wiki/Induktive_Verzerrung), [Hypothesenraum](https://de.wikipedia.org/wiki/Hypothesenraum), [Versionsraum](https://de.wikipedia.org/wiki/Versionsraum) und [Sample_space (Ergebnisraum)](https://en.m.wikipedia.org/wiki/Sample_space)"
      ],
      "metadata": {
        "id": "TJyjPP6sPHXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept class**\n",
        "\n",
        "A concept class in computational learning theory is a set of target concepts or functions that a learning algorithm is trying to learn. For example, a concept class could be the set of all Boolean functions, or the set of all linear functions.\n",
        "\n",
        "**Hypothesis**\n",
        "\n",
        "A hypothesis in computational learning theory is a candidate solution to a learning problem. For example, a hypothesis for learning a Boolean function could be a decision tree, or a neural network.\n",
        "\n",
        "**Learner**\n",
        "\n",
        "A learner in computational learning theory is an algorithm that takes training examples as input and outputs a hypothesis. The goal of the learner is to find a hypothesis that is consistent with the training data and that will generalize well to new data.\n",
        "\n",
        "Here are some specific examples of concept classes, hypotheses, and learners in computational learning theory:\n",
        "\n",
        "**Concept class:** The set of all Boolean functions\n",
        "**Hypothesis:** A decision tree\n",
        "**Learner:** The ID3 algorithm\n",
        "\n",
        "**Concept class:** The set of all linear functions\n",
        "**Hypothesis:** A linear regression model\n",
        "**Learner:** The least squares algorithm\n",
        "\n",
        "**Concept class:** The set of all images of cats\n",
        "**Hypothesis:** A convolutional neural network\n",
        "**Learner:** The Adam optimizer\n",
        "\n",
        "It is important to note that a learning algorithm may use different hypotheses for different concept classes. For example, the ID3 algorithm can be used to learn decision trees for any concept class, but the least squares algorithm can only be used to learn linear functions.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Concept classes, hypotheses, and learners are the fundamental building blocks of computational learning theory. By understanding these concepts, we can better understand how machine learning algorithms work and how to choose the right algorithm for a particular problem."
      ],
      "metadata": {
        "id": "J9VhxgFyXAQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In computational learning theory, there are three fundamental components: concept class, hypothesis, and learner. Let's define each of these concepts and provide examples:\n",
        "\n",
        "1. Concept Class:\n",
        "   - Concept class refers to the set of all possible concepts or functions that the learner is trying to learn or approximate. It represents the underlying target concept that the learner is trying to capture.\n",
        "   - Example: In a binary classification problem where you want to distinguish between spam and non-spam emails, the concept class could be the set of all possible decision boundaries that separate spam emails from non-spam emails in the feature space. Each decision boundary represents a different concept in the concept class.\n",
        "\n",
        "2. Hypothesis:\n",
        "   - Hypothesis is a subset of the concept class that the learner uses to approximate or represent the target concept. It's the set of functions or models that the learner considers as potential solutions to the learning problem.\n",
        "   - Example: In the context of linear classification, the hypothesis space could be the set of all possible linear classifiers (e.g., support vector machines, logistic regression). Each linear classifier within this space is a hypothesis used to approximate the target concept.\n",
        "\n",
        "3. Learner:\n",
        "   - The learner is the algorithm or method used to search through the hypothesis space and select the best hypothesis that approximates the target concept based on available training data.\n",
        "   - Example: For supervised learning tasks, a learner could be a decision tree algorithm, a neural network, or a k-nearest neighbors classifier. The learner's role is to use the training data to find the hypothesis (model) that minimizes a certain loss function or error measure.\n",
        "\n",
        "To summarize with an example:\n",
        "Suppose you have a binary classification problem of identifying whether an email is spam or not. The concept class includes all possible ways to separate spam and non-spam emails in the feature space. Hypotheses within this class might include different decision trees, support vector machines with various kernel functions, or neural network architectures. The learner is the specific algorithm you choose to train on your labeled data, like a decision tree learning algorithm or a neural network training algorithm. The learner's job is to select the best hypothesis (e.g., the best decision tree or neural network parameters) to approximate the underlying concept of distinguishing spam from non-spam emails."
      ],
      "metadata": {
        "id": "MzFCmSojXBxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning [quantum circuit complexity](https://en.m.wikipedia.org/wiki/Circuit_complexity)**\n",
        "\n",
        "*For example Minimum Circuit Size Problem (MCSP), a Meta-Complexity problem*\n",
        "\n",
        "> [Linear growth of quantum circuit complexity](https://arxiv.org/abs/2106.05305): Consider constructing deeper and deeper circuits for an n-qubit system, by applying random two-qubit gates. At what rate does the circuit complexity increase?\n",
        "\n",
        "> [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004): Determining the quantum circuit complexity of a unitary operation is closely related to the problem of finding minimal length paths in a particular curved geometry.\n",
        "\n",
        "* [Quantum Computation as Geometry](https://arxiv.org/abs/quant-ph/0603161)\n",
        "\n",
        "\n",
        "* [Linear growth of quantum circuit complexity](https://www.nature.com/articles/s41567-022-01539-6)\n",
        "\n",
        "* [On the average-case complexity of learning output distributions of quantum circuits](https://arxiv.org/abs/2305.05765)\n",
        "\n",
        "* [Quantum circuit complexity](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory)"
      ],
      "metadata": {
        "id": "tHuhYcixzHNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Undecidability of Learnability (Learnability = Computable?)**\n",
        "* https://arxiv.org/abs/2106.01382\n",
        "* However, there is no known general-purpose procedure for rigorously evaluating whether newly proposed models indeed successfully learn from data. We show that such a procedure cannot exist.\n",
        "* For PAC binary classification, uniform and universal online learning, and exact learning through teacher-learner interactions, learnability is in general undecidable, both in the sense of independence of the axioms in a formal system and in the sense of uncomputability.\n",
        "* Our proofs proceed via computable constructions of function classes that encode the consistency problem for formal systems and the halting problem for Turing machines into complexity measures that characterize learnability.\n",
        "* Our work shows that undecidability appears in the theoretical foundations of machine learning: There is no one-size-fits-all algorithm for deciding whether a machine learning model can be successful. **We cannot in general automatize the process of assessing new learning models.**\n",
        "* Figure 1: A depiction of our line of reasoning. “Complexity” is to be understood in terms of VC-dimension, teaching dimension, Littlestone dimension, or Littlestone trees, depending on the learning model. To conclude undecidability, we use G ̈odel’s second incompleteness theorem and the uncomputability of the halting problem, respectively.\n",
        "* [Lat96] made an early investigation into the relationship between computability and learnability. The main question in [Lat96] is whether and under which notions of “learnability” one can consider an uncomputable problem to be learnable. More precisely, [Lat96] considered the task of learning the halting problem relative to an oracle."
      ],
      "metadata": {
        "id": "yYkMF8Fk3lwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Learning unitary dynamics, which is a fundamental primitive for a range of QML algorithms (from: Out-of-distribution generalization for learning quantum dynamics)*\n",
        "\n",
        "* **Quantum dynamics learning**: At its simplest, the target unitary could be the unknown dynamics of an experimental quantum system. For this case, which has close links with quantum sensing [26] and Hamiltonian learning [27– 29], the aim is essentially to learn a digitalization of an analog quantum process.\n",
        "* **Quantum compilation learning**: Alternatively, the target unitary could take the form of a known gate sequence that one seeks to compile into a shorter depth circuit or a particular structured form\n",
        "\n",
        "The compilation could be performed either on a quantum computer, see Fig. 1c), or entirely classically, see Fig. 1d)\n"
      ],
      "metadata": {
        "id": "2NP3VL4OlRWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Shadows to Learn Quantum States**\n",
        "\n",
        "* Machine Learning Aids Classical Modeling of Quantum Systems. By using “classical shadows,” ordinary computers can beat quantum computers at the tricky task of understanding quantum behaviors.\n",
        "\n",
        "* By combining a new way of modeling quantum systems with increasingly sophisticated machine learning algorithms, researchers have established a method for classical machines to model and predict quantum behavior.\n",
        "\n",
        "https://www.quantamagazine.org/machine-learning-aids-classical-modeling-of-quantum-systems-20230914/"
      ],
      "metadata": {
        "id": "XE06-XJ0mf7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Learner (Model)*"
      ],
      "metadata": {
        "id": "qMfQjLMqmzuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The Learner or learning algorithm (e.g. gradient descent, logic regression, linear regression, decision trees and random forests) is **the algorithm that produces a specific model from the hypothesis class using data** - the learner is the algorithm that **chooses the best hypothesis from the hypothesis class** given the training data.*"
      ],
      "metadata": {
        "id": "tvnsAeoaoQm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAC learning**\n",
        "\n",
        "* **PAC Learning**: is a method to measure sample complexity. The Probably Approximately Correct (PAC) learning framework provides a formal definition of learning and can be used to derive bounds on the sample complexity needed to learn a function to a given degree of accuracy with a certain probability.\n",
        "\n",
        "* PAC learning is a framework in computational learning theory where an algorithm aims to learn a target function based on samples such that it performs well on unseen data with high probability.\n",
        "\n",
        "* **For linear regression to be a PAC learner, the following conditions are typically required:**\n",
        "\n",
        "  * The true relationship between the variables is (approximately) linear.\n",
        "  * The noise in the data is bounded.\n",
        "  * The features (input data) have a bounded range.\n",
        "\n",
        "* Under these conditions, with a sufficiently large sample size, the linear regression model can be shown to generalize well to unseen data with high probability, thereby satisfying the PAC learning criteria.\n",
        "\n",
        "> **However, if the true underlying relationship is non-linear or if the noise is unbounded, then linear regression might not be a PAC learner.**\n",
        "\n",
        "* Several algorithms and models in machine learning can be considered PAC learners under appropriate conditions. Here are some examples:\n",
        "\n",
        "  1. **Decision Trees**: Decision tree algorithms, like ID3, C4.5, and CART, can be PAC learners under certain conditions. They can approximate discrete-valued functions well given a sufficient number of samples.\n",
        "\n",
        "  2. **Finite Automata Learning**: Algorithms that learn regular languages, like the L* algorithm, can be seen as PAC learners for certain classes of regular languages.\n",
        "\n",
        "  3. **Neural Networks**: Under certain assumptions, feed-forward neural networks with a fixed number of hidden layers can be PAC learners. However, their sample complexity can be high, especially as the size of the network grows.\n",
        "\n",
        "  4. **k-Nearest Neighbors (k-NN)**: Under certain conditions, the k-NN algorithm can be a PAC learner, particularly when the data is uniformly distributed.\n",
        "\n",
        "  5. **Support Vector Machines (SVM)**: SVMs can be considered PAC learners when the margin between classes is reasonably large.\n",
        "\n",
        "  6. **Boosting Algorithms**: Algorithms like AdaBoost, which combine multiple weak learners to create a strong learner, can be seen as PAC learners under the right conditions.\n",
        "\n",
        "  7. **Monotone DNF**: Algorithms that learn disjunctive normal form (DNF) expressions where all literals are positive can be PAC learners.\n",
        "\n",
        "* It's worth noting that for any learning algorithm to be a PAC learner, it must be able to produce hypotheses that generalize well to unseen data with high probability, given polynomially many samples in the size of the concept/class, the error bound, and the confidence level. The specific conditions under which these algorithms are PAC learnable can vary based on the complexity of the model, the nature of the data, and the distribution from which the data is drawn.\n",
        "\n",
        "*Under which conditions is a neural network or a k-Nearest Neighbors (k-NN) not a PAC learner?*\n",
        "\n",
        "* Both neural networks and k-Nearest Neighbors (k-NN) have conditions under which they might not be PAC learners:\n",
        "\n",
        "  1. **Neural Networks**:\n",
        "\n",
        "   - **Complexity and Overfitting**: If the neural network is too complex (e.g., too many layers or neurons) relative to the amount of training data available, it can overfit. Overfitting means the network memorizes the training data rather than generalizing from it, leading to poor performance on unseen data.\n",
        "   \n",
        "   - **Non-IID Data**: Neural networks assume that data is identically and independently distributed (IID). If there's a temporal or spatial structure in the data, without appropriate handling, a standard neural network might not generalize well.\n",
        "   \n",
        "   - **Unbounded Activation Functions**: Activation functions that aren't bounded might lead to weights that grow indefinitely, causing issues with learning.\n",
        "   \n",
        "   - **Unsuitable Data Distribution**: If the data distribution is not well-suited for the neural network's assumptions or if it changes over time (non-stationary), the network might not be a PAC learner for that distribution.\n",
        "\n",
        "  2. **k-Nearest Neighbors (k-NN)**:\n",
        "\n",
        "   - **Curse of Dimensionality**: As the dimensionality of the dataset increases, the volume of the space increases exponentially, and data becomes sparse. k-NN suffers in high-dimensional spaces because the notion of \"nearness\" becomes less meaningful. This can cause k-NN to be a poor learner in high dimensions unless the dataset is exceptionally large.\n",
        "   \n",
        "   - **Non-uniform Feature Scales**: If features have different scales and aren't normalized, the distance measure used by k-NN can be dominated by certain features, making it a poor learner.\n",
        "   \n",
        "   - **Noisy Data**: k-NN is sensitive to noise in the dataset. If a significant portion of the data has errors or misclassifications, k-NN's performance can degrade, and it may not be a PAC learner for very noisy datasets.\n",
        "   \n",
        "   - **Variable Densities**: If some regions of the input space are densely populated and others are sparse, k-NN might not generalize well across all regions.\n",
        "\n",
        "* For both neural networks and k-NN, it's important to note that being a PAC learner is about the ability to generalize to unseen data with high probability given a polynomial number of samples. If either model fails to meet this criterion under certain conditions, it's not considered a PAC learner under those conditions.\n",
        "\n",
        "*Why is a query learner not a PAC learner?*\n",
        "\n",
        "* A query learner and a PAC learner are not inherently mutually exclusive. They are two different frameworks within computational learning theory that describe the manner and conditions under which learning happens. However, the methods by which they acquire data and their learning objectives are what differentiate them:\n",
        "\n",
        "  1. **Data Acquisition**:\n",
        "   - **Query Learner**: The learner can actively ask an oracle about specific inputs or hypotheses. The types of questions might include membership queries (asking if a specific instance belongs to the target concept) and equivalence queries (proposing a hypothesis and asking if it's correct). There might be other types of queries as well.\n",
        "   - **PAC Learner**: The learner passively receives random samples drawn from a distribution. It doesn't get to choose specific examples it wants to learn from.\n",
        "\n",
        "  2. **Learning Objective**:\n",
        "   - **Query Learner**: Depending on the model, the goal might be exact learning, where the learner tries to find a hypothesis that exactly matches the target concept. This is often the objective in models that use equivalence queries.\n",
        "   - **PAC Learner**: The goal is to find a hypothesis that is probably approximately correct. That is, with high probability, the hypothesis should be approximately accurate on new, unseen examples drawn from the same distribution.\n",
        "\n",
        "* Given these distinctions, a learning algorithm could theoretically be both a query learner and a PAC learner under different circumstances or with different assumptions. For instance, one might imagine a scenario where a learner uses queries to gather information and then ensures that its hypothesis is probably approximately correct for a given distribution.\n",
        "\n",
        "* However, the traditional definitions of these models focus on their distinct characteristics, which is why they are often treated as separate learning paradigms in computational learning theory.\n",
        "\n",
        "\n",
        "*Leslie Valiant’s Probably Approximately Correct (PAC) model gives a precise complexity- theoretic definition of what it means for a concept class to be (efficiently) learnable. - Source: Optimal Quantum Sample Complexity of Learning Algorithms (2017)*\n",
        "\n",
        "* [PAC Learning](https://en.m.wikipedia.org/wiki/Error_tolerance_(PAC_learning)): learner is evaluated on its predictive power of a test set.\n",
        "\n",
        "  * Dimensionality measures are important tools in machine learning because they can be used to bound the sample complexity of learning algorithms. For example, the following theorem provides an upper bound on the sample complexity of PAC learning in terms of the VC dimension:\n",
        "\n",
        "  * **Theorem:** Let H be a hypothesis class with VC dimension d. Then, the sample complexity of PAC learning H is given by:\n",
        "\n",
        "  * $n >= O(d / ε^2 * ln(1/δ))$\n",
        "\n",
        "  * where ε is the desired error tolerance and δ is the desired confidence level.\n",
        "\n",
        "  * This theorem tells us that if we know the VC dimension of a hypothesis class, then we can bound the number of training examples needed to learn a hypothesis in that class with high probability.\n",
        "  \n",
        "  * In general, the VC dimension is the most widely used dimensionality measure in PAC learning. It is simple to compute and easy to interpret. However, there are other dimensionality measures that can be more effective for certain types of learning problems.\n",
        "  \n",
        "* [PAC learning](https://en.m.wikipedia.org/wiki/Probably_approximate) (Probably approximate learner): A PAC learner is a machine learning algorithm that can learn a function from a set of labeled examples with high accuracy. The PAC learner is given a set of labeled examples, where each example is a pair of an input and its corresponding output. The PAC learner then learns a function that maps from inputs to outputs. The function learned by the PAC learner should have high accuracy, meaning that it should output the correct output for most inputs.\n",
        "\n",
        "* Hypothesis class is finite and labeling function is consistent with some hypothesis in the hypothesis class. Ideal for more simpler applications like Classification, regression, clustering.\n",
        "\n",
        "* PAC learning (Probably Approximately Correct learning) is a framework for machine learning that allows us to measure the accuracy of a learning algorithm. In PAC learning, we assume that the learner has access to a training set of labeled examples, and the goal is to learn a hypothesis that predicts the label of new, unseen examples with high accuracy.\n",
        "\n",
        "* **Proper quantum PAC learner** with optimal sample complexity, i.e., one whose output hypothesis lies in C itself\n",
        "\n",
        "* ps: sample complexity for the PAC and agnostic models, quantum examples do not provide an advantage (*Survey of Quantum Learning Theory*)\n",
        "\n",
        "* „quantum sample complexity of PAC learning“ versus „sample complexity of PAC learning n-qubit quantum states“?\n",
        "\n",
        "  * The quantum sample complexity of PAC learning refers to the number of quantum examples needed to learn a concept class with probability 1-delta, where each example is a coherent quantum state.\n",
        "\n",
        "  * The sample complexity of PAC learning n-qubit quantum states refers to the number of classical examples needed to learn a concept class of n-qubit quantum states, where each example is a classical description of an n-qubit quantum state.\n",
        "\n",
        "  * In general, the quantum sample complexity of PAC learning is lower than the sample complexity of PAC learning n-qubit quantum states. This is because quantum examples can contain more information than classical examples.\n",
        "\n",
        "  * For example, consider the task of learning a concept class of binary functions. A classical example of a binary function is a pair of inputs (x, y), where x is a binary number and y is the output of the function on x. A quantum example of a binary function is a quantum state |ψ⟩ that encodes the function. **It can be shown that the quantum sample complexity of PAC learning binary functions is O(log(d)/ε), where d is the VC dimension of the concept class and ε is the error tolerance. The classical sample complexity of PAC learning binary functions is O(d/ε), which is larger than the quantum sample complexity.**\n",
        "\n",
        "  * This shows that quantum examples can be more powerful than classical examples for some learning tasks. However, it is important to note that the quantum sample complexity of PAC learning is still an active area of research, and there are many open questions."
      ],
      "metadata": {
        "id": "IqioVhxR7yy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear regression is an example of a PAC learner. A PAC learner is a type of machine learning algorithm that can learn a concept class with high probability, given a sufficient number of training examples. The concept class is the set of all possible functions that the learner is trying to learn. In the case of linear regression, the concept class is the set of all linear functions.*\n",
        "\n",
        "* Linear regression has been shown to be PAC learnable under a variety of different conditions. For example, if the training data is drawn from a distribution where the target values are generated by a linear function, then linear regression can learn the correct function with high probability, given a sufficient number of training examples.\n",
        "\n",
        "* However, it is important to note that the PAC learnability of linear regression depends on the specific hypothesis class that is being used. For example, if the hypothesis class includes all possible linear functions, then linear regression is not PAC learnable. This is because there are infinitely many linear functions, and it is not possible to learn any infinite hypothesis class with a finite number of training examples.\n",
        "\n",
        "* In practice, linear regression is often used with a restricted hypothesis class. For example, the hypothesis class might be restricted to only include linear functions that have a certain number of parameters. This makes the problem of learning the correct function more feasible, and linear regression can often be used to learn the correct function with a relatively small number of training examples.\n",
        "\n",
        "* Here is an example of how linear regression can be used as a PAC learner:\n",
        "\n",
        "* Suppose we want to learn a linear function that predicts the weight of a person based on their height. We can collect a training dataset of people's heights and weights. Then, we can use a linear regression algorithm to learn a linear function that fits the training data.\n",
        "\n",
        "* The PAC learnability of linear regression tells us that, if the training data is drawn from a distribution where the weights are generated by a linear function of height, then the linear regression algorithm will learn the correct function with high probability, given a sufficient number of training examples.\n",
        "\n",
        "* Of course, we cannot guarantee that the linear regression algorithm will always learn the correct function. However, the PAC learnability of linear regression tells us that the algorithm will learn the correct function with high probability, if the training data is sampled from the correct distribution."
      ],
      "metadata": {
        "id": "OrqN6xS5PYJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, a PAC learner would be a learner in computational learning theory. A PAC learner is a machine learning algorithm that can learn a hypothesis that generalizes well to new examples, with high probability.\n",
        "\n",
        "The PAC model is a theoretical framework for evaluating the performance of learning algorithms. In the PAC model, the learner is given a set of training examples and is asked to learn a hypothesis that generalizes well to new examples. The hypothesis is a function that maps from inputs to outputs, and the learner's goal is to find a hypothesis that is close to the target function, which is the unknown function that generated the training examples.\n",
        "\n",
        "A PAC learner is a learner that can learn a hypothesis that generalizes well to new examples, with high probability. This means that the learner can learn a hypothesis that will make few mistakes on new examples, most of the time.\n",
        "\n",
        "PAC learners are important because they provide a way to evaluate the performance of learning algorithms in a rigorous way. The PAC model is also useful for understanding the theoretical limitations of learning algorithms.\n",
        "\n",
        "Here are some examples of PAC learners:\n",
        "\n",
        "* Linear regression\n",
        "* Logistic regression\n",
        "* Decision trees\n",
        "* Support vector machines\n",
        "* Neural networks\n",
        "\n",
        "These algorithms can all be used to learn hypotheses that generalize well to new examples, with high probability.\n",
        "\n",
        "It is important to note that the PAC model is a theoretical framework, and it is not always possible to find a PAC learner that works well in all practical settings. However, the PAC model provides a useful way to think about the problem of learning and to evaluate the performance of learning algorithms."
      ],
      "metadata": {
        "id": "lf78dXP8PpNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA stands for AutoRegressive Integrated Moving Average, and it is a statistical model that is used to forecast time series data. ARIMA models are based on the assumption that the future values of a time series can be predicted from its past values.\n",
        "\n",
        "In the context of computational learning theory, ARIMA can be considered a learner, but it is a special type of learner that is designed for time series forecasting. ARIMA models learn a hypothesis that maps from past time series values to future time series values. The hypothesis is a linear function of the past values, and it is estimated using a statistical procedure called maximum likelihood estimation.\n",
        "\n",
        "ARIMA models are widely used in a variety of applications, such as forecasting sales, stock prices, and weather patterns. They are relatively simple to implement and interpret, and they can be used to generate accurate forecasts for a variety of time series data.\n",
        "\n",
        "Here is an example of how ARIMA could be used as a learner in computational learning theory:\n",
        "\n",
        "Suppose we have a time series of daily sales data for a particular product. We want to learn a hypothesis that can be used to predict future sales. We can use an ARIMA model to learn this hypothesis.\n",
        "\n",
        "First, we would need to identify the order of the ARIMA model. The order of the model determines how many past values of the time series are used to predict future values. We can use a statistical procedure called autocorrelation to help us identify the order of the model.\n",
        "\n",
        "Once we have identified the order of the model, we can estimate the parameters of the model using maximum likelihood estimation. This procedure will give us a hypothesis that is the best fit to the training data.\n",
        "\n",
        "We can then use the hypothesis to predict future sales. To do this, we would simply plug the past values of the time series into the hypothesis. The output of the hypothesis will be the predicted value for the next time step.\n",
        "\n",
        "ARIMA models are a powerful tool for time series forecasting, and they can be used as learners in computational learning theory. However, it is important to note that ARIMA models are only suitable for forecasting time series data that is stationary. Stationary time series data is data that has a constant mean, variance, and autocorrelation over time. If the time series data is not stationary, then it may be necessary to transform the data before using an ARIMA model."
      ],
      "metadata": {
        "id": "xSVXX1H7PxnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the PAC model, the learner is given a set of training examples and is asked to learn a hypothesis that generalizes well to new examples. The hypothesis is a function that maps from inputs to outputs, and the learner's goal is to find a hypothesis that is close to the target function, which is the unknown function that generated the training examples.\n",
        "\n",
        "The PAC model is used to measure the generalization error of a learning algorithm. The generalization error is the probability that the hypothesis will make a mistake on a new example. The PAC model guarantees that, with high probability, the learning algorithm will output a hypothesis with low generalization error.\n",
        "\n",
        "And what would be an example of a learner in this context?\n",
        "\n",
        "An example of a learner in the context of computational learning theory is a machine learning algorithm, such as linear regression, decision trees, or support vector machines. These algorithms are given a set of training examples and are asked to learn a hypothesis that generalizes well to new examples.\n",
        "\n",
        "For example, a linear regression algorithm could be used to learn a hypothesis that predicts the price of a house based on its square footage, number of bedrooms, and other features. The algorithm is given a set of training examples, which are houses with known prices and features. The algorithm then learns a linear function that best fits the training data. This function can then be used to predict the price of a new house, given its features.\n",
        "\n",
        "Another example of a learner is a decision tree algorithm. Decision tree algorithms learn a hypothesis by constructing a tree structure, where each node in the tree represents a decision and each leaf node represents a prediction. The algorithm is given a set of training examples, and it constructs a tree that best separates the positive examples from the negative examples. The tree can then be used to predict the class of a new example by traversing the tree from the root node to a leaf node.\n",
        "\n",
        "These are just two examples of learners in computational learning theory. There are many other machine learning algorithms that can be used as learners in this context.\n",
        "\n",
        "It is important to note that the PAC model is a theoretical framework, and it is not always possible to find a learning algorithm that works well in all practical settings. However, the PAC model provides a useful way to think about the problem of learning and to evaluate the performance of learning algorithms."
      ],
      "metadata": {
        "id": "NkAyGxjLP8__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One example of a quantum PAC learner is the Grover's algorithm. Grover's algorithm is a quantum search algorithm that can search through a database of unsorted items in polynomial time, whereas classical search algorithms require exponential time.\n",
        "\n",
        "Grover's algorithm can be used to implement a quantum PAC learner by using it to search through the space of all possible hypotheses to find one that is consistent with the training data. This can be done by constructing a quantum oracle that takes a hypothesis as input and outputs 1 if the hypothesis is consistent with the training data and 0 otherwise.\n",
        "\n",
        "Once the oracle is constructed, Grover's algorithm can be used to search for a hypothesis that outputs 1. Grover's algorithm will find such a hypothesis in polynomial time, even if the space of all possible hypotheses is very large.\n",
        "\n",
        "Another example of a quantum PAC learner is the HHL algorithm. The HHL algorithm is a quantum algorithm for solving linear systems of equations. It can be used to implement a quantum PAC learner by using it to solve the linear system of equations that represents the training data.\n",
        "\n",
        "Once the linear system of equations is solved, the coefficients of the solution can be interpreted as the weights of a hypothesis. This hypothesis can then be used to predict the labels of new examples.\n",
        "\n",
        "Both Grover's algorithm and the HHL algorithm have been shown to achieve polynomial speedups over classical PAC learners for certain types of concepts. However, it is important to note that these algorithms are still in their early stages of development, and it is not yet clear how well they will perform on real-world problems.\n",
        "\n",
        "Here is a simple example of how Grover's algorithm could be used to implement a quantum PAC learner:\n",
        "\n",
        "\n",
        "1. Initialize a quantum register to be in the superposition of all possible hypotheses.\n",
        "2. Apply the quantum oracle to the quantum register.\n",
        "3. Measure the quantum register.\n",
        "4. If the measurement result is 1, then the hypothesis is consistent with the training data and the algorithm terminates.\n",
        "5. Otherwise, apply the Grover diffusion operator to the quantum register and repeat steps 2-4.\n",
        "\n",
        "\n",
        "The Grover diffusion operator is a quantum operator that amplifies the probability of measuring the correct hypothesis. After a few iterations of the Grover algorithm, the probability of measuring the correct hypothesis will be very high.\n",
        "\n",
        "I hope this helps!"
      ],
      "metadata": {
        "id": "Lwlt5nzkwpfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agnostic learning**:\n",
        "\n",
        "  * Hypothesis class is finite but labeling function is not assumed to be consistent with any hypothesis in the hypothesis class. Ideal for more general applications, such as natural language processing and computer vision.\n",
        "  * The PAC model assumes that the labeled examples are generated according to a target concept c ∈ C . However, in many learning situations that is not a realistic assumption, for example when the examples are noisy in some way or when we have no reason to believe there is an underlying target concept at all. The agnostic model of learning, introduced by Haussler [Hau92] and Kearns et al. [KSS94], takes this into account (Optimal Quantum Sample Complexity of Learning Algorithms).\n",
        "\n",
        "  * Agnostic learning is a more general concept than PAC learning. PAC learning is a framework for machine learning that guarantees that a learner can find a hypothesis that is approximately correct with high probability, given a finite amount of training data. The learner does this by making two assumptions:\n",
        "\n",
        "    1. The hypothesis class is finite.\n",
        "    2. The labeling function is consistent with some hypothesis in the hypothesis class.\n",
        "\n",
        "  * Agnostic learning relaxes the second assumption. In agnostic learning, the learner does not assume that the labeling function is consistent with any hypothesis in the hypothesis class. This makes agnostic learning a more challenging problem, but it also allows the learner to learn more complex concepts.\n",
        "\n",
        "  * In other words, PAC learning is a special case of agnostic learning where the labeling function is consistent with some hypothesis in the hypothesis class."
      ],
      "metadata": {
        "id": "8DexccI_oHSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Online learning**\n",
        "  * The online model can be viewed as a variant of tomography and PAC learning (Survey on the complexity of learning quantum states, page 15)\n",
        "  * [PAC Learning and Online Learning](https://courses.corelab.ntua.gr/pluginfile.php/7949/course/section/925/lecture2.pdf)"
      ],
      "metadata": {
        "id": "8vQUT-kMoE3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Active Learning**\n",
        "\n",
        "* Semi-supervised learning problems include active learning, where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels."
      ],
      "metadata": {
        "id": "WQ80qL7y_oSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exact learning** (uses queries, not sample!)\n",
        "\n",
        "* This is a specific type of query learning where the objective is to learn the target concept exactly.\n",
        "\n",
        "* Exact learning is a part of PAC learning. Exact learning is a special case of PAC learning where the learner is guaranteed to learn the correct hypothesis with certainty, given enough training examples. This is in contrast to PAC learning, where the learner is only guaranteed to learn a hypothesis that is accurate with high probability.\n",
        "* **Query complexity of exact learning**. In quantum computational learning theory, exact learning is typically studied in the context of Probably Approximately Correct (PAC) learning, where the learner is given a set of labeled examples, and is asked to learn a hypothesis that classifies new examples with high accuracy.\n",
        "* There are a number of different ways to perform exact learning in quantum computational learning theory. One approach is to use quantum algorithms to perform tomography on the target function. However, this approach is often inefficient, as it requires a large number of measurements.\n",
        "* Another approach is to use quantum algorithms to perform machine learning directly, without the need for tomography. This approach is more promising, as it can be more efficient. However, it is still a relatively new area of research, and there are many open problems.\n",
        "* (N,M)-quantum query complexity of exact learning. Page 8 under \"4.1 Query complexity of exact learning\" notation: **{s ∈ S : s_i = 0}** in this [paper](https://arxiv.org/abs/1701.06806)\n",
        "\n",
        "  * The notation {s ∈ S : s_i = 0} is a set comprehension notation that selects all elements s in a set S such that the ith index of s is equal to 0. In other words, it is the set of all elements in S whose ith digit is 0.\n",
        "    * For example, if S is the set {1, 2, 3, 4, 5}, then the set {s ∈ S : s_i = 0} is the set {0, 2, 4}.\n",
        "    * Compare in binary: 0 : **00**, 1 : **01**, 2: **10**, 3: **11**, 4 : **100**, 5: **101** (look at the last digit, the ith digit)\n",
        "\n",
        "  * The (N,M)-quantum query complexity of exact learning is **the minimum number of queries to an oracle that a quantum algorithm needs to make in order to learn a Boolean function f : {0, 1}^n → {0, 1} with error probability at most 1/M, where N is the number of input variables and M is a positive integer**.\n",
        "\n",
        "  * In other words, the (N,M)-quantum query complexity is the quantum analogue of the (N,M)-classical query complexity, which is the minimum number of queries to an oracle that a classical algorithm needs to make in order to learn f with error probability at most 1/M.\n",
        "\n",
        "  * The (N,M)-quantum query complexity of exact learning is a difficult problem to study, and it is not known for many functions f. However, there are some functions for which the (N,M)-quantum query complexity is known to be better than the (N,M)-classical query complexity.\n",
        "\n",
        "  * For example, it is known that the (N,M)-quantum query complexity of learning a k-junta with error probability at most 1/M is O(n(log n)/k) queries, while the (N,M)-classical query complexity is Ω(n^k/k^2) queries. This means that a quantum algorithm can learn a k-junta with fewer queries than a classical algorithm, if M is sufficiently large.\n",
        "\n",
        "*Exact Learning with Membership and Equivalence Queries: In this model, the learner tries to identify a target concept exactly (not approximately) by asking two types of queries:*\n",
        "\n",
        "  * Membership Queries: The learner poses an input and asks the oracle if it belongs to the target concept.\n",
        "  * Equivalence Queries: The learner proposes a hypothesis and asks the oracle if it's equivalent to the target concept. If not, the oracle provides a counterexample."
      ],
      "metadata": {
        "id": "13qWjplwoCW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Learning**\n",
        "\n",
        "* This is a broader category where the learner has access to an oracle (similar to the membership and equivalence query model) but may have various types of queries at its disposal. The nature of the queries and the information that can be extracted determine the learnability of the target concept.\n",
        "\n",
        "* both query learning and exact learning involve the use of queries. However, the distinction lies in the objective of the learning and the broader context in which the terms are used.\n",
        "\n",
        "  1. **Objective**:\n",
        "   - **Query Learning**: The term \"query learning\" refers broadly to any learning model where the learner can actively query an oracle about the target concept. The queries might include membership queries, equivalence queries, and potentially other types of queries. The goal is not necessarily to learn the target concept exactly—it could be approximate or within certain bounds.\n",
        "   - **Exact Learning**: This is a specific type of query learning where the objective is to learn the target concept exactly. Exact learning often involves equivalence queries, where the learner proposes a hypothesis and asks the oracle if it exactly matches the target concept. If not, the oracle provides a counterexample.\n",
        "\n",
        "  2. **Types of Queries**:\n",
        "   - **Query Learning**: Can encompass various types of queries, depending on the specific learning model.\n",
        "   - **Exact Learning**: Primarily uses equivalence queries but may also use membership queries to refine its hypothesis based on the counterexamples received.\n",
        "\n",
        "3. **Context**:\n",
        "   - **Query Learning**: This is a broader term in computational learning theory, covering any model that uses some form of queries to gather information about the target concept.\n",
        "   - **Exact Learning**: This is a more specific model under the umbrella of query learning. It represents a subset of query learning models where the objective is to pinpoint the target concept without any approximation.\n",
        "\n",
        "In essence, all exact learning can be considered a form of query learning, but not all query learning aims for exact learning. Some query learning models might be content with approximate solutions or may have other objectives beyond exactness.\n"
      ],
      "metadata": {
        "id": "A25h4TlTNxRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Empirical Risk Minimization (ERM)**\n",
        "\n",
        "* Bounding the sample complexity of empirical risk minimization (ERM): ERM is a popular machine learning algorithm that learns a function by minimizing the empirical risk, which is the average loss on the training data.\n",
        "\n",
        "* Concentration inequalities can be used to show that ERM learns a function with a small generalization error with high probability, given a sufficient number of training samples."
      ],
      "metadata": {
        "id": "7Bm4nqqWQEJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Occam learning**\n",
        "\n",
        "* [Occam learning](https://en.m.wikipedia.org/wiki/Occam_learning): the objective of the learner is to output a succinct representation of received training data.\n",
        "* Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables and decision lists.\n",
        "* Occam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples."
      ],
      "metadata": {
        "id": "tUzyFOZagC2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Fourier features and Classical Surrogates for QML**\n",
        "\n",
        "* [Potential and limitations of random Fourier features for dequantizing quantum machine learning](https://scirate.com/arxiv/2309.11647): we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization. On a higher level, this work contributes to delineating the boundary between quantum and classical processes.\n",
        "\n",
        "* [Classical Surrogates for Quantum Learning Models](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.100803)"
      ],
      "metadata": {
        "id": "TBU-HI_ugksq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum statistical query model (QSQ)**\n",
        "  * e.g. quantum example oracle\n",
        "  * QSQ (Quantum statistical query model) is a model of quantum machine learning that allows us to ask queries about the distribution of a quantum state. In QSQ, the learner has access to a quantum oracle that can answer queries about the distribution of a quantum state, and the goal is to learn a hypothesis that predicts the output of the oracle with high accuracy.\n",
        "  * A quantum example oracle is a black box that takes a quantum state as input and gives a quantum state as output. The quantum example oracle is not accessible to the user, and the user only knows how to interact with it through a specific set of operations.\n",
        "  * The main difference between a quantum example oracle and a PAC learner is that a quantum example oracle provides the learner with quantum states, while a PAC learner provides the learner with labeled examples. Quantum states are more powerful than labeled examples (from PAC), because they can represent a superposition of multiple inputs and outputs. This means that a quantum example oracle can provide the learner with more information about the function being learned. ***As a result, quantum example oracles can be used to learn functions that are more difficult to learn with classical PAC learners.*** For example, it has been shown that DNF formulas can be learned efficiently with a quantum example oracle, but they are not known to be efficiently learnable with a classical PAC learner.\n",
        "  * Another example of a quantum oracle is the Deutsch-Jozsa algorithm. This algorithm takes a function f:{0,1}^n->{0,1} as input, and determines whether f is constant or balanced. A constant function is one that always outputs the same value, regardless of its input. A balanced function is one that outputs 1 half of the time and 0 half of the time. The Deutsch-Jozsa algorithm works by first creating a superposition of all possible inputs to f. It then applies the oracle to this superposition. If f is constant, then the oracle will not change the superposition. However, if f is balanced, then the oracle will flip the phase of half of the states in the superposition. Finally, the algorithm measures the superposition. If the measurement result is 0, then f is constant. If the measurement result is 1, then f is balanced.\n"
      ],
      "metadata": {
        "id": "yUEN_ECDmc0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tomography**\n",
        "  * **Quantum state tomography (QST)**:\n",
        "    * State tomography is an alternative to boolean functions in quantum computational learning.\n",
        "    * To solve shadow tomography, the goal is to find a quantum state σ that satisfies Tr(σEi) ≈ Tr(ρEi) for all i (the trace distance is almost zero, the states are identical). Further, one would like to minimize the number of copies of ρ, suggesting that σ should be no more informative than matching the above expectations.\n",
        "    * In state tomography, the complete state vector or density matrix of the quantum state is reconstructed. This requires measuring the state in a complete set of bases, which can be exponentially many bases for a large quantum state.\n",
        "  * **shadow tomography**:\n",
        "    * In shadow tomography, only a subset of the bases is measured. This allows the state to be reconstructed with fewer copies of the state, but the reconstructed state may not be as accurate as the state reconstructed using state tomography.\n",
        "    * The main difference between state tomography and shadow tomography is the number of copies of the quantum state that are needed to reconstruct the state.\n",
        "  * **alternate algorithm for shadow tomography**: Max-entropy principle and Matrix Multiplicative Weight Update."
      ],
      "metadata": {
        "id": "ojKgJkAHmabx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Learners**\n",
        "* [Communication complexity models](https://en.m.wikipedia.org/wiki/Communication_complexity)\n",
        "* The perceptron algorithm.\n",
        "* The ID3 algorithm.\n",
        "* The backpropagation algorithm"
      ],
      "metadata": {
        "id": "Fw0MNiKsmNqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Backpropagation and Barren Plateaus (Overparametrization) in Quantum Machine Learning**\n",
        "\n",
        "*Why can backpropagation not easily scale in quantum machine learning?*\n",
        "\n",
        "1. **Non-commutativity of operators**: Quantum operations, unlike classical ones, are represented by operators that generally do not commute, meaning the order in which they are applied matters. This non-commutativity makes the computation of gradients more complex than in the classical case.\n",
        "\n",
        "2. **Measurement**: In quantum mechanics, obtaining information about the state of a system involves measurement, which is a probabilistic process that collapses the state of the system. The inherent randomness of measurement makes the direct application of backpropagation problematic.\n",
        "\n",
        "3. **Complexity of quantum states**: Quantum states live in a complex vector space and can exist in a superposition of multiple states simultaneously, further complicating the process of backpropagation.\n",
        "\n",
        "4. **Barren plateaus**: In the context of variational quantum algorithms, it's been found that the cost function landscape often suffers from the problem of \"barren plateaus\", where the function is flat almost everywhere. This makes it difficult for gradient-based methods like backpropagation to find a direction to move in to improve the function.\n",
        "\n",
        "*Are Barren Plateaus a sign for Overparametrization and too high capacity of my quantum machine learning model?*\n",
        "\n",
        "* Yes, barren plateaus can be a sign of overparametrization and too high capacity of a quantum machine learning model. Overparametrization occurs when a model has more parameters than it needs to learn the task at hand. This can lead to the model fitting the training data too well, which can cause it to perform poorly on new data.\n",
        "\n",
        "* When a model is overparametrized, the cost function landscape can become very flat. This means that there are many different parameter settings that give the same or very similar cost values. As a result, the optimization algorithm can get stuck in a local minimum, where the cost function is not very low. This is what is known as a barren plateau.\n",
        "\n",
        "* There are a few things that can be done to mitigate the problem of barren plateaus:\n",
        "\n",
        "  * Reduce the number of parameters in the model. This can be done by simplifying the model architecture or by using regularization techniques.\n",
        "  \n",
        "  * Use a more robust optimization algorithm. Some optimization algorithms are more resistant to getting stuck in local minima than others.\n",
        "\n",
        "  * Use a more informative cost function. A more informative cost function can help the optimization algorithm find a better solution.\n",
        "\n",
        "* Here are some additional resources that you may find helpful:\n",
        "\n",
        "  * Avoiding Barren Plateaus in Variational Quantum Algorithms: https://arxiv.org/abs/2204.13751\n",
        "  * Barren Plateaus in Quantum Neural Network Training Landscapes: https://www.nature.com/articles/s41467-018-07090-4\n",
        "  * Solving 'barren plateaus' is the key to quantum machine learning: https://discover.lanl.gov/news/0319-barren-plateaus/\n",
        "\n",
        "*Techniques for improving gradient scaling and thus learning in quantum machine learning systems include:*\n",
        "\n",
        "1. **Variational Quantum Algorithms:** In the field of quantum machine learning, Variational Quantum Algorithms (VQAs) like the Variational Quantum Eigensolver (VQE) or Quantum Approximate Optimization Algorithm (QAOA) are commonly used. These algorithms employ a hybrid quantum-classical approach, which allows classical optimization techniques to be used in the learning process. This means classical techniques for managing gradient scaling, like batch normalization or gradient clipping, can still be applicable in this quantum setting.\n",
        "\n",
        "2. **Parameter Shift Rule:** The parameter-shift rule is a method for computing gradients in quantum circuits, which is particularly important for variational quantum algorithms. The rule ensures that for certain types of quantum gates (those that generate rotations), the gradient of the expectation value of a quantum circuit with respect to a parameter can be computed exactly, regardless of the number of qubits or the complexity of the circuit. This rule allows the derivative of a quantum circuit output with respect to its parameters to be computed in terms of circuit evaluations. Given a parameterized quantum gate (say, a rotation gate), the derivative of the expectation value of an observable with respect to the parameter can be computed as the difference between the expectation values of the observable for two slightly different values of the parameter. These \"shifted\" parameter values are typically chosen to be a small positive or negative shift from the original parameter value. The parameter-shift rule is powerful because it allows us to compute exact gradients using only additional evaluations of the quantum circuit, with no need for complex computations involving the inner workings of the quantum operations.\n",
        "\n",
        "3. **Adjoint method**: The adjoint method, also known as the reverse-mode differentiation, is a technique borrowed from classical automatic differentiation, generalized to the context of quantum circuits. It involves running the quantum circuit forward, storing the state at each step, and then running a modified version of the circuit backward to calculate the derivatives. This method is efficient in terms of the number of quantum operations required, especially for circuits with many parameters but a single output (as is common in quantum machine learning models). However, it requires the ability to run quantum operations in reverse, as well as the ability to store quantum states, which can be challenging to implement on near-term quantum devices.\n",
        "\n",
        "4. **Randomized Layerwise Training:** Another strategy suggested for training deep quantum circuits involves training one layer at a time with a random initialization for the rest of the circuit, a technique inspired by classical machine learning strategies. This approach can help to alleviate barren plateaus -- regions in the cost function landscape where the variance of the gradients vanishes exponentially with increasing system size.\n",
        "\n",
        "5. **Natural Gradient Descent:** There have been some initial studies into quantum natural gradient descent, which is an analogue to the classical natural gradient descent algorithm and is believed to be more robust to issues with gradient scaling.\n",
        "\n",
        "6. **Quantum-aware optimizers:**\n",
        "  * shot-frugal optimizers [51–54] can employ stochastic gradient descent while adapting the number of shots (or measurements)\n",
        "  * Quantum natural gradi- ent [55, 56] adjusts the step size according to the local geometry of the landscape (based on the quantum Fisher information metric).\n",
        "\n",
        "7. **New Research: Use unbounded objective function**\n",
        "\n",
        "  * Other barren plateaus also don't apply for unbounded objective function. Almost all of QML uses bounded operators.\n",
        "  * **KL divergence** in classical, would be quantum relativ entropy, but that's too hard to compute. **better: Maximal Quantum Rényi Divergence**\n",
        "  * compute with Extended swap test (Generalizes swap test and Hadamard test)\n",
        "  * Learning thermal states: Generative algorithm to thermal state learning,Access to LCU decomposition of the Hamiltonian\n",
        "  * Abstract [Quantum Generative Training Using Rényi Divergences](https://arxiv.org/abs/2106.09567): Quantum neural networks (QNNs) are a framework for creating quantum algorithms that promises to combine the speedups of quantum computation with the widespread successes of machine learning. A major challenge in QNN development is a concentration of measure phenomenon known as a barren plateau that leads to exponentially small gradients for a range of QNNs models.\n",
        "  * In this work, **we examine the assumptions that give rise to barren plateaus and show that an unbounded loss function can circumvent the existing no-go results**. We propose a training algorithm that minimizes the maximal **Renyi divergence** of order two and present techniques for gradient computation. We compute the closed form of the gradients for Unitary QNNs and Quantum Boltzmann Machines and **provide sufficient conditions for the absence of barren plateaus in these models**. We demonstrate our approach in two use cases: thermal state learning and Hamiltonian learning. In our numerical experiments, we observed rapid convergence of our training loss function and frequently archived a 99% average fidelity in fewer than 100 epochs.\n",
        "  * Video: [Maria Kieferova - Training quantum neural networks with an unbounded loss function - IPAM at UCLA](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "8. **New Research: Algebraic solution to solve Barren Plateaus (overparametrization = too much capacity)**\n",
        "\n",
        "  * How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - **Overparamerization (too much capacity)** arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren’t needed anymore.\n",
        "\n",
        "  * Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g. Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "  * From: [QHack 2022: Marco Cerezo —Barren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "  * *Exponentiate Lie algebra to get lie groups:*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)\n",
        "\n",
        "\n",
        "*What is backpropagation scaling?*\n",
        "\n",
        "\"Backpropagation scaling\" usually refers to techniques that manage the magnitudes of gradients during the process of backpropagation, which is used to train neural networks. Proper scaling is important because it can impact the speed and effectiveness of learning.\n",
        "\n",
        "Here are two common scaling issues that may arise during backpropagation:\n",
        "\n",
        "1. **Vanishing gradients:** When deep neural networks are trained, gradients of the loss function can become very small. As a result, weight updates during training become insignificant, and the network takes a very long time to learn, if it can learn at all. This problem is particularly common with activation functions like the sigmoid or hyperbolic tangent, which squish a large input space into a small output range.\n",
        "\n",
        "2. **Exploding gradients:** Conversely, gradients can also become very large, leading to large updates to the weights and causing the model to oscillate around the optimal solution, or even to diverge entirely. This is often a problem in recurrent neural networks (RNNs).\n",
        "\n",
        "Various techniques have been proposed to mitigate these issues:\n",
        "\n",
        "1. **Gradient clipping:** This is a common technique to prevent exploding gradients. If the norm of the gradient exceeds a certain threshold, we scale it back to prevent it from getting too large.\n",
        "\n",
        "2. **Weight initialization:** Properly initializing the weights can prevent gradients from vanishing or exploding too quickly. For example, Xavier/Glorot and He initialization are strategies that consider the sizes of the input and output layers.\n",
        "\n",
        "3. **Choice of activation function:** The choice of activation function can help alleviate the vanishing gradients problem. For example, ReLU (Rectified Linear Unit) and its variants (like Leaky ReLU and Parametric ReLU) are commonly used because they do not saturate for positive inputs.\n",
        "\n",
        "4. **Batch normalization:** This technique normalizes the activations of each layer to prevent the distribution of inputs to each layer from changing too much during training, which can help mitigate both vanishing and exploding gradients.\n",
        "\n",
        "5. **Use of optimizers:** Certain optimization algorithms like RMSProp, Adam, and Nadam can adaptively scale learning rates to mitigate both exploding and vanishing gradients.\n",
        "\n",
        "In summary, backpropagation scaling techniques are important to ensure that the magnitude of updates during training is appropriate and to prevent issues related to vanishing or exploding gradients.\n",
        "\n",
        "\n",
        "*Expectation Value and Backpropagation*\n",
        "\n",
        "The average value (expectation value) of the measurement result is given by the\n",
        "Born rule:\n",
        "\n",
        "> $\\langle B\\rangle=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle$\n",
        "\n",
        "Just linear algebra! Every step is a matrix-vector or matrix-matrix multiplication\n",
        "\n",
        "Expectation values depend continuously on the gate parameters\n",
        "\n",
        "*Backpropagating Through Quantum Circuits*\n",
        "\n",
        "However, as long as we don't \"zoom in\" to what is happening in the quantum circuit, backpropagation can treat the quantum circuit as a single indivisible function\n",
        "\n",
        "The expectation value of a quantum circuit is a differentiable function\n",
        "\n",
        "> $\n",
        "f(\\theta)=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle=\\langle B\\rangle$\n",
        "\n",
        "Running on hardware and using the parameter-shift rule, we can provide both ingredients needed by backpropagation\n",
        "\n",
        "> $\n",
        "\\left(\\langle B\\rangle, \\frac{\\partial}{\\partial \\theta}\\langle B\\rangle\\right)\n",
        "$\n",
        "\n",
        "[Automatic Differentiation of Quantum Circuits](https://youtu.be/McgBeSVIGus)\n",
        "\n",
        "[Variational Quantum Algorithms](https://youtu.be/YtepXvx5zdI)\n",
        "\n",
        "[Hybrid Quantum-Classical Machine Learning](https://youtu.be/t9ytqPTij7k)\n",
        "\n",
        "**Algebraic solution to solve Barren Plateaus (overparametrization = too much capacity)**\n",
        "\n",
        "\n",
        "* How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - **Overparamerization (too much capacity)** arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren’t needed anymore.\n",
        "\n",
        "* Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g. Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "* From: [QHack 2022: Marco Cerezo —Barren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "*Exponentiate Lie algebra to get lie groups:*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)"
      ],
      "metadata": {
        "id": "VXWcAsxVYr3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***$\\hookrightarrow$ Model Complexity***"
      ],
      "metadata": {
        "id": "m_M1o-kVjMZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Jarrod McClean - The role of data, precomputation, and communication in a quantum learning landscape](https://youtu.be/dOUppTONVDM?t=2744)\n",
        "\n",
        "[Expressibility of Parametrized Quantum Circuits & Classification Accuracy of Quantum Neural Networks](https://www.youtube.com/watch?v=Igxr1HLhdrM&list=WL&index=6)\n",
        "\n",
        "[QHack 2022: Zoe Holmes —Expressibility & trainability: balancing the ingredients of an effective VQA](https://www.youtube.com/watch?v=RO3g7B0-IKA&list=WL&index=5&t=65s)\n",
        "\n",
        "[Google Random circuit sampling: \"Quantum Computational Supremacy\" lecture by Scott Aaronson](https://www.youtube.com/watch?v=XazjgK3yQB8&list=WL&index=4&t=3535s)\n",
        "\n",
        "[]()\n",
        "\n",
        "[]()"
      ],
      "metadata": {
        "id": "_wjdQ2k7TRkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> How complex a model needs to be to accurately learn from data without overfitting? - Derive bounds on the generalization error of a learning algorithm (e.g. Inequality: bound probability of random variable deviating from its expected value / bounds probability of estimator deviating from its expected value by a certain amount = provide guarantees on accuracy of estimator).\n",
        "\n",
        "**Expressibility (Expressiveness or Representational Capacity)**:\n",
        "* Expressibility refers to the ability of a learning model or algorithm to represent a wide variety of functions or concepts. It's about the richness of the hypothesis space that the model can capture.\n",
        "* For example, a neural network with more layers and nodes has higher expressibility because it can represent more complex functions compared to a simpler network.\n",
        "* Expressibility is closely related to the concepts like VC dimension or Rademacher complexity, which measure the capacity of a model class to fit data."
      ],
      "metadata": {
        "id": "tyB6nQ3AXv8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In order to solve a specific problem, one has to find a model with sufficient capacity, but not too much, and an according sample size to the model?**\n",
        "\n",
        "\n",
        "Sufficient Capacity: The model should have enough capacity (or complexity) to capture the underlying patterns or relationships in the data. If the model's capacity is too low, it will underfit, meaning it cannot capture the complexity of the data.\n",
        "\n",
        "Avoiding Excess Capacity: On the other hand, if the model's capacity is too high, it risks overfitting, where it starts to learn the noise and random fluctuations in the training data as if they were meaningful patterns. This leads to poor generalization to new, unseen data."
      ],
      "metadata": {
        "id": "X2n0nVThRb_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Complexity I: Model Capacity / Power / Expressivity (for Generalization)**\n",
        "\n",
        "> Model complexity is the size or complexity of the function that an algorithm can learn.Bounding model complexity of learning algorithms: showing that the algorithm is unlikely to learn a function that is too complex, given a sufficient number of training samples. This helps us to prevent the algorithm from overfitting the training data. e.g. **Bounding the model complexity of neural networks**:  Concentration inequalities can be used to show that neural networks are unlikely to learn a function that is too complex, given a sufficient number of training samples and a suitable regularization scheme.\n",
        "\n",
        "* **Capacity** = **depth of functions** (for Generalization): complex functions (can be wide or narrow), also called model power or expressivity. Masure of how many possible hypotheses a learning algorithm can consider (e.g. number of parameters in ML model). Complex models can learn more complex functions, but also more likely to overfit training data and not generalize well to new data.\n",
        "\n",
        "  * (attention, double check this) It is important to distinguish between model capacity and expressivity. Capacity refers to the ability of a model to learn any function, while expressivity refers to the ability of a model to learn a specific class of functions. For example, a neural network with a large number of parameters may have high capacity, but it may not be expressive enough to learn a complex function such as the XOR function.\n",
        "\n",
        "* **Objective of Measuring Model Complexity: understanding how well model can generalize on dataset (prediction error)**\n",
        "  * No overfitting on training (high capacity networks, number of possible hypothesis not too small, but also not too large), not too high variance.\n",
        "  * **Determine bounds** on smallest possible variance, which is ultimately achievable precision, and on shattered (separated) points.\n",
        "  * Generalization (Prediction error) depends on both the training error as well as the complexity of the trained model. The prediction error is small only if training error is itself small and the **complexity of trained model is moderate** (i.e., sufficiently smaller than training data size).\n",
        "\n",
        "* Paper: [On the expressivity of embedding quantum kernels](https://arxiv.org/abs/2309.14419)\n",
        "\n",
        "*There is a relationship between Generalization and capacity (max capacity is not necessarily what we want):*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1399.png)\n",
        "\n",
        "* [Neural network capacity](https://en.m.wikipedia.org/wiki/Artificial_neural_network#Capacity): A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\n",
        "\n",
        "* **Model capacity**: complexity of the patterns a model is capable of learning from the data. Is often associated with size or complexity of model — a model with more parameters (like a deep neural network) has a higher capacity than one with fewer parameters (like a simple linear regression).\n",
        "  * A higher capacity model is theoretically able to learn more complex relationships in the data, but this also opens up the risk of overfitting, where the model learns the noise or specific quirks of the training data instead of the general underlying patterns.\n",
        "\n",
        "* **Expressivity**: how well a model can approximate a wide variety of functions? A neural network with a higher number of layers and neurons would be considered more expressive than a network with fewer layers and neurons because it can theoretically approximate a greater variety of functions [Source](https://www.youtube.com/watch?v=ETxQNIR6dAg&t=684s).\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1602.png)\n",
        "\n",
        "**Metrics for Model Complexity (Measuring Bounds)**\n",
        "\n",
        "*Different measures (metrics) of model capacity and their usefulness (ED: effective dimension):*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1644.png)\n",
        "\n",
        "*Model Complexity - Measure Metrics*\n",
        "* VC dimension\n",
        "* Rademacher complexity\n",
        "* Fisher-Rao norm, Fisher Information and Quantum Cramér-Rao Bound\n",
        "* Covering number\n",
        "* Fat-Shattering Dimension\n",
        "* Effective dimensions\n",
        "* Frobenius norm\n",
        "* Spectral norm\n",
        "\n",
        "*Model Complexity of classical deep learning*\n",
        "\n",
        "* the expressive power of a model is used to bound the generalization error\n",
        "* Vapnik-Chervoneniks (VC) dimension [26]. Besides, Rademacher complexity and Gaussian complexity [8, 12] are also used to mea- sure model complexity of logistic regression models\n",
        "* Comparing to VC dimension, Rademacher complexity takes data distribution into consideration and therefore reflects finer-grained model complexity.\n",
        "* suggest that model complexity of logistic regression models is related to the number of distinguishable distributions that can be rep- resented by the models.\n",
        "* Bulso et al. [21] define a complexity measure of logistic regression models based on the determinant of the Fisher Information matrix.\n",
        "* Spiegelhater et al. [93] define a model complexity measure by the number of effective parameters. Using the information theoretic argument, they show that this complexity measure can be estimated by the difference between the posterior mean of the deviance and the deviance at the posterior estimates of the parameters of interest, and is approximately the trace of the product of Fisher’s information [33] and the posterior covariance matrix.\n",
        "* Complexity measures are often model specific and complexity measures of different model frameworks cannot be compared\n",
        "* Expressibility: A notion to capture hypothesis space complexity is Rademacher complexity [12], which measures the degree to which a hypothesis space can fit random noise. Another notion is VC dimen- sion [26], which reflects the size of the largest set that can be shattered by the hypothesis space. Exploring expressive capacity helps to obtain the guarantee of learnability of deep models and derive generalization bounds\n",
        "* Page 11: The comparison of deep and shallow sum- product networks representing the same function indicates that, to represent the same functions, the number of neurons in a shallow network has to grow exponentially but only a linear growth is needed for deep networks.\n",
        "* Page 26: First, based on the piecewise linear property, Novak et al. [81] propose the Jacobian norm to measure the local sensitivity under the assumption that the input is perturbed within the same linear region. (Includes Frobenius norm)\n",
        "* To approach the generalization problem of deep learning models, Liang et al. [60] introduce a new notion of model complexity measure, the Fisher-Rao norm.\n",
        "* In statistical learning theory, expressive capacity (i.e., hypothesis space com- plexity) is used to bound generalization error [69]\n",
        "* Page 33: Based on these desiderata, Neyshabur et al. [76] investigate several complex- ity measures including norms [79], robustness [97], and sharpness [50]. They show that, these measures can meet some of the above requirements, but not all.\n",
        "* Novak et al. [81] define two complexity measures from the perspective of model sensitivity, and identify an empirical correlation between the complexity measures and model generalization capability. They show that operations that lead to poor generalization, such as full batch training, correspond to high sen- sitivity, and in turn imply high effective model complexity. Similarly, operations that lead to good generalization, such as data augmentation, correspond to low sensitivity, and thus imply low effective model complexity.\n",
        "* In other words, many different parameterizations may lead to the same prediction. Thus, the specific parameterization of deep models should not affect the generalization and the complexity measure. They show that the Fisher-Rao norm honors this invari- ance property and thus is able to explain the generalization capability of deep learning models.\n",
        "* On one hand, making predictions with high accuracy is the essential goal of learning a model [69]. A model is expected to be able to capture the underlying patterns hidden in the training data and achieve predictions of accuracy as high as possible. In order to represent a large amount of knowledge and obtain high accuracy, a model with a high expressive capacity, a large degree of freedom and a large training set is required [13]. To this extent, a model with more parameters and higher complexity is favored.\n",
        "    * On the other hand, an overly complex model may be difficult to train and may incur unnecessary resource consumption, such as storage, computation and time cost [72]. Unnecessary resource consumption should be avoided particularly in practical large scale applications [42]. To this extent, a simpler model with comparable accuracy is preferred than a more complicated one.\n",
        "* Can we obtain a lower bound of expressive capacity of deep learning models that are sufficient for a given task? Does a narrow layer limit the expressive capacity of a model even if the model itself has a large number of parameters?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZOQKo-v-PWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Complexity II: Model Expressibility (Trainability)**\n",
        "\n",
        "* **Expressibility** = **width of functions** (for trainability): wide array of functions (can be complex or shallow)\n",
        "\n",
        "* Expressibility of circuits: Learning an unknown unitary\n",
        "\n",
        "* **Expressibility & trainability is a tradeoff!!**\n",
        "\n",
        "* Expressibility generally refers to the ease with which a model can be trained to approximate a desired function.\n",
        "\n",
        "* Zoe Holmes: reduce expressibility to increase trainability. Find ansatz that fits the use case problem.\n",
        "\n",
        "* https://pennylane.ai/qml/demos/tutorial_haar_measure.html\n",
        "\n",
        "* Barren plateaus, and hence expressibility issues, are not so much in classical ML, because vanishing cost gradients are not so much of an issue in classical ML because we don‘t have precision limitations in the same way.  You don‘t have to evauluate your cost function using many many shots, or is so resource intensive to get gradients.\n",
        "* In quantum ML: you can use ideas from control theory to try to assess whether or not your ansatz is gonna be trainable or not in advance. Thats an important strategy.\n",
        "* The other approach: use symmetries of your problem / you gonna have to use physics to come up with whats a good ansatz. ZB: use VGQ for some system with various (particle number conserving) translational symmetries, you want to build all of those symmetries into your ansatz and hence reduce expressibility while capturing some of the solution space.\n",
        "\n",
        "* Maria schuld paper: how expressive are circuits? You can distribute circuits and how flexible are they? - identity gate maps to one point only. If you have a couple of more gates it maps to more points.\n",
        "\n",
        "https://arxiv.org/abs/1905.10876: Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1408.png)\n",
        "\n",
        "Is expressivity even important? (maria schuld) https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1384s\n",
        "\n",
        "*Model Complexity in Deep Learning:*\n",
        "\n",
        "* **Expressive capacity**: model complexity may refer to capacity of deep models in expressing or approximating complicated distribution functions (seems to be **expressibility** = width, wide array of functions, complex or shallow)\n",
        "* **Effective complexity**: describes how complicated the distribution functions are with some parameterized deep models (seems to be **capacity** = depth, complex functions, wide or narrow)\n",
        "\n",
        "**There cases where expressibility is high and capacity is small and vice versa:**\n",
        "\n",
        "* You can have High expressibility, small capacity (shallow neural nets), but unlikely to have Low expressibility, high capacity.\n",
        "\n",
        "* **High expressibility, small capacity**: This could potentially occur in situations where a model has a wide range of different functions it can represent (high expressibility) but is limited in the complexity of those functions (small capacity). For example, a shallow neural network (only a few layers deep) can represent a wide variety of different functions, but it may struggle to accurately represent highly complex functions or patterns (like those present in high-dimensional data or complex tasks). This is because it lacks the depth necessary for creating intricate compositional representations.\n",
        "\n",
        "* **Low expressibility, high capacity**: This situation might be harder to come by, but one could imagine a scenario where a model has the potential to represent very complex functions (high capacity) but is restricted in the variety of functions it can actually express due to constraints on its parameters (low expressibility). An example could be a deep neural network with high capacity but with parameters constrained in such a way that it can only express a narrow range of functions. However, such a situation might be considered somewhat artificial.\n",
        "\n",
        "*Expressibility generally refers to the ease with which a model can be trained to approximate a desired function. Here some methods give you a more targeted approach to evaluating and measure model expressibility! It remains a nuanced topic with many factors at play:*\n",
        "\n",
        "1. **Training Convergence**: You can monitor the training convergence of the model, i.e., how quickly and reliably it reaches a minimum of the loss function during training.\n",
        "\n",
        "2. **Gradient-Based Metrics**: Examine the gradients during training, since the behavior of gradients (like vanishing or exploding gradients) can affect the expressibility of the model.\n",
        "\n",
        "3. **Loss Landscape Analysis**: Analyze the loss landscape of the model using techniques such as visualization of loss landscapes to understand the complexity and the potential difficulties in training the model.\n",
        "\n",
        "4. **Hessian-Based Analysis**: Use Hessian-based analysis to study the second-order properties of the loss function, which can provide insights into the local curvature of the loss landscape and potentially the expressibility of the model.\n",
        "\n",
        "5. **Optimization Trajectory**: Study the optimization trajectory of the model during training. Some models may have smoother, more predictable trajectories that suggest better expressibility.\n",
        "\n",
        "6. **Sensitivity to Initialization**: Investigate how the model's performance varies with different initialization strategies. A model that can train effectively from a wide range of initializations might be said to have good expressibility.\n",
        "\n",
        "7. **Sensitivity to Hyperparameters**: Analyze the sensitivity of the model to hyperparameter settings. If a model can only be trained effectively with a very narrow range of hyperparameter settings, it might have limited expressibility.\n",
        "\n",
        "8. **Generalization Gap**: Study the generalization gap, which is the difference between training error and test error. A smaller generalization gap might indicate better expressibility, as it suggests that the model is able to learn a more useful representation of the data.\n",
        "\n",
        "9. **Empirical Studies**: Conduct empirical studies where you train the model on a range of different tasks and datasets to see how easily it can adapt to different kinds of data and problem structures.\n",
        "\n",
        "10. **Computational Resources**: Evaluate the computational resources (time, memory) required to train the model. A model that requires less computational resources to reach a certain level of performance might be said to have better expressibility.\n"
      ],
      "metadata": {
        "id": "kuNW1Ap_Kzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Myths Buster in study of complexity notions**\n",
        "\n",
        "https://www.inference.vc/generalization-and-the-fisher-rao-norm-2/\n",
        "\n",
        "*Myth #1: Current theory is lacking because deep neural networks have too many parameters*\n",
        "\n",
        "* P. Bartlett, “The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network,” 1998\n",
        "* Margin theory was developed to address this very problem for Boosting and NN (e.g. Koltchinskii & Panchenko ’02 and references therein)\n",
        "* Example: linear classifiers $\\left\\{x \\mapsto \\operatorname{sign}(\\langle w, x\\rangle):\\|w\\|_2 \\leq 1\\right\\}$ and assume margin. Then dimension of $w$ (num. of params in 1-layer NN) nevel appears in generalization bounds (and can be infinite). This observation already appears in the 60 's.\n",
        "* In Statistics, one often deals with infinite-dimensional models\n",
        "* Number of parameters is rarely the right notion of complexity (true, in classical statistics still the case for linear regression or simple models)\n",
        "* VC dimension is known to be a loose quantity (distribution-free, only an upper bound)\n",
        "* Our own (arguably incomplete) take on this problem:\n",
        "T. Liang, T. Poggio, J. Stokes, A.R. “Fisher-Rao Metric, Geometry, and Complexity of Neural Networks,” 2017.\n",
        "  * Fisher local norm as a common starting point for many measures of complexity currently studied in the literature (see work of Srebro’s group and Bartlett et al).\n",
        "  * Information Geometry suggests Natural Gradient as the optimization method. Appears to resolve ill-conditioned problems in Shalev-Shwartz et al ’17.\n",
        "\n",
        "*Myth #2: To prove good out-of-sample performance, we need to show uniform convergence (a la Vapnik) over some class*\n",
        "\n",
        "* The oldest counter-example: Cover and Hart, “Nearest neighbor pattern classification,” 1967.\n",
        "* Second (related) issue: uniform vs universal consistency.\n",
        "* **Uniform Consistency**: There exists a sequence $\\left\\{\\hat{y}_t\\right\\}_{t=1}^{\\infty}$ of estimators, such that for any $\\epsilon>0$, there exists $n_e$ such that for any distribution $P \\in \\mathcal{P}$ and $n \\geq n_e$, $\n",
        "\\mathbb{E} L\\left(\\widehat{y}_n\\right)-\\inf L(f) \\leq \\epsilon\n",
        "$\n",
        "* **Universal Consistency**: There exists a sequence $\\left\\{\\widehat{y}_t\\right\\}_{t=1}^{\\infty}$ of estimators, such that for any distribution $P \\in \\mathcal{P}$ and any $\\epsilon>0$, there exists $n_e$ such that for $n \\geq n_e(P)$, $\n",
        "\\mathbb{E} L\\left(\\widehat{y}_n\\right)-\\inf L(f) \\leq \\epsilon\n",
        "$\n",
        "* Importantly, can interpolate between the two notions using penalization. A few more approaches (e.g. use bracketing entropy)\n",
        "\n",
        "*Myth #3: Sample complexity of neural nets scales exponentially with depth*\n",
        "\n",
        "* A common pitfall of making conclusions based on (possibly loose) upper bounds. Mostly resolved: N. Golowich, A.R., O. Shamir, “Size-Independent Sample Complexity of Neural Networks,” 2017\n",
        "* From $2^{\\mathrm{d}}$ to $\\sqrt{\\mathrm{d}}$ dependence was simply a technical issue. From $\\sqrt{\\mathrm{d}}$ to $O(1)$ requires more work.\n",
        "\n",
        "*Myth #4: If we can fit any set of labels, then Rademacher complexity is too large and, hence, nothing useful can be concluded*\n",
        "\n",
        "* Related to Myth #2, but let’s illustrate with a slightly di↵erent technique. Bottom line: we can have a very large overall model, but performance depends on a **posteriori** complexity of the **obtained** solution.\n",
        "* Most trivial example: take a large $\\mathcal{F}=\\cup_k \\mathcal{F}_k$, where $\\mathcal{F}_k=\\left\\{f: \\operatorname{compl}_n(f) \\leq k\\right\\}$ and for simplicity assume $\\operatorname{compl}_n(f)$ is positive homogenous. Suppose (this is standard) we have that with high probability\n",
        "  * $\n",
        "\\forall f \\in \\mathcal{F}_1, \\quad \\mathbb{E f}-\\widehat{\\mathbb{E}} \\mathrm{f} \\lesssim \\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)+\\ldots\n",
        "$\n",
        "* where $\\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)$ is empirical Rademacher. Then with same probability\n",
        "  * $\n",
        "\\forall f \\in \\mathcal{F}, \\quad \\mathbb{E} f-\\widehat{\\mathbb{E}} f \\lesssim \\operatorname{compl}_n(f) \\cdot \\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)+\\ldots\n",
        "$\n",
        "* Conclusion: an a posteriori data-dependent guarantee for all $\\mathrm{f}$ based on complexity of $f$, yet $\\widehat{\\mathscr{R}}(\\mathcal{F})$ never appears (huge or infinite). If complexity is not positive homogenous, use union bound instead.\n",
        "\n",
        "*Is there anything left to do? Yes, tons. Perhaps need to ask different questions*\n",
        "* What are the properties of solutions that optimization methods find in a nonconvex landscape? Is there “implicit regularization” that we can isolate? - a nice line of work by Srebro and co-authors\n",
        "* What are the salient features of the random landscape? Uniform deviations for gradients and Hessians? - nice work by Montanari and co-authors\n",
        "* How can one exploit randomness to make conclusions about optimization solutions? (e.g. see the SGLD work of Raginsky et al, as well as papers on escaping saddles)\n",
        "* What geometric notions can be associated to multi-layer neural nets? How can this geometry be exploited in optimization methods and be reflected in sample complexity?\n",
        "* Theoretical understanding of adversarial examples. etc."
      ],
      "metadata": {
        "id": "CobPNZxamooq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***$\\hookrightarrow$ Sample and Query Complexity***"
      ],
      "metadata": {
        "id": "02eilyDPlFh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Sample Complexity](https://en.m.wikipedia.org/wiki/Sample_complexity): number of training examples that the most efficient learning algorithm needs to see in order to **learn a concept with a certain accuracy** (an arbitrarily small error of best possible function, with probability arbitrarily close to 1)\n",
        " - It's a measure of how data-efficient a learning algorithm is.\n",
        "  * A high sample complexity means that a large amount of data is needed to ensure good generalization.\n",
        "  * a learning algorithm with low sample complexity can achieve good performance with less data.\n",
        "\n",
        "* estimate how large a sample size is needed to ensure that the empirical average of a random variable is close to its true mean with high probability (especially relevant in scenarios like empirical risk minimization, where one seeks to minimize the discrepancy between the empirical risk (based on finite samples) and the true risk)\n",
        "\n",
        "* Bounding sample complexity:\n",
        "  * showing that output of learning algorithm is close to its expected value with high probability, given a sufficient number of training samples. Allows us to set a confidence bound on  accuracy of the algorithm, even if we have only seen a small number of training samples.\n",
        "  * e.g. **Bounding sample complexity of empirical risk minimization (ERM) or online learning**: (ERM=average loss on the training data). Concentration inequalities can show that ERM learns a function with a small generalization error with high probability, given a sufficient number of training samples.\n",
        "  * For only particular class of target functions (e.g. linear functions) sample complexity is finite and depends linearly on [VC dimension](https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension) on class of target functions. Sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C.\n",
        "\n",
        "* Factors that affect sample complexity of learning a target function:\n",
        "  * Size of hypothesis space: Large space = more samples\n",
        "  * Error tolerance (desired level of accuracy and confidence): Smaller error tolerance = more samples\n",
        "  * Noise: Higher noise = more samples\n",
        "  * Complexity of target function or concept to be learned: More complex target function = more samples\n",
        "\n",
        "* Difference:\n",
        "  * sample complexity focuses on the data requirements for learning,\n",
        "  * model complexity deals with the representational power of the model.\n",
        "\n",
        "* Trade-Offs:\n",
        "  * models with higher complexity (capacity) can represent more complex functions but may require more data (higher sample complexity) to generalize well and avoid overfitting.\n",
        "  * Conversely, simpler models might generalize better with less data but are limited in the complexity of functions they can learn."
      ],
      "metadata": {
        "id": "pA922WyYbUKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Complexity - Measure Metrics** (bounds and distances)\n",
        "\n",
        "* [Rademacher Complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity): upper bound on sample complexity = on learnability of function classes (deriving generalization bounds). Measures ability of functions in class to fit to random noise. [Rademacher Complexity PDF](https://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf). When the Rademacher complexity is small, it is possible to learn the hypothesis class H using [empirical risk minimization](https://en.m.wikipedia.org/wiki/Empirical_risk_minimization).\n",
        "\n",
        "* [Cauchy–Schwarz inequality](https://en.m.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality). Cauchy-Schwarz inequality used to bound sum of squared residuals (cost function for linear regression) by the sum of the squared predicted values and the sum of the squared actual values. Used in *A Survey of Quantum Learning Theory* page 11 to compute upper bounds of learning algorithm.\n",
        "\n",
        "* [Cramér–Rao bound](https://en.m.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound): lower bound (min value) on variance of an unbiased estimator (but cannot tell about probability of deviation) - this value is inverse of [Fisher information](https://de.m.wikipedia.org/wiki/Fisher-Information#Verwendung)(how much information data provides about parameters being estimated). Used to design estimators that are as efficient as possible.\n",
        "\n",
        "* Metric entropy\n",
        "\n",
        "* Covering number\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "* Haar measure: Quantum learning algorithm: define **average complexity** by sampling a unitary matrix from the Haar measure and then measuring sample complexity of the algorithm on the resulting quantum state. Average complexity is then the expected value of the sample complexity over all possible unitary matrices. Train machine learning models on quantum data: can be done by sampling a large number of random unitary operations and applying them to the data. The Haar measure ensures that all possible unitary operations have an equal chance of being sampled.\n",
        "* VC dimension, Rademacher complexity and Kullback Leibler divergence can all be used to measure the **worst-case complexity** (sample complexity of algorithm on worst possible quantum state)."
      ],
      "metadata": {
        "id": "eZZsreIaEoy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Up to constant factors\" in sample complexity**\n",
        "\n",
        "* two quantities are equal, with an error that is bounded by a constant (= error is always within a certain range, regardless of input, e.g. absolute value of error will never be greater than 1)\n",
        "\n",
        "* useful to make statements about performance or complexity of algorithms, without having to worry about the exact values of the constants (exact sample complexity of learning problem often difficult to determine)\n",
        "\n",
        "* If sample complexity of learning a concept is up to constant factors $c$, then any two learning algorithms that learn concept must use number of samples that is within a factor of $c$ of each other.\n",
        "\n",
        "* e.g. PAC learning model guarantees that any concept that can be PAC-learned can be learned with a sample complexity that is polynomial in the size of the hypothesis space and logarithmic in the inverse of the desired error rate. This means that the sample complexity is \"up to constant factors\" of the logarithm of the inverse of the error rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "xOHRIRBzzqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Complexity**\n",
        "\n",
        "* PAC and agnostic learning: sample complexities. Exact learning: query complexity.\n",
        "\n",
        "* QSQ: query to learn about quantum state of system (measure expectation value of observable, or extract information about entanglement of state)\n",
        "\n",
        "* Diagonal-QSQ is way of restricting types of QSQs that can be used to learn the output distributions of constant-depth circuits.\n",
        "\n",
        "  * only QSQs that can be expressed in terms of diagonal elements of density matrix of state can be used\n",
        "  \n",
        "  * How many QSQs are required to learn the output distributions of the circuits with a certain accuracy? - difficult to solve (no known general upper bounds on QSQ complexity, and best known lower bounds are exponential in number of qubits in circuits)\n",
        "\n",
        "  * Use QML to learn output distributions of these circuits with fewer QSQs.\n"
      ],
      "metadata": {
        "id": "Eh4FwtN5_Mh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### *Complexity Measures*"
      ],
      "metadata": {
        "id": "Cmqnes9brFPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Introduction*"
      ],
      "metadata": {
        "id": "LOnEwXajSzfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cramer rao bound amd Cauchy–Schwarz inequality, Frobeniusnorm:\n",
        "variance ideal for quantum Hilbert space?\n",
        "\n",
        "How much is a loss function concentrated around its mean (not partial derivative concentration)?\n",
        "\n",
        "\n",
        "**Currently: how to study Parametrized Quantum Circuits (PQCs)?**\n",
        "\n",
        "* Expressivity / Capacity\n",
        "* Trainability\n",
        "  * subset of computational complexity and circuit complexity?\n",
        "* Generalization > which model or algorithm to choose best?\n",
        "* Sample complexity\n",
        "\n",
        "**Generalization in Machine Learning:**\n",
        "\n",
        "* Generalization refers to the ability of an algorithm to **perform accurately on new, unseen data after being trained on a sample dataset**. The goal is to learn patterns that are not just specific to the training data but also applicable to new data.\n",
        "\n",
        "* Uniform Generalization Bounds: These bounds provide a measure of how well the learned model will generalize across all possible data distributions, not just the one it was trained on. They are called 'uniform' because they provide a guarantee that holds uniformly over a class of functions or models.\n",
        "\n",
        "* Role of Sample Size: One key aspect of these bounds is that they typically improve with an increase in the size of the training sample. As more data is used to train the model, we can be more confident that the model will generalize well.\n",
        "\n",
        "* Complexity of the Hypothesis Class: **The bounds are also influenced by the complexity of the hypothesis class** (the set of models being considered). More complex classes, such as those with a large number of parameters, typically have **looser bounds, meaning they need more data to ensure good generalization**.\n",
        "\n",
        "* Mathematical Formulation: **The bounds are often derived using tools from probability and statistics, such as concentration inequalities (like Hoeffding’s inequality or McDiarmid's inequality)**. These tools help quantify the likelihood that the observed performance of a model on a sample is close to its true performance on the entire population.\n",
        "\n",
        "* Practical Implication: In practical terms, **uniform generalization bounds help in understanding and choosing models and algorithms**. They provide a theoretical foundation for preferring certain models over others based on their generalization capabilities, given a finite amount of data."
      ],
      "metadata": {
        "id": "lWl0TKgLrsJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Besides model complexity, sample complexity and computational complexity, are there more dimension that need to be considered on computational learning theory?**\n",
        "\n",
        "Yes, besides model complexity, sample complexity, and computational complexity, there are several other dimensions and factors that are important in computational learning theory:\n",
        "\n",
        "1. **Algorithmic Stability**:\n",
        "    - This refers to how sensitive a learning algorithm is to small changes in the training set. Algorithms that are more stable are generally better at generalizing, as they don't overfit to the noise or specific details in the training data.\n",
        "\n",
        "2. **Noise Tolerance**:\n",
        "    - The ability of a learning algorithm to perform well in the presence of noise (errors or randomness in the training data) is crucial. This includes dealing with mislabeled data or data with measurement errors.\n",
        "\n",
        "3. **Robustness**:\n",
        "    - Robustness is the ability of the model to maintain performance when faced with input data that differs from the training data. This includes handling adversarial examples, where small, intentional changes to input data can mislead the model.\n",
        "\n",
        "4. **Learning Rate and Convergence**:\n",
        "    - This pertains to how quickly a learning algorithm converges to a good solution. Fast convergence is desirable for efficiency, but it's also important that the convergence leads to a solution that generalizes well.\n",
        "\n",
        "5. **Bias-Variance Tradeoff**:\n",
        "    - A fundamental concept in statistical learning, the bias-variance tradeoff involves balancing underfitting (bias) and overfitting (variance). It's about finding the right level of model complexity that neither oversimplifies (high bias) nor overcomplicates (high variance) the problem.\n",
        "\n",
        "6. **Data Distribution**:\n",
        "    - The nature of the data distribution, including how representative the training data is of the real-world scenarios the model will encounter, plays a crucial role in the effectiveness of learning.\n",
        "\n",
        "7. **Feature Representation**:\n",
        "    - The way data is represented or pre-processed can significantly impact the performance of a learning algorithm. Good feature representation can ease the learning process and improve generalization.\n",
        "\n",
        "8. **Scalability**:\n",
        "    - Scalability refers to how well a learning algorithm or model can handle increasing amounts of data or more complex problems. This is particularly important in big data applications.\n",
        "\n",
        "9. **Ethical and Social Considerations**:\n",
        "    - This includes considerations of fairness, privacy, and transparency. It's essential to ensure that learning algorithms do not perpetuate or exacerbate biases and are respectful of privacy and ethical norms.\n",
        "\n",
        "10. **Transferability and Adaptability**:\n",
        "    - This refers to the ability of a model to adapt to new tasks or transfer learned knowledge to different but related problems. It's a key aspect of research in areas like transfer learning and domain adaptation.\n",
        "\n",
        "11. **Interpretability and Explainability**:\n",
        "    - The ability to understand and interpret the decisions made by a model is increasingly important, especially for applications in sensitive areas like healthcare or finance.\n",
        "\n",
        "Each of these dimensions adds complexity to the design and evaluation of learning algorithms, and they are often interrelated. Balancing these factors is a key challenge in developing effective and responsible machine learning systems."
      ],
      "metadata": {
        "id": "3dgYVzt0a8lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are standard scientific methods to measure model complexity, sample complexity and computational complexity?**\n",
        "\n",
        "Measuring model complexity, sample complexity, and computational complexity in machine learning involves a blend of theoretical analysis and empirical evaluation. Each type of complexity has standard scientific methods associated with it:\n",
        "\n",
        "*1. Model Complexity*\n",
        "Model complexity refers to the ability of a model to fit a wide range of functions. It's often associated with the flexibility of the model to adapt to the underlying data structure.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **VC Dimension**: For classification models, the Vapnik-Chervonenkis (VC) dimension is a theoretical measure of model capacity. It's the size of the largest set of points that the model can shatter.\n",
        "- **Number of Parameters**: In many models, such as neural networks, the number of trainable parameters is a direct measure of complexity.\n",
        "- **Model Depth and Architecture**: For neural networks, depth (number of layers) and architecture details (like types of layers and connections) indicate complexity.\n",
        "- **Information Criteria**: Metrics like AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used in statistical modeling to balance model fit and complexity.\n",
        "\n",
        "- Number of Parameters: One of the most common measures of model complexity is the number of parameters. This is a simple and intuitive measure, as it directly reflects the number of degrees of freedom in the model. However, it is not always the most informative measure, as it does not take into account the structure of the model or the relationships between its parameters.\n",
        "\n",
        "- Effective Number of Parameters: A more sophisticated measure of model complexity is the effective number of parameters. This measure takes into account the fact that some parameters may be more important than others, and it can be used to compare models with different numbers of parameters.\n",
        "\n",
        "- VC Dimension: The VC dimension is a theoretical measure of model complexity that is based on the concept of shattering. A shattered set of points is a set of points that can be perfectly classified by a hypothesis class in all possible ways. The VC dimension of a hypothesis class is the maximum size of a shattered set. A higher VC dimension indicates a more complex hypothesis class.\n",
        "\n",
        "- Kolmogorov Complexity: Kolmogorov complexity is a measure of the information content of an object. It is based on the idea that the complexity of an object is the shortest possible program that can generate it. Kolmogorov complexity can be used to measure the complexity of models, as well as the complexity of data.\n",
        "\n",
        "*2. Sample Complexity*\n",
        "Sample complexity relates to the amount of data required for a model to achieve a certain level of performance.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Empirical Evaluation**: Experimentally determining how model performance varies with different training set sizes. This often involves plotting learning curves.\n",
        "- **PAC Learning Framework**: Probably Approximately Correct (PAC) learning provides theoretical bounds on the number of samples needed for a model to learn a concept within a certain error margin and confidence level.\n",
        "- **Cross-validation**: Techniques like k-fold cross-validation can help assess how well a model with certain complexity performs with the available data.\n",
        "\n",
        "- Learning Curves: Learning curves are a simple and effective way to measure sample complexity. A learning curve is a plot of the training and generalization error of a model as a function of the number of training examples. A steeper learning curve indicates that the model is more sensitive to the number of training examples and therefore has higher sample complexity.\n",
        "\n",
        "- Error Bounds: Error bounds are a more theoretical way to measure sample complexity. Error bounds provide an upper bound on the generalization error of a model with a given level of confidence. A smaller error bound indicates lower sample complexity.\n",
        "\n",
        "- Rademacher Complexity: Rademacher complexity is a measure of the generalization error of a model that is based on the concept of Rademacher averages. Rademacher averages are a type of random average that is used to control for the randomness of the training data. A smaller Rademacher complexity indicates lower sample complexity.\n",
        "\n",
        "*3. Computational Complexity*\n",
        "Computational complexity is about the resources (time, memory) required to train or use a model.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Big O Notation**: Theoretical analysis of algorithms in terms of time and space complexity (e.g., O(n), O(n^2)) is standard.\n",
        "- **Time Profiling**: Empirically measuring the actual time taken to train models on datasets of varying sizes.\n",
        "- **Space Profiling**: Measuring the memory requirements of a model during training and inference.\n",
        "- **Algorithm Analysis**: Analyzing the specific steps of an algorithm to determine their computational requirements.\n",
        "\n",
        "- Time Complexity: Time complexity is a measure of the amount of time that an algorithm takes to run as a function of the input size. Time complexity is typically expressed using Big O notation, which provides an upper bound on the worst-case performance of the algorithm.\n",
        "\n",
        "- Space Complexity: Space complexity is a measure of the amount of memory that an algorithm requires to run as a function of the input size. Space complexity is also typically expressed using Big O notation.\n",
        "\n",
        "- Scalability: Scalability is a measure of how well an algorithm can handle increasing input sizes. An algorithm is said to be scalable if its time and space complexity grow sublinearly with the input size.\n",
        "\n",
        "*Integrating Theoretical and Empirical Approaches*\n",
        "In practice, a combination of these methods is often used. Theoretical measures provide a guideline or a framework, while empirical methods offer practical insights based on real-world data and computing environments. The choice of method depends on the specific model, the nature of the task, and the available resources. For example, in deep learning, empirical evaluation is heavily relied upon due to the complexity and non-linearity of the models, while in more traditional machine learning or statistical models, theoretical measures might be more prominently used."
      ],
      "metadata": {
        "id": "7FbmpALiSvmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Dimension</font> (VC, Fat-shattering, Pseudo, Effective, Hausdorff)*"
      ],
      "metadata": {
        "id": "ijtvzpIDxEzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensions**\n",
        "\n",
        "[Dimension](https://en.m.wikipedia.org/wiki/Dimension).Dimensionality measures are used to quantify the complexity of a set of points or a function class. They are often used in machine learning to bound the sample complexity of learning algorithms.\n",
        "* in mathematics, is a particular way of describing the size of an object (contrasting with measure and other, different, notions of size). Dimensionality measures are also used to study the relationships between different complexity classes. For example, it is known that the complexity class PSPACE is contained in the complexity class EXPTIME. This can be shown using the following theorem: Theorem: The metric entropy of any hypothesis class with VC dimension d is at most d. This theorem tells us that the complexity class PSPACE, which contains all problems that can be solved by a polynomial space Turing machine, is contained in the complexity class EXPTIME, which contains all problems that can be solved by an exponential time Turing machine. Dimensionality measures are a powerful tool for studying the complexity of computational problems. They can be used to bound the resource requirements of algorithms, and to study the relationships between different complexity classes.\n",
        "\n",
        "*Examples:*\n",
        "\n",
        "* [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension) is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting. There are several variations (various notions of effective dimension) of which the most common is effective Hausdorff dimension.\n",
        "\n",
        "  * effective dimension is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting\n",
        "\n",
        "  * to measure power / capacity of a model [Source](https://www.youtube.com/watch?v=fDIGmkq9xNE&t=2067s)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1398.png)\n",
        "\n",
        "* [Hausdorff dimension](https://en.m.wikipedia.org/wiki/Hausdorff_dimension) generalizes the well-known integer dimensions assigned to points, lines, planes, etc. by allowing one to distinguish between objects of intermediate size between these integer-dimensional objects.\n",
        "\n",
        "* [Intrinsic dimension](https://en.m.wikipedia.org/wiki/Intrinsic_dimension): The intrinsic dimension of a set of points is the smallest number of parameters needed to represent the points accurately.\n",
        "\n",
        "* [Fractal dimension](https://en.m.wikipedia.org/wiki/Fractal_dimension) provides a rational statistical index of complexity detail in a pattern. A fractal pattern changes with the scale at which it is measured. It is also a measure of the space-filling capacity of a pattern, and it tells how a fractal scales differently, in a fractal (non-integer) dimension.\n",
        "\n",
        "* [Lebesgue covering dimension](https://en.m.wikipedia.org/wiki/Lebesgue_covering_dimension) or topological dimension of a topological space is one of several different ways of defining the dimension of the space in a topologically invariant way\n",
        "\n",
        "* [Krull dimension](https://en.m.wikipedia.org/wiki/Krull_dimension)\n",
        "\n",
        "* [Inductive dimension](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "\n",
        "* [Minkowski–Bouligand dimension](https://en.m.wikipedia.org/wiki/Minkowski–Bouligand_dimension) also known as Minkowski dimension or box-counting dimension, is a way of determining the fractal dimension of a set S in a Euclidean space $\\mathbb {R} ^{n}$, or more generally in a metric space (X,d).\n",
        "\n",
        "* [Information dimension](https://en.m.wikipedia.org/wiki/Information_dimension) is a measure of the fractal dimension of a probability distribution. It characterizes the growth rate of the Shannon entropy given by successively finer discretizations of the space.\n",
        "\n",
        "  * In 2010, Wu and Verdú gave an operational characterization of [Rényi information dimension = Rényi entropy](https://en.m.wikipedia.org/wiki/Rényi_entropy) as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder.\n",
        "\n",
        "  * Enge Verbindung zwischen Entropy und Dimension measures.\n",
        "\n",
        "* [Correlation dimension](https://en.m.wikipedia.org/wiki/Correlation_dimension) is a measure of the dimensionality of the space occupied by a set of random points, often referred to as a type of fractal dimension.\n",
        "\n",
        "* [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "\n",
        "* [Equilateral dimension](https://en.m.wikipedia.org/wiki/Equilateral_dimension)\n",
        "\n",
        "* [Dimensions of commutative algebra](https://en.m.wikipedia.org/wiki/Glossary_of_commutative_algebra#dimension), like Embedding dimension"
      ],
      "metadata": {
        "id": "DFFd1liCm28M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vapnik-Chervonenkis, Pseudo- and Fat-Shattering Dimensions are three distinct notions of “dimension”. The phrase “dimension” is rather unfortunate, as the three “dimensions” have nothing to do with the dimension of a vector space, except in very special situations. Rather, these “dimensions” are combinatorial parameters that measure the “richness” of concept classes or function classes.*\n",
        "\n",
        "* [Vapnik–Chervonenkis dimension](https://en.m.wikipedia.org/wiki/Vapnik–Chervonenkis_dimension)\n",
        "  * It quantifies the expressive power or complexity of a hypothesis class in terms of its ability to \"shatter\" sets of points. It's a scalar value that provides insight into the capacity of a class of functions or models.\n",
        "  * A hypothesis class is said to \"shatter\" a set of points if, for every possible labeling of the points, there exists a hypothesis in the class that can perfectly classify those points according to that labeling. VC dimension is defined in terms of shattering.\n",
        "  * It is the most widely used complexity metric in PAC learning, and it has been used to prove a number of important theoretical results.\n",
        "  * However, the VC dimension is not a perfect metric for the complexity of a hypothesis class. For example, the VC dimension of the class of all linear classifiers is infinite, even though linear classifiers can be learned efficiently from a small number of training examples.\n",
        "  * In PAC learning: Let H be a hypothesis class with VC dimension d. Then, the sample complexity of PAC learning H is given by: $n >= O(d / ε^2 * ln(1/δ))$, where ε is the desired error tolerance and δ is the desired confidence level.\n",
        "  \n",
        "  * [*Bias-Variance Tradeoff*](https://en.m.wikipedia.org/wiki/Bias–variance_tradeoff): This is a key principle in statistical learning that helps to understand the tradeoff between model complexity and the risk of overfitting, which impacts the number of samples needed for learning.\n",
        "\n",
        "  * Higher VC-dimension of a class = more samples are required to learn that class. Bounds on sample complexity using VC-dimension are often given in the form, where d is the VC-dimension:\n",
        "\n",
        "  * $m \\geq \\frac{8}{\\epsilon} \\ln \\left(\\frac{4}{\\delta}\\right)+\\frac{4}{\\epsilon} d \\ln \\left(\\frac{2 e m}{d}\\right)$\n",
        "\n",
        "  * measure of the capacity: The more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data (-that was assumed for neural nets).\n",
        "\n",
        "  * The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data. sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C. measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm.\n",
        "\n",
        "  * A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data. The VC Dimension can provide an **upper bound on the sample complexity in terms of the size of the hypothesis class**, the desired error rate, and the confidence level.\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1397.png)\n",
        "\n",
        "  * *Shatter points = separate points: how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity. - Shattered:  A set of points is shattered by H if for every possible labeling of the points, there exists a hypothesis in H that agrees with that labeling.*\n",
        "\n",
        "  * One of the most fundamental results in learning theory is that the sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C (Source: Optimal Quantum Sample Complexity of Learning Algorithms)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1643.png)\n",
        "\n",
        "  * VC dimension is still a useful concept in machine learning. It can be used to understand the complexity of a neural network and to choose a neural network that is not too complex, so that it can generalize well to new data.\n",
        "\n",
        "  * Limitations of the VC dimension (VC dimension theorem would seem to imply that large neural networks would overfit very badly and never generalize well):\n",
        "    * The VC dimension is only a theoretical bound, and it is not always accurate in practice. The VC dimension theorem assumes that the training data is drawn from an i.i.d. (independent and identically distributed) distribution. However, in practice, the training data is often not i.i.d., and this can make the VC dimension theorem less accurate.\n",
        "    * The VC dimension does not take into account the complexity of the target function. The VC dimension is only a bound on the generalization error. It is possible for a neural network to have a large VC dimension and still generalize well, if the training data is large enough.\n",
        "    * The VC dimension does not take into account the optimization algorithm used to train the neural network. There are a number of techniques that can be used to prevent neural networks from overfitting, such as regularization and dropout. These techniques can help to reduce the complexity of the neural network and make it more likely to generalize well.\n",
        "\n",
        "  * https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_VC_Dimension_Shatter.php\n",
        "\n",
        "  * Code example: https://www.geeksforgeeks.org/vapnik-chervonenkis-dimension/\n",
        "\n",
        "  * Video: https://youtu.be/puDzy2XmR5c?si=FTPneApRNiWQkJey\n",
        "\n",
        "  * VC dimension is a method to measure model complexity. It is a measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm. A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data.\n",
        "\n",
        "  * The VC Dimension can provide an upper bound on the sample complexity in terms of the size of the hypothesis class, the desired error rate, and the confidence level.\n",
        "\n",
        "  * In [Vapnik–Chervonenkis theory](https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory), the [Vapnik–Chervonenkis (VC) dimension](v) is a **measure of the capacity (complexity, expressive power, richness, or flexibility)** of a set of functions that can be learned by a statistical binary classification algorithm.\n",
        "\n",
        "  * It is defined as the cardinality of the largest set of points that the algorithm can [shatter](https://en.m.wikipedia.org/wiki/Shattered_set), which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points.\n",
        "\n",
        "  * Relationship:\tThe VC dimension can be used to derive bounds on the sample complexity.\tThe sample complexity can be used to estimate the VC dimension.\n",
        "\n",
        "  * The VC dimension of a set of functions can be used to bound the generalization error of a learning algorithm. The generalization error is the error that a learning algorithm makes on new data that it has not seen before. The VC dimension theorem states that the generalization error of a learning algorithm is bounded by the VC dimension of the hypothesis space divided by the number of training examples. In other words, **the more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data** (-that was assumed for neural nets). The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data.\n",
        "\n",
        "* [Sauer's Lemma (or Sauer–Shelah Lemma)](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma): A combinatorial bound that relates the growth function to the VC dimension and the number of data points. It says that if a hypothesis class has a VC dimension \\( d \\) and does not shatter any set of \\( d+1 \\) points, then the number of dichotomies it can produce on any set of \\( n \\) points is bounded by the sum of binomial coefficients from \\( i = 0 \\) to \\( d \\).\n",
        "\n",
        "* [Natarajan dimension](https://en.m.wikipedia.org/wiki/Natarajan_dimension): An extension of VC dimension to multiclass classification problems.\n",
        "\n",
        "* **Fat-shattering dimension**:\n",
        "\n",
        "  * An extension of the VC dimension that considers the ability of a hypothesis class to shatter sets of points with margins. It's useful for analyzing algorithms that make use of margins, like Support Vector Machines. / [Fat-shattering dimension](https://mlweb.loria.fr/book/en/fatshattering.html) at a certain scale of a function class is defined as the maximal number of points that can be fat-shattered by this function class at that scale. It can be bounded for popular function classes such as for the set of linear functions.\n",
        "\n",
        "  * https://mathoverflow.net/questions/307201/vc-dimension-fat-shattering-dimension-and-other-complexity-measures-of-a-clas\n",
        "\n",
        "  * Covering Numbers and Fat-Shattering Dimension: measures of complexity of function classes that can be used to **derive upper bounds on sample complexity** (bounds on expressivity of class of CPTP maps (or unitaries) that a quantum machine learning model (QMLM) can implement in terms of number of trainable elements)\n",
        "\n",
        "  * **fat-shattering dimension and VC dimension (Vapnik-Chervonenkis dimension)** fat-shattering dimension is not an example of an inequality. It is a measure of the complexity of a function class. A function class with a small fat-shattering dimension is said to be easy to learn, while a function class with a large fat-shattering dimension is said to be difficult to learn.\n",
        "\n",
        "  * The fat-shattering dimension is a complexity measure used in statistical learning theory. It's one way of quantifying the **complexity or expressive power of a class of functions, and it's used to derive bounds on the generalization error** of a learning algorithm.\n",
        "\n",
        "  * The formal definition of the fat-shattering dimension is a bit technical, but here's the general idea: suppose you have a class of functions, and you want to understand how complex this class is. One way of doing this is to look at how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity.\n",
        "\n",
        "  * **Now, for the fat-shattering dimension, you add an additional constraint: you require a certain margin (the \"fatness\") between points that are labeled differently**. The largest set of points that can be shattered with a given margin is used to define the fat-shattering dimension of the function class.\n",
        "\n",
        "  * The fat-shattering dimension is related to the VC dimension (Vapnik-Chervonenkis dimension), another complexity measure that is widely used in statistical learning theory. However, **while the VC dimension only considers exact separation of points, the fat-shattering dimension takes into account this idea of a margin, which makes it more suitable for dealing with \"noisy\" or non-separable data.**\n",
        "\n",
        "  * the concept of the fat-shattering dimension is often used in the analysis of machine learning algorithms, particularly those based on empirical risk minimization. It can help to understand the trade-off between the expressive power of a function class (which often corresponds to the complexity of a learning algorithm) and the ability of the algorithm to generalize well to unseen data.\n",
        "\n",
        "* **Pseudo-Dimension**: Similar in spirit to the VC dimension but adapted for real-valued function classes rather than binary classifiers. / [Pseudo-dimension](https://link.springer.com/chapter/10.1007/978-1-4471-3748-1_4), also Pollard dimension, is a generalization of the VC-dimension to real-valued functions. The fat-shattering dimension, unlike the Pseudo-dimension, is a “scale-sensitive” measure of richness.\n"
      ],
      "metadata": {
        "id": "yKc3PDS6bXwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Inequality (bound, complexity)</font> (Cramér–Rao, Trace , Cauchy-Schwarz, Rademacher, Hoeffding's)*"
      ],
      "metadata": {
        "id": "ThL-AmiaxI8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Concentration inequalities**](https://en.m.wikipedia.org/wiki/Concentration_inequality) = bounds probability of estimator deviating from its expected value by a certain amount = provide guarantees on accuracy of estimator. Inequalities above just bound magnitude of deviation, bound probability. Will a learning algorithm converge to correct hypothesis with high probability, even if data is noisy or incomplete? Concentration inequalities can be used to bound VC dimension of hypothesis space to design ML algorithms that are guaranteed to generalize well to new data. And in regularization (to control complexity of ML models): analyze effects of regularization on generalization performance of ML models. Concentration inequalities don't guarantee learning a function with small error, but provide high degree of confidence that algorithm is likely to learn a good function, given a sufficient number of training samples and a suitable regularization scheme.\n",
        "\n",
        "* Matrix concentration inequalities: use by Ewin tang for dequantization, see also: https://arxiv.org/abs/1501.01571 - An Introduction to Matrix Concentration Inequalities. Some common matrix concentration inequalities that might appear in Tang's work include:\n",
        "\n",
        "  * Matrix Bernstein Inequality: Provides bounds on the deviation of the sum of random matrices from its expectation. It has variants tailored to handling sums of matrices that aren't independent.\n",
        "  * Matrix Hoeffding Inequality: Generalizes Hoeffding's inequality (which is for sums of random variables) to the matrix setting.\n",
        "  * Matrix Chernoff Bound: Gives tail bounds on how much a random matrix might deviate from its mean.\n",
        "\n",
        "\n",
        "* [Markov's inequality](https://en.m.wikipedia.org/wiki/Markov%27s_inequality): simplest concentration inequality. Bounds probability that random variable deviates from its expected value by a certain amount = bounds probability of errors in statistical estimators, proves convergence of learning algorithms, analyzes performance of randomized algorithms.\n",
        "\n",
        "* [Hoeffding's inequality](https://en.m.wikipedia.org/wiki/Hoeffding%27s_inequality):\n",
        "  * We want to guarantee that a hypothesis with a small training error will have good accuracy on unseen examples, and one way to do so is with Hoeffding bounds. This characterizes the deviation between the true probability of some event and its observed frequency over m independent trails. [Source](https://www.cis.upenn.edu/~danroth/Teaching/CS446-17/LectureNotesNew/colt/main.pdf)\n",
        "  * states that probability that average of a number of independent and identically distributed random variables deviates from its expected value by more than a certain amount is exponentially small in number of random variables.\n",
        "  * average of n independent random variables deviates from its expected value by more than ε at most $2e^{(-2nε^2)}$ probability and decays exponentially with number of random variables -> Probability of deviation becomes very small as number of random variables increases. **This inequality bounds generalization error of a learning algorithm, regardless of the model complexity or the sample complexity. Can show that sample complexity of learning a linear regression model with a certain level of accuracy is proportional to logarithm of number of features.**  \n",
        "  * Hoeffding's inequality is a **special case of the Azuma–Hoeffding inequality and McDiarmid's inequality**. It is **similar to the Chernoff bound**, but tends to be less sharp, in particular when the variance of the random variables is small. It is **similar to, but incomparable with, one of Bernstein's inequalities**.\n",
        "  * Example: bound sample complexity of online learning algorithm (with perceptron):\n",
        "    * $X_1, X_2, ..., X_n$ be independent random variables with mean $\\mu$ and variance $\\sigma^2$.\n",
        "    * For any $\\delta > 0$, $P(\\bar{X} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\bar{X}$ is average of $X_i$.\n",
        "    * $L_i$ be loss of Perceptron on $i$th training sample. Average loss of Perceptron on training data is: $\\bar{L} = \\frac{1}{n} \\sum_{i=1}^n L_i$.\n",
        "    * Objective: Use concentration inequality to show that average loss of Perceptron on training data is close to its expected value with high probability, given a sufficient number of training samples: $P(\\bar{L} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\mu$ is expected loss of Perceptron on training data.\n",
        "    * With probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$, average loss of Perceptron on training data is within $\\delta$ of its expected value.\n",
        "    * Set $\\delta$ to be a small value, such as $0.01$, to ensure that average loss of Perceptron on training data is very close to its expected value with high probability.\n",
        "    * Hoeffdings inequality bounds sample complexity: $n \\geq \\frac{2 \\sigma^2 \\ln(1 / \\delta)}{\\delta^2}$ tells us that if we have $n$ training samples, then average loss of Perceptron on training data is within $\\delta$ of its expected value with probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$.\n",
        "\n",
        "* [Azuma's inequality](https://en.m.wikipedia.org/wiki/Azuma%27s_inequality)\n",
        "\n",
        "* [Chernoff bounds](https://en.m.wikipedia.org/wiki/Chernoff_bound): Bound probability of sum of independent random variables deviating from its expected value by more than a certain amount, even if the random variables are not identically distributed. Generalization of Hoeffding's inequality, used to bound tails of probability distributions.\n",
        "\n",
        "* [Bernstein's inequality](https://en.m.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)): generalization of Chernoff bounds. Bounds probability, even if the random variables are not identically distributed and have non-identical variances (used to bound tails of sub-Gaussian distributions).\n",
        "\n",
        "* [McDiarmid's inequality](https://en.m.wikipedia.org/wiki/McDiarmid%27s_inequality): bounds deviation between sampled value and expected value of certain functions (= functions that satisfy a bounded differences property, meaning that replacing a single argument to the function while leaving all other arguments unchanged cannot cause too large of a change in the value of the function) when they are evaluated on independent random variables.\n",
        "\n",
        "* [Bennett's inequality](https://en.m.wikipedia.org/wiki/Bennett%27s_inequality): provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. **Can be used to: Bound the generalization error of learning algorithms.**\n",
        "\n",
        "* [(Bienaymé–) Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality): provide upper bounds on the probability that a random variable deviates from its expected value by more than a certain amount. Chebyshev's inequality is particularly useful for bounding the deviations of random variables with finite mean and variance. One way to think about Chebyshev's inequality is that it provides a measure of how concentrated the probability distribution of a random variable is around its expected value. The more concentrated the probability distribution is, the lower the probability is that the random variable will deviate from its expected value by more than a certain amount. **Can be used to: Bound the probability of rare events**. Design confidence intervals for population parameters. Develop efficient algorithms for statistical testing.\n",
        "\n",
        "* [Vysochanskij–Petunin inequality](https://en.m.wikipedia.org/wiki/Vysochanskij%E2%80%93Petunin_inequality):  It is a refinement of Chebyshev's inequality that provides tighter bounds on the probability that a random variable deviates from its expected value by more than a certain amount, especially for random variables with unimodal distributions."
      ],
      "metadata": {
        "id": "2WMz6oFrnTD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Inequality (complexities)](https://en.m.wikipedia.org/wiki/Inequality_(mathematics)): bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.** See [Inequalities in information theory](https://en.m.wikipedia.org/wiki/Inequalities_in_information_theory), [Information geometry](https://en.m.wikipedia.org/wiki/Information_geometry), [Information projection](https://en.m.wikipedia.org/wiki/Information_projection), and [Confidence Interval](https://en.m.wikipedia.org/wiki/Confidence_interval).\n",
        "\n",
        "Evaluate performance of estimators: bounds on Prediction Errors, Sample Complexity, Model complexity, and Deviation of Estimators. By bounding error: ensure that model is not too far from optimal solution (Regularize models by bounding complexity of model: L1 norm to bound number of parameters, L2 norm to bound sum of squared parameters).\n",
        "\n",
        "* [Triangle inequality](https://en.m.wikipedia.org/wiki/Triangle_inequality). K-means clustering: cost function is sum of squared distances between data points and cluster centroids. Squared distances can be bounded by triangle inequality.\n",
        "\n",
        "* [Cramér–Rao bound](https://en.m.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound): lower bound (min value) on variance of an unbiased estimator (but cannot tell about probability of deviation) - this value is inverse of [Fisher information](https://de.m.wikipedia.org/wiki/Fisher-Information#Verwendung)(how much information data provides about parameters being estimated). Used to design estimators that are as efficient as possible.\n",
        "\n",
        "  * [*Quantum Cramér-Rao Bound*](https://en.m.wikipedia.org/wiki/Quantum_Cramér–Rao_bound)\n",
        "\n",
        "  * QFIM and Quantum Cramér-Rao Bound used in quantum metrology to **quantify ultimate limit to precision** that can be achieved in estimating parameters (CRB (derived from FIM) tells us smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator. Fisher Information is a measure of how much information a random variable provides about an unknown parameter).\n",
        "\n",
        "  * Is an inequality: bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.\n",
        "\n",
        "  * The Cramér-Rao bound (CRB) belongs to **model complexity** = Bounds on Precision of Estimator. It gives a measure of the best possible precision that can be achieved when estimating that parameter from a given set of data. The **quantum Cramér-Rao bound provides a limit on the precision with which these properties can be estimated, given the inherent uncertainties of quantum mechanics**.\n",
        "\n",
        "  * It provides a lower bound on the covariance matrix of any unbiased quantum estimator (The covariance matrix determines the uncertainty or precision of the estimated parameter). It characterizes the best possible precision that can be achieved in estimating a parameter of interest from quantum measurements.\n",
        "\n",
        "  * The quantum Cramér-Rao bound serves as a powerful tool for understanding the fundamental limits of precision in quantum state estimation (without consoidering noise, imperfections etc).\n",
        "\n",
        "  * The quantum Cramér-Rao bound is given by the inverse of the **Fisher information matrix**, which in the quantum case is calculated using the quantum state and the POVM (Positive Operator-Valued Measure) elements describing the measurements. It can provide insights into the optimal measurement strategies for estimating the parameters of a quantum state, and can help guide the design of quantum sensors and other quantum technologies.\n",
        "\n",
        "  * The basic formula for calculating the quantum Cramér-Rao bound (QCRB) for parameter estimation:\n",
        "\n",
        "  * $QCRB = Tr(F^{-1} M)$\n",
        "\n",
        "  * where:\n",
        "\n",
        "    - QCRB is the quantum Cramér-Rao bound, which provides a lower bound on the covariance matrix of any unbiased estimator.\n",
        "    - $F$ is the Fisher information matrix, which quantifies the amount of information provided by the measurements about the parameter of interest.\n",
        "    - $M$ is the symmetric, positive semidefinite matrix representing the measurement operators' influence on the parameter estimation.\n",
        "\n",
        "  * To calculate the QCRB, you'll need to obtain the Fisher information matrix (F) and the measurement matrix (M) for the chosen estimation strategy. The Fisher information matrix can be computed based on the measurement operators and the quantum state being measured.\n",
        "\n",
        "  * Cramér-Rao is a lower bound on the variance of any unbiased estimator of a parameter, given a fixed model. The CRB depends on the Fisher information, which is a measure of how much information the data provides about the parameter. A higher Fisher information means that the data is more informative about the parameter, and therefore the CRB is lower.\n",
        "\n",
        "  * In general, the CRB is a more fundamental concept than the sample complexity. It is used to understand the fundamental limits of parameter estimation, regardless of the amount of data available. The sample complexity, on the other hand, is more practical, as it tells us how much data we need to achieve a certain level of accuracy.\n",
        "\n",
        "  * In machine learning, the Cramér-Rao Bound (CRB) is often used as a tool for **understanding the fundamental limits of learning algorithms, particularly in the field of parameter estimation and model selection**. Here are some specific applications:\n",
        "\n",
        "    1. Variance Estimation: Similar to financial economics, the CRB provides a theoretical lower limit for the variance of an unbiased estimator. This can help understand how well a learning algorithm might perform in terms of parameter estimation, given a certain amount of data.\n",
        "\n",
        "    2. Model Complexity: The CRB can provide insights into how model complexity affects estimation accuracy. A model with too many parameters may have a higher CRB (i.e., higher variance for the best possible estimator), indicating that it could be more prone to overfitting.\n",
        "\n",
        "    3. Algorithm Evaluation: Researchers might use the CRB to evaluate and compare the performance of different learning algorithms. If an algorithm's performance is close to the CRB, it might be deemed near-optimal.\n",
        "\n",
        "    4. Designing Neural Networks: In the design of neural networks, the CRB can be used to understand the limit of what the network can learn from the data, which can help in making decisions about network architecture and training strategies.\n",
        "\n",
        "* [Trace_inequality](https://en.m.wikipedia.org/wiki/Trace_inequality) bounds the trace of a matrix by the sum of the traces of its eigenvalues. Trace inequality, also known as the triangle inequality for the trace distance, states that for any three quantum states ρ, σ, and τ, the following inequality holds:\n",
        "  * $δ(ρ, σ) + δ(σ, τ) ≥ δ(ρ, τ)$\n",
        "  * where δ(ρ, σ) is the trace distance between ρ and σ.\n",
        "  * Bounding the error of quantum learning algorithms: The trace inequality can be used to derive upper bounds on the error of quantum learning algorithms. This is useful for understanding the performance of quantum learning algorithms and for comparing them to classical learning algorithms.\n",
        "  *  The trace inequality can be used to derive bounds on the error of a wide variety of quantum learning algorithms, including algorithms for learning linear classifiers, support vector machines, and neural networks.\n",
        "  * It is important to note that the bounds derived using the trace inequality are often loose. This is because the trace inequality is a very general tool, and it does not take into account the specific structure of the quantum learning algorithm or the training data. However, the trace inequality can still be used to get a rough estimate of the error of a quantum learning algorithm.\n",
        "\n",
        "* [Bohnenblust-Hille inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality):\n",
        "\n",
        "  * The Bohnenblust-Hille inequality says that the $\\ell^{\\frac{2 m}{m+1}}$-norm of the coefficients of an $m$-homogeneous polynomial $P$ on $\\mathbb{C}^n$ is bounded by $\\|P\\|_{\\infty}$ times a constant independent of $n$, where $\\|\\cdot\\|_{\\infty}$ denotes the supremum norm on the polydisc $\\mathbb{D}^n$. [Source](https://annals.math.princeton.edu/wp-content/uploads/annals-v174-n1-p13-s.pdf)\n",
        "\n",
        "\n",
        "* [Littlewood's 4/3 inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality)\n",
        "\n",
        "  * is a norm inequality for multilinear forms and polynomials. It is used to study the behavior of random variables and multilinear forms. Can be used to derive concentration inequalities in some cases. For example, it can be used to prove that the coefficients of a random homogeneous polynomial are concentrated around their expected values. This can then be used to prove concentration inequalities for other random variables, such as the output of a neural network. Used in \"Learning to predict arbitrary quantum processes\".\n",
        "\n",
        "\n",
        "* [Golden–Thompson inequality](https://en.m.wikipedia.org/wiki/Golden%E2%80%93Thompson_inequality) bounds difference between logarithm of trace of a matrix and sum of logarithms of its eigenvalues. Used in 'A survey on the complexity of learning quantum states' page 17 - difference between real state and shadow tomography state)\n",
        "\n",
        "* [Cauchy–Schwarz inequality](https://en.m.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality). Cauchy-Schwarz inequality used to bound sum of squared residuals (cost function for linear regression) by the sum of the squared predicted values and the sum of the squared actual values. Used in *A Survey of Quantum Learning Theory* page 11 to compute upper bounds of learning algorithm.\n",
        "\n",
        "* [Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality) bounds probability that a random variable deviates from its expected value by more than a certain amount (k standard deviations at most 1/k^2).\n",
        "\n",
        "* [Rademacher Complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity):\n",
        "  * upper bound on sample complexity = on learnability of function classes (deriving generalization bounds). Measures ability of functions in class to fit to random noise. [Rademacher Complexity PDF](https://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf).\n",
        "  * When the Rademacher complexity is small, it is possible to learn the hypothesis class H using [empirical risk minimization](https://en.m.wikipedia.org/wiki/Empirical_risk_minimization).\n",
        "\n",
        "* [Gaussian complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity#Gaussian_complexity):  similar complexity to Rademacher. Can be obtained from Rademacher using random variables $g_i$ instead of $\\sigma_i$, where $g_i$ are Gaussian i.i.d. random variables with zero-mean and variance 1, i.e. $g_i \\sim \\mathcal{N}(0,1)$. Gaussian and Rademacher are equivalent up to logarithmic factors.\n",
        "\n",
        "* [Fano's inequality](https://en.m.wikipedia.org/wiki/Fano%27s_inequality): is a lower bound on the average probability of error in a multiple hypothesis testing problem. It states that the average probability of error in a multiple hypothesis testing problem is at least as high as the entropy of the hypothesis distribution divided by the natural logarithm of two. It can be used to: Design efficient multiple hypothesis testing algorithms. Lower bound the generalization error of learning algorithms. Develop new algorithms for information compression and transmission. Fano's inequality is a powerful tool for analyzing the performance of multiple hypothesis testing algorithms and other machine learning algorithms.\n",
        "\n",
        "* [Data processing inequality](https://en.m.wikipedia.org/wiki/Data_processing_inequality): is a concept that states that the information content of a signal cannot be increased via a local physical operation. This can be expressed concisely as 'post-processing cannot increase information'. It is an inequality that relates the mutual information between two random variables before and after a data processing channel. It states that the mutual information between two random variables after a data processing channel cannot be greater than the mutual information between the two random variables before the channel. It can be used to: prove that certain learning problems are impossible to solve perfectly.\n",
        "Design efficient algorithms for information compression and transmission. It is a powerful tool for analyzing the flow of information through systems."
      ],
      "metadata": {
        "id": "r6LqFuIXrNmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A norm inequality (like Bohnenblust-Hille inequalities) is an inequality that involves the norms of vectors and matrices. Norms are functions that measure the size of vectors and matrices. There are many different types of norms, but some of the most common include the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "Norm inequalities are useful in many different areas of mathematics, including functional analysis, numerical linear algebra, and machine learning. For example, norm inequalities can be used to prove bounds on the error of numerical algorithms, to design efficient machine learning algorithms, and to analyze the stability of differential equations.\n",
        "\n",
        "Here are some examples of norm inequalities:\n",
        "\n",
        "* **Triangle inequality:** $\\|x + y\\| \\leq \\|x\\| + \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Cauchy-Schwarz inequality:** $\\|⟨x, y⟩\\| \\leq \\|x\\| \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Hölder's inequality:** $\\left\\| \\sum_{i=1}^n a_i b_i \\right\\| \\leq \\left\\| \\sum_{i=1}^n |a_i|^p \\right\\|^{1/p} \\left\\| \\sum_{i=1}^n |b_i|^q \\right\\|^{1/q}$, where $p$ and $q$ are conjugate exponents (i.e., $1/p + 1/q = 1$).\n",
        "* **Submultiplicativity of the Frobenius norm:** $\\|AB\\|_F \\leq \\|A\\|_F \\|B\\|_F$ for all matrices $A$ and $B$.\n",
        "\n",
        "Norm inequalities can be used to prove many other important results in mathematics. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations. The Cauchy-Schwarz inequality can be used to prove the Bessel inequality, which is a fundamental result in Fourier analysis. Hölder's inequality can be used to prove the Minkowski inequality, which is another important result in functional analysis.\n",
        "\n",
        "Norm inequalities are a powerful tool in mathematics, and they have many applications in science and engineering.\n",
        "\n",
        "*Yes, the triangle inequality is a norm inequality. The triangle inequality states that the norm of the sum of two vectors is less than or equal to the sum of the norms of the two vectors. In other words, for any two vectors $x$ and $y$, we have:*\n",
        "\n",
        "```\n",
        "||x + y|| <= ||x|| + ||y||\n",
        "```\n",
        "\n",
        "This inequality holds for any norm, including the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "The triangle inequality is a fundamental property of norms, and it has many important consequences. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations, and it can also be used to design efficient machine learning algorithms.\n",
        "\n",
        "The triangle inequality is also related to the triangle inequality in Euclidean geometry. The triangle inequality in Euclidean geometry states that the sum of the lengths of any two sides of a triangle must be greater than or equal to the length of the third side. This inequality can be proven using the triangle inequality for norms.\n",
        "\n",
        "In conclusion, the triangle inequality is a norm inequality that has many important applications in mathematics, science, and engineering."
      ],
      "metadata": {
        "id": "6eeeLVg3nFxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means that a generalization bound is uniform?**\n",
        "\n",
        "A generalization bound is said to be uniform if it holds for all hypotheses in a given hypothesis class, rather than just for a specific hypothesis. This is important because it allows us to make guarantees about the performance of our learning algorithm on any new data, regardless of the specific hypothesis that it learns.\n",
        "\n",
        "Uniform generalization bounds are typically derived using a technique called **Rademacher complexity**. Rademacher complexity is a measure of the complexity of a hypothesis class, and it can be used to bound the generalization error of any learning algorithm that uses that hypothesis class.\n",
        "\n",
        "Uniform generalization bounds are particularly useful for complex hypothesis classes, such as the class of all neural networks. These hypothesis classes have infinite VC-dimension, which makes it difficult to derive generalization bounds using traditional methods. However, uniform generalization bounds based on Rademacher complexity can be applied to any hypothesis class, regardless of its VC-dimension.\n",
        "\n",
        "Some examples of uniform generalization bounds include:\n",
        "\n",
        "* The generalization bound for uniformly stable learning algorithms\n",
        "* The generalization bound for overparameterized neural networks in kernel regimes\n",
        "\n",
        "Uniform generalization bounds are an important tool for understanding and analyzing the performance of machine learning algorithms. They allow us to make guarantees about the ability of our algorithms to generalize to new data, even when the hypothesis class is complex.\n",
        "\n",
        "Here is a simple analogy to help understand uniform generalization bounds:\n",
        "\n",
        "Imagine that you are trying to train a model to predict whether a given email is spam or not. You have a dataset of labeled emails, and you use this dataset to train a machine learning algorithm.\n",
        "\n",
        "Once the algorithm is trained, you want to be able to use it to predict the spam/ham label of any new email, even if that email is not in the training dataset.\n",
        "\n",
        "A uniform generalization bound tells you that the probability that your algorithm will make a mistake on any new email is less than a certain threshold. This threshold is independent of the specific email that you are trying to predict.\n",
        "\n",
        "In other words, a uniform generalization bound guarantees that your algorithm will generalize well to any new data, even if that data is very different from the training data."
      ],
      "metadata": {
        "id": "WLrjg16KskW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Norm</font> (Frobenius, Jacobian, Spectral, Trace)*"
      ],
      "metadata": {
        "id": "8SsJmnB0xNNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) measure size or length of vectors**. Compute error of model or regularize models. Analyze complexity of algorithms in terms of space or time. A norm-based measure of complexity in computational learning theory is a **measure that quantifies the difficulty of learning a function based on the norm of the function**. The norm of a function is a measure of the size or magnitude of the function.\n",
        "* Both the Frobenius norm and the spectral norm can be used to derive generalization bounds for machine learning algorithms. **Norm-based generalization bounds provide a strong theoretical guarantee on the performance of a machine learning algorithm on unseen data.**\n",
        "* Specific machine learning task require specific norm-based measure of complexity. All norm-based measures of complexity provide a useful way to quantify the difficulty of learning a function.\n",
        "* Norm kann von [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall: norm of vector is square root of inner product of vector by itself. Complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "* In optimization: define objective function and constraints. Computer science: define complexity of algorithms.\n",
        "* Classical vs Quantum probability theory:\n",
        "  * classical probability theory: 1-Norm: ∑ pi = ∑ | p | = || p || 1 = 1\n",
        "  * quantum probabiliyt theory: 2-Norm: || | ψ > || ^2 = 1\n",
        "* Reeller Vektor für 1-, 2-, 3- und $\\infty$-Normen des Vektors $x=(3,-2,6)$:\n",
        "  * $\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "* Komplexer Vektor für 1-, 2-, 3- und $\\infty$-Normen des Vektors $x=(3-4 i,-2 i)$:\n",
        "  * $\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}$"
      ],
      "metadata": {
        "id": "n7MLTBvFGN7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Normen auf endlichdimensionalen Vektorräumen*](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "* [Zahlnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): zB [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion)\n",
        "* [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen):\n",
        "  * zB [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$.\n",
        "  * [Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) ,\n",
        "  * Euklidische Norm und [Maximumsnorm](https://de.m.wikipedia.org/wiki/Maximumsnorm). L2 norm more robust to outliers than L1 norm.\n",
        "* [Matrixnorm](https://de.m.wikipedia.org/wiki/Matrixnorm):\n",
        "  * Matrix norm is a function that measures the size of a matrix. It must satisfy the following properties:\n",
        "    * ||A|| ≥ 0 for all matrices A.\n",
        "    * ||A|| = 0 if and only if A is the zero matrix.\n",
        "    * ||cA|| = |c| ||A|| for all scalars c and all matrices A.\n",
        "    * ||A + B|| ≤ ||A|| + ||B|| for all matrices A and B.\n",
        "  * **The trace norm is a matrix norm**, but it is not the only one. **Other examples of matrix norms include the Frobenius norm and the spectral norm**.\n",
        "  * [Natürliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): größtmöglicher Streckungsfaktor, der durch Matrix auf Vektor entsteht. [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm): größtmöglicher Streckungsfaktor, der durch Matrix auf Vektor der Länge Eins entsteht = maximalen Singulärwert, [Singulärwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "  * Various matrix norms provide different ways to quantify the size or complexity of matrices. While the Fisher Information Matrix is not a matrix norm, it's still a matrix, and depending upon the application, different matrix norms might be used to analyze its properties.\n",
        "* $\\hookrightarrow$ [Frobenius norm](https://de.m.wikipedia.org/wiki/Frobeniusnorm):\n",
        "  * special case of matrix norm. Frobenius norm of a matrix is the square root of the sum of the squares of all the elements of the matrix. Relatively easy to compute. Used in quantum state tomography (see *A Survey of Quantum Learning Theory* page 15. [interesting article](https://www.inference.vc/generalization-and-the-fisher-rao-norm-2/).\n",
        "  * E.g. Learn linear regression model from set of training data. One way: use ordinary least squares (OLS) algorithm (minimizes sum of squared residuals to find model). Frobenius norm: derive generalization bound for OLS algorithm that guarantees that probability of OLS algorithm making a large mistake on unseen data is small.\n",
        "* $\\hookrightarrow$ [Schatten p-Norm](https://en.m.wikipedia.org/wiki/Schatten_norm):\n",
        "  * The Schatten norm of the difference between the unitary matrices generated by the circuit and a target unitary operation can be used as a measure of the expressivity. Smaller values of the Schatten norm indicate higher expressivity. Useful measure of complexity for learning linear models and neural networks.\n",
        "  * Schatten norm is a class of matrix norms that includes the trace norm as a special case. The Schatten norm of order p, denoted by ||A||_p, is defined as the pth root of the sum of the pth powers of the singular values of A.\n",
        "* $\\hookrightarrow$ **Jacobian norm**\n",
        "  * is the maximum norm of the Jacobian matrix of a function. It is a measure of how the function changes with respect to its inputs.\n",
        "  * Jacobian norm is a useful tool for measuring the sensitivity of a function to its inputs and for designing robust control systems.\n",
        "  * The Jacobian matrix represents how changes in input affect changes in output in a vector-valued function. The Fisher Information matrix can sometimes be expressed in terms of derivatives (like the Jacobian), particularly when trying to understand how parameter changes influence the model's predictive distribution.\n",
        "* $\\hookrightarrow$ **Spectral norm**\n",
        "  * special case of matrix norm\n",
        "  * spectral morm of a matrix is the largest singular value of the matrix. Spectral norm is a more restrictive measure of complexity than the Frobenius norm, and more difficult to compute.\n",
        "  * This measures the largest singular value of a matrix. In the context of neural networks, it has been used to understand the smoothness of the loss landscape, which can be somewhat related to Fisher Information by providing insights into how parameters changes impact the output.\n",
        "  * Spectral norm is a useful tool for analyzing the stability of numerical algorithms and for studying the behavior of dynamical systems.\n",
        "  * The spectral norm of the Jacobian matrix of a function is an upper bound on the Jacobian norm of the function itself. This means that the spectral norm of the Jacobian matrix can be used to measure the maximum amount by which the function can change with respect to its inputs. Here is a simple example: Consider the following function: f(x) = x^2\n",
        "    * The Jacobian matrix of this function is simply the matrix [2x]. The spectral norm of the Jacobian matrix is 2x, and the Jacobian norm of the function is simply |2x|.\n",
        "    * If we set x = 1, then the spectral norm of the Jacobian matrix is 2, and the Jacobian norm of the function is also 2. However, if we set x = 10, then the spectral norm of the Jacobian matrix is 20, but the Jacobian norm of the function is still 2.\n",
        "    * This shows that the spectral norm of the Jacobian matrix is an upper bound on the Jacobian norm of the function itself.\n",
        "* $\\hookrightarrow$ [Trace norm (Nuclear norm)](https://en.m.wikipedia.org/wiki/Matrix_norm#Schatten_norms):\n",
        "  * It is sum of singular values of a matrix. Useful measure of complexity for learning linear models and neural networks. Special case of the Schatten norm, which is a class of matrix norms.\n",
        "  * Common distance measures in quantum information theory: here, \"distance measure\" is used in an informal sense, only three of the quantities introduced below are actual \"distances\" in the sense of a metric. First, and maybe most naturally, we can equip the set $\\mathcal{S}\\left(\\mathbb{C}^d\\right)$ with the (for convenience scaled) trace norm $I_i / 2$. This allows us to measure the difference between two quantum states $\\rho, \\sigma \\in$ $S\\left(\\mathbb{C}^d\\right)$ via the trace distance $\\left.\\left.\\|\\rho-\\sigma\\|_1 / 2=\\operatorname{tr} \\| \\rho-\\sigma\\right]\\right] / 2$. In fact, this distance is not only intuitive from a mathematical perspective, it also has an operational interpretation: The maximal success probability in distinghuishing $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$, assuming that either of the two is prepared with probability $1 / 2$, by performing a 2-outcome measurement on a single copy of the unknown state is given by $\\sup _{E \\in \\mathcal{E}(\\mathrm{C})} \\operatorname{tr}[E(\\rho-\\sigma)]+\\frac{1}{2}=\\frac{\\|\\rho-\\sigma\\|_1+1}{2}$ (see, e.g., $[17$, Chapter 9$]$ for a derivation).\n",
        "  * The trace distance is defined as half of the trace norm of the difference of the matrices\n",
        "  * The trace norm (also known as the nuclear norm) is a specific case of Schatten p-norm when p=1. It's the sum of singular values of a matrix. Sometimes, in the context of regularizing machine learning models, these norms are considered to constrain the learning capacity of the model. The trace of the Fisher Information Matrix can give a measure of the average sensitivity of the log-likelihood to parameter changes.\n",
        "* **Basis-path norm**: https://arxiv.org/abs/1809.07122\n"
      ],
      "metadata": {
        "id": "u_QCvV6brYZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Normen auf unendlichdimensionalen Vektorräumen*](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorräumen)\n",
        "* [Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen): $\\ell^{p}$ -Normen Verallgemeinerung der $p$ -Normen auf Folgenräume: $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$. Mit Normen werden $\\ell$ - Räume zu vollständigen normierten Räumen.\n",
        "* [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm): Für Grenzwert $p \\rightarrow \\infty$ ergibt sich Raum der beschränkten Folgen $\\ell^{\\infty}$\n",
        "* [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum, L-p-Raum.  $\\mathcal{L}^{p}$ -Normen definiert als $\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}$ (Summe durch Integral ersetzt).\n",
        "* [Diamond norm](https://en.m.wikipedia.org/wiki/Diamond_norm) (completely bounded trace norm) a measure defined between two quantum operations (two linear maps between operator spaces). Captures max difference between effects of two quantum operations.\n",
        "\n",
        "[*Normed Vector Spaces*](https://en.m.wikipedia.org/wiki/Normed_vector_space)\n",
        "* [L<sup>p</sup>-space](https://de.m.wikipedia.org/wiki/Lp-Raum) (Lebesgue Space) bestehen aus p-fach integrierbaren Funktionen. Für jede Zahl $0 < p \\leq \\infty$ ist $L^{p}$ -Raum definiert.\n",
        "* [Banach space](https://de.wikipedia.org/wiki/Banachraum): $\\mathbb{R}$<sup>n</sup> together with p-norm = vollständiger normierter Vektorraum (Lp-space over R^n). Viele Folgenräume $\\ell$ oder Funktionenräume $L$ sind unendlichdimensionale Banachräume.\n",
        "* [Hilbert space](https://de.wikipedia.org/wiki/Hilbertraum): Banachraum, dessen Norm durch Skalarprodukt induziert ist (p2-Norm, aber nicht p1 -> Prähilbertraum).\n",
        "* [Hardy space](https://de.m.wikipedia.org/wiki/Hardy-Raum): Untersucht man statt messbarer Funktionen nur holomorphe und harmonische Funktionen auf Integrierbarkeit im $L^{p}$-Raum\n",
        "* [F-space](https://en.m.wikipedia.org/wiki/F-space): for Lp-space 0 < p < 1. Admits complete translation-invariant metric with respect to which vector space operations are continuous.\n",
        "* [Fréchet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space): locally convex F-spaces.\n",
        "* [Sobolev space](https://de.wikipedia.org/wiki/Sobolev-Raum): Funktionenraum von schwach differenzierbaren Funktionen. Variationsrechnung: zur Lösungstheorie partieller Differentialgleichungen"
      ],
      "metadata": {
        "id": "8WHLzF42GF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Inner Product](https://de.wikipedia.org/wiki/Skalarprodukt) (auch Dot Product / Scalar Product / Skalarprodukt) ordnet zwei Vektoren eine Zahl (Skalar) zu, die die Ähnlichkeit messen** (über Länge, Winkel von Vektoren, oder ob diese senkrecht zueinander stehen)\n",
        "  * Skalarprodukt zweier Vektoren: $\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})$. Wenn $\\vec{a} \\cdot \\vec{b}= 0$, dann stehen orthogonal zueinander.\n",
        "  * [Dot product](https://en.m.wikipedia.org/wiki/Dot_product) or scalar product (special case): dot a ⋅ b.\n",
        "  * The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically can be viewed as volume scaling factor of linear transformation described by matrix.\n",
        "  * \"Inner Product of Functions\" says how similar two functions are (used in Fourier Transform): if they are orthogonal, then zero. if they are very similar, then they have a large inner product: $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "  * Take samples from functions - Up to infinity, you get integral (Riemann approximation of continuuos integral above): $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n",
        "  * [Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space) (Prähilbertraum bzw. Skalarproduktraum) generalize Euclidean spaces (in which inner product is dot product = scalar product) to vector spaces of any (possibly infinite) dimension."
      ],
      "metadata": {
        "id": "FeFQyIqardUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Entropy</font> (Metric, Relative, Conditional, Cross)*"
      ],
      "metadata": {
        "id": "l-DiG5X3xTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper: [The Entropy-Based Quantum Metric](https://www.mdpi.com/1099-4300/16/7/3878)"
      ],
      "metadata": {
        "id": "VKof1_SSUMvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Entropy](https://en.m.wikipedia.org/wiki/Entropy_(information_theory) measures uncertainty or information content of random variable, randomness (of a quantum state). For feature selection, information retrieval, and anomaly detection, & analysis of informational complexity of algorithms, mMeasuring amount of information that can be stored in quantum system, determining security of quantum communication protocols**\n",
        "* [Shannon entropy](https://en.m.wikipedia.org/wiki/Entropy_(information_theory)): Measure of randomness, disorder, average unpredictability, expected value of information, complexity of dataset - Shannon entropy is a probabilistic measure of average unpredictability in data (stochastic random process).\n",
        "* [Von Neumann entropy](https://en.m.wikipedia.org/wiki/Von_Neumann_entropy): most commonly entropy function. Is a measure of the mixedness of a quantum state. It is defined as the von Neumann entropy of the density matrix of the state. Defined as Shannon entropy of eigenvalues of density matrix of quantum state (=probabilities of finding the system in the corresponding eigenstates). - Entanglement = von neumann entropy?\n",
        "  * The von Neumann entropy can be used to characterize the amount of information present in a quantum state. For a pure state, the von Neumann entropy is zero, indicating that the state is completely known. For a mixed state, the von Neumann entropy is greater than zero, indicating that the state is not fully known.\n",
        "  * The von Neumann entropy can also be used to quantify the amount of entanglement between two quantum systems. Entanglement is a non-local correlation between two quantum systems, such that the state of one system cannot be fully described without knowing the state of the other system. The von Neumann entropy of entanglement is defined as the difference between the von Neumann entropy of the joint system and the sum of the von Neumann entropies of the individual systems. A higher von Neumann entropy of entanglement indicates a stronger entanglement between the two systems.\n",
        "  * Example: Consider two qubits, A and B, which are entangled in the Bell state: $|Bell_{00}> = \\frac{1}{\\sqrt{2}} (|00> + |11>)$. The von Neumann entropy of this state is zero, indicating that it is a pure state. However, if we measure the state of qubit A, we will collapse the state of qubit B to either |0> or |1>, depending on the outcome of the measurement. This means that the state of qubit B is not fully known until we measure qubit A.The von Neumann entropy of entanglement between qubit A and qubit B is 1 bit, indicating that they are maximally entangled. This means that the correlation between the two qubits is as strong as possible.\n",
        "\n",
        "* [Gibbs entropy](https://en.m.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula): the Gibbs entropy expression of the statistical entropy is a discretized version of Shannon entropy. The von Neumann entropy formula is an extension of the Gibbs entropy formula to the quantum mechanical case.\n",
        "* [Tsallis divergence (entropy)](https://en.m.wikipedia.org/wiki/Tsallis_entropy): Generalization of Shannon entropy, allows for wider range of values / different degrees of non-linearity.\n",
        "* [Renyi entropy](https://en.m.wikipedia.org/wiki/R%C3%A9nyi_entropy): Generalization of Shannon entropy. It is defined as $S_\\alpha(\\rho) = \\frac{1}{1-\\alpha} \\log \\left( \\sum_i p_i^\\alpha \\right)$, where $p_i$ are the eigenvalues of the density matrix $\\rho$ and $\\alpha$ is a real number.\n",
        "  * [(Max) Quantum Rényi Divergence](https://de.m.wikipedia.org/wiki/R%C3%A9nyi-Entropie): KL divergence in classical, would be quantum relativ entropy, but too hard to compute. Better: Maximal Quantum Rényi Divergence. arxiv.org/abs/2106.09567 and [video](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "* [Quantum relative entropy](https://en.m.wikipedia.org/wiki/Quantum_relative_entropy): Despite there being many other useful ways of determining distances between quantum states, we only discuss one more, namely the quantum relative entropy.\n",
        "  * The relative entropy H (P|Q) can, in some sense, be thought of as a measure of how much P and Q “resemble” each other. 72. Indeed, it takes its maximum value (i.e. 0) if and only if P = Q; it may become −∞ if P and Q have disjoint support, (i.e. when P (y)Q (y) = 0 for all y ∈ Y .)\n",
        "  * The quantum relative entropy between two quantum states $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$ that satisfy $\\operatorname{supp}(\\rho) \\cap \\operatorname{ker}(\\sigma)=\\emptyset$ is defined to be $D(\\rho \\| \\sigma):=\\operatorname{tr}[\\rho \\log (\\rho)]-\\operatorname{tr}[\\rho \\log (\\sigma)]$. Here, the logarithm is taken with base 2 . We can rewrite the relative entropy using the von Neumann entropy $S(\\rho)$, which is defined as $S(\\rho):=-\\operatorname{tr}[\\rho \\log (\\rho)]$.\n",
        "  * With this, the relative entropy becomes $D(\\rho \\| \\sigma)=-\\operatorname{tr}[\\rho \\log (\\sigma)]-S(\\rho)$. If $\\operatorname{supp}(\\rho) \\cap \\operatorname{ker}(\\sigma) \\neq \\emptyset$, then we define $D(\\rho \\| \\sigma):=+\\infty$. A useful result in quantum information is the non-negativity of the relative entropy, i.e., that $D(\\rho \\| \\sigma) \\geq 0$ holds for any two quantum states $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$. Moreover, for $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right), D(\\rho \\| \\sigma)=0$ implies $\\rho=\\sigma$ (see, e.g., [17, Theorem 11.7] for a proof).\n",
        "  * **However, the quantum relative entropy is neither symmetric nor does it satisfy a triangle inequality, so it does not define a metric**. Nevertheless, it is an important tool for comparing two quantum states. For example, when comparing a bipartite state $\\rho_{A B} \\in \\mathcal{S}\\left(\\mathbb{C}^{d_A} \\otimes \\mathbb{C}^{d_B}\\right)$ to $\\rho_A \\otimes \\rho_B$, the tensor product of its reduced density matrices, we obtain a measure for the correlation between the $A$ - and the $B$-system in the state $\\rho_{A B}$. This defines the quantum mutual information $I(A: B)_\\rho:=D\\left(\\rho_{A B} \\| \\rho_A \\otimes \\rho_B\\right)$.\n",
        "\n",
        "\n",
        "* [Kolmogov complexity](https://en.m.wikipedia.org/wiki/Kolmogorov_complexity): Measure randomness, disorder, or complexity in dataset - from algorithmic information theory to measure of computational resources needed to reproduce a piece of data, string etc. (length of shortest possible description of string, not computable, but gives absolute complexity of string independently of any specific probability distribution)\n",
        "* [Conditional entropy](https://en.m.wikipedia.org/wiki/Conditional_entropy): measures uncertainty of quantum state given knowledge of another quantum state. It is defined as $S(A|B) = S(\\rho_{AB}) - S(\\rho_B)$, where $\\rho_{AB}$ is the joint density matrix of the two quantum states, $\\rho_B$ is density matrix of second quantum state, and $S(\\rho)$ is entropy of density matrix $\\rho$.\n",
        "* Relative entropy: measures distinguishability between two quantum states. It is defined as $D(\\rho \\| \\sigma) = Tr (\\rho \\log \\rho) - Tr (\\rho \\log \\sigma)$, where $\\rho$ and $\\sigma$ are two quantum states.\n",
        "* [Metric entropy](https://en.m.wikipedia.org/wiki/Measure-preserving_dynamical_system#Measure-theoretic_entropy) measure of complexity of metric space. It is defined as the logarithm of the minimum number of open balls of radius δ needed to cover the space. Space with high metric entropy is more difficult to compress than a space with low metric entropy. Additionally, metric entropy can be used to estimate rate of convergence of certain algorithms. See [here](https://mathoverflow.net/questions/307201/vc-dimension-fat-shattering-dimension-and-other-complexity-measures-of-a-clas) metric entropy named under model complexity metrics\n",
        "* [Mutual information](https://en.m.wikipedia.org/wiki/Mutual_information) measures amount of information that one random variable X shares about another random variable Y. This information can be used to reduce amount of information required to describe X.\n",
        "* [Holevo's theorem](https://en.m.wikipedia.org/wiki/Holevo%27s_theorem) (Holevo's bound) upper limit on amount of classical information that can be extracted from quantum system: single qubit can exist in superposition of states with infinite amount of information, when we measure that qubit, we can extract at most 1 bit of classical information. Clear distinction between information capacity of quantum states and accessible information via measurements.\n",
        "* [Cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions over same underlying set of events measures average number of bits needed to identify an event drawn from set if a coding scheme used for set is optimized for an estimated probability distribution rather than true distribution. Cross-entropy will calculate a score that summarizes average difference between actual and predicted probability distributions for all classes. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* [Binary entropy](https://en.m.wikipedia.org/wiki/Binary_entropy_function): is defined as the entropy of a Bernoulli process with probability p of one of two values"
      ],
      "metadata": {
        "id": "kGLHOLSRrQ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Metrics</font> (Fisher information, Trace distance, Minkowski, Manhattan)*"
      ],
      "metadata": {
        "id": "oKvbzZ8cxOS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics: generally used to define a distance between two points in a metric space. Häufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion). See [more](https://franknielsen.github.io/Divergence/index.html). Properties:**\n",
        "  1. Positive Definitheit (**positive definiteness**):\n",
        "    * $d(x, y) \\geq 0$ (**non-negativity**)\n",
        "    * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) für alle $x, y \\in M$.\n",
        "  2. $d(x, y)=d(y, x)$ (**symmetry**)\n",
        "  3. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "  * Divergence fullfills property of positive definiteness (1). Distance (pseudometric) fullfills property of positive definiteness and symmetrie (1 + 2, but not 3 triangle inequality. Metrics and Norms fullfill property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3)."
      ],
      "metadata": {
        "id": "B8Sz3kzJn6VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Aus Normen erzeugte Metriken:*\n",
        "* [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances): aus einer $p$ -Norm abgeleitet (but p cannot be less than 1, because otherwise the triangle inequality does not hold). Wichtige Spezialfälle sind: Manhattan, Euclidean, Chebyshev.\n",
        "* [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "* [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$. The Euclidean distance is often used in classification problems because it is a natural measure of similarity between two vectors.\n",
        "* [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "* [Mahalanobis distance](https://de.m.wikipedia.org/wiki/Mahalanobis-Abstand) (see under 'Distances') is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables.\n",
        "\n",
        "* [Fréchet-Metrik](https://de.m.wikipedia.org/wiki/Fréchet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. Sie stellt eine Verbindung zwischen Metrik und Norm her.\n",
        "* [Cayley–Klein metric](https://en.m.wikipedia.org/wiki/Cayley%E2%80%93Klein_metric) is used in a variety of mathematical applications, including geometry, physics, and computer science. For example, it is used to define the distance between two points on the Poincaré disk, which is a model of the hyperbolic plane. It is an example of [Left- / right-/ bi-invariant Riemann metric](https://ncatlab.org/nlab/show/invariant+metric). Used in [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004)."
      ],
      "metadata": {
        "id": "IJBkzU6eFx1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nicht aus Normen erzeugte Metriken:*\n",
        "* [Riemannsche Mannigfaltigkeit (Metrik)](https://en.m.wikipedia.org/wiki/Riemannian_manifold), zB  Die kürzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geodäten) sind nicht zwingend Geradenstücke, sondern können gekrümmte Kurven sein. Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch größer (z. B. Kugel) oder kleiner (hyperbolische Räume) als 180° sein.\n",
        "* [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man könnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zurück.\n",
        "* [Fisher information metric](https://en.m.wikipedia.org/wiki/Fisher_information_metric):\n",
        "  * also: Fisher-Rao norm: as a common starting point for many measures of complexity currently studied in the literature (see work of Srebro’s group and Bartlett et al). https://www.mit.edu/~rakhlin/papers/myths.pdf, Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes (2017) Fisher-Rao Metric, Geometry, and Complexity of Neural Networks, https://arxiv.org/abs/1711.01530\n",
        "  * Fisher-Rao norm (or Fisher Information Metric) measures distance in the space of probability distributions, and it is defined in the context of information geometry. It is not a matrix norm per se\n",
        "  * The Fisher information metric is not a norm-induced metric. A norm-induced metric is a metric that can be defined from a norm on a vector space. The Fisher information metric is defined on a statistical manifold, which is a more general object than a vector space.\n",
        "  * To see why the Fisher information metric is not a norm-induced metric, consider the following. A norm-induced metric satisfies the triangle inequality, which states that the distance between two points is less than or equal to the sum of the distances between each point and a third point. However, the Fisher information metric does not satisfy the triangle inequality in general.\n",
        "  * For example, consider the statistical manifold of all Gaussian distributions with mean zero and variance one. The Fisher information metric on this manifold is given by the metric tensor $I^{-1}$, where $I$ is the Fisher information matrix. The Fisher information matrix for a Gaussian distribution with mean zero and variance one is given by $I = \\frac{1}{\\sigma^2}$, where $\\sigma$ is the standard deviation.\n",
        "  * Now, consider the following three points on this statistical manifold:\n",
        "    * $p_1$: The Gaussian distribution with mean zero and variance one.\n",
        "    * $p_2$: The Gaussian distribution with mean zero and variance two.\n",
        "    * $p_3$: The Gaussian distribution with mean zero and variance three.\n",
        "  * The Fisher information metric distances between these points are given by:\n",
        "    * $d(p_1, p_2) = \\sqrt{2}$\n",
        "    * $d(p_2, p_3) = \\sqrt{2}$\n",
        "    * $d(p_1, p_3) = \\sqrt{6}$\n",
        "  * We can see that the triangle inequality is not satisfied, since $\\sqrt{2} + \\sqrt{2} < \\sqrt{6}$. Therefore, the Fisher information metric is not a norm-induced metric.\n",
        "  * Although the Fisher information metric is not a norm-induced metric, it is still a useful metric for measuring the distance between points on a statistical manifold. It is often used in machine learning and statistics to quantify the amount of information that a set of data contains about a parameter.\n",
        "  * **The topic information geometry uses this to connect [Fisher information](https://en.m.wikipedia.org/wiki/Fisher_information) to differential geometry, and in that context, this metric is known as the Fisher information metric.**\n",
        "  * Fisher information is expected value of second derivative of log likelihood function. Fisher information measures how much the log likelihood function changes when the parameters of the distribution are changed (a measure of the sensitivity of the log likelihood function to changes in the parameters. The larger the Fisher information, the more sensitive the log likelihood function is to changes in the parameters).\n",
        "  * The Quantum Fisher Information Matrix (QFIM) is an extension of the concept of Fisher Information Matrix from classical statistics to quantum mechanics. It measures the amount of information that a quantum state carries about an unknown parameter. This parameter might be related to some aspect of the quantum state or a transformation applied to it.\n",
        "  * The Fisher Information Matrix (FIM) is a matrix whose entries are the expected values of the squared derivatives of the log-likelihood with respect to the parameters. The inverse of the FIM gives the Cramér-Rao Bound, which provides a lower limit on the covariance of any unbiased estimator. In other words, **the CRB (derived from the FIM) tells us the smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator.**\n",
        "  * **Fisher Information is a measure of how much information a random variable provides about an unknown parameter**. The Fisher information is defined as the expected value of the squared derivative of the log-likelihood function with respect to the parameter.\n",
        "  * The Fisher information can be used to construct confidence intervals and hypothesis tests for the parameter. It can also be used to derive the asymptotic properties of maximum likelihood estimators.\n",
        "  * The Fisher information is a non-negative quantity. A random variable that provides no information about the parameter has a Fisher information of 0.\n",
        "  * Measure of redundancy. How much of model is active vs inactive.\n",
        "  * Fisher information: Sensitivity of my parameters to my model space --> measure of model capacity (?), see video from amira qhack 2022\n",
        "  * Fisher information is the expected value of the second derivative of the log likelihood function\n",
        "  * Given a parameterized quantum state ρ(θ), where θ is the parameter vector we are interested in, the QFIM is defined in terms of the symmetric logarithmic derivative (SLD), which is a Hermitian operator L(θ) that satisfies a particular equation related to the derivative of the state ρ(θ).\n",
        "  * The entries of the QFIM, denoted as H(θ), are given by:\n",
        "  * $H_ij(θ) = \\frac{1}{2} Tr [ρ(θ) {L_i(θ), L_j(θ)}]$\n",
        "  * where $L_i$ and $L_j$ are the SLDs corresponding to the parameters $θ_i$ and $θ_j$, respectively, and { , } denotes the anticommutator.\n",
        "  * Like the classical Fisher Information Matrix, the QFIM can be used to define a lower bound on the variance of an unbiased estimator for the parameters θ. This is the Quantum Cramér-Rao Bound.\n",
        "  * In practical terms, the QFIM and Quantum Cramér-Rao Bound are used in quantum metrology to quantify the ultimate limit to precision that can be achieved in estimating parameters, such as phase shifts or magnetic fields, based on quantum mechanical measurements.\n",
        "  * **Yes, the Fisher-Rao norm, or Fisher Information Metric**, is indeed a metric in the sense that it defines a distance function on the space of probability distributions. Specifically, it provides a way to measure the distance between different probability distributions in a manner that takes into account their geometrical structure.\n",
        "  * Mathematically, for a parametric family of probability distributions $\\( P_\\theta \\)$, the Fisher Information Metric $\\( g_{\\theta \\theta'} \\)$ is defined by taking the expectation of the outer product of the score (the gradient of the log-likelihood) with respect to the parameters $\\( \\theta \\)$ and $\\( \\theta' \\)$.\n",
        "  * $\\[ g_{\\theta \\theta'}(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial \\log P_\\theta(X)}{\\partial \\theta} \\right) \\left( \\frac{\\partial \\log P_\\theta(X)}{\\partial \\theta'} \\right) \\right] \\]$\n",
        "  * Where the expectation is taken with respect to the distribution $\\( P_\\theta(X) \\)$.\n",
        "  * This matrix gives a Riemannian metric on the parameter space, defining a geometry that reflects how changes in parameters change the distributions. Consequently, it provides a natural distance measure between distributions (or models) that are nearby in parameter space.\n",
        "  * However, it's worth noting that while it's a \"metric\" in a geometrical sense, the Fisher Information Metric doesn’t satisfy all properties of a metric in the strict mathematical sense (like the triangle inequality). It does not measure \"distance\" in the conventional sense but quantifies the similarity or dissimilarity between statistical models or distributions based on their parametrizations. This concept and its implications are explored in the field of information geometry.\n",
        "* *what is a degenerate Fisher information matrix?*\n",
        "  * A degenerate Fisher information matrix is a Fisher information matrix that is not full rank. This means that there is at least one direction in which the Fisher information is zero. This can happen for a number of reasons, such as:\n",
        "    * When the model is overparameterized, meaning that there are more parameters than necessary to describe the data.\n",
        "    * When the parameters are not identifiable, meaning that there are multiple sets of parameters that can produce the same data.\n",
        "    * When the data is not informative enough to estimate all of the parameters.\n",
        "  * When the Fisher information matrix is degenerate, it means that there is some uncertainty in the estimation of the parameters that cannot be reduced by increasing the sample size. This is because the Fisher information matrix is a measure of the amount of information that the data contains about the parameters.\n",
        "  * Degenerate Fisher information matrices can be a problem in statistical inference, as they can lead to biased and inefficient estimates of the parameters. However, there are a number of ways to deal with degenerate Fisher information matrices, such as:\n",
        "    * Using regularization techniques to reduce the number of parameters in the model.\n",
        "    * Using constraints on the parameters to make them identifiable.\n",
        "    * Using Bayesian methods to estimate the parameters.\n",
        "  * It is important to note that a degenerate Fisher information matrix does not necessarily mean that the model is wrong. It is possible for a model to be correct even if the Fisher information matrix is degenerate. However, it is important to be aware of the potential problems that can arise from degenerate Fisher information matrices when interpreting the results of statistical analyses.\n",
        "  * Here are some examples of models that can have degenerate Fisher information matrices:\n",
        "    * A linear regression model with more predictors than observations.\n",
        "    * A logistic regression model with a constant predictor.\n",
        "    * A time series model with a seasonal component that is not identified.\n",
        "* [Trace distance](https://en.m.wikipedia.org/wiki/Trace_distance) is a **metric** on the space of density matrices and gives a measure of the distinguishability between two states. In quantum mechanics, the trace distance is used to measure the distinguishability between two quantum states. It is the quantum generalization of the Kolmogorov distance for classical probability distributions.\n",
        "  * The trace distance is defined as half of the [trace norm](https://en.m.wikipedia.org/wiki/Matrix_norm#Schatten_norms) of the difference of the matrices ${\\displaystyle T(\\rho ,\\sigma ):={\\frac {1}{2}}\\|\\rho -\\sigma \\|_{1}={\\frac {1}{2}}\\mathrm {Tr} \\left[{\\sqrt {(\\rho -\\sigma )^{\\dagger }(\\rho -\\sigma )}}\\right],}$\n",
        "  * Trace distance is a measure of the distinguishability between two quantum states. It is defined as half of the L1 norm of the difference between the density matrices of the two states:\n",
        "  * $δ(ρ, σ) = 1/2 ||ρ - σ||_1$\n",
        "  * where ||ρ - σ||_1 is the trace norm of ρ - σ.\n",
        "  * Trace distance is a metric on the space of density matrices, meaning that it satisfies the following properties:\n",
        "    * δ(ρ, σ) ≥ 0 for all ρ, σ\n",
        "    * δ(ρ, σ) = 0 if and only if ρ = σ\n",
        "    * δ(ρ, σ) = δ(σ, ρ)\n",
        "    * δ(ρ, τ) ≤ δ(ρ, σ) + δ(σ, τ) for all ρ, σ, τ\n",
        "  * The trace distance is a more general measure of distinguishability than the trace inequality. The trace inequality only states that the trace distance between two quantum states is at least as large as the sum of the trace distances between each state and a third state. The trace distance, on the other hand, can be used to measure the distinguishability between any two quantum states, regardless of whether there is a third state involved.\n",
        "  * Here is an example of how the trace distance can be used to measure the distinguishability between two quantum states: Suppose we have two quantum states ρ and σ that are represented by the following density matrices:\n",
        "    * $ρ = |0 > < 0| + | 1 > < 1|$\n",
        "    * $σ = | 0 > < 0| + 0.5 | 1 > <1|$\n",
        "  * The trace distance between ρ and σ can be calculated as follows:\n",
        "    * δ(ρ, σ) = 1/2 ||ρ - σ||_1\n",
        "    * = 1/2 ||(|0><0| + |1><1|) - (|0><0| + 0.5 |1><1|)||_1\n",
        "    * = 1/2 ||0.5 |1><1|)||_1\n",
        "    * = 1/2 * 0.5\n",
        "    * = 1/4\n",
        "  * This means that the two states ρ and σ are indistinguishable with a probability of 1/4.\n",
        "  * The trace distance is a powerful tool for analyzing quantum information processing protocols and for studying the effects of noise and decoherence on quantum systems. It is also used in quantum machine learning and quantum algorithms.\n",
        "  * the trace distance is a metric because:\n",
        "    * Non-negativity: The distance between two points is always non-negative.\n",
        "    * Identity: The distance between two identical points is zero.\n",
        "    * Symmetry: The distance between two points is the same as the distance between the points in the opposite order.\n",
        "    * Triangle inequality: The distance between two points is less than or equal to the sum of the distances between each point and a third point.\n",
        "* [Fubini–Study metric](https://en.m.wikipedia.org/wiki/Fubini–Study_metric): When extended to complex projective Hilbert space, the Fisher information metric becomes the Fubini–Study metric\n",
        "* [Bures metric](https://en.m.wikipedia.org/wiki/Bures_metric): when written in terms of mixed states, the Fisher information mettic is the quantum Bures metric. The Bures metric can be seen as the quantum equivalent of the Fisher information metric\n",
        "* [Französische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Französische_Eisenbahnmetrik).\n",
        "* [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) Code space metric: gibt Unterschiedlichkeit von (gleich langen) Zeichenketten an (number of items that are different between two subsets).\n",
        "* [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) Extension of Hamming-Abstands. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro–Winkler distance](https://en.m.wikipedia.org/wiki/Jaro–Winkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "* More [nicht aus Normen erzeugten Metriken](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)"
      ],
      "metadata": {
        "id": "3HV9y-rvrhKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Measure</font> (Haar, Dirac, Radon, Hausdorff, Lebesgue)*"
      ],
      "metadata": {
        "id": "eLjOkdyQxLeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measures**\n",
        "\n",
        "A measure provides a description for how things are distributed in a mathematical set or space. From Video: [The Haar Measure | PennyLane Tutorial](https://www.youtube.com/watch?v=d4tdGeqcEZs&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=3)\n",
        "\n",
        "[Measure](https://de.m.wikipedia.org/wiki/Ma%C3%9F_(Mathematik)): Funktion, die Teilmengen einer Grundmenge Zahlen zuordnet, die als „Maß“ für die Größe dieser Mengen interpretiert werden können. In Stochastik werden Wahrscheinlichkeitsmaße verwendet, um zufälligen Ereignissen, die als Teilmengen eines Ergebnisraums aufgefasst werden, Wahrscheinlichkeiten zuzuordnen.\n",
        "\n",
        "  * See also [Geometric measure theory](https://en.m.wikipedia.org/wiki/Geometric_measure_theory) and [outer measure](https://en.m.wikipedia.org/wiki/Outer_measure).\n",
        "\n",
        "* [Lebesgue measure](https://en.m.wikipedia.org/wiki/Lebesgue_measure): is the standard way of assigning a measure to subsets of n-dimensional Euclidean space. For n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume. In general, it is also called n-dimensional volume, n-volume, or simply volume.\n",
        "\n",
        "  * [Hausdorff measure](https://en.m.wikipedia.org/wiki/Hausdorff_measure) is a generalization of the traditional notions of area and volume to non-integer dimensions, specifically fractals and their Hausdorff dimensions. It is a type of outer measure that assigns number in [0,∞] to each set in $\\mathbb {R} ^{n}$ or, more generally, in any metric space.\n",
        "\n",
        "  * [Jordan measure](https://en.m.wikipedia.org/wiki/Jordan_measure) is an extension of the notion of size (length, area, volume) to shapes more complicated than, for example, a triangle, disk, or parallelepiped.\n",
        "\n",
        "* [Radon measure](https://en.m.wikipedia.org/wiki/Radon_measure)\n",
        "\n",
        "* [Wiener measure](https://en.wikipedia.org/wiki/Wiener_process) zur Beschreibung des Wiener-Prozesses (Brownsche Bewegung). It is the probability law on the space of continuous functions g, with g(0) = 0, induced by the Wiener process.\n",
        "\n",
        "* [Random measure](https://en.m.wikipedia.org/wiki/Random_measure) is a measure-valued random element, for example used in the theory of random processes, where they form many important point processes such as Poisson point processes and Cox processes.\n",
        "\n",
        "* [Vector measure](https://en.m.wikipedia.org/wiki/Vector_measure) eine Verallgemeinerung des Maßbegriffes dar: Das Maß ist nicht mehr reellwertig, sondern vektorwertig. Vektormaße werden unter anderem in der Funktionalanalysis benutzt (Spektralmaß).\n",
        "\n",
        "  * [Projection-valued measure (Spektralmaß)](https://en.m.wikipedia.org/wiki/Projection-valued_measure) ist eine Abbildung, die gewissen Teilmengen einer fest gewählten Menge orthogonale Projektionen eines Hilbertraums zuordnet. Spektralmaße werden verwendet, um Ergebnisse in der Spektraltheorie linearer Operatoren zu formulieren, wie z. B. den Spektralsatz für normale Operatoren.\n",
        "\n",
        "  * [Complex measure](https://en.m.wikipedia.org/wiki/Complex_measure)\n",
        "\n",
        "  * [Signed measure](https://en.m.wikipedia.org/wiki/Signed_measure)\n",
        "\n",
        "* [Moment measure](https://en.m.wikipedia.org/wiki/Moment_measure)\n",
        "\n",
        "* [Dirac measure](https://en.m.wikipedia.org/wiki/Dirac_measure) assigns a size to a set based solely on whether it contains a fixed element x or not. It is one way of formalizing the idea of the Dirac delta function.\n",
        "\n",
        "* [Discrete measure](https://en.m.wikipedia.org/wiki/Discrete_measure) is similar to the Dirac measure, except that it is concentrated at countably many points instead of a single point. More formally, a measure on the real line is called a discrete measure (in respect to the Lebesgue measure) if its support is at most a countable set.\n",
        "\n",
        "* [Poisson random measure](https://en.m.wikipedia.org/wiki/Poisson_random_measure) and [Poisson-type random measure](https://en.m.wikipedia.org/wiki/Poisson-type_random_measure)\n",
        "\n",
        "* [Haar measure](https://en.m.wikipedia.org/wiki/Haar_measure) assigns invariant volume (left- and right-invariant = (bi-)invariant measure). Generalization of Lebesgue measure. Sample unitary matrix uniformly at random according to Haar measure, and then apply it to another unitary matrix, the resulting unitary matrix will also be sampled uniformly at random according to Haar measure.\n",
        "  \n",
        "  * **Define expectation value of an observable = average value of the observable** over all possible quantum states. e.g. in QML: define <u>**average complexity**</u> by sampling a random unitary matrix (who represent quantum operations, like gates and channels) from Haar measure (= unitary matrix that is chosen with equal probability from all possible unitary matrices), **and then measuring sample complexity of algorithm on the resulting quantum state**. Average complexity is expected value of sample complexity over all possible unitary matrices. (*Train QML on quantum data: sampling large number of random unitary operations and applying them to data. Haar measure ensures that all possible unitary operations have an equal chance of being sampled.*)\n",
        "\n",
        "  * VC dimension, Rademacher complexity and Kullback Leibler divergence can all be used to measure the <u>**worst-case complexity**</u> (sample complexity of algorithm on worst possible quantum state).\n",
        "\n",
        "  * Sample unitary matrices according to Haar measure: Metropolis-Hastings algorithm iteratively proposes new unitary matrix and accepts or rejects proposal based on probability distribution (chosen that it converges to sampling from Haar measure)\n",
        "\n",
        "* Gibbs measure\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wUzfIQ8Ddoxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Set Complexity</font> (Covering number, Maximal packing nets)*"
      ],
      "metadata": {
        "id": "V36uKfvAxRCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Complexity**\n",
        "\n",
        "* [Covering problems](https://en.m.wikipedia.org/wiki/Covering_problems)\n",
        "\n",
        "* [Covering number](https://en.m.wikipedia.org/wiki/Covering_number): smallest number of sets that can be used to cover a given set with a certain tolerance.\n",
        "  * Covering number of a set with respect to a metric quantifies how many balls of a specified radius are needed to cover the set. In learning theory, covering numbers can be used to analyze the complexity of function classes in certain metric spaces. [Covering number](https://en.m.wikipedia.org/wiki/Covering_number): helps in understanding the capacity of a hypothesis class, which can be crucial in understanding generalization errors and the learnability of a class. Specifically, it can provide bounds on the number of samples required to learn a target function to a desired accuracy. Covering numbers are from [Combinatorial (discrete) geometry](https://en.m.wikipedia.org/wiki/Discrete_geometry), like [Kissing number](https://en.m.wikipedia.org/wiki/Kissing_number) (Newton number or Contact number) and [Polygon number](https://en.m.wikipedia.org/wiki/Polygon_covering).\n",
        "  * Measure of the complexity of a set: Bound sample complexity of learning algorithms (meanwhile Lebesgue covering dimension is used to bound generalization error of learning algorithms. eg Covering number tells how many sets we need to cover unit circle, while Lebesgue covering dimension tells how many dimensions we need to represent unit circle). Covering number is more directly related to sample complexity and generalization error of learning algorithms.\n",
        "  * How can one calculate the covering number bounds for the space of pure output states of polynomial-size quantum circuits? **Lower bound** on covering number is number of pure states that can be represented by a polynomial-size quantum circuit: given by $2^{n_q}$, where $n_q$ is number of qubits in circuit. Lower bound is number of states that a polynomial-size quantum circuit can represent, and upper bound is number of states that any quantum circuit can represent.\n",
        "\n",
        "  * Covering numbers are a measure of the complexity of a set (in sample complexity). A set with a small covering number is said to be easy to cover, while a set with a large covering number is said to be difficult to cover\n",
        "\n",
        "  * Example: *we provide bounds on the expressivity of the class of CPTP maps (or unitaries) that a quantum machine learning model (QMLM) can implement in terms of the number of trainable elements used in the architecture. As a measure of expressivity, we choose covering numbers and metric entropies w.r.t. (the metric induced by) the diamond norm.* (from: Generalization in quantum machine learning from few training data)\n",
        "\n",
        "  * [The Most Important Concept in Topology and Analysis | Compactness](https://youtu.be/td7Nz9ATyWY)\n",
        "\n",
        "* [Kissing number](https://en.m.wikipedia.org/wiki/Kissing_number)\n",
        "\n",
        "* [Polygon covering](https://en.m.wikipedia.org/wiki/Polygon_covering)\n",
        "\n",
        "* [Spherical code](https://en.m.wikipedia.org/wiki/Spherical_code)\n",
        "\n",
        "* [Equilateral dimension](https://en.m.wikipedia.org/wiki/Equilateral_dimension)\n",
        "\n",
        "* [Packing problems](https://en.m.wikipedia.org/wiki/Packing_problems)\n",
        "\n",
        "  * **Maximal packing nets** - used in \"Information-theoretic bounds on quantum advantage in machine learning\". See (https://en.m.wikipedia.org/wiki/Close-packing_of_equal_spheres) and (https://en.m.wikipedia.org/wiki/Packing_density)\n",
        "  * \"measures the cardinality of a maximal packing net that depends on the input distribution\""
      ],
      "metadata": {
        "id": "w3xsatVa9aZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Divergence</font> (Kullback Leibler, Jensen-Shannon, Hellinger, Bregman, f, Bhattacharyya)*"
      ],
      "metadata": {
        "id": "hiezBxLBxVLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Divergence](https://en.m.wikipedia.org/wiki/Divergence_(statistics)) is a (contrast) function which measure the difference between two probability distributions (statistical manifolds). Is a weaker notion than that of the distance (not symmetric, no triangle inequality).**\n",
        "\n",
        "* [Kullback Leibler](https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence) (relative entropy): measure of how one probability distribution (not only Gaussian) differs from a baseline distribution or amount of information lost when one distribution is converted to another. Equivalent to multi-class cross-entropy in multi-class classification, but used to approximate more complex function, e.g. autoencoder for learning a dense feature representation. Minimizing KL divergence (+ squared Euclidean distance) is main way to solve linear inverse problem, via principle of max entropy & least squares (logistic + linear regression). Used in clustering, feature selection (find features that minimize KL divergence between model's distribution and  true distribution when features are excluded), anomaly detection, GANs (similarity between real and generated data). Fisher information is expected value of second derivative of log likelihood function. KL divergence is quadratic form whose coefficients are given by elements of Fisher information matrix. FIM defines local curvature of KL divergence. Fisher information can be used to estimate the KL divergence between two distributions.\n",
        "  \n",
        "\n",
        "* [Jensen-Shannon divergence](https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence): smoothed version of KLd = symmetrization of KL divergence and with finite values. Symmetric means that does not matter which distribution is source and which is target. More robust to noise than other measures of similarity like KLd. JSD is also non-parametric: does not make any assumptions about underlying distribution of data. Square root of Jensen–Shannon divergence is Jensen-Shannon distance. Used in dimensionality reduction, GAN, clustering, anomaly detection, novelty detection.\n",
        "\n",
        "* [Hellinger distance](https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to Bhattacharyya) quantifies similarity between two probability distributions. Type of f-divergence. Hellinger distance is good choice when goal is to measure difference between the cumulative distribution functions of two distributions.\n",
        "\n",
        "* [f-Divergence](https://en.m.wikipedia.org/wiki/F-divergence): Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific f-divergence between model and data distribution.\n",
        "\n",
        "* [Bregman divergences](https://en.m.wikipedia.org/wiki/Bregman_divergence): calculate bi-tempered logistic loss, performing better than softmax function with noisy datasets. The Mahalanobis distance is an example of Bregman, and Squared (Euclidean) distance is a special case for Bregman.\n",
        "  \n",
        "* [Squared Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance): special case of Bregman (corresponding to function x<sup>2</sup>) for certain choice of generating function when generating convex function is quadratic - represents measure of divergence between two points in space. The cost function for K-means clustering is sum of squared distances between data points and cluster centroids. Squared distances are more sensitive to large errors than absolute distances.\n",
        "\n",
        "* [Bhattacharyya distance](https://en.m.wikipedia.org/wiki/Bhattacharyya_distance): determine relative closeness of two samples. Measure separability of classes in classification and more reliable than Mahalanobis when standard deviations of classes are same. When two classes have similar means but different standard deviations, Mahalanobis would tend to zero, whereas Bhattacharyya grows depending on difference between standard deviations.\n",
        "\n",
        "* Maximal Quantum Rényi Divergence: see under entropy"
      ],
      "metadata": {
        "id": "CdhvTHoqrUCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Distances</font> (Wasserstein, Mahalanobis, Manhattan)*"
      ],
      "metadata": {
        "id": "IJCykPfwxdQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Statistical_distance\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Total_variation_distance_of_probability_measures\n",
        "\n",
        "from: https://en.m.wikipedia.org/wiki/Trace_distance"
      ],
      "metadata": {
        "id": "KAgLEAgDJYCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Distances](https://en.m.wikipedia.org/wiki/Distance), especially [Statistical distances](https://en.m.wikipedia.org/wiki/Statistical_distance): quantify separation between two elements. Can be defined on a wider variety of spaces than metrics and are often more interpretable than metrics. Distances can be more flexible than metrics: can be defined on a wider variety of spaces when data is not Euclidean (no natural notion of distance like in text, images, or graphs), data is noisy (more robust), or data is imbalanced. Is a weaker notion than that of the metric (there is no triangle inequality).**\n",
        "\n",
        "* [Manhattan distance (taxicab)](https://en.m.wikipedia.org/wiki/Taxicab_geometry): defined as sum of absolute values of the differences between the corresponding components of two vectors. The Manhattan distance is not symmetric, which means that the distance between two points is not the same as the distance between the reverse of the two points.\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance): maximum absolute difference between corresponding components of two vectors. The Chebyshev distance is not symmetric, and it does not satisfy the triangle inequality.\n",
        "* [Jaccard distance](https://en.m.wikipedia.org/wiki/Jaccard_index): size of intersection of two sets divided by size of the union of the two sets. The Jaccard distance is not a metric because it does not satisfy the triangle inequality. The Jaccard distance is used for text classification tasks. It is defined as the size of the intersection of two sets divided by the size of the union of the two sets.\n",
        "* [Left- / right-/ bi-invariant Riemann metric](https://ncatlab.org/nlab/show/invariant+metric). Used in [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004). Example in discrete space: [Kendall tau distance](https://en.m.wikipedia.org/wiki/Kendall_tau_distance): is a measure of the similarity between two rankings of a set of objects. It is defined as the number of pairs of objects that are ranked in opposite order in the two rankings, divided by the total number of pairs of objects. The Kendall tau distance is not a metric because it does not satisfy the triangle inequality.\n",
        "* Lp-norm defined as the pth root of sum of the pth powers of the differences between the corresponding components of two vectors. The Lp norm is not a metric for p < 1.\n",
        "* [Wasserstein distance](https://en.m.wikipedia.org/wiki/Wasserstein_metric) (Earth Mover’s Distance): comparing probability distributions. The Wasserstein distance is not a metric for comparing vectors. Depending on the order parameter \\( p \\), it might not satisfy the triangle inequality property. When \\( p = 1 \\), it is a metric, but for other values of \\( p \\), it might not be. Is used for image segmentation tasks. It is defined as the minimum amount of work required to move one set of pixels to another set of pixels.  The Wasserstein distance is used where the data is represented as probability distributions. It is defined as the minimum cost of transporting one distribution to another.\n",
        "* Squared Euclidean distance is also a distance measure in a broader sense, as it quantifies the separation between two points in a Euclidean space. Squared Euclidean distance is also a divergence, specifically a special case of a Bregman divergence for a certain choice of generating function when the generating convex function is a quadratic function -  In this context, it represents a measure of divergence between two points in the space.\n",
        "* [Bhattacharyya distance](https://en.m.wikipedia.org/wiki/Bhattacharyya_distance)\n",
        "* [Mahalanobis distance](https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D, used in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set. In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance. The Mahalanobis distance does not satisfy the triangle inequality because it can be zero even if the two points are not the same. This can happen if the covariance matrix is singular. It is often used in multivariate anomaly detection, classification, and regression. [Mahalanobis distance](https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n"
      ],
      "metadata": {
        "id": "OTsj83x7GbyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Topological Invariants</font> (Homology Groups, Betti numbers, Characteristic classes)*"
      ],
      "metadata": {
        "id": "zXCXyX9DxfdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploiting topological properties of the loss landscape of symmetry-machine-learning (in the data and problem) as a preprocessing step to reduce Barren plateaus - How do topological properties of the loss landscape vary in a QML algorithm that learns symmetries of data and problems\n",
        "https://pennylane.ai/qml/demos/tutorial_geometric_qml/"
      ],
      "metadata": {
        "id": "w_Y-pFySypvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Invariants** (Distanz- und Ähnlichkeitsmaß um topologische Räumen / Objekte zu differenzieren)\n",
        "* **What is it used for?**: Similarity metrics for topology: A topological invariant is a property or characteristic of a space that remains unchanged under homeomorphisms, which are continuous transformations of the space that can be continuously undone. Topological invariants are used in:\n",
        "  * **Mathematics:** study properties of topological spaces and manifolds (Euler characteristic of surface to distinguish between different surfaces, such as spheres and tori)\n",
        "  * **Physics:** study the properties of quantum systems (Chern class of band insulator to determine whether the insulator is topological or not. also classify different types of insulators and superconductors)\n",
        "  * **Chemistry:** study properties of molecules and materials (winding number of a molecule to determine whether molecule is chiral or not. In materials science, to design new materials with desired properties, such as high strength and conductivity, or new drugs and catalysts)\n",
        "  * **Computer science:** study properties of algorithms and data structures ( genus of a graph to determine how difficult it is to color the graph)\n",
        "  * **Machine learning:** develop new machine learning algorithms more robust to noise and perturbations(TDA to extract features from data that are invariant to certain transformations)\n",
        "\n",
        "* **Topologische Räume:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erklärt, dass wir eine Menge  Ω  C  R  offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen lässt. * [Separation Axioms](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms): Topologische Räume können klassifiziert werden nach Kolmogorov:\n",
        "  * Frechet Räume: sind Vektorräume von glatten Funktionen (unendlich oft differenzierbar, stetig). Diese Räume lassen sich zwar mit verschiedenen Normen ausstatten, sind aber bezüglich keiner Norm vollständig, also keine Banachräume. Man kann auf ihnen aber eine Topologie definieren, sodass viele Sätze, die in Banachräumen gelten, ihre Gültigkeit behalten.\n",
        "  * Uniforme Räume: erlauben es zwar nicht Abstände einzuführen, aber Begriffe wie gleichmäßige Stetigkeit, Cauchy-Folgen, Vollständigkeit und Vervollständigung zu definieren\n",
        "\n",
        "* **Basic topological invariants** (more \"Geometric\" or foundational / fundamental than the more algebraic invariants mentioned after)\n",
        "  * Connectedness: A space is connected if it cannot be divided into two disjoint non-empty open sets. This property is preserved under continuous maps, making it a topological invariant. There are several related notions as well, such as path-connectedness (there is a path connecting any two points in the space).\n",
        "  * Compactness: A space is compact if every open cover has a finite subcover. Like connectedness, this property is preserved under continuous functions, marking it as a topological invariant.\n",
        "\n",
        "* **Homotopy Invariants**\n",
        "These invariants are primarily concerned with the study of continuous deformations of topological spaces. Allows for classification of spaces based on their loop structures and offering a way to distinguish between different topological spaces\n",
        "  * **Fundamental Group (π₁)**, including the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) (also known as the first homotopy group). It gives information about the loops in the space, essentially characterizing how one can loop around one point and come back to the same point.\n",
        "  * **Higher Homotopy Groups (π₂, π₃, ...)**: These groups generalize the concept of the fundamental group to higher dimensions, providing information about surfaces and higher-dimensional \"holes\" in a space.\n",
        "\n",
        "* **Homology and Cohomology**\n",
        "These invariants are associated with algebraic structures that help quantify various aspects of topological spaces.\n",
        "  * **Homology Groups** (including singular homology, simplicial homology, etc.)\n",
        "  * **Cohomology Groups** (including singular cohomology, sheaf cohomology, etc.)\n",
        "  * **Euler Characteristic** (can also be derived from homology)\n",
        "  * **Betti Numbers** (related to homology groups)\n",
        "\n",
        "* **[Characteristic class](https://en.m.wikipedia.org/wiki/Characteristic_class)**\n",
        "These are cohomology classes which help to study and classify fibred spaces and vector bundles (Higher dimensional invariants) are defined on vector bundles)\n",
        "  * [Chern classes](https://en.m.wikipedia.org/wiki/Chern_class) ( characteristic classes of complex vector bundles. To study complex geometry of manifolds.)\n",
        "  * [Chern Numbers](https://en.m.wikipedia.org/wiki/Chern_class#Chern_numbers) (they are associated with Chern classes, and give global information about the structure of a bundle)\n",
        "  * [Stiefel-Whitney classes](https://en.m.wikipedia.org/wiki/Stiefel%E2%80%93Whitney_class) (for real vector bundles, characteristic classes of principal bundles. Study the topology of manifolds)\n",
        "  * [Pontriagyn classes](https://en.m.wikipedia.org/wiki/Pontryagin_class) (characteristic classes of real vector bundles, particularly oriented ones. Study the real geometry of manifolds)\n",
        "  * [Segre classes](https://en.m.wikipedia.org/wiki/Segre_class): study of cones, a generalization of vector bundles (total Segre class is inverse to total Chern class, advantage of Segre class: it generalizes to more general cones, while Chern class does not)\n",
        "  * [Euler characteristic](https://en.m.wikipedia.org/wiki/Euler_characteristic) (a type of characteristic class associated with real vector bundles). It can be used to distinguish between orientable and non-orientable manifolds. It can distinguish between spheres and tori.\n",
        "  * Background: Characteristic classes are vector bundle invariants, while Betti numbers are defined on topological spaces.\n",
        "    * This means that **Betti numbers can only distinguish between topological spaces that are not homeomorphic, while characteristic classes can also distinguish between topological spaces that are homeomorphic but have different vector bundles**.\n",
        "    * For example, the sphere and the torus are homeomorphic, but they have different Euler classes. This means that the Betti numbers of the two spaces are the same, but they can be distinguished using characteristic classes.\n",
        "    * Characteristic classes can also be used to study the relationships between different manifolds and fiber bundles. For example, the Gysin homomorphism is a map between the cohomology groups of two manifolds that is defined using characteristic classes.\n",
        "    * Characteristic classes are global invariants, meaning that they do not change under local deformations of a manifold or fiber bundle. This makes them useful for classifying and differentiating topological spaces.\n",
        "\n",
        "* [Knot Invariants](https://en.m.wikipedia.org/wiki/Knot_invariant)\n",
        "These invariants are specific to the study of knots in three-dimensional space.\n",
        "  * [Jones Polynomial](https://de.m.wikipedia.org/wiki/Jones-Polynom)\n",
        "  * [Alexander Polynomial](https://de.m.wikipedia.org/wiki/Alexander-Polynom)\n",
        "  * [Winding Number](https://en.m.wikipedia.org/wiki/Winding_number)\n",
        "\n",
        "* **Differential Topology**\n",
        "These are invariants in the study of smooth manifolds and their properties.\n",
        "  * **De Rham Cohomology**\n",
        "  * **Differential Forms**\n",
        "\n",
        "* **Other Invariants Related to Manifold Geometry**\n",
        "  * **Genus** (related to the classification of surfaces)\n",
        "  * **Sectional Curvature**, **Ricci Curvature**, **Scalar Curvature** (these are invariants in Riemannian geometry, a field closely related to algebraic topology)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k_t6_qozAeTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Routines**"
      ],
      "metadata": {
        "id": "OhIkej28gmaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q"
      ],
      "metadata": {
        "id": "-9RCNq3pbJLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrsTqiiWae2Z"
      },
      "outputs": [],
      "source": [
        "import cirq\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Fourier Transform*"
      ],
      "metadata": {
        "id": "AXGHc7oUfx_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Space and Position Space with Quantum Fourier Transform\n",
        "\n",
        "https://youtu.be/W8QZ-yxebFA"
      ],
      "metadata": {
        "id": "9wrzQ3norW5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Fourier Transform is the change from one basis (computational) to another (Fourier basis)**\n",
        "\n",
        "* Quantum Fourier Transform is the inverse Discrete Fourier Transform)\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_047.png)\n",
        "\n",
        "**General formula**\n",
        "\n",
        "* Remember: <font color=\"blue\">$e^{2\\pi i}$ = 1</font> (identity operation), and see why $e^{\\pi i}$ = -1 in [this video](https://youtu.be/-AyE1Wpgo3Q)\n",
        "\n",
        "\n",
        "* In QFT we change the <font color=\"blue\">$\\theta$ = phase in $e^{2\\pi i \\theta}$</font> = Eigenvalue of Oracle function $U$ associated with an eigenvector |u⟩\n",
        "\n",
        "* The phase $\\theta$ is expressed as: <font color=\"blue\">$\\theta$ = $\\frac{x_n}{2^{k_n}}$</font> with:\n",
        "\n",
        "  * <font color=\"blue\">$x_n$ = 0 or 1</font> state\n",
        "\n",
        "  * <font color=\"blue\">$k_n$</font> number of Qubits\n",
        "\n",
        "* This is expressed in a so-called \"controlled-R quantum gate\" that **applies a relative phase change to |1>**\n",
        "\n",
        "* The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i \\frac{x_n}{ 2^{k_n}}}\\end{array}\\right)$</font>"
      ],
      "metadata": {
        "id": "I_dkpT8Cdvyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits: Introduction*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0 or 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1, $k_2$ = 2, $k_3$ = 3</font>\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^1}+\\frac{x_{2}}{2^2}+\\frac{x_{3}}{2^3}\\right)}|1\\rangle\\right)$  = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_1}$ is activated, then it is a 180° Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 90° S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 45° T-rotation of $\\frac{\\pi}{4}$ radians = between 1 and i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_2}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^{k_1}}+\\frac{x_3}{2^{k_2}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2^1}+\\frac{x_3}{2^2}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_2}{2}+\\frac{x_3}{4}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* If only $\\tilde{x_2}$ is activated, then it is a 180° Z-rotation of $\\pi$ radians = -1\n",
        "\n",
        "* If only $\\tilde{x_3}$ is activated, then it is a 90° S-rotation of $\\frac{\\pi}{2}$ radians = i\n",
        "\n",
        "* If both $\\tilde{x_2}$ and $\\tilde{x_3}$ are activated, then it is a 180° + 90° = 170° rotation of $\\pi + \\frac{\\pi}{2}$ radians = -i\n",
        "\n",
        "> <font color=\"blue\">$\\tilde{x_3}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_3}{2^1}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{\\pi \\mathrm{i}x_3}|1\\rangle\\right)$\n",
        "\n",
        "* If $\\tilde{x_3}$ is activated, then it is a 180° Z-rotation of $\\pi$ radians = -1\n"
      ],
      "metadata": {
        "id": "xOBGPt6Pdvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90° rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)\n",
        "\n",
        "*Here including the 8x8 matrix form for the complete operator:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0801.png)"
      ],
      "metadata": {
        "id": "J8n3fjsedvyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 1 Qubit*\n",
        "\n",
        "**Computational Basis States:** <font color=\"blue\">$\\tilde{x_1}$ = 0 or 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1</font>\n",
        "\n",
        "\n",
        "*Linear transformation of a qubit in the computational basis 0 and 1 each separately to the Fourier basis:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0813.png)\n",
        "\n",
        "**Computational Basis in $|0\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 0 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$  $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i} 0}$  = $\\mathrm{e}^{0}$ = 1 (no rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0810.png)\n",
        "\n",
        "**Computational Basis in $|1\\rangle$**\n",
        "\n",
        "> <font color=\"blue\">For $x_1$ = 1 $\\Rightarrow$</font> <font color=\"blue\">$\\tilde{x_1}$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^{k_1}}\\right)}|1\\rangle\\right)$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{x_1}{2^1}\\right)}|1\\rangle\\right)$ $\\Rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}\\right)}$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180° Z-rotation)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0811.png)"
      ],
      "metadata": {
        "id": "hBqueHp-dvyu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4Vdy6sdvyv"
      },
      "source": [
        "*Quantum Fourier Transform with 1 Qubit is a Hadamard transform!*\n",
        "\n",
        "**One qubit QFT matrix**: $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & \\mathrm{e}^{\\pi i}\\end{array}\\right)$, where $\\mathrm{e}^{\\pi \\mathrm{i}}$ = -1. So it is: <font color=\"blue\"> QFT für x=1 = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**Compare with Hadamard transform matrix:**\n",
        "\n",
        "In quantum computing, the Hadamard gate is a one-qubit rotation, mapping the qubitbasis states $|0\\rangle$ and $|1\\rangle$ to two **superposition** states with **equal weight of the computational basis** states $|0\\rangle$ and $|1\\rangle$. Usually the phases are chosen so that\n",
        "\n",
        ">$\n",
        "H=\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}\\langle 0|+\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}\\langle 1|\n",
        "$\n",
        "\n",
        "in Dirac notation. This corresponds to the transformation matrix\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "H_{1}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "1 & -1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "in the $|0\\rangle,|1\\rangle$ basis, also known as the computational basis. The states $\\frac{|0\\rangle+|1\\rangle}{\\sqrt{2}}$ and $\\frac{|0\\rangle-|1\\rangle}{\\sqrt{2}}$ are known as $|+\\rangle$ and $|-\\rangle$ respectively, and together constitute the polar basis in quantum computing.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_073.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJhiyX4dvyw"
      },
      "source": [
        "**Why Hadamard transform is exactly a 1 qubit Quantum Fourier Transform:** (see result of + for 0 state and - for 1 state) - Matrix-Vector-Multiplication (Single Qubit)\n",
        "\n",
        "> <font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "> <font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_045.png)\n",
        "\n",
        "2 im denominator verschwindet hier. 2^n für n=1 qubit. mit 2 oben und unten verschwinden beide."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|001\\rangle$*\n",
        "\n",
        "**Computational Basis in $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0804.png)\n",
        "\n",
        "**Fourier Basis for $|001\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0805.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 0</font>, <font color=\"blue\">$\\tilde{x_2}$ = 0</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{0}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$  = <font color=\"blue\">$\\frac{\\pi i}{4}$</font> (45° T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{0}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = <font color=\"blue\">$\\frac{\\pi i}{2}$</font> (90° S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180° Z-rotation)"
      ],
      "metadata": {
        "id": "XH-LUgBzdvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90° rotation is only activated if the second qubit $x_2$ is in state 1. Here it is not activated because $x_2$ = 0:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "tAr5mMp3dvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Fourier Transform with 3 Qubits for $|111\\rangle$*\n",
        "\n",
        "**Computational Basis in $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0802.png)\n",
        "\n",
        "**Fourier Basis for $|111\\rangle$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0803.png)\n",
        "\n",
        "**Computational States:** <font color=\"blue\">$\\tilde{x_1}$ = 1</font>, <font color=\"blue\">$\\tilde{x_2}$ = 1</font>, <font color=\"blue\">$\\tilde{x_3}$ = 1</font>. Number of Qubits: <font color=\"blue\">$k_1$ = 1 qubit, $k_2$ = 2 qubits, $k_3$ = 3 qubits</font>\n",
        "\n",
        "> <font color=\"blue\">Qubit 1 = $\\tilde{x_1}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2^{k_1}}+\\frac{x_{2}}{2^{k_2}}+\\frac{x_{3}}{2^{k_3}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}|1\\rangle\\right)$ = $\\mathrm{e}^{2 \\pi i 0.875} = \\mathrm{e}^{\\pi i 1.75}$ (180° Z-rotation + 90° S-rotation + 45° T-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 2 = $\\tilde{x_2}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{2}}{2^{k_1}}+\\frac{x_{3}}{2^{k_2}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{1}{2}+\\frac{1}{4}\\right)}|1\\rangle\\right)$ = $e^{\\pi i 1.5} =$ <font color=\"blue\">$-i$</font> (180° Z-rotation + 90° S-rotation)\n",
        "\n",
        "> <font color=\"blue\">Qubit 3 = $\\tilde{x_3}$</font> = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{3}}{2^{k_1}}\\right)}|1\\rangle\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i} \\frac{1}{2}}|1\\rangle\\right)$ = $e^{\\pi i 1} =$ <font color=\"blue\">$-1$</font> (180° Z-rotation)"
      ],
      "metadata": {
        "id": "qXRBmM0advy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Construction**\n",
        "\n",
        "*Compare the equations above with the circuit activations below (how a circuits computes the results). For example for the first qubit the operator / gate $S$ = 90° rotation is only activated if the second qubit $x_2$ is in state 1. Here it is activated because $x_2$ = 1:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "HYY1eloedvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Cirq Code for Quantum Fourier Transform*\n",
        "\n",
        "*Compare the code above with the circuit activations below (how a circuits computes the results):*\n",
        "\n",
        "* $H$ gate = bring qubit in superposition.\n",
        "\n",
        "  * *For $x=0$, no further rotation*\n",
        "\n",
        "  * *For $x=1$, then appy additional *$Z$ gate = 180° rotation = $\\pi$**\n",
        "\n",
        "* *$S$ gate = 90° rotation = $\\frac{\\pi}{2}$*\n",
        "\n",
        "* *$T$ gate = 45° rotation = $\\frac{\\pi}{4}$*\n",
        "\n",
        "$C R_{j}=C Z^{1 / 2^{j-1}}$\n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse)\n",
        "\n",
        "* $S$ entspricht $\\frac{\\pi}{2}$, also wenn qubit 1 = 1, dann bei qubit 0 das $S$ transform anwenden (0,5)\n",
        "\n",
        "  * S: The square root of Z gate, equivalent to cirq.Z ** 0.5\n",
        "\n",
        "  * See: [Cirq Gates](https://quantumai.google/cirq/gates)\n",
        "\n",
        "* $T$ entspricht $\\frac{\\pi}{4}$"
      ],
      "metadata": {
        "id": "lWJ2Oz2Ydvy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q\n",
        "import cirq\n",
        "\n",
        "def make_qft(qubits):\n",
        "\n",
        "    # Generate list of qubits\n",
        "    qreg = list(qubits)\n",
        "\n",
        "    # Make sure list is longer than 0 qubits:\n",
        "    while len(qreg) > 0:\n",
        "\n",
        "    # Remove first qubit from list and return its value (set as head-qubit):\n",
        "        q_head = qreg.pop(0)\n",
        "\n",
        "    # Apply Hadamard superposition to this head-qubit\n",
        "        yield cirq.H(q_head)\n",
        "\n",
        "    # Enumerate through list with i (index position) and corresponding qubit value (0 or 1)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "\n",
        "    # Apply Controlled-Z * Theta-Phase-Shift on target ('q-head') if control-qubit ('qubit') is in state 1\n",
        "            yield (cirq.CZ ** (1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "    # Do the inverse QFT as subroutine in quantum phase estimation\n",
        "    #        yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Use inverse QFT as subroutine in quantum phase estimation\n",
        "# phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Iterating through until \"while len(qreg) = 0\", then processes stops\n",
        "\n",
        "\"\"\"Visually check the QFT circuit.\"\"\"\n",
        "qubits = cirq.LineQubit.range(17)\n",
        "qft = cirq.Circuit(make_qft(qubits))\n",
        "print(qft)"
      ],
      "metadata": {
        "id": "myOgAQXddvy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0815.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0812.png)"
      ],
      "metadata": {
        "id": "mzHIk-hJdvy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTg0Uyudvy6"
      },
      "source": [
        "*Inverse Quantum Fourier Transform ('QFT Dagger' - Dagger is a complex conjugate operation!)*\n",
        "\n",
        "Reminder of QFT:\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$\n",
        "\n",
        "**Remember: Dagger is a complex conjugate operation!**\n",
        "\n",
        "QFT inverse (see -2 turning i in -i which is a complex conjugate operation):\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$\n",
        "\n",
        "\n",
        "The operator is then (\n",
        "We have already seen that the Hadamard gate is self-inverse, and the same is clearly true for the SWAP gate; the inverse of the rotations gate $R_k$ is given by):\n",
        "\n",
        "> The matrix form of inverse QFT operator is: <font color=\"blue\">${R^{\\dagger}}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{-2 \\pi i / 2^{k}}\\end{array}\\right)$</font> and compare with QFT operator:  <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "https://www.cl.cam.ac.uk/teaching/1920/QuantComp/Quantum_Computing_Lecture_9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Shor's Algorithm*"
      ],
      "metadata": {
        "id": "A90mbGhcftHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/shor.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "5sOhM-vlXeD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shor’s algorithm for factoring, which is not NP-complete, has been run on real quantum computers. The largest number it has been used to factor (that I could find a reference to) is 56153: New largest number factored on a quantum device is 56,153. And that was on 4 qubits (https://phys.org/news/2014-11-largest-factored-quantum-device.html).\n",
        "\n",
        "*(RSA) Encryption*\n",
        "\n",
        "Video [Breaking RSA](https://youtu.be/-ShwJqAalOk)\n",
        "\n",
        "Public key cryptography\n",
        "* https is developed on top of this\n",
        "* RSA: revest shavor edelmann, based on prime numbers, older more established algoithm\n",
        "* Elliptic curve cryptography\n",
        "* Modular arithmetic\n",
        "* Feistel Cypher (block cypher): https://de.m.wikipedia.org/wiki/Feistelchiffre\n",
        "* Diffie Hellmann key exchange\n",
        "* It's an exponential problem on classical computers that quantum computers can solve in polynomial time\n",
        "\n",
        "Extended Euclidean algorithm\n",
        "\n",
        "$\\begin{aligned} & (e, n)(d) \\\\ & n=p \\cdot q \\\\ & \\Phi(n)=(p-1) \\cdot(q-1) \\\\ & e \\cdot d \\equiv 1 \\bmod (\\Phi(n))\\end{aligned}$\n",
        "\n",
        "Fermat‘s factorization algorithm: if p and q are close to each other, this gives you a solution extremely quickly\n",
        "\n",
        "$\\begin{aligned} & N=a^2-b^2=(a+b) \\cdot(a-b) \\\\ & b^2=a^2-N \\\\ & a=[\\sqrt{N}]\\end{aligned}$\n",
        "\n",
        "[Prime Numbers and RS Encryption Algorithm](https://youtu.be/JD72Ry60eP4)\n",
        "\n"
      ],
      "metadata": {
        "id": "m6cJFp1SUIMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2019) How to factor 2048 bit RSA integers in 8 hours using 20 million noisy qubits\n",
        "\n",
        "https://arxiv.org/abs/1905.09749\n",
        "\n",
        "Yes, Craig Gidney has made significant contributions to the study of the discrete logarithm problem. In particular, he has developed quantum algorithms for solving the DLP that are significantly faster than classical algorithms.\n",
        "\n",
        "In 2010, Gidney published a paper in which he showed that the DLP could be solved using a quantum algorithm that is exponentially faster than the best classical algorithm. This algorithm is based on the use of modular forms, which are a type of mathematical function that has certain properties that make them well-suited for quantum computation.\n",
        "\n",
        "Gidney's work on the DLP has had a major impact on the field of cryptography. It has shown that the DLP is not as secure as previously thought, and it has led to the development of new cryptographic systems that are more resistant to quantum attacks.\n",
        "\n",
        "In addition to his work on the DLP, Gidney has also made significant contributions to other areas of quantum computing, such as quantum error correction and quantum simulation. He is a leading expert in the field of quantum computing, and his work is helping to pave the way for the development of practical quantum computers.\n",
        "\n",
        "Here are some of the specific contributions that Craig Gidney has made to the study of the discrete logarithm problem:\n",
        "\n",
        "* In 2010, he showed that the DLP could be solved using a quantum algorithm that is exponentially faster than the best classical algorithm.\n",
        "* In 2012, he developed a quantum algorithm for solving the DLP in the presence of noise.\n",
        "* In 2019, he co-authored a paper that showed how to use the DLP to factor RSA integers.\n",
        "\n",
        "Gidney's work on the DLP has had a major impact on the field of cryptography. It has shown that the DLP is not as secure as previously thought, and it has led to the development of new cryptographic systems that are more resistant to quantum attacks.\n",
        "\n",
        "I hope this helps! Let me know if you have other questions."
      ],
      "metadata": {
        "id": "HM92BvMZCJST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumai.google/cirq/experiments/shor"
      ],
      "metadata": {
        "id": "1S_wdVn7P6_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Shor-Algorithmus lässt sich am besten für die Primfaktorzerlegung erklären. Damit Quantencomputer diese Aufgabe meistern können, muss man das Problem allerdings etwas umformulieren. Denn der Quantenalgorithmus stützt sich auf eine Anleitung zum Faktorisieren von Zahlen, die aus den 1970er Jahren stammt. Damals fand man heraus, dass nur vier Schritte nötig sind, um die Primfaktoren p und q einer Zahl N = p·q zu berechnen.\n",
        "\n",
        "1. Wähle eine zufällige Zahl a < N.\n",
        "\n",
        "2. Finde die Periodenlänge r von a Modulo N.\n",
        "\n",
        "3. Stelle sicher, dass r eine gerade Zahl ist und dass (a^(r/2) + 1) nicht durch N teilbar ist.\n",
        "\n",
        "4. Dann ist p der größte gemeinsame Teiler von (a^(r/2) − 1) und N. Der andere Primteiler q ist entsprechend der größte gemeinsame Teiler von (a^(r/2) + 1) und N.\n",
        "\n",
        "https://www.spektrum.de/kolumne/shor-algorithmus-wie-quantencomputer-verschluesselungen-knacken/2133048\n"
      ],
      "metadata": {
        "id": "cbvNcG-vOM6M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6yeVZAuzrMk"
      },
      "source": [
        "**Classical Calculation**\n",
        "\n",
        "* <font color=\"blue\">Factoring is equivalent to finding a nontrivial squareroot of 1 mod N.\n",
        "\n",
        "* all we need to do is find this nontrivial squareroot of unity, and we can factor whatever number we need. As promised, we can do this with period finding, specifically by computing the order of a random integer\n",
        "\n",
        "* The order of some integer x modulo N is the smallest integer r such that $x^r$ = 1 mod N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCxHYDSsb6-y"
      },
      "source": [
        "*Modular Arithmetic*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_088.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_089.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axJsmDe1lF5Y"
      },
      "source": [
        "**<font color=\"blue\">Step 1: Pick coprime of N**\n",
        "\n",
        "*Drei mögliche Verfahren zur Berechnung des ggT :*\n",
        "\n",
        "Erstes Verfahren: Euklidischer Algorithmus\n",
        "* 15\t:\t13\t  = \t1\t  Rest  \t2.\t  Also ist ggT (15,13)= ggT (13,2)\n",
        "* 13\t:\t2\t  = \t6\t  Rest  \t1.\t  Also ist ggT (13,2)= ggT (2,1)\n",
        "* 2\t:\t1\t  = \t2\t  Rest  \t0.\t  Also ist ggT (2,1)= ggT (1,0)\n",
        "* Ergebnis: Der ggT von 15 und 13 ist 1.\n",
        "\n",
        "Zweites Verfahren: Vergleichen der Teilermengen .\n",
        "* Die Teilermenge von 15 lautet: {1,3,5,15}.\n",
        "* Die Teilermenge von 13 lautet: {1,13}.\n",
        "* Die größte in beiden Teilermengen vorkommende Zahl ist 1. Also ist 1 der ggT von 15 und 13.\n",
        "\n",
        "Dritte Möglichkeit: Vergleichen der Primfaktorzerlegung\n",
        "* Die Primfaktorzerlegung von 15 lautet: 15= 3·5.\n",
        "* Die Primfaktorzerlegung von 13 lautet: 13= 13.\n",
        "* Die gemeinsamen Primfaktoren sind: 1.\n",
        "* Also ist 1 der ggT.\n",
        "\n",
        "*Modulo (kurz: mod) berechnet den Rest einer Division zweier Zahlen. In Mathematischen Formeln wird modulo mit mod abgekürzt, beispielsweise: 23 mod 8 = 7. Bei dieser Rechnung kommt 7 heraus, weil die 8 zweimal in die 23 passt und dann 7 übrig bleiben.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48daYc8to4oz"
      },
      "source": [
        "# Product of two prime numbers (to check later if result is correct)\n",
        "N=5 * 3\n",
        "\n",
        "# Pick coprime (!) number to N to factorize N into primes\n",
        "a=13\n",
        "\n",
        "# Code Example to understand periodicity in the context of factoring prime numbers:\n",
        "\n",
        "import math\n",
        "# Compute greated common divisor between a and N\n",
        "math.gcd(a, N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqnmPhvoTQw"
      },
      "source": [
        "**<font color=\"blue\">Step 2: Find the period of $a^r$ $\\equiv$ 1 $(modN)$:**\n",
        "\n",
        "* <font color=\"blue\">the order of x is just the period of the function f(i) = $x^i$ mod N.\n",
        "\n",
        "* <font color=\"blue\">In quantum computing you use QFT in order to determine the period !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SoazOyVpdiU"
      },
      "source": [
        "import matplotlib. pyplot as plotter\n",
        "sns.set(rc={'figure.figsize':(12, 5), \"lines.linewidth\": 1.5})\n",
        "\n",
        "r = list(range(N))\n",
        "y= [a**r0 % N for r0 in r]\n",
        "\n",
        "plotter.plot (r, y)\n",
        "plotter.xlabel('r')\n",
        "plotter.ylabel('Rest:' f'{a}^r (mod{N})')\n",
        "plotter.title('Periode der Restwerte (aus den Multiples von r)')\n",
        "plotter.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qyWTdoZDtL"
      },
      "source": [
        "<font color=\"red\">**Beispiel: Choose any number $a$ and takes its multiple $r$ so many times, until the rest in modulo is 1, (except r=0)**</font>\n",
        "\n",
        "> $13^0$ (mod 15) = 1 (mod 15) = 1\n",
        "\n",
        "> $13^1$ (mod 15) = 13\n",
        "\n",
        "> $13^2$ (mod 15) = 169 (mod 15) = 4\n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 11 = 165, bis zur 169 verbleibt ein Rest 4*\n",
        "\n",
        "> $13^3$ (mod 15) = 2197 (mod 15) = 7\n",
        "\n",
        "* <font color=\"blue\">*Erlauterung: Nimm 15 * 146 = 2190, bis zur 2197 verbleibt ein Rest 7*\n",
        "\n",
        "> $13^4$ (mod 15) = 28561 (mod 15) = 1 (<font color=\"blue\"><u>hier started die Periode wieder, that's the r we are looking for!</u>)\n",
        "\n",
        "> usw.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dblciep4qxfM"
      },
      "source": [
        "r= r[y[1:].index(1)+1]\n",
        "print(f'r = {r}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HLzUiXj6AR"
      },
      "source": [
        "**<font color=\"blue\">Step 3: Bestimme $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$**. Mindestens einer der beiden Primfaktoren von N={p,q} is beinhalted in gcd(x+1, N) bzw. gcd(x-1, N)\n",
        "\n",
        "*In this case with a=13, N=15 and r=4:*\n",
        "\n",
        "* $x \\equiv a^{\\frac{r}{2}}(\\operatorname{mod} N)$\n",
        "\n",
        "* $x \\equiv 13^{\\frac{4}{2}}(\\operatorname{mod} 15)$\n",
        "\n",
        "* x = 169 (mod 15) = 4\n",
        "\n",
        "  * gcd(x-1, N) = 3 = p\n",
        "\n",
        "  * gcd(x+1, N) = 5 = q\n",
        "\n",
        "Achtung: in einem anderen Beispiel: N=11*7 (Primzahlen), a=18, ergibt x=43.\n",
        "\n",
        "* Davon x-1=42 und x+1=44.\n",
        "* Das sind naturlich keine Primzahlen,\n",
        "* Aber deren Faktoren sind: 44 = 2 * 2 * 11 und 42 = 2 * 3 * 7\n",
        "* das heisst, x-1 und x+1 kann auch die Primzahlen indirekt enthalten!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8MwnRiUrqhR"
      },
      "source": [
        "if r % 2 == 0:\n",
        "  x = (a**(r/2.)) % N\n",
        "  print(f'x = {x}')\n",
        "  if ((x + 1) % N) != 0:\n",
        "    print(math.gcd((int(x)+1), N), math.gcd((int(x)-1), N))\n",
        "  else:\n",
        "      print (\"x + 1 is 0 (mod N)\")\n",
        "else:\n",
        "  print (f'r = {r} is odd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rj2GI7_cC86"
      },
      "source": [
        "**Shor's Algorithm**\n",
        "\n",
        "* When finding order using the period finding algorithm, it is important to use enough qubits. A sensible rule is that you need to use m qubits so that $2^m$ >> $N^2$, where N is the number we are trying to factor, because the order of a random number might be as large as N\n",
        "\n",
        "* Example: Lets factor N=119. Suppose we pick the number 16 to start with. Wie viele Qubits m sollten wir mindestens nehmen? $N^2$ = $119^2$ =14.161 und $2^m$ muss deutlich grosser sein, also mindestens = $2^{14}$ = 16.384. Wir brauchen also mindestens 14 Qubits, um 119 zu faktorisieren.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_090.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrrdCnywczMT"
      },
      "source": [
        "* Because we know that the order of x will be even and $x^{s/2}$ will be a nontrivial square root with probability at least 1/2, we can be confident that we will be able to factor N in just a few runs of the algorithm. Because the time it takes to find the period grows as a polynomial in the number of bits, and the number of bits grows like 2logN(by the above requirement), we expect the time it takes to factor N to grow as a polynomial in logN.\n",
        "\n",
        "* Here is the circuit for Shor’s Algorithm. It relies heavily on period finding, and so the circuit looks a lot like the circuit for period finding. The key difference is that we are finding the period of f(i) = xi, and the number of bits we need to input is very large.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_091.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQthOfm_JfT"
      },
      "source": [
        "**How does it work in the quantum circuit?**\n",
        "\n",
        "That's the function in $U$: given an $x$, the $U$ will compute:\n",
        "\n",
        "> $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_092.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_093.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mApRUwSPydz"
      },
      "source": [
        "**<font color=\"blue\">Shor's Algorithm: Step by Step**\n",
        "\n",
        "**Beispiel: a=13 und N=15, was macht Shor's Algorithm genau im Circuit an der Stelle $U_{f_{(a,N)}}$ und $QFT^{\\dagger}$?**\n",
        "\n",
        "ps: a muss ein Coprime von N sein. Wenn es kein Coprime ist, muessen wir nicht durch Shor's Algorithm gehen, weil a dann einen Faktor mit N teilt :) Aber es ist very unlikely to find a coprime of a large number N.\n",
        "\n",
        "**First let's divide it into steps. 1-5:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrEvCMunLBeC"
      },
      "source": [
        "**Step 1**: Get Qubits in state 0 and apply Hadamard Superposition\n",
        "\n",
        "We start with 4 Qubits all in zeros, mit den Registers x und w, und jedes 4 Mal Tensorproduct multipliziert, weil wir 4 Qubits haben:\n",
        "\n",
        "> $|0\\rangle_{x}^{\\otimes 4}$ $|0\\rangle_{w}^{\\otimes 4}$\n",
        "\n",
        "All Hamadard Gates are applied to top 4 Qubits (x register), and right part (w register) gets nothing applied to it:\n",
        "\n",
        "> $[H^{\\otimes 4}|0\\rangle] \\,\\, |0\\rangle^{\\otimes^{4}}$\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle+|1\\rangle+|2\\rangle+\\cdots+|15\\rangle]$ $|0\\rangle$\n",
        "\n",
        "* Reminder 1: Multiplikation mit $\\frac{1}{4}$, weil 4 Qubits in Hadamard-Superposition\n",
        "\n",
        "* Reminder 2: this is the 4 bit representation of the decimal number, so for example 15 in binary = 1111. Daher kann man auch die 4 angeben als Erinnerung der Bit representation:\n",
        "\n",
        "> = $\\frac{1}{4}[|0\\rangle_4+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $|0\\rangle_4$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKOyXX5bG_i1"
      },
      "source": [
        "**Step 2**: Compute $U$ with $f_{a, N}(x) \\equiv a^{x}(\\bmod N)$ - Was passiert genau in der Box mit $U_{f_{(a,N)}}$?\n",
        "\n",
        "**Given an $x$, the $U$ will compute: <font color=\"red\">$f_{a, N}(x) \\equiv a^{x}(\\bmod N)$</font>**\n",
        "\n",
        "Schauen wir nochmal im vorherigen Schritt und markieren eine Komponente:\n",
        "\n",
        "> = $\\frac{1}{4}[$ <font color=\"red\">$|0\\rangle_4$</font> $+|1\\rangle_4+|2\\rangle_4+\\cdots+|15\\rangle_4]$ $\\,$ $|0\\rangle_4$\n",
        "\n",
        "<font color=\"red\">$U_{f_{(a,N)}}$</font> macht dann folgendes:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|  0 \\bigoplus 13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|0 \\bigoplus 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|0 \\bigoplus 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|0 \\bigoplus 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Remember: $\\bigoplus$ means \"addition modular 2\" bzw. \"XOR\". Anything XORs with 0, is thing itself: 0 $\\bigoplus$ Z = Z. damit ergibt sich folgende Rechnung:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\, \\left|   13^{0}(\\bmod 15)\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left| 13^{1}(\\bmod 15)\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left| 13^{2}(\\bmod 15)\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left| 13^{3}(\\bmod 15)\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "\n",
        "Aus der Modulo-Rechnung ergeben sich die Restwerte:\n",
        "\n",
        "* <font color=\"red\">$13^{0}(\\bmod 15)$ = 1</font>\n",
        "\n",
        "* $13^{1}(\\bmod 15)$ = 13\n",
        "\n",
        "* $13^{2}(\\bmod 15)$ = 4\n",
        "\n",
        "* $13^{3}(\\bmod 15)$ = 7\n",
        "\n",
        "* $13^{4}(\\bmod 15)$ = 1\n",
        "\n",
        "* usw..\n",
        "\n",
        "Since it's periodic, it will repeat, with the x and w register:\n",
        "\n",
        "> = $\\frac{1}{4}$ <font color=\"red\">[$|0\\rangle_{4}\\,\\left|1\\right\\rangle_{4}$</font> + $|1\\rangle_{4}\\left|13\\right\\rangle_{4}$ + $|2\\rangle_{4}\\left|4\\right\\rangle_{4}$ + $|3\\rangle_{4}\\left|7\\right\\rangle_{4}$ etc..]\n",
        "\n",
        "Hier nochmal untereinander mit denselben Restwerten zur besseren Visualisierung:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_094.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-QZylnP7nv"
      },
      "source": [
        "**Step 3: Measurement of the w register / bottom 4 Qubits**\n",
        "\n",
        "* the outputs of the w-register measurements are either 1, 13, 4 or 7 (die Restwerte) with equal probability\n",
        "\n",
        "* let's say we measure 7, what happens to x? X becomes either 3, 7, 11 or 15 (the value in front of the qubit with 7!) with equal probability:\n",
        "\n",
        "  * after $|\\omega\\rangle$ = $|7\\rangle_4$ , $|x\\rangle$ becomes:\n",
        "\n",
        "  * <font color=\"blue\">$|x\\rangle$ $|\\omega\\rangle$ = $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$ $\\otimes |7\\rangle_4$\n",
        "\n",
        "  * Normalization has changed: before we had 16 combinations mit 1/4, here we have only 4 combinations with 1/2 (=one over square root of 4)\n",
        "\n",
        "* **For the next step 4, the Restwert doesn't matter anymore, here: $\\otimes |7\\rangle_4$. We can ignore it. Because it step 4 we apply the measured $|x\\rangle$ in the $QFT^{\\dagger}$, and don't care about $|\\omega\\rangle$ anymore**. And $|x\\rangle$ is in this case: $\\frac{1}{2}\\left[|3\\rangle_{4}+|7\\rangle_{4}+|11\\rangle_{4}+ |15 \\rangle_{4}\\right]$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_095.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exkurs: Eine komplexe Zahl $z=a+b i$ und die zu ihr konjugiert komplexe Zahl $\\bar{z}=a-b i$*:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Komplexe_konjugation.svg/294px-Komplexe_konjugation.svg.png)\n",
        "\n",
        "Ändert man das Vorzeichen des Imaginärteils $b$ einer komplexen Zahl\n",
        "\n",
        "> $z=a+b \\mathrm{i}$\n",
        "\n",
        "so erhält man die zu $z$ konjugiert komplexe Zahl\n",
        "\n",
        "> $\\bar{z}=a-b \\mathrm{i}$\n",
        "\n",
        "(manchmal auch $z^{*}$ geschrieben).\n",
        "\n",
        "Die Konjugation $\\mathbb{C} \\rightarrow \\mathbb{C}, z \\mapsto \\bar{z}$ ist ein (involutorischer) Körperautomorphismus, da sie mit Addition und Multiplikation verträglich ist, d. h., für alle $y, z \\in \\mathbb{C}$ gilt\n",
        "\n",
        ">$\n",
        "\\overline{y+z}=\\bar{y}+\\bar{z}, \\quad \\overline{y \\cdot z}=\\bar{y} \\cdot \\bar{z}\n",
        "$\n",
        "\n",
        "In der Polardarstellung hat die konjugiert komplexe Zahl $\\bar{z}$ bei unverändertem Betrag gerade den negativen Winkel von $z$.\n",
        "\n",
        "* **Man kann die Konjugation in der komplexen Zahlenebene also als die Spiegelung an der reellen Achse interpretieren**.\n",
        "\n",
        "* <font color=\"blue\">**Insbesondere werden unter der Konjugation genau die reellen Zahlen auf sich selbst abgebildet**.\n",
        "\n",
        "Das Produkt aus einer komplexen Zahl $z=a+b$ i und ihrer komplex Konjugierten $\\bar{z}$ ergibt das Quadrat ihres Betrages:\n",
        "\n",
        "> $\n",
        "z \\cdot \\bar{z}=(a+b i)(a-b i)=a^{2}+b^{2}=|z|^{2}\n",
        "$\n",
        "\n",
        "Die komplexen Zahlen bilden damit ein triviales Beispiel einer [C*-Algebra](https://de.m.wikipedia.org/wiki/C*-Algebra)."
      ],
      "metadata": {
        "id": "imQLT4Zc7er9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_7uaW02Wyvg"
      },
      "source": [
        "**Step 4**: Apply inverse $QFT^{\\dagger}$ on the $|x\\rangle$ register\n",
        "\n",
        "* $QFT\\,\\,|x\\rangle=|\\tilde{x}\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{2 \\pi i}{N} x y} |y\\rangle$ (Reminder!)\n",
        "\n",
        "* $QFT^{\\dagger}|\\tilde{x}\\rangle=|x\\rangle=$ $\\frac{1}{\\sqrt{N}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i}{N} x y} |y\\rangle$ (see -2 turning i in -i which is a **complex conjugate operation**)\n",
        "\n",
        "* We want to know what QFT dagger is doing to (it is $\\frac{1}{\\sqrt{16}}$ because we have 4 Qubits)\n",
        "\n",
        "  * $QFT^{\\dagger}|3\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 3 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|7\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 7 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|11\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 11 y}{16}}|y\\rangle$\n",
        "\n",
        "  * $QFT^{\\dagger}|15\\rangle_4$ = $\\frac{1}{\\sqrt{16}} \\sum_{y=0}^{N-1} e^{\\frac{-2 \\pi i 15 y}{16}}|y\\rangle$\n",
        "\n",
        "Alltogether:\n",
        "\n",
        "  * $QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}} \\sum_{y=0}^{15}$ [ $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$] $|y\\rangle$\n",
        "\n",
        "    * with: $e^{-i\\frac{ 3 \\pi}{8}y}$ = $\\cos \\left(\\frac{3 \\pi}{8} y\\right)-i \\sin \\left(\\frac{3 \\pi}{8} y\\right)$ (und aquivalent fur alle anderen drei)\n",
        "\n",
        "    * siehe coding rechnung unten was genau passiert hier!\n",
        "\n",
        "  * <font color=\"blue\">$QFT^{\\dagger}|x\\rangle$ = $\\frac{1}{{8}}$ [ $4|0\\rangle_4$ + $4i|4\\rangle_4$ $-4|8\\rangle_4$ $-4i|12\\rangle_4$ ]</font>\n",
        "\n",
        "  * Remember we had a sum before: $\\frac{1}{{8}} \\sum_{y=0}^{15}$. And notice how all the other terms now vanished to zero, because you had equal contributions of plus and minus.\n",
        "\n",
        "    * **This is exactly what it means when people tell you that quantum computers take advantage of interference!! = when a lot of the terms vanish, and the answer only converges to the terms that we care about.**\n",
        "\n",
        "    * here is the calculation what happened, you see many zeros:\n",
        "\n",
        "<font color=\"red\">Hier Beispielrechnung fur y=1, um vanishing components zu verstehen</font>. Unten im Code die Ergebnisse, zum Beispiel fur y=1 als Ergebnis = 0, $QFT^{\\dagger}|x\\rangle$ fur y = 1:\n",
        "\n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}y}$ + $e^{-i\\frac{ 7 \\pi}{8}y}$ + $e^{-i\\frac{ 11 \\pi}{8}y}$ + $e^{-i\\frac{ 15 \\pi}{8}y}$ =\n",
        "\n",
        "  * $e^{-i\\frac{ 3 \\pi}{8}1}$ + $e^{-i\\frac{ 7 \\pi}{8}1}$ + $e^{-i\\frac{ 11 \\pi}{8}1}$ + $e^{-i\\frac{ 15 \\pi}{8}1}$ =\n",
        "\n",
        "    * $e^{-i\\frac{ 3 \\pi}{8}1}$ = <font color=\"green\">0,382683432 - 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 7 \\pi}{8}1}$ = <font color=\"orange\">-0,923879533 - 0,382683432 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 11 \\pi}{8}1}$ = <font color=\"green\">-0,382683432 + 0,923879533 i</font>\n",
        "\n",
        "    * $e^{-i\\frac{ 15 \\pi}{8}1}$ = <font color=\"orange\">0,923879533 + 0,382683432 i</font>\n",
        "\n",
        "  * Wie man sieht canceln sich die Terme aus (in gleicher Farbe), weshalb als Ergebnis fur y=1 Null entsteht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWn_Jmu_OMSJ"
      },
      "source": [
        "# Hier Beispiel fur y=1 und den ersten e-Term:\n",
        "y = 1\n",
        "pi = np.pi\n",
        "coeff = np.exp(-1j*3*pi/8 * y)\n",
        "if abs(coeff) < 1e-10: coeff= 0\n",
        "print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iERBMPR2y9hm"
      },
      "source": [
        "# Hier die komplette Rechnung fur alle y und alle 4 e-Terme:\n",
        "import numpy as np\n",
        "\n",
        "pi = np.pi\n",
        "for y in range (15) :\n",
        "  coeff = np.exp(-1j*3*pi/8 * y) + \\\n",
        "          np.exp(-1j*7*pi/8 * y) + \\\n",
        "          np.exp(-1j*11*pi/8* y) + \\\n",
        "          np.exp(-1j*15*pi/8* y)\n",
        "  if abs(coeff) < 1e-10: coeff= 0\n",
        "  print(y, coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXrcIqo14n0"
      },
      "source": [
        "**Step 5: Measure the |x> register**\n",
        "\n",
        "* You get either 0 or 4 or 8 or 12 with equal probability\n",
        "\n",
        "* Remaining steps are classical post-processing\n",
        "\n",
        "* You can already see the periodicity in the result: the difference is always 4\n",
        "\n",
        "* Analyse what happens for each outcome: **The measurement results peak near $j\\frac{N}{r}$ for same integer j $\\in Z$. And r is the period that we are looking for. N = $2^n$ Qubits!**\n",
        "\n",
        "  * if we measure |4>$_4$: $j\\frac{16}{r}$ = 4, true if j=1 and r=4\n",
        "\n",
        "  * there are multiple values that would work, but this is the lowest one\n",
        "\n",
        "* now check our protocoll for r=4:\n",
        "\n",
        "  * Is r even? yes!\n",
        "\n",
        "  * $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{4 / 2}(\\bmod 15)$ = 4\n",
        "\n",
        "  * x+1 = 5 and x-1 = 3\n",
        "\n",
        "* This looks good, now check:\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(5,15)=5$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "What do you do if r = 8 ?\n",
        "\n",
        "* |8>$_4$: $j\\frac{16}{r}$ = 8, true if j=1 and r=2 AND j=2 and r=4\n",
        "\n",
        "* if r=4 we are back in the case before\n",
        "\n",
        "* if r=2 then $x \\equiv a^{r / 2}(\\bmod N)$ = $13^{2 / 2}(\\bmod 15)$ = 2, which brings x+1 = 3 and x-1 = 1\n",
        "\n",
        "  * $\\operatorname{gcd}(x+1, N)=\\operatorname{gcd}(3,15)=3$\n",
        "\n",
        "  * $\\operatorname{gcd}(x-1, N)=\\operatorname{gcd}(1,15)=1$\n",
        "\n",
        "* This leads you to a partial solution. Now you can back out the other solution, with checking 3 divides into 15\n",
        "\n",
        "* If we get r=0, then we need to do the experiment again\n",
        "\n",
        "Hier die Faktorisierungsergebnisse fur verschiedene QC-Ausgaben r. Mit r=0 geht es nicht, also kann man in 3 von 4 Faellen faktorisieren (und mit r=8 bekommt man eine partial solution, kann aber immer noch faktorisieren).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_097.png)\n",
        "\n",
        "Aus dem 2001 Paper von IBM, Faktorisierung von 15 auf einem Quantum Computer:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_096.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8beuwX8zVd_"
      },
      "source": [
        "**Appendix: What is the Gate structure in $U$?**\n",
        "\n",
        "* $a^{x_1}$, $a^{x_2}$, $a^{x_n}$ tells you this is a controlled operation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_098.png)\n",
        "\n",
        "* look now how the exponent doesn't contain $x_1$, $x_2$, .. $x_n$ anymore\n",
        "\n",
        "* this is done by implementing it by doing these controls\n",
        "\n",
        "* this is exactly like quantum phase estimation\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_099.png)\n",
        "\n",
        "**Der linke Term stammt aus QPE, der rechte Term ist der Teil $U$ aus Shor's Algorithms:**\n",
        "\n",
        "> <font color=\"blue\">$U^{2^{x}}=a^{2^{x}}(\\bmod N)$</font>\n",
        "\n",
        "continue: https://youtu.be/IFmkzWF-S2k?t=1181"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Phase Estimation*"
      ],
      "metadata": {
        "id": "GXXGNyygfl3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exkurs: Quantum Phase Kickback*"
      ],
      "metadata": {
        "id": "Op-cit2CFEiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNOT Gate and Phase Kickback**\n",
        "\n",
        "<font color=\"blue\">*CNOT-Gate applied to the computational basis 0 and 1*\n",
        "\n",
        "* https://qiskit.org/textbook/ch-gates/phase-kickback.html\n",
        "\n",
        "* Main article about Phase Kickback: https://towardsdatascience.com/quantum-phase-kickback-bb83d976a448\n",
        "\n",
        "The CNOT-gate is a two-qubit gate. Thus, it transforms qubit states whose state we represent by a four-dimensional vector.\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\alpha|0\\rangle|0\\rangle+\\beta|0\\rangle|1\\rangle+\\gamma|1\\rangle|0\\rangle+\\delta|1\\rangle|1\\rangle=\\left[\\begin{array}{c}\n",
        "\\alpha \\\\\n",
        "\\beta \\\\\n",
        "\\gamma \\\\\n",
        "\\delta\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Remember Vector-Vector-Multiplikation (Kronecker / tensor product):\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$.\n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Accordingly, the CNOT-gate has a $4 \\times 4$ transformation matrix.\n",
        "\n",
        ">$\n",
        "C N O T=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "There is no effect if the control qubit (at the left-hand position in the Dirac notation) is in state |0⟩, as in states |00⟩ and |01⟩.\n",
        "\n",
        "> CNOT $\\cdot|00\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=|00\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|01\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=|01\\rangle$\n",
        "\n",
        "<font color=\"blue\">But if the control qubit is in state |1⟩, then the controlled (target) qubit switches from |0⟩ to |1⟩ and vice versa.</font>\n",
        "\n",
        "> CNOT $\\cdot|10\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=|11\\rangle$\n",
        "\n",
        "> CNOT $\\cdot|11\\rangle=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=|10\\rangle$\n",
        "\n",
        "When we describe the quantum states and operations in terms of mathematical formulae, we use the vectors |0⟩ and |1⟩ as a basis. |0⟩ and |1⟩ denote the standard or computational basis states. These states correspond to the possible measurements we might obtain when looking at the qubit. We measure a qubit in state |0⟩ as 0 with absolute certainty. And, we measure a qubit in state |1⟩ as 1, accordingly. While the basis {|0⟩,|1⟩} is convenient to work with mathematically, it is just a representation of the underlying physics.\n"
      ],
      "metadata": {
        "id": "-0AI_GVFZk0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">*CNOT-Gate applied to the superposition basis + and -*\n",
        "\n",
        "The mathematical basis we chose leads to a specific representation of the CNOT-transformation. But this is not the only possible representation. In fact, there are infinitely many other possible choices. Our qubits are not limited to these two states. Qubits can be in a superposition of both states. For instance, there are the states that result from applying the Hadamard-gate on the basis states:\n",
        "\n",
        "> $|+\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ and $|-\\rangle=\\left[\\begin{array}{c}\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}}\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "Now apply Hadamard gate on a qubit that is in the |1> state:\n",
        "\n",
        "> $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "Mathematically, the following matrix represents the application of Hadamard gates on each of the two qubits.\n",
        "\n",
        "> $H \\otimes H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}H & H \\\\ H & -H\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]$\n",
        "\n",
        "So, if we apply this matrix on two qubits in state |00⟩, they end up in state |++⟩.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|00\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |01⟩ results in state |+−⟩.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|01\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "The input state |10⟩ results in state |−+⟩.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|10\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "Finally, if we apply this transformation on two qubits in state |11⟩, we put them into state |−−⟩.\n",
        "\n",
        "> $\\begin{aligned} H \\otimes H(|11\\rangle) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$"
      ],
      "metadata": {
        "id": "7s9KUHDdV_5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s apply the CNOT-gate on qubits in superposition. We can calculate the overall transformation matrix by multiplying the matrices of the CNOT-gate and the H⊗H transformation. The CNOT-gate switches the second and fourth columns of the H⊗H-matrix.\n",
        "\n",
        "> $\\operatorname{CNOT}(H \\otimes H)=\\left[\\begin{array}{cccc}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0\\end{array}\\right] \\cdot \\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right]$\n",
        "\n",
        "* And now, we apply this transformation to the four combinations of basis states.\n",
        "\n",
        "<font color=\"blue\">If the target qubit (at the right-hand side) is in state |1⟩, the state of the control qubit (at the left-hand side) flips from |+⟩ to |−⟩ and vice versa:\n",
        "\n",
        ">\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|00\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle+|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|++\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|01\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ -1 \\\\ 1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle-|1\\rangle|0\\rangle+|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|--\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|10\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle+|0\\rangle|1\\rangle-|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\\\ &=|-+\\rangle \\end{aligned}$\n",
        "\n",
        "> $\\begin{aligned} \\operatorname{CNOT}(H \\otimes H(|11\\rangle)) &=\\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & -1 & -1 & 1 \\\\ 1 & 1 & -1 & -1\\end{array}\\right] \\cdot\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{c}1 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right] \\\\ &=\\frac{1}{2}(|0\\rangle|0\\rangle-|0\\rangle|1\\rangle+|1\\rangle|0\\rangle-|1\\rangle|1\\rangle) \\\\ &=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle) \\otimes \\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle) \\\\ &=|+-\\rangle \\end{aligned}$\n",
        "\n",
        "In short, we can say:\n",
        "\n",
        "> $\\operatorname{CNOT}(|++\\rangle)=|++\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|+-\\rangle)=|--\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|-+\\rangle)=|-+\\rangle$\n",
        "\n",
        "> $\\operatorname{CNOT}(|--\\rangle)=|+-\\rangle$\n",
        "\n",
        "The two states |+⟩ and |−⟩ have the same measurement probabilities of |0⟩ and |1⟩. They result in either value with a probability of 0.5. **So, the CNOT-gate does not have any directly measurable implications**. <font color=\"blue\">However, the control qubit switches its phase. It takes on the phase of the controlled (target) qubit.</font>\n",
        "\n",
        "> For the phase of the target qubit is kicked up to the control qubit, we call this phenomenon phase kickback.\n",
        "\n",
        "We learned the CNOT-gate is not a one-sided operation. It clearly has the potential to affect the state of the control qubit. Even though the phase is not directly measurable, there are ways to exploit differences in the phase between states. In fact, prominent algorithms, such as Grover’s search algorithm, exploit this effect.\n"
      ],
      "metadata": {
        "id": "Tg8PpY31YMh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Phase Kickback: Control qubit changes:**\n",
        "* from 0 to 1 or from 1 to 0 if target qubit was in state 1,\n",
        "* from (+) to (-) or from (-) to (+) if the target was in (-)"
      ],
      "metadata": {
        "id": "9Z_IIWzYdHD3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwuxaKm4lF2"
      },
      "source": [
        "**Quantum Phase Estimation**\n",
        "\n",
        "* algorithm for determining the eigenvalues of a unitary operator\n",
        "\n",
        "* the [quantum phase estimation algorithm](https://en.m.wikipedia.org/wiki/Quantum_phase_estimation_algorithm) (also referred to as quantum eigenvalue estimation algorithm), is a quantum algorithm to estimate the phase (or eigenvalue) of an eigenvector of a unitary operator.\n",
        "\n",
        "* More precisely, given a unitary matrix $U$ and a quantum state $|\\psi\\rangle$ such that $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$, the algorithm estimates the value of $\\theta$ with high probability within additive error $\\varepsilon$, using $O(\\log (1 / \\varepsilon))$ qubits (without counting the ones used to encode the eigenvector state) and $O(1 / \\varepsilon)$ controlled- $U$ operations.\n",
        "\n",
        "* The algorithm was initially introduced by Alexei Kitaev in 1995.\n",
        "\n",
        "* Phase estimation is frequently used as a subroutine in other quantum algorithms, such as Shor's algorithm and the quantum algorithm for linear systems of equations.\n",
        "\n",
        "<font color=\"blue\">*One Qubit Phase Estimation (with Hadamard Gate):*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_117.png)\n",
        "\n",
        "<font color=\"blue\">*Multi-Qubit Phase Estimation (with inverse Quantum Fourier Transform):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/a/a5/PhaseCircuit-crop.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7UKWz2y03L"
      },
      "source": [
        "Remember in **Quantum Fourier Transform**:\n",
        "\n",
        "\n",
        "> x1 = $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "* <font color=\"blue\">$e^{2\\pi i}$ = 1 = identity</font>\n",
        "\n",
        "* In Quantum Fourier Transform we change the phase <font color=\"blue\">$\\theta$ in $e^{2\\pi i}$</font> <font color=\"red\">$^{\\theta}$</font>\n",
        "\n",
        "  * <font color=\"red\">= Eigenvalue of Oracle function $U$ associated with an eigenvector |u⟩</font>\n",
        "\n",
        "* Phase <font color=\"blue\">$\\theta$ is $\\frac{x_n}{2^{k}}$ with $x_n$ 0 or 1</font> state and $k$ number of Qubits.\n",
        "\n",
        "* A controlled-R quantum gate applies a relative phase change to |1>. The matrix form of this operator is: <font color=\"blue\">$\\hat{R}_{k}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & e^{2 \\pi i / 2^{k}}\\end{array}\\right)$\n",
        "\n",
        "**Now in Phase Estimation**:\n",
        "\n",
        "> In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$ mit dem Operator: $U^{2^n} = \\phi$\n",
        "\n",
        "Quantum phase estimation addresses the following problem:\n",
        "* We have a $n$-qubit oracle function $U$, encoded in the form of a controlled- $U$ unitary.\n",
        "* **$U$ has an eigenvalue $e^{2 \\pi i \\phi}$, associated with an eigenvector $|u\\rangle$ which we can prepare.**\n",
        "* <font color=\"red\">**We wish to estimate the phase, $\\phi$, of the eigenvalue to $t$ bits of precision.**\n",
        "\n",
        "> <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$** $\\quad$ (based on Eigenvalue equation)</font>\n",
        "\n",
        "* Here $|\\psi\\rangle$ is an eigenvector / eigenstate and $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue.\n",
        "\n",
        "* <font color=\"red\">For example: the eigenvalues of X are −1 and 1 and have the eigenvectors |−⟩ and |+⟩ respectively.*</font>\n",
        "\n",
        "*Since $U$ is unitary, all of its eigenvalues have a norm of 1.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-6ThVrEdXJ"
      },
      "source": [
        "Reminder: QFT\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_084.png)\n",
        "\n",
        ">**See below: <font color=\"red\">Remember that a unitary matrix has eigenvalues of the form $e^{i \\theta_{\\psi}}$ (ohne $2 \\pi$ wie oben bei QFT) and that it has eigenvectors that form an orthonormal basis**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_083.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p-zzXyW-tU6"
      },
      "source": [
        "The problem: in both cases the probability is 0,5, just differs by the phase added: $=e^{\\frac{i \\pi}{2}}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_078.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSvCLdgF_YQC"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_079.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rHCBrjo_ZWr"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_080.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scRiAADhBR3t"
      },
      "source": [
        "**The probability of measuring 0 and 1 is each 0,5, but there is a small factor that makes them differ from 0,5, depending on the phase (angle):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0oYz_yZBCoU"
      },
      "source": [
        "> **In the different between the probability of measuring 0 or 1, you've encoded that phase! (In other words: you've taken that phase information and turned it into and amplitude that you can measure.**\n",
        "\n",
        "* How to do this experimentally: you do a million shots of the experiment, collect statistics and check what the statistics say. How many times did I get zero? How many times did I get one? The hope is that the difference between the statistics of zero and one would allow us to back out theta\n",
        "\n",
        "* Next level: now getting more precision with more qubits: (there is another circuit to prepare Psi yet, which is assumed to be given here)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_082.png)\n",
        "\n",
        "writing out the calculation:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_085.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1qf44PHLJJ"
      },
      "source": [
        "**Comparing QPE with QFT (QPE is the same as QFT with a different phase):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_086.png)\n",
        "\n",
        "It's like applying a QFT of something (of a special phase $\\frac{\\theta_{\\psi}}{2^{n}} 2 \\pi$, the green box above!), and in order to get back to the original state you need to apply an inverse QFT at the end:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_087.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 1: Set up the unitary and number of bits to use in phase estimation*\n",
        "\n",
        "<font color=\"blue\">*Let's take as an example the T-gate, and use Quantum Phase Estimation to estimate its phase.*\n",
        "\n",
        "You will remember that the $T$-gate adds a phase of $e^{\\frac{i \\pi}{4}}$ to the state $|1\\rangle$ :\n",
        "\n",
        "$\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$\n",
        "\n",
        "Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$\n",
        "\n",
        "<font color=\"red\">We expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "We first perform a Hadamard gate on the first qubit to get the state\n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "\n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "\n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "  * <font color=\"blue\">this part above is the rule from tensor products: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition: $\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "</font>\n",
        "\n",
        "    * The Hadamard states ∣+⟩ and ∣−⟩ are considered superposition states because they are a combination of the two computational states:\n",
        "\n",
        "    * State: $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$ so for + it is: $|\\+\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "  * we have intentionally omitted the normalization factor of 1/√2 for clarity\n",
        "\n",
        "> $|+\\rangle \\otimes|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] \\otimes\\left[\\begin{array}{l}\\psi\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\, [\\psi] \\\\ 1 \\, [\\psi]\\end{array}\\right]$\n",
        "\n",
        "Remember: Apply Hadamard gate on a qubit that is in the |0> state:\n",
        "\n",
        "> $|+\\rangle$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ =  $\\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle$"
      ],
      "metadata": {
        "id": "mV4nEZ1l2VMu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zxsy6Cc5Wrx"
      },
      "source": [
        "# Value of θ which appears in the definition of the unitary U above.\n",
        "# Try different values.\n",
        "theta = 0.125\n",
        "\n",
        "# Define the unitary U-Gate:\n",
        "U = cirq.Z ** (2 * theta)\n",
        "\n",
        "# Accuracy of the estimate for theta. Try different values.\n",
        "n_bits = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8u3NsIlTNsO"
      },
      "source": [
        "Here details about unitary U-Gate:\n",
        "\n",
        "$U$ = $Z^{2^{n-n}}$\n",
        "\n",
        "Z = $e^{\\pi}$\n",
        "\n",
        "* $Z$ entspricht $\\pi$ (ein halber Kreis, zB von +1 zu -1 auf X-Achse)\n",
        "\n",
        "\n",
        "then:\n",
        "\n",
        "> <font color=\"blue\">$U$ = $e^{\\pi * 2^{n-n}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 2: Build the first part of the circuit for phase estimation with controlled U-gate (Phase Kickback)*\n",
        "\n",
        "We then perform a controlled U operation, which we have written as $U^{2^0}$. Here applies the **Phase Kickback!**\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+|1\\rangle$ <font color=\"red\">$U$</font> $|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle|\\psi\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle|\\psi\\rangle$ =\n",
        "\n",
        "  * $|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font> $|1\\rangle) \\otimes|\\psi\\rangle$\n",
        "\n",
        "* Here are 2 things very important:\n",
        "\n",
        "    * The second qubit register containing |ψ⟩ hasn’t changed. We shouldn’t expect it to, **since |ψ⟩ is an eigenstate of U (Remember: <font color=\"blue\">**Given a unitary operator $U$, the algorithm estimates $\\theta$ in $U|\\psi\\rangle=e^{2 \\pi i \\theta}|\\psi\\rangle$ based on the Eigenvalue equation**</font>). Thus, no matter how many times we apply U to this register, nothing happens to |ψ⟩**. But if we apply it more often it will 'amplify' the phase (Not in the sense of amplitude amplification) - we amplify it with adding more qubits and hence more $\\phi$ to get more precision\n",
        "\n",
        "    * what’s the point of applying U then? The effect was that **it wrote some information about the eigenvalue into the relative phase of the first qubit**. Namely, the entire effect was to\n",
        "map: $|0\\rangle+|1\\rangle \\mapsto|0\\rangle+e^{2 \\pi i 0. \\phi_{1}}|1\\rangle$"
      ],
      "metadata": {
        "id": "iQE_Unsl2YXo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMG3fjxw5aXU"
      },
      "source": [
        "# Get qubits for the phase estimation circuit.\n",
        "qubits = cirq.LineQubit.range(n_bits)\n",
        "u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "# Build the first part of the phase estimation circuit.\n",
        "phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "# Set the input state of the eigenvalue register: Add gate to change initial state to |1>\n",
        "phase_estimator.insert(0, cirq.X(u_bit))\n",
        "\n",
        "# bit = cirq.LineQubit\n",
        "for i, bit in enumerate(qubits):\n",
        "    phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - i - 1)))\n",
        "    # explanation: U-rot control aktiviert wenn entsprechendes qubit in state 1 (??)\n",
        "    # dann aktiviere formel: U^2^(n-1) ...U^2^(n-2) ...U^2^(n-n)\n",
        "\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoxXF9-W9orx"
      },
      "source": [
        "> <font color=\"blue\">$U$ = $Z^{2^{n-n}}$ = $e^{\\pi * 2^{n-n}}$ fur das erste Gate: = $e^{\\pi * (-0.128)}$ ????\n",
        "\n",
        "\n",
        "*Why are we adding Pauli-X? The initial state for u_bit is the  state, but the phase for this state is trivial with the operator we chose. Inserting a Pauli  operator at the begining of the circuit changes this to the  state, which has the nontrivial  phase.*\n",
        "\n",
        "*The controlled u gate*:\n",
        "\n",
        "$|00\\rangle \\mapsto|00\\rangle$\n",
        "\n",
        "$|01\\rangle \\mapsto|01\\rangle$\n",
        "\n",
        "$|10\\rangle \\mapsto|1\\rangle \\otimes U|0\\rangle=|1\\rangle \\otimes\\left(u_{00}|0\\rangle+u_{10}|1\\rangle\\right)$\n",
        "\n",
        "$|11\\rangle \\mapsto|1\\rangle \\otimes U|1\\rangle=|1\\rangle \\otimes\\left(u_{01}|0\\rangle+u_{11}|1\\rangle\\right)$\n",
        "\n",
        "The matrix representing the controlled $U$ is\n",
        "\n",
        ">$\n",
        "\\mathrm{C} U=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & u_{00} & u_{01} \\\\\n",
        "0 & 0 & u_{10} & u_{11}\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "**When U is one of the Pauli operators, X,Y, Z, the respective terms \"controlled-X\", \"controlled-Y\", or \"controlled-Z\" are sometimes used**.\n",
        "Sometimes this is shortened to just CX, CY and CZ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti0MmESYOnrd"
      },
      "source": [
        "<font color=\"blue\">*Why should we use more than one control Qubit?*\n",
        "\n",
        "**Remember from Eigenvalue problem: Ax = λx in our case with the unitary operator: Ux = λx**\n",
        "\n",
        "> $Ux =$ <font color=\"red\">$e^{2πi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$\n",
        "\n",
        "> Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2πi*0}$ = λ = 1, so dass Ux = 1x. Damit ist λ = 1 ist der Eigenwert von f.\n",
        "\n",
        "* Since |λ| = 1, we can write it without loss of generality as λ = $e^{2πiφ}$, where <font color=\"red\">$e^{2πi}$ = 1 (= identity, if you insert 2*π*i into exponent at random, you will not change the result. Sometimes it can be a useful identity [Source](https://www.physicsforums.com/threads/e-2-pi-i-where-from.430393/), from Euler identity)</font> and **0 ≤ φ ≤ 1 is called the phase. This is what we want to estimate!**\n",
        "\n",
        "* We saw that in QFT, φ being between 0 and 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}$\n",
        "\n",
        "> **The term “estimation” comes about not from the fact that quantum computation is probabilistic, but rather in the degree of precision that we are going to compute, or estimate, the phase to.**\n",
        "\n",
        "* The phase φ is going to be between zero and one, so we can write it as a decimal in binary notation as follows: $φ = 0.φ_1 φ_2 ···φ_n$, where each φi is either zero or one\n",
        "\n",
        "  * The expression $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n}$ is equivalent to $\\phi=0 . \\phi_{1} \\phi_{2} \\cdots \\phi_{n} \\Longleftrightarrow \\phi=\\sum_{k=1}^{n} \\phi_{k} 2^{-k}$. Some numbers as binary decimals:\n",
        "\n",
        "    * <font color=\"blue\">The number 0.5 in decimal is 0.1 in binary, since 0.1 ≡ (1) · $2^{−1}$ = 1/2 = 0.5. So: $0.5_{10} = 0.1_2$. Note that 0.1 is the same as 0.100000....</font>\n",
        "\n",
        "    * <font color=\"blue\">The number 0.75 in decimal is 0.11 in binary, since 0.11 ≡ (1)·$2^{−1}$ +1·$2^{−2}$ = 1/2+1/4 = 3/4 = 0.75. To get this we need 2 Qubits. So we get more precision with more qubits</font>\n",
        "\n",
        "    * 0.111 = 0.875\n",
        "\n",
        "    * 0.1111 = 0.9375 in decimal, because: $0 \\cdot 2^{0}+1 \\cdot 2^{-1}+1 \\cdot 2^{-2}+1 \\cdot 2^{-3}+1 \\cdot 2^{-4}=0 \\cdot 1+1 \\cdot 0.5+1 \\cdot 0.25+1 \\cdot 0.125+1 \\cdot 0.0625=0+0.5+0.25+0.125+0.0625=0.937510$\n",
        "\n",
        "  * Check also what is the value of the infinitely repeating binary decimal 0.1111111...\n",
        "\n",
        "  * If it needed to be proved, the above exercise proves that 0 ≤ 0.φ1φ2 · · · ≤ 1\n",
        "\n",
        "\n",
        "*Operator $U^{2^n}$ in QPE*\n",
        "\n",
        "* $U^{2^0}$: 1 (decimal) = 00001\n",
        "\n",
        "* $U^{2^1}$: 2 (decimal) = 00010\n",
        "\n",
        "* $U^{2^2}$: 4 (decimal) = 00100\n",
        "\n",
        "* $U^{2^3}$: 8 (decimal) = 01000\n",
        "\n",
        "* $U^{2^4}$: 16 (decimal) = 10000\n",
        "\n",
        "\n",
        "**So for falls die Phase 0.111 ist, wuerde bei 3 Qubits QPE berechnen:**\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "  * $2^0$ = 1 in decimal = 001 in binary\n",
        "\n",
        "  * $2^1$ = 2 in decimal = 010 in binary\n",
        "\n",
        "  * $2^2$ = 4 in decimal = 100 in binary\n",
        "\n",
        "* in this case the phase $\\theta$ = 0.111\n",
        "\n",
        "\n",
        "**Compare that with Quantum Fourier Transform:**\n",
        "\n",
        "* In $\\frac{1}{\\sqrt{2}}\\left(|0\\rangle+\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8}\\right)}|1\\rangle\\right)$ dieser Teil ist die **Phase $\\theta$** = $(\\frac{\\mathrm{x}_{1}}{2}+\\frac{x_{2}}{4}+\\frac{x_{3}}{8})$\n",
        "\n",
        "* Let's say all $x_1, x_2$ and $x_3$ = 1 $\\rightarrow$ $\\mathrm{e}^{2 \\pi \\mathrm{i}\\left(\\frac{{1}}{2}+\\frac{1}{4}+\\frac{1}{8}\\right)}$ = $\\mathrm{e}^{2 \\pi \\mathrm{i}(0.5+0.25+0.125)}$ and in binary form: <font color=\"red\">$\\mathrm{e}^{2 \\pi \\mathrm{i}(0.100+0.010+0.001)}$</font>\n",
        "\n",
        "* **We see that in QFT and QPE it's the same (both in red)!**\n",
        "\n",
        "<font color=\"red\">Jedes $U^{2^n}$ wird immer dann aktiviert, wenn im Control-Qubit oben eine 1 gemessen wird (siehe Bild hier unten):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzz61dyQB_6X"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_118.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et3XRupiWg0n"
      },
      "source": [
        "*Step 3: Perform the inverse QFT on the estimation qubits and measure them*\n",
        "\n",
        "\n",
        "How can we read out this information from the quantum state? Consider the effect of applying another Hadamard transformation on the first qubit (without another H we will always measure 50/50 % a 0 or 1), which will produce (ignoring the normalization factor of 1/2):\n",
        "\n",
        "  * $H(|0\\rangle+$ <font color=\"red\">$e^{2 \\pi i 0 \\cdot \\phi_{1}}$</font> $|1\\rangle)=$ $(1+$<font color=\"red\">$e^{2 \\pi i 0. \\phi_{1}}$</font>$)|0\\rangle$ + $(1-$<font color=\"red\">$e^{2 \\pi i 0 . \\phi_{1}}$</font>$)|1\\rangle$\n",
        "\n",
        "  * this shares the phase with the first Qubit and allows us to read it out\n",
        "\n",
        "  * Now, $\\phi_{1}$ can only be zero or one. In the case that $\\phi_{1}=0, e^{2 \\pi i 0 . \\phi_{1}}=1$, hence the state is exactly $|0\\rangle$: $(\\frac{1}{2}\\left(1+e^{2 \\pi i 0 . 0}\\right)|0\\rangle+\\frac{1}{2}\\left(1-e^{2 \\pi i 0 . 0}\\right)|1\\rangle$ = $\\frac{1}{2}\\left(1+1\\right)|0\\rangle+\\frac{1}{2}\\left(1-1\\right)|1\\rangle$ = $|0\\rangle$\n",
        "\n",
        "  * these values in front of $|0\\rangle$ and $|1\\rangle$ are probabilities (here 0 has probability of being measured = 1, but small differences her reveal the phase and hence the Eigenvalue in other cases. See here:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_081.png)\n",
        "\n",
        "* **For 1 Qubit we can use a Hadamard Gate, and for more than 1 Qubit we use the inverse Fourier Transform**: on Quantum Phase Estimation $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ then the inverse Quantum Fourier transform:  <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font> so that: $\\frac{1}{2^{\\frac{n}{2}}} \\sum_{k=0}^{2^{n}-1} e^{2 \\pi i \\theta k}$ <font color=\"red\">$ \\frac{1}{2^{\\frac{n}{2}}} \\sum_{x=0}^{2^{n}-1} e^{\\frac{-2 \\pi i k x}{2^{n}}}|x\\rangle$</font>\n",
        "\n",
        "  * inverse QFT for 1 Qubit is: $ \\frac{1}{2^{\\frac{1}{2}}} \\sum_{x=0}^{2^{1}-1} e^{\\frac{-2 \\pi i k x}{2^{1}}}|x\\rangle$ = $\\frac{1}{\\sqrt{2}} e^{-1 \\pi i k x}$ fur $k$ = $\\varphi$ = 0 and $x$ = 0. --> somehting is not right here yet!\n",
        "\n",
        "Thus, we measure with certainty (i.e., not probabilistically) a state that tells us exactly what the phase,\n",
        "and hence the eigenvalue, is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv_xQnY__HDH"
      },
      "source": [
        "def make_qft_inverse(qubits):\n",
        "    \"\"\"Generator for the inverse QFT on a list of qubits.\"\"\"\n",
        "    qreg = list(qubits)[::-1]\n",
        "    while len(qreg) > 0:\n",
        "        q_head = qreg.pop(0)\n",
        "        yield cirq.H(q_head)\n",
        "        for i, qubit in enumerate(qreg):\n",
        "            yield (cirq.CZ ** (-1 / 2 ** (i + 1)))(qubit, q_head)\n",
        "\n",
        "# Do the inverse QFT\n",
        "phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "# Add measurements to the end of the circuit\n",
        "phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "print(phase_estimator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax explanation for list(qubits)[::-1]: list[<start>:<stop>:<step>]\n",
        "# So, when you do a[::-1], it starts from the end towards the first taking each element.\n",
        "# So it reverses a. This is applicable for lists/tuples as well.\n",
        "# Example: >>> a = '1234' >>> a[::-1] will get you: '4321'"
      ],
      "metadata": {
        "id": "4GAmFOoFH6wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 4: Simulate the circuit and convert from measured bit values to estimated θ values*"
      ],
      "metadata": {
        "id": "oJVEbP4OGB6p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPBFj8JyDYCd"
      },
      "source": [
        "# Simulate the circuit.\n",
        "sim = cirq.Simulator()\n",
        "result = sim.run(phase_estimator, repetitions=10)\n",
        "\n",
        "# Convert from output bitstrings to estimate θ values.\n",
        "theta_estimates = np.sum(2 ** np.arange(n_bits) * result.measurements['m'], axis=1) / 2**n_bits\n",
        "print(theta_estimates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(theta_estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of trials\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "id": "7kifm9aPIcJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iff49oDyK9"
      },
      "source": [
        "def phase_estimation(theta, n_bits, n_reps=10, prepare_eigenstate_gate=cirq.X):\n",
        "    # Define qubit registers.\n",
        "    qubits = cirq.LineQubit.range(n_bits)\n",
        "    u_bit = cirq.NamedQubit('u')\n",
        "\n",
        "    # Define the unitary U.\n",
        "    U = cirq.Z ** (2 * theta)\n",
        "\n",
        "    # Start with Hadamards on every qubit.\n",
        "    phase_estimator = cirq.Circuit(cirq.H.on_each(*qubits))\n",
        "\n",
        "    # Do the controlled powers of the unitary U.\n",
        "    for i, bit in enumerate(qubits):\n",
        "        phase_estimator.append(cirq.ControlledGate(U).on(bit, u_bit) ** (2 ** (n_bits - 1 - i)))\n",
        "\n",
        "    # Do the inverse QFT.\n",
        "    phase_estimator.append(make_qft_inverse(qubits[::-1]))\n",
        "\n",
        "    # Add measurements.\n",
        "    phase_estimator.append(cirq.measure(*qubits, key='m'))\n",
        "\n",
        "    # Gate to choose initial state for the u_bit. Placing X here chooses the |1> state.\n",
        "    phase_estimator.insert(0, prepare_eigenstate_gate.on(u_bit))\n",
        "\n",
        "    # Code to simulate measurements\n",
        "    sim = cirq.Simulator()\n",
        "    result = sim.run(phase_estimator, repetitions=n_reps)\n",
        "\n",
        "    # Convert measurements into estimates of theta\n",
        "    theta_estimates = np.sum(2**np.arange(n_bits)*result.measurements['m'], axis=1)/2**n_bits\n",
        "\n",
        "    return theta_estimates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Analyze convergence vs n_bits.\"\"\"\n",
        "# Set the value of theta. Try different values.\n",
        "theta = 0.123456\n",
        "\n",
        "max_nvals = 16\n",
        "nvals = np.arange(1, max_nvals, step=1)\n",
        "\n",
        "# Get the estimates at each value of n.\n",
        "estimates = []\n",
        "for n in nvals:\n",
        "    estimate = phase_estimation(theta=theta, n_bits=n, n_reps=1)[0]\n",
        "    estimates.append(estimate)\n",
        "\n",
        "print(theta_estimates)\n",
        "print(estimates)\n",
        "\n",
        "\"\"\"Plot the results.\"\"\"\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "plt.plot(nvals, estimates, \"--o\", label=\"Phase estimation\")\n",
        "plt.axhline(theta, label=\"True value\", color=\"black\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of bits\")\n",
        "plt.ylabel(r\"$\\theta$\");"
      ],
      "metadata": {
        "id": "N5ZJqQs63tbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 5: Compute the Eigenvalues from the theta value*\n",
        "\n",
        "Eigenvalue: $e^{2 \\pi i \\theta}$ is the corresponding eigenvalue, so for $\\theta$ = 0.125 $\\rightarrow$ $e^{2 * \\pi * i * 0.125}$ = <font color=\"blue\">0.707106781 + 0.707106781 i (Eigenvalue of T-gate)</font>\n",
        "\n",
        "* Verification: $\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$ $\\rightarrow$ <font color=\"blue\">$e^{\\frac{i \\pi}{4}}$ is the same as $e^{2 * \\pi * i * 0.125}$ bzw. $e^{\\frac{2 * \\pi * i}{8}}$</font>\n",
        "\n",
        "* $e^{2 \\pi i 0. \\varphi_{1} \\varphi_{2} \\varphi_{3}}$</font> = $e^{2 \\pi i 0.(U^{2^0} + U^{2^1} + U^{2^2})}$  = <font color=\"red\">$e^{2 \\pi i 0.001 + 010 + 100)}$</font>  = $e^{2 \\pi i 0.111}$\n",
        "\n",
        "* ps: $e^{2 \\pi i}$ =  1 (identity) - ohne theta, die phase\n",
        "\n",
        "* From Eigenvalue euqation: $Ux =$ <font color=\"red\">$e^{2πi*0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} }$</font> $x$. Beispiel: Wenn $0.\\varphi_{1} \\varphi_{2} \\cdots \\varphi_{n} = 0$, dann ist $e^{2πi*0}$ = λ = 1, so dass Ux = 1x. Damit ist λ = 1 ist der Eigenwert von f.\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/9577/how-to-find-eigenvalues-and-eigenvector-for-a-quantum-gate\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_115.png)\n"
      ],
      "metadata": {
        "id": "DvJzmiWfKQi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Harrow-Hassidim-Lloyd Algorithm (HHL)*"
      ],
      "metadata": {
        "id": "w9lDedY3fgnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Solving a system of linear equations with a quantum computer (HHL)**\n",
        "\n",
        "* **Given a matrix $A \\in \\mathbb{C}^{N \\times N}$ and a vector $\\vec{b} \\in \\mathbb{C}^{N}$, find $\\vec{x} \\in \\mathbb{C}^{N}$ satisfying $A \\vec{x}=\\vec{b}$**\n",
        "\n",
        "* The spectrum of $A$ is given by: $A\\left|v_{j}\\right\\rangle=\\lambda_{j}\\left|v_{j}\\right\\rangle, 1 \\geq\\left|\\lambda_{j}\\right| \\geq 1 / \\kappa$\n",
        "\n",
        "\n",
        "**Objective: We want to solve a system of linear equations by finding $\\vec{x}$**\n",
        "\n",
        "* Familiar methods of solutions: Substitution method, Graphical method, Matrix method, Cramer's rule, Gaussian elimination\n",
        "\n",
        "* The classical algorithm returns the full solution, while the HHL can only approximate functions of the solution vector.\n",
        "\n",
        "\n",
        "> $A \\vec{x} = \\vec{b}$\n",
        "\n",
        "Classically you would take the inverse of $A$ (via spectral decomposition / eigendecomposition):\n",
        "\n",
        "> $\\vec{x} = A^{-1} \\vec{b}$\n",
        "\n",
        "The first step towards solving a system of linear equations with a quantum computer is to encode the problem in the quantum language.\n",
        "\n",
        "* By rescaling the system, we can assume $\\vec{b}$ and $\\vec{x}$ to be normalised and map them to the respective quantum states $|b\\rangle$ and $|x\\rangle$.\n",
        "\n",
        "* Usually the mapping used is such that $i^{\\text {th }}$ component of $\\vec{b}$ (resp. $\\vec{x}$ ) corresponds to the amplitude of the $i^{\\text {th }}$ basis state of the quantum state $|b\\rangle$ (resp. $|x\\rangle$ ).\n",
        "\n",
        "From now on, we will focus on the rescaled problem\n",
        "\n",
        "><font color=\"blue\">$A|x\\rangle=|b\\rangle\n",
        "$</font> $\\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        "><font color=\"blue\">$|x\\rangle=A^{-1}|b\\rangle$</font> $\\quad$ (the solution is: $|x\\rangle = \\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle$)\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        "\n",
        ">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        ">$\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$\n",
        "\n",
        "and the right hand side of the system can be written in the eigenbasis of $A$ as\n",
        "\n",
        ">$\n",
        "|b\\rangle=\\sum_{j=0}^{N-1} b_{j}\\left|u_{j}\\right\\rangle, \\quad b_{j} \\in \\mathbb{C}\n",
        "$\n",
        "\n",
        "It is useful to keep in mind that the goal of the HHL is to exit the algorithm with the readout register in the state\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle\n",
        "$\n",
        "\n",
        "Note that here we already have an implicit normalisation constant since we are talking about a quantum state."
      ],
      "metadata": {
        "id": "KM2Pu-BkaiCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HHL-Algorithm**\n",
        "\n",
        "*Main Subroutines in HHL: Hamiltonian simulation, Phase estimation (newer: linear combination of unitaries) and (Variable-time) amplitude amplification*\n",
        "\n",
        "1. Prepare the initial state $|b\\rangle$. Note that $|b\\rangle=\\sum_{j} c_{j}\\left|v_{j}\\right\\rangle$.\n",
        "\n",
        "2. Use the so-called phase estimation algorithm to perform the map\n",
        "$|b\\rangle \\rightarrow \\sum_{j} c_{j}\\left|v_{j}\\right\\rangle\\left|\\tilde{\\lambda}_{j}\\right\\rangle$\n",
        "\n",
        "* $|\\tilde{\\lambda}_{j}\\rangle$ -> This register contains the eigenvalue estimates.\n",
        "\n",
        "3. Apply a one-qubit conditional rotation to perform the map\n",
        "$|0\\rangle \\rightarrow \\frac{1}{\\kappa \\tilde{\\lambda}_{j}}|0\\rangle+\\sqrt{1-\\frac{1}{\\kappa^{2} \\tilde{\\lambda}_{j}^{2}}}|1\\rangle$\n",
        "\n",
        "4. Undo step 2 - apply the inverse of phase estimation\n",
        "$\\sum_{j} \\frac{c_{j}}{\\kappa \\tilde{\\lambda}_{j}}\\left|v_{j}\\right\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle \\approx \\frac{1}{\\kappa A}|b\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle$\n",
        "\n",
        "5. Use amplitude amplification to get rid of the „bad“ part of the state with |1>\n"
      ],
      "metadata": {
        "id": "eMEyUUJGasUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=wrong-or-nonexistent-copyright-notice\n",
        "# https://github.com/quantumlib/Cirq/blob/main/examples/hhl.py\n",
        "\n",
        "\"\"\"Demonstrates the algorithm for solving linear systems by Harrow, Hassidim, Lloyd (HHL).\n",
        "\n",
        "The HHL algorithm solves a system of linear equations, specifically equations of the form Ax = b,\n",
        "where A is a Hermitian matrix, b is a known vector, and x is the unknown vector. To solve on a\n",
        "quantum system, b must be rescaled to have magnitude 1, and the equation becomes:\n",
        "\n",
        "|x> = A**-1 |b> / || A**-1 |b> ||\n",
        "\n",
        "The algorithm uses 3 sets of qubits: a single ancilla qubit, a register (to store eigenvalues of\n",
        "A), and memory qubits (to store |b> and |x>). The following are performed in order:\n",
        "1) Quantum phase estimation to extract eigenvalues of A\n",
        "2) Controlled rotations of ancilla qubit\n",
        "3) Uncomputation with inverse quantum phase estimation\n",
        "\n",
        "For details about the algorithm, please refer to papers in the REFERENCE section below. The\n",
        "following description uses variables defined in the HHL paper.\n",
        "\n",
        "This example is an implementation of the HHL algorithm for arbitrary 2x2 Hermitian matrices. The\n",
        "output of the algorithm are the expectation values of Pauli observables of |x>. Note that the\n",
        "accuracy of the result depends on the following factors:\n",
        "* Register size\n",
        "* Choice of parameters C and t\n",
        "\n",
        "The result is perfect if\n",
        "* Each eigenvalue of the matrix is in the form\n",
        "\n",
        "  2π/t * k/N,\n",
        "\n",
        "  where 0≤k<N, and N=2^n, where n is the register size. In other words, k is a value that can be\n",
        "  represented exactly by the register.\n",
        "* C ≤ 2π/t * 1/N, the smallest eigenvalue that can be stored in the circuit.\n",
        "\n",
        "The result is good if the register size is large enough such that for every pair of eigenvalues,\n",
        "the ratio can be approximated by a pair of possible register values. Let s be the scaling factor\n",
        "from possible register values to eigenvalues. One way to set t is\n",
        "\n",
        "t = 2π/(sN)\n",
        "\n",
        "For arbitrary matrices, because properties of their eigenvalues are typically unknown, parameters C\n",
        "and t are fine-tuned based on their condition number.\n",
        "\n",
        "\n",
        "=== REFERENCE ===\n",
        "Harrow, Aram W. et al. Quantum algorithm for solving linear systems of\n",
        "equations (the HHL paper)\n",
        "https://arxiv.org/abs/0811.3171\n",
        "\n",
        "Coles, Eidenbenz et al. Quantum Algorithm Implementations for Beginners\n",
        "https://arxiv.org/abs/1804.03719\n",
        "\n",
        "=== CIRCUIT ===\n",
        "Example of circuit with 2 register qubits.\n",
        "\n",
        "(0, 0): ─────────────────────────Ry(θ₄)─Ry(θ₁)─Ry(θ₂)─Ry(θ₃)──────────────M──\n",
        "                     ┌──────┐    │      │      │      │ ┌───┐\n",
        "(1, 0): ─H─@─────────│      │──X─@──────@────X─@──────@─│   │─────────@─H────\n",
        "           │         │QFT^-1│    │      │      │      │ │QFT│         │\n",
        "(2, 0): ─H─┼─────@───│      │──X─@────X─@────X─@────X─@─│   │─@───────┼─H────\n",
        "           │     │   └──────┘                           └───┘ │       │\n",
        "(3, 0): ───e^iAt─e^2iAt───────────────────────────────────────e^-2iAt─e^-iAt─\n",
        "\n",
        "Note: QFT in the above diagram omits swaps, which are included implicitly by\n",
        "reversing qubit order for phase kickbacks.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import sympy\n",
        "import cirq\n",
        "\n",
        "\n",
        "class PhaseEstimation(cirq.Gate):\n",
        "    \"\"\"A gate for Quantum Phase Estimation.\n",
        "\n",
        "    The last qubit stores the eigenvector; all other qubits store the estimated phase,\n",
        "    in big-endian.\n",
        "\n",
        "    Args:\n",
        "        num_qubits: The number of qubits of the unitary.\n",
        "        unitary: The unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        yield cirq.H.on_each(*qubits[:-1])\n",
        "        yield PhaseKickback(self.num_qubits(), self.U)(*qubits)\n",
        "        yield cirq.qft(*qubits[:-1], without_reverse=True) ** -1\n",
        "\n",
        "\n",
        "class HamiltonianSimulation(cirq.EigenGate):\n",
        "    \"\"\"A gate that represents e^iAt.\n",
        "\n",
        "    This EigenGate + np.linalg.eigh() implementation is used here purely for demonstrative\n",
        "    purposes. If a large matrix is used, the circuit should implement actual Hamiltonian\n",
        "    simulation, by using the linear operators framework in Cirq, for example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, A, t, exponent=1.0):\n",
        "        cirq.EigenGate.__init__(self, exponent=exponent)\n",
        "        self.A = A\n",
        "        self.t = t\n",
        "        ws, vs = np.linalg.eigh(A)\n",
        "        self.eigen_components = []\n",
        "        for w, v in zip(ws, vs.T):\n",
        "            theta = w * t / math.pi\n",
        "            P = np.outer(v, np.conj(v))\n",
        "            self.eigen_components.append((theta, P))\n",
        "\n",
        "    def _num_qubits_(self) -> int:\n",
        "        return 1\n",
        "\n",
        "    def _with_exponent(self, exponent):\n",
        "        return HamiltonianSimulation(self.A, self.t, exponent)\n",
        "\n",
        "    def _eigen_components(self):\n",
        "        return self.eigen_components\n",
        "\n",
        "\n",
        "class PhaseKickback(cirq.Gate):\n",
        "    \"\"\"A gate for the phase kickback stage of Quantum Phase Estimation.\n",
        "\n",
        "    It consists of a series of controlled e^iAt gates with the memory qubit as the target and\n",
        "    each register qubit as the control, raised to the power of 2 based on the qubit index.\n",
        "    unitary is the unitary gate whose phases will be estimated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, unitary):\n",
        "        super(PhaseKickback, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.U = unitary\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        qubits = list(qubits)\n",
        "        memory = qubits.pop()\n",
        "        for i, qubit in enumerate(qubits):\n",
        "            yield cirq.ControlledGate(self.U ** (2**i))(qubit, memory)\n",
        "\n",
        "\n",
        "class EigenRotation(cirq.Gate):\n",
        "    \"\"\"Perform a rotation on an ancilla equivalent to division by eigenvalues of a matrix.\n",
        "\n",
        "    EigenRotation performs the set of rotation on the ancilla qubit equivalent to division on the\n",
        "    memory register by each eigenvalue of the matrix. The last qubit is the ancilla qubit; all\n",
        "    remaining qubits are the register, assumed to be big-endian.\n",
        "\n",
        "    It consists of a controlled ancilla qubit rotation for each possible value that can be\n",
        "    represented by the register. Each rotation is a Ry gate where the angle is calculated from\n",
        "    the eigenvalue corresponding to the register value, up to a normalization factor C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_qubits, C, t):\n",
        "        super(EigenRotation, self)\n",
        "        self._num_qubits = num_qubits\n",
        "        self.C = C\n",
        "        self.t = t\n",
        "        self.N = 2 ** (num_qubits - 1)\n",
        "\n",
        "    def num_qubits(self):\n",
        "        return self._num_qubits\n",
        "\n",
        "    def _decompose_(self, qubits):\n",
        "        for k in range(self.N):\n",
        "            kGate = self._ancilla_rotation(k)\n",
        "\n",
        "            # xor's 1 bits correspond to X gate positions.\n",
        "            xor = k ^ (k - 1)\n",
        "\n",
        "            for q in qubits[-2::-1]:\n",
        "                # Place X gates\n",
        "                if xor % 2 == 1:\n",
        "                    yield cirq.X(q)\n",
        "                xor >>= 1\n",
        "\n",
        "                # Build controlled ancilla rotation\n",
        "                kGate = cirq.ControlledGate(kGate)\n",
        "\n",
        "            yield kGate(*qubits)\n",
        "\n",
        "    def _ancilla_rotation(self, k):\n",
        "        if k == 0:\n",
        "            k = self.N\n",
        "        theta = 2 * math.asin(self.C * self.N * self.t / (2 * math.pi * k))\n",
        "        return cirq.ry(theta)\n",
        "\n",
        "\n",
        "def hhl_circuit(A, C, t, register_size, *input_prep_gates):\n",
        "    \"\"\"Constructs the HHL circuit.\n",
        "\n",
        "    Args:\n",
        "        A: The input Hermitian matrix.\n",
        "        C: Algorithm parameter, see above.\n",
        "        t: Algorithm parameter, see above.\n",
        "        register_size: The size of the eigenvalue register.\n",
        "        *input_prep_gates: A list of gates to be applied to |0> to generate the desired input\n",
        "            state |b>.\n",
        "\n",
        "    Returns:\n",
        "        The HHL circuit. The ancilla measurement has key 'a' and the memory measurement is in key\n",
        "        'm'.  There are two parameters in the circuit, `exponent` and `phase_exponent` corresponding\n",
        "        to a possible rotation  applied before the measurement on the memory with a\n",
        "        `cirq.PhasedXPowGate`.\n",
        "    \"\"\"\n",
        "\n",
        "    ancilla = cirq.LineQubit(0)\n",
        "    # to store eigenvalues of the matrix\n",
        "    register = [cirq.LineQubit(i + 1) for i in range(register_size)]\n",
        "    # to store input and output vectors\n",
        "    memory = cirq.LineQubit(register_size + 1)\n",
        "\n",
        "    c = cirq.Circuit()\n",
        "    hs = HamiltonianSimulation(A, t)\n",
        "    pe = PhaseEstimation(register_size + 1, hs)\n",
        "    c.append([gate(memory) for gate in input_prep_gates])\n",
        "    c.append(\n",
        "        [\n",
        "            pe(*(register + [memory])),\n",
        "            EigenRotation(register_size + 1, C, t)(*(register + [ancilla])),\n",
        "            pe(*(register + [memory])) ** -1,\n",
        "            cirq.measure(ancilla, key='a'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    c.append(\n",
        "        [\n",
        "            cirq.PhasedXPowGate(\n",
        "                exponent=sympy.Symbol('exponent'), phase_exponent=sympy.Symbol('phase_exponent')\n",
        "            )(memory),\n",
        "            cirq.measure(memory, key='m'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "def simulate(circuit):\n",
        "    simulator = cirq.Simulator()\n",
        "\n",
        "    # Cases for measuring X, Y, and Z (respectively) on the memory qubit.\n",
        "    params = [\n",
        "        {'exponent': 0.5, 'phase_exponent': -0.5},\n",
        "        {'exponent': 0.5, 'phase_exponent': 0},\n",
        "        {'exponent': 0, 'phase_exponent': 0},\n",
        "    ]\n",
        "\n",
        "    results = simulator.run_sweep(circuit, params, repetitions=5000)\n",
        "\n",
        "    for label, result in zip(('X', 'Y', 'Z'), list(results)):\n",
        "        # Only select cases where the ancilla is 1.\n",
        "        # TODO: optimize using amplitude amplification algorithm.\n",
        "        # Github issue: https://github.com/quantumlib/Cirq/issues/2216\n",
        "        expectation = 1 - 2 * np.mean(result.measurements['m'][result.measurements['a'] == 1])\n",
        "        print(f'{label} = {expectation}')\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main program loop.\n",
        "\n",
        "    Simulates HHL with matrix input, and outputs Pauli observables of the resulting qubit state |x>.\n",
        "    Expected observables are calculated from the expected solution |x>.\n",
        "    \"\"\"\n",
        "\n",
        "    # Eigendecomposition:\n",
        "    #   (4.537, [-0.971555, -0.0578339+0.229643j])\n",
        "    #   (0.349, [-0.236813, 0.237270-0.942137j])\n",
        "    # |b> = (0.64510-0.47848j, 0.35490-0.47848j)\n",
        "    # |x> = (-0.0662724-0.214548j, 0.784392-0.578192j)\n",
        "    A = np.array(\n",
        "        [\n",
        "            [4.30213466 - 6.01593490e-08j, 0.23531802 + 9.34386156e-01j],\n",
        "            [0.23531882 - 9.34388383e-01j, 0.58386534 + 6.01593489e-08j],\n",
        "        ]\n",
        "    )\n",
        "    t = 0.358166 * math.pi\n",
        "    register_size = 4\n",
        "    input_prep_gates = [cirq.rx(1.276359), cirq.rz(1.276359)]\n",
        "    expected = (0.144130, 0.413217, -0.899154)\n",
        "\n",
        "    # Set C to be the smallest eigenvalue that can be represented by the\n",
        "    # circuit.\n",
        "    C = 2 * math.pi / (2**register_size * t)\n",
        "\n",
        "    # Simulate circuit.\n",
        "    print(\"Expected observable outputs:\")\n",
        "    print(\"X =\", expected[0])\n",
        "    print(\"Y =\", expected[1])\n",
        "    print(\"Z =\", expected[2])\n",
        "    print(\"Actual: \")\n",
        "    simulate(hhl_circuit(A, C, t, register_size, *input_prep_gates))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47vFYyP7bv6S",
        "outputId": "7adf2cd0-32f6-417b-c6f7-eca01b2751c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected observable outputs:\n",
            "X = 0.14413\n",
            "Y = 0.413217\n",
            "Z = -0.899154\n",
            "Actual: \n",
            "X = 0.17136329017517138\n",
            "Y = 0.4664561957379637\n",
            "Z = -0.9179331306990881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of HHL**\n",
        "\n",
        "* Systems of linear equations arise naturally in many real-life applications in a wide range of areas, such as in the solution of Partial Differential Equations, the calibration of financial models, fluid simulation or numerical field calculation.\n",
        "\n",
        "* Used in many quantum machine learning algorithms as a building block\n",
        "\n",
        "\n",
        "* The quantum algorithm for linear systems of equations has been applied to a support vector machine, which is an optimized linear or non-linear binary classifier (https://arxiv.org/abs/1307.0471v2)\n",
        "\n",
        "* for Least-squares fitting (https://arxiv.org/abs/1204.5242)\n",
        "\n",
        "* for finite-element-methods (https://arxiv.org/abs/1512.05903) (but only for higher problems which include solutions with higher-order derivatives and large spatial dimensions. For example, problems in many-body dynamics require the solution of equations containing derivatives on orders scaling with the number of bodies, and some problems in computational finance, such as Black-Scholes models, require large spatial dimensions)"
      ],
      "metadata": {
        "id": "EPYugWRQat26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Promise**:\n",
        "\n",
        "* Solving 10,000 linear equation: a classical computer needs in best case 10,000 steps. HHL just 13. The [quantum algorithm for linear systems of equations](https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd: Provided the linear system is sparse and has a low condition number $\\kappa_{1}$ and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of $O\\left(\\log (N) \\kappa^{2}\\right)$, where $N$ is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in $O(N \\kappa)$ (or $O(N \\sqrt{\\kappa})$ for positive semidefinite matrices).\n",
        "\n",
        "* Unlike the classical solutions to the Deutsch-Jozsa and search problems, most of our classical methods for matrix manipulation do work in polynomial time. However, as data analysis becomes more and more powerful (and more and more demanding on today’s computers), the size of these matrices can make even polynomial time too long.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* solution vector is not yielded (rather it prepares a quantum state that is proportional to the solution): Actually reading out the solution vector would take O(N)time, so we can only maintain the logarithmic runtime by sampling the solution vector like ⟨x|M|x⟩, where M is a quantum-mechanical operator. Therefore, **HHL is useful mainly in applications where only samples from the solution vector are needed**.\n",
        "\n",
        "* Entries of matrix have to be sparse: Additionally, although HHL is exponentially faster than Conjugate Gradient in N, it is polynomially slower in s and 𝜅, so HHL is restricted to only those matrices that are sparse and have low condition numbers.\n",
        "\n",
        "* Must satisfy robust invertibility (means that entries of matrix must all approx. of same size)\n",
        "\n",
        "* Preparation of input vector is complicated\n"
      ],
      "metadata": {
        "id": "rIe_VZCQavl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Grover Search*"
      ],
      "metadata": {
        "id": "2VU6BC9nYCGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://web.archive.org/web/20171221161408/http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm\n",
        "\n",
        "https://www.lesswrong.com/posts/5vZD32EynD9n94dhr/configurations-and-amplitude"
      ],
      "metadata": {
        "id": "pLwQheEKWqG1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgjkcQ3gZhD"
      },
      "source": [
        "> **[Grover’s algorithm](https://en.m.wikipedia.org/wiki/Grover's_algorithm) teaches us how we can search for an item in an unsorted list without needing to look at each item one by one but by looking at them all at once.**\n",
        "\n",
        "It accomplishes that using two techniques:\n",
        "\n",
        "  * First, it uses a quantum oracle to mark the searched state.\n",
        "\n",
        "  * Second, it uses a diffuser that amplifies the amplitude of the marked state to increase its measurement probability.\n",
        "\n",
        "Grover's Algorithm : Suche in grossen Datenbanken (Squared speedup: get result in the square root of time that on classical computers)\n",
        "\n",
        "* Auf einem klassischen Computer ist der prinzipiell schnellstmögliche Suchalgorithmus in einer unsortierten Datenbank die [lineare Suche](https://de.m.wikipedia.org/wiki/Lineare_Suche), die ${\\mathcal {O}}\\left(N\\right)$ Rechenschritte erfordert (Der Suchaufwand wächst linear mit der Anzahl der Elemente in der Liste.)\n",
        "\n",
        "* Die effizientere [Binäre Suche](https://de.m.wikipedia.org/wiki/Binäre_Suche) kann nur bei geordneten Listen benutzt werden. Die Binäre Suche ist deutlich schneller als die lineare Suche, welche allerdings den Vorteil hat, auch in unsortierten Feldern zu funktionieren. In Spezialfällen kann die [Interpolationssuche](https://de.m.wikipedia.org/wiki/Interpolationssuche) schneller sein als die binäre Suche.\n",
        "\n",
        "* Makes use of Amplitude Amplification, Quantum Walk & Quantum Counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBmtO9AqRfgC"
      },
      "source": [
        "> Grover's Algorithm uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states (=Phase Flip of Desired Outcome + Probability Amplitudes Inversion about the Mean to Amplify)\n",
        "\n",
        "Grover’s algorithm solves oracles that **add a negative phase to the solution states**. I.e. for any state |x⟩ in the computational basis:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=\\left\\{\\begin{aligned}|x\\rangle & \\text { if } x \\neq \\omega \\\\-|x\\rangle & \\text { if } x=\\omega \\end{aligned}\\right.$\n",
        "\n",
        "We create a function $f$ that takes a proposed solution $x$, and returns\n",
        "\n",
        "* $f(x)=0$ if $x$ is not a solution ( $x \\neq \\omega)$\n",
        "\n",
        "* $f(x)=1$ for a valid solution $(x=\\omega)$.\n",
        "\n",
        "The oracle can then be described as:\n",
        "\n",
        "> $U_{\\omega}|x\\rangle=(-1)^{f(x)}|x\\rangle$\n",
        "\n",
        "* you can see this is an Eigenvalue equation\n",
        "\n",
        "The oracle's matrix will be a diagonal matrix of the form:\n",
        "\n",
        "> $U_{\\omega}=\\left[\\begin{array}{cccc}(-1)^{f(0)} & 0 & \\cdots & 0 \\\\ 0 & (-1)^{f(1)} & \\cdots & 0 \\\\ \\vdots & 0 & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & (-1)^{f\\left(2^{n}-1\\right)}\\end{array}\\right]$\n",
        "\n",
        "*Source: https://qiskit.org/textbook/ch-algorithms/grover.html*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEw1xzY4XrR"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_105.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9OKn5QL4ZEt"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_106.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9O3gEWD4ayN"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_107.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJDLETO4cKX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_108.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrowEYzZ4Ptk"
      },
      "source": [
        "*Step 1: Hadamard-Operator for Superposition + Assign a Phase -1 to the desired outcome*\n",
        "\n",
        "Start with a balanced superposition, and assign a phase of -1 to the chosen ket, 111). Assigning -1 means applying a Pauli-Z-operator in the superposition to this one !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_102.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_101.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuU4yrjT4NB"
      },
      "source": [
        "**<font color=\"blue\">More about Amplitude Amplification**\n",
        "\n",
        "* [Amplitude amplification](https://en.m.wikipedia.org/wiki/Amplitude_amplification) is a technique in quantum computing which **generalizes the idea behind the Grover's search algorithm**, and gives rise to a family of quantum algorithms.\n",
        "\n",
        "* In a quantum computer, amplitude amplification can be used to **obtain a quadratic speedup over several classical algorithms**.\n",
        "\n",
        "1. If there are $G$ good entries in the database in total, then we can find them by initializing a quantum register $|\\psi\\rangle$ with $n$ qubits where $2^{n}=N$ into a uniform superposition of all the database elements $N$ such that\n",
        "\n",
        ">$| \\psi \\rangle=\\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1}|k\\rangle\n",
        "$\n",
        "\n",
        "2. and running the above algorithm. In this case the overlap of the initial state with the good subspace is equal to the square root of the frequency of the good entries in the database, $\\sin (\\theta)=|P| \\psi\\rangle \\mid=\\sqrt{G / N}$. If $\\sin (\\theta) \\ll 1$,\n",
        "\n",
        "3. we can\n",
        "approximate the number of required iterations as\n",
        "\n",
        ">$\n",
        "n=\\left\\lfloor\\frac{\\pi}{4 \\theta}\\right\\rfloor \\approx\\left\\lfloor\\frac{\\pi}{4 \\sin (\\theta)}\\right\\rfloor=\\left\\lfloor\\frac{\\pi}{4} \\sqrt{\\frac{N}{G}}\\right\\rfloor=O(\\sqrt{N})\n",
        "$\n",
        "\n",
        "Measuring the state will now give one of the good entries with high probability.\n",
        "\n",
        "Since each application of $S_{P}$ requires a single oracle query (assuming that the oracle is implemented as a quantum gate), we can find a good entry with just $O(\\sqrt{N})$ oracle queries, thus obtaining a quadratic speedup over the best possible classical algorithm. (The classical method for searching the database would be to perform the query for every $e \\in\\{0,1, \\ldots, N-1\\}$ until a solution is found, thus costing $O(N)$ queries.) Moreover, we can find all $G$ solutions using $O(\\sqrt{G N})$ queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoynUeh6XX0t"
      },
      "source": [
        "* Now, let’s say four qubits are enough and Mr. Grover is known as |0010⟩. The oracle uses the specific characteristic of this state to identifying it. That is the state has a |1⟩ at the third position and |0⟩ otherwise.\n",
        "\n",
        "* Since the quantum oracle takes all qubits as input, it can easily apply a transformation of this exact state. It doesn’t matter whether we use four qubits or 33. The oracle identifies Mr. Grover in a single turn.\n",
        "\n",
        "* The transformation the oracle applies to the searched state is an inversion of the amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_109.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_110.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkLvP9cX-_N"
      },
      "source": [
        "* In the representation above representation (the dark one) of the amplitudes, we can clearly see a difference between the searched state and all the other states. We could prematurely declare the search is over.\n",
        "\n",
        "* The only difference is in the sign of the amplitude. For the measurement probability results from the amplitude’ absolute square, the sign does not matter at all.\n",
        "\n",
        "* The amplitude originates from the concept that every quantum entity may be described not only as a particle but also as a wave. The main characteristic of a wave is that it goes up and down as it moves. The amplitude is the distance between the center and the crest of the wave.\n",
        "\n",
        "* If we invert the amplitude of a wave at all positions, the result is the same wave shifted by half of its wavelength.\n",
        "\n",
        "* These two waves differ only in their relative position. This is the phase of the wave. For the outside world, the phase of a wave is not observable. Observed individually, the two waves appear identical. So, the problem is we can’t tell the difference between these two waves.\n",
        "\n",
        "> As a consequence, the system does not appear any different from the outside. Even though the oracle marked the searched state and it, therefore, differs from the other states, all states still have the same measurement probability.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_111.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XWXZNDvSt2X"
      },
      "source": [
        "*Step 2: Invert all probability amplitudes about the mean + Measure*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_103.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WSsCfvzaSx-"
      },
      "source": [
        "* We need to turn the difference into something measurable. We need to increase the measurement probability of the marked state. This is the task of the diffuser. The diffuser applies an inversion about the mean amplitude.\n",
        "\n",
        "* Let’s have a look at the average amplitude.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_112.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63cW7pbbQVL"
      },
      "source": [
        "* With four qubits, we have 16 different states. Each state has an amplitude of 1/sqrt(16)=1/4. In fact, each but one state — the searched state has this amplitude. The searched state has an amplitude of −1/4. Thus, the average is (15∗1/4−1/4)/16=0.21875.\n",
        "\n",
        "* The average is a little less than the amplitude of all states we did not mark. If we invert these amplitudes by this mean, they end up a little lower than the average at 0.1875.\n",
        "\n",
        "* For the amplitude of the marked state is negative, it is quite far away from the average. The inversion about the mean has a greater effect. It flips the amplitude from −0.25 by 2∗(0.25+0.21875) to 0.6875 (bzw: =0,21875-(-0,25-0,21875)).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_113.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZMnRRIbolE"
      },
      "source": [
        "* The inversion about the mean works well if we search for a single or a few negative amplitudes among many positive amplitudes. Then, this operation increases the negative amplitudes we know are the correct ones. And this operation decreases the positive amplitudes, we know are wrong.\n",
        "\n",
        "> This operation increases the negative amplitude by a large amount while decreasing the positive amplitudes by a small amount.\n",
        "\n",
        "* But the more states we have, the lower the overall effect will be. In our example, we calculated the new amplitude of the searched state as 0.6875. The corresponding measurement probability is 0.6875^2=0.47265625. Accordingly, we measure this system only about every other time in the state we are looking for. Otherwise, we measure it in any other case.\n",
        "\n",
        "* Of course, we could now measure the system many times and see our searched state as the most probable one. But running the algorithm so many times would give away any advantage we gained from not searching all the states.\n",
        "\n",
        "> **Instead, we repeat the algorithm. We use the same oracle to negate the amplitude of the searched state. Then we invert all the amplitudes around the mean, again**.\n",
        "\n",
        "However, we must not repeat this process too many times. There is an optimal number of times of repeating this process to get the greatest chance of measuring the correct answer.\n",
        "\n",
        "* The probability of obtaining the correct result grows until we reach about π/4*sqrt(N) with N is the number of states of the quantum system. Beyond this number, the probability of measuring the correct result decreases again.\n",
        "\n",
        "* In our example with four qubits and N=16 states, the optimum number of iterations is 3.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_104.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuIq27h5ge43"
      },
      "source": [
        "https://towardsdatascience.com/towards-understanding-grovers-search-algorithm-2cdc4e885660"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWV-r4APgANP"
      },
      "source": [
        "*Grover Search: Simple Examples (Z-Gate and I-Gate as amplifiers. with H as Diffuser)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77PyMqZgnEg"
      },
      "source": [
        "**Example 1: Z Gate to switch from 0 to 1**\n",
        "\n",
        "* Let’s say the state |1⟩ depicts the favorable state we want to find. Then, the oracle consists of the Z-gate that switches the amplitude when the corresponding qubit is in state |1⟩.\n",
        "\n",
        "* As a result, we see the amplitude changed for state |1⟩. The qubit is now in state |−⟩. Its two states |0⟩ and |1⟩ are in two different phases, now.\n",
        "\n",
        "* In other words, we flipped the amplitude of state |1⟩ from positive to negative.\n",
        "\n",
        "* Both states still have a measurement probability of 0.5. It is the task of the diffuser to magnify the amplitude to favor the searched state.\n",
        "\n",
        "* **The diffuser in a single-qubit circuit is quite simple. It is another H-gate**. This circuit results in state |1⟩ with absolute certainty.\n",
        "\n",
        "We apply an important sequence on the qubit, the HZH-circuit. This circuit is known as an identity to the NOT-gate (X-gate) that turns state |0⟩ into |1⟩ and vice versa.\n",
        "\n",
        "The following equation proves this identity.\n",
        "\n",
        "> $H Z H=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right] \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=X$\n",
        "\n",
        "Then, why would we use the HZH-sequence? If it is similar to the NOT-gate, why don’t we use that instead?\n",
        "\n",
        "<font color=\"red\">Simply put, the HZH-sequence is more flexible. It is the simplest form of Grover’s search algorithm.</font> It starts with all states being equal (the first H-gate). It applies an oracle (Z-gate).\n",
        "\n",
        "And, <font color=\"blue\">**it uses a diffuser that amplifies the amplitude of the selected state |1⟩ (the second H-gate)**.\n",
        "\n",
        "> While we could rewrite these two circuits more succinctly, the circuit identities of HZH=X and HIH=I let us use the general structure of Grover’s algorithm. Simply by changing the oracle, we can mark and amplify different states. We don’t need to come up with a new algorithm for each possible state we want to select out of a list. But we only need to find an appropriate oracle. This ability comes in handy the more states our quantum system has.\n",
        "\n",
        "> The search for one of two possible states does not even deserve to be called a search. But the general structure of Grover’s algorithm is not different from this very simple example. **It uses a phase shift to increase the amplitude of the favorable state and to decrease the amplitudes of all other states**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bMTt-MVijqL"
      },
      "source": [
        "**Quantum Counting**\n",
        "\n",
        "> **In quantum counting, we simply use the quantum phase estimation algorithm to find an eigenvalue of a Grover search iteration.**\n",
        "\n",
        "* how many solutions exist?\n",
        "\n",
        "* does there any solution exist?\n",
        "\n",
        "\n",
        "The percentage number of solutions in our search space affects the difference between $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$. For example, if there are not many solutions, $|s\\rangle$ will be very close to $\\left|s^{\\prime}\\right\\rangle$ and $\\theta$ will be very small. It turns out that the eigenvalues of the Grover iterator are $e^{\\pm i \\theta}$, and **we can extract this using quantum phase estimation (QPE) to estimate the number of solutions $(M)$**.\n",
        "\n",
        "**First we want to get $\\theta$ from measured_int. (phase estimation)**\n",
        "\n",
        "* You will remember that $\\mathrm{QPE}$ gives us a measured value $=2^{n} \\phi$ from the eigenvalue $e^{2 \\pi i \\phi}$,\n",
        "\n",
        "* so to get $\\theta$ we need to do:\n",
        "\n",
        "> $\n",
        "\\theta=\\text { value } \\times \\frac{2 \\pi}{2^{t}}\n",
        "$\n",
        "\n",
        "**Second, we calculate the inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle:$**\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\cos \\frac{\\theta}{2}\n",
        "$\n",
        "\n",
        "* And that $|s\\rangle$ (a uniform superposition of computational basis states) can be written in terms of $|\\omega\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ as:\n",
        "\n",
        "> $\n",
        "|s\\rangle=\\sqrt{\\frac{M}{N}}|\\omega\\rangle+\\sqrt{\\frac{N-M}{N}}\\left|s^{\\prime}\\right\\rangle\n",
        "$\n",
        "\n",
        "* The inner product of $|s\\rangle$ and $\\left|s^{\\prime}\\right\\rangle$ is:\n",
        "\n",
        "> $\n",
        "\\left\\langle s^{\\prime} \\mid s\\right\\rangle=\\sqrt{\\frac{N-M}{N}}=\\cos \\frac{\\theta}{2}$\n",
        "\n",
        "* From this, we can use some trigonometry and algebra to show:\n",
        "\n",
        "> $\n",
        "N \\sin ^{2} \\frac{\\theta}{2}=M\n",
        "$\n",
        "\n",
        "**Third, calculate number of solutions**\n",
        "\n",
        "* From the Grover's algorithm chapter, you will remember that a common way to create a diffusion operator, $U_{s}$, is actually to implement $-U_{s}$.\n",
        "\n",
        "* This implementation is used in the Grover iteration provided in this chapter. In a normal Grover search, this phase is global and can be ignored, but now we are controlling our Grover iterations, this phase does have an effect.\n",
        "\n",
        "* The result is that we have effectively searched for the states that are not solutions, and our quantum counting algorithm will tell us how mâny states are not solutions. To fix this, we simply calculate\n",
        "\n",
        "> $N-M$\n",
        "\n",
        "The ability to perform quantum counting efficiently is needed in order to use Grover's search algorithm (because running Grover's search algorithm requires knowing how many solutions exist). Moreover, this algorithm solves the quantum existence problem (namely, deciding whether any solution exists) as a special case."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Walks*"
      ],
      "metadata": {
        "id": "a1VNgmJMpYSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum walks in finance: https://youtu.be/mRIjRbIQyE4?si=AI9NhARYgBT9ZmC9\n",
        "\n",
        "Quantum walks in hierarchical graphs: https://youtu.be/b53G80uQ3t0?si=Nsqov_FaD2t9EbN0"
      ],
      "metadata": {
        "id": "7n_brbbCpT1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2401.14932: Super-exponential quantum advantage for finding the center of a sphere\n",
        "\n"
      ],
      "metadata": {
        "id": "XmS2tJ73lndF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doubling Efficiency of Hamiltonian Simulation via Generalized Quantum Signal Processing https://arxiv.org/abs/2401.10321\n",
        "\n",
        "Improve quantum walk operator for hamiltonian simulation"
      ],
      "metadata": {
        "id": "g8EDP_1MtfTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Algorithms**"
      ],
      "metadata": {
        "id": "fSOCv2LsgeIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum-inspired Algorithms*"
      ],
      "metadata": {
        "id": "dRx4Wa1NZjf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2024-02-classical-surpass-quantum-counterparts.amp"
      ],
      "metadata": {
        "id": "cw0SiQO4mpum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Topological Data Analysis*"
      ],
      "metadata": {
        "id": "s9R2XXEqZeYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complexity-Theoretic Limitations on Quantum Algorithms for Topological Data Analysis\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040349"
      ],
      "metadata": {
        "id": "MEICYVfID1Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [QIP2023 | Quantifying Quantum Advantage in Topological Data Analysis (Vedran Dunjko)](https://youtu.be/T8ygjc-Lpn0?si=4W1n6iEveTXSbEsD)"
      ],
      "metadata": {
        "id": "4GJyErpahBn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tandfonline.com/doi/full/10.1080/23746149.2023.2202331"
      ],
      "metadata": {
        "id": "KJyD_mprZuNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Essential Steps in the qTDA Paper**\n",
        "\n",
        "* **Connect Laplacian with TDA**:\n",
        "  * harmonic part of the spectrum (eigenvalues, multiples of zero-th eigenvalue) of the persistent Laplacian fully captures the homological persistence and can thus be used to compute the persistent Betti numbers of filtrations of simplicial complexes. (harmonic part of a spectrum is the set of frequency components whose frequencies are whole number multiples of the fundamental frequency)\n",
        "  * The eigenvalues of the combinatorial Laplacian can be used to measure the topological features of the data. For example, the smallest non-zero eigenvalue of the Laplacian is related to the number of connected components in the data. The second smallest non-zero eigenvalue is related to the number of holes in the data.\n",
        "  * Specifically the **dimension of the kernel of the combinatorial Laplacian for k simplices is the kth Betti number**. (*The kernel is a function that measures the similarity between two data points). In other words, the number of data points that are in the kernel of the Laplacian is equal to the number of holes in the data set.\n",
        "  \n",
        "* The kernel of the combinatorial Laplacian specifically captures connectedness.: The kernel (or null space) of the Laplacian captures the number of connected components in the graph. The combinatorial Laplacian is defined for a graph \\( G \\) with vertex set \\( V \\) and edge set \\( E \\) as \\( L = D - A \\), where \\( D \\) is the degree matrix and \\( A \\) is the adjacency matrix. **The number of zero eigenvalues of the combinatorial Laplacian equals the number of connected components of the graph**. The eigenvectors associated with the zero eigenvalues (which form the kernel of the Laplacian) provide **harmonic functions on the graph**.\n",
        "\n",
        "* Aber: combinatorial laplacian vs persistent laplacian: Persistent homology provides information at multiple scales, while the kernel of the combinatorial Laplacian is typically focused on the connected components of a fixed graph.)\n",
        "\n",
        "* The persistent Laplacian is a graph signal processing tool that can be used to **analyze the harmonic part of a spectrum** (=*harmonic part of a spectrum is the set of frequency components whose frequencies are whole number multiples of the fundamental frequency. These frequencies are known as harmonics. harmonic part of the spectrum is then the set of eigenvalues that remain after all the small eigenvalues have been removed*). The persistent Laplacian is calculated by **taking the Laplacian of a graph, and then iteratively removing the smallest eigenvalues of the Laplacian**.\n",
        "  * The harmonic part of the spectrum is then the set of eigenvalues that remain after all the small eigenvalues have been removed\n",
        "  * this is why we were using the **Chebyshev polynomial to optimally filter the zero eigenvalues** (spectral projector that distinguishes the zero eigenvalue from the remaining nonzero eigenvalues of the Dirac operator)\n",
        "  * ps: spectral projector is a projection operator that projects onto the eigenspace of a self-adjoint operator corresponding to a particular eigenvalue because if you project onto the eigenspace of a matrix corresponding to a particular eigenvalue, you can develop efficient algorithms for solving systems of linear equations, or if you project onto the eigenspace of the Laplacian corresponding to a particular eigenvalue, you can study the behavior of solutions to the differential equation in the long-time limit."
      ],
      "metadata": {
        "id": "C5OhzjViVWHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel and Spectral Gap**\n",
        "\n",
        "* there is a connection between kernel and spectral gap. The spectral gap is a measure of how well-separated the eigenvalues of a matrix are. A large spectral gap means that the eigenvalues are well-separated, which means that the matrix is more stable and easier to invert.\n",
        "\n",
        "* The kernel of a matrix is the set of all vectors that are orthogonal to all the eigenvectors of the matrix with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the matrix.\n",
        "\n",
        "* The spectral gap of a matrix is related to the kernel of the matrix in the following way: **the larger the spectral gap, the smaller the dimension of the kernel**. This is because if the spectral gap is large, then the eigenvalues of the matrix are well-separated, which means that there are fewer vectors that are orthogonal to all the eigenvectors with eigenvalue 0."
      ],
      "metadata": {
        "id": "tTqWqxscIG1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Eigenvalue in TDA:\n",
        "* „In ordinary TDA applications one is typically concerned with the computation of the exact count of zero eigenvalues of combinatorial Laplacians“\n",
        "* = number of connected compontents (you cannot break it up any smaller).\n",
        "* „deciding if a combinatorial Laplacian has a trivial or non-trivial kernel (i.e., Betti number zero or non-zero)“\n",
        "\n",
        "Cheeger’s inequality demonstrates that the magnitudes of the small non-zero eigenvalues of the graph Laplacian characterises the connectedness of the graph [20], and similar results hold for combinatorial Laplacians.\n",
        "\n",
        "Higher Eigenvalues and why filter out first Eigenvalue:\n",
        "* Compute spectral gap, large gap = dense graph connectivity\n",
        "* Filtrations in Persistent Homology: In TDA, one often constructs filtrations, which are sequences of growing simplicial complexes. Monitoring how the first non-zero eigenvalue changes throughout a filtration can give insights into how the topological features of the data evolve across scales.\n",
        "* Robustness and Sensitivity: The magnitude of the first non-zero eigenvalue can indicate the robustness or sensitivity of the data's topological features. Smaller values might suggest features that are more sensitive to noise or perturbations.\n",
        "* Dimensionality Reduction: Techniques like Laplacian eigenmaps use the first few eigenvalues and their associated eigenvectors for dimensionality reduction. The first non-zero eigenvalue plays a critical role in capturing the most significant variance or structure in the data in such methods.\n",
        "* Clustering and Segmentation: The first non-zero eigenvalue and its corresponding eigenvector can be useful in clustering or segmenting data. Methods like spectral clustering leverage this property.\n"
      ],
      "metadata": {
        "id": "ziII-FeODo4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hodge theory](https://de.m.wikipedia.org/wiki/Hodge-Zerlegung): combinatorial laplacian (linear operator) which is a generalization of graph laplacian\n",
        "* The key idea in Hodge theory is that the cohomology groups of a smooth manifold **can be decomposed into a direct sum of spaces of harmonic forms.**\n",
        "* A harmonic form is a differential form that satisfies the Laplace equation. The Laplace equation is a partial differential equation that arises naturally from the study of differential forms.\n",
        "* This theorem has many important consequences, such as the Hodge theorem, which states that the integral of a harmonic form over a closed manifold is zero."
      ],
      "metadata": {
        "id": "n4L44J66OJQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Homology vs Hohomolgy*\n",
        "\n",
        "Homology and cohomology are two closely related concepts in mathematics that are used to study the topology of a space. They are both defined using chain complexes, but they differ in the way that the chain complexes are constructed.\n",
        "\n",
        "In homology, the chain complex is constructed using simplices, which are geometric objects that can be thought of as building blocks for a space. The cochain complex is constructed using cochains, which are functions on the simplices.\n",
        "\n",
        "**The homology groups of a space are the groups of equivalence classes of cycles in the chain complex**. A cycle is a collection of simplices that can be assembled to form a closed loop. The cohomology groups of a space are the groups of equivalence classes of cocycles in the cochain complex. A cocycle is a function on the simplices that satisfies certain conditions.\n",
        "\n",
        "The main difference between homology and cohomology is that homology groups are contravariant functors, while cohomology groups are covariant functors. This means that if you take the opposite of a space, the homology groups of the opposite space are the same as the cohomology groups of the original space.\n",
        "\n",
        "Intuitively, homology can be thought of as a way of counting the number of holes in a space, while cohomology can be thought of as a way of measuring the amount of \"stuff\" in a space.\n",
        "\n",
        "Here is a table that summarizes the key differences between homology and cohomology:\n",
        "\n",
        "| Feature | Homology | Cohomology |\n",
        "|---|---|---|\n",
        "| Construction | Simplicial chain complex | Cochain complex |\n",
        "| Equivalence classes | Cycles | Cocycles |\n",
        "| Functor type | Contravariant | Covariant |\n",
        "| Interpretation | Holes in a space | Stuff in a space |\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "vUEI23JLMNli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantifying Quantum Advantage in Topological Data Analysis*\n",
        "\n",
        "https://arxiv.org/abs/2209.13581\n",
        "\n",
        "QIP2023 | Quantifying Quantum Advantage in Topological Data Analysis (Vedran Dunjko)\n",
        "https://youtu.be/T8ygjc-Lpn0\n",
        "\n",
        "What means in the context of quantum computing: „Block encoding & qubitization instead of time evolution. Use filtering of the zero eigenvalue rather than phase estimation.“\n",
        "\n",
        "To understand this, let's first break down some key concepts in quantum computing:\n",
        "\n",
        "1. **Block Encoding**: In quantum computing, block encoding is a method of representing a matrix in a larger, unitary matrix so that the original matrix becomes a sub-block of this larger matrix. This technique is used to allow us to perform certain operations more easily.\n",
        "\n",
        "2. **Qubitization**: Qubitization is a quantum algorithm technique, which uses the block-encoding of a matrix to perform a quantum walk on it. The method uses significantly fewer resources than other quantum algorithms for similar problems.\n",
        "\n",
        "3. **Time Evolution**: Time evolution in quantum computing typically refers to how a quantum state changes over time according to the Schrödinger equation. It is often used to simulate physical systems in quantum simulations.\n",
        "\n",
        "4. **Phase Estimation**: Quantum Phase Estimation (QPE) is a fundamental algorithm in quantum computing which estimates the phase (or eigenvalue) of an eigenvector of a unitary operator. It's a key component in many quantum algorithms, such as Shor's algorithm and quantum simulation algorithms.\n",
        "\n",
        "5. **Filtering the Zero Eigenvalue**: This refers to the process of isolating or eliminating states with zero eigenvalues. It can be a way of refining results or simplifying calculations in quantum algorithms.\n",
        "\n",
        "Now, to the context you provided: \"Block encoding & qubitization instead of time evolution. Use filtering of the zero eigenvalue rather than phase estimation.\"\n",
        "\n",
        "This essentially suggests a strategy for tackling a quantum computing task. Instead of using the time evolution operator (a more traditional approach to quantum simulations), this is advocating the use of block encoding and qubitization. These methods can provide a more resource-efficient approach for certain types of quantum computations.\n",
        "\n",
        "Similarly, \"using filtering of the zero eigenvalue rather than phase estimation\" suggests a preferential way of isolating the states you're interested in. While phase estimation is a powerful tool, it can be resource-intensive. Filtering the zero eigenvalue might be a more efficient approach in certain contexts, particularly when zero eigenvalues represent \"unwanted\" states that might complicate or slow the computation."
      ],
      "metadata": {
        "id": "nQipY_1efAhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: how can you use the Chebyshev polynomial to optimally filter the zero eigenvalues?**\n",
        "\n",
        "The Chebyshev polynomials are a family of orthogonal polynomials that can be used to filter the zero eigenvalues of a matrix. The Chebyshev polynomials are defined by the following recursion:\n",
        "\n",
        "$$T_0(x) = 1$$\n",
        "$$T_1(x) = x$$\n",
        "$$T_n(x) = 2xT_{n-1}(x) - T_{n-2}(x)$$\n",
        "\n",
        "for all positive integers $n$.\n",
        "\n",
        "The Chebyshev polynomials have the property that they are orthogonal to the constant function on the interval $[-1,1]$. This means that for any two Chebyshev polynomials $T_n(x)$ and $T_m(x)$, the following inner product is zero:\n",
        "\n",
        "$$\\int_{-1}^1 T_n(x) T_m(x) \\, dx = 0$$\n",
        "\n",
        "where $n \\neq m$.\n",
        "\n",
        "This property can be used to filter the zero eigenvalues of a matrix $A$. The idea is to multiply $A$ by a Chebyshev polynomial $T_n(x)$, where $n$ is large enough so that the Chebyshev polynomial is non-zero at all eigenvalues of $A$ except for the zero eigenvalues. The resulting matrix will have all of its eigenvalues except for the zero eigenvalues scaled by $T_n(x)$. The zero eigenvalues will be filtered out because they will be multiplied by 0.\n",
        "\n",
        "The optimal value of $n$ for filtering the zero eigenvalues depends on the matrix $A$. A good choice for $n$ is the smallest integer such that $T_n(x)$ is non-zero for all eigenvalues of $A$ except for the zero eigenvalues.\n",
        "\n",
        "**The Chebyshev polynomial filter is a powerful tool for filtering the zero eigenvalues of a matrix**. It is simple to implement and it is very effective. The Chebyshev polynomial filter has been used in a variety of applications, including image processing, machine learning, and numerical analysis."
      ],
      "metadata": {
        "id": "soH4k5g-Wc_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">***Kernel of Combinatorial Laplacians and Betti numbers***"
      ],
      "metadata": {
        "id": "99lWWSlK_kig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The kernel of the combinatorial Laplacian is isomorphic to the first homology group of the simplicial complex. This means that the dimension of the kernel of the Laplacian is equal to the number of connected components of the simplicial complex."
      ],
      "metadata": {
        "id": "8TTHgppL-fMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persistent homology and the kernel of a combinatorial Laplacian** both arise in the study of topological and geometric properties of data, but they address different aspects and are based on somewhat different concepts.\n",
        "\n",
        "*Kernel of a Combinatorial Laplacian:*\n",
        "\n",
        "1. **Purpose**: The combinatorial Laplacian (or graph Laplacian) is a matrix associated with a graph that gives insights into the properties and structure of the graph. The kernel (or null space) of the Laplacian captures the number of connected components in the graph.\n",
        "\n",
        "2. **Construction**: The combinatorial Laplacian is defined for a graph \\( G \\) with vertex set \\( V \\) and edge set \\( E \\) as \\( L = D - A \\), where \\( D \\) is the degree matrix and \\( A \\) is the adjacency matrix.\n",
        "\n",
        "3. **Output**: The number of zero eigenvalues of the combinatorial Laplacian equals the number of connected components of the graph. The eigenvectors associated with the zero eigenvalues (which form the kernel of the Laplacian) provide harmonic functions on the graph.\n",
        "\n",
        "*Commonalities:*\n",
        "\n",
        "1. **Topology**: Both are tools used to study topological or geometric properties of spaces (or data). While persistent homology studies a wide range of topological features across scales, <font color=\"blue\">the kernel of the combinatorial Laplacian specifically captures connectedness.</font>\n",
        "\n",
        "2. **Graph-based**: Both can be discussed in the context of graphs or networks. Simplicial complexes in persistent homology can be viewed as higher-dimensional generalizations of graphs.\n",
        "\n",
        "*Differences:*\n",
        "\n",
        "1. **Scale**: Persistent homology provides information at multiple scales, while the kernel of the combinatorial Laplacian is typically focused on the connected components of a fixed graph.\n",
        "\n",
        "2. **Features**: While both can capture the number of connected components, persistent homology can also capture other features like loops and voids.\n",
        "\n",
        "3. **Mathematical machinery**: Persistent homology involves techniques from algebraic topology and often requires more sophisticated computational tools. The combinatorial Laplacian involves spectral graph theory and linear algebra.\n",
        "\n",
        "4. **Applications**: While both are used in data analysis, persistent homology has found a broader range of applications in studying the shape of data, while the Laplacian and its kernel are often used in graph theory, network analysis, and spectral clustering.\n",
        "\n",
        "In summary, while both persistent homology and the kernel of the combinatorial Laplacian deal with topological features of spaces or data, they focus on different aspects and are based on somewhat different mathematical concepts."
      ],
      "metadata": {
        "id": "PhPKkNu4RAQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The persistent Laplacian (persistent homology) is a generalization of the combinatorial Laplacian that can be used to compute persistent Betti numbers. Persistent Betti numbers are a way to track the birth and death of topological features in a simplicial complex as a function of a parameter. The persistent Laplacian is closely related to the kernel of the combinatorial Laplacian, and it can be used to compute persistent Betti numbers in a more efficient way.\n",
        "\n",
        "* persistent Laplacian, an extension of the standard combinatorial Laplacian to the setting of pairs: https://epubs.siam.org/doi/10.1137/21M1435471\n",
        "\n"
      ],
      "metadata": {
        "id": "_Av3fQr2-vUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Betti Numbers via Combinatorial Laplacians\n",
        "\n",
        "https://dl.acm.org/doi/pdf/10.1145/237814.237985\n",
        "\n",
        "omputing homology at present seems hard; currently algorithms require computing the [Smith normal form](https://en.m.wikipedia.org/wiki/Smith_normal_form) of the matrix. The latter can be done in polynomial time\n",
        "[KB79]), but the current polynomial required is still quite large.\n",
        "\n",
        "Normal form: https://en.m.wikipedia.org/wiki/Canonical_form"
      ],
      "metadata": {
        "id": "IXRFkIOG7lQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why is the kernel also the Betti number in topological data analysis?**\n",
        "\n",
        "* The kernel in topological data analysis is not the same as the Betti number. **The kernel is a function that measures the similarity between two data points**. The Betti number is a topological invariant that measures the number of holes in a data set.\n",
        "\n",
        "> However, there is a connection between the kernel and the Betti number. **The dimension of the kernel of the combinatorial Laplacian for k simplices is the kth Betti number. In other words, the number of data points that are in the kernel of the Laplacian is equal to the number of holes in the data set.**\n",
        "\n",
        "* This connection between the kernel and the Betti number can be used to develop topological data analysis methods that are based on kernel methods. Kernel methods are a type of machine learning method that uses kernels to measure the similarity between data points. By using the kernel of the Laplacian, we can develop kernel methods that are sensitive to topological features in data.\n",
        "\n",
        "* For example, we can use the kernel of the Laplacian to develop a kernel-based clustering algorithm that can cluster data points based on their topological features. We can also use the kernel of the Laplacian to develop a kernel-based classification algorithm that can classify data points based on their topological features.\n",
        "\n",
        "* The connection between the kernel and the Betti number is a powerful tool for topological data analysis. It allows us to use kernel methods to develop machine learning algorithms that are sensitive to topological features in data. This can be helpful for a variety of tasks, such as clustering, classification, and anomaly detection."
      ],
      "metadata": {
        "id": "HWu4pbDRVFQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the kernel of the combinatorial Laplacian?**\n",
        "\n",
        "* The kernel of the combinatorial Laplacian is the set of all vectors in the vector space that are orthogonal to all the eigenvectors of the Laplacian with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the Laplacian.\n",
        "\n",
        "* The kernel of the combinatorial Laplacian is important in topological data analysis because it is isomorphic to the homology groups of the underlying simplicial complex. This means that the kernel can be used to determine the Betti numbers of the simplicial complex, which are topological invariants that measure the number of connected components, holes, and voids in the complex.\n",
        "\n",
        "* For example, the kernel of the combinatorial Laplacian of a circle is one-dimensional, which corresponds to the fact that the circle has one Betti number, namely $\\beta_1=1$. This is because the only vector in the kernel of the Laplacian is the constant vector, which corresponds to the fact that the circle is a connected topological space.\n",
        "\n",
        "* In general, the dimension of the kernel of the combinatorial Laplacian is equal to the sum of the Betti numbers of the simplicial complex. **This means that the kernel can be used to count the number of connected components, holes, and voids in a simplicial complex.**\n",
        "\n",
        "* **The kernel of the combinatorial Laplacian can also be used to define a graph kernel, which is a similarity measure between graphs.** The graph kernel is based on the idea that two graphs are similar if their kernels are similar. This kernel has been shown to be effective for a variety of tasks in machine learning, such as graph classification and clustering.\n"
      ],
      "metadata": {
        "id": "EVXrfLEA43UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Support Vector Machine*"
      ],
      "metadata": {
        "id": "dDeahAqJZZUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@patrick.huembeli/introduction-into-quantum-support-vector-machines-727f3ccfa2b4\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_kernels_module/\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_kernel_based_training/"
      ],
      "metadata": {
        "id": "FZxoqCVVZZU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum PCA*"
      ],
      "metadata": {
        "id": "dLcMEj-iZUWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/nphys3029: Quantum principal component analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "QdfC93gVArrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\xi$ be a precision parameter. Let there be efficient quantum access to the top $k$ right singular vectors $\\bar{V}^{(k)} \\in \\mathbb{R}^{m \\times k}$ of a matrix $A=U \\Sigma V^T \\in \\mathbb{R}^{n \\times m}$, such that $\\left\\|V^{(k)}-\\vec{V}^{(k)}\\right\\| \\leq \\frac{\\xi}{\\sqrt{2}}$. Given efficient quantum access to a row $a_i$ of $A$, the quantum state $\\left|\\bar{y}_i\\right\\rangle=\\frac{1}{\\left\\|\\bar{y}_i\\right\\|} \\sum_i^k \\bar{y}_k|i\\rangle$, proportional to its projection onto the PCA space, can be created in time $\\tilde{O}\\left(\\frac{\\left\\|a_i\\right\\|}{\\left\\|\\tilde{y}_i\\right\\|}\\right)$ with probability at least $1-1 / \\operatorname{poly}(m)$ and precision $\\|\\left|y_i\\right\\rangle-\\left|\\bar{y}_i\\right\\rangle \\| \\leq \\frac{\\left\\|a_i\\right\\|}{\\left\\|\\bar{y}_i\\right\\|} \\xi$. An estimate of $\\left\\|\\bar{y}_i\\right\\|$, to relative error $\\eta$, can be computed in $\\widetilde{O}(1 / \\eta)$."
      ],
      "metadata": {
        "id": "mEBPFCERChpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions\n",
        "\n",
        "https://arxiv.org/abs/1811.00414"
      ],
      "metadata": {
        "id": "NdhZ5Ww9DKay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumalgorithms.org/dimensionality-reduction-1.html"
      ],
      "metadata": {
        "id": "mA8V1kp8437-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Neural Network*"
      ],
      "metadata": {
        "id": "nzm9UioSZQMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ddd"
      ],
      "metadata": {
        "id": "d5gl_2mFZOIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*QAOA*"
      ],
      "metadata": {
        "id": "yIEGR7ZtZKfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/UP-Zuke7IUg?si=mdlJKh-eb6sq0OOx\n",
        "\n",
        "\n",
        "https://youtu.be/RqGpnRh7rCM?si=IA5CPfisTgsDUCNj\n",
        "\n",
        "Video: [QAOA explained](https://youtu.be/tuGcPup8DjY?si=tdOyGsgvqozGIARY)\n",
        "\n",
        "Video: [QAOA for combinatorial optimization](https://youtu.be/AOKM9BkweVU?si=PrLHcdZ1wDAjYTnY)\n",
        "\n",
        "Video: [QAOA Peter Shor 2018]()https://youtu.be/HHIWUi3GmdM?si=c1hLXgeLCEkme_6R)"
      ],
      "metadata": {
        "id": "bs3zZ-DCQxps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/de/blogs/quantum-computing/citi-and-classiq-advance-quantum-solutions-for-portfolio-optimization/\n",
        "\n",
        "> From a computational point of view, the portfolio **optimization problem is NP-hard for minimization over binary or integer domains with additional constraints**. In simpler words, **the runtime to solve this problem grows exponentially as we increase the number of assets** (the size of ω). Therefore, finding ways of improving the runtime and the quality of the results is important when the number of assets is large.\n",
        "\n",
        "* QAOA is a hybrid algorithm, which means it combines both classical and quantum computation. The algorithm searches through all potential solutions using a mixer layer (which shuffles between alternative solutions) and a cost layer (which favors good ones). The circuit parameters are fine-tuned classically in an iterative manner, converging toward their optimal values with each run of QAOA.\n",
        "\n"
      ],
      "metadata": {
        "id": "iIVULjq3XGQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumai.google/cirq/experiments/qaoa/example_problems"
      ],
      "metadata": {
        "id": "585EyhGnpmt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Computers for Combinatorial Optimization**\n",
        "\n",
        "https://arxiv.org/abs/2210.14936: A super-polynomial quantum-classical separation for density modelling\n",
        "\n",
        "* An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory\n",
        "\n",
        "* It is unclear to what extent quantum algorithms can outperform classical algorithms for problems of combinatorial optimization. In this work, by resorting to computational learning theory and cryptographic notions, we give a fully constructive proof that hashtag#quantumcomputers feature a super-polynomial advantage over classical computers in approximating combinatorial hashtag#optimization problems. Specifically, by building on seminal work by Kearns and Valiant, we provide special instances that are hard for classical computers to approximate up to polynomial factors. Simultaneously, we give a quantum algorithm that can efficiently approximate the optimal solution within a polynomial factor. The quantum advantage in this work is ultimately borrowed from Shor’s quantum algorithm for factoring. We introduce an explicit and comprehensive end-to-end construction for the advantage bearing instances. For such instances, quantum computers have, in principle, the power to approximate combinatorial optimization solutions beyond the reach of classical efficient algorithms.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1702.jpg)"
      ],
      "metadata": {
        "id": "txLRsGwpWWwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorization of Optimization Problem with different orders of their objective function**\n",
        "* Einführung: [Optimization_problem](https://en.m.wikipedia.org/wiki/Optimization_problem)\n",
        "* **First order**: [Linear optimization](https://en.m.wikipedia.org/wiki/Linear_programming): Many practical problems in [operations research](https://en.m.wikipedia.org/wiki/Operations_research) can be expressed as linear programming problems. See [Simplex_algorithm](https://en.m.wikipedia.org/wiki/Simplex_algorithm)\n",
        "* **Second Order**: [Quadratic optimization (Quadratic programming)](https://en.m.wikipedia.org/wiki/Quadratic_programming): finance (for portfolio optimization), machine learning (for support vector machines), and operations research. Example: In the Markowitz model, the objective function to be minimized is the portfolio variance, which is a quadratic function of the decision variables (asset weights in the portfolio), given the covariance matrix of the returns of the assets.\n",
        "* **Higher Order**: Polynomial optimization problems or non-linear optimization problems (optimize design of a machine part, where the objective function includes terms related to the volume of material used which might be cubic if we're considering three-dimensional parts. Often require more advanced techniques, such as [interior-point methods](https://de.m.wikipedia.org/wiki/Innere-Punkte-Verfahren), branch-and-bound techniques, or heuristic methods."
      ],
      "metadata": {
        "id": "zsVPCyezZKfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optimization Algorithms*"
      ],
      "metadata": {
        "id": "cS920zuu5gBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Approximate Optimization Algorithm (QAOA)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Quantum_optimization_algorithms\n",
        "\n",
        "Farhi, Edward; Goldstone, Jeffrey; Gutmann, Sam (2014). \"A Quantum Approximate Optimization Algorithm\"\n",
        "\n",
        " arXiv:1411.4028: https://arxiv.org/abs/1411.4028"
      ],
      "metadata": {
        "id": "6BoAS_DV5laX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "5NxB9AolItWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation\n",
        "\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - clasislcayl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: glorothims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary, more: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but causes overhead increased asymptocatily like n to n^2 bits. Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practical engineer, our devices have this property. Required number of wires between qubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SxjKemSUmire"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system\n",
        "\n",
        "Imaginary time is a concept derived from quantum mechanics and statistical mechanics. It introduces the idea of replacing \"real time\" with \"imaginary time\" by a Wick rotation in the complex plane. That is, the time variable 't' is replaced with an imaginary number 'it', where 'i' is the imaginary unit.\n",
        "\n",
        "Imaginary time evolution plays a significant role in several areas of physics, including quantum field theory, statistical mechanics, and quantum computing.\n",
        "\n",
        "1. **Quantum Field Theory and Statistical Mechanics**: In these fields, the use of imaginary time is often a mathematical trick that simplifies calculations. By transforming to imaginary time, the calculations of quantum mechanics often become calculations in statistical mechanics. This is utilized in the technique called \"path integral formulation,\" where the evolution of a system in imaginary time makes the system go to its lowest energy state or the ground state.\n",
        "\n",
        "2. **Quantum Computing**: In quantum computing, the idea of imaginary time evolution can be used to design quantum algorithms for tasks such as finding the ground state of a system. This is used in the Quantum Approximate Optimization Algorithm (QAOA) and the Quantum Imaginary Time Evolution (QITE) algorithm.\n",
        "\n",
        "Remember that \"imaginary time\" is not about time in the sense that we experience it. Instead, it's a mathematical construct that physicists use to solve certain types of problems.\n",
        "\n",
        "Imaginary time is an unphysical, yet powerful, mathematical concept. It has been utilised in numerous physical domains, including quantum mechanics, statistical mechanics and cosmology. Often referred to as performing a ‘Wick rotation’\n",
        "\n",
        "https://www.nature.com/articles/s41534-019-0187-2"
      ],
      "metadata": {
        "id": "KL1eEEfd9csH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Boltzmann Machine*"
      ],
      "metadata": {
        "id": "RbnAnPCJZDh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@roysuman088/advancing-machine-learning-with-quantum-boltzmann-machines-a-new-paradigm-dcf5d3aa7e74\n",
        "\n",
        "https://arxiv.org/abs/2304.08631: Training Quantum Boltzmann Machines with the β-Variational Quantum Eigensolver\n",
        "\n",
        "https://arxiv.org/abs/1601.02036: Quantum Boltzmann Machine\n",
        "\n",
        "Short video: https://www.youtube.com/watch?v=52Ylq3aDU8o"
      ],
      "metadata": {
        "id": "4z30NyT33Sue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Annealer and Adiabatic Algorithms*"
      ],
      "metadata": {
        "id": "SfaztpziY2sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *QUBO*"
      ],
      "metadata": {
        "id": "egmKQRu3GHft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mdr-inc/quadratic-unconstrained-binary-optimization-qubo-on-dwave-chimera-graph-part-1-a7eb05e3f155"
      ],
      "metadata": {
        "id": "RiSZ_jhPGJL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Speedup in Combinatorial Optimization*"
      ],
      "metadata": {
        "id": "tnFQtxBD9KEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Speedup in Combinatorial Optimization: the status**\n",
        "\n",
        "A pivotal challenge in quantum information science's ever-evolving realm is designing quantum algorithms that outpace their classical counterparts. A recent experimental observation showcased a superlinear quantum speedup in solving the Maximum Independent Set problem on specific graph instances. This has spurred the development of a comprehensive theoretical framework to compare the performance of quantum adiabatic algorithms and classical Markov chain Monte Carlo algorithms.\n",
        "\n",
        "The Quantum-Classic Conundrum\n",
        "\n",
        "Combinatorial optimization problems, foundational to modern computer science, encompass NP-hard problems that remain elusive to efficient solutions through known algorithms. While classical combinatorial optimization algorithms focus on minimizing a cost function over bit strings, Quantum adiabatic algorithms (QAAs) serve as their quantum analogs, preparing low-energy states of a classical cost Hamiltonian through adiabatic evolution. However, the relative efficacy of QAA and classical counterparts like simulated annealing (SA) remains a topic of debate and exploration.\n",
        "\n",
        "From Theory to Application\n",
        "\n",
        "The recent experimental results, particularly the superlinear speedup observed using a programmable Rydberg atom array, have ignited a renewed interest in this domain. This study presents a theoretical framework that zeroes in on problem instances characterized by flat energy landscapes. Here, the quantum algorithm's performance hinges on the (de)localization of the low-energy eigenstates of the adiabatic Hamiltonian. When these eigenstates are delocalized and the quantum evolution is optimized, the QAA can achieve a significant speedup over classical algorithms like SA.\n",
        "\n",
        "Toward a Quantum Future\n",
        "\n",
        "Building on this foundation, the researchers introduce a modified QAA that showcases a quadratic speedup over SA on specific problem instances. This algorithm employs local Hamiltonians without a sign problem, making them more amenable to quantum Monte Carlo simulations. The study concludes by applying these insights to interpret experimental observations, identifying instances where quantum algorithms either outperform or underperform classical ones due to the localization properties of the low-energy eigenstates.\n",
        "\n",
        "In Summary\n",
        "\n",
        "This work offers a deep dive into the intricate dance between quantum and classical algorithms in the context of combinatorial optimization. Shedding light on the conditions under which quantum algorithms can achieve a speedup paves the way for further advancements in the quantum computing landscape.\n",
        "\n",
        "Link to the publication:\n",
        "\n",
        "https://arxiv.org/pdf/2306.13123.pdf\n"
      ],
      "metadata": {
        "id": "gh1J-ajfc5_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quantum annealing is a quantum computing algorithm that can be used to solve a variety of problems, including NP-hard problems.\n",
        "* However, the efficiency of quantum annealing is limited by the **exponential closing of the spectral gap with increasing system size.**\n",
        "* The spectral gap is a measure of the connectivity of a quantum system. **A large spectral gap means that the system is well-connected, while a small spectral gap means that the system is poorly connected**. The exponential closing of the spectral gap means that the spectral gap decreases exponentially with increasing system size.\n",
        "* This means that quantum annealing becomes exponentially slow for solving NP-hard problems as the system size increases. This is because **the quantum system becomes more and more poorly connected as the system size increases, and it takes longer and longer for the system to reach its ground state**.\n",
        "* There are a number of ways to address the problem of exponential closing of the spectral gap. One approach is to use a technique called adiabatic quantum computation. Adiabatic quantum computation is a variation of quantum annealing that uses a slower annealing schedule. This can help to prevent the spectral gap from closing too quickly.\n",
        "* Another approach to addressing the problem of exponential closing of the spectral gap is to use a technique called quantum error correction. Quantum error correction is a technique that can be used to protect quantum systems from noise. This can help to prevent the quantum system from becoming too poorly connected as the system size increases.\n",
        "* The problem of exponential closing of the spectral gap is a major challenge for quantum annealing. However, there are a number of promising approaches to addressing this problem. As quantum annealing technology continues to develop, it is possible that quantum annealing will become a viable solution for solving NP-hard problems.\n"
      ],
      "metadata": {
        "id": "8eSN4dg5Y2sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kerr ising model*"
      ],
      "metadata": {
        "id": "_UQPJ8Oz9iXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kerr and ising gate: non linear gates\n",
        "\n",
        "* The Kerr gate is a gate that introduces nonlinearity into a quantum system. The Kerr gate can be used to implement a variety of quantum operations, such as quantum logic gates and quantum simulations.\n",
        "\n",
        "* The quartic gate is a gate that introduces even higher-order nonlinearity into a quantum system. The quartic gate can be used to implement a variety of quantum operations, such as quantum error correction and quantum machine learning.\n",
        "\n",
        "* The cubic gate is a gate that introduces cubic nonlinearity into a quantum system. The cubic gate can be used to implement a variety of quantum operations, such as quantum cryptography and quantum teleportation."
      ],
      "metadata": {
        "id": "9nkXUBDaY2sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the Kerr ising model?**\n",
        "\n",
        "\n",
        "The Kerr Ising model (KIM) is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity. The Ising model is a classical model of interacting spins, while the Kerr nonlinearity is a quantum effect that describes the interaction of light with matter.\n",
        "\n",
        "The KIM is described by the following Hamiltonian:\n",
        "\n",
        "> $H = -\\sum_{i,j} J_{ij} \\sigma^z_i \\sigma^z_j - \\sum_i \\frac{\\alpha}{2} \\sigma^x_i^2$\n",
        "\n",
        "where $\\sigma^z_i$ and $\\sigma^x_i$ are Pauli matrices that represent the spin of the $i$th qubit, $J_{ij}$ is the Ising coupling strength between the $i$th and $j$th qubits, and $\\alpha$ is the Kerr nonlinearity coefficient.\n",
        "\n",
        "The KIM has been studied in a variety of contexts, including quantum annealing, quantum simulation, and quantum information processing. It has been shown that the KIM can be used to solve certain NP-complete problems, and it can also be used to simulate the dynamics of other quantum many-body systems.\n",
        "\n",
        "Here are some of the key features of the Kerr Ising model:\n",
        "\n",
        "* It is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity.\n",
        "* It has been shown to be useful for quantum annealing, quantum simulation, and quantum information processing.\n",
        "* It has been studied in a variety of contexts, including theoretical studies and experimental implementations.\n",
        "\n",
        "The Kerr Ising model is a promising platform for a variety of quantum applications. As our understanding of the model continues to grow, we can expect to see even more exciting applications in the future.\n",
        "\n",
        "https://iopscience.iop.org/article/10.1088/1367-2630/ab7255"
      ],
      "metadata": {
        "id": "nMEaxCxVY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *adiabatic quantum computing and quantum annealing*"
      ],
      "metadata": {
        "id": "JFVnpVvY9T1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the difference between adiabatic quantum computing and quantum annealing?**\n",
        "\n",
        "Adiabatic quantum computing (AQC) and quantum annealing (QA) are both approaches to quantum computing that use a time-dependent Hamiltonian to solve problems. However, there are some key differences between the two approaches.\n",
        "\n",
        "In AQC, the Hamiltonian is slowly varied from an initial Hamiltonian that is easy to prepare to a final Hamiltonian that encodes the solution to the problem. The system is then allowed to evolve adiabatically, meaning that it remains in its ground state throughout the evolution. This ensures that the system will end up in the ground state of the final Hamiltonian, which is the solution to the problem.\n",
        "\n",
        "In QA, the Hamiltonian is also slowly varied from an initial Hamiltonian to a final Hamiltonian. However, the final Hamiltonian does not necessarily encode the solution to the problem. Instead, the goal of QA is to find the ground state of the final Hamiltonian, which is a global minimum of the energy landscape. This can be useful for solving optimization problems, where the goal is to find the minimum value of a function.\n",
        "\n",
        "Another difference between AQC and QA is that AQC is a universal model of quantum computation, while QA is not. This means that AQC can be used to solve any problem that can be solved by a classical computer, while QA is only able to solve optimization problems.\n",
        "\n",
        "In practice, QA is often used to solve problems that are difficult for classical computers to solve. This is because the adiabatic evolution can help the system to tunnel through energy barriers that would be insurmountable for a classical computer. However, there is still some debate about whether QA is truly a quantum-mechanical phenomenon, or whether it can be explained by classical physics.\n",
        "\n",
        "Here is a table that summarizes the key differences between AQC and QA:\n",
        "\n",
        "| Feature | Adiabatic Quantum Computing | Quantum Annealing |\n",
        "|---|---|---|\n",
        "| Hamiltonian | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that encodes the solution to the problem. | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that is a global minimum of the energy landscape. |\n",
        "| Universality | Yes | No |\n",
        "| Applications | Any problem that can be solved by a classical computer | Optimization problems |\n",
        "| Quantum-mechanical phenomenon | Yes | Possibly |\n"
      ],
      "metadata": {
        "id": "rbIyjx5hY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104129"
      ],
      "metadata": {
        "id": "wRJv1ngPY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exponential Closing of Spectral Gap*"
      ],
      "metadata": {
        "id": "QnYJ0XpX82sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/math-ph/0507008: Spectral Gap and Exponential Decay of Correlations\n",
        "\n",
        "https://arxiv.org/abs/2212.13649 : Why Adiabatic Quantum Annealing is unlikely to yield speed-up"
      ],
      "metadata": {
        "id": "ZlZovX7quE9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity and Solution Finding in Quantum Annealing**\n",
        "\n",
        "Quantum annealing is a quantum computing technique used to find the minimum value of a given objective function over a defined set of possible solutions. It draws inspiration from classical annealing, a process in which a system is gradually cooled to find its lowest energy state. The hope in quantum annealing is to leverage quantum properties, such as superposition and tunneling, to more efficiently explore the solution space and find the ground state, which represents the minimum of the objective function.\n",
        "\n",
        "Connectivity plays an essential role in this process. Let's understand this with a few key points:\n",
        "\n",
        "1. **Local Minima and Global Minima**: In complex optimization problems, there are often many local minima. Finding the global minimum (or a sufficiently good approximation of it) is the main challenge. High connectivity means that the system can more easily explore the solution space, jumping from one potential solution to another and avoiding getting trapped in local minima.\n",
        "\n",
        "2. **Quantum Tunneling**: One of the primary advantages of quantum annealing over classical annealing is quantum tunneling. Tunneling allows the system to \"pass through\" barriers between states, meaning that the system can more easily escape local minima and move towards the global minimum. If the quantum system's connectivity is low, the benefit from quantum tunneling can be diminished.\n",
        "\n",
        "3. **Effective Exploration of Solution Space**: In a highly connected system, any given qubit (quantum bit) is more likely to interact with many other qubits. <font color=\"blue\">This allows for a richer exploration of the solution space. With fewer connections, the qubits are limited in their interactions, making the exploration less effective.</font>\n",
        "\n",
        "4. **Scaling Issues**: <font color=\"blue\">as the system size increases, maintaining high connectivity becomes more challenging. The difficulty in keeping high connectivity in larger systems is one reason why quantum annealing's efficiency decreases for more complex NP-hard problems</font>. The sparser the connectivity, the less able the system is to effectively leverage quantum effects.\n",
        "\n",
        "5. **Error Correction and Noise**: Highly connected quantum systems can be more susceptible to certain types of errors or noise. So, while high connectivity can aid in the exploration of solution space, it can also introduce challenges in terms of maintaining the coherence and reliability of the quantum information.\n",
        "\n",
        "In summary, high connectivity in quantum annealing is crucial for effectively exploring the solution space, leveraging quantum effects like tunneling, and ensuring the system doesn't get trapped in local minima. As system sizes increase, maintaining this connectivity becomes more challenging, which contributes to the reduced efficiency of quantum annealing for large NP-hard problems."
      ],
      "metadata": {
        "id": "IKNrTGMaY2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Does spectral gap stand in quantum annealing for difference between global and local optimum, or graph connectivity?* For graph connectivity it would be bad to have a small spectral gap, meanhwile for optimum it would be potentially good.\n",
        "\n",
        "*Spectral gap stands for graph connectivity. if that is low, then there are fewer connections in the graph, and that results that it's harder to find the global optimum*"
      ],
      "metadata": {
        "id": "7eO90_7EY2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Around the same time, it was argued that QA is exponentially slow for solving NP-hard problems due to exponential closing of the spectral gap with increasing system size*\n",
        "https://arxiv.org/pdf/2212.13649.pdf\n",
        "\n",
        "[20]  Jorg, T., Krzakala, F., Kurchan, J. & Maggs, A. Simple glass models and their quantum annealing. Physical review letters 101, 147204 (2008).\n",
        "[21]  Altshuler, B., Krovi, H. & Roland, J. Adiabatic quantum optimization fails for random in- stances of np-complete problems. arXiv preprint arXiv:0908.2782 (2009).\n",
        "\n",
        "*exponential closing of the quantum gap with increasing problem size* https://www.nature.com/articles/s41467-018-05239-9\n",
        "\n",
        "*The efficiency of Adiabatic Quantum Annealing is limited by the scaling with system size of the minimum gap that appears between the ground and first excited state in the annealing energy spectrum. In general the algorithm is unable to find the solution to an optimisation problem in polynomial time due to the presence of avoided level crossings at which the gap size closes exponentially with system size.* https://arxiv.org/pdf/2203.06779.pdf\n",
        "  * A particular problem which has been highlighted is the potential for so called perturbative crossings to form between the low energy eigenstates of the problem Hamiltonian towards the end of the anneal [11, 12]. The corresponding energy gap has been found to be exponentially small with the Hamming distance between the eigenstates, which can generally be expected to grow with the system size [11]."
      ],
      "metadata": {
        "id": "IQZSg2D4Y2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In quantum annealing, an exponential spectral gap generally presents a challenge** = energy difference between ground state and first excited state is very small. This small gap can make it harder for the quantum annealer to distinguish between the two states, especially when there's noise in the system. \"However, efficiency of quantum annealing is limited by the exponential closing of the spectral gap with increasing system size.\" Problems:\n",
        "  * In order to stay close to ground state throughout annealing process, evolution parameter \\( s \\) or \\( t \\) needs to change very slowly. This can lead to very long quenching times, making annealing process computationally inefficient.\n",
        "  * Thermal Excitations: thermal fluctuations can easily push system out of ground state because energy difference is small. This makes finding the correct solution (the ground state) less likely.\n",
        "  * Small spectral gap implies sensitivity to external noise. Small gap can cause system to be easily perturbed away from desired evolution.\n",
        "  * Techniques and strategies, like optimized annealing schedules or error-correction methods, may be needed to cope with these challenges.\n",
        "* An exponential spectral gap might mean that the first excited state is close to ground state in terms of energy, the implications for solution quality can vary. Depending on the problem, this could mean you have a nearly optimal solution, or it could mean you're still far from the best possible configuration.\n",
        "  1. **Nature of the Problem**: The importance of the energy difference can vary depending on the nature of the problem you're trying to solve. In some optimization problems, even a small energy difference might correspond to a significant difference in solution quality. In others, the difference might be negligible in practical terms.\n",
        "  2. **Many-body Systems**: Quantum annealing is often applied to complex many-body systems. In these systems, the difference in energy between the ground state and the first excited state might correspond to a complex reconfiguration of many particles or spins. So, the \"distance\" between these states in terms of system configuration might be significant even if their energy difference is small.\n",
        "  3. **Landscapes with Many Local Minima**: In complex optimization problems, there can be many local minima (sub-optimal solutions that are better than their immediate neighbors). The presence of an exponential spectral gap might indicate that the system is easily getting trapped in one of these local minima. While the first excited state might be close in energy to the ground state, there might be other states with much higher energies that are also close in terms of the system's configuration.\n",
        "  4. **Goal of Quantum Annealing**: The primary goal of quantum annealing is to find the global minimum (or a very close approximation to it). So, while the first excited state might offer a \"good\" solution, the aim is typically to find the \"best\" solution, especially in problems where small differences can have amplified outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDrzyqJlY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Scott Aaronson - Dismantling quantum hype](https://youtu.be/qs0D9sdbKPU)"
      ],
      "metadata": {
        "id": "A8U-2gdBY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@quantum_wa/quantum-annealing-cdb129e96601\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Annealing_(materials_science)"
      ],
      "metadata": {
        "id": "rneoSBMUY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Gap (and and Combinatorial (Graph) Laplacians)**\n",
        "\n",
        "* Spectral gap = **measure of connectivity of a quantum system** and **difference between second smallest and smallest eigenvalues** of normalized Laplacian matrix of graph (smallest eigenvalue is always zero for connected graphs)\n",
        "\n",
        "* Eigenvalues: $\\lambda_0 \\leq \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_{n-1}$, $n$ is number of nodes (vertices) in graph. Spectral gap: $ \\lambda_1$ = smallest non-zero eigenvalue.\n",
        "\n",
        "* Graph with large spectral gap $\\lambda_1 >> 0$ is well-connected. Graph with a small spectral gap is poorly connected.\n",
        "  * It is related to the graph's **Cheeger constant**: The Cheeger constant is a measure of the graph's connectivity that is closely related to the spectral gap.\n",
        "  * It is related to the **graph's mixing time**: The mixing time is the time it takes for a random walk on the graph to converge to its **stationary distribution**. A graph with a **large spectral gap has a short mixing time**, while a graph with a small spectral gap has a long mixing time (relevant in quantum annealing). Similar: **Markov Chains**: In the study of Markov chains, the spectral gap is related to the rate of convergence to the stationary distribution. A larger spectral gap typically means faster convergence.\n",
        "  * Cluster analysis: The spectral gap can be used to find clusters in graphs.\n",
        "  * Graph partitioning: The spectral gap can be used to partition graphs into two pieces with similar properties.\n",
        "  * Random walks: The spectral gap can be used to analyze the behavior of random walks on graphs.\n",
        "  * Diffusion processes: The spectral gap can be used to analyze the behavior of diffusion processes on graphs.\n",
        "* In TDA the smallest non-zero eigenvalue of combinatorial Laplacian is related to number of connected components in the data. The second smallest non-zero eigenvalue is related to the number of holes in the data.\n",
        "  * In gene expression data analysis, the combinatorial Laplacian can be used to identify clusters of genes that are co-expressed. This can be helpful for finding genes that are involved in the same biological process.\n",
        "  * In protein interaction data analysis, the combinatorial Laplacian can be used to identify protein complexes. This can be helpful for understanding how proteins interact with each other to carry out biological functions.\n",
        "  * In image data analysis, the combinatorial Laplacian can be used to identify objects in images. This can be helpful for image segmentation and classification."
      ],
      "metadata": {
        "id": "EeisMgf-84pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Spectral Gap**\n",
        "\n",
        "* Exponential spectral gap: spectral gap decreases exponentially with respect to some parameter of system.\n",
        "As system size grows, **difference between ground state energy (lowest eigenvalue) and first excited state (next smallest eigenvalue) might decrease exponentially**. This can have important implications for the system's behavior, especially in the context of quantum phase transitions or the computational hardness of approximating certain states.\n",
        "\n",
        "* **The exact interpretation and importance of an exponential spectral gap can vary depending on the context**. In some cases, having such a gap is favorable for certain properties or behaviors, while in other contexts, it might pose challenges or signal specific types of behavior.\n",
        "\n",
        "* In spectral graph theory, an exponential spectral gap is a property of a graph that ensures that the mixing time of a random walk on the graph is exponentially small in the number of vertices. This means that the random walk will converge to its stationary distribution very quickly. Exponential spectral gaps are important for a number of applications, including:\n",
        "  * A graph has an exponential spectral gap if the second smallest eigenvalue of its normalized Laplacian matrix is exponentially small in the number of vertices. This means that the graph is very well-connected, and that the random walk will quickly spread throughout the graph.\n",
        "  * Distributed algorithms: Exponential spectral gaps can be used to design distributed algorithms that are efficient and scalable.\n",
        "  * Communication networks: Exponential spectral gaps can be used to design communication networks that are reliable and efficient.\n",
        "  * Machine learning: Exponential spectral gaps can be used to design machine learning algorithms that are robust to noise and outliers.\n",
        "  * Here are some examples of graphs with exponential spectral gaps:\n",
        "    * Expander graphs: Expander graphs are graphs that have good connectivity properties. They are often used in the design of distributed algorithms and communication networks.\n",
        "    * Ramanujan graphs: Ramanujan graphs are a special type of expander graph that has an exponential spectral gap. They are often used in the design of cryptography protocols.\n",
        "    * Random graphs: Random graphs with a large number of vertices have an exponential spectral gap with high probability. This means that they can be used to design efficient and scalable algorithms for a variety of problems."
      ],
      "metadata": {
        "id": "sMd8jEF587Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel and Spectral Gap**\n",
        "\n",
        "* there is a connection between kernel and spectral gap. The spectral gap is a measure of how well-separated the eigenvalues of a matrix are. A large spectral gap means that the eigenvalues are well-separated, which means that the matrix is more stable and easier to invert.\n",
        "\n",
        "* The kernel of a matrix is the set of all vectors that are orthogonal to all the eigenvectors of the matrix with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the matrix.\n",
        "\n",
        "* The spectral gap of a matrix is related to the kernel of the matrix in the following way: the larger the spectral gap, the smaller the dimension of the kernel. This is because if the spectral gap is large, then the eigenvalues of the matrix are well-separated, which means that there are fewer vectors that are orthogonal to all the eigenvectors with eigenvalue 0.\n",
        "\n",
        "* This connection between kernel and spectral gap has implications for machine learning. Kernel methods are a type of machine learning algorithm that use kernels to compute the similarity between two data points. The spectral gap of the kernel matrix is related to the generalization performance of kernel methods. In general, kernel methods with a larger spectral gap have better generalization performance.\n",
        "\n",
        "* In addition, the kernel of a matrix can be used to define a graph kernel. Graph kernels are a type of similarity measure between graphs. The graph kernel is based on the idea that two graphs are similar if their kernels are similar. The spectral gap of the kernel matrix is related to the quality of the graph kernel. In general, graph kernels with a larger spectral gap have better quality.\n",
        "\n",
        "* Overall, the kernel and spectral gap are two important concepts in machine learning. They are related to each other in a number of ways, and they both have implications for the performance of machine learning algorithms."
      ],
      "metadata": {
        "id": "toLEwiEj89GB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More on Spectral theory**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Laplacian_matrix\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Discrete_Laplace_operator\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_graph_theory\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_shape_analysis\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_clustering\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Laplace_operator"
      ],
      "metadata": {
        "id": "gUQX_Eay8_JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Dequantization and Quantum-Inspired Algorithms*"
      ],
      "metadata": {
        "id": "mL9gnsMGYrOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper: [Ewin Tang: Quantum Machine Learning Without Any Quantum](https://inspirehep.net/literature/2716091)"
      ],
      "metadata": {
        "id": "OuJiSsXIeITe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Dequantization*"
      ],
      "metadata": {
        "id": "GUue56lG_YaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quantum-inspired classical algorithm for recommendation systems - Ewin Tang\n",
        "\n",
        "https://arxiv.org/abs/1807.04271"
      ],
      "metadata": {
        "id": "9dbIAh0C5FjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewan Tang is known for finding a classical algorithm that solves the problem of recommendation systems as efficiently as the best-known quantum algorithm. The quantum version of the problem was solved using an algorithm called Quantum Singular Value Transformation (QSVD). The classical version, however, had no known efficient algorithm, until Tang's work.\n",
        "\n",
        "Ewan Tang's dequantization approach takes advantage of the structure of the QSVD algorithm. He noted that the QSVD operates on sparse, low-rank matrices, and developed a classical algorithm that could handle this structure efficiently.\n",
        "\n",
        "Here's a simplified overview of Tang's algorithm:\n",
        "\n",
        "1. **Input Data Conversion**: Transform the input data into a sparse, low-rank format. In the context of recommendation systems, this might mean constructing a matrix where the rows correspond to users, the columns correspond to products, and each entry reflects the user's preference for the product.\n",
        "\n",
        "2. **Sampling Procedure**: Use a randomized sampling procedure to select a small number of \"landmark\" columns (products). The selection probability for each column is proportional to its contribution to the total preference scores.\n",
        "\n",
        "3. **Low-Rank Approximation**: Construct a low-rank approximation of the original preference matrix based on the sampled columns.\n",
        "\n",
        "4. **Estimation**: Use the low-rank approximation to estimate the user's preferences for all other products.\n",
        "\n",
        "This is a simplified version of the process. The actual algorithm involves a lot of linear algebra, randomization, and complexity analysis. If you're interested in the full technical details, you can look up Ewan Tang's paper \"A quantum-inspired classical algorithm for recommendation systems\".\n",
        "\n",
        "To summarize, the dequantization process involved taking a quantum algorithm and identifying the aspects of the quantum algorithm that could be implemented classically, thereby creating a classical version of the algorithm with similar efficiency."
      ],
      "metadata": {
        "id": "BNQYlE1k42Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum-inspired Algorithms**\n",
        "\n",
        "* quantum computers still exhibit an exponential speedup over all known classical algorithms for **sparse, full-rank matrix problems**, including the quantum Fourier transform, eigenvector and eigenvalue analysis, linear systems, and others https://arxiv.org/abs/1905.10415\n",
        "* Quantum algorithms for linear algebra are a flagship application of quantum computing, partic- ularly due to their relevance in machine learning. These algorithms typically [scale polylogarithmically](https://en.wikipedia.org/wiki/Polylogarithmic_function) with dimension, which, at the time they were reported, implied an asymptotic exponential speedup compared to state-of-the-art classical methods. For this reason, significant interest has been generated in the dequantization approach that led to breakthrough quantum- inspired classical algorithms for linear algebra problems with sublinear complexity https://arxiv.org/abs/1905.10415\n",
        "\n",
        "* https://crypto.stackexchange.com/questions/48638/whats-the-difference-between-polylogarithmic-and-logarithmic\n",
        "\n",
        "* For example, matrix chain ordering can be solved in polylogarithmic time on a Parallel Random Access Machine."
      ],
      "metadata": {
        "id": "mUzaGhCi1Y26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dequantization a quantum algorithm**\n",
        "* Not all problems are amenable to quantum speed-up. Quantum algorithms are specifically efficient for problems where a superposition of solutions can be created and manipulated. Linear algebra problems, especially in high dimensions, may not easily lend themselves to such superposition-based speed-ups.\n",
        "* **Output limitations**: Even if a quantum computer can perform a computation faster, it might not be able to provide the result in a way that's useful or can be checked in a reasonable time by a classical computer. This limitation is a crucial aspect in quantum linear algebra problems. For example, **you can load a vector into a quantum state exponentially faster, but you cannot read out all the entries in less than exponential time due to the 'no-cloning' theorem of quantum mechanics.** (limits ability to fully read out the state of a quantum system: each measurement only gives you a tiny piece of the picture, and you would have to repeat the process an exponential number of times to see the full state.)\n",
        "* Tang’s algorithm builds on a classical data structure called a \"tree of singular vectors,\" which is used to approximate the product of the preference matrix with a vector that represents a new user's preferences. This approximation is good enough to recommend items in a way that's nearly as good as the quantum algorithm.\n",
        "* The approach relies on a form of algorithm called \"randomized sampling algorithms\" and works by creating a probability distribution that closely matches the one that would have been created by a quantum computer. By cleverly sampling from this distribution, Tang's algorithm can estimate the answers to the recommendation problem almost as accurately as the quantum version.\n",
        "* Tang's algorithm relies heavily on the specifics of the recommendation system problem: the user-product preference matrix is \"low rank.\" Therefore, it doesn't mean that all quantum algorithms can be dequantized in the same way.\n",
        "* https://arxiv.org/abs/1905.10415 with https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf\n",
        "* There are two ways to compute the average of many values. A first method is to add all of them together and divide the result by the total number of values. An alternative approach is to select a few values at random, then compute their average using the first method. Of course, in the second case we get only an approximation of the true average, but in many cases that is good enough and can be done in significantly less time.\n",
        "* **This is the strategy of quantum-inspired algorithms: the coefficients of the solution vector are inner products between vectors, which can be expressed as the average of a collection of numbers that depend on the entries of the vectors**. Instead of directly computing this average (which takes linear time) we select a few of the values at random and compute their average instead.\n",
        "* The question is, how many values do we need to sample to get a good approximation? It turns out that **the number of samples grows with the precision of the approximation, the rank, and the condition number of the input matrix.**\n",
        "* In practice, this estimation method is only faster than a direct calculation of the coefficients if (i) precision, rank, and condition number are small, and (ii) the input matrix has a colossal dimension.\n",
        "* In practice, it pays off to be slightly more sophisticated: we keep each number with likelihood proportional to its value, such that large numbers are kept with high probability and small numbers with low probability. This is the basis of a technique called [rejection sampling](https://en.m.wikipedia.org/wiki/Rejection_sampling), **which is employed in quantum-inspired algorithms to sample from the solution vectors**. Once the approximate singular value decomposition has been obtained and the coefficients have been estimated, it is possible to query any entry of the solution vector in sublinear time.\n",
        "* https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n",
        "* https://cstheory.stackexchange.com/questions/42338/list-of-quantum-inspired-algorithms\n",
        "* https://github.com/XanaduAI/quantum-inspired-algorithms"
      ],
      "metadata": {
        "id": "w5EmlC2n2rN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Networks*"
      ],
      "metadata": {
        "id": "vUaRXgHIgW2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://iopscience.iop.org/article/10.1088/2058-9565/aaea94\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_tn_circuits#huggins"
      ],
      "metadata": {
        "id": "TJ5Ir5fTM-DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor networks applied in quantum-inspired algorithms?** Solving Optimization Problems: Tensor networks can also be used to solve optimization problems. By mapping the problem onto a tensor network, we can use techniques like the Density Matrix Renormalization Group (DMRG) to find the optimal solution. This is particularly useful for problems where the number of variables is large, as is often the case in optimization problems."
      ],
      "metadata": {
        "id": "VyCQ6sLRpzRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**are Tensor Networks a form of quantum-inspired algorithm or not?**\n",
        "\n",
        "Tensor networks are a mathematical tool widely used in many-body quantum physics and quantum information theory. They provide a way to efficiently represent and manipulate high-dimensional tensors (or high-dimensional arrays of numbers), which frequently occur in these fields.\n",
        "\n",
        "Quantum many-body systems, which involve multiple interacting quantum particles, give rise to high-dimensional tensors. When these interactions are local, it turns out that the resulting state of the system can be represented more efficiently with a tensor network, allowing for computational tractability.\n",
        "\n",
        "However, tensor networks are not inherently quantum-inspired algorithms. They are more of a framework or a technique for representing complex multi-dimensional data structures, which can be used in the development of algorithms for quantum and classical computations.\n",
        "\n",
        "Where the quantum inspiration comes in is their application: tensor networks are extensively used to study quantum systems. For instance, they are used in algorithms for simulating quantum systems, understanding quantum entanglement, quantum error correction, and so on. They have also found applications beyond quantum physics, such as machine learning and artificial intelligence.\n",
        "\n",
        "In summary, tensor networks themselves are not quantum-inspired algorithms, but they are a critical tool in the development and application of algorithms (including quantum-inspired ones) used in quantum physics and quantum computing."
      ],
      "metadata": {
        "id": "Zn8-X-V23faS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tensor networks are mathematical constructs that are used to represent and manipulate quantum states, particularly those with many parts or \"qubits\".\n",
        "\n",
        "* In the context of quantum computing, each number in the tensor can represent an amplitude of a quantum state.\n",
        "\n",
        "* A tensor network is a particular way of organizing and manipulating these tensors. In a tensor network, each tensor is represented as a node, and the indices of the tensor are represented as edges connecting the nodes. By organizing the tensors in this way, **certain computations can be performed more efficiently, particularly those involving high-dimensional tensors**.\n",
        "\n",
        "* The real power of tensor networks comes from their ability to efficiently represent and manipulate certain types of quantum states, particularly those that exhibit some form of entanglement. **In many physical systems, and particularly in systems that are near their ground state, the quantum state of the system can be well approximated by a tensor network with a relatively simple structure**.\n",
        "\n",
        "* This makes **tensor networks a very useful tool for simulating such systems on a classical computer**.\n",
        "\n",
        "* For instance, the class of states that can be efficiently represented by a tensor network includes many states that arise in condensed matter physics and quantum chemistry, as well as some states that are used in quantum error correction and quantum computing.\n",
        "\n",
        "* One common type of tensor network is the [Matrix Product State](https://en.m.wikipedia.org/wiki/Matrix_product_state) (MPS), which is used in the [Density Matrix Renormalization Group](https://en.m.wikipedia.org/wiki/Density_matrix_renormalization_group) (DMRG) method, a powerful method for simulating one-dimensional quantum systems.\n",
        "\n",
        "  * The density matrix renormalization group (DMRG) is a numerical variational technique devised to obtain the [low-energy physics](https://en.m.wikipedia.org/wiki/Macroscopic_scale) of quantum many-body systems with high accuracy. As a variational method, DMRG is an efficient algorithm that attempts to find the lowest-energy matrix product state wavefunction of a Hamiltonian.\n",
        "\n",
        "( Other types of tensor networks include the Projected Entangled Pair State (PEPS), the Multi-scale Entanglement Renormalization Ansatz (MERA), and others."
      ],
      "metadata": {
        "id": "vtMw-E_BgcNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Variational Quantum Eigensolver*"
      ],
      "metadata": {
        "id": "pqNjjLBMYen-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Molecular Simulation with VQE](https://youtu.be/fJUzXbr5MTo?si=1JMrO5SZ0LRPjtYK)\n",
        "\n",
        "Pennylane: [A brief overview of VQE](https://pennylane.ai/qml/demos/tutorial_vqe/)\n",
        "\n",
        "Pennylane: [Accelerating VQEs with quantum natural gradient](https://pennylane.ai/qml/demos/tutorial_vqe_qng/)\n",
        "\n",
        "Science: [The Variational Quantum Eigensolver: A review of methods and best practices](https://www.sciencedirect.com/science/article/pii/S0370157322003118)"
      ],
      "metadata": {
        "id": "U17xCMOzQVGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane\n",
        "import pennylane as qml"
      ],
      "metadata": {
        "id": "qflJUgxEoaRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q\n",
        "import cirq"
      ],
      "metadata": {
        "id": "Y_exZOM0olAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from jax import numpy as np\n",
        "import jax\n",
        "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
        "jax.config.update('jax_enable_x64', True)\n",
        "\n",
        "symbols = [\"H\", \"H\"]\n",
        "coordinates = np.array([0.0, 0.0, -0.6614, 0.0, 0.0, 0.6614])"
      ],
      "metadata": {
        "id": "WL3XibcPop69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H, qubits = qml.qchem.molecular_hamiltonian(symbols, coordinates)\n",
        "print(\"Number of qubits = \", qubits)\n",
        "print(\"The Hamiltonian is \", H)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCxAGQMZoUC8",
        "outputId": "b4b0ccf4-c394-41e0-882b-1109b7f78836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of qubits =  4\n",
            "The Hamiltonian is    (-0.2427450126094144) [Z2]\n",
            "+ (-0.2427450126094144) [Z3]\n",
            "+ (-0.042072551947439224) [I0]\n",
            "+ (0.1777135822909176) [Z0]\n",
            "+ (0.1777135822909176) [Z1]\n",
            "+ (0.12293330449299361) [Z0 Z2]\n",
            "+ (0.12293330449299361) [Z1 Z3]\n",
            "+ (0.16768338855601356) [Z0 Z3]\n",
            "+ (0.16768338855601356) [Z1 Z2]\n",
            "+ (0.17059759276836803) [Z0 Z1]\n",
            "+ (0.1762766139418181) [Z2 Z3]\n",
            "+ (-0.044750084063019925) [Y0 Y1 X2 X3]\n",
            "+ (-0.044750084063019925) [X0 X1 Y2 Y3]\n",
            "+ (0.044750084063019925) [Y0 X1 X2 Y3]\n",
            "+ (0.044750084063019925) [X0 Y1 Y2 X3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = qml.device(\"lightning.qubit\", wires=qubits)"
      ],
      "metadata": {
        "id": "lNzMZ21Ro9KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "electrons = 2\n",
        "hf = qml.qchem.hf_state(electrons, qubits)\n",
        "print(hf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0XkmxHrpcA_",
        "outputId": "b47538a5-7ca1-459c-c68f-7819b145e1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev)\n",
        "def circuit(param, wires):\n",
        "    qml.BasisState(hf, wires=wires)\n",
        "    qml.DoubleExcitation(param, wires=[0, 1, 2, 3])\n",
        "    return qml.expval(H)"
      ],
      "metadata": {
        "id": "9D8Tz5KLpZ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fn(param):\n",
        "    return circuit(param, wires=range(qubits))"
      ],
      "metadata": {
        "id": "SC484qkipfCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library [optax](https://optax.readthedocs.io/en/latest/) offers different optimizers. Here we use a basic gradient-descent optimizer."
      ],
      "metadata": {
        "id": "lSRK_jEbpj85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "max_iterations = 100\n",
        "conv_tol = 1e-06\n",
        "\n",
        "opt = optax.sgd(learning_rate=0.4)"
      ],
      "metadata": {
        "id": "JIocB1FFpiNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.array(0.)\n",
        "\n",
        "# store the values of the cost function\n",
        "energy = [cost_fn(theta)]\n",
        "\n",
        "# store the values of the circuit parameter\n",
        "angle = [theta]\n",
        "\n",
        "opt_state = opt.init(theta)\n",
        "\n",
        "for n in range(max_iterations):\n",
        "\n",
        "    gradient = jax.grad(cost_fn)(theta)\n",
        "    updates, opt_state = opt.update(gradient, opt_state)\n",
        "    theta = optax.apply_updates(theta, updates)\n",
        "\n",
        "    angle.append(theta)\n",
        "    energy.append(cost_fn(theta))\n",
        "\n",
        "    conv = np.abs(energy[-1] - energy[-2])\n",
        "\n",
        "    if n % 2 == 0:\n",
        "        print(f\"Step = {n},  Energy = {energy[-1]:.8f} Ha\")\n",
        "\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "\n",
        "print(\"\\n\" f\"Final value of the ground-state energy = {energy[-1]:.8f} Ha\")\n",
        "print(\"\\n\" f\"Optimal value of the circuit parameter = {angle[-1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_-Pae1dpxIX",
        "outputId": "3125c07b-6da0-4347-8aa2-9d7e3ada3d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pennylane/math/utils.py:227: UserWarning: Contains tensors of types {'jax', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step = 0,  Energy = -1.12799983 Ha\n",
            "Step = 2,  Energy = -1.13466246 Ha\n",
            "Step = 4,  Energy = -1.13590595 Ha\n",
            "Step = 6,  Energy = -1.13613667 Ha\n",
            "Step = 8,  Energy = -1.13617944 Ha\n",
            "Step = 10,  Energy = -1.13618736 Ha\n",
            "Step = 12,  Energy = -1.13618883 Ha\n",
            "\n",
            "Final value of the ground-state energy = -1.13618883 Ha\n",
            "\n",
            "Optimal value of the circuit parameter = 0.2089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_figheight(5)\n",
        "fig.set_figwidth(12)\n",
        "\n",
        "# Full configuration interaction (FCI) energy computed classically\n",
        "E_fci = -1.136189454088\n",
        "\n",
        "# Add energy plot on column 1\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.plot(range(n + 2), energy, \"go\", ls=\"dashed\")\n",
        "ax1.plot(range(n + 2), np.full(n + 2, E_fci), color=\"red\")\n",
        "ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
        "ax1.set_ylabel(\"Energy (Hartree)\", fontsize=13)\n",
        "ax1.text(0.5, -1.1176, r\"$E_\\mathrm{HF}$\", fontsize=15)\n",
        "ax1.text(0, -1.1357, r\"$E_\\mathrm{FCI}$\", fontsize=15)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add angle plot on column 2\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(range(n + 2), angle, \"go\", ls=\"dashed\")\n",
        "ax2.set_xlabel(\"Optimization step\", fontsize=13)\n",
        "ax2.set_ylabel(\"Gate parameter $\\\\theta$ (rad)\", fontsize=13)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3, bottom=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "H3oayNtGp1RT",
        "outputId": "8c085e3c-52d8-4eea-9200-2d95c8328d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABB8AAAGbCAYAAACMITjBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC2o0lEQVR4nOzdfVzN9/8/8Me703WdSsqUSiQqub5uInM527CWMCRsbL6GzFwMG2YuJj6uhhlLroa1bGYTQuR6uRpKTOWqmpQ6UZ1U5/dHv86cdeF0unjX6XG/3bpx3u/X+/1+nM7mfc7zvC4EhUKhABERERERERFRFdEROwARERERERERaTcWH4iIiIiIiIioSrH4QERERERERERVisUHIiIiIiIiIqpSLD4QERERERERUZVi8YGIiIiIiIiIqhSLD0RERERERERUpVh8ICIiIiIiIqIqpSt2AKocBQUFSExMhFQqhSAIYschIqJaTKFQIDMzE7a2ttDR4fcU2ojvG4iIqDKU5z0Diw9aIjExEfb29mLHICIiLfLgwQPY2dmJHYOqAN83EBFRZVLnPQOLD1pCKpUCKHzRzczMRE5DRES1mUwmg729vfLeQtqH7xuIiKgylOc9A4sPWqKoy6SZmRnfRBARUaVgd3ztxfcNRERUmdR5z8CBnFSlpk2bBkEQyvyRSCSQyWRiRyUiIiIiIqIqwp4PVKWuXbsGAHj77bdRv379EttYWVnxWxciIiIiIiItxuIDVami4sOWLVvw2muviZyGiIiIiIiIxMBhF1Rl7t+/j6dPn8LS0pKFByIiIiIiojqMxQeqMlevXgUAuLq6ihuEiIiIiIiIRMXiA1WZoiEXbm5uIichIiIiIiIiMbH4QFWmqOcDiw9ERERERER1G4sPpCK/IB8RCRH48fqPiEiIQH5BvsbnKur5EBAQUOoym59//rmyvb+/P7p3717iuezs7LBgwQKVtiWdLzAwUOO8RERERERE2q4yP/OVB1e7IKXQmFBMDZuKh7KHym12ZnZYM2ANvF29y3WuzMxMxMXFQRAE+Pn5ldrunXfe0Thv69at8d1336lsa9y4scbnIyIiIiIiqm75BfmIvB+JpMwk2Eht4OngCYmOpEquVZmf+cqr1hcfkpKSsGbNGly4cAFRUVF49uwZTpw4AS8vL7WOj42NxaZNm3DhwgVcvnwZcrkc8fHxcHR0LNZ27969+O2333DhwgX8/fff6NmzJyIiIoq18/f3R3BwcKnXfPjwIRo1agQA8PLywsmTJ4u16d+/P8LCwtR6DpUhNCYUPvt8oIBCZfsj2SP47PNBiG9Iuf5j/Ouvv6BQKNCkSRNs27atktMWkkql6Nq1a5Wcm4iIiIiI6p7qLAQA1VsMqOzPfOVV64sPsbGxWL58OZydndGqVSucO3euXMefO3cOa9euhZubG1xdXZXzFJRk48aNuHTpEjp16oTU1NRS202cOBF9+vRR2aZQKPDRRx/B0dFRWXgoYmdnh6VLl6pss7W1LdfzqIj8gnxMDZta7D9CAFBAAQECpoVNw+AWg9X+H6/o99i2bdtKTEpERERERHWJNvcKqM5iQFV85iuvWl986NChA1JTU2FpaYmQkBAMHTq0XMcPGjQI6enpkEqlCAwMLLP4sGPHDjRq1Ag6Ojpwd3cvtV23bt3QrVs3lW2nT59GVlYWRo4cWay9ubk5Ro0aVa7clSnyfqTK/2D/pYACD2QPEHk/El6OXmqds2i+hzZt2pQ7T15enkZtBUGARFJ1VUkiIiIiIqo+2torQKFQIPtFNj459EmpxQAAmHhwIvIK8pBfkI8CRQFGtv73s+S+m/tw68kt5ObnQp4nR25+7r8/BbnYPmQ7BEEAACw6uQgh0SGV/pmvvGp98UEqlVboeEtLS7Xb2tvba3yd3bt3QxAEvP/++yXuz8vLQ05ODkxNTTW+hqaSMpMqtR3wb8+H8hYfzpw5Az09PY3aSiSSchUuiIiIiIioZqpJvQIAqPQK2HdzHxIzE5H1IgvZL7IL/8wr/NNEzwTfvvWt8vjR+0fjz0d/KtsUtS/pWv/1JOsJhoUMAwDo6eipFB92Xd+FA7EHSj1266Ct0JfoAwBiU2Nx/fF1tX4X5fnMV161vvhQG7x48QL79u2Dh4dHiXNJ3L59GyYmJsjNzcVrr72GDz/8EF988YXaH8IrykZqU6nt8vPzcePGDQDlLz60adMGW7ZsKbb9rbfeemXbosoeERERERFVjeoYBqHJEIEX+S+QnZcNMwMzZdvDfx9GSlYKZHIZMnIyCv+UF/5pbWyN/w34H4BX9wQHoNIrYPmZ5bicdLnEdtbG1irFhwcZDxCbGlvu30GRFvVboJFZI+hL9KFQKJSfeQY4DYCtqS30Jfol/gj497PRJ50/QXPL5lhwcsErr6fuZz5NsPhQDQ4fPozU1NQSh1w4OTmhV69eaNWqFZ4/f46QkBAsXrwYt2/fxt69e0s9p1wuh1wuVz6WyWQa5/N08ISdmR0eyR6V+D+4AAF2ZnbwdPBU63y3b99GdnY2AGDhwoWltuvatSs++ugjlW2mpqbo2LFjsbYlFWJKa0tERERERJWvqodBKBQKpOek47fbv6k1RKDx6sZ4UfACMrkMOXk56GDTAVETopTtPv79Y8Snx5d4jmaWzZTFh/L2BO/v1B8t6reAsZ4xjHSNCv/UK/zT3MBc5ZjAfoHIepFVrF1UYhTe+fHVK/9tentTicMgPu70sVqZAaCrXVd0su2ELVe2VNpnPk3UqOJDQUEBcnNz1WprYGBQa77p3r17N/T09ODr61ts39atW1Uejx49GhMmTMD333+PgICAUldzWLp0aZkf7MtDoiPBmgFr4LPPBwIElf8YiypmqwesVruiWTTfA4AyV/1wdnbWMDEREREREVUnTYdBPM99jpSsFDx+/lj5Y6pvCt+W/3428gzyxN20u0jJSkFegfrDqB9lPlJ5nCHPUHn8usPrcLJ0gpmBGcwNzFX+bGjaUNmuvD3Bl/ReonbGjrYlf1n6ZrM3K/UL4Fep7M98mqhRxYdTp06hV69earWNiYmBi4tLFSequGfPnuHXX39F//79Ub9+fbWO+fTTT/H9998jPDy81OLDnDlzMH36dOVjmUxWoTkpvF29EeIbUmIlc/WA1eWqZA4fPhzDhw/XOAsREREREdUc6syJ8OGBD1WGQbTZ1AZ/p/2NrBdZxY7pYNNBpfiQlJmEpGf/9j4w0TPB8xfPX5lr7YC18HL0gpmBGcwMzCA1UJ0PcMe7O9R6fpXdE1wdYhQDKvMznyZqVPHBxcUFQUFBarW1sam6sSiV6Zdffil1lYvSFBUR0tLSSm1jYGAAAwODCud7mberNwa3GIxPj3yKNRfWoKtdV5wee7pKq19ERERERKS5qpiD4Xnuc8SnxyMnLwcdbTuqNSdCWk6aykoJWS+ylIUHQ11DNDBpoPxxtXJVOXan904YSAzQwKQBrIytoKujC8c1jq8sBkzqNKlSPquI1StAjGJA0We+6lq+9GU1qvjQsGFD+Pv7ix2jUu3atQumpqYYNGiQ2sfExcUBAKytrasqVqkkOhIMbjEYay6sQcrzFBYeiIiIiIhqqMqYg2H7te2IfRKLuPQ4xD+NR3x6PB4/fwwAaG/THpcmXNJodbz9w/bDWM8YDUwawETPpMwh813tivf2riu9AsQoBkh0JFW2nGZZalTxoardv38fWVlZ1TZcIyUlBeHh4RgxYgSMjY2L7ZfJZMV6MCgUCixevBgA0L9//2rJ+V9u1m5wtHCEq7UrChQF0BF0quW627ZtK3Xfw4eqlday2hIRERERabtXzcGwddBWuFm7Ie5pHOLT45V/muqb4tfhvyrbL4lcUuJqDPUM68HC0AKAZqvjuTdw1+BZ/asu9QoQqxhQ3bSi+FD0Yf3mzZsAgB07duD06dMAgHnz5inb+fn54eTJk1Ao/v0fNCMjA+vWrQMAnDlzBgCwfv16WFhYwMLCApMnT1a2PXXqFE6dOgWgsLDw/Plz5bV79OiBHj16qOTau3cv8vLySh1ycfnyZYwYMQIjRoxAs2bNkJ2djf379+PMmTOYMGEC2rdvr/kvpQJeM30N8VNLnhWWiIiISiaXy/HFF19gx44dePr0KVq3bo3Fixejb9++ZR4XGhqKvXv34s8//0RycjLs7e3x9ttvY/78+bCwsCjW/sCBA1iwYAGio6PRoEEDjB07FvPnz4eurla8rSMiNagzB8O4A+NKPLaeYT2Vx74tfZGalYom9Zqgab2maGLRBE3qNVEWHgBx5kQA6lavgLpAULz8SbyWKqsLz8tPz8vLq1jxISEhAU2aNCnx2MaNGyMhIUH5eMGCBaWuMPHll19iwYIFKtu6deuGuLg4JCYmQiIp/j9IfHw8Zs2apXyzoaOjA1dXV3z44YeYMGFCuVbzkMlkMDc3R0ZGBszMzF59ABERUSl4T9HMiBEjEBISgmnTpsHZ2Rnbtm3Dn3/+iRMnTqB79+6lHmdlZQVbW1sMGTIEDg4OuH79OjZt2oSmTZvi8uXLMDIyUrY9dOgQ3nrrLXh5eWHEiBG4fv06vv32W0yYMAEbN25UOytfY6LaJ1OeiSvJV3Ap8RL+uPMHwuPDX3mMlbEVXK1clUWFpvWaokm9Jnjd/vVyrxxY1NMCQInDIEpb7YK0W3nuJ1pRfKCqexMhz5PDQLdyJ7YkIqKajR9My+/ixYvo0qULVqxYgRkzZgAAcnJy4O7ujgYNGuDs2bOlHhsREQEvLy+Vbdu3b8eYMWPw/fff44MPPlBub9myJfT09BAVFaXs6TBv3jwsWbIE0dHRag8t5WtMVDUqa/JHmVyG6JRolbkQ3gh+AycSTpTrPLu9d2NEqxHlvn5pSppjwt7MvlpWSqCaqTz3k+oZzE+1zt4be2G9whqj9o8SOwoREVGNFxISAolEggkTJii3GRoaYvz48Th37hwePHhQ6rH/LTwAwLvvvgugcGnxItHR0YiOjsaECRNUhlhMmjQJCoUCISEhlfBMiEhToTGhcFzjiF7BvfB+6PvoFdwLjmscERoTWuZxGTkZOBF/AoFnAzHi5xFosb4FzJeZo9vWbkjPSVe2a2/THnZmdhjcYjDGthmrViZ152pQl7erNxKmJuDEmBPY7b0bJ8acQPzUeBYeSC0cHEglMjc0x5OsJ4hOiRY7ChERUY135coVNG/evNi3Pp07dwYAXL16VbmUtjqSk5MBFA7JePkaANCxY0eVtra2trCzs1PuL4lcLodcLlc+lslkamchold71eSPRUMS0nPSYapvCl2dwo9hnx/7HEtPLy3xnPZm9niQ8UA598KyPssQ2C8QQGEPi6PxR6t9DgaAcyKQ5lh8oBK5WbsBAO6k3kFeQZ7yH0giIiIqLikpCTY2xb9hLNqWmJhYrvMtX74cEokEPj4+Ktd4+Zz/vU5Z11i6dGmp81YRUcWoM/njqNBRsDG1QVx6HKI+jEIH2w4AAAdzBwBAY/PG6GDbAR1sOqC9TXt0sOkAaxNrlXO9/H5coiOp9qUoiSqKnyipRPZm9jDRM8HzF89xN+0uWli1EDsSERFRjZWdna2ydHYRQ0ND5X517d69G1u3bsXMmTPh7Oyscg0ApV6nrN4Mc+bMwfTp05WPZTJZuXpiEFHpIu9HqsyBUJLsvGzEpccBAGKexCiLDyPcR8DHzQdWxlZlHV4iMZaiJKoIFh+oRIIgwMXKBZeSLiE6JZrFByIiojIYGRmpDGsokpOTo9yvjsjISIwfPx79+/fH119/XewaAEq9TlnXMDAwKLFoQUQVk/wsGbuu71Kr7ZzuczDDYwYsjSyV28wNzSt0fTGWoiTSFIsPVCo3azdcSrqEmCcxeBfvih2HiIioxrKxscGjR4+KbS8aKmFra/vKc1y7dg2DBg2Cu7s7QkJCVCaVLLpG0Tn/22shKSlJOb8EEVUdhUKBG49v4EDsARy4fQAXH11U+9h+Tv1UCg+VhXMwUG3B4gOVytXKFQA46SQREdErtG3bFidOnIBMJlOZdPLChQvK/WW5e/cuBgwYgAYNGuCPP/6AqalpidcAgKioKJVCQ2JiIh4+fKiy0gYRVa6sF1mYHT4bB2IP4F7GPZV9HW064u+nfyMjJ6PaJ38kqk241CaVqqNtR/Rp2gftGrYTOwoREVGN5uPjg/z8fGzevFm5TS6XIygoCF26dFH2VLh//z5u3bqlcmxycjL69esHHR0dHD58GNbWqpPMFWnZsiVcXFywefNm5OfnK7dv3LgRgiCoTE5JRBWTlp2GM/fPKB8b6Rrhl1u/4F7GPRjqGuLt5m9j89ub8Wj6I/w54U9sHbQVwL+TPRbh5I9E/xIUCkXx8hzVOjKZDObm5sjIyCi2zBcREVF58J6iGV9fX+zfvx8BAQFo1qwZgoODcfHiRRw7dgw9evQAAHh5eeHkyZN4+e1X27Ztce3aNcycOROtWrVSOedrr72Gvn37Kh8fPHgQgwYNQq9evTB8+HDcuHED69evx/jx41UKH6/C15jqivyCfLXnQ7iTekc5nOLM/TMwNzTHPzP+Ua4ysfv6bpjqm6JP0z4w1jMudnxoTGixyR/tzew5+SNptfLcT1h80BJ8E0FERJWF9xTN5OTkYP78+di5cyeePn2K1q1b46uvvkL//v2VbUoqPgiCUNLpAAA9e/ZERESEyrZffvkFCxcuRExMDKytreHv748vvvgCenp6amfla0x1QUnFADszO6wZsEZZDLicdBl7buzBgdgDiE2NVTm+VYNWOPj+QeVymOooT7GDSBuw+FAHVeWbiIycDAAVn42XiIhqB34w1X58jUnbhcaEwmefT7E5GIqGQYT4hsDb1Rvzj8/H4sjFAABdHV14OXphUPNBeKfFO3C0cKzu2ES1TnnuJ5xwkso04bcJ+P7y9wjsG4hPPT4VOw4RERERUZnyC/IxNWxqiZM/Fm2bFjYNg1sMhrerN+LT4zGoxSD0d+rPL9uIqhCLD1QmG9PCZb244gURERER1QaR9yNVhlqU5IHsASLvR8LL0Qs7vXdWUzKiuo2rXVCZ3KzdAAAxT2JETkJERERE9GpxT+PUapeUmVTFSYjoZSw+UJlcrV0BFPZ84PQgRERERFTTNbFoolY7G6lNFSchopex+EBlal6/OXQEHWTIM5D8LFnsOERERERESvI8ObZe3opuW7shU54JAOjRuAcamDRQTi75XwIE2JvZw9PBszqjEtV5LD5QmQx1DdG0XlMAHHpBRERERDVDRk4GvjnzDZqsaYIPfvsA5x+ex5bLWwAAEh0JNr61EQCKFSCKHq8esJpLYBJVMxYf6JVcrf4dekFEREREJJbEzETMOjoLDqsdMCt8FpKeJaGRtBEC+wbig/YfKNt5u3ojxDcEjcwaqRxvZ2anXGaTiKoXV7ugV3q7+dtoJG0E9wbuYkchIiIiojoqLTsNzdY2Q3ZeNoDCidFneszEiFYjoC/RL9be29Ubg1sMRuT9SCRlJsFGagNPB0/2eCASCYsP9EoTOkwQOwIRERER1UG3U2+jef3mAABLI0u80+IdJGUmYebrMzHQeSB0hLI7ckt0JPBy9KqGpET0Kiw+EBERERFRjVGgKMDvt3/H8jPLcfbBWcROjoVzfWcAwLbB22CkZyRyQiLSBIsPpJasF1m49eQWXK1c+Q8+EREREWkkvyC/1GEQ8jw5dl/fjRVnVygnOteX6OP8w/PK4gPfhxLVXiw+kFrcvnXDvYx7ODPuDDzsPcSOQ0RERES1TGhMKKaGTcVD2UPlNjszO3zT5xs8lD3E6gurkZiZCAAwMzDDxx0/xpQuU2ArtRUrMhFVIhYfSC0uVi64l3EP0SnRLD4QERERUbmExoTCZ58PFFCobH8ke4SRoSNhrGeM5y+ew1Zqi4CuAZjQYQLMDMxESktEVYHFB1KLq5UrDt89jJiUGLGjEBEREVEtkl+Qj6lhU4sVHgBAAQUECNCX6GPNgDUY3WZ0iStXEFHtV/b0sET/n6u1KwAg+km0yEmIiIiIqDaJvB+pMtTivxRQ4GnOUzhZOrHwQKTFWHwgtbhZuwEAez4QERERUbkkZSZVajsiqp1qffEhKSkJs2fPRq9evSCVSiEIAiIiItQ+PjY2FgEBAfDw8IChoSEEQUBCQkKxdqmpqVixYgV69OgBa2trWFhYoGvXrti7d2+J55XL5Zg1axZsbW1hZGSELl264OjRoyW2PXv2LLp37w5jY2M0bNgQU6ZMwbNnz9R+DtXB1aqw58O9jHt4nvtc5DREREREVBsoFApcSb6iVlsbqU0VpyEiMdX64kNsbCyWL1+OR48eoVWrVuU+/ty5c1i7di0yMzPh6upaZru5c+fC0tIS8+bNw9dffw1jY2MMHz4cX375ZbH2/v7+WLVqFUaOHIk1a9ZAIpFg4MCBOH36tEq7q1evonfv3sjKysKqVavwwQcfYPPmzRg6dGi5n0tVqm9cH9bG1gCAW09uiZyGiIiIiGqDKYemYMXZFWW2ESDA3sweng6e1ZSKiMQgKBSK4jO/1CKZmZl48eIFLC0tERISgqFDh+LEiRPw8vJS6/i0tDTo6elBKpUiMDAQn332GeLj4+Ho6KjSLj4+Hjo6OmjcuLFym0KhQJ8+fXDmzBmkpqbCxMQEAHDx4kV06dIFK1aswIwZMwAAOTk5cHd3R4MGDXD27FnlOQYOHIirV6/i1q1bMDMrnNF3y5Yt+PDDD3H48GH069dPrechk8lgbm6OjIwM5Xkq2/LTyyHRkWC4+3DYmdlVyTWIiEh81XFPIXHxNabqciL+BN7c9SZ8W/pi5187AUBl4kkBAgAgxDcE3q7eomQkIs2V535S63s+SKVSWFpaany8paUlpFLpK9s1adJEpfAAAIIgYMiQIZDL5YiLi1NuDwkJgUQiwYQJE5TbDA0NMX78eJw7dw4PHjwAUPhCHT16FKNGjVJ5ofz8/GBqaop9+/Zp/LyqwqzuszDDYwYLD0RERERUouwX2Tj74N8v2no16YWEaQnY/u52hPiGoJFZI5X2dmZ2LDwQ1RFcarOCkpOTAQBWVlbKbVeuXEHz5s2LVX46d+4MoHCohb29Pa5fv468vDx07NhRpZ2+vj7atm2LK1fUGx9HRERERCS2a8nXMDJ0JOLT43F14lU413cGADQ0bQgA8Hb1xuAWgxF5PxJJmUmwkdrA08ETEh2JmLGJqJqw+FABaWlp2LJlCzw9PWFj8+8EOUlJSSqPixRtS0xMVLZ7eft/20ZGRpZ6bblcDrlcrnwsk8k0exLlkFeQh1tPbuFe+j281fytKr8eEREREdV8BYoCrDq3CnOPz0Vufi5eM3kNyc+SlcWHl0l0JPBy9Kr+kEQkuhpVfCgoKEBubq5abQ0MDCAIQhUnKl1BQQFGjhyJ9PR0rFu3TmVfdnY2DAwMih1jaGio3P/yn6W1LdpfkqVLl2LhwoUa59fEP8/+QauNraAj6CDr8ywY6BbPTURERER1x4OMBxjzyxicSDgBABjcYjC+f+d7WJtYi5yMiGqaGjXnw6lTp2BkZKTWT2xsrKhZP/nkE4SFhWHLli1o06aNyj4jIyOVXglFcnJylPtf/rO0tkX7SzJnzhxkZGQof4rmkahKtlJbmBmYoUBRgDtpd6r8ekRERERUc+27uQ+tN7XGiYQTMNYzxvfvfI/9w/az8EBEJapRPR9cXFwQFBSkVtuShipUl4ULF2LDhg1YtmwZRo8eXWy/jY0NHj16VGx70TALW1tbZbuXt/+3bVG7khgYGJTYY6IqCYIAVytXXHh0ATEpMXBv4F6t1yciIiKimuNa8jWk56Sjc6PO2PnuzhKHWRARFalRxYeGDRvC399f7Bhl+vbbb7FgwQJMmzYNs2bNKrFN27ZtceLECchkMpVJJy9cuKDcDwDu7u7Q1dVFVFQUfH19le1yc3Nx9epVlW01hZu1Gy48uoDolGixoxARERFRNXuR/wJ6Ej0AwJdeX8JWaosJHSYotxERlaZGDbuoavfv38etW7c0Pn7v3r2YMmUKRo4ciVWrVpXazsfHB/n5+di8ebNym1wuR1BQELp06QJ7e3sAgLm5Ofr06YOdO3ciMzNT2XbHjh149uwZhg4dqnHWquJq5QoAiHkSI3ISIiIiIqouufm5mBM+B92DuuNF/gsAgL5EH//X+f9YeCAitdSong+aWrx4MQDg5s2bAAo/vJ8+fRoAMG/ePGU7Pz8/nDx5EgqFQrktIyNDOWHkmTNnAADr16+HhYUFLCwsMHnyZADAxYsX4efnh/r166N3797YtWuXSgYPDw80bdoUANClSxcMHToUc+bMwePHj9GsWTMEBwcjISEBW7duVTnu66+/hoeHB3r27IkJEybg4cOHWLlyJfr164cBAwZU2u+osrhaFxYf2POBiIiIqG649eQWRoaOxOWkywCA327/Bm9Xb5FTEVFtIyhe/iReS5W16sXLT8/Ly6tY8SEhIQFNmjQp8djGjRsjISEBALBt2zaMHTu21OsEBQWpDBnJycnB/PnzsXPnTjx9+hStW7fGV199hf79+xc79vTp05g1axYuX74MqVQKX19fLF26FFKptNTr/ZdMJoO5uTkyMjJUhnpUtrincXBa6wQDiQGef/6c6zITEWmh6rqnkHj4GpM6FAoFNkZtxIwjM5Cdlw1LI0t8/873LDwQkVJ57idaUXyg6nsTkV+Qj2/OfAMXKxe83fxtdrMjItJC2vrB9Pbt27h58yYeP34MQRBgbW0Nd3d3ODvXvUnytPU1psrzz7N/MO7AOPxx5w8AQN+mfbFtyDbYSkufEJ2I6p7y3E+0YtgFVR+JjgRzPOeIHYOIiEgtMTEx2LRpE0JCQpCcnAzg316RRT0nX3vtNfj6+mLixIlwdXUVLSuRGPIL8hF5PxJJmUmwkdrA08ETEh0JPvjtA/xx5w8YSAzwTd9vMLnzZOgIdWq6OCKqZCw+EBERkda5e/cuZs2ahf3798PIyAienp6YOHEinJycUL9+fSgUCqSlpeHvv//G+fPnsWXLFqxbtw7e3t5Yvny5ch4nIm0WGhOKqWFT8VD2ULnNzswOawaswap+q5CWnYbv3v6Oy6sTUaVg8YHKLeV5Cs4+OAuJjgRvN39b7DhERETFuLm5oVWrVti2bRu8vb1hYmJSZvvnz58jJCQEa9asgZubG3JycqopKZE4QmNC4bPPBwqojsB+JHsEn30+CPENwemxp8ucW42IqDzYd4rK7dS9UxiydwgWnVwkdhQiIqIS/fTTT4iKisLo0aNfWXgAABMTE4wZMwaXL1/G3r17qyEhkXjyC/IxNWxqscIDAOW2aWHTUKAoqO5oRKTFWHygcnOzdgMAxDyJAecrJSKimmjQoEEaHzt48OBKTEJU80Tej1QZavFfCijwQPYAkfcjqzEVEWk7Fh+o3JpZNoOuji6e5T4r88ZFRERERDVPUmZSpbYjIlIHiw9UbnoSPThbFi5LFvMkRuQ0RERERFQeNlKbSm1HRKQOTjhJGnG1dkXMkxjEpMSgn1M/seMQERGpeOONN8p9jCAIOHbsWBWkIapZBJQ9iaQAAXZmdvB08KymRERUF7D4QBpxtSpcBz06JVrkJERERMXFxcUVm6X/+fPnePLkCQDAwsICAJCeng4AsLKygqmpaXVGJBJNT8eeGN16NHb8tQMCBJWJJ4sKE6sHrIZERyJWRCLSQhx2QRp5edJJIiKimiYhIQHx8fHKn2PHjsHIyAhTp05FYmIi0tLSkJaWhsTEREyZMgXGxsbs9UBa7+WJwre/ux0/+/6MRmaNVNrYmdkhxDcE3q7e1R2PiLScoOByBVpBJpPB3NwcGRkZMDMzq/LrPZI9wpkHZ9CqQSu4WrtW+fWIiKj6VPc9pToMHjwYxsbG+PHHH0vcP3z4cOTk5OCXX36p3mAi0cbXmMp29O5RBJ4LxE9Df4KZwb+veX5BPiLvRyIpMwk2Uht4OniyxwMRqa089xP2fCCNNDJrBN+Wviw8EBFRrRAREYGePXuWut/LywsRERHVF4ioGp17cA5D9g7BkbtH8M2Zb1T2SXQk8HL0wohWI+Dl6MXCAxFVGRYfiIiISOsJgoCYmNKHCt68ebMa0xBVn+v/XMfA3QOR9SIL/Zz6YX6P+WJHIqI6isUH0lhUYhRWnl2JiIQIsaMQERGVqV+/fti4cSO2b9+uMu5doVAgODgY3333Hfr14+pNpF3+Tvsb/Xb2Q3pOOrrZdUOobygMdA3EjkVEdRSLD6SxvTf2YsbRGQiNCRU7ChERUZlWrVqFRo0aYezYsWjUqBF69uyJnj17olGjRhg3bhxsbW2xatUqsWMSVZpHskfou6Mvkp8lo/VrrfH7+7/DRN9E7FhEVIex+EAaK5rvgSteEBFRTWdnZ4erV69i1qxZqFevHi5evIiLFy+iXr16mDVrFq5evQo7OzuxYxJVCoVCgRE/j0BCegKaWTbD4VGHUc+ontixiKiO0xU7ANVeRcttRqdEi5yEiIjo1czNzbFkyRIsWbJE7ChEVUoQBGx4awPG/ToO+4buQ0PThmJHIiJi8YE052pV2PMhMTMRGTkZMDc0FzkREREREQGAewN3XPjgAgRBEDsKEREAFh+oAswNzWErtUViZiJinsSgq11XsSMRERGVKSoqChcuXMDTp09RUFCgsk8QBMyfz5UAqHbKK8jDuF/HYXy78ejpWLisLAsPRFSTsPhAFeJq5VpYfEhh8YGIiGqu7OxseHt748iRI1AoFBAEQbnqRdHfWXyg2qpAUYDxB8Zjx1878Nvt35AwNYE9UomoxuGEk1QhRfM+cNJJIiKqyRYtWoQjR45g7ty5OHHihHKJzUOHDsHT0xOdOnVCdDTnMKLaR6FQICAsANuvbYdEkGD7kO0sPBBRjcTiA1XI5M6TEfVhFL7s+aXYUYiIiEoVEhKCoUOHYtGiRXB3dwcANGrUCP3790d4eDhyc3Oxbds2cUMSaWDRyUVYe3EtAGDbkG14p8U7IiciIioZiw9UIc3rN0cH2w5cN5qIiGq0Bw8eoGfPwnHwEokEAJCbmwsA0NXVxYgRI7Bnzx7R8hFpYs35NVhwcgEAYN2b6zCq9ShxAxERlYHFByIiItJ6UqkUeXl5yr/r6OggMTFRud/c3BzJyclixSMqtyN3j2Da4WkAgEVeizC582RxAxERvQKLD1RhP1z5AR8f/Bh3Uu+IHYWIiKhETk5OuH37NoDCng8tW7ZESEgIgMIx86GhobC3txczIlG5eDl6YVjLYQjoGoB5PeaJHYeI6JVYfKAKC7oahE2XNiEqMUrsKERERCXq06cPfv75Z+Tn5wMAJk6ciLCwMDg5OcHZ2Rnh4eEYP368yCmJ1Kcv0ccu710I7BfIJTWJqFZg8YEqzNXKFQAQncJZwomIqGaaPXu2cpULAJg0aRICAwNhbm6OevXqYcmSJZg5c6bIKYnKdvHRRXx25DMUKAoAABIdCXQEvp0notqh1v9rlZSUhNmzZ6NXr16QSqUQBAERERFqHx8bG4uAgAB4eHjA0NAQgiAgISGhWLvU1FSsWLECPXr0gLW1NSwsLNC1a1fs3bu3WNs///wTkydPRsuWLWFiYgIHBwf4+voqu3u+zN/fH4IgFPtxcXEpz69BVFxuk4iIarLs7GyEhoYiPT0durq6yu3Tp0/H5cuX8eeff2LWrFn89phqtJuPb+LNXW8i8FwgVp1bJXYcIqJy0311k5otNjYWy5cvh7OzM1q1aoVz586V6/hz585h7dq1cHNzg6urK65evVpqu7lz52LgwIGYN28edHV18fPPP2P48OGIjo7GwoULlW2XL1+OM2fOYOjQoWjdujWSk5Oxfv16tG/fHufPn1cu8VXEwMAAW7ZsUdlmbl571mdmzwciIqrJDAwM8MEHH2Dt2rXo0qWL2HGIyi3+aTz67eyHtOw0dGnUBR91/EjsSERE5Vbriw8dOnRAamoqLC0tlWt4l8egQYOQnp4OqVSKwMDAUosPLVu2xJ07d9C4cWPltkmTJqFPnz5Yvnw5Zs6cCROTwuUmp0+fjt27d0NfX1/ZdtiwYWjVqhWWLVuGnTt3qpxbV1cXo0bV3qWRXK0Liw930u7gRf4L6En0RE5ERET0Lx0dHTg4OEAmk4kdhajckjKT0GdHHyRmJsK9gTv+GPkHTPVNxY5FRFRuGg27yMrKwpEjR/C///0Pc+bMweeff47//e9/OHr0KLKysio7Y5mkUiksLS01Pt7S0hJSqfSV7Zo0aaJSeAAAQRAwZMgQyOVyxMXFKbd7eHioFB4AwNnZGS1btkRMTMlDE/Lz82vtmyJ7M3uY6psiryAPf6f9LXYcIiKiYsaMGYMdO3ZALpdX2TXkcjlmzZoFW1tbGBkZoUuXLjh69Ogrj1N3CCgAODo6ljhc86OP+E24NsgvyEdEQgR+vP4jIhIikPI8Bf129kPc0zg0rdcUh0cdhqWR5u97iYjEVK6eD4cOHcKmTZsQFhaGvLw85aRNRQRBgK6uLt5880189NFHGDBgQKWGrYmK1gS3srIqs51CocA///yDli1bFtuXlZUFMzMzZGVloV69ehgxYgSWL18OU9PaUdUWBAEuVi6ISozCnbQ7yp4QRERENYWHhwdCQ0PRtm1bTJo0Cc7OzjA2Ni7WrkePHhpfw9/fHyEhIZg2bRqcnZ2xbds2DBw4ECdOnED37t1LPU7dIaBF2rZti08//VRlW/PmzTXOTTVDaEwopoZNxUPZQ+U2fYk+cvNzYWNqg6Ojj8JWaitiQiKiilGr+BAZGYlPP/0UUVFRcHR0xLhx49CtWzc4OTmhfv36UCgUSEtLw99//41z587h8OHDGDhwIDp27IhVq1aVecOtzdLS0rBlyxZ4enrCxsamzLa7du3Co0ePsGjRIpXtNjY2mDlzJtq3b4+CggKEhYVhw4YNuHbtGiIiIlQmxnqZXC5X+fZG7F4Te97bg/rG9WFhaCFqDiIiopL07dtX+fepU6cWm1xSoVBAEATlUpzldfHiRezZswcrVqzAjBkzAAB+fn5wd3fHzJkzcfbs2VKPVXcIaJFGjRrV6uGaVFxoTCh89vlAAdUv9nLzcwEAM1+fiab1mooRjYio0qhVfPDy8sKQIUOwcuVKeHp6ltru9ddfx5gxYwAAJ0+exOrVq+Hl5YW8vDy1whQUFCA3N1ettgYGBqLOSl1QUICRI0ciPT0d69atK7PtrVu38H//93/o1q2b8vdTZOnSpSqPhw8fjubNm2Pu3LkICQnB8OHDSzzn0qVLVSa5FJuTpZPYEYiIiEoVFBRUpecPCQmBRCLBhAkTlNsMDQ0xfvx4fP7553jw4AHs7e1LPFaT4aO5ubl48eKFcr4pqr3yC/IxNWxqscJDEQECVp1bhU86fwKJjqSa0xERVR61ig+XL19GmzZtynXinj17omfPnq+s3r/s1KlT6NWrl1ptY2JiRF2O8pNPPkFYWBi2b99e5u8mOTkZb731FszNzZVvTF4lICAA8+fPR3h4eKnFhzlz5mD69OnKxzKZrNQ3NURERHXdf4v/le3KlSto3rw5zMzMVLZ37twZAHD16tVKu08fP34cxsbGyM/PR+PGjREQEICpU6eWeUxN6zFJ/4q8H6ky1OK/FFDggewBIu9HwsvRq/qCERFVMrWKD+UtPLysbdu2ard1cXFR+5uJVw1zqEoLFy7Ehg0bsGzZMowePbrUdhkZGXjzzTeRnp6OyMhI2NqqN07PyMgI9evXR1paWqltDAwMYGBgUO7sVSVTnon5J+bj77S/cWDEAegIGs1lSkREVCslJSWV+N6kaFtiYmKlXKd169bo3r07WrRogdTUVGzbtg3Tpk1DYmIili9fXupxNa3HJP0rKTOpUtsREdVUFV5qUy6X48mTJ7C2ti62wkN5NWzYEP7+/hWNVKW+/fZbLFiwANOmTcOsWbNKbZeTk4N33nkHt2/fRnh4ONzc3NS+RmZmpvJ3WlsY6RlhY9RG5Obn4l76PTSp10TsSEREVIcdO3YMvXv31ujY8PBw9OnTp1zHZGdnl/ilgKGhoXJ/ZThw4IDK47Fjx+LNN9/EqlWr8Mknn8DOzq7E49hjsuaykar3hZq67YiIaiqNv56+fPky3njjDUilUjg4OOD06dMAgMePH6N3794IDw+vtJCV5f79+7h165bGx+/duxdTpkzByJEjsWrVqlLb5efnY9iwYTh37hx++ukndOvWrcR2OTk5yMzMLLb9q6++gkKhqFWrhejq6KJF/RYAgOiUaJHTEBFRXTdgwAC88cYbOHjwoFqTSL548QL79+9Hz549MXDgwHJfz8jIqMRlPHNycpT7q4IgCAgICEBeXh4iIiJKbWdgYAAzMzOVH6oZPB08YWdWctEIKJzzwd7MHp4Opc+7RkRUG2jU8+Hq1avw9PSElZUV/Pz8VIZKNGjQANnZ2QgODi73twaaWrx4MQDg5s2bAIAdO3YoiyHz5s1TtvPz88PJkydVlgjNyMhQThh55swZAMD69ethYWEBCwsLTJ48GUDhLNZ+fn6oX78+evfujV27dqlk8PDwQNOmhbMQf/rppzhw4ADeeecdpKWlYefOnSpti2aoTk5ORrt27TBixAjl/BWHDx/GH3/8gQEDBmDw4MGV8NupPq7Wrrj++DpinsTgreZviR2HiIjqsCtXrmD69OkYNGgQrK2t0adPH3Tu3BlOTk6wtLRUrtR1584dnD9/HseOHUN6ejr69etXrvmqitjY2ODRo0fFticlFXaVV3fopSaKejCUNVyTai6JjgRf9PgCEw5OKLZPQOHk6qsHrOZkk0RU62lUfPjiiy9ga2uLK1euICcnBz/88IPK/t69e2Pfvn2VElAd8+fPV3n8cp6Xiw8lefr0abHjV65cCQBo3LixsvgQHR2N3NxcpKSkYNy4ccXOExQUpCw+FL1p+e233/Dbb78Va1tUfLCwsMDbb7+No0ePIjg4GPn5+WjWrBmWLFmCGTNmQEends2b4GZVOLQkJiVG5CRERFTXubu748iRIzh37hw2bNiAX3/9FT/++GOJS2yamZnB29sbH3/8MTp16qTR9dq2bYsTJ05AJpOp9Cq4cOGCcn9ViYuLA4BaNVyTVB2JOwIA0JfoK5fXBAA7MzusHrAa3q7eYkUjIqo0GhUfIiMjMWfOHJiampbYxdDBwaHSJlZSx8s9GcpSUndER0dHtY739/dXez6Ksro9vszCwgI7duxQq21t4GrtCgCIfsJhF0REVDN069YN3bp1Q35+Pi5duoTo6GikpKRAEARYW1vD3d0d7dq1q3DB38fHB4GBgdi8eTNmzJgBoHBerKCgIHTp0kXZO+H+/fvIysrSaMWutLQ0mJubq6yc9eLFCyxbtgz6+vpqrxhGNcuRu0cQEh0CiSDB+fHnkSHPQFJmEmykNvB08GSPByLSGhoVH3JycmBubl7qfi7fVDe5Wf/b80GhUBT7domIiEgsEokEnTt3Vi59Wdm6dOmCoUOHYs6cOXj8+DGaNWuG4OBgJCQkYOvWrcp2FRkCeuDAASxevBg+Pj5o0qQJ0tLSsHv3bty4cQNLlixBw4YNq+S5UdXqZtcNAV0DoKuji3Y27cSOQ0RUZTQqPjg5OeHSpUul7j9+/Hi5Vncg7eBs6QwdQQcSHQnSstNQ37i+2JGIiIiqzfbt2zF//nzs2LEDT58+RevWrXHw4EH06NGjzOPUHQLaqlUruLm5YefOnUhJSYG+vj7atm2Lffv2YejQoVXzpKjKSQ2kWNV/ldo9eYmIaiuNig/vv/8+vvrqK/j6+qJdu8IKbdG33CtXrkRYWBjWrFlTeSmpVjDQNcDjGY9haWTJXg9ERFTnGBoaYsWKFVixYkWpbSoyBLRDhw7Fltqk2is9Jx3mBubK90x870RE2k6j4sOMGTNw9OhR9O/fHy4uLsplnlJSUpCcnIy+ffti0qRJlZ2VagH2diAiIiIqm0KhgM8+H8jz5dg6aCua128udiQioiqn0exK+vr6OHr0KAIDA2FkZARDQ0Pcvn0bVlZW+Oabb3Dw4MFat1IDEREREVF12HNjD47FH0NUYhQkAieUJKK6QaOeDwCgq6uLgIAABAQEVGYequUuJ13GFye+QD2jetjxrvas5EFERERUGTJyMjD9yHQAwOfdP4eTpZPIiYiIqofGxQeikhQoCvD7nd/xmslrYkchIiIiqnHmHZ+H5GfJaF6/OWa+PlPsOERE1UbjsREPHjzAuHHjYGdnB319fRw/fhwAkJKSgnHjxuHPP/+stJBUe7hYFa5b/s/zf5CalSpyGiIiquuePXuG8+fP4/Dhwzh79iySk5PFjkR12KXES9gQtQEAsGHgBhjoGoiciIio+mhUfIiPj0fHjh3x888/o2XLlsjPz1fus7a2RlRUFLZs2VJpIan2MNU3hYO5AwAg5kmMyGmIiKiuksvl+Pjjj2FtbY3XX38dAwcOhKenJxo1aoRGjRrBz88Pv//+O5c3pGqTX5CPj3//GAWKAoxwH4HeTXuLHYmIqFppVHyYO3cudHR0cOPGDezatavYjXvgwIE4ffp0pQSk2sfVyhUAEJPC4gMREYljxowZ+O677+Dl5YUlS5YgMDAQM2bMgEKhQE5ODnbu3IlBgwbB3d0dx44dEzsu1QGPnz9Gbn4uzAzMsLLfSrHjEBFVO42KD+Hh4Zg0aRLs7e1LXJO4cePGePjwYYXDUe3kZu0GAIhOiRY5CRER1VV79+7FuHHjcOjQIcyaNQsBAQGYObNwfP1PP/2EuLg4LFu2DNnZ2ejfvz/+97//iZyYtJ2N1AZRE6Jw0v8kbKQ2YschIqp2GhUfZDIZbGxK/0czNzcXeXl5Goei2k3Z84HDLoiISCTZ2dno1q1bqfsdHR3x2WefITY2FgEBAZgxYwaOHj1ajQmpLtLV0UXbhm3FjkFEJAqNig/29va4efNmqfvPnz+PZs2aaRyKajdXa1dI9aUw1DUUOwoREdVRHTt2xIkTJ17ZTk9PDytWrMDgwYPx9ddfV0MyqmsiEiKw6OQi5OTliB2FiEhUGhUfvL298cMPP+DGjRvKbUXDL37++Wf89NNP8PX1rZyEVOt42HsgY3YGfhn+i9hRiIiojpo9ezZ2796t9nCKgQMH4tKlS1Wciuqa3PxcfPz7x/gy4kssiVwidhwiIlFpPOGknZ0dunTpglGjRkEQBCxbtgzdunWDr68v2rRpg08//bSys1ItoSPolDgXCBERUXXp378/VqxYgRkzZqBz587YuXMnMjMzS21/8uRJGBsbV2NCqgsCzwbi1pNbaGDSANO7TRc7DhGRqHQ1OcjMzAznzp3D/PnzsXv3bigUChw9ehQWFhaYNGkSvv76axgasss9ERERiefTTz9FmzZtMHXqVPj5+UFXVxeCIGDDhg34888/IZVKIZPJcPjwYZw6dQpTp04VOzJpkfin8fjq1FcAgJX9VsLC0ELcQEREIhMU5VzgOj8/H48ePYKpqSksLS0BACkpKVAoFLC2tuY33iKRyWQwNzdHRkYGzMzMxI6DzZc243/n/wcfVx989cZXYschIqJyqGn3lIpSKBQICwvD3r17ERERgfv376vsNzY2xgcffIAVK1ZAT09PpJTVS9te45pGoVDgnR/fwe93fkcvx1445neM75GJSCuV535S7p4PL168QNOmTbF06VJ89tlnAABra2vNkpLWkufJcevJLfz1+C+xoxARUR0nCALefPNNvPnmmwCA1NRUxMXF4dmzZzAzM4ObmxuMjIxETkna5NfYX/H7nd+hp6OHDW9tYOGBiAgaFB8MDQ1hZWUFExOTqshDWsLN2g0AEJPC5TaJiKhmqV+/PurXry92DNJSBYoCzAqfBQD4zOMzuFi5iJyIiKhm0GjCyYEDB+LgwYOVnYW0iKu1KwDg7tO7kOfJRU5DREREVD10BB0cGnkI49qOw9wec8WOQ0RUY2hUfPjmm2+QlJSEMWPG4Pr168jJ4brFpMrG1AbmBuYoUBTgduptseMQERERVZum9Zpi6+CtMNbjCipEREU0Kj40aNAAf/31F3bs2IG2bdvCxMQEEolE5UdXV6OFNEhLCIKg7P0Q84RDL4iIiEi7FSgKcDX5qtgxiIhqLI0qBH5+fpw4h17JzcoN5x+eR3RKtNhRiIiIiKpU8NVgjDswDlO7TMXqAavFjkNEVONoVHzYtm1bJccgbdTOph3aJrdFfSNO6kVEROLJzs7GTz/9hBYtWqBLly5ixyEtlJqVis+OFq4C10jaSOQ0REQ1k0bDLrZv346EhIRS99+7dw/bt2/XNBNpicmdJ+PKxCv4pMsnYkchIqI6zMDAAB9++CGuXLkidhTSUrPDZyM1OxXuDdwxres0seMQEdVIGhUfxo4di7Nnz5a6//z58xg7dqzGoYiIiIgqi46ODuzt7SGTycSOQlro7IOz2HJlCwBg41sboSfREzkREVHNpFHxQaFQlLn/xYsX0NHR6NSkhfIL8vEi/4XYMYiIqA4bM2YMduzYAbmcyz9T5ckryMPHv38MABjbdiy6O3QXORERUc2lcYWgtAkn09PT8fvvv8PGxkbjUOWRlJSE2bNno1evXpBKpRAEAREREWofHxsbi4CAAHh4eMDQ0BCCIJQ4pCQ1NRUrVqxAjx49YG1tDQsLC3Tt2hV79+4t1jYiIgKCIJT4c/78+WLtz549i+7du8PY2BgNGzbElClT8OzZs/L8GmqsET+PgMkSExz6+5DYUYiIqA7z8PCArq4u2rZti3Xr1iEsLAynTp0q9kNUHusurMNf//wFSyNLfNP3G7HjEBHVaGpPOLlw4UIsWrQIQGHhYdSoURg1alSp7T/99NOKp1NDbGwsli9fDmdnZ7Rq1Qrnzp0r1/Hnzp3D2rVr4ebmBldXV1y9erXUdnPnzsXAgQMxb9486Orq4ueff8bw4cMRHR2NhQsXFjtmypQp6NSpk8q2Zs2aqTy+evUqevfuDVdXV6xatQoPHz5EYGAg7ty5g0OHav8Hdh1BB/J8OaJTojGoxSCx4xARUR3Vt29f5d+nTp1a7EsUhUIBQRCQn59f3dGoFmto2hBWxlZY8sYSWBlbiR2HiKhGU7v40LZtW/j5+UGhUGD79u3w9PRE06ZNVdoIggBTU1N07doVI0aMqPSwJenQoQNSU1NhaWmJkJAQDB06tFzHDxo0COnp6ZBKpQgMDCy1+NCyZUvcuXMHjRs3Vm6bNGkS+vTpg+XLl2PmzJkwMTFROcbT0xM+Pj5lXv/zzz9HvXr1EBERATMzMwCAo6MjPvzwQxw5cgT9+vUr1/Opadys3AAAMU9iRE5CRER1WVBQkNgRSAuNaDUCA5oNgLmhudhRiIhqPLWLD4MHD8bgwYMBFK5mMW/ePPTu3bvKgqlLKpVW6HhLS0u12jVp0qTYNkEQMGTIEBw/fhxxcXFo1apVsTaZmZkwMjKCrm7xX7VMJsPRo0cREBCgLDwAgJ+fHwICArBv375aX3xwtXYFAESnRIuchIiI6rIxY8aIHYG0VD2jemJHICKqFco958OzZ8/g6OiItLS0qshT6yQnJwMArKyKd7UbO3YszMzMYGhoiF69eiEqKkpl//Xr15GXl4eOHTuqbNfX10fbtm21YkkwV6vC4kNMSswrJyolIiKqDnK5HI8ePUJubq7YUagWyn6RjZ7bemLfzX18b0NEVA7lLj6Ymppi7969XK4KQFpaGrZs2QJPT0+VCTb19fXx3nvvYc2aNfj111+xePFiXL9+HZ6enioFhaSkJAAocXJOGxsbJCYmlnptuVwOmUym8lMTNbNsBl0dXTx/8RwPZA/EjkNERHXY5cuX8cYbb0AqlcLBwQGnT58GADx+/Bi9e/dGeHi4yAmpNlh6eilO3TuF6YenI+tFlthxiIhqDY1Wu3BzcytxRYiKKigoQE5Ojlo/YleaCwoKMHLkSKSnp2PdunUq+zw8PBASEoJx48Zh0KBBmD17Ns6fPw9BEDBnzhxlu+zsbACAgYFBsfMbGhoq95dk6dKlMDc3V/7Y29tX0jOrXHoSPThbOgMo7P1AREQkhqtXr8LT0xN3796Fn5+fyr4GDRogOzsbwcHBIqWj2uJ26m0sP7McALB6wGqY6Ju84ggiIiqiUfFh5syZ2LhxI27fvl2pYU6dOgUjIyO1fmJjYyv12uX1ySefICwsDFu2bEGbNm1e2b5Zs2YYPHgwTpw4oZxJ28jICABKXHM8JydHub8kc+bMQUZGhvLnwYOa26tgQLMBeM/1PUgNKjY/BxERkaa++OIL2Nra4ubNm1i2bFmxLzF69+6NixcvipSOagOFQoFJv09Cbn6u8r0NERGpT+0JJ19269Yt2Nvbo1WrVnj77bfh7OwMY2NjlTaCIGD+/PnlOq+Li4vas1GXNFShuixcuBAbNmzAsmXLMHr0aLWPs7e3R25uLp4/fw4zMzPlcygafvGypKQk2NralnouAwODEntM1ESr+q8SOwIREdVxkZGRmDNnDkxNTUss+js4OJQ53JHqpvyCfETej0RSZhJinsTgWPwxGOoaYv2b64st10pERGXTqPiwYMEC5d/3799fYhtNig8NGzaEv7+/JpGqzbfffosFCxZg2rRpmDVrVrmOjYuLg6GhIUxNTQEA7u7u0NXVRVRUFHx9fZXtcnNzcfXqVZVtREREpLmcnByYm5e+HGJNnTuJxBMaE4qpYVPxUPZQZfuQFkPgZOkkUioiotpLo+JDfHx8ZeeoFvfv30dWVhZcXFw0On7v3r2YMmUKRo4ciVWrSv82PyUlBdbW1irbrl27hgMHDuDNN9+Ejk7haBdzc3P06dMHO3fuxPz585XLhu7YsQPPnj3D0KFDNcpZEykUCjyUPYSdmR2/KSAiomrn5OSES5culbr/+PHjcHNzq8ZEVJOFxoTCZ58PFCg+x9jem3sxtOVQeLt6i5CMiKj20qj40Lhx48rOUSGLFy8GANy8eRNA4Yf3ohms582bp2zn5+eHkydPqozzzMjIUE4YeebMGQDA+vXrYWFhAQsLC0yePBkAcPHiRfj5+aF+/fro3bs3du3apZLBw8MDTZs2BQAMGzYMRkZG8PDwQIMGDRAdHY3NmzfD2NgYy5YtUznu66+/hoeHB3r27IkJEybg4cOHWLlyJfr164cBAwZU2u9ITLn5ubBeYQ2ZXIZ/ZvyDBiYNxI5ERER1zPvvv4+vvvoKvr6+aNeuHQAoi+ErV65EWFgY1qxZI2ZEqiHyC/IxNWxqiYWHItPCpmFwi8GQ6EiqMRkRUe0mKMReNqISlPVN+stPz8vLq1jxISEhAU2aNCnx2MaNGytX9di2bRvGjh1b6nWCgoKUQ0bWrl2LXbt24e+//4ZMJoO1tTV69+6NL7/8Es2aNSt27OnTpzFr1ixcvnwZUqkUvr6+WLp0qbInhDpkMhnMzc2RkZEBMzMztY+rLk5rnRD3NA4nxpyAl6OX2HGIiKgMNf2eoonc3Fz0798fp06dgouLC27duoVWrVohJSUFycnJ6Nu3L/744w9l70Rtp42vcWWJSIhAr+Ber2zH9zREROW7n2hcfMjLy8Mvv/yCCxcu4OnTpygoKFA9sSBg69atmpyaNFDT30S88+M7OHj7IDYM3ICPO30sdhwiIipDTb+naCovLw/r1q3Drl27EBMTA4VCAWdnZ/j5+WHq1KnQ1dWoQ2itpK2vcWX48fqPeD/0/Ve22+29GyNajaiGRERENVd57ica3WXT0tLQq1cv3LhxAwqFAoIgKHsTFP2dxQd6mauVKw7ePojolGixoxARUR2lq6uLgIAABAQEiB2FajAbqXorqqnbjoiICmnUt3DevHm4desWtmzZgrt370KhUODw4cOIiYnBiBEj0KlTJ6SmplZ2VqrF3KwLJ/GKeRIjchIiIqqLxo0bhwsXLpS6/+LFixg3blw1JqKaytPBs3CCbJQ8rFeAAHsze3g6eFZzMiKi2k2j4sPvv/8OPz8/jB07Vtm1QiKRoEWLFti5cyeMjIwwZ86cSg1KtZurlSsAFh+IiEgc27Ztw927d0vdHx8fj+Dg4GpMRDWVREeCNQPWlDjhZFFBYvWA1ZxskoionDQqPiQnJ6NTp04AoBwfmZOTo9w/ZMgQHDhwoBLikbZwsSpc3jQxMxEZORkipyEiIlL1/Plz6OnpiR2DaoghLkPwmslrxbbbmdkhxDeEy2wSEWlAozkfLC0t8fz5cwCAVCqFnp4eHjx4oNyvp6eHp0+fVk5C0grmhuYY02YMGpo2xIuCF2LHISKiOuD+/fvKVasA4NatWzh16lSxdmlpadi4cWOJK1JR3XQs7hj+ef4PpPpS7PXZi/ScdNhIbeDp4MkeD0REGtKo+NC8eXNERxdOHKijo4N27dph27Zt8Pf3R35+PrZv346mTZtWalCq/bYN2SZ2BCIiqkOCgoKwcOFCCIIAQRDw9ddf4+uvvy7WTqFQQEdHB0FBQSKkpJrou0vfAQDGtBmDN53fFDkNEZF20Kj40K9fPwQGBmL9+vUwMDDA9OnTMXz4cFhaWkIQBGRnZ2Pz5s2VnZWIiIhIbUOGDIGjoyMUCgXGjRuHCRMmoFu3biptBEGAqakpOnXqBHt7e5GSUk0ik8tw6O9DAICJHSeKnIaISHsIiqI1MstBoVAgNzcXBgYGym2hoaHYuXMnJBIJfHx8MGzYsEoNSmWrDet1KxQK/PP8H/zz7B+0adhG7DhERFSK2nBPKa+FCxfivffeg7u7u9hRagRtfI0r0z/P/sGhvw/Bv62/2FGIiGq08txPNCo+UM1TG95ERN6LRI9tPeBo4Yj4qfFixyEiolLUhntKRcjlcjx58gTW1tbQ19cXO44otP01JiKi6lGe+4lGq10QaaJoxYt76feQ9SJL5DRERFTXXL58GW+88QakUikcHBxw+vRpAMDjx4/Ru3dvhIeHi5yQxJabnyt2BCIiraX2nA+hoaHlPrm3N5chon9Zm1ijvlF9pGanIvZJLNrZtBM7EhER1RFXr16Fp6cnrKys4OfnpzK5ZIMGDZCdnY3g4GD06dNHxJQktqE/DUVGTgZW9luJDrYdxI5DRKRV1C4++Pj4QBAEtdoqFAoIgoD8/HyNg5F2crV2xen7pxGdEs3iAxERVZsvvvgCtra2uHLlCnJycvDDDz+o7O/duzf27dsnUjqqCR5kPMDB2wdRoCiAsZ6x2HGIiLSO2sWH/y4/lZmZiSlTpuCzzz6Dm5tbpQcj7eRm5YbT908j5kmM2FGIiKgOiYyMxJw5c2Bqagq5XF5sv4ODAxITE0VIRjXF1itbUaAoQI/GPeBq7Sp2HCIiraN28WHMmDEqj1NTUzFlyhT0798fb7zxRqUHI+1UdDOPTokWOQkREdUlOTk5MDc3L3W/TCarxjRU0+QV5GHL5S0AgI86fCRyGiIi7cQJJ6lauVoVFh/Y84GIiKqTk5MTLl26VOr+48ePsydnHfbHnT/wKPMRrIyt4O3KOcuIiKoCiw9Urdo0bIOArgGY6TFT7ChERFSHvP/++9ixY4fKihZFc1mtXLkSYWFhGD16tFjxSGSbojYBAPzb+MNA10DkNERE2kntYRdElaGhaUOs6r9K7BhERFTHzJgxA0ePHkX//v3h4uICQRAQEBCAlJQUJCcno2/fvpg0aZLYMUkECekJCPs7DAAwocMEkdMQEWkv9nwgIiIiraevr4+jR48iMDAQRkZGMDQ0xO3bt2FlZYVvvvkGBw8ehI4O3xbVRQ1NG2LHuzswvet0ONd3FjsOEZHWEhQKhUKdhmlpaSqPU1NT0aJFC+zfvx+enp4lHmNpaVnxhKQWmUwGc3NzZGRkwMzMTOw4ZcrIycCNxzdgrGfM5TaJiGqg2nRPIc3wNSYiospQnvuJ2sMurKyslGMjX+btXfKkPIIgIC8vT93TUx2yKWoTZh+bjRHuI7D7vd1ixyEiIiIiIqIqpnbxwc/Pr8TiA1F5uVkXzibOFS+IiIhITBN/m4hmls3wQfsPUM+onthxiIi0mtrFh23btlVhDKpLXK0Ll9u89eQW8gvyIdGRiJyIiIjqgt27d+Pbb7/FnTt3kJqaWmw/e23WLXfT7mLz5c0QIMDHzYfFByKiKsbVLqjaNbFoAgOJAXLycnAv4x6a1msqdiQiItJyixcvxpdffonXXnsNHh4eqFev8j9oyuVyfPHFF9ixYweePn2K1q1bY/Hixejbt2+Zx8XGxmLTpk24cOECLl++DLlcjvj4eDg6OpbY/sCBA1iwYAGio6PRoEEDjB07FvPnz4euLt/Wlcf3l78HAPRv1h9N6jUROQ0RkfZT6y6Vl5en8Q2tIseSdpLoSNDCqgX++ucvxKTEsPhARERVbsOGDfDy8kJYWBj09PSq5Br+/v4ICQnBtGnT4OzsjG3btmHgwIE4ceIEunfvXupx586dw9q1a+Hm5gZXV1dcvXq11LaHDh3CkCFD4OXlhXXr1uH69etYvHgxHj9+jI0bN1bBs9JOufm5+OHKDwCAiR0mipyGiKhuUGtNqRYtWmD79u3Iz89X+8R5eXn44Ycf0Lx5c43DkfZytSocehGdEi1yEiIiqgtkMhl8fX2rrPBw8eJF7NmzB0uXLsWKFSswYcIEHD9+HI0bN8bMmTPLPHbQoEFIT0/H9evXMXLkyDLbzpgxA61bt8aRI0fw4YcfYu3atZgzZw6+++473Lp1qzKfklbbH7MfKVkpsJXa4u3mb4sdh4ioTlCr+ODj44OJEyeiUaNGmD59Oo4ePYr09PRi7dLS0vDHH39g8uTJsLGxweTJkzFs2LDKzkxagJNOEhFRdWrXrh0ePHhQZecPCQmBRCLBhAkTlNsMDQ0xfvx4nDt3rsxrW1paQiqVvvIa0dHRiI6OxoQJE1R6lU6aNAkKhQIhISEVexJ1yHeXvgMAfNDuA+jqsIcuEVF1UOtf2+XLl+Ojjz7CsmXL8P3332PNmjUAgHr16sHS0hIKhQJpaWnKgoSpqSlGjRqFmTNnonHjxlUWnmqvt5u/jfpG9dHFrovYUYiIqA5YvHgx3nvvPbz33nto165dpZ//ypUraN68ebE1zjt37gwAuHr1Kuzt7St8DQDo2LGjynZbW1vY2dkp95dELpdDLpcrH8tksgplqc1in8TiRMIJ6Ag6+KD9B2LHISKqM9Qu9TZp0gTfffcdAgMD8fvvv+PUqVOIjo5GSkoKBEFA69at4e7uDi8vLwwYMAAmJiZVmVspKSkJa9aswYULFxAVFYVnz57hxIkT8PLyUut4dSd5Sk1NxQ8//IDffvsNMTExePHiBVxcXBAQEFCsd4e/vz+Cg4NLvebDhw/RqFEjAICXlxdOnjxZrE3//v0RFham1nOojdrbtEd7m/ZixyAiojqiZ8+e2Lp1K7p27YquXbvC0dEREonqakuCIGDr1q0anT8pKQk2NjbFthdtS0xM1Oi8/73Gy+f873XKusbSpUuxcOHCCmfQBoIgYIT7COTm58LevGIFISIiUl+5+5lJpVIMHz4cw4cPr4o85RYbG4vly5fD2dkZrVq1wrlz58p1vLqTPJ07dw5z587FwIEDMW/ePOjq6uLnn3/G8OHDER0drXJDnzhxIvr06aNyvEKhwEcffQRHR0dl4aGInZ0dli5dqrLN1ta2XM+DiIiISnfhwgWMGTMGL168QGRkJCIjI4u1qUjxITs7GwYGBsW2GxoaKvdXVNE5SrtOWb0Z5syZg+nTpysfy2SyCvfEqK2a12+O3e/thkKhEDsKEVGdUusHuXXo0AGpqamwtLRESEgIhg4dWq7jiyZ5kkqlCAwMLLX40LJlS9y5c0dlGMmkSZPQp08fLF++HDNnzlT29ujWrRu6deumcvzp06eRlZVV4kRS5ubmGDVqVLlya4Pr/1zH5aTL6GbfDc3rc2JSIiKqOlOnToW+vj5+/fVXeHp6wsLColLPb2RkpDKsoUhOTo5yf2VcA0Cp1ynrGgYGBiUWLeoyQRDEjkBEVKeoNeFkTSaVSmFpaanx8epO8tSkSZNi81cIgoAhQ4ZALpcjLi6uzON3794NQRDw/vvvl7g/Ly8Pz549Uz+4Fvgi4gv4/+qPsL+1d3gJERHVDH/99RdmzJiBd955p9ILD0DhsIeiYREvK9pWGT0ai4ZblHYd9pp8tdXnV3OlLSIikdT64oPYkpOTAQBWVlaltnnx4gX27dsHDw+PYnNJAMDt27dhYmICqVSKhg0bYv78+Xjx4kVVRa4xipbbjEnhihdERFS1GjRoAH19/So7f9u2bXH79u1iQx8uXLig3F8Z1wCAqKgole2JiYl4+PBhpVxDm918fBMBhwPQemNrpDxPETsOEVGdw+JDBaSlpWHLli3w9PQscfKnIocPH0ZqamqJQy6cnJwwd+5c/Pjjj9i+fTu6dOmCxYsXv3IYhlwuh0wmU/mpbYqKD9FP+A0EERFVrXHjxmHnzp3Iy8urkvP7+PggPz8fmzdvVm6Ty+UICgpCly5dlPMr3L9/H7du3dLoGi1btoSLiws2b96M/Px85faNGzdCEAT4+PhU7Elouc2XCl+bd1q8A2sTa5HTEBHVPTVqzoeCggLk5uaq1dbAwEDUsXoFBQUYOXIk0tPTsW7dujLb7t69G3p6evD19S22778TW40ePRoTJkzA999/j4CAAHTt2rXEc2rDrNVu1m4A2POBiIiqXvfu3XHw4EF07doVkyZNQpMmTYqtdgEAPXr00Oj8Xbp0wdChQzFnzhw8fvwYzZo1Q3BwMBISElTu9X5+fjh58qTKZIcZGRnK9xJnzpwBAKxfvx4WFhawsLDA5MmTlW1XrFiBQYMGoV+/fhg+fDhu3LiB9evX44MPPoCrq6tG2euCrBdZCL5WuBLZRx0+EjkNEVHdVKOKD6dOnUKvXr3UahsTEwMXF5cqTlS6Tz75BGFhYdi+fTvatGlTartnz57h119/Rf/+/VG/fn21zv3pp5/i+++/R3h4eKnFB22YtdrFqvD1S8lKwZOsJ7AyLn3oChERUUW8vArVBx98UOwLDIVCAUEQVHoUlNf27dsxf/587NixA0+fPkXr1q1x8ODBVxY0nj59ivnz56tsW7lyJQCgcePGKsWHt99+G6GhoVi4cCE++eQTWFtb4/PPP8cXX3yhce66YN/NfciQZ6CJRRP0deordhwiojpJo+LD/fv34eDgUNlZ4OLigqCgILXaljXMoaotXLgQGzZswLJlyzB69Ogy2/7yyy+lrnJRmqIiQlpaWqlttGHWahN9EzQ2b4x7GfcQkxIDz8aeYkciIiItpe77i4owNDTEihUrsGLFilLbREREFNvm6OhYrmUfhwwZgiFDhmiQsO767tJ3AIAP238IHYGjjomIxKBR8aFJkybo168fPvjgAwwePBi6upXTgaJhw4bw9/evlHNVlW+//RYLFizAtGnTMGvWrFe237VrF0xNTTFo0CC1r1G0coa1tfaPR3SxcsG9jHsIvhaMfEU+PB08IdEp3g2WiIioIsaMGSN2BBLJX//8hfMPz0NXRxfj2o0TOw4RUZ2lUen3o48+woULF+Dr6wtbW1vMmDEDMTE1f9x+RSZ5AoC9e/diypQpGDlyJFatWvXK9ikpKQgPD8e7774LY2PjYvtlMlmxtboVCgUWL14MAOjfv7/GWWuD0JhQXE66DADYemUregX3guMaR4TGhIqcjIiIiLRF3NM4WBlb4V2Xd/Ga6WtixyEiqrMERXn6+b1ELpcjJCQEW7duxcmTJwEUTrb04YcfYtiwYSV+2K4qRR/Wb968iT179mDcuHFo0qQJAGDevHnKdl5eXq+c5CksLAyffvppsUmeLl68CE9PT5ibm2P58uXQ09NTyeDh4YGmTZuqbFu/fr1yboiSCgkREREYMWIERowYgWbNmiE7Oxv79+/HmTNnMGHCBHz33Xdq/w5kMhnMzc2RkZEBMzMztY8TS2hMKHz2+UAB1f/8BBSOwQ3xDYG3q7cY0YiI6rzadk8pj6ioKFy4cAFPnz5FQUGByj5BEIrNvaCttPk1Lok8T46nOU/R0LSh2FGIiLRKee4nGhcfXhYfH4+tW7ciODgYiYmJMDU1xbBhw/DBBx+gc+fOFT39K5W16sXLT6+k4kNCQoKyUPFfjRs3RkJCAgBg27ZtGDt2bKnXCQoKKjZkpFu3boiLi0NiYmKJM2rHx8dj1qxZ+PPPP5GcnAwdHR24urriww8/xIQJE8q1mkdtehORX5APxzWOeCh7WOJ+AQLszOwQPzWeQzCIiERQm+4p6srOzoa3tzeOHDminFyy6P1A0d8rOuFkbaKNrzEREVW/ai8+FMnKysJHH32EnTt3Fp5cENC6dWt8/vnnGDp0aGVdhkpQm95ERCREoFfwq1c1OTHmBLwcvao+EBERqahN9xR1zZkzB9988w3mzp2L3r17o1evXggODkaDBg2wdOlSZGdnY/v27WjRooXYUauFNr7GJTl9/zQ87D04ySQRURUpz/2kUv4l/uuvvzB16lQ4ODhg586daNy4MRYtWoSlS5dCJpNh+PDhWLRoUWVcirRAUmZSpbYjIiJ6lZCQEAwdOhSLFi2Cu7s7AKBRo0bo378/wsPDkZubi23btokbkirVpcRL8AzyhPsGd+QV5Ikdh4ioztO4+CCTybBp0yZ06tQJ7dq1w8aNG9GzZ0/88ccfiIuLw7x58zBz5kzcvn0bPj4++PbbbyszN9ViNlL1lklVtx0REdGrPHjwAD179gQA5VDI3NxcAICuri5GjBiBPXv2iJaPKl/R8pptG7aFrk7lrMxGRESa0+hf4tGjRyM0NBTZ2dlo0qQJFi9ejHHjxuG114rPICyRSDB48GD89NNPFQ5L2sHTwRN2ZnZ4JHtUbMJJ4N85HzwdPEVIR0RE2kgqlSIvL0/5dx0dHSQmJir3m5ubIzk5Wax4VMlkchl2X98NAPio40cipyEiIkDDng/79u3DwIEDcfjwYdy9exdz5swpsfBQxMPDA0FBQRqHJO0i0ZFgzYA1AP5d3eK/Vg9YzckmiYio0jg5OeH27dsACr8YadmyJUJCQgAUTk4dGhoKe3t7MSNSJdr11y48f/Ecrlau/DKDiKiG0Kj48PDhQ/z000/o27evWu0dHR0xZswYTS5FWsrb1RshviFoZNZIZbtEkGDf0H1cZpOIiCpVnz598PPPPytXs5g4cSLCwsLg5OQEZ2dnhIeHY/z48SKnpMqgUCiUQy4mdCjf6mFERFR1NBp2YW1tXdk5qA7ydvXG4BaDEXk/EgnpCZj0+yRk52WjgUkDsaMREZGWmT17NkaPHq1cXnPSpEnIycnBzp07IZFI8OGHH2LmzJkip6TKcPHRRVz75xoMJAbwa+MndhwiIvr/NCo+jBs3rsz9giDAyMgIDg4O6Nu3L9q1a6dRONJ+Eh2JcjnNyHuROHnvJDJyMsQNRUREWiU7OxuhoaFo0aIFdHX/feszffp0TJ8+XcRkVBV+jf0VAODb0heWRpYipyEioiKCougrgHLQ0dFRdmH77+H/3S4IAoYPH47t27crZ5emyqcN63U/y30GEz0Tdo8kIhKZNtxTXlZQUABDQ0OsXbsWH33EyQcB7XuNX6ZQKHDmwRnUN6oPV2tXseMQEWm18txPNJrzISUlBe3bt8fQoUNx4cIFpKenIz09HefPn4ePjw86duyI+Ph4/Pnnn/Dx8cGePXvwzTffaPRkqO4w1Tdl4YGIiCqdjo4OHBwcIJPJxI5C1UAQBHR36M7CAxFRDaNRz4exY8ciJSUFBw8eLHH/W2+9hQYNGihXuOjRowdSU1Nx8+bNiqWlUmnTNxjyPDnOPzyPno49xY5CRFQnadM9pchXX32Fffv2ISoqCgYGBmLHEZ02vsYKhQLZedkw1jMWOwoRUZ1R5T0ffvvtNwwcOLDU/W+99RZ+++035eNBgwYhPj5ek0tRHZOekw7bVbZ4Y/sbeCR7JHYcIiLSEh4eHtDV1UXbtm2xbt06hIWF4dSpU8V+qPY6++AsbFbaYNbRWWJHISKiEmg04WROTg4SExNL3f/w4UPk5OQoH5uYmKhM8ERUGgtDC7hZu+H0/dPY+ddOzOrONxBERFRxLy8PPnXq1GLD/BQKBQRBUC7FSbXPpkubIJPLkJqdKnYUIiIqgUYVAQ8PD6xbtw5vv/02unbtqrLv3LlzWL9+PTw8PJTbrl+/Dnt7+4olpTpjTJsxOH3/NIKvBWPm6zM5DwQREVVY0VBQ0k6pWan46eZPAICJHSaKnIaIiEqiUfEhMDAQnp6eeP3119G5c2e0aNECABAbG4uLFy/C1NQUgYGBAAp7SRw/fhxDhgyptNCk3Ya6DcUnhz5BzJMYRCVGoVOjTmJHIiKiWm7MmDFiR6AqFHwtGPJ8Odo1bIeOth3FjkNERCXQqPjQunVrXLp0CZ9//jkOHTqECxcuACgcXvHee+9h8eLFaN68OQDA0NAQt27dqrzEpPXMDc3xrsu7+PHGjwi+FsziAxEREZVKoVBg86XNAAp7PbDHJBFRzVTu4kN+fj4ePXoES0tL7Nu3DwUFBUhJSQEAWFtbQ0dHozksiVSMaTMGP974ET/e+BEr+62EgS5nJiciooqLiorChQsX8PTpUxQUFKjsEwQB8+fPFykZaerkvZOITY2Fqb4p3m/1vthxiIioFOUuPrx48QJNmzbF0qVL8dlnn0FHRwevvfZaVWSjOqxP0z6wldoiMTMRkfcj0adpH7EjERFRLZadnQ1vb28cOXJEOblk0WrjRX9n8aF22hS1CQAwstVISA2kIqchIqLSlLv4YGhoCCsrK5iYmFRFHiIAgERHgq2DtsLRwhEuVi5ixyEiolpu0aJFOHLkCObOnYvevXujV69eCA4ORoMGDbB06VJkZ2dj+/btYsckDSzpvQRNLJpguPtwsaMQEVEZNBojMXDgQBw8eLCysxCpGNBsAAsPRERUKUJCQjB06FAsWrQI7u7uAIBGjRqhf//+CA8PR25uLrZt2yZuSNJI03pNsbTPUrRp2EbsKEREVAaNig/ffPMNkpKSMGbMGFy/fh05OTmVnYtIRYGi4NWNiIiISvHgwQP07NkTACCRSAAAubm5AABdXV2MGDECe/bsES0fERGRttOo+NCgQQP89ddf2LFjB9q2bQsTExNIJBKVH11djRbSIFIR/zQew0KGwTPIU+woRERUi0mlUuTl5Sn/rqOjg8TEROV+c3NzJCcnixWPNHAs7hiG7BmCY3HHxI5CRERq0KhC4Ofnx2WMqFqYGZhhf8x+vCh4gRuPb8C9gbvYkYiIqBZycnLC7du3ART2fGjZsiVCQkIwbtw4KBQKhIaGwt7eXuSUVB4bozbi19hfYWdmh95Ne4sdh4iIXkGj4gPHRFJ1qW9cH283fxv7b+1H8NVgrOi3QuxIRERUC/Xp0wc//PADVq9eDYlEgokTJ2Ly5MlwcnKCIAiIj4/HkiVLxI5JakrKTMKvsb8CACZ2mChyGiIiUodGwy6IqtOYNmMAADuv70ReQZ7IaYiIqDaaPXs2Tpw4oVxec9KkSQgMDIS5uTnq1auHJUuWYObMmSKnpFfJL8hHREIEpoZNRV5BHrrZdUOr11qJHYuIiNQgKIruwuWUn5+PXbt24ciRI/jnn3/wzTffoF27dnj69Cl+++039O7dG40aNarsvFQKmUwGc3NzZGRkwMzMTOw4lSo3PxeNVjXCk6wn+OP9P/Cm85tiRyIi0mrafE+hQrXxNQ6NCcXUsKl4KHuo3FbPsB62DNoCb1dvEZMREdVd5bmfaNTzISsrCz179oS/vz9+/fVXHD9+HE+fPgUAmJmZYfbs2di4caMmpyYqRl+ijxHuIwAAwdeCRU5DRERE1S00JhQ++3xUCg8AkJ6TDp99PgiNCRUpGRERqUuj4sOCBQsQFRWF/fv3Iy4uDi93npBIJPD29sbhw4crLSRR0dCLX279gvScdHHDEBFRrSWXy3H48GFs3LgRGzduxOHDh7lkeA2XX5CPqWFToUDxzrpF26aFTUN+QX51RyMionLQqPjw008/YcKECRg8eDB0dIqfolmzZkhISKhoNrUkJSVh9uzZ6NWrF6RSKQRBQEREhNrHx8bGIiAgAB4eHjA0NIQgCKVmDwgIQPv27WFpaQljY2O4urpiwYIFePbsWbG2crkcs2bNgq2tLYyMjNClSxccPXq0xPOePXsW3bt3h7GxMRo2bIgpU6aUeM66rL1Newx1G4qv3/gaOgKnKiEiovLbvn07GjVqhIEDB+L//u//8H//938YOHAgGjVqxMm0a7DI+5HFejy8TAEFHsgeIPJ+ZDWmIiKi8tJotYvExES0adOm1P3GxsbIzMzUOFR5xMbGYvny5XB2dkarVq1w7ty5ch1/7tw5rF27Fm5ubnB1dcXVq1dLbfvnn3/C09MTY8eOhaGhIa5cuYJly5YhPDwcp06dUinE+Pv7IyQkBNOmTYOzszO2bduGgQMH4sSJE+jevbuy3dWrV9G7d2+4urpi1apVePjwIQIDA3Hnzh0cOnSo3L8PbSUIAvYN3Sd2DCIiqqX27t0Lf39/ODg4YMaMGXBzcwMA3Lx5E5s2bcL48eNhZGSEYcOGiZyU/ispM6lS2xERkTg0Kj7Ur18fjx49KnX/zZs3YWtrq3Go8ujQoQNSU1NhaWmJkJAQDB06tFzHDxo0COnp6ZBKpQgMDCyz+HD69Oli25ycnDBjxgxcvHgRXbt2BQBcvHgRe/bswYoVKzBjxgwAgJ+fH9zd3TFz5kycPXtWefznn3+OevXqISIiQjlBh6OjIz788EMcOXIE/fr1K9fzISIiouKWLFkCFxcXnD9/XmVCrEGDBmHSpEno0qULlixZwuJDDWQjtanUdkREJA6N+q/37t0bQUFByMrKKrYvPj4eP/zwAwYMGFDhcOqQSqWwtLTU+HhLS0tIpVKNj3d0dAQApKenK7eFhIRAIpFgwoQJym2GhoYYP348zp07hwcPHgAonBn06NGjGDVqlMobIT8/P5iammLfPn7T/1/Pcp8h+Gowtl/bLnYUIiKqRWJjYzF27NgSZ+I2NzfH2LFjcfv2bRGS0at4OnjCzswOAoQS9wsQYG9mD08Hz2pORkRE5aFR8eHLL7/E06dP0alTJ2zcuBGCICAsLAxz5sxB+/btYWBggDlz5lR21hohLy8PT548QWJiIo4cOYJ58+ZBKpWic+fOyjZXrlxB8+bNi73BKWpT1Lvi+vXryMvLQ8eOHVXa6evro23btrhy5UrVPpla6ODtg/D/1R9fnPgCBYoCseMQEVEt0bBhwzL3C4KA1157rZrSUHlIdCRYM2BNiRNOFhUkVg9YDYmOpLqjERFROWhUfGjWrBmOHTsGXV1dfPHFF1AoFAgMDMTy5cthb2+PY8eOwd7evrKz1ghRUVGwtrZGo0aN0L9/fygUChw4cECl90VSUhJsbIp3/SvalpiYqGz38vb/ti1qVxK5XA6ZTKbyUxcMbjEYZgZmuJdxD6funRI7DhER1RL+/v4ICgoqcUJnmUyGoKAgjB07VoRkpA5vV28Ma1l8SIydmR1CfEPg7eotQioiIioPjeZ8AArnWrh27Rpu3LiBmJgYKBQKODs7o127dhqHKSgoQG5urlptDQwMIAgld7+rSm5ubjh69CieP3+Os2fPIjw8vNgbmezsbBgYGBQ71tDQULn/5T9La1u0vyRLly7FwoULNX4etZWRnhF83Xyx5coWBF8Lhpejl9iRiIioFvD09MTBgwfRqlUrTJo0CS4uLgCAmJgYbNy4EVZWVvD09MSpU6qF7R49eogRl0qQmFn4pUxA1wB0su0EG6kNPB082eOBiKiW0Lj4UMTd3R3u7u6VkQWnTp1Cr1691GobExOjfONQnczMzNCnTx8AwODBg7F7924MHjwYly9fVq4AYmRkBLlcXuzYonXEjYyMVP4srW3R/pLMmTMH06dPVz6WyWRa29vkv8a0HYMtV7YgJDoE699cDxN9E7EjERFRDde3b1/l32fNmqX8AkOhKOzKf+/ePZU2CoUCgiAgPz+/eoNSiZ7lPsO5h4Urmk3uPBlN6zUVOREREZVXhYsPWVlZSE1NVd68X+bg4FCuc7m4uCAoKEittiUNVRCDt7c3Ro8ejT179iiLDzY2NiWuBlI0zKJoJZCi51C0/b9ty1oxxMDAoMQeE3XB6/avw6meE+4+vYvQmFCMbjNa7EhERFTDqfv+gmqmU/dOIa8gD00smrDwQERUS2lUfCgoKMA333yDdevWITk5udR25f22oGHDhvD399ckkmjkcjkKCgqQkZGh3Na2bVucOHECMplMZdLJCxcuKPcDhb1GdHV1ERUVBV9fX2W73NxcXL16VWUb/UsQBPi18cOXEV8i+Fowiw9ERPRKY8aMETsCVUB4XDgAoE/TPiInISIiTWlUfJg9ezYCAwPRsmVLvPfee6hfv35l56oS9+/fR1ZWlkbDNdLT02FiYgI9PT2V7Vu2bAEAlRUrfHx8EBgYiM2bN2PGjBkACosUQUFB6NKli3J4hLm5Ofr06YOdO3di/vz5yiU/d+zYgWfPnmHo0KEaPc+6wK+NHxaeLJzz4kX+C+hJ9F5xBBEREdVWRcWHvk37vqIlERHVVBoVH3bu3IkBAwbgjz/+qOw8Glm8eDEA4ObNmwAKP7yfPn0aADBv3jxlOz8/P5w8eVJliEhGRgbWrVsHADhz5gwAYP369bCwsICFhQUmT54MAIiIiMCUKVPg4+MDZ2dn5ObmIjIyEqGhoejYsSNGjRqlPGeXLl0wdOhQzJkzB48fP0azZs0QHByMhIQEbN26VSX7119/DQ8PD/Ts2RMTJkzAw4cPsXLlSvTr1w8DBgyo7F+V1nC0cETi9ES8Zspl0YiIiLTdoZGHcCz+GHo37S12FCIi0pCgKGmyhlcwMjLC6tWrMXHixKrIVG5lrXrx8tPz8vIqVnxISEhAkyZNSjy2cePGSEhIAADcvXsXixYtwunTp5GUlASFQgEnJyf4+Pjgs88+g4mJ6qSHOTk5mD9/Pnbu3ImnT5+idevW+Oqrr9C/f/9i1zl9+jRmzZqFy5cvQyqVwtfXF0uXLlX2hFCHTCaDubk5MjIyVIZ6EBERlRfvKdqPrzEREVWG8txPNCo+dO7cGQMHDsSCBQs0zUiVrC6/iUh+loy8gjzYmdmJHYWISCvU5XtKXcHXmIiIKkN57ic6mlzgyy+/xKZNm/DgwQONAhJVlpVnV8JulR2WRC4ROwoRERFVMoVCgZGhI7Hy7EpkyjPFjkNERBWg0ZwPly5dQuPGjeHm5oZ3330XTZo0gUQiUWkjCALmz59fKSGJStOmYRvkK/Kx58Ye/K///2CgWzeXHyUiIvXJ5XI8efIE1tbW0NfXFzsOlSE2NRa7r+/Gz9E/Y1KnSWLHISKiCtCo+PDycIudO3eW2IbFB6oOvRx7oZG0ER5lPsJvt3+Dj5uP2JGIiKiGunz5MmbMmIHTp08jPz8fR48exRtvvIHHjx9jxIgRmDNnDvr04VKONcnRu0cBAN0dusNIz0jkNEREVBEaDbuIj49/5U9cXFxlZyUqRqIjwejWowEAwdeCRU5DREQ11dWrV+Hp6Ym7d+/Cz89PZV+DBg2QnZ2N4GDeR2qa8HgusUlEpC006vnQuHHjys5BpLExbcdg2ZllOHTnEP559g+X3yQiomK++OIL2Nra4sqVK8jJycEPP/ygsr93797Yt2+fSOmoJHkFeTgRfwIA0Kcpe6QQEdV2GvV8eJWsrCz2fKBq42Llgs6NOiNfkY/d13eLHYeIiGqgyMhIfPjhhzA1NS1xiW4HBwckJiaKkIxKc/HRRWTmZsLSyBLtbNqJHYeIiCpI7eKDvr4+9uzZo3ycmZmJQYMG4fr168Xa7t+/H87OzpWTkEgNY9qMAQD8eONHkZMQEVFNlJOTA3Nz81L3y2SyakxD6giPKxxy0btJb+gIVfJ9GRERVSO1h13k5eWhoKBA+Tg3NxcHDx7EtGnTqiIXUbkMdx8OHUEHw1oOEzsKERHVQE5OTrh06VKp+48fPw43N7dqTESvkpGTAQOJAYdcEBFpCZaRSStYGlnio44foZ5RPbGjEBFRDfT+++9jx44dCA8PV24rGn6xcuVKhIWFYfTo0WLFoxKs7L8ST2c9xajWo8SOQkRElUCjCSeJajqFQlHimF4iIqqbZsyYgaNHj6J///5wcXGBIAgICAhASkoKkpOT0bdvX0yaNEnsmPQfXF6TiEh7sOcDaZWdf+1Ely1d8MedP8SOQkRENYi+vj6OHj2KwMBAGBkZwdDQELdv34aVlRW++eYbHDx4EDo6fFtUU2S/yBY7AhERVTL2fCCtEpUYhYuPLiL4WjDeav6W2HGIiKgG0dXVRUBAAAICAsSOQq/QeUtnCBCw490daNOwjdhxiIioEpSr+PDHH38gOTkZQOFymoIg4KeffsLVq1dV2pU1oRNRVRrTZgzWXFiDX2N/xdPsp5wDgoiIqJZJykzCjcc3IECAnZmd2HGIiKiSlKv4sHv3buzevVtl23fffVdiW463JzG0bdgWrRq0wvXH17H35l581PEjsSMREVENMG7cOEycOBFdunQpcf/FixexadMm/PDDD9WcjP7rWPwxAEB7m/aob1xf5DRERFRZ1B7ceOLEiXL9HD9+vCpzE5VIEASMaTMGABB8LVjkNEREVFNs27YNd+/eLXV/fHw8goMrdt+Qy+WYNWsWbG1tYWRkhC5duuDo0aNqHfvo0SP4+vrCwsICZmZmGDx4MOLi4oq1EwShxJ9ly5ZVKHtNcjSu8HfGJTaJiLSL2j0fevbsWZU5iCrNyNYjMSt8Fs4/PI/bqbfRvH5zsSMREVEN9/z5c+jp6VXoHP7+/ggJCcG0adPg7OyMbdu2YeDAgThx4gS6d+9e6nHPnj1Dr169kJGRgc8//xx6enr43//+h549e+Lq1auoX1/12/++ffvCz89PZVu7du0qlL2mUCgUCI8rXA61b9O+IqchIqLKxAknSes0NG2I/s364487f2D7te1Y/MZisSMREZEI7t+/j4SEBOXjW7du4dSpU8XapaWlYePGjWjWrJnG17p48SL27NmDFStWYMaMGQAAPz8/uLu7Y+bMmTh79mypx27YsAF37tzBxYsX0alTJwDAm2++CXd3d6xcuRJLlixRad+8eXOMGjVK46w12a0nt5CYmQhDXUO87vC62HGIiKgSsfhAWml8u/HQEXTQ3aH0b5qIiEi7BQUFYeHChcqhCV9//TW+/vrrYu0UCgV0dHQQFBSk8bVCQkIgkUgwYcIE5TZDQ0OMHz8en3/+OR48eAB7e/tSj+3UqZOy8AAALi4u6N27N/bt21es+AAA2dnZEAQBhoaGGmeuiYqGXHR36A5DXe16bkREdR2LD6SVvF294e3qLXYMIiIS0ZAhQ+Do6AiFQoFx48ZhwoQJ6Natm0obQRBgamqKTp06lVocUMeVK1fQvHlzmJmZqWzv3LkzAODq1aslnr+goAB//fUXxo0bV2xf586dceTIEWRmZkIqlSq3b9u2DRs2bIBCoYCrqyvmzZuH999/v8x8crkccrlc+Vgmk5Xr+VWX1q+1xpg2Y/C6PXs9EBFpGxYfiIiISCu1adMGbdq0AQDcu3cP7733Htzd3avkWklJSbCxsSm2vWhbYmJiicelpaVBLpe/8tgWLVoAADw8PODr64smTZogMTER3377LUaOHImMjAx8/PHHpeZbunQpFi5cWO7nVd28HL3g5egldgwiIqoCLD6QVktIT8DOv3ZiWtdpMNU3FTsOERGJ5Msvv6zS82dnZ8PAwKDY9qJhEdnZ2aUeB0DtY8+cOaPSZty4cejQoQM+//xz+Pv7w8jIqMTrzJkzB9OnT1c+lslkFerpQUREVF4sPpDWUigUGLBzAGJTY2FvZo8xbceIHYmIiEQWFRWFCxcu4OnTpygoKFDZJwgC5s+fr9F5jYyMVIY1FMnJyVHuL+04ABodCwD6+vqYPHkyPvroI1y6dKnUVTUMDAxKLHDUJGcfnIWRrhHaNGwDHUHt1eCJiKiWYPGBtJYgCBjVehTmn5iP4GvBLD4QEdVh2dnZ8Pb2xpEjR6BQKCAIAhQKBQAo/16R4oONjQ0ePXpUbHtSUhIAwNbWtsTjLC0tYWBgoGxXnmOLFPVgSEtLK1fmmmZW+Cycvn8aWwdtxbh2xefAICKi2o1lZdJqo1uPBgCcSDiBe+n3RE5DRERiWbRoEY4cOYK5c+fixIkTUCgUCA4OxqFDh+Dp6YlOnTohOjpa4/O3bdsWt2/fLjaR44ULF5T7S6Kjo4NWrVohKiqq2L4LFy6gadOmKpNNliQuLg4AYG1trUHymiFTnonzD88DAHo59hI5DRERVQUWH0irNbZorHwTs+OvHSKnISIisYSEhGDo0KFYtGiRctLJRo0aoX///ggPD0dubi62bdum8fl9fHyQn5+PzZs3K7fJ5XIEBQWhS5cuyt4J9+/fx61bt4od++eff6oUIGJjY3H8+HEMHTpUuS0lJaXYdTMzM7F69WpYWVmhQ4cOGucX28l7J5FXkAenek5oUq+J2HGIiKgKsPhAWm9Mm8LhFtuvbVd2sSUiorrlwYMH6NmzJwBAIpEAAHJzcwEAurq6GDFiBPbs2aPx+bt06YKhQ4dizpw5mDlzJjZv3ow33ngDCQkJ+Oabb5Tt/Pz84OrqqnLspEmT4OTkhLfeegsrVqzA6tWr0bdvX7z22mv49NNPle2+/fZbtG3bFvPnz8f333+PRYsWoVWrVoiLi8Pq1auhr6+vcX6xhceFAwD6NO0jchIiIqoqnPOBtN57bu/h//74P9xJu4NzD8/Bw95D7EhERFTNpFIp8vLylH/X0dFRWf7S3NwcycnJFbrG9u3bMX/+fOzYsQNPnz5F69atcfDgQfTo0eOV2SIiIhAQEIDFixejoKAAXl5e+N///qcylOL111/H2bNnsWXLFqSmpsLExASdO3fGDz/8gDfeeKNC2cV2NO4oABYfiIi0Wa3v+ZCUlITZs2ejV69ekEqlEAQBERERah8fGxuLgIAAeHh4wNDQEIIgICEhocS2AQEBaN++PSwtLWFsbAxXV1csWLAAz549U2n3559/YvLkyWjZsiVMTEzg4OAAX19f3L59u9g5/f39IQhCsR8XF5fy/BqoDKb6pnjP7T2Y6pviTuodseMQEZEInJyclPdhiUSCli1bIiQkBEDh6kihoaEVXnrS0NAQK1asQFJSEnJycnDx4kX0799fpU1ERESJvfDs7Ozw008/ISMjA5mZmfjtt9/QrFkzlTZ9+/bFkSNHkJSUhNzcXDx9+hSHDx+u9YWHxMxERKdEQ4DA+R6IiLRYre/5EBsbi+XLl8PZ2RmtWrXCuXPnynX8uXPnsHbtWri5ucHV1RVXr14tte2ff/4JT09PjB07FoaGhrhy5QqWLVuG8PBwnDp1Cjo6hbWc5cuX48yZMxg6dChat26N5ORkrF+/Hu3bt8f58+eVY02LGBgYYMuWLSrbzM3Ny/U8qGzLei/DhoEbYKJvInYUIiISQZ8+ffDDDz9g9erVkEgkmDhxIiZPngwnJycIgoD4+HgsWbJE7Jh10rG4YwCA9jbtUd+4vshpiIioqtT64kOHDh2QmpoKS0tL5WRS5TFo0CCkp6dDKpUiMDCwzOLD6dOni21zcnLCjBkzcPHiRXTt2hUAMH36dOzevVtl7OWwYcPQqlUrLFu2DDt37lQ5h66uLkaNGlWu3FQ+NlIbsSMQEZGIZs+ejdGjRyt7HUyaNAk5OTnYuXMnJBIJPvzwQ8ycOVPklHWTb0tf2JnZITc/V+woRERUhWp98eFVy0+9iqWlZYWOd3R0BACkp6crt3l4FJ9TwNnZGS1btkRMTEyJ58nPz8fz589hZmZWoTxUtrz8POy9uRc6gg5spDbwdPCEREcidiwiIqpipqamaNGihcq26dOnY/r06SIloiIGugbo1YTDLYiItF2tLz5Ut7y8PKSnpyM3Nxc3btzAvHnzIJVK0blz5zKPUygU+Oeff9CyZcti+7KysmBmZoasrCzUq1cPI0aMwPLly2FqalpVT6NO2nV9F/x/8UdeQZ5ym52ZHdYMWANvV28RkxERUVVJSkqCIAho2LAhACAnJwcbNmwo1s7e3r7cvSeJiIhIfSw+lFNUVBS6deumfNyiRQscOHDglT0odu3ahUePHmHRokUq221sbDBz5ky0b98eBQUFCAsLw4YNG3Dt2jVERERAV7fkl0gul0Mulysfy2SyCjwr7RcaE4rRoaOhgOokX49kj+CzzwchviEsQBARaZnY2Fi4u7tj8eLFmDVrFgDg+fPnmDFjBgRBUJn4UVdXF23btoWzs7NYceukH6//iPMPz2O4+3B0s+/26gOIiKjWqlHFh4KCAuWa269iYGAAQRCqOFFxbm5uOHr0KJ4/f46zZ88iPDy82GoX/3Xr1i383//9H7p164YxY8ao7Fu6dKnK4+HDh6N58+aYO3cuQkJCMHz48BLPuXTpUixcuLBiT6aOyC/Ix9SwqcUKDwCggAICBEwLm4bBLQZzCAYRkRYJCgqCpaUlAgICiu0LDAxE+/btARS+//Dx8cEPP/xQ7L5MVWv3jd04ePsgGpk1YvGBiEjL1ailNk+dOgUjIyO1fmJjY0XJaGZmhj59+mDw4MFYvnw5Pv30UwwePBjXrl0rsX1ycjLeeustmJubIyQkBBLJqz/cBgQEQEdHB+Hh4aW2mTNnDjIyMpQ/Dx480Pg5abvI+5F4KHtY6n4FFHgge4DI+5HVmIqIiKra8ePHMWjQIJUJoIu0adMGPXv2RM+ePdGrVy8MGzYMx44dEyFl3fUi/wUiEiIAAH2a9hE3DBERVbka1fPBxcUFQUFBarW1sakZqxd4e3tj9OjR2LNnD9q0aaOyLyMjA2+++SbS09MRGRkJW1tbtc5pZGSE+vXrIy0trdQ2BgYGMDAwqFD2uiIpM6lS2xERUe1w586dYj0OS+Pi4oI9e/ZUcSJ62YVHF/As9xnqG9VH24ZtxY5DRERVrEYVHxo2bAh/f3+xY5SLXC5HQUEBMjIyVLbn5OTgnXfewe3btxEeHg43Nze1z5mZmYknT57A2tq6suPWSeous8nlOImItMvz58+LTd5cr149XL9+HU2aNFHZbmZmhufPn1dnvDovPK6wh2fvpr2hI9SozrhERFQF6tS/9Pfv38etW7c0OjY9PR0vXrwotn3Lli0AgI4dOyq35efnY9iwYTh37hx++uknlQkqX5aTk4PMzMxi27/66isoFAoMGDBAo6ykytPBE3ZmdhBQ8hwhAgTYm9nD08GzmpMREVFVsrCwQFKSaq82HR0dtGzZEsbGxirbk5OTYW5uXp3x6ryi4kOfJhxyQURUF9Song+aWrx4MQDg5s2bAIAdO3bg9OnTAIB58+Yp2/n5+eHkyZMqs1tnZGRg3bp1AIAzZ84AANavXw8LCwtYWFhg8uTJAICIiAhMmTIFPj4+cHZ2Rm5uLiIjIxEaGoqOHTti1KhRynN++umnOHDgAN555x2kpaVh586dKnmL2iYnJ6Ndu3YYMWIEXFxcAACHDx/GH3/8gQEDBmDw4MGV90uqwyQ6EqwZsAY++3wgQFCZeLKoILF6wGpONklEpGVatWqFI0eOYPbs2a9se+TIEbRq1aoaUhEAyOQynH94HgDneyAiqisExcufxGupsla9ePnpeXl5FSs+JCQkFOt6WaRx48ZISEgAANy9exeLFi3C6dOnkZSUBIVCAScnJ/j4+OCzzz6DiYlJseu8KlN6ejo++eQTnD9/HomJicjPz0ezZs0wcuRIzJgxA3p6emo9f6BwqU1zc3NkZGTAzMxM7ePqktCYUEwNm6oy+aS9mT1WD1iNd13exaWkS+ho27GMMxAR1Q3ack/ZsGEDPvnkE+zfvx+DBg0qtd0vv/yC9957D+vXr8fHH39cjQnFI/ZrfC35Gt758R3oS/Tx95S/q/36RERUOcpzP9GK4gOJ/yaitsgvyEfk/UgkZSbBRmoDTwdPFCgKMO7AOPx4/Uf8/v7v6N+sv9gxiYhEpS33FLlcjnbt2iEuLg4zZ87E+PHj0bhxY+X+e/fuYcuWLVixYgWaNWuGS5cu1ZnJnGvCa6xQKJCanQorYytRrk9ERBVXnvuJVgy7IFKXREcCL0cvlW06Ch3oCDrIV+Rj6E9DcXb8Wbg3cBcnIBERVRoDAwMcPHgQb731FhYvXoyvv/4aZmZmMDMzg0wmg0wmg0KhgIuLCw4ePFhnCg81hSAILDwQEdUhdWrCSaKSCIKAzW9vRo/GPZCZm4m3dr+F5GfJYsciIqJK0LRpU1y5cgVr1qxB9+7dIZFIkJSUBIlEAk9PT6xduxaXL1+Go6Oj2FHrjNz8XOQX5Isdg4iIqhmHXWiJmtB9srZLy05Dt63dcDv1NjrZdkKEfwSM9YxffSARkZbhPUX7ifkab7+2HdPCpuGjjh9hSe8l1XptIiKqXOW5n7DnA9H/Z2lkid/f/x31jerjz8Q/4bffDwWKArFjERERaZXwuHA8zXkqdgwiIqpmLD4QvaSZZTP8MvwX6Ev08Wvsr4hKjBI7EhERkdZQKBQIjwsHwCU2iYjqGk44SfQf3R26Y/uQ7bA2sUbnRp3FjkNERKQ1olOikfQsCYa6hvCw9xA7DhERVSMWH4hKMMx9mMpjhUIBQRBESkNERKQdino99GjcA4a6hiKnISKi6sRhF0SvEJMSgw6bO+Dm45tiRyEiIqrVwuP//5CLJhxyQURU17D4QPQKs8Jn4UryFby1+y388+wfseMQERHVSi/yXyAiIQIA53sgIqqLWHwgeoWgwUFoZtkM9zLuYfCewch+kS12JCIiolonJy8HAV0DMKDZALRp2EbsOEREVM1YfCB6hfrG9fH7+7/D0sgSFx5dgN8vXIKTiIiovKQGUizqtQiHRh6CjsC3oEREdQ3/5SdSQ/P6zbF/2H7o6eghJDoEc4/NFTsSERERERFRrcHiA5GaejTugS2DtgAAlp1Zhh+v/yhyIiIiotrhWe4z/HLrF2TkZIgdhYiIRMKlNonKwa+NH/5O+xsn751EP6d+YschIiKqFU7En8C7e99Fi/otcGvyLbHjEBGRCFh8ICqnhV4L8aLgBfQl+mJHISIiqhXC4wqX2Ozl2EvkJEREJBYOuyAqJ0EQVAoPwVeD8fj5YxETERER1WxH444CAPo69RU5CRERiYXFB6IKWHFmBfx/9ecSnERERKV4JHuEmCcxECCw5wMRUR3G4gNRBQxqMQj1DOvh/MPz8P/Vn0twEhER/cex+GMAgI62HVHPqJ7IaYiISCwsPhBVQAurFggdFgo9HT3su7kP84/PFzsSERFRjaIcctGUQy6IiOoyFh+IKsjL0Qvfv/M9AGDJ6SUIuhIkciIiIqKaQaFQKCeb7NO0j8hpiIhITCw+EFWCMW3HYK7nXADAhIMTcCL+hMiJiIiIxCcIAs6MO4NNb21CN/tuYschIiIRsfhAVEkW9VqE4e7DkVeQhxuPb4gdh4iIqEZoWq8pJnacCENdQ7GjEBGRiHTFDkCkLXQEHQQNDsK4tuNUlhLLL8hH5P1IJGUmwUZqA08HT0h0JCImJSIiIiIiql4sPhBVIkNdQ5XCw66/dmFW+Cw8ynyk3GZnZoc1A9bA29VbjIhERETV4kX+C4zaPwo9HHrgg/YfwEDXQOxIREQkIg67IKoimy9txv9r787joir3P4B/hn2bQdkEBQRlEUUSVyQRLHPJBUVyX1C6roly00yve26liVrXLU3MbFETU7v5U0vcQtAbdrVU3FBAMETZdzi/P2gmxwEBmeHA+Hnf17xynvOcc74PM5f58p3nOWds1FilwgNQcb/z4H3BOHjtoEiRERERaV5sSiz2/b4Py04vg76uvtjhEBGRyFh8INKAsvIyLPx5YaXbBAgAgNnHZqOsvKw+wyIiIqo3J25X3GLz9VavQ0fClJOI6GXHTwIiDTh7/yzS89Or3C5AQFJ2Es7eP1uPUREREdWfk3f/usWmM2+xSUREWlB8SE1Nxfvvv49evXpBKpVCIpEgOjq6xvvfuHED4eHh8PX1hZGRESQSCRITEyvtGx4ejo4dO8LCwgImJibw8PDA0qVLkZubq9QvOjoaEomk0seFCxdUjvvLL7+gR48eMDExga2tLcLCwlSOSY1Lak6qWvsRERE1JtlF2YhNjgUApWshERHRy6vRX3Dyxo0b+PDDD+Hq6or27dsjJiamVvvHxMRg06ZNaNu2LTw8PHD58uUq+168eBF+fn6YOHEijIyMEB8fjzVr1uDkyZM4c+YMdHSUazlhYWHo0qWLUpuLi4vS88uXL+P111+Hh4cH1q9fj+TkZKxbtw43b97Ejz/+WKuxaNLs2bOxcePG5/bR0dHBkydPIJPJ6imqhstOaqfWfkRERI1JdGI0yoQyuFq4wtHcUexwiIioAWj0xYdOnTohIyMDFhYWOHDgAN56661a7T948GBkZmZCKpVi3bp1zy0+nDt3TqWtdevWmDNnDuLi4uDj46O0zc/PD8HBwc89/4IFC9C0aVNER0cr/mh3cnLCP/7xDxw/fhx9+vSp1Xg05bfffgMADBw4EJaWlpX2sbKyYuHhL36OfrCX2SMlO0VxjYenSSCBvcwefo5++CP9DzjIHCA1lIoQKRERkfqdvPPXkotWXHJBREQVGn3xQSqt2x9sFhYWddrfyckJAJCZmVnp9pycHBgbG0NPT/VHnZ2djRMnTiA8PFzpj/bx48cjPDwc+/bta3DFhx07dqBZs2YiR9Pw6eroYmO/jQjeFwwJJEoFCAkkAIAN/TYAAN7a/xbSctMwp/sczOw2E2YGZmKETEREpDZZRVnQ09HDG6245IKIiCo0+ms+1LfS0lI8evQIDx48wPHjx7Fw4UJIpVJ07dpVpe/EiRMhk8lgZGSEXr164dKlS0rbr1y5gtLSUnTu3Fmp3cDAAB06dEB8fLxGx1JT9+/fx5MnT2BhYcHCQy0EeQThwPADaCFrodRuL7PHgeEHEOQRhJScFJSWl+JxwWMs+HkBnDc646PzHyGvOE+kqImIiOpu95DdePzeY/R37S92KERE1EA0+pkP9e3SpUvo3r274rm7uzsOHz6sNIPCwMAAw4YNw5tvvgkrKyv88ccfWLduHfz8/PDLL7/A29sbQMXFMgHAzk513b+dnR3Onq36TghFRUUoKipSPM/Ozq7z2KoiX4ri4eGhsXNoqyCPIAS6B+Ls/bNIzUmFndQOfo5+0NXRBQA4mjvi9+m/4+srX2P5meW49fgW5p2ch3W/rMN7r76H6V2mw0TfRORREBER1R6XExIR0dMaVPGhvLwcxcXFNepraGgIiUSi4YhUtW3bFidOnEBeXh5++eUXnDx5UuXOFL6+vvD19VU8Hzx4MIKDg+Hl5YX58+fj2LFjAICCggIAFWN5lpGRkWJ7ZVavXo1ly5apY0jVki+5aNu2bb2cT9vo6ugiwCmgyu16OnoY98o4jGo/Cnv/txcfnPkAt5/cxtwTc+Fp44l+Lv3qL1giIqI6KiwthJGekdhhEBFRA9Ogll2cOXMGxsbGNXrcuHFDlBhlMhl69+6NwMBAfPjhh3j33XcRGBio+AO9Ki4uLggMDMSpU6dQVlYGADA2NgYApRkMcoWFhYrtlZk/fz6ysrIUj6SkpDqM6vnkMx9YfNAsPR09TOgwAddmXMPngz/HSM+R6Nu6r2J7XEocCkqqLkgRERGJTRAEtN7UGl0/64rEzESxwyEiogakQc18aNOmDXbt2lWjvpUtVRBDUFAQxo0bh2+++QavvPLKc/s6ODiguLgYeXl5kMlkijHIl188LTU1Fc2bN6/yWIaGhpXOmNAEeWElPDwc4eHhlfaZP38+Vq1aBQAICQnB7t27VfqsXbsWc+bMUTzfu3cvtm7dit9++w3FxcVwdXXF8OHDER4eDjMzM4SEhODWrVuV3mVEm+nr6mOi90RM9J6oaMsszETfL/vCWM8Y7/d4H5M7Tea3SkRE1OD8nv47HuQ8wJOCJ7Azaxi5GhERNQwNqvhga2uLkJAQscOolaKiIpSXlyMrK6vavnfu3IGRkRHMzCruZuDp6Qk9PT1cunQJw4cPV/QrLi7G5cuXldrEkpOTgzt37kAikWD8+PFV9hs0aJDScy8vL2zbtk2prWXLlop/T58+HZ999hkmT56M+fPnw9jYGL/++is2bdqE8vJyLFmyRL0DaeRuP74NmaEM97PuY9axWfjw/IeY32M+3u74tkoRoqy8rMprTBAREWmS/BabPVv2hKFe/XxJQkREjUODKj5o2v3795Gfn482bdrUet/MzEyYmppCX19fqX3Hjh0AoHTHivT0dFhbWyv1++2333D48GH0798fOjoVq13Mzc3Ru3dvfPnll1i0aJHitqF79uxBbm4u3nrrrVrHqW7/+9//IAgCnJ2dERkZWeP9pFIpfHx8Kt0WFRWFLVu2YO/evRg9erSivVevXpg6darKXUEI6NS8E27OvIld8buw8uxKJGUnYeaPM7Hm3Bos8FuAUO9QGOoZ4uC1g5h1bBaSs5MV+9rL7LGx30YEeQSJOAIiInoZnLhzAgDQu1VvkSMhIqKGRiuKDytWrAAA/P777wAq/niXT9VfuHChot/48eNx+vRpCIKgaMvKysInn3wCADh//jwA4NNPP0WTJk3QpEkTvPPOOwCA6OhohIWFITg4GK6uriguLsbZs2dx8OBBdO7cGWPHjlUcc8SIETA2Noavry9sbGzwxx9/YPv27TAxMcGaNWuUYl+5ciV8fX3h7++PyZMnIzk5GR9//DH69OmDfv3Ev9Cg/HoPHTp0UNsxN27ciK5duyoVHuRMTU3h7++vtnNpEwNdA0zpPAUhHULwefznWHVuFZKzkxH2YxjeaPUGrvx5BcH7giFAUNovJTsFwfuCFbf3JCIi0oTismKcTjwNAHij1RsiR0NERA2NVhQfFi1apPT8888/V/z76eJDZZ48eaKy/8cffwygYpmAvPjQvn179OrVC99//z1SU1MrLqjUujUWL16MuXPnwsDAQLH/kCFDsHfvXqxfvx7Z2dmwtrZGUFAQlixZAhcXF6VzdezYESdPnsS8efMQHh4OqVSK0NBQrF69uvY/CA2QX++huutZVKa0tFTxb4lEAl1dXZSUlODChQuYO3eu2mJ82RjqGWJal2mY5D0JO37dgftZ99GqaSu89sVrKoUHABAgQAIJZh+bjUD3QC7BICIijYhNjkVeSR6sTazRvll7scMhIqIGRiuKD0/PZHie6OholTYnJ6ca7d+6detKL6JYmbCwMISFhdWoLwD06NFDMeuioZHPfKht8eH8+fNKS1R0dXVRWlqKjIwMFBUVwcHBQZ1hvpQM9Qwxo+sMAEB0YrTSUotnCRCQlJ2Es/fPPve2n0RERC9KvuTi9VavQ0fSoG6oRkREDYBWFB9IM8rKynD16lUAtS8+vPLKK4rrYQAVMx9Ic1JzVO+YUpnY5FgWH4iISCO6tuiKEe1GYIj7ELFDISKiBojFB6pSQkICCgoKAADLli2rsp+Pjw+mTp2q1GZmZqZ0EU45S0tLGBgYIDm56m/pqfbspDW7ndnTVx6PTozGz3d/ho+9D7q16AZLE0tNhUdERC+BgW4DMdBtoNhhEBFRA8XiA1VJfr0HAM9dcuLq6lrjY+rr66N79+44ceIEli9fXqf46G9+jn6wl9kjJTul0us+AIDUQIqJHSYqnh+6fggbYzcqnrtYuKBbi27wsfeBj70POth2gJ5O9b8ieGtPIiIiIiKqDhfkUZVGjhwJQRCqffzrX/+q1XFnzZqFCxcu4Ntvv1XZlp+fjzNnzqhrCC8NXR1dbOxXUUiQQHmJi+Sv/0UOiYS5kbmiPcApABNemQB3S3cAwK3Ht7D3yl7M/HEmunzWRWkpx7X0a7ifdV/l+igHrx2E00Yn9NrdC6MPjkav3b3gtNEJB68d1NRQiYioATqdeBrX0q/V+DpcRET08uHMB6p3Q4cOxbRp0zBu3DicO3cOAwYMgJGRES5fvoyNGzciJCQEPXv2FDvMRifIIwgHhh/ArGOzlC4+aS+zx4Z+G1RuszmkzRAMaTMEAPCk4AniUuIQmxKLC8kXkJSdBHuZvaLvvJPzcCThCGzNbCtmRrTwQVFZEZZGL633W3typgURUcMz9YepuP7oOr4f+T0Guw8WOxwiImqAWHwgUWzevBmvvvoqNm/ejMjISJSUlMDNzQ1vv/02Zs+eLXZ4jVaQRxAC3QNr/cd5U+Om6OvSF31d+la6vbisGLoSXaTlpuHQ9UM4dP1QlceS39pz1rFZar+158FrBystrmzst1EjhQ45FjyIiKqWnJ2M64+uQ0eiAz9HP7HDISKiBkoicH6cVsjOzoa5uTmysrIgk8nEDoe0UH5JPv774L+ITYnFkRtHcOZ+9ctjTPVN4WDuAFszW9ia2aK9TXss8Fug2H4z4yZkhjJYmVhV+8f8wWsHEbwvWGWmhXyZiaZmWohR8KjvYocYxRVtH2NjPx8/U15MUVERFi9ejD179uDJkyfw8vLCihUr8MYbb1S7b0pKCsLDw3H8+HGUl5ejV69eiIiIQKtWrVT67ty5E+vWrcPdu3fh4OCAsLAwzJw5s1axquM1lr/vvrn6Dbb9dxu6NO+CuH/EvdCxiIiocarN5wmLD1qCiSLVp6+vfI3RB0fXer9XHV7FuUnnFM8dIhyQnJ0MHYkObExtFEUKeaHin93/CaAiwbWPsEdablqlx5VAAnuZPe7Ouqv2mRb1XfCo72KHGMUVbR+jNpyPnykvZtSoUThw4ABmz54NV1dXREZG4uLFizh16hR69OhR5X65ubno2LEjsrKy8O6770JfXx8REREQBAGXL1+GpeXfdyPatm0bpk6dimHDhqFv3744e/Ys9uzZgzVr1mDevHk1jrWur3Fl7zupgRSRQyI1OhONiIgaFhYfXkJMFKk+RSdGo9fuXtX2iwyMhKO5I9Jy05CWmwYrEyuMe2UcAEAQBDhucKzyDh09HHvg7MSztTrfqQmnEOAUgA0XNqC4rBim+qYw0TeBqUHFf030TWBpbIlXbF9R7JNXnAdDPUOVO3uUlZfBaaOTUmL9NE0UPOq72CFWcUWbx6gt5+NnSu3FxcWhW7duWLt2LebMmQMAKCwshKenJ2xsbPDLL79Uue9HH32EefPmIS4uDl26dAEAXL9+HZ6ennjvvfewatUqAEBBQQEcHBzg4+ODo0ePKvYfO3YsDh06hKSkJDRt2rRG8dblNa7qfQdUvPc0NRONiIgaHhYfXkJqSxQFAcjPV19gpJXKysvg8W8PPMh5UOmNPSUAWkhb4I8Zf1T7h3lpeSke5T3Cw7yHeJj7UPFfGzMbjH9lPADg26vfYtLh0Grj2jX4cwz3HI5WG1vhYd6flfbxatYeMaExfz/f4oXbT+7AQFcfJvomMNYzhqm+KcqFctzJvFvtOYe1CYJTEydYmlhils8sRfv+3/cjoyADejp60NfRh56OnuJhom+C/q79FX2vPLyCrMIsjDs0Dn/mpVd6HvnP9MzEMygXyqEj0YFEIoGOREfpITP8+///JWUlAKDYJpH8fScU+WuYkvPgueeryWtYU/V9zpf1fPn6FSerS4GMxYfae++997B+/Xo8fvxY6We2evVqLFiwAPfv34eDg0Ol+3bt2hVARQHjaX379sXt27dx69YtAMB//vMfDBgwAD/88APefPNNRb+YmBj4+vpiz549GDt2bI3ifdHXWIzCLBERNVy1+TzhBSdJWX4+YGYmdhTUwOkCSKi2Vwow37zaXnoAbP96qJoOABjx16NaqyYBmIQ7z+10BZj193v8f4p/lQDI+utRG0/fVvTv286+VYsjtP/rv9WXOlKA+a1rfFz952xT52tYU/V9zpf1fKYLgHyDiou/JmUn4ez9swhwClDLOalq8fHxcHNzU0m85IWFy5cvV1p8KC8vx//+9z9MmjRJZVvXrl1x/Phx5OTkQCqVIj4+HgDQuXNnpX6dOnWCjo4O4uPjqyw+FBUVoaioSPE8Ozu7dgP8y9n7Z6ssPAB83xERUdV0xA6AiIiINCc1J1XsEF4KqampsLOzU2mXtz14UPmMmMePH6OoqKhG+6ampkJXVxc2NjZK/QwMDGBpaVnlOYCKGRjm5uaKR1WzMKpT0/cT33dERPQsznwgZSYmQG6u2FFQI1JWXobzSeeRlpMGW6ktXnV4VSNTbb+//j3GHBwDAEpLPeQLCfYG7UVgm0C1nU+dS0tq4sy9M+i/981q+/045j/o2bKn4rkgCCgXyhUPQz1Dxbb8knwUlxUrbZf3j0mKwbhD46s937aBW9HJrhPcLN0U43yQ/QAZBRkV54cAQRAUa78FQUA7m3Yw0DUAANzPvI+0vIoLhcanxuOfx9+t9pwf9f4QXs28FMeX87b1htRQqjjuncy/57g8u4Kwg20HXPnzSo1+pt8O+wYD3QcCAFKyU/BH+h9V9vVq5oVmZs0AVPxxdeXhFcW2K39eweLoJdWeb5n/UnjaeCq1tbVuC8cmjgCAR/mPcDHlYpX7u1u6o5VFqyrfM/nPTHmxk6r+UUvqV1BQAENDQ5V2IyMjxfaq9gNQo30LCgpgYGBQ6XGMjIyqPAcAzJ8/H//85z8Vz7Ozs1+oAFHT9xPfd0RE9CwWH0iZRAKYmoodBTUiugB6tu1fbb+6Cuw0Gl+YGKlcXd1B5oAN/TYgUM0XN9MFsCbwEwTvCwag/Eew/GJ+qwM3QVeqnvXwr7bpAwsr+yovwClfR/1qmz7AU8UOyV+xVlb+MIEpTKo4X6CNMyzOLaj2fKO6va1SXGlu6ormcK3RuBxNPeAIDwBAJ5eeWPFrRLXnnOr/brUFHUdTDzi28Hhun1ct7Gr0M+3fIVjxM21h6oYWdm41GpudqQvsbF0Uz18vH4wNVz+r9nzhvRc+d3xWpqbob92y2vPX9D3j5+hXo/FQ3RgbGysta5ArLCxUbK9qPwA12tfY2BjFxcWVHqewsLDKcwAVxY3KChy15efoB3sZ33dERFR7XHZBRI1GkEcQEmcl4tSEU/gq6CucmnAKd2fd1dhV1YM8gnBg+AG0kLVQareX2av9au66OrrY2G8jgL+LG3Ly5xv6bVDbrJL6Pp8Y5+T51P8aUtXs7OyQmqq61EDe1rx580r3s7CwgKGhYY32tbOzQ1lZGf78U/mCusXFxcjIyKjyHOrE9x0REb0oFh+IqFHR1dFFgFMARrUfhQCnAI0nuPVZ8KjPYocY5xPjnDwfb3dYXzp06ICEhASVCznGxsYqtldGR0cH7du3x6VLl1S2xcbGolWrVpBKpUrHeLbvpUuXUF5eXuU51I3vOyIiehG81aaW4G3RiLRHWXkZzt4/i9ScVNhJ7eDn6KfRIkt9n0+Mc/J8tcPPlNqLjY2Fj48P1q5dizlz5gCoWErh6ekJS0tLXLhwAQBw//595Ofno02bNop9P/zwQ7z//vu4ePGi4k4WN27cQLt27TBnzhysWbMGQMU1H+zt7eHr64sjR44o9h83bhwOHjyIpKQkWFhY1ChedbzGYvzuICKihqU2nycsPmgJJopERKQu/Ex5McOHD0dUVBTCw8Ph4uKC3bt3Iy4uDj/99BN69qy4UGxAQABOnz6tdJHUnJwceHt7IycnB3PmzIG+vj7Wr1+PsrIyXL58GdbW1oq+mzdvxowZMxAcHIy+ffvi7Nmz+OKLL7By5UosWLCgxrHyNSYiInWozecJLzhJREREpAZffPEFFi1ahD179uDJkyfw8vLC0aNHFYWHqkilUkRHRyM8PBwrVqxAeXk5AgICEBERoVR4AIDp06dDX18fH3/8MQ4fPgwHBwdERERg1qxZmhwaERFRnXHmg5bgNxhERKQu/EzRfnyNiYhIHWrzecILThIRERERERGRRrH4QEREREREREQaxeIDEREREREREWkUiw9EREREREREpFEsPhARERERERGRRrH4QEREREREREQapSd2AKQe8jumZmdnixwJERE1dvLPEt6NW3sxbyAiInWoTc7A4oOWyMnJAQA4ODiIHAkREWmLnJwcmJubix0GaQDzBiIiUqea5AwSgV9raIXy8nI8ePAAUqkUEomkTsfKzs6Gg4MDkpKSIJPJ1BRhw6Ht4wO0f4zaPj6AY9QGjXl8giAgJycHzZs3h44OV2hqI3XlDY35fV5THGPjp+3jA7R/jNo+PqDxjrE2OQNnPmgJHR0d2Nvbq/WYMpmsUb3xa0vbxwdo/xi1fXwAx6gNGuv4OONBu6k7b2is7/Pa4BgbP20fH6D9Y9T28QGNc4w1zRn4dQYRERERERERaRSLD0RERERERESkUSw+kApDQ0MsWbIEhoaGYoeiEdo+PkD7x6jt4wM4Rm2g7eMjAl6O9znH2Php+/gA7R+jto8PeDnGyAtOEhEREREREZFGceYDEREREREREWkUiw9EREREREREpFEsPhARERERERGRRrH4QEREREREREQaxeIDKRQVFWHevHlo3rw5jI2N0a1bN5w4cULssNTi4sWLeOedd9CuXTuYmprC0dERw4cPR0JCgtihaczKlSshkUjg6ekpdihq9euvv2Lw4MGwsLCAiYkJPD09sWnTJrHDUpubN29i5MiRsLe3h4mJCdq0aYPly5cjPz9f7NBqLTc3F0uWLEG/fv1gYWEBiUSCyMjISvteu3YN/fr1g5mZGSwsLDBu3Dikp6fXb8C1VJPxlZeXIzIyEoMHD4aDgwNMTU3h6emJFStWoLCwUJzAidRAm3MGgHmDNmHe0Hgwb9D+vIF3uyCFUaNG4cCBA5g9ezZcXV0RGRmJixcv4tSpU+jRo4fY4dVJcHAwzp8/j7feegteXl5IS0vDp59+itzcXFy4cEHrPmiTk5Ph7u4OiUQCJycnXL16VeyQ1OL48eMYNGgQvL29MWLECJiZmeH27dsoLy/HRx99JHZ4dZaUlAQvLy+Ym5tj6tSpsLCwQExMjOJD6Pvvvxc7xFpJTEyEs7MzHB0d0apVK0RHR2PXrl0ICQlR6pecnAxvb2+Ym5sjLCwMubm5WLduHRwdHREXFwcDAwNxBlCNmowvNzcXUqkUPj4+GDhwIGxsbBATE4Pdu3ejZ8+e+PnnnyGRSMQbBNEL0uacAWDewLyhcWDewLyh0RGIBEGIjY0VAAhr165VtBUUFAitW7cWunfvLmJk6nH+/HmhqKhIqS0hIUEwNDQUxowZI1JUmjNixAjhtddeE/z9/YV27dqJHY5aZGVlCc2aNROGDh0qlJWViR2ORqxcuVIAIFy9elWpffz48QIA4fHjxyJF9mIKCwuF1NRUQRAE4eLFiwIAYdeuXSr9pk2bJhgbGwv37t1TtJ04cUIAIGzbtq2+wq21moyvqKhIOH/+vMq+y5YtEwAIJ06cqI9QidRK23MGQWDeoA2YNzBvaGiYNwgCl10QAODAgQPQ1dXF5MmTFW1GRkYIDQ1FTEwMkpKSRIyu7nx9fVWqoK6urmjXrh2uXbsmUlSacebMGRw4cAAbNmwQOxS1+uqrr/Dw4UOsXLkSOjo6yMvLQ3l5udhhqVV2djYAoFmzZkrtdnZ20NHRabCV/KoYGhrC1ta22n7fffcdBg4cCEdHR0Vb79694ebmhn379mkyxDqpyfgMDAzg6+ur0j506FAA0LrfP/Ry0PacAWDeoA2YNzBvaGiYN/CaD/SX+Ph4uLm5QSaTKbV37doVAHD58mURotIsQRDw8OFDWFlZiR2K2pSVlWHmzJl4++230b59e7HDUauTJ09CJpMhJSUF7u7uMDMzg0wmw7Rp07RiDRwABAQEAABCQ0Nx+fJlJCUl4dtvv8WWLVsQFhYGU1NTcQPUgJSUFPz555/o3LmzyrauXbsiPj5ehKg0Ly0tDQC06vcPvTxexpwBYN7Q2DBvYN6gTbQlb2DxgQAAqampsLOzU2mXtz148KC+Q9K4vXv3IiUlBSNGjBA7FLXZunUr7t27hw8++EDsUNTu5s2bKC0tRWBgIPr27YvvvvsOkyZNwtatWzFx4kSxw1OLfv364YMPPsCJEyfg7e0NR0dHjBw5EjNnzkRERITY4WlEamoqAFT5++fx48coKiqq77A07qOPPoJMJkP//v3FDoWo1l7GnAFg3tDYMG9g3qBNtCVv0BM7AGoYCgoKYGhoqNJuZGSk2K5Nrl+/jhkzZqB79+6YMGGC2OGoRUZGBhYvXoxFixbB2tpa7HDULjc3F/n5+Zg6dariKtVBQUEoLi7Gtm3bsHz5cri6uoocZd05OTmhZ8+eGDZsGCwtLfHDDz9g1apVsLW1xTvvvCN2eGon/91S3e+fyrY3VqtWrcLJkyexefNmNGnSROxwiGrtZcsZAOYNjRHzBuYN2kKb8gYWHwgAYGxsXGmVUD4tzdjYuL5D0pi0tDQMGDAA5ubminWr2mDhwoWwsLDAzJkzxQ5FI+TvwVGjRim1jx49Gtu2bUNMTEyjTyK++eYbTJ48GQkJCbC3twdQkSiVl5dj3rx5GDVqFCwtLUWOUr3kr+vL8vvn22+/xcKFCxEaGopp06aJHQ7RC3mZcgaAeUNjxbyBeYM20La8gcsuCEDFNCX5NKanyduaN29e3yFpRFZWFvr374/MzEwcO3ZMa8Z18+ZNbN++HWFhYXjw4AESExORmJiIwsJClJSUIDExEY8fPxY7zDqRv1bPXlTJxsYGAPDkyZN6j0ndNm/eDG9vb0UCITd48GDk5+dr5TpG+bTJqn7/WFhYaM23FydOnMD48eMxYMAAbN26VexwiF7Yy5IzAMwbGjPmDcwbGjttzBtYfCAAQIcOHZCQkKC4aq5cbGysYntjV1hYiEGDBiEhIQFHjx5F27ZtxQ5JbVJSUlBeXo6wsDA4OzsrHrGxsUhISICzszOWL18udph10qlTJwAVY32afG2xNkwZffjwIcrKylTaS0pKAAClpaX1HZLGtWjRAtbW1rh06ZLKtri4OK343QNU/C4dOnQoOnfujH379kFPjxMPqfF6GXIGgHkD84aGj3mDMuYNDR+LDwQACA4ORllZGbZv365oKyoqwq5du9CtWzc4ODiIGF3dlZWVYcSIEYiJicH+/fvRvXt3sUNSK09PT0RFRak82rVrB0dHR0RFRSE0NFTsMOtk+PDhAICdO3cqte/YsQN6enqKKz43Zm5uboiPj0dCQoJS+9dffw0dHR14eXmJFJlmDRs2DEePHlW6Pd9PP/2EhIQEvPXWWyJGph7Xrl3DgAED4OTkhKNHj2rVdFB6OWl7zgAwb2De0Dgwb2De0NhIBEEQxA6CGobhw4cjKioK4eHhcHFxwe7duxEXF4effvoJPXv2FDu8Opk9ezY2btyIQYMGKT6MnjZ27FgRotK8gIAAPHr0CFevXhU7FLUIDQ3F559/juHDh8Pf3x/R0dHYv38/5s+fj1WrVokdXp2dOXMGr732GiwtLfHOO+/A0tISR48exY8//oi3334bn332mdgh1tqnn36KzMxMPHjwAFu2bEFQUBC8vb0BADNnzoS5uTmSkpLg7e2NJk2aYNasWcjNzcXatWthb2+PixcvNujpk9WNT0dHB+3atUNKSgpWrVqFFi1aKO3funVrrfujhl4O2pwzAMwbmDc0DswbmDc0OgLRXwoKCoQ5c+YItra2gqGhodClSxfh2LFjYoelFv7+/gKAKh/ayt/fX2jXrp3YYahNcXGxsHTpUqFly5aCvr6+4OLiIkRERIgdllrFxsYK/fv3F2xtbQV9fX3Bzc1NWLlypVBSUiJ2aC+kZcuWVf7/7u7du4p+V69eFfr06SOYmJgITZo0EcaMGSOkpaWJF3gNVTe+u3fvPvd3z4QJE8QeAtEL0eacQRCYN2gL5g2ND/MG7c4bOPOBiIiIiIiIiDSK13wgIiIiIiIiIo1i8YGIiIiIiIiINIrFByIiIiIiIiLSKBYfiIiIiIiIiEijWHwgIiIiIiIiIo1i8YGIiIiIiIiINIrFByIiIiIiIiLSKBYfiIiIiIiIiEijWHwgohcWEhICiUSikWNLJBKEhIRo5Ng1ERkZCYlEgujoaNFiICIi0hbMGYiIxQeiRi47OxsffPABOnbsCKlUChMTE7Rt2xZz587Fw4cP63z8yMhIbNiwoe6BNkDR0dFYunQpMjMzxQ6lTjZs2IDIyEixwyAiogaOOcOLY85AVHcSQRAEsYMgoheTkJCAvn374t69ewgKCkKvXr2gr6+PCxcu4Msvv4RMJsORI0fQvXv3Fz5HQEAAEhMTkZiYqLKtpKQEZWVlMDIyqsMoKldYWAhdXV3o6+ur/dhyS5cuxbJly3D37l04OTkpbSsrK0NJSQkMDAygo9Ow67ROTk5wcnLiNy5ERFQl5gx1w5yBqO70xA6AiF5Mfn4+Bg0ahJSUFBw5cgQDBgxQbJs8eTKmT5+O3r17IzAwEFeuXEGzZs3UHoO+vr7GPug1kZzUhq6uLnR1dUWNgYiISB2YM2gWcwaimmnYpTkiqtLOnTuRkJCA2bNnKyURcp07d8aqVauQnp6OtWvXKtqjo6MhkUgQGRmJTz75BG5ubjAyMoKbmxs++eQTpWM4OTnh9OnTuHfvHiQSieIhr5ZXtn5T3paRkYGQkBBYWVlBKpViyJAhSEtLAwBs374dHh4eMDIyQps2bfD999+rxP/s+k35cat6yF2/fh3Tp09Hu3btFFNKO3XqhB07dqjEuWzZMgCAs7Oz4jhLly4FUPX6zUePHmHGjBlwcHCAgYEBHBwcMGPGDGRkZCj1k+//888/Y926dWjdujUMDQ3h5uaG3bt3q4y3MuXl5diwYQO8vLwglUohk8ng7u6O0NBQlJSUKH5O9+7dw+nTp5V+Hk9/63Tp0iUMHToUVlZWMDQ0hLu7O1auXInS0lKl8wUEBMDJyQl37txBYGAgzM3NIZPJMHToUNy5c6dGMRMRUcPDnIE5g/znxJyBxMSZD0SN1IEDBwBUfGNRlZCQEMyePRvfffcd1q1bp7Ttk08+QVpaGqZMmQKpVIqvv/4aYWFhePz4MZYsWQKgYl3g/Pnz8ejRI0RERCj29fDwqDa+fv36wd7eHsuXL8etW7ewadMmDB06FEFBQdi+fTtCQ0NhZGSETZs2ITg4GAkJCXB2dq7yeFOmTEHv3r2V2jIyMjB37lw0bdpU0RYdHY0zZ85g4MCBcHZ2Rl5eHvbv349//OMfSE9Px/z58xXHy87ORlRUFCIiImBlZQUA8PLyqjKGrKws+Pr64tatW5g0aRI6duyI+Ph4bNmyBT///DPi4uIglUqV9lmwYAEKCgowZcoUGBoaYsuWLQgJCYGLiwteffXV5/4MV65cicWLF2PQoEGYOnUqdHV1cffuXRw+fBhFRUXQ19fHnj17EB4eDisrK/zrX/9S7GttbQ0A+OGHHxAUFAQXFxe8++67sLCwQExMDBYvXozLly9j//79SufMy8tDQEAAunXrhtWrV+PmzZvYvHkzLly4gPj4eNja2j43ZiIianiYMzBnYM5ADYJARI2ShYWFIJVKq+3Xvn17AYCQk5MjCIIgnDp1SgAgmJmZCUlJSYp+RUVFQpcuXQQ9PT2ldn9/f6Fly5aVHnvChAnCs79G5G3Tp09Xag8PDxcACA4ODkJWVpai/bfffhMACO+//75SfwDChAkTqhxXUVGR4OfnJxgZGQkxMTGK9tzcXJW+ZWVlgr+/vyCTyYTi4mJF+5IlSwQAwt27d1X22bVrlwBAOHXqlKJtwYIFAgDh3//+t1LfTz/9VAAgLFy4UGX/Dh06CEVFRYr25ORkwcDAQBg5cmSVY5Pz9vYWPDw8qu3XsmVLwd/fX6W9oKBAaNasmeDn5yeUlJQobVu/fr3K+Pz9/QUAwqxZs5T6Hjx4UAAgTJkypdpYiIio4WHOwJxBjjkDiYnLLogaqezsbJibm1fbTyaTAaiowD9tzJgxsLe3Vzw3MDBAeHg4SktLceTIkTrHN3v2bKXnfn5+AIDx48crYgIqvjWQyWS4efNmrY4fGhqKc+fOITIyEj4+Pop2U1NTxb8LCwuRkZGBx48fo0+fPsjOzsb169dfYDQVoqKiYG1trfLN0ZQpU2BtbY2oqCiVfaZPnw4DAwPF8xYtWsDNza1G4zU3N0dKSgrOnTv3QvGeOHECDx8+xMSJE5GZmYlHjx4pHm+++SYA4Pjx4yr7vf/++0rPhw4dCnd3dxw6dOiF4iAiInExZ2DOUB3mDFQfWHwgaqRkMhmys7Or7Sfv82zSUdk0yLZt2wKAWtbqtWrVSum5fJpjZdMkmzZtqrL+8XmWLVuGL7/8EsuWLcOIESOUtuXm5mLOnDlwdHSEsbExrKysYG1trZhe+OTJk9oOReHu3btwd3eHnp7yijU9PT24ublV+nN79ucAAJaWljUa76pVq2BkZAQ/Pz+0aNECY8aMwVdffYXi4uIaxXvt2jUAwKRJk2Btba30aNOmDQCo3FqtSZMmlU6T9PDwwMOHD5GXl1ejcxMRUcPBnIE5Q3WYM1B94DUfiBopT09PnDlzBrdu3YKLi0ulffLz83H9+nU4OTnBzMysXuOr6qrPVbULNbzr7969e7F06VKMGzcOixYtUtk+evRoHD16FJMnT0bPnj1haWkJXV1d/Oc//0FERATKy8trPgg1qMt4u3fvjtu3b+P//u//cOrUKZw6dQpfffUVVqxYgXPnzsHCwuK5+8vPsXbtWnTo0KHSPs2bN682DiIiatyYMzBnYM5ADQGLD0SNVFBQEM6cOYMdO3ZgzZo1lfb54osvUFJSgqCgIJVt8gr30/744w8AypX3Z69MLaZz584hNDQUfn5+KleiBoDMzEwcPXoU48aNw9atW5W2nTx5UqV/bcfWqlUr3LhxA6WlpUrfZJSWliIhIaHSbyzqyszMDMOGDcOwYcMAAJs3b8aMGTOwc+dOzJ07F0DV43B1dQVQMa302QtvVSUzMxNpaWkq32Rcu3YNNjY2SlNUiYiocWDOwJyBOQM1BFx2QdRIvf3223BxccH69etx7Ngxle2//vor5s+fD2tra8UHztP27t2L5ORkxfPi4mJERERAV1cXAwcOVLSbmZnhyZMnNf6WQVNu376NIUOGwN7eHlFRUUprIuXk3xg8G2tqamqliYf8m53Hjx/XKIYhQ4YgPT1d5VifffYZ0tPTMXTo0Bodp6YePXqk0taxY0cAyjGbmZlVOoa+ffvCxsYGa9asqXR7QUEBcnJyVNqfTUyjoqJw48YNDBkypLZDICKiBoA5A3MGOeYMJCbOfCBqpExNTXH48GH069cPAwYMwLBhwxAQEAA9PT3ExcVhz549MDMzw6FDhypdj+fm5oZu3bph6tSpkEql+Oqrr3Dx4kUsWrQIDg4Oin4+Pj44evQo3nnnHfj6+kJXVxevvfYabGxs6nO4GD16NDIyMjBt2jT8+OOPKtvHjh0LqVSKPn364Msvv4SxsTG6dOmCe/fuYdu2bXB2dlZZMym/6NS8efMwZswYGBkZwdPTE56enpXG8N5772H//v2YMWMGfv31V3h7eyM+Ph47d+6Eu7s73nvvPbWO2cPDAz4+PujWrRuaN2+O1NRUbN++HQYGBhg5cqTSOHbu3IlFixbBw8MDOjo6GDRoEExNTfHFF19gyJAhcHd3x6RJk+Di4oLMzExcv34dBw8eRFRUFAICAhTHsrKywsGDB/HgwQMEBAQobpvVrFkzxf3MiYiocWHOoIw5A3MGEolYt9kgIvXIzMwUli1bJrzyyiuCqampYGRkJLi7uwvvvvuukJqaqtJfftusXbt2CRs3bhRcXFwEAwMDwcXFRdiwYYNK/7y8PGHSpEmCjY2NoKOjo3SrpefdNut5531WZbd9wjO3zWrZsqUAoMqHXHp6uhAaGirY2dkJhoaGgqenp7B9+/ZKb4MlCILw4YcfCs7OzoKenp4AQFiyZIkgCJXfNksQBOHPP/8Upk2bJrRo0ULQ09MTWrRoIUyfPl1IT09X6lfV/oLw/FuRPW316tWCn5+fYG1tLRgYGAj29vZCcHCw8N///lep38OHD4WgoCChadOmgkQiUbkV2JUrV4QxY8YIzZs3F/T19QUbGxuhe/fuwvLly4WMjAyVuG7fvi0MHjxYkEqlgpmZmTB48GDh5s2b1cZLREQNG3MG5gzMGUhMEkEQeV4UEdWr6Oho9OrVC7t27UJISIjY4VADEhAQgMTERCQmJoodChERNQDMGagqzBnoRfCaD0RERERERESkUSw+EBEREREREZFGsfhARERERERERBrFaz4QERERERERkUZx5gMRERERERERaRSLD0RERERERESkUSw+EBEREREREZFGsfhARERERERERBrF4gMRERERERERaRSLD0RERERERESkUSw+EBEREREREZFGsfhARERERERERBrF4gMRERERERERadT/Ax7UNhJxFKaqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Challenges**"
      ],
      "metadata": {
        "id": "Bs0vbHxkhFvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Error Correction*"
      ],
      "metadata": {
        "id": "Apl61SPnakw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Calibrating large quantum processors](https://www.youtube.com/watch?v=fk2HH9M4FqA&list=WL&index=8&t=224s)"
      ],
      "metadata": {
        "id": "ixbZSt8G76I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Estimating overheads for quantum fault-tolerance in the honeycomb code](https://youtu.be/ND9OoqJ0NMw?si=y2jnMCryfqdOc4wn)\n",
        "\n",
        "video: [Pulse sequence design for crosstalk mitigation](https://youtu.be/suO1_7rZAW0?si=XSNiKQX8t23yZOe6)\n",
        "\n",
        "honeycomb: [Craig gidney: [Talk] Software and the Honeycomb Code](https://youtu.be/O3NaTGmY0Rw?si=3fp6f9ApE8Gu5sF2)\n",
        "\n",
        "honeycomb: [A short history of the honeycomb code](https://youtu.be/frnym8S5qM4?si=wrnIJ6A6_a188Xug)"
      ],
      "metadata": {
        "id": "UcLw3fA4zCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [QIP 2022 | Floquet Codes (Matthew Hastings)](https://www.youtube.com/watch?v=JTvByDNy8zE&list=WL&index=6&t=68s)\n",
        "\n",
        "video: [QIP2023 | Floquet codes without parent subsystem codes (Margarita Davydova)](https://www.youtube.com/watch?v=nq021Pw7orc&list=WL&index=7&t=40s)"
      ],
      "metadata": {
        "id": "bMmznOFT7sSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumpoet.com/superconducting-quantum-computing/"
      ],
      "metadata": {
        "id": "dSMojPGuvt7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/how-quantum-computers-will-correct-their-errors-20211116/"
      ],
      "metadata": {
        "id": "4h_aGy2Kf4UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/new-codes-could-make-quantum-computing-10-times-more-efficient-20230825/\n",
        "\n",
        "https://www.quantamagazine.org/physicists-create-elusive-particles-that-remember-their-pasts-20230509/"
      ],
      "metadata": {
        "id": "oqNWZS_EPXdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*difference between \"code space\", \"code word\" and \"stabilizer code\"?*\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/1822/what-is-the-difference-between-code-space-code-word-and-stabilizer-code"
      ],
      "metadata": {
        "id": "yRaDer6BKAp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error Correction Codes**\n",
        "\n",
        "https://www.quantamagazine.org/the-basic-algebra-behind-secret-codes-and-space-communication-20230123/"
      ],
      "metadata": {
        "id": "pvAtUauVDPN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Correction Zoo**\n",
        "\n",
        "https://errorcorrectionzoo.org/c/honeycomb\n",
        "\n",
        "https://errorcorrectionzoo.org/list/ag\n",
        "\n",
        "https://errorcorrectionzoo.org/list/homological\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0110143"
      ],
      "metadata": {
        "id": "N8zOwuitr9nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1603.02286\n",
        "\n",
        "Sometimes transversal means specifically \"the logical operation has constant depth, regardless of code distance\". For example, the transversal S gate in the folded surface code uses physical gates that aren't the S gate. It uses two qubit gates across the folded halves.\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/24269/what-is-formally-a-transversal-operator"
      ],
      "metadata": {
        "id": "8PeCMq-UIJ5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does quantum error correction work?**\n",
        "\n",
        "Quantum error correction is a set of techniques for protecting quantum information from errors due to decoherence and other quantum noise. Here's a high-level summary:\n",
        "\n",
        "1. **Encoding**: The first step in quantum error correction is to encode the quantum information. Rather than storing a quantum bit of information (a \"qubit\") in a single physical qubit, we store it in multiple physical qubits. This is done in such a way that, even if some of the physical qubits are corrupted by noise, the original quantum information can still be recovered. This encoding is done using a quantum error-correcting code. There are many different types of quantum error-correcting codes, each with its strengths and weaknesses.\n",
        "\n",
        "2. **Syndrome measurement**: Once the quantum information has been encoded, the next step is to periodically check for errors. This is done by performing a syndrome measurement, which is a special kind of quantum measurement that can detect whether an error has occurred, and if so, what kind of error it was. Importantly, this measurement does not disturb the encoded quantum information.\n",
        "\n",
        "3. **Error correction**: If the syndrome measurement indicates that an error has occurred, the next step is to perform an error correction. This involves applying a series of quantum gates to the physical qubits to correct the error, based on the result of the syndrome measurement.\n",
        "\n",
        "4. **Repeat**: Because quantum systems are always subject to noise, this process of syndrome measurement and error correction needs to be repeated periodically to keep the errors in check.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "* It's worth noting that the whole process of quantum error correction requires a significant overhead in terms of additional physical qubits and quantum operations. This is a major challenge in the development of large-scale, fault-tolerant quantum computers.\n",
        "\n",
        "* It's also worth noting that, although quantum error correction can protect against many types of errors, it cannot protect against all possible errors. In particular, **it's assumed that the errors are relatively rare and do not all occur at once, and that they are independent and identically distributed across the physical qubits**. **If these assumptions are violated, then quantum error correction may not be able to correct the errors**. This is another major challenge in the field of quantum error correction."
      ],
      "metadata": {
        "id": "JjYa1KwBkNRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) very good\n",
        "Video: [The superconducting transmon qubit](https://youtu.be/dKTNBN99xLw)\n",
        "Video: [The transmon qubit](https://youtu.be/cb_f9KpYipk)\n",
        "Video: [Making quantum error correction practical](https://youtu.be/YPFpll1NFQc)\n",
        "Video: [Steven Girvin](https://youtu.be/nhUKHf-GN_Y)\n",
        "Video: [Quantum Industry Talks](https://youtu.be/eyICn3KCUPI)\n",
        "Video: [Qiskit QEC](https://youtu.be/ZY8PddknCos), [Qiskit](https://youtu.be/SHr3uSv9Bts), [qiskit](https://youtu.be/96a0G4G5ZH8)"
      ],
      "metadata": {
        "id": "68dkfSxfVj8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Concepts:*** *Ancilla qubits, surface code, distance code, X (bit flip) and Z (phase flip) error, threshold theory, required fault-tolerance, logical vs physical qubit*\n",
        "\n",
        "*Tasks: Error detection, error mitigation, error correction, error suppression*\n",
        "\n",
        "Video [Progress Towards Quantum Error Correction with the Surface Code | Qiskit Seminar Series](https://www.youtube.com/watch?v=si5a9RJP01A)"
      ],
      "metadata": {
        "id": "uRkoUVMsuEg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use more than one qubit to represent a state, use neighboring qubit check (so you don't measure the exact state which would collapse the quantum state). You can decode with it and see that the error was on the last qubit. Then you can correct the physical qubit to get back the correct logical qubit state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1278.png)"
      ],
      "metadata": {
        "id": "l3Ja_nTmmvIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[An Introduction to Quantum Error Correction and Fault-Tolerant Quantum Computation](https://arxiv.org/pdf/0904.2557.pdf)"
      ],
      "metadata": {
        "id": "2OdDTy_mW_Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For what do you need abelian groups in quantum computing?\n",
        "\n",
        "In quantum computing, Abelian groups are used to describe the symmetry of a quantum system. Symmetry is a fundamental concept in quantum mechanics, and it refers to the idea that a physical system will remain unchanged under certain transformations. For example, the symmetry of a quantum system may be described by a group of rotations, translations, or reflections. Abelian groups are used to describe the symmetry of a quantum system because they have the useful property of being commutative, meaning that the order in which the transformations are applied does not affect the outcome. This property makes it possible to use Abelian groups to describe the symmetries of a quantum system in a way that is mathematically tractable and easy to work with."
      ],
      "metadata": {
        "id": "4KQRd-aYkmU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error detection and correction**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Error_detection_and_correction\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Repetition_code"
      ],
      "metadata": {
        "id": "mGJ-sWNFvF47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faut-Tolerant QC (Quantum Error Correction)**\n",
        "\n",
        "* Bit flip (from 0 to 1) or dephasing (from superposition to exact state)\n",
        "\n",
        "* Challenge: we need to keep that states correct without looking at them (because then WE dephase them)\n",
        "\n",
        "* We need a method to **build relatively noiseless qubits (logical qubits)out of many noisy ones (physical qubits)**. This is quantum error correction.\n",
        "\n",
        "* Solution: one way (repetition encoding), sit our qubits on a line. We then go along and ask every pair of next-door-neighbours whether they agree or disagree with each other. This tells us nothing about whether they are 0 or 1. But repetition encoding, which protects against bit flip errors so well, actually makes dephasing more likely!\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_error_correction\n",
        "\n",
        "\n",
        "* Examples of QEC: **repetition code** (simplest QEC) and **surface code** (and color codes?)\n",
        "\n",
        "* techniques: syndrome measurements, decoding, logical operations\n",
        "\n",
        "* https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/\n",
        "\n",
        "\n",
        "* [An introduction to Fault-tolerant Quantum Computing](https://arxiv.org/abs/1508.03695)\n",
        "\n",
        "http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html\n",
        "\n",
        "* [INTRODUCTION TO\n",
        "QUANTUM ERROR\n",
        "CORRECTION](https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2327/files/2019/11/QECIntro.pdf)"
      ],
      "metadata": {
        "id": "B-hRX5qBqHdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Error Mitigation**\n",
        "\n",
        "* quantum error correction is long term goal, meanwhile we try to mitigate it\n",
        "\n",
        "* Error mitigation techniques: statistical corrections (on histogram for example)\n",
        "\n",
        "\t* https://qiskit.org/textbook/ch-quantum-hardware/measurement-error-mitigation.html\n",
        "\n",
        "\t* https://arxiv.org/abs/2005.10189"
      ],
      "metadata": {
        "id": "jX7JYnTLqNo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DiVincenzo's criteria**\n",
        "\n",
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) (very good!)\n",
        "\n",
        "Video: [Gottesman 1](https://youtu.be/ltJ1jXQeDl8) and [Gottesman 2](https://youtu.be/cUqys29d0YA)\n",
        "\n",
        "[DiVincenzo's criteria](https://en.m.wikipedia.org/wiki/DiVincenzo%27s_criteria) are conditions necessary for constructing a quantum computer, conditions proposed in 2000 by the theoretical physicist David P. DiVincenzo.\n",
        "\n",
        "**1. A scalable physical system with well-characterized qubits.**\n",
        "\n",
        "**2. The ability to initialize the state of the qubits to a simple fiducial state, such as 000...).**\n",
        "\n",
        "**3. Long relevant decoherence times, much longer than the gate operation time.**\n",
        "\n",
        "**4. A \"universal\" set of quantum gates** (that approximate any unitary operation - a unitary transformation preserves the inner product, which is a property of the Hilbert space)\n",
        "\n",
        "**5. A qubit-specific measurement capability** (ability to measure individual qubits)\n",
        "\n",
        "* Trapped Ion and superconducting qubits do really well on all five criteria\n",
        "\n",
        "6. *The ability to interconvert stationary and flying qubits.*\n",
        "\n",
        "7. *The ability to faithfully transmit flying qubits between specified locations.*\n",
        "\n",
        "* The DiVincenzo criteria consist of seven conditions an experimental setup must satisfy to successfully implement quantum algorithms such as Grover's search algorithm or Shor factorization.\n",
        "\n",
        "* The first five conditions regard quantum computation itself. Two additional conditions regard implementing quantum communication, such as that used in quantum key distribution. One can demonstrate that DiVincenzo's criteria are satisfied by a classical computer.\n",
        "\n",
        "* Comparing the ability of classical and quantum regimes to satisfy the criteria highlights both the complications that arise in dealing with quantum systems and the source of the quantum speed up.\n",
        "\n",
        "*Universal quantum computing and quantum annealer: not all criteria match the quantum annealers*\n",
        "\n",
        "**Definition of Quantum Computing**\n",
        "\n",
        "[Quantum computing](https://en.m.wikipedia.org/wiki/Quantum_computing) is a type of computation whose operations can harness the phenomena of quantum mechanics, such as superposition, interference, and entanglement to perform computation. Devices that perform quantum computations are known as quantum computers.\n",
        "\n",
        "*Harnessing effects of quantum mechanics: by this definition also quantum annealers are quyantum computers.*"
      ],
      "metadata": {
        "id": "9b6pMJ8JVL_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizer Code and Parity Check Measurement**"
      ],
      "metadata": {
        "id": "v41iaGUgY8hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code**\n",
        "\n",
        "*Exkurs: Stabilizer Code & Ancilla qubits*\n",
        "\n",
        "* A [stabilizer](https://en.m.wikipedia.org/wiki/Stabilizer_code) quantum error-correcting code appends [ancilla qubits](https://en.m.wikipedia.org/wiki/Ancilla_bit) to qubits that we want to protect.\n",
        "\n",
        "**A stabilizer quantum error-correcting code appends ancilla qubits to qubits that we want to protect**. A unitary encoding circuit rotates the global state into a subspace of a larger Hilbert space. This highly entangled, encoded state corrects for local noisy errors.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Stabilizer_code\n",
        "\n",
        "In quantum computing, **a stabilizer code is a type of error-correcting cod**e that is used to protect quantum information from the effects of noise and decoherence. These codes are based on the concept of stabilizer operators, which are a special type of operator that can be used to detect and correct errors in a quantum system. The basic idea behind stabilizer codes is to encode the quantum information using a set of stabilizer operators, such that any errors that occur in the system can be detected and corrected by measuring the values of these operators. GPT\n",
        "\n",
        "\n",
        "Many quantum error correction schemes can be classified as stabilizer codes, where a single bit of **quantum information is encoded in the joint state of many physical qubits**, which we refer to as data qubits. Interspersed among the data qubits are **measure qubits**, which periodically measure the parity of chosen combinations of data qubits. https://arxiv.org/pdf/2102.06132.pdf\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s\n",
        "\n",
        "https://leftasexercise.com/2019/01/28/basics-of-quantum-error-correction/\n",
        "\n",
        "https://leftasexercise.com/2019/02/04/q-fault-tolerant-quantum-computing/\n",
        "\n",
        "https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "https://leftasexercise.com/2019/04/08/quantum-error-correction-the-surface-code/\n",
        "\n",
        "https://leftasexercise.com/2019/02/11/quantum-error-correction-with-stabilizer-codes/\n",
        "\n",
        "https://leftasexercise.com/2018/09/10/quantum-computing-an-overview/"
      ],
      "metadata": {
        "id": "2ns0241plUDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stabilizer code with parity check circuits (for Z and X errors)"
      ],
      "metadata": {
        "id": "4c1Ooe-Odl3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parity measurement**: is it even (correct) or odd (error)? You will have a square with one dimension for X error and another dimension for Z error measurement. You have an **ancilla qubit** to make the measurements with C-Z-gate and C-X gate (but you first put it into an equal superposition).\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1283.png)"
      ],
      "metadata": {
        "id": "iCtG8eoVtaGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You put both X and Z together and get a 2D lattice of surface code with a determined code distance. We have now data qubits, X ancilla qubits and Z ancilla qubits.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1284.png)"
      ],
      "metadata": {
        "id": "ujX9f7Wevt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Repetition Codes**\n",
        "\n",
        "* look at majority of bits\n",
        "\n",
        "* https://qiskit.org/textbook/ch-quantum-hardware/error-correction-repetition-code.html\n",
        "\n",
        "* repetition code: redundancy (repetition) is a way to make sure the message gets delivered (i.e. with majority voting, for d repetition: $P=\\sum_{n=0}^{[ a / 2]}\\left(\\begin{array}{l}d \\\\ n\\end{array}\\right) p^{n}(1-p)^{d-n} \\sim\\left(\\frac{p}{(1-p)}\\right)^{[ d / 2]}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_100.png)"
      ],
      "metadata": {
        "id": "zEwG8U1SwWhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZX-calculus**"
      ],
      "metadata": {
        "id": "cZPcI0kqrh4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1307.7025\n",
        "\n",
        "The ZX-calculus is complete for stabilizer quantum mechanics"
      ],
      "metadata": {
        "id": "0fudQnLJODY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZX Calculus: Hopf rule\n",
        "\n",
        "Reasoning with connectivity using diagrammatic reasoning\n",
        "\n",
        "ZX Calculus: Hadamard rule\n",
        "\n",
        "Evaluating quantum circuits using diagrammatic reasoning."
      ],
      "metadata": {
        "id": "5gLhGntqCW37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/ZX-calculus\n",
        "\n",
        "https://en.wikipedia.org/wiki/Penrose_graphical_notation\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_network_theory?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Unified_field_theory\n",
        "\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_product_state?wprov=sfti1\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Density_matrix_renormalization_group?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Categorical_quantum_mechanics?wprov=sfti1\n",
        "\n",
        "And finally I'll learn about fault-tolerance cause now it's in a language that I can understand, cause I co-invented it with Ross Duncan!\n",
        "\n",
        "https://lnkd.in/dX2YDzfF\n",
        "\n",
        "But the “newly introduced ZX-instruments” want to be the bastard spiders of the dodo-book. They go back a long time, even before ZX calculus itself, and published here:\n",
        "\n",
        "https://lnkd.in/e4VvrGHA\n",
        "\n",
        "Quantum In Pictures also has that stuff, and so does this paper:\n",
        "\n",
        "https://lnkd.in/eSwFWkHZ"
      ],
      "metadata": {
        "id": "KbDcHaTjrqmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity (Error Probability)**"
      ],
      "metadata": {
        "id": "f0O6pdoUYhec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Physical and Logical Qubits\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Physical_and_logical_qubits"
      ],
      "metadata": {
        "id": "j7puSMXHeUxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity**: For Shor's algorithm with estimated 10^9 physical gates required, the error should be less than 10^-9, ideally 10^-10. We are still several orders of magnitude away from that accuracy / faut-tolerance.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1279.png)"
      ],
      "metadata": {
        "id": "EM0u6HMroENg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many applications call for error rates in the 10−15 regime [2–9], but state-of-the-art quantum platforms typically have physical error rates near 10−3\n",
        "https://arxiv.org/pdf/2102.06132.pdf"
      ],
      "metadata": {
        "id": "FcbSD88fkhOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run quantum algorithms perfectly we need error probability of 1 in a billion or 1 in a trillion - but we are at 1 in a thousand\n",
        "\n",
        "Video: [Suppressing quantum errors by scaling a surface code logical qubit](https://www.youtube.com/watch?v=dVkLNwSTBU0)"
      ],
      "metadata": {
        "id": "pQbRXxY779rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Fidelity of quantum states**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Fidelity_of_quantum_states\n",
        "\n",
        "* Noise transforms pure states into mixed states.\n",
        "\n",
        "  * There are also simpler ones: Fidelity between two pure states\n",
        "\n",
        "  * And there are also more complex ones: Fidelity between two mixed states\n",
        "\n",
        "* fidelity is generally defined as the quantity:\n",
        "\n",
        "> ${\\displaystyle F(\\rho ,\\sigma )=\\left(\\operatorname {tr} {\\sqrt {{\\sqrt {\\rho }}\\sigma {\\sqrt {\\rho }}}}\\right)^{2}}$\n",
        "\n",
        "* most useless state is fidelity 0,5. because fidelity = 0 means orthogonal, and =1 means exactly the same.\n",
        "\n",
        "* Video: [Fidelity](https://www.youtube.com/watch?v=GWi_HIVz2B4)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1275.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1276.png)"
      ],
      "metadata": {
        "id": "GV4K-Cl0qIIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of errors that you want to detect: **Bit-flip** (represented with Pauli X gate) and **Phase-flip** (represented with Pauli-Z gate). But 1D string physical qubits cannot protect from bit and phase flip at the same time. You need a 2D string.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1281.png)"
      ],
      "metadata": {
        "id": "In3mQfhOp0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene: Qubits können auch ganz verloren gehen**\n",
        "\n",
        "* Inzwischen können Quantencomputer mit einer gewissen Anzahl von Rechenfehlern, wie zum Beispiel Bitflip- oder Phasenflip-Fehlern, umgehen. Zusätzlich zu diesen Fehlern können jedoch auch Qubits ganz aus dem Quantenregister verloren gehen.\n",
        "\n",
        "* Je nach Art des Quantencomputers kann dies auf den tatsächlichen Verlust von Teilchen wie Atomen oder Ionen zurückzuführen sein, oder darauf, dass Quantenteilchen beispielsweise in unerwünschte Energiezustände übergehen, welche nicht mehr als Qubit erkannt werden. Wenn ein Qubit verloren geht, wird die Information in den verbleibenden Qubits unlesbar und ungeschützt. Für das Ergebnis der Berechnung kann dieser Prozess zu einem potentiell verheerenden Fehler werden.\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neue_Methode_schützt_Quantencomputer_vor_Ausfällen.html\n",
        "\n",
        "Resolving catastrophic error bursts from cosmic rays in large arrays of superconducting qubits.\n",
        "\n",
        "https://arxiv.org/abs/2104.05219\n",
        "\n",
        "https://physicsworld.com/a/cosmic-ray-threat-to-quantum-computing-greater-than-previously-thought/"
      ],
      "metadata": {
        "id": "0N8AlBcAz4Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Syndrome Measurement**\n",
        "\n",
        "**|1> measured in instead of a |0> : The permutations of outputted |1〉’s is called the “error syndrome”.**\n",
        "\n",
        "Syndrome: what qubit the error is on\n",
        "\n",
        "Error syndrome measurement:\n",
        "\n",
        "https://www.quora.com/Error-Correcting-Codes-What-is-a-syndrome:\n",
        "\n",
        "* The syndrome measurement provides information about the error that has happened, but not about the information that is stored in the logical qubit—as otherwise the measurement would destroy any quantum superposition of this logical qubit with other qubits in the quantum computer, which would prevent it from being used to convey quantum information. (https://en.m.wikipedia.org/wiki/Quantum_error_correction)\n",
        "\n",
        "* It is the result of multiplying a parity check matrix times a vector. By convention, codewords of a code have syndrome zero, so that by linearity of the code, the syndrome of a word is the syndrome of the \"error\" vector. Typically from the syndrome you would either try to determine whether there was an error (is the syndrome nonzero?) and recover the error from it, so that in turn you can recover the data from the received word.\n",
        "\n",
        "syndrome. = error?\n",
        "\n",
        "here slide 4: https://people.engr.tamu.edu/andreas-klappenecker/689/stabilizer.pdf"
      ],
      "metadata": {
        "id": "fYAPsGZXAPsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Distance and Surface Code**"
      ],
      "metadata": {
        "id": "dpys_ybfYkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Five-qubit_error_correcting_code"
      ],
      "metadata": {
        "id": "KVjhpug-hkYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum **Threshold theory** ensure that there is a limit that helps to get error under control even with larger numbers of physical qubits. The **code distance** is then a result of the max error rate and represents the number of physical qubits for one state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1280.png)"
      ],
      "metadata": {
        "id": "RuL0PiU_okd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Quantum Threshold Theorem**\n",
        "\n",
        "* Challenge: quantum computer will not be able to perform gate operations perfectly, some small constant error is inevitable\n",
        "\n",
        "* [quantum threshold theorem](https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem) (or quantum fault-tolerance theorem) states that a quantum computer\n",
        "  * **with a physical error rate below a certain threshold** can,\n",
        "  * **through application of quantum error correction schemes**,\n",
        "  * suppress the logical error rate to arbitrarily low levels.\n",
        "\n",
        "* This shows that quantum computers can be made fault-tolerant, as an analogue to von Neumann's threshold theorem for classical computation\n",
        "\n",
        "* The formal statement of the threshold theorem depends on the types of error correction codes and error model being considered.\n",
        "\n",
        "* for any particular error model (such as having each gate fail with independent probability p), use **error correcting codes** to build better gates out of existing gates.\n",
        "\n",
        "  * Though these \"better gates\" are larger, and so are more prone to errors within them, their error-correction properties mean that they have a lower chance of failing than the original gate (provided p is a small-enough constant).\n",
        "\n",
        "  * Then, one can use these better gates to recursively create even better gates, until one has gates with the desired failure probability, which can be used for the desired quantum circuit.\n",
        "\n",
        "* Current estimates put the threshold for the [surface code](https://en.m.wikipedia.org/wiki/Toric_code) (here: Toric code) on the order of 1%, though estimates range widely and are difficult to calculate due to the exponential difficulty of simulating large quantum systems.\n",
        "\n",
        "* At a 0.1% probability of a [depolarizing](https://en.m.wikipedia.org/wiki/Depolarization) error, the surface code would require approximately 1,000-10,000 physical qubits per logical data qubit, though more pathological error types could change this figure drastically.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1xEbTDdNwar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Distance Code / Code Distance**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory\n",
        "\n",
        "\n",
        "* Over all 25 cycles of error correction, the distance-5 code realises lower logi- cal error probabilities pL than the average of the subset distance-3 codes - [Paper](https://arxiv.org/pdf/2207.06431.pdf)\n",
        "\n",
        "* the distance is the shortest path in a certain \"space of errors\" which maps between two orthogonal quantum states that are in the code.\n",
        "\n",
        "* The natural space of errors is that of single qubit errors of the form 𝜎𝑋, 𝜎𝑌 or 𝜎𝑧, in the case where the Hilbert space is that of 𝑛 qubits.\n",
        "\n",
        "* So you can think of distance as the shortest path to get from one state to another by operations on single qubits, applied one at a time sequentially.\n",
        "\n",
        "* [Source](https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory)"
      ],
      "metadata": {
        "id": "ieotWtPN7sz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Code*"
      ],
      "metadata": {
        "id": "KSaDCK89Y3_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, code distance and surface code are not the same in quantum error correction.\n",
        "\n",
        "* **Code distance** is a measure of the ability of a quantum error-correcting code to correct errors. A code with a higher code distance can correct more errors than a code with a lower code distance.\n",
        "* **Surface code** is a specific type of quantum error-correcting code that has a high code distance. Surface codes are often used in quantum computers because they are relatively easy to implement and can correct a large number of errors.\n",
        "\n",
        "In other words, code distance is a property of all quantum error-correcting codes, while surface code is a specific type of quantum error-correcting code that has a high code distance.\n",
        "\n",
        "Here is an analogy to help you understand the difference between code distance and surface code:\n",
        "\n",
        "* Code distance is like the number of lanes on a highway. A highway with more lanes can handle more traffic than a highway with fewer lanes.\n",
        "* Surface code is like a specific type of highway that is designed to handle a lot of traffic. Surface highways have more lanes than other types of highways, so they can handle more traffic.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "XJl73OAdfcoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D Surface code** protects from X error and Z error (X and Z - that's why 2 D). Errors typically arise only locally. The gate structure needs to fit the physical geometry of the quantum processor. Error per gate should be 0,5%, but overall threshold depends on case.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1282.png)"
      ],
      "metadata": {
        "id": "jQZePon1slGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code $\\rightarrow$ Surface Code $\\rightarrow$ Toric Code**\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/2106/what-is-the-surface-code-in-the-context-of-quantum-error-correction\n",
        "\n",
        "* **Surface codes**: family of quantum error correcting codes defined on a 2D lattice of qubits.\n",
        "\n",
        "* Each code has [stabilizers](https://en.m.wikipedia.org/wiki/Stabilizer_code) that are defined equivalently in the bulk, but differ from one another in their boundary conditions.\n",
        "\n",
        "* The members of the surface code family are sometimes also described by more specific names:\n",
        "\n",
        "  * The [toric code](https://en.m.wikipedia.org/wiki/Toric_code) is a surface code with periodic boundary conditions,\n",
        "\n",
        "  * the planar code is one defined on a plane, etc.\n",
        "\n",
        "* How many qubits are in a surface code? - While the surface code requires four-qubit measurements to encode a single logical qubit, we introduce families of quantum error correcting codes that use only three-qubit measurements. [Paper](https://www.ucl.ac.uk/quantum/news/2021/aug/subsystem-codes-outperform-surface-code)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [Topological quantum memory (paper)](https://arxiv.org/abs/quant-ph/0110143)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation (paper)](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [My blog series introducing surface codes](http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html)\n",
        "\n",
        "* The surface codes can also be generalized to qudits. For more on that, [see here (Fault-tolerant quantum computation by anyons)](https://arxiv.org/abs/quant-ph/9707021)"
      ],
      "metadata": {
        "id": "Ur4bkT3KooVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Toric Code**\n",
        "\n",
        "* For the toric code we don’t put our qubits in a line, we put them in a grid pattern.\n",
        "\n",
        "* Video: [INTRODUCTION TO TOPOLOGICAL ORDER, DEMONSTRATION VIA THE TORIC CODE](https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s)\n",
        "\n",
        "* https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code-part-2.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/8-toric-code-part-3.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/9-toric-code-part-4.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/10-toric-code-part-5.html"
      ],
      "metadata": {
        "id": "zpXlgK2jyNPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Color Code**\n",
        "\n",
        "* [Fault-tolerant quantum computing with color codes](https://arxiv.org/abs/1108.5738)\n",
        "\n",
        "* https://physics.stackexchange.com/questions/169176/quantum-error-correction-surface-code-vs-color-code\n",
        "\n",
        "The color code and surface code are very similar. They are stabilizer codes composed of qubits arranged in two dimensions, requiring only geometrically local stabilizer measurements.\n",
        "\n",
        "From the theory point of view, the codes are very similar. In fact, with collaborators we have proven that the color code is equivalent to a surface code (paper) up to a geometrically local unitary (one which only makes nearby qubits interact). One can think by analogy of the surface code* as a napkin with two rough and two smooth sides and the color code as folding this napkin along its diagonal. Because in the folded napkin, there are new things that are now close, it is possible to do more logical gates \"transversally\". This is good because it keeps errors from propagating and is relatively easy. However, the color code needs more qubits to interact in each stabilizer so ends up leading to a lower noise threshold. So one can say that although very similar, each code has its advantages and disadvantages.\n",
        "\n",
        "At this point, only very small versions of either of these codes are being demonstrated. The Rainer Blatt group demonstrated the smallest possible color-code which also uses 7 qubits (this instance is also referred to as the Steane code). However, the underlying geometry in which the qubits are laid out in the Blatt setup is a linear chain of ions, so I would say that this is not the natural setting to extend to larger and larger system sizes.\n",
        "\n",
        "**The superconducting qubit people (Martinis, IBM, DiCarlo, ...) on the other hand, are concentrating more on surface codes**. While in principle, their architecture should allow them to go full fledge 2D, for now, they are having the classical logic come in from the sides, which is something that needs to change.\n",
        "\n",
        "*There is actually an ambiguity as to what to call surface codes, but I will refer to the quantum double of Z2 with rough and smooth boundaries defined by Bravyi and Kitaev (paper)."
      ],
      "metadata": {
        "id": "9tWyVLKvxLE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Distance 2 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "cNo9Qsp7ZTQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go down from a seven qubit surface code to a simpler two qubit surface code. Smallest meaningful is a 2x2 lattice. But it's just an error detection code, because it's too small to do error correction.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1286.png)"
      ],
      "metadata": {
        "id": "d503fihiwLGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green line is a flux line**. We put a magnetic flux through the script loop of each qubit to put a a specific frequency where we want it to be. **Pink line is a charge line** that is used for single qubit gates. All the rest (red, blue, purple box) are part of the readout. It's very important to have a good readout - we need to measure the ancilla qubits during the operation to see if there wasn't an error. You see that sort of resonator over each Qubit (die dinger die aussehen wir alte Heizungskoerper) - this is very standard, there is a harmonic oscillator.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1287.png)"
      ],
      "metadata": {
        "id": "ugRZQcwN0eoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1288.png)"
      ],
      "metadata": {
        "id": "Mbmct0-22-2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1289.png)"
      ],
      "metadata": {
        "id": "sddTSG2S7J3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*7 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "QHclLheAYhtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a 7 qubit gate circuit. You can't correct the error, but you can detect it. At the end we verify by measuring the actual state of each qubit.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1290.png)\n"
      ],
      "metadata": {
        "id": "eqXaY73s5YSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run multiple measurements in (20) microseconds for Z and X operator:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1291.png)"
      ],
      "metadata": {
        "id": "ywM2-Zqb6dQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D = 3 Surface Code Stabilizer Gate Sequence**\n",
        "\n",
        "* Red: data qubits\n",
        "* Blue: X-type ancilla qubit\n",
        "* Green Z-type ancilla qubit\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1300.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1301.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1302.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1303.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1304.png)\n"
      ],
      "metadata": {
        "id": "5sejATAN9-FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1305.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1306.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1307.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1308.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1L2hW2GU_SsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Future of Quantum Error Correction*"
      ],
      "metadata": {
        "id": "0PltPkkfZ1Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future of quantum error correction**\n",
        "* we still have problems like leakage etc\n",
        "  * When they ask about thresholds, they are also derived from ideal models. Things like circular ZZ coupling can make it much harder.\n",
        "  * If you add leakage to your CZ gates, you could take a surface code that would sort of there was zero leakage below the physical error rate. But if you add leakage to that 0.1 percent degrees (the red line), which is a second. When you do the decoding (the green line), suddenly you are not below the threshold anymore\n",
        "* Also, if we want distance n=17 (mentioned in the beginning) you have an insane amount of data coming out of the device:\n",
        "  * we need ($n^2 -1$) physical qubits for error correction. So if one readout is 1 bit, we need 288 bits per microsecond ($\\mu$s) per qubit).\n",
        "  * This amounts to 288 Gbits per s for 1000 logical qubits\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1292.png)"
      ],
      "metadata": {
        "id": "yps3FbFbuHOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyons, Transmon and Fluxonium**"
      ],
      "metadata": {
        "id": "oQZJ3rU97I-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyon (Quasi Particles)**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Anyon\n",
        "\n",
        "* PBS Video on Quasiparticles: https://youtu.be/le_ORQZzkmE"
      ],
      "metadata": {
        "id": "gy8QFKOMICxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transmon and Fluxonium Qubit**\n",
        "\n",
        "* Better hardware to protect against noise orders of magnitude better (to get down to 10^-5 instead of 10^-9\n",
        "* they can show cherence times above 1 millisecond, they had single qubit errors of 0.9999 (only 4x) with Fluxonium\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1293.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1299.png)\n",
        "\n",
        "*Source: https://theorie.physik.uni-konstanz.de/burkard/sites/default/files/images/Seminar_3_TrFl.pdf*\n",
        "\n",
        "\n",
        "Video: [Google Keynote: Superconducting qubits for quantum computation: transmon vs fluxonium](https://www.youtube.com/watch?v=qsizrKrUZDg)"
      ],
      "metadata": {
        "id": "We68Qy_9vgEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyons, Qubit Braiding and Topological Quantum Computing**\n",
        "\n",
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/#loop-excitations-and-error-correction\n",
        "\n",
        "https://www.quantamagazine.org/physicists-create-elusive-particles-that-remember-their-pasts-20230509/\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1568.png)\n",
        "\n",
        "\n",
        "\n",
        "https://arthurpesah.me/blog/2023-05-13-surface-code/\n",
        "\n",
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/\n",
        "\n",
        "https://arthurpesah.me/blog/2023-01-31-stabilizer-formalism-1/\n",
        "\n",
        "https://arthurpesah.me/blog/2022-01-25-intro-qec-1/\n",
        "\n",
        "https://blog.google/technology/research/an-important-step-towards-improved-quantum-computers/\n",
        "\n",
        "https://phys.org/news/2023-05-google-quantum-ai-braids-non-abelian.html\n",
        "\n",
        "https://www.spektrum.de/news/nichtabelsche-anyonen-auf-quantenprozessor-simuliert/2138241\n",
        "\n",
        "\n",
        "\n",
        " I am also frustrated by us and the media representing these anyon simulations as a \"step towards fault-tolerance\". Certainly topological codes are inspired by the physics of anyons and there are direct analogies between how those codes work and are realized and these simulations. I also have no doubt that certain aspects of these simulations have some degree of error robustness. But my understanding is that once you get serious about turning such simulations into a form of fault-tolerance appropriate for our devices and then try to optimize its realization you end up with the surface code. So while thinking about the physics of error-correction from this perspective might be useful and there are myriad mathematical similarities between what is going on in these simulations and topological codes, I think it is misleading/wrong to claim that performing these simulations is taking us any closer to fault-tolerance (whether the surface code or a different form of it). People keep telling me that I am misunderstanding this (and maybe I am!) but I have not been convinced of that yet.\n",
        "\n",
        "my objection, which might be different from Cody's, is that the title of this blog post suggests that this experiment is getting us closer to realizing universal fault-tolerant quantum computers. In particular, when we say it is a step closer to improved quantum computers, it makes it sound like this approach is somehow a step on our roadmap for realizing fault-tolerant quantum computers, or that it is going to make our ultimate goal easier or something like that. That is reading between the lines a little bit, but it is how it sounds to me. Is that actually true? Or is this in the category of \"nice demonstration, but not something we're going to follow up on, and thus not a step towards us improving quantum computers other than in the very broad sense that it helps test hardware capabilities like most physics team experiments do\"? My understanding is that nothing about this experiment is likely to change anything about how we are planning to realize fault-tolerance. Maybe that is incorrect, or it is correct and I am just reading too much into the implications of statements like the title. It would be nice to understand this better.\n",
        "\n",
        "The main sense in which the non-abelian anyons are useful for fault tolerance is that they are isomorphic to twists in the surface code, and you can use twists to store logical qubits. But we already knew about twists independent of this work e.g. from https://arxiv.org/abs/1609.04673 so I don't know what it adds on the fault tolerance side.\n",
        "\n",
        "I did give a presentation to the authors of our abelian paper on how to do it fault tolerantly (they were technically \"using\" a surface code, but they were preparing it entirely unitarily which is not fault tolerant; you have to use measurements for everything). They weren't so interested I think, because the measurement version is harder and maybe also because they didn't consider it \"really doing it\" in the same way. I actually did run some quick shots of my versions of the circuits on the device back when pink was in M2 shape, and it worked, in that I got non-zero signal (the error rate was close to max but not max). I wasn't doing adept or etc or etc; if an experimentalist did it it would have been better. Also I now have much better versions of the circuit, which are described in https://arxiv.org/abs/2302.07395 .\n",
        "\n",
        "My point was that if you interpret TQC as broadly (\"you have things that act like anyons\") then this is demonstrating something interesting (multiparticle entangled EC state, FT gates on that state).  But I also agree with Ryan in that I think the blog post does not convey this well.\n",
        "\n",
        "Fair enough, I'm just trying to put forth a perspective on why it is fine to say this is an interesting experiment in topological quantum computation.  And if you want some idea of how it might lead to noise protection, yes  https://arxiv.org/abs/quant-ph/9912040 which was expanded upon in https://arxiv.org/abs/0907.3988\n",
        "\n",
        "And indeed our own team's work investigating how simulations of many body systems are related does or does not give protection of topologically protected information is in this direction.\n",
        "\n",
        "I think I agree with everything you've said Dave. But I don't think any of that really justifies the framing of the blog post title, which to me suggests that this is moving us closer to building a fault-tolerant quantum computer as opposed to exploring the error robustness of anyon simulation."
      ],
      "metadata": {
        "id": "yvGciNEzWO-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, anyons and transmons are related in quantum computing. Anyons are theoretical particles that have unique properties that make them well-suited for quantum computing. Transmons are physical implementations of qubits that can be used to build quantum computers.\n",
        "\n",
        "One of the key properties of anyons is that they can be braided together to perform quantum operations. This is because the braiding of anyons is a topological operation, meaning that it is not affected by the environment. This makes anyons very resistant to noise, which is a major challenge in quantum computing.\n",
        "\n",
        "Transmons, on the other hand, are physical particles that can be used to build qubits. Qubits are the basic unit of information in quantum computing. Transmons are made of superconducting circuits and can be manipulated using microwaves.\n",
        "\n",
        "Transmons are not as topologically protected as anyons, but they are much easier to implement in a practical quantum computer. This is because transmons can be manufactured using existing semiconductor technology.\n",
        "\n",
        "So, anyons and transmons are both promising candidates for quantum computing. Anyons are more topologically protected, but transmons are easier to implement. It is still not clear which approach will ultimately be more successful, but both anyons and transmons are active areas of research.\n",
        "\n",
        "Here is a table that summarizes the key differences between anyons and transmons:\n",
        "\n",
        "| Property | Anyons | Transmons |\n",
        "|---|---|---|\n",
        "| Topological protection | Yes | No |\n",
        "| Ease of implementation | No | Yes |\n",
        "| Current research status | Active | Active |\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "CaNGqKxZgJy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Jellium\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Cooper-Paar\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Bose–Einstein_condensate\n",
        "\n",
        "https://opg.optica.org/oe/fulltext.cfm?uri=oe-2-8-299&id=63264\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Phonon\n",
        "\n",
        "\n",
        "Part of this correlation is the formation of pairs of electrons called Cooper pairs. According to Josephson, under certain circumstances these Cooper pairs move from one superconductor to the other across the thin insulating layer. Such motion of pairs of electrons constitutes the Josephson current, and the process by which the pairs cross the insulating layer is called Josephson tunneling.\n",
        "\n",
        "https://www.britannica.com/science/Josephson-effect\n",
        "\n",
        "\n",
        "Meissner effect\n",
        "\n",
        "Meissner effect, the expulsion of a magnetic field from the interior of a material that is in the process of becoming a superconductor, that is, losing its resistance to the flow of electrical currents when cooled below a certain temperature, called the transition temperature, usually close to absolute zero. The Meissner effect, a property of all superconductors, was discovered by the German physicists W. Meissner and R. Ochsenfeld in 1933.\n",
        "\n",
        "https://slideplayer.com/slide/5010186/\n",
        "\n",
        "Squid"
      ],
      "metadata": {
        "id": "I2GgHQvDATOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Josephson Junction*"
      ],
      "metadata": {
        "id": "siEtU4-r3-Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Superconducting integrated circuits**\n",
        "\n",
        "- Conductance is not constant but varies with how much current is flowing, making it an unharmonic oscillator\n",
        "- Potential of the conductor is a cosine\n",
        "- Low energy excitations are pairs of electrons slashing back and forth between the two antenna pads\n",
        "- From ground state to first excited state: 5 gigahertz\n",
        "- From first excited state to second excited state transition: 4.9 Ghz (due to flattened curve of cosine)\n",
        "\n",
        "More details: https://www.youtube.com/watch?v=uD69GCYF9Zg&t=2023s\n"
      ],
      "metadata": {
        "id": "yx8LP-FXATe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übergangsdipolmoment (Transition dipole moment)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Übergangsdipolmoment"
      ],
      "metadata": {
        "id": "MTADPYzI5eaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZZ: Residual ZZ coupling, circular ZZ coupling, ZZ crosstalk**\n",
        "\n",
        "*  Noise is a significant obstacle to quantum computing, and 𝑍 𝑍 cross- talk is one of the most destructive types of noise affecting supercon- ducting qubits. Previous approaches to suppressing 𝑍𝑍 crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to sup- press 𝑍𝑍 crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved. https://arxiv.org/pdf/2202.07628.pdf\n",
        "\n",
        "* In superconductors a destructive type of noise known as 𝑍𝑍 crosstalk. This refers to an always-on 𝜎𝑧 ⊗ 𝜎𝑧 inter- action between qubits connected by couplings, which originates from the interaction between the computational and non-computational energy levels of qubits.\n",
        "\n",
        "* different types of crosstalks:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1295.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1296.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1297.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1298.png)\n",
        "\n",
        "\n",
        "*Source: https://www.youtube.com/watch?v=si5a9RJP01A&t=3645s*\n",
        "\n",
        "* second graph: top red is perfect readout, and black light is for max 10% readout\n",
        "* We need very good readout and very small ZZ error\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1294.png)"
      ],
      "metadata": {
        "id": "y9wvhg9DyWQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Barren Plateaus*"
      ],
      "metadata": {
        "id": "bIfbMtM1Y-7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sss"
      ],
      "metadata": {
        "id": "zetQy67PY8x2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Symmetries*"
      ],
      "metadata": {
        "id": "ZVxXcD-mYYhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sss"
      ],
      "metadata": {
        "id": "NMOxh06aYWi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Encoding*"
      ],
      "metadata": {
        "id": "91Gi3EMaYlBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "PB2uSkU6JIZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "reC2GT6fUkEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "xYRmG4v9vwCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "Fxjw-h8ox4Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "F2UqpWiN0Dv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "GbtZMigG2mij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "T8NOeUKI5dOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "BtmFtmBU6JFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Chaos*"
      ],
      "metadata": {
        "id": "_ZLCspR7YR3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "revelant for black holes and other things\n",
        "\n",
        "Absolutely! There's a fascinating connection between quantum chaos and black holes. Here's why:\n",
        "\n",
        "**Black Holes as Chaotic Systems**\n",
        "\n",
        "* **Scrambling:** Black holes are believed to be the fastest \"scramblers\" of information in the universe. Anything falling into a black hole essentially has its information rapidly encoded and spread across the black hole's event horizon. This scrambling is seen as analogous to chaos in classical systems.\n",
        "* **Entropy & Thermodynamics:** Black holes possess entropy, a thermodynamic property often associated with disorder and chaos. The connection between a black hole's entropy and its microscopic characteristics remains a key mystery.\n",
        "\n",
        "**Quantum Chaos & Black Holes**\n",
        "\n",
        "* **Information Paradox:**  Black holes challenge a fundamental law of quantum mechanics – information can't be destroyed. Quantum chaos may help resolve this paradox. It offers possible mechanisms on how information could be 'hidden' within seemingly random features on the event horizon and potentially retrieved through subtle effects associated with Hawking radiation.\n",
        "* **Holographic Duality:** Some theoretical models, like the AdS/CFT correspondence, suggest a deep connection between the physics of black holes in specific spacetimes and certain quantum field theories without gravity. These theories may use notions from quantum chaos to establish this link.\n",
        "* **Understanding the Early Universe:** The extremely hot and dense state of the early universe might have had properties related to quantum chaos.  \n",
        "\n",
        "**Current Research**\n",
        "\n",
        "* **Out-of-Time-Order-Correlators (OTOCs):**  OTOCs are tools used to measure how quickly information scrambles or spreads in a system. They're applied to study quantum chaos and are showing promise in understanding black hole dynamics.\n",
        "* **Spectral Statistics:** Researchers examine the statistics of energy levels in systems related to black holes, looking for signatures of quantum chaos with features like 'level repulsion'.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The complex nature of black holes makes them a testing ground for ideas connected to quantum chaos. The research aims to use the chaotic nature of black holes to potentially resolve fundamental issues in physics like the information paradox, and even gain insights into how physics worked during the earliest moments of our universe.\n",
        "\n",
        "**Let me know if you want more details on any of these connections!**\n"
      ],
      "metadata": {
        "id": "-KarkScz8S6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Quantum learning for quantum chaos](https://www.youtube.com/watch?v=d1iZ-ov5QOI&list=WL&index=9&t=163s)"
      ],
      "metadata": {
        "id": "VoyXxYS28U5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of quantum chaos, including the key ideas and why it matters:\n",
        "\n",
        "**What is Quantum Chaos?**\n",
        "\n",
        "Quantum chaos is a field of physics that explores the connections between quantum mechanics (the physics of the very small, like atoms and particles) and classical chaos (the behavior of systems that are highly sensitive to starting conditions).\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Classical Chaos:** Systems like the weather or a double pendulum are \"chaotic\" because tiny changes in the initial conditions can lead to wildly different outcomes over time. This is the famous \"butterfly effect\".\n",
        "* **Quantum Mechanics:** In the quantum world, things like energy and position aren't precisely defined. Instead, they exist in fuzzy probabilities and wave-like behavior.\n",
        "* **The Correspondence Principle:**  In general, quantum mechanics should align with classical mechanics in the limit of large systems. Quantum chaos asks: if classical systems can be chaotic, can their quantum counterparts exhibit similar behavior?\n",
        "\n",
        "**How Does Quantum Chaos Manifest?**\n",
        "\n",
        "It turns out that while true chaos can't fully exist in quantum systems, there are interesting ways the \"fingerprint\" of chaos can show up:\n",
        "\n",
        "* **Energy Levels:** In chaotic classical systems, the energy levels tend to be more irregular and bunched up. Similar patterns can be found in the energy levels of certain quantum systems.\n",
        "* **Wavefunctions:** Instead of chaotic trajectories like in classical systems, we look at quantum wavefunctions (which describe the probability of finding a particle somewhere). Wavefunctions of chaotic quantum systems tend to be highly irregular and spread out.\n",
        "* **Level Repulsion:**  In chaotic quantum systems, energy levels tend to \"repel\" each other, in contrast to more predictable, evenly spaced levels seen in regular quantum systems.\n",
        "\n",
        "**Why Does Quantum Chaos Matter?**\n",
        "\n",
        "Quantum chaos research has applications in:\n",
        "\n",
        "* **Understanding Complex Systems:** Studying the ways chaos emerges in the quantum world helps us understand the behavior of complex systems like atoms with many electrons, or the properties of certain materials.\n",
        "* **Quantum Computing:** The properties of quantum chaos could have implications for designing more robust quantum computers.\n",
        "* **Fundamental Physics:** Quantum chaos helps us explore the boundaries between quantum and classical physics, potentially leading to new insights about the fundamental nature of reality.\n",
        "\n",
        "**Let me know if you want a deeper dive into any of these aspects, or if you'd like some examples!**\n"
      ],
      "metadata": {
        "id": "B-kQaiajieIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how quantum chaos impacts the world of quantum computing:\n",
        "\n",
        "**The Challenge of Decoherence**\n",
        "\n",
        "* Quantum computers rely on fragile quantum states for computation. These states are easily disrupted by outside interactions (think of them as very delicate superpositions). This loss of quantum information is called \"decoherence\".\n",
        "* Decoherence leads to errors in computation and is one of the major obstacles to making large-scale, reliable quantum computers.\n",
        "\n",
        "**How Chaos Plays a Role**\n",
        "\n",
        "* **Sensitivity to Errors:** Systems exhibiting quantum chaos can be incredibly sensitive to even tiny errors or perturbations. This means they could be much more prone to decoherence.\n",
        "* **Scrambling of Information:** A central feature of quantum chaos is the rapid \"scrambling\" of quantum information. This makes the recovery of information and correction of errors significantly harder.\n",
        "\n",
        "**Implications for Quantum Computing**\n",
        "\n",
        "* **Limits of Fault Tolerance:**  Quantum chaos may pose limits on how effective traditional error correction techniques can be in quantum computers.\n",
        "* **Designing Robust Gates:** Understanding quantum chaos could aid in designing quantum gates (the building blocks of quantum computations) and algorithms that are less susceptible to chaotic effects and decoherence.\n",
        "* **Hardware Design:** The implications of quantum chaos could even influence the physical design of quantum computer hardware to try and minimize its impact.\n",
        "\n",
        "**Ongoing Research**\n",
        "\n",
        "* **Quantifying Chaos:** Researchers are developing rigorous ways to measure and quantify quantum chaos in actual quantum computing systems. This will help us to understand its impact better.\n",
        "* **Chaos as a Tool (Possibly):** Some scientists are exploring whether aspects of quantum chaos can be harnessed for new kinds of quantum algorithms. Although speculative, this is a fascinating new field.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "Quantum chaos poses a significant challenge for building large-scale quantum computers by making them inherently more susceptible to errors and information loss. Research focused on understanding and potentially mitigating quantum chaos is critical for the advancement of quantum computing.\n",
        "\n",
        "**Let me know if you'd like to explore any of these aspects further!**\n"
      ],
      "metadata": {
        "id": "ZJJ5-FDHipjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Applications**"
      ],
      "metadata": {
        "id": "RcixBlEjhSch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Hidden Subgroup Problem*"
      ],
      "metadata": {
        "id": "g9-cNnkyYxVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hidden subgroup problem**\n",
        "\n",
        "The dihedral hidden subgroup problem https://arxiv.org/abs/2106.09907\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem"
      ],
      "metadata": {
        "id": "K5mtRvKj1f07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Examples of HSP over non-Abelian groups](https://www.youtube.com/watch?v=D-pfWjGqENg)\n",
        "\n",
        "* We describe the HSP over symmetric and dihedral groups and their interesting applications.\n",
        "\n",
        "* example 2 graphs A and B: it's not sufficient if they have same number of edges and vertices\n",
        "\n",
        "* it's also important to know if they have the same connectivity (ismorphic): edge 5 and 10 in graph A are connected as well as edge 5 and 10 in graph B\n",
        "\n",
        "* compelixity of graph ismorphism is NP: if you have the solution, it's easy to check if it's true.\n",
        "\n",
        "* Graph automorpphism: you relabel the graph and it looks the same (non trivial besides identity map)\n"
      ],
      "metadata": {
        "id": "idQjSalFVKtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hidden Subgroup Problem](https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem) (HSP) generalizes many problems on which quantum computers offer a potential exponential speed-up over classical computers and derives its significance from two important and related questions. First, the study of the problem sheds some light on what it is that quantum computers are really good at. Second, the ability to solve HSP for non-Abelian groups in polynomial time would enable us to extend the set of problems on which quantum computers offer substantial performance advantage. In this introductory talk, I will recap essential concepts from group theory before defining the Hidden Subgroup Problem and describing the quantum algorithm that solves it quickly. I will end by showing how to express a number of well-known problems, such as Simon's problem, discrete logarithm and graph isomorphism, as instances of HSP.\n"
      ],
      "metadata": {
        "id": "X1R28Zo3W95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hidden Subgroup Problem (HSP) is a computational problem in the field of quantum computing and cryptography. It is a generalization of the well-known problem called the Discrete Logarithm Problem (DLP) and the Integer Factorization Problem (IFP), which are both important in modern cryptography.\n",
        "\n",
        "In the Hidden Subgroup Problem, the input is a group G and a function f: G -> X, where G is a finite group and X is a finite set. The goal is to find a subgroup H of G such that f(x) is constant for all elements x in the same coset of H. In simpler terms, the problem involves finding a hidden structure within a group based on the information provided by the function.\n",
        "\n",
        "The Hidden Subgroup Problem has important implications in cryptography because certain mathematical problems, such as factoring large numbers, can be reduced to it. If an efficient quantum algorithm were developed to solve the Hidden Subgroup Problem, it would have significant consequences for public-key cryptosystems like RSA and elliptic curve cryptography, which rely on the difficulty of factoring large numbers or solving the DLP.\n",
        "\n",
        "Efforts have been made to find efficient quantum algorithms for solving the Hidden Subgroup Problem, particularly using quantum Fourier transform-based algorithms like Shor's algorithm. However, no general efficient quantum algorithm has been discovered yet for arbitrary groups, although there are some specific cases where efficient algorithms have been developed.\n",
        "\n",
        "The study of the Hidden Subgroup Problem and its related algorithms is an active area of research in quantum computing and cryptography, with the aim of better understanding the limits and possibilities of quantum computing's impact on cryptographic systems."
      ],
      "metadata": {
        "id": "Se1miG3Q2qH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the challenge in the hidden subgroup problem?\n",
        "\n",
        "The main challenge in the Hidden Subgroup Problem (HSP) is to efficiently find the hidden subgroup within a given group based on the information provided by a function. The difficulty lies in identifying the underlying structure or pattern that the function reveals about the subgroup.\n",
        "\n",
        "One of the key aspects that makes the HSP challenging is that the function only provides partial information about the subgroup. The function maps elements of the group to a set of values, but it does not directly reveal the subgroup itself. Instead, the function is designed in such a way that it distinguishes elements of different cosets of the hidden subgroup while keeping elements within the same coset indistinguishable. This limited information makes it difficult to directly extract the subgroup.\n",
        "\n",
        "Another challenge is that the HSP encompasses a wide range of group structures. While some specific cases of the problem have been efficiently solved using quantum algorithms, finding a general solution that works for arbitrary groups remains an open question. The difficulty varies depending on the properties and structure of the group involved.\n",
        "\n",
        "Additionally, the HSP is of particular interest in the field of quantum computing because it has the potential to break certain cryptographic protocols. Efficiently solving the HSP for specific groups could lead to the development of quantum algorithms that can break widely used encryption schemes based on problems like factoring large numbers or solving the Discrete Logarithm Problem.\n",
        "\n",
        "Despite efforts to develop quantum algorithms for the HSP, no general solution has been found yet. The challenge lies in understanding the underlying mathematical structures, devising efficient algorithms, and leveraging the power of quantum computation to solve the problem in a scalable and practical manner."
      ],
      "metadata": {
        "id": "r6qrUX10Wh-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you have a concrete example for a hidden subgroup?\n",
        "\n",
        "\n",
        "Certainly! One of the well-known examples of the Hidden Subgroup Problem involves the symmetric group, denoted as S<sub>n</sub>. The symmetric group consists of all possible permutations of n elements. In this case, the hidden subgroup problem involves finding the hidden subgroup based on the information provided by a function.\n",
        "\n",
        "Let's consider an example with n = 3, where the elements of the symmetric group S<sub>3</sub> are permutations of three objects, let's say {1, 2, 3}. The hidden subgroup problem would involve finding a hidden subgroup within S<sub>3</sub> based on the function f: S<sub>3</sub> -> X, where X is some set.\n",
        "\n",
        "In this scenario, the function f could be defined as follows:\n",
        "f(1, 2, 3) = 'A'\n",
        "f(1, 3, 2) = 'A'\n",
        "f(2, 1, 3) = 'B'\n",
        "f(2, 3, 1) = 'B'\n",
        "f(3, 1, 2) = 'C'\n",
        "f(3, 2, 1) = 'C'\n",
        "\n",
        "Here, the function assigns labels 'A', 'B', and 'C' to certain permutations in S<sub>3</sub>. The goal of the Hidden Subgroup Problem would be to determine the hidden subgroup, which, in this case, corresponds to the permutations that are labeled with the same letter.\n",
        "\n",
        "In this particular example, the hidden subgroup consists of the identity permutation (1, 2, 3), the permutation (2, 1, 3), and the permutation (3, 2, 1). The function f labels these permutations with the letter 'A'. So, the hidden subgroup in this case is { (1, 2, 3), (2, 1, 3), (3, 2, 1) }.\n",
        "\n",
        "The challenge is to find the hidden subgroup based on the given function, which may involve clever algorithms and techniques to deduce the structure of the subgroup from the function's output."
      ],
      "metadata": {
        "id": "8XCw9wUqWk3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would be a potential algorithm in this case?\n",
        "\n",
        "In the specific case of the Hidden Subgroup Problem (HSP) for the symmetric group S<sub>3</sub> with the given function f: S<sub>3</sub> -> X, there is a known efficient algorithm called the Coset Sampling Algorithm. This algorithm allows us to find the hidden subgroup by exploiting the structure of the symmetric group.\n",
        "\n",
        "The Coset Sampling Algorithm for S<sub>3</sub> proceeds as follows:\n",
        "\n",
        "1. Start with an initial candidate subgroup H = {identity permutation}.\n",
        "2. For each element g in S<sub>3</sub> - H:\n",
        "   a. Compute the coset gH.\n",
        "   b. Apply the function f to each element in the coset gH and record the results.\n",
        "3. Analyze the recorded results to determine if there is a consistent pattern.\n",
        "   a. If the results show that all elements in a coset map to the same value, update H to include the elements of that coset.\n",
        "   b. If the results do not exhibit a consistent pattern, return to step 2 with the next element in S<sub>3</sub> - H.\n",
        "4. Repeat steps 2 and 3 until no new cosets are added to H.\n",
        "5. The final H obtained is the hidden subgroup.\n",
        "\n",
        "In the case of S<sub>3</sub>, the Coset Sampling Algorithm will correctly identify the hidden subgroup { (1, 2, 3), (2, 1, 3), (3, 2, 1) } based on the given function f.\n",
        "\n",
        "It's important to note that the Coset Sampling Algorithm is specific to the symmetric group S<sub>3</sub> and may not generalize to other groups. The Hidden Subgroup Problem for other groups often requires different algorithms and techniques tailored to the specific group's structure."
      ],
      "metadata": {
        "id": "Y38g40y_WosB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you write this as a python function?\n",
        "\n",
        "Certainly! Here's an example of a Python function that implements the Coset Sampling Algorithm for the Hidden Subgroup Problem in the case of the symmetric group S<sub>3</sub>:\n",
        "\n",
        "In this example, the hidden_subgroup_algorithm_S3 function implements the Coset Sampling Algorithm for S<sub>3</sub>. The f parameter represents the given function that maps permutations to certain values. The algorithm iterates through the elements of S<sub>3</sub> and updates the candidate subgroup H based on the function's output until no new cosets are added to H. Finally, the function returns the discovered hidden subgroup.\n",
        "\n",
        "You can run this code to find the hidden subgroup in the provided example function example_function. Feel free to modify the function example_function or provide your own function f to experiment with different scenarios."
      ],
      "metadata": {
        "id": "nixfuFWvWrEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Speedup in Combinatorial Optimization: the status**\n",
        "\n",
        "A pivotal challenge in quantum information science's ever-evolving realm is designing quantum algorithms that outpace their classical counterparts. A recent experimental observation showcased a superlinear quantum speedup in solving the Maximum Independent Set problem on specific graph instances. This has spurred the development of a comprehensive theoretical framework to compare the performance of quantum adiabatic algorithms and classical Markov chain Monte Carlo algorithms.\n",
        "\n",
        "The Quantum-Classic Conundrum\n",
        "\n",
        "Combinatorial optimization problems, foundational to modern computer science, encompass NP-hard problems that remain elusive to efficient solutions through known algorithms. While classical combinatorial optimization algorithms focus on minimizing a cost function over bit strings, Quantum adiabatic algorithms (QAAs) serve as their quantum analogs, preparing low-energy states of a classical cost Hamiltonian through adiabatic evolution. However, the relative efficacy of QAA and classical counterparts like simulated annealing (SA) remains a topic of debate and exploration.\n",
        "\n",
        "From Theory to Application\n",
        "\n",
        "The recent experimental results, particularly the superlinear speedup observed using a programmable Rydberg atom array, have ignited a renewed interest in this domain. This study presents a theoretical framework that zeroes in on problem instances characterized by flat energy landscapes. Here, the quantum algorithm's performance hinges on the (de)localization of the low-energy eigenstates of the adiabatic Hamiltonian. When these eigenstates are delocalized and the quantum evolution is optimized, the QAA can achieve a significant speedup over classical algorithms like SA.\n",
        "\n",
        "Toward a Quantum Future\n",
        "\n",
        "Building on this foundation, the researchers introduce a modified QAA that showcases a quadratic speedup over SA on specific problem instances. This algorithm employs local Hamiltonians without a sign problem, making them more amenable to quantum Monte Carlo simulations. The study concludes by applying these insights to interpret experimental observations, identifying instances where quantum algorithms either outperform or underperform classical ones due to the localization properties of the low-energy eigenstates.\n",
        "\n",
        "In Summary\n",
        "\n",
        "This work offers a deep dive into the intricate dance between quantum and classical algorithms in the context of combinatorial optimization. Shedding light on the conditions under which quantum algorithms can achieve a speedup paves the way for further advancements in the quantum computing landscape.\n",
        "\n",
        "Link to the publication:\n",
        "\n",
        "https://arxiv.org/pdf/2306.13123.pdf\n"
      ],
      "metadata": {
        "id": "NzA9GTLcVnpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorization of Optimization Problem with different orders of their objective function**\n",
        "* Einführung: [Optimization_problem](https://en.m.wikipedia.org/wiki/Optimization_problem)\n",
        "* **First order**: [Linear optimization](https://en.m.wikipedia.org/wiki/Linear_programming): Many practical problems in [operations research](https://en.m.wikipedia.org/wiki/Operations_research) can be expressed as linear programming problems. See [Simplex_algorithm](https://en.m.wikipedia.org/wiki/Simplex_algorithm)\n",
        "* **Second Order**: [Quadratic optimization (Quadratic programming)](https://en.m.wikipedia.org/wiki/Quadratic_programming): finance (for portfolio optimization), machine learning (for support vector machines), and operations research. Example: In the Markowitz model, the objective function to be minimized is the portfolio variance, which is a quadratic function of the decision variables (asset weights in the portfolio), given the covariance matrix of the returns of the assets.\n",
        "* **Higher Order**: Polynomial optimization problems or non-linear optimization problems (optimize design of a machine part, where the objective function includes terms related to the volume of material used which might be cubic if we're considering three-dimensional parts. Often require more advanced techniques, such as [interior-point methods](https://de.m.wikipedia.org/wiki/Innere-Punkte-Verfahren), branch-and-bound techniques, or heuristic methods."
      ],
      "metadata": {
        "id": "caPGPXRMFtqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quantum annealing is a quantum computing algorithm that can be used to solve a variety of problems, including NP-hard problems.\n",
        "* However, the efficiency of quantum annealing is limited by the **exponential closing of the spectral gap with increasing system size.**\n",
        "* The spectral gap is a measure of the connectivity of a quantum system. **A large spectral gap means that the system is well-connected, while a small spectral gap means that the system is poorly connected**. The exponential closing of the spectral gap means that the spectral gap decreases exponentially with increasing system size.\n",
        "* This means that quantum annealing becomes exponentially slow for solving NP-hard problems as the system size increases. This is because **the quantum system becomes more and more poorly connected as the system size increases, and it takes longer and longer for the system to reach its ground state**.\n",
        "* There are a number of ways to address the problem of exponential closing of the spectral gap. One approach is to use a technique called adiabatic quantum computation. Adiabatic quantum computation is a variation of quantum annealing that uses a slower annealing schedule. This can help to prevent the spectral gap from closing too quickly.\n",
        "* Another approach to addressing the problem of exponential closing of the spectral gap is to use a technique called quantum error correction. Quantum error correction is a technique that can be used to protect quantum systems from noise. This can help to prevent the quantum system from becoming too poorly connected as the system size increases.\n",
        "* The problem of exponential closing of the spectral gap is a major challenge for quantum annealing. However, there are a number of promising approaches to addressing this problem. As quantum annealing technology continues to develop, it is possible that quantum annealing will become a viable solution for solving NP-hard problems.\n"
      ],
      "metadata": {
        "id": "OzaliSbDGPm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity and Solution Finding in Quantum Annealing**\n",
        "\n",
        "Quantum annealing is a quantum computing technique used to find the minimum value of a given objective function over a defined set of possible solutions. It draws inspiration from classical annealing, a process in which a system is gradually cooled to find its lowest energy state. The hope in quantum annealing is to leverage quantum properties, such as superposition and tunneling, to more efficiently explore the solution space and find the ground state, which represents the minimum of the objective function.\n",
        "\n",
        "Connectivity plays an essential role in this process. Let's understand this with a few key points:\n",
        "\n",
        "1. **Local Minima and Global Minima**: In complex optimization problems, there are often many local minima. Finding the global minimum (or a sufficiently good approximation of it) is the main challenge. High connectivity means that the system can more easily explore the solution space, jumping from one potential solution to another and avoiding getting trapped in local minima.\n",
        "\n",
        "2. **Quantum Tunneling**: One of the primary advantages of quantum annealing over classical annealing is quantum tunneling. Tunneling allows the system to \"pass through\" barriers between states, meaning that the system can more easily escape local minima and move towards the global minimum. If the quantum system's connectivity is low, the benefit from quantum tunneling can be diminished.\n",
        "\n",
        "3. **Effective Exploration of Solution Space**: In a highly connected system, any given qubit (quantum bit) is more likely to interact with many other qubits. <font color=\"blue\">This allows for a richer exploration of the solution space. With fewer connections, the qubits are limited in their interactions, making the exploration less effective.</font>\n",
        "\n",
        "4. **Scaling Issues**: <font color=\"blue\">as the system size increases, maintaining high connectivity becomes more challenging. The difficulty in keeping high connectivity in larger systems is one reason why quantum annealing's efficiency decreases for more complex NP-hard problems</font>. The sparser the connectivity, the less able the system is to effectively leverage quantum effects.\n",
        "\n",
        "5. **Error Correction and Noise**: Highly connected quantum systems can be more susceptible to certain types of errors or noise. So, while high connectivity can aid in the exploration of solution space, it can also introduce challenges in terms of maintaining the coherence and reliability of the quantum information.\n",
        "\n",
        "In summary, high connectivity in quantum annealing is crucial for effectively exploring the solution space, leveraging quantum effects like tunneling, and ensuring the system doesn't get trapped in local minima. As system sizes increase, maintaining this connectivity becomes more challenging, which contributes to the reduced efficiency of quantum annealing for large NP-hard problems."
      ],
      "metadata": {
        "id": "B3FyVDMsHbsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Does spectral gap stand in quantum annealing for difference between global and local optimum, or graph connectivity?* For graph connectivity it would be bad to have a small spectral gap, meanhwile for optimum it would be potentially good.\n",
        "\n",
        "*Spectral gap stands for graph connectivity. if that is low, then there are fewer connections in the graph, and that results that it's harder to find the global optimum*"
      ],
      "metadata": {
        "id": "bFO9RXzswtM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Around the same time, it was argued that QA is exponentially slow for solving NP-hard problems due to exponential closing of the spectral gap with increasing system size*\n",
        "https://arxiv.org/pdf/2212.13649.pdf\n",
        "\n",
        "[20]  Jorg, T., Krzakala, F., Kurchan, J. & Maggs, A. Simple glass models and their quantum annealing. Physical review letters 101, 147204 (2008).\n",
        "[21]  Altshuler, B., Krovi, H. & Roland, J. Adiabatic quantum optimization fails for random in- stances of np-complete problems. arXiv preprint arXiv:0908.2782 (2009).\n",
        "\n",
        "*exponential closing of the quantum gap with increasing problem size* https://www.nature.com/articles/s41467-018-05239-9\n",
        "\n",
        "*The efficiency of Adiabatic Quantum Annealing is limited by the scaling with system size of the minimum gap that appears between the ground and first excited state in the annealing energy spectrum. In general the algorithm is unable to find the solution to an optimisation problem in polynomial time due to the presence of avoided level crossings at which the gap size closes exponentially with system size.* https://arxiv.org/pdf/2203.06779.pdf\n",
        "  * A particular problem which has been highlighted is the potential for so called perturbative crossings to form between the low energy eigenstates of the problem Hamiltonian towards the end of the anneal [11, 12]. The corresponding energy gap has been found to be exponentially small with the Hamming distance between the eigenstates, which can generally be expected to grow with the system size [11]."
      ],
      "metadata": {
        "id": "ZpMmh8PMFC_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In quantum annealing, an exponential spectral gap generally presents a challenge** = energy difference between ground state and first excited state is very small. This small gap can make it harder for the quantum annealer to distinguish between the two states, especially when there's noise in the system. \"However, efficiency of quantum annealing is limited by the exponential closing of the spectral gap with increasing system size.\" Problems:\n",
        "  * In order to stay close to ground state throughout annealing process, evolution parameter \\( s \\) or \\( t \\) needs to change very slowly. This can lead to very long quenching times, making annealing process computationally inefficient.\n",
        "  * Thermal Excitations: thermal fluctuations can easily push system out of ground state because energy difference is small. This makes finding the correct solution (the ground state) less likely.\n",
        "  * Small spectral gap implies sensitivity to external noise. Small gap can cause system to be easily perturbed away from desired evolution.\n",
        "  * Techniques and strategies, like optimized annealing schedules or error-correction methods, may be needed to cope with these challenges.\n",
        "* An exponential spectral gap might mean that the first excited state is close to ground state in terms of energy, the implications for solution quality can vary. Depending on the problem, this could mean you have a nearly optimal solution, or it could mean you're still far from the best possible configuration.\n",
        "  1. **Nature of the Problem**: The importance of the energy difference can vary depending on the nature of the problem you're trying to solve. In some optimization problems, even a small energy difference might correspond to a significant difference in solution quality. In others, the difference might be negligible in practical terms.\n",
        "  2. **Many-body Systems**: Quantum annealing is often applied to complex many-body systems. In these systems, the difference in energy between the ground state and the first excited state might correspond to a complex reconfiguration of many particles or spins. So, the \"distance\" between these states in terms of system configuration might be significant even if their energy difference is small.\n",
        "  3. **Landscapes with Many Local Minima**: In complex optimization problems, there can be many local minima (sub-optimal solutions that are better than their immediate neighbors). The presence of an exponential spectral gap might indicate that the system is easily getting trapped in one of these local minima. While the first excited state might be close in energy to the ground state, there might be other states with much higher energies that are also close in terms of the system's configuration.\n",
        "  4. **Goal of Quantum Annealing**: The primary goal of quantum annealing is to find the global minimum (or a very close approximation to it). So, while the first excited state might offer a \"good\" solution, the aim is typically to find the \"best\" solution, especially in problems where small differences can have amplified outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "9NOM1t6kumrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Scott Aaronson - Dismantling quantum hype](https://youtu.be/qs0D9sdbKPU)"
      ],
      "metadata": {
        "id": "OuTUAgMok4ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@quantum_wa/quantum-annealing-cdb129e96601\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Annealing_(materials_science)"
      ],
      "metadata": {
        "id": "W_qghovmjQpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kerr and ising gate: non linear gates\n",
        "\n",
        "* The Kerr gate is a gate that introduces nonlinearity into a quantum system. The Kerr gate can be used to implement a variety of quantum operations, such as quantum logic gates and quantum simulations.\n",
        "\n",
        "* The quartic gate is a gate that introduces even higher-order nonlinearity into a quantum system. The quartic gate can be used to implement a variety of quantum operations, such as quantum error correction and quantum machine learning.\n",
        "\n",
        "* The cubic gate is a gate that introduces cubic nonlinearity into a quantum system. The cubic gate can be used to implement a variety of quantum operations, such as quantum cryptography and quantum teleportation."
      ],
      "metadata": {
        "id": "k0Tcx25SAxn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the Kerr ising model?**\n",
        "\n",
        "\n",
        "The Kerr Ising model (KIM) is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity. The Ising model is a classical model of interacting spins, while the Kerr nonlinearity is a quantum effect that describes the interaction of light with matter.\n",
        "\n",
        "The KIM is described by the following Hamiltonian:\n",
        "\n",
        "> $H = -\\sum_{i,j} J_{ij} \\sigma^z_i \\sigma^z_j - \\sum_i \\frac{\\alpha}{2} \\sigma^x_i^2$\n",
        "\n",
        "where $\\sigma^z_i$ and $\\sigma^x_i$ are Pauli matrices that represent the spin of the $i$th qubit, $J_{ij}$ is the Ising coupling strength between the $i$th and $j$th qubits, and $\\alpha$ is the Kerr nonlinearity coefficient.\n",
        "\n",
        "The KIM has been studied in a variety of contexts, including quantum annealing, quantum simulation, and quantum information processing. It has been shown that the KIM can be used to solve certain NP-complete problems, and it can also be used to simulate the dynamics of other quantum many-body systems.\n",
        "\n",
        "Here are some of the key features of the Kerr Ising model:\n",
        "\n",
        "* It is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity.\n",
        "* It has been shown to be useful for quantum annealing, quantum simulation, and quantum information processing.\n",
        "* It has been studied in a variety of contexts, including theoretical studies and experimental implementations.\n",
        "\n",
        "The Kerr Ising model is a promising platform for a variety of quantum applications. As our understanding of the model continues to grow, we can expect to see even more exciting applications in the future.\n",
        "\n",
        "https://iopscience.iop.org/article/10.1088/1367-2630/ab7255"
      ],
      "metadata": {
        "id": "SfRYAYFrizmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the difference between adiabatic quantum computing and quantum annealing?**\n",
        "\n",
        "Adiabatic quantum computing (AQC) and quantum annealing (QA) are both approaches to quantum computing that use a time-dependent Hamiltonian to solve problems. However, there are some key differences between the two approaches.\n",
        "\n",
        "In AQC, the Hamiltonian is slowly varied from an initial Hamiltonian that is easy to prepare to a final Hamiltonian that encodes the solution to the problem. The system is then allowed to evolve adiabatically, meaning that it remains in its ground state throughout the evolution. This ensures that the system will end up in the ground state of the final Hamiltonian, which is the solution to the problem.\n",
        "\n",
        "In QA, the Hamiltonian is also slowly varied from an initial Hamiltonian to a final Hamiltonian. However, the final Hamiltonian does not necessarily encode the solution to the problem. Instead, the goal of QA is to find the ground state of the final Hamiltonian, which is a global minimum of the energy landscape. This can be useful for solving optimization problems, where the goal is to find the minimum value of a function.\n",
        "\n",
        "Another difference between AQC and QA is that AQC is a universal model of quantum computation, while QA is not. This means that AQC can be used to solve any problem that can be solved by a classical computer, while QA is only able to solve optimization problems.\n",
        "\n",
        "In practice, QA is often used to solve problems that are difficult for classical computers to solve. This is because the adiabatic evolution can help the system to tunnel through energy barriers that would be insurmountable for a classical computer. However, there is still some debate about whether QA is truly a quantum-mechanical phenomenon, or whether it can be explained by classical physics.\n",
        "\n",
        "Here is a table that summarizes the key differences between AQC and QA:\n",
        "\n",
        "| Feature | Adiabatic Quantum Computing | Quantum Annealing |\n",
        "|---|---|---|\n",
        "| Hamiltonian | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that encodes the solution to the problem. | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that is a global minimum of the energy landscape. |\n",
        "| Universality | Yes | No |\n",
        "| Applications | Any problem that can be solved by a classical computer | Optimization problems |\n",
        "| Quantum-mechanical phenomenon | Yes | Possibly |\n"
      ],
      "metadata": {
        "id": "AX_5iM-LivWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system\n",
        "\n",
        "Imaginary time is a concept derived from quantum mechanics and statistical mechanics. It introduces the idea of replacing \"real time\" with \"imaginary time\" by a Wick rotation in the complex plane. That is, the time variable 't' is replaced with an imaginary number 'it', where 'i' is the imaginary unit.\n",
        "\n",
        "Imaginary time evolution plays a significant role in several areas of physics, including quantum field theory, statistical mechanics, and quantum computing.\n",
        "\n",
        "1. **Quantum Field Theory and Statistical Mechanics**: In these fields, the use of imaginary time is often a mathematical trick that simplifies calculations. By transforming to imaginary time, the calculations of quantum mechanics often become calculations in statistical mechanics. This is utilized in the technique called \"path integral formulation,\" where the evolution of a system in imaginary time makes the system go to its lowest energy state or the ground state.\n",
        "\n",
        "2. **Quantum Computing**: In quantum computing, the idea of imaginary time evolution can be used to design quantum algorithms for tasks such as finding the ground state of a system. This is used in the Quantum Approximate Optimization Algorithm (QAOA) and the Quantum Imaginary Time Evolution (QITE) algorithm.\n",
        "\n",
        "Remember that \"imaginary time\" is not about time in the sense that we experience it. Instead, it's a mathematical construct that physicists use to solve certain types of problems.\n",
        "\n",
        "Imaginary time is an unphysical, yet powerful, mathematical concept. It has been utilised in numerous physical domains, including quantum mechanics, statistical mechanics and cosmology. Often referred to as performing a ‘Wick rotation’\n",
        "\n",
        "https://www.nature.com/articles/s41534-019-0187-2"
      ],
      "metadata": {
        "id": "yO5fzJsOye7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104129"
      ],
      "metadata": {
        "id": "dbVdUXkbE4f-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Random Circuit Sampling*"
      ],
      "metadata": {
        "id": "-91jSrDIX7jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[New Algorithm Closes Quantum Supremacy Window](https://www.quantamagazine.org/new-algorithm-closes-quantum-supremacy-window-20230109/)\n",
        "\n",
        "*Random circuit sampling, a popular technique for showing the power of quantum computers, doesn’t scale up if errors go unchecked.*\n",
        "\n",
        "If you imagine continually increasing the number of qubits as complexity theorists do, and you also want to account for errors, you need to decide whether you’re also going to keep adding more layers of gates — increasing the circuit depth, as researchers say. Suppose you keep the circuit depth constant at, say, a relatively shallow three layers, as you increase the number of qubits. You won’t get much entanglement, and the output will still be amenable to classical simulation. On the other hand, if you increase the circuit depth to keep up with the growing number of qubits, the cumulative effects of gate errors will wash out the entanglement, and the output will again become easy to simulate classically."
      ],
      "metadata": {
        "id": "N_8INTIQe7Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80 qubits: fidelyt decreases"
      ],
      "metadata": {
        "id": "JD-K1jPIy-6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/s41586%20019%201666%205\n",
        "\n",
        "* Original experiment from Aaronson: boson sampling. Google: random circuit sampling (RCS). In both it is exponentially hard to compute the output distribution (also for quantum -> limits of quantum computing). But a quantum computer can sample efficiently from this distribution\n",
        "* Challenge: you can‘t go to 80 qubits or so, because then it is impossible for a classical computer to verify. Google used 53 qubits, because then you can at least verify (with 10,000 years on classical computer, doing extrapolation). But this opens the door to spoof the results classically (-> what did aaronson mean by that?)\n",
        "* Google did two things badly:\n",
        "    * They didn‘t properly estimate the 10.000 years on a supercomputer (breaking down and then extrapolate)\n",
        "    * google didn‘t use the best classical algorithm to compare with: later people used tensor networks to speed up the classical calculation and now it‘s down to 1 few seconds.\n",
        "* But this raises a question of what means supremacy: in terms of electrticity usage, quantum is still exponentially better than classical in this example: kilowatt vs megawatt or so\n",
        "* In terms of clockspeed though, classical is equally fast for this expoeriement, because classical computing is highly parallelizable (many operations in parallel)\n"
      ],
      "metadata": {
        "id": "Sie8LRdbgjj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spoofing: technical term, do something fraudently, how do i know that they ran the computation and got a result?\n",
        "easy to sample random bits from porter thomas distribution.\n",
        "\n",
        "page 3: https://www.cs.cmu.edu/~odonnell/quantum18/lecture25.pdf\n",
        "\n",
        "for RCS we dont have an euqivalent way of verfying this? KL divergence between what it should look like\n",
        "\n",
        "spoofing: clasical ciruti doesnt smiluate, but does anything up to simulaotyon. it can fool a verifyer."
      ],
      "metadata": {
        "id": "U_wyYl8ExRxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Finance*"
      ],
      "metadata": {
        "id": "uxq8EP-8YIIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum option pricing via the Karhunen-Loève expansion\n",
        "\n",
        "https://arxiv.org/abs/2402.10132"
      ],
      "metadata": {
        "id": "GcB55l_GH0pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum computing for finance: Overview and prospects: https://www.sciencedirect.com/science/article/pii/S2405428318300571\n",
        "\n",
        "Quantum computing for finance: https://arxiv.org/abs/2307.11230"
      ],
      "metadata": {
        "id": "1J77NPkGfErd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Chemistry*"
      ],
      "metadata": {
        "id": "U2lhywuKYNRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 20 molecules for quantum computing: https://pennylane.ai/blog/2024/01/top-20-molecules-for-quantum-computing/\n",
        "\n",
        "Video: [Jennifer Cano: topological quantum chemistry](https://youtu.be/-aSTTyes5jI?si=st1Hq6T-9NNoKRUg)\n",
        "\n",
        "Video: [Aurora Clarke: topology in quantum chemistry applications](https://youtu.be/qSJeynJbmqU?si=EDc99iTdVxG9CY0w)\n",
        "\n",
        "Video: [Hunting for real-world applications of quantum algorithms for chemistry](https://www.youtube.com/watch?v=XOLHX6GfRpQ&list=WL&index=6&t=153s)"
      ],
      "metadata": {
        "id": "y3LZgO4OYNSA"
      }
    }
  ]
}