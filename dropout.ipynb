{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdu-Ydo47Gk",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6vAlModDeF-",
        "colab_type": "text"
      },
      "source": [
        "## What is it and how it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buEcf-ZDItO",
        "colab_type": "text"
      },
      "source": [
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(künstliches_neuronales_Netz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoPCh0TADiP0",
        "colab_type": "text"
      },
      "source": [
        "## Is Dropout outdated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSALD_oJBghU",
        "colab_type": "text"
      },
      "source": [
        "Neural Network:  Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don’t Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6kH4LNgDoh3",
        "colab_type": "text"
      },
      "source": [
        "## Use Dropout along with L1/L2 Regularization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQnowbqDxdW",
        "colab_type": "text"
      },
      "source": [
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ℓ 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of “switching off” some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network’s over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can “work together” or rather end up “fighting each other”. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHu0whFdwoH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmKZX1K4Jrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niyell7b3bLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYk9nB0c3fJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Select Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuddIzZYu6O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8zk0a7VB1nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SebH8DxRCDIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "1f9b3121-b31e-44cc-82ca-42fed7054455"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(dropout))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=5, validation_data=(x_test, y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.7253 - acc: 0.7387 - val_loss: 0.4972 - val_acc: 0.8146\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.5815 - acc: 0.7885 - val_loss: 0.4638 - val_acc: 0.8242\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 75us/sample - loss: 0.5458 - acc: 0.8011 - val_loss: 0.4306 - val_acc: 0.8457\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.5247 - acc: 0.8082 - val_loss: 0.4233 - val_acc: 0.8427\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.5168 - acc: 0.8110 - val_loss: 0.4228 - val_acc: 0.8456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9b0c3a6320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}