{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "activation_function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7rZnJaGTWQw0",
        "outputId": "1af5f0db-d6b0-4d84-ea0a-5744f572f9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNuFUedQd_vs",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksiOCVh6k232",
        "colab_type": "text"
      },
      "source": [
        "https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZpemZIdcay",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "* Use ReLU. Be careful with your learning rates\n",
        "* Try out Leaky ReLU / Maxout / ELU\n",
        "* Try out tanh but don’t expect much\n",
        "* Don’t use sigmoid\n",
        "\n",
        "\n",
        "**Activation Functions - Key Points**\n",
        "* Reside within neurons\n",
        "* Transform input values into acceptable and useful range\n",
        "* Allow pass-through of values which are useful in subsequent layers of neurons\n",
        "* Default hidden layer activation function is ReLU\n",
        "* Sigmoid only for binary classification output layer\n",
        "\n",
        "https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68EhvxK9Pet9",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_3.png)\n",
        "\n",
        "Source: [Stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dJRlYxPVGZ",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0vpxOzOXKr",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4-kecUPvDN",
        "colab_type": "text"
      },
      "source": [
        "Comparison of Activation Functions:\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyZPPEcYc0",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "Squashes numbers to range [0,1]. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. But 3 problems:\n",
        "* Saturated neurons “kill” the gradients (look at x= -10, 0 and 10)\n",
        "* Sigmoid outputs are not zero-centered (Consider what happens when the input to a neuron (x) is always positive. What can we say about the gradients on w? They are always all positive or all negative (and run zig-zag)! (this is also why you want zero-mean data!)\n",
        "* exp() is a bit compute expensive\n",
        "* Logistic Regression for binary classification problems. between 0 and 1. \n",
        "* sigmoid activation derived from the mean field solution of a Boltzmann Machine. \n",
        "* Old way, slow learner! sigmoid stört zero mean, weil zw 0 und 1, erwartungswert nicht mehr bei Null. learning time geht länger. also be derived as the€maximum likelihood solution to for logistic regression in statistics). \n",
        "* Problem: logistic sigmoid€can cause a neural network€to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since€neural networks use€the feed-forward€activations to calculate parameter gradients, this can result in model parameters€that are updated less regularly than we would like, and are thus “stuck” in their current state. \n",
        "* sigmoid works well for a classifier: approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence\n",
        "* The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
        "* The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
        "* The function is monotonic but function’s derivative is not.\n",
        "* The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
        "* The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
        "\n",
        "\n",
        "![Sigmoid](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_2.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDAIwoT1dnWo",
        "colab_type": "text"
      },
      "source": [
        "## tanh\n",
        "\n",
        "LeCun et al., 1991\n",
        "* Squashes numbers to range [-1,1]\n",
        "* zero centered (nice)\n",
        "* The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
        "* The function is differentiable.\n",
        "* The function is monotonic while its derivative is not monotonic.\n",
        "* The tanh function is mainly used classification between two classes.\n",
        "* still kills gradients when saturated\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O35eK5VlSY87",
        "colab_type": "text"
      },
      "source": [
        "## ReLU\n",
        "\n",
        "* Krizhevsky et al., 2012\n",
        "* rectified linear units, faster and more efficient, since fewer neurons are activated (less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations). \n",
        "* No gradient vanishing problem, as Relu’s gradient is constant = 1. Sparsity: since output 0 for negative values of x! When W*x < 0, Relu gives 0, which means sparsity. Less calculation load. This may be least important. \n",
        "* However, ReLu may amplify the signal inside the network more than softmax and sigmoid. \n",
        "* But: dying ReLU problem for values zero and smaller: neurons will never reactivated. Solution: leaky ReLU, noisy ReLU (in RBMs) and ELU (exponential linear units)\n",
        "* ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.\n",
        "\n",
        "Plus Side\n",
        "\n",
        "* Does not saturate (in +region)\n",
        "* Very computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
        "Actually more biologically plausible than sigmoid\n",
        "\n",
        "Bad Side\n",
        "\n",
        "* Not zero-centered output\n",
        "* An annoyance: what is the gradient when x < 0? What happens when x = -10, 0 or 10?\n",
        "\n",
        "People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWsQRCAfmaK",
        "colab_type": "text"
      },
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "* Mass et al., 2013 and He et al., 2015\n",
        "* Does not saturate\n",
        "* Computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice! (e.g. 6x) will not “die”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUyiCkgf4Aw",
        "colab_type": "text"
      },
      "source": [
        "## ELU\n",
        "\n",
        "* Exponential Linear Units\n",
        "* Clevert et al., 2015\n",
        "* All benefits of ReLU\n",
        "* Closer to zero mean outputs\n",
        "* Negative saturation regime compared with Leaky ReLU adds some robustness to noise \n",
        "* But Computation requires exp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2ikh67gXRC",
        "colab_type": "text"
      },
      "source": [
        "## SELU\n",
        "\n",
        "* scaled exponential linear units\n",
        "* instead of normalizing the output of the activation function — the activation function suggested (SELU — scaled exponential linear units) outputs normalized values. https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "* Background: batchnormalization for feedfirward networks: Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. (https://arxiv.org/abs/1502.03167)\n",
        "* Negative values sometimes: Scaling the function is the mechanism by which the authors accomplish the goal (of self-normalizing properties). As a byproduct, they sometimes output negative values, but there's no hidden meaning in it. It just makes the math work out. \n",
        "* **SELU vs RELU**: https://www.hardikp.com/2017/07/24/SELU-vs-RELU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GDi-lkbeMo",
        "colab_type": "text"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers.\n",
        "* usually used in the last layer\n",
        "* Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) \n",
        "* is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We use the (standard) Logistic Regression model in binary classification tasks. in softmax regression (SMR), we replace the sigmoid logistic function by the so-called€softmax function€φ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB13XKQhyz9",
        "colab_type": "text"
      },
      "source": [
        "## Maxout\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ckAo-8hCBH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_KlO-if5DL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "d0f71048-746e-4dfe-ced6-8f871f3a8b66"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_CG5ooCgDnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "aa18283d-876f-446f-8edc-c60ed822f132"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "sample, sample_label = x_train[0], y_train[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jNUhO2Og63T",
        "colab_type": "text"
      },
      "source": [
        "## Select Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqePy-4uoZ9T",
        "colab_type": "text"
      },
      "source": [
        "Activation functions can be defined as layers:\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n",
        "\n",
        "Activation functions can be defined as activation:\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT9RYASwgadc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation='tanh' # tanh is default in LSTM\n",
        "recurrent_activation='relu' # sigmoid is default in LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMwV7R8ohG3d",
        "colab_type": "text"
      },
      "source": [
        "## Build Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBTHnZyagE-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
        "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
        "input_dim = 28\n",
        "\n",
        "units = 64\n",
        "output_size = 10  # labels are from 0 to 9\n",
        "\n",
        "# Build the RNN model\n",
        "def build_model(allow_cudnn_kernel=True):\n",
        "  # CuDNN is only available at the layer level, and not at the cell level.\n",
        "  # This means `LSTM(units)` will use the CuDNN kernel,\n",
        "  # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
        "  if allow_cudnn_kernel:\n",
        "    # The LSTM layer with default options uses CuDNN.\n",
        "    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(None, input_dim), activation=activation, recurrent_activation=recurrent_activation)\n",
        "  else:\n",
        "    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
        "    lstm_layer = tf.keras.layers.RNN(\n",
        "        tf.keras.layers.LSTMCell(units, activation=activation, recurrent_activation=recurrent_activation),\n",
        "        input_shape=(None, input_dim))\n",
        "  model = tf.keras.models.Sequential([\n",
        "      lstm_layer,\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(output_size)]\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQW7fKWVgIvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(allow_cudnn_kernel=True)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-V0si7bgL8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "645e1055-bbca-4461-9783-ecd485bf80fd"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          validation_data=(x_test, y_test),\n",
        "          batch_size=batch_size,\n",
        "          epochs=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 29s 488us/sample - loss: 1.0450 - acc: 0.6795 - val_loss: 0.5050 - val_acc: 0.8444\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 30s 500us/sample - loss: 0.3503 - acc: 0.8957 - val_loss: 0.3569 - val_acc: 0.8892\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 29s 490us/sample - loss: 0.2285 - acc: 0.9322 - val_loss: 0.1998 - val_acc: 0.9400\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 29s 491us/sample - loss: 0.1788 - acc: 0.9469 - val_loss: 0.2153 - val_acc: 0.9364\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 30s 493us/sample - loss: 0.1501 - acc: 0.9552 - val_loss: 0.1484 - val_acc: 0.9524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f34dd8793c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}