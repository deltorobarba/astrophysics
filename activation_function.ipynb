{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "activation_function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7rZnJaGTWQw0",
        "outputId": "1af5f0db-d6b0-4d84-ea0a-5744f572f9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNuFUedQd_vs",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksiOCVh6k232",
        "colab_type": "text"
      },
      "source": [
        "https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZpemZIdcay",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "* Use ReLU. Be careful with your learning rates\n",
        "* Try out Leaky ReLU / Maxout / ELU\n",
        "* Try out tanh but don’t expect much\n",
        "* Don’t use sigmoid\n",
        "\n",
        "\n",
        "**Activation Functions - Key Points**\n",
        "* Reside within neurons\n",
        "* Transform input values into acceptable and useful range\n",
        "* Allow pass-through of values which are useful in subsequent layers of neurons\n",
        "* Default hidden layer activation function is ReLU\n",
        "* Sigmoid only for binary classification output layer\n",
        "\n",
        "https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68EhvxK9Pet9",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_3.png)\n",
        "\n",
        "Source: [Stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dJRlYxPVGZ",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0vpxOzOXKr",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4-kecUPvDN",
        "colab_type": "text"
      },
      "source": [
        "Comparison of Activation Functions:\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyZPPEcYc0",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "Squashes numbers to range [0,1]. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. But 3 problems:\n",
        "* Saturated neurons “kill” the gradients (look at x= -10, 0 and 10)\n",
        "* Sigmoid outputs are not zero-centered (Consider what happens when the input to a neuron (x) is always positive. What can we say about the gradients on w? They are always all positive or all negative (and run zig-zag)! (this is also why you want zero-mean data!)\n",
        "* exp() is a bit compute expensive\n",
        "* Logistic Regression for binary classification problems. between 0 and 1. \n",
        "* sigmoid activation derived from the mean field solution of a Boltzmann Machine. \n",
        "* Old way, slow learner! sigmoid stört zero mean, weil zw 0 und 1, erwartungswert nicht mehr bei Null. learning time geht länger. also be derived as the€maximum likelihood solution to for logistic regression in statistics). \n",
        "* Problem: logistic sigmoid€can cause a neural network€to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since€neural networks use€the feed-forward€activations to calculate parameter gradients, this can result in model parameters€that are updated less regularly than we would like, and are thus “stuck” in their current state. \n",
        "* sigmoid works well for a classifier: approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence\n",
        "* The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
        "* The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
        "* The function is monotonic but function’s derivative is not.\n",
        "* The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
        "* The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDAIwoT1dnWo",
        "colab_type": "text"
      },
      "source": [
        "## tanh\n",
        "\n",
        "LeCun et al., 1991\n",
        "* Squashes numbers to range [-1,1]\n",
        "* zero centered (nice)\n",
        "* The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
        "* The function is differentiable.\n",
        "* The function is monotonic while its derivative is not monotonic.\n",
        "* The tanh function is mainly used classification between two classes.\n",
        "* still kills gradients when saturated\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O35eK5VlSY87",
        "colab_type": "text"
      },
      "source": [
        "## ReLU\n",
        "\n",
        "* Krizhevsky et al., 2012\n",
        "* rectified linear units, faster and more efficient, since fewer neurons are activated (less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations). \n",
        "* No gradient vanishing problem, as Relu’s gradient is constant = 1. Sparsity: since output 0 for negative values of x! When W*x < 0, Relu gives 0, which means sparsity. Less calculation load. This may be least important. \n",
        "* However, ReLu may amplify the signal inside the network more than softmax and sigmoid. \n",
        "* But: dying ReLU problem for values zero and smaller: neurons will never reactivated. Solution: leaky ReLU, noisy ReLU (in RBMs) and ELU (exponential linear units)\n",
        "* ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.\n",
        "\n",
        "Plus Side\n",
        "\n",
        "* Does not saturate (in +region)\n",
        "* Very computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
        "Actually more biologically plausible than sigmoid\n",
        "\n",
        "Bad Side\n",
        "\n",
        "* Not zero-centered output\n",
        "* An annoyance: what is the gradient when x < 0? What happens when x = -10, 0 or 10?\n",
        "\n",
        "People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWsQRCAfmaK",
        "colab_type": "text"
      },
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "* Mass et al., 2013 and He et al., 2015\n",
        "* Does not saturate\n",
        "* Computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice! (e.g. 6x) will not “die”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUyiCkgf4Aw",
        "colab_type": "text"
      },
      "source": [
        "## ELU\n",
        "\n",
        "* Exponential Linear Units\n",
        "* Clevert et al., 2015\n",
        "* All benefits of ReLU\n",
        "* Closer to zero mean outputs\n",
        "* Negative saturation regime compared with Leaky ReLU adds some robustness to noise \n",
        "* But Computation requires exp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2ikh67gXRC",
        "colab_type": "text"
      },
      "source": [
        "## SELU\n",
        "\n",
        "* scaled exponential linear units\n",
        "* instead of normalizing the output of the activation function — the activation function suggested (SELU — scaled exponential linear units) outputs normalized values. https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "* Background: batchnormalization for feedfirward networks: Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. (https://arxiv.org/abs/1502.03167)\n",
        "* Negative values sometimes: Scaling the function is the mechanism by which the authors accomplish the goal (of self-normalizing properties). As a byproduct, they sometimes output negative values, but there's no hidden meaning in it. It just makes the math work out. \n",
        "* **SELU vs RELU**: https://www.hardikp.com/2017/07/24/SELU-vs-RELU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GDi-lkbeMo",
        "colab_type": "text"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers.\n",
        "* usually used in the last layer\n",
        "* Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) \n",
        "* is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We use the (standard) Logistic Regression model in binary classification tasks. in softmax regression (SMR), we replace the sigmoid logistic function by the so-called€softmax function€φ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB13XKQhyz9",
        "colab_type": "text"
      },
      "source": [
        "## Maxout\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf3Nk-jgaRor",
        "colab_type": "text"
      },
      "source": [
        "## Import Data\n",
        "\n",
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">[weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfcHNO94aW1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"https://www.bgc-jena.mpg.de/wetter/\" (weather time series dataset)\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "# Read file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Select Univariate Data\n",
        "uni_data = df['T (degC)']\n",
        "uni_data.index = df['Date Time']\n",
        "\n",
        "# Define Window Size\n",
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "# Train test Split (first 300,000 rows of the data will be the training dataset, \n",
        "# there remaining will be the validation dataset. \n",
        "# This amounts to ~2100 days worth of training data.\n",
        "TRAIN_SPLIT = 300000\n",
        "tf.random.set_seed(13)\n",
        "uni_data = uni_data.values\n",
        "\n",
        "# Compute mean and standard deviation of training data\n",
        "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
        "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
        "\n",
        "# Standardize the data\n",
        "uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
        "\n",
        "# Create Data Pipeline (the model will be given the last 20 recorded temperature observations, and needs to learn to predict the temperature at the next time step.)\n",
        "univariate_past_history = 20\n",
        "univariate_future_target = 0\n",
        "\n",
        "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
        "                                           univariate_past_history,\n",
        "                                           univariate_future_target)\n",
        "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
        "                                       univariate_past_history,\n",
        "                                       univariate_future_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Choose Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqePy-4uoZ9T",
        "colab_type": "text"
      },
      "source": [
        "Activation functions can be defined as layers:\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n",
        "\n",
        "Activation functions can be defined as activation:\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O0BGImTXUqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation='tanh' # tanh is default in LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oy4a71ZnFg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recurrent_activation='sigmoid' # sigmoid is default in LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kk-evkrmMWh9",
        "colab": {}
      },
      "source": [
        "# tf.data to shuffle, batch, and cache the dataset. \n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# LSTM requires the input shape of the data it is being given.\n",
        "\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:], activation=activation, recurrent_activation=recurrent_activation),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYz6RN_mMyau"
      },
      "source": [
        "## Train & Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0opH9xi5MtIk",
        "outputId": "2f6b50a2-8b8b-44c7-e34c-e834b689c1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "EVALUATION_INTERVAL = 200\n",
        "EPOCHS = 10\n",
        "\n",
        "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 200 steps, validate for 50 steps\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 3s 17ms/step - loss: 0.4075 - val_loss: 0.1351\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.1118 - val_loss: 0.0360\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.0490 - val_loss: 0.0289\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.0257\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0299 - val_loss: 0.0235\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0317 - val_loss: 0.0224\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0208\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.0200\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0254 - val_loss: 0.0182\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0228 - val_loss: 0.0174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec45bf7240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMlGFScmnKTa",
        "colab_type": "text"
      },
      "source": [
        "The recurrent activation function 'relu' has a much lower evaluation loss than 'sigmoid', but it takes longer to compute."
      ]
    }
  ]
}