{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "activation_function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N7OFxbU41pw",
        "colab_type": "text"
      },
      "source": [
        "*Author: Alexander Del Toro Barba*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNuFUedQd_vs",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksiOCVh6k232",
        "colab_type": "text"
      },
      "source": [
        "https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZpemZIdcay",
        "colab_type": "text"
      },
      "source": [
        "**Summary**\n",
        "* Use ReLU. Be careful with your learning rates\n",
        "* Try out Leaky ReLU / Maxout / ELU\n",
        "* Try out tanh but don’t expect much\n",
        "* Don’t use sigmoid\n",
        "\n",
        "\n",
        "**Activation Functions - Key Points**\n",
        "* Reside within neurons\n",
        "* Transform input values into acceptable and useful range\n",
        "* Allow pass-through of values which are useful in subsequent layers of neurons\n",
        "* Default hidden layer activation function is ReLU\n",
        "* Sigmoid only for binary classification output layer\n",
        "\n",
        "https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68EhvxK9Pet9",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_3.png)\n",
        "\n",
        "Source: [Stanford.edu](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dJRlYxPVGZ",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0vpxOzOXKr",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4-kecUPvDN",
        "colab_type": "text"
      },
      "source": [
        "Comparison of Activation Functions:\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyZPPEcYc0",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "Squashes numbers to range [0,1]. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. But 3 problems:\n",
        "* Saturated neurons “kill” the gradients (look at x= -10, 0 and 10)\n",
        "* Sigmoid outputs are not zero-centered (Consider what happens when the input to a neuron (x) is always positive. What can we say about the gradients on w? They are always all positive or all negative (and run zig-zag)! (this is also why you want zero-mean data!)\n",
        "* exp() is a bit compute expensive\n",
        "* Logistic Regression for binary classification problems. between 0 and 1. \n",
        "* sigmoid activation derived from the mean field solution of a Boltzmann Machine. \n",
        "* Old way, slow learner! sigmoid stört zero mean, weil zw 0 und 1, erwartungswert nicht mehr bei Null. learning time geht länger. also be derived as the€maximum likelihood solution to for logistic regression in statistics). \n",
        "* Problem: logistic sigmoid€can cause a neural network€to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since€neural networks use€the feed-forward€activations to calculate parameter gradients, this can result in model parameters€that are updated less regularly than we would like, and are thus “stuck” in their current state. \n",
        "* sigmoid works well for a classifier: approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence\n",
        "* The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
        "* The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
        "* The function is monotonic but function’s derivative is not.\n",
        "* The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
        "* The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
        "\n",
        "\n",
        "![Sigmoid](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_2.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDAIwoT1dnWo",
        "colab_type": "text"
      },
      "source": [
        "## tanh\n",
        "\n",
        "LeCun et al., 1991\n",
        "* Squashes numbers to range [-1,1]\n",
        "* zero centered (nice)\n",
        "* The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
        "* The function is differentiable.\n",
        "* The function is monotonic while its derivative is not monotonic.\n",
        "* The tanh function is mainly used classification between two classes.\n",
        "* still kills gradients when saturated\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O35eK5VlSY87",
        "colab_type": "text"
      },
      "source": [
        "## ReLU\n",
        "\n",
        "* Krizhevsky et al., 2012\n",
        "* rectified linear units, faster and more efficient, since fewer neurons are activated (less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations). \n",
        "* No gradient vanishing problem, as Relu’s gradient is constant = 1. Sparsity: since output 0 for negative values of x! When W*x < 0, Relu gives 0, which means sparsity. Less calculation load. This may be least important. \n",
        "* However, ReLu may amplify the signal inside the network more than softmax and sigmoid. \n",
        "* But: dying ReLU problem for values zero and smaller: neurons will never reactivated. Solution: leaky ReLU, noisy ReLU (in RBMs) and ELU (exponential linear units)\n",
        "* ReLU as the activation function for hidden layers and sigmoid for the output layer (these are standards, didn’t experiment much on changing these). Also, I used the standard categorical cross-entropy loss.\n",
        "\n",
        "Plus Side\n",
        "\n",
        "* Does not saturate (in +region)\n",
        "* Very computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
        "Actually more biologically plausible than sigmoid\n",
        "\n",
        "Bad Side\n",
        "\n",
        "* Not zero-centered output\n",
        "* An annoyance: what is the gradient when x < 0? What happens when x = -10, 0 or 10?\n",
        "\n",
        "People like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWsQRCAfmaK",
        "colab_type": "text"
      },
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "* Mass et al., 2013 and He et al., 2015\n",
        "* Does not saturate\n",
        "* Computationally efficient\n",
        "* Converges much faster than sigmoid/tanh in practice! (e.g. 6x) will not “die”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUyiCkgf4Aw",
        "colab_type": "text"
      },
      "source": [
        "## ELU\n",
        "\n",
        "* Exponential Linear Units\n",
        "* Clevert et al., 2015\n",
        "* All benefits of ReLU\n",
        "* Closer to zero mean outputs\n",
        "* Negative saturation regime compared with Leaky ReLU adds some robustness to noise \n",
        "* But Computation requires exp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2ikh67gXRC",
        "colab_type": "text"
      },
      "source": [
        "## SELU\n",
        "\n",
        "* scaled exponential linear units\n",
        "* instead of normalizing the output of the activation function — the activation function suggested (SELU — scaled exponential linear units) outputs normalized values. https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "* Background: batchnormalization for feedfirward networks: Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. (https://arxiv.org/abs/1502.03167)\n",
        "* Negative values sometimes: Scaling the function is the mechanism by which the authors accomplish the goal (of self-normalizing properties). As a byproduct, they sometimes output negative values, but there's no hidden meaning in it. It just makes the math work out. \n",
        "* **SELU vs RELU**: https://www.hardikp.com/2017/07/24/SELU-vs-RELU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GDi-lkbeMo",
        "colab_type": "text"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers.\n",
        "* usually used in the last layer\n",
        "* Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) \n",
        "* is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We use the (standard) Logistic Regression model in binary classification tasks. in softmax regression (SMR), we replace the sigmoid logistic function by the so-called€softmax function€φ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB13XKQhyz9",
        "colab_type": "text"
      },
      "source": [
        "## Maxout\n",
        "\n",
        "* is an activation function that is not function of a single fold x from the previous layer or layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ckAo-8hCBH",
        "colab_type": "text"
      },
      "source": [
        "## Import & Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYwX3hRKnLWV",
        "colab_type": "code",
        "outputId": "36dfa074-46a9-4ab5-92e1-8370adcc01df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCY-TGgqnOFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "d3adae49-ed13-4f5d-801c-dc1f1da70b91"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jNUhO2Og63T",
        "colab_type": "text"
      },
      "source": [
        "## Select Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unoQZNcWnTrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_dense = 'relu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x81WYFo2vr_S",
        "colab_type": "text"
      },
      "source": [
        "* Activation functions can be defined as [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
        "\n",
        "* Activation functions can be defined as [activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMwV7R8ohG3d",
        "colab_type": "text"
      },
      "source": [
        "## Build Model & Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFylU3N3nQQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation=activation_dense),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5wwLJyEnlvd",
        "colab_type": "code",
        "outputId": "542c25c0-19dd-46f6-ad93-e5e820b59187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "def train_model():\n",
        "  \n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  model.fit(x=x_train, \n",
        "            y=y_train, \n",
        "            epochs=5, \n",
        "            validation_data=(x_test, y_test), \n",
        "            callbacks=[tensorboard_callback])\n",
        "\n",
        "train_model()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "   32/60000 [..............................] - ETA: 58:38 - loss: 2.5283 - acc: 0.0312WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.142308). Check your callbacks.\n",
            "60000/60000 [==============================] - 14s 227us/sample - loss: 0.4957 - acc: 0.8223 - val_loss: 0.4286 - val_acc: 0.8421\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.3810 - acc: 0.8606 - val_loss: 0.3700 - val_acc: 0.8655\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 12s 204us/sample - loss: 0.3513 - acc: 0.8708 - val_loss: 0.3637 - val_acc: 0.8684\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 12s 195us/sample - loss: 0.3289 - acc: 0.8775 - val_loss: 0.3446 - val_acc: 0.8720\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 12s 199us/sample - loss: 0.3101 - acc: 0.8848 - val_loss: 0.3525 - val_acc: 0.8708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nQqIaq5noY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard\n",
        "\n",
        "# %tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}