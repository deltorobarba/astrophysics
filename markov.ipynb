{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/markov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV",
        "colab_type": "text"
      },
      "source": [
        "# **Markov Processes (Markov Chains)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U5_6xH7tAQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_psDmmPW0P",
        "colab_type": "text"
      },
      "source": [
        "A Markov chain is a **discrete-time stochastic process** that progresses from one state to another with certain probabilities that can be **represented by a graph and state transition matrix P** as indicated below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kenkureQPi8b",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/machinelearning/master/markov.PNG\" alt=\"markov\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nObrmk-lP6KK",
        "colab_type": "text"
      },
      "source": [
        "Such chains, if they are first-order Markov Chains, exhibit the Markov property, being that the next state is only dependent on the current state, and not how it got there:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zra3gDpyvKSH",
        "colab_type": "text"
      },
      "source": [
        "Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties. Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkZuIs2kCulf",
        "colab_type": "text"
      },
      "source": [
        "* A Markov Chain is a process where the next state depends only on the current state. (A state in this context refers to the assignment of values to the parameters). \n",
        "\n",
        "* A Markov Chain is memoryless because only the current state matters and not how it arrived in that state. \n",
        "\n",
        "* The concept of a Markov Chain is that we do not need to know the entire history of a process to predict the next output, an approximation that works well in many real-world situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx2mtE40PR0z",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/markov-chain-analysis-and-simulation-using-python-4507cee0b06e"
      ]
    }
  ]
}