{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "optimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7rZnJaGTWQw0",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNuFUedQd_vs",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dJRlYxPVGZ",
        "colab_type": "text"
      },
      "source": [
        "![Optimizer](https://raw.githubusercontent.com/deltorobarba/repo/master/optimizer_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0vpxOzOXKr",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4-kecUPvDN",
        "colab_type": "text"
      },
      "source": [
        "Comparison of Activation Functions:\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
        "\n",
        "\n",
        "[tf.keras Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZvzO_jXqJ2j",
        "colab_type": "text"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjwq2sSddqEa",
        "colab_type": "text"
      },
      "source": [
        "## Adam\n",
        "\n",
        "https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n",
        "\n",
        "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlN8ilZEqQj7",
        "colab_type": "text"
      },
      "source": [
        "## Adadelta\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-oQ1xbVqSGF",
        "colab_type": "text"
      },
      "source": [
        "## Adagrad\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWK8KSUTqV3O",
        "colab_type": "text"
      },
      "source": [
        "## RMSprop\n",
        "\n",
        "* xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf3Nk-jgaRor",
        "colab_type": "text"
      },
      "source": [
        "## Import Data\n",
        "\n",
        "This tutorial uses a <a href=\"https://www.bgc-jena.mpg.de/wetter/\" class=\"external\">[weather time series dataset</a> recorded by the <a href=\"https://www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfcHNO94aW1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "33ab2238-24ef-4ca2-d531-32075ad9b37e"
      },
      "source": [
        "# \"https://www.bgc-jena.mpg.de/wetter/\" (weather time series dataset)\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "# Read file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Select Univariate Data\n",
        "uni_data = df['T (degC)']\n",
        "uni_data.index = df['Date Time']\n",
        "\n",
        "# Define Window Size\n",
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "# Train test Split (first 300,000 rows of the data will be the training dataset, \n",
        "# there remaining will be the validation dataset. \n",
        "# This amounts to ~2100 days worth of training data.\n",
        "TRAIN_SPLIT = 300000\n",
        "tf.random.set_seed(13)\n",
        "uni_data = uni_data.values\n",
        "\n",
        "# Compute mean and standard deviation of training data\n",
        "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
        "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
        "\n",
        "# Standardize the data\n",
        "uni_data = (uni_data-uni_train_mean)/uni_train_std\n",
        "\n",
        "# Create Data Pipeline (the model will be given the last 20 recorded temperature observations, and needs to learn to predict the temperature at the next time step.)\n",
        "univariate_past_history = 20\n",
        "univariate_future_target = 0\n",
        "\n",
        "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
        "                                           univariate_past_history,\n",
        "                                           univariate_future_target)\n",
        "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
        "                                       univariate_past_history,\n",
        "                                       univariate_future_target)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "13574144/13568290 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV-EingDXE_w",
        "colab_type": "text"
      },
      "source": [
        "## Choose Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oy4a71ZnFg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = 'adam'\n",
        "# optimizer = 'adamax'\n",
        "# optimizer = 'adadelta'\n",
        "# optimizer = 'adagrad'\n",
        "# optimizer = 'ftrl'\n",
        "# optimizer = 'nadam'\n",
        "# optimizer = 'optimizer'\n",
        "# optimizer = 'RMSprop'\n",
        "# optimizer = 'sgd'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfVxKUnwXK8Z",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kk-evkrmMWh9",
        "colab": {}
      },
      "source": [
        "# tf.data to shuffle, batch, and cache the dataset. \n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# LSTM requires the input shape of the data it is being given.\n",
        "\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer=optimizer, loss='mae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYz6RN_mMyau"
      },
      "source": [
        "## Train & Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0opH9xi5MtIk",
        "outputId": "f0343de2-ee47-46fd-bddf-279088b275b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "EVALUATION_INTERVAL = 200\n",
        "EPOCHS = 10\n",
        "\n",
        "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 200 steps, validate for 50 steps\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 9s 44ms/step - loss: 0.4075 - val_loss: 0.1351\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0360\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.0490 - val_loss: 0.0289\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.0443 - val_loss: 0.0258\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.0299 - val_loss: 0.0235\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.0317 - val_loss: 0.0226\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0206\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.0199\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0254 - val_loss: 0.0182\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.0228 - val_loss: 0.0173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe25b4b5d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMlGFScmnKTa",
        "colab_type": "text"
      },
      "source": [
        "The recurrent activation function 'relu' has a much lower evaluation loss than 'sigmoid', but it takes longer to compute."
      ]
    }
  ]
}