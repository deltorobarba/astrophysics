{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0CA_i-0l_mu4",
        "dEY-ynaCNWkJ",
        "02eilyDPlFh8",
        "qMfQjLMqmzuw",
        "Az7EjhgzgTLq",
        "m_M1o-kVjMZ1",
        "0md9DQ34gkax",
        "jRi0tikbgikC",
        "Apl61SPnakw3",
        "BCIAafwqXMVD",
        "sQ4SU9ZTTVqw",
        "O_fSoE3sg4UM",
        "vyaK3FPEe2vx",
        "Gz0oSYxOTkLl",
        "k5rk1UYyjFyF",
        "UQgau4E3u8MF",
        "y7r0yPBYKf60",
        "s9R2XXEqZeYH",
        "1luzBIPqumre",
        "pqNjjLBMYen-",
        "mL9gnsMGYrOZ",
        "SfaztpziY2sb",
        "nzm9UioSZQMr",
        "yIEGR7ZtZKfS",
        "vysHiG6QtWbx",
        "7aVt8gJY03nn",
        "ZVxXcD-mYYhA",
        "91Gi3EMaYlBw",
        "vLLaN6mPXKth",
        "QlLUjXlCD2jj",
        "g9-cNnkyYxVk",
        "-91jSrDIX7jC",
        "efzpvSJocU3I",
        "Gj-d2EYc2e7s",
        "gNGVQC0OENoM",
        "hlzi59BP8aqU",
        "2rAAm4JiTDN3",
        "2Xmk4oUjXRHh",
        "D9VH8cY6Ia40",
        "KR_qJb-WbsTp",
        "Zd3SbuXPo-mz",
        "DA5HMxDY9iHT",
        "8APTJDua9uNY",
        "OQhOPkypmczT",
        "ypsYD62SW6N9",
        "HX_Y0zpTYD8b",
        "T3lxJ4d7BriG",
        "jBlZmRcPPd8C",
        "c1GtjXg4e2yE",
        "Oz7oDSQ5Bm6H",
        "La_aZKRrBaXe",
        "-wgjwv48EPNF",
        "pgvi8Yztfy7j",
        "XyPztHZDog1j",
        "mUfHuBR4Nhcn",
        "MmDqHiWSaNdY",
        "9IpTffRSg1w4",
        "ql8hl9HHzAOe",
        "-OrLJOuEP4GO",
        "qOXTdQWbu6wm",
        "GwAZXBb_beZP",
        "8fYo65BMPmqp",
        "P4NJL0kOLbfi",
        "Dgy31jigwHId",
        "DOOx_crf83HT",
        "I2xPw5u9vGWI",
        "y4YO0LQ4A4IX",
        "aZ3xS4t7CWRV",
        "gj5tJ7BnBLC0",
        "XW6H46KpBVW-",
        "nMRTXb5a56Aq",
        "AbTRWe0r27UX",
        "fP_EL_HY_Xew",
        "wagDIGtWD_f9",
        "s2KbaipaE5zT",
        "Y-1lpMAgXzD9",
        "04vVZZU1Q4rZ",
        "f4lHqJyMa24i",
        "DLTQm1K2gqMa",
        "jNQs-MXfBmVV",
        "_OexAhQsM_ex",
        "UDNzWru2eKpK",
        "u6Cz4-k-vvpo",
        "Nj_F9wX47fTp",
        "ZiAK4jblJhPd",
        "WqR3v9EaJrvV",
        "JYooOdCtJ76C",
        "MMxCx0fZKDii",
        "fVQl5cbp7iqO",
        "PsodM6QW8wsN",
        "zTjh6Dkf9Imc",
        "icJDI9EQ9gc0",
        "VkuVX0zf959A",
        "Eg2UlbUr-YCc",
        "GCJMH1Oj-vBg",
        "YgcZdTDc_TCw",
        "V44oHA2a_1hG",
        "VcEin6DXApdH",
        "457rg_0-BFtj",
        "vq-kSppvBbU7",
        "F4gfVMumBdd4",
        "pBNDxf7EBkj-",
        "pAHNvHgpB0r-",
        "r-rllUswCV7X",
        "8h7LKwE9CYwF",
        "ihDto5NgCay8",
        "BJaouQGICdBE",
        "QHq7RwnYCe8j",
        "SOpIknui8ULm",
        "YUvl0oGH8ULh",
        "0SjFey3T8ULx",
        "PkPdLE5H8UL-",
        "hu1qRXiA8UL-",
        "XiU5eKGE8UMH",
        "dnkSoEwu8UMN",
        "q09fihf25FPw",
        "oEn5oZesJVdK",
        "HrtIDW9V0p96",
        "YWqVgPOwPCIJ",
        "4Q18zgBcxYJc",
        "ur1A1K3aOl3W",
        "QBuL0NbxO6cP",
        "Ym5Ax5nCOtuS",
        "M5NxmpI85zgL",
        "SN4YJmrE6HF3",
        "Gdr6F1zuU4MG",
        "9AR9PIXysIa4",
        "eloiR80j3LKg",
        "sprbeM9PikBv",
        "KYzlWwuA8hXl",
        "puX57AODxaUG",
        "SqYBnfUAD1yS",
        "w5zER_lMlN46",
        "DfJ1ATjNLmjN",
        "b8EVGNJmEfDo",
        "zXtTBgYGRpLy",
        "2KRiQv6BoMmA",
        "O8cFhBpbIFFd",
        "OkGNbw_qosx7",
        "pHwO7VW3o75X",
        "82Lb_gBqUzi8",
        "KaRe47oJEybG",
        "XgUdyonIRpMb",
        "9kh5t94z0n7k",
        "fQEB0NP4Fk57",
        "y1flpzoQFTVa",
        "8lpmflh50O1X",
        "8chCbs1o_Iud",
        "8idRoUCpsoKR",
        "vUbYRNP_zXe4",
        "m0K1gPOoznzE",
        "LVOw3me6qbku",
        "ECcHaTG2nKiM",
        "P4KDiI20GSS8",
        "JGhitrlOQEHM",
        "T4A4TeH7SewU",
        "ErN__bggDh9m",
        "Vc8T8hpoiWFR",
        "IoZiS2oORjId",
        "4S4z_HeLRbtc",
        "2Aoj2PpxvDQV",
        "wc37qhDWywkD",
        "_j3LHUI5y6Co",
        "qCTiyeYsy0Y0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Complexity**"
      ],
      "metadata": {
        "id": "xEVSiuftty-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0000.png)"
      ],
      "metadata": {
        "id": "YiqcrmGetjeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Quantum Computing**"
      ],
      "metadata": {
        "id": "0CA_i-0l_mu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Computational Learning*"
      ],
      "metadata": {
        "id": "dEY-ynaCNWkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***$\\hookrightarrow$ Sample and Query Complexity***"
      ],
      "metadata": {
        "id": "02eilyDPlFh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Sample Complexity](https://en.m.wikipedia.org/wiki/Sample_complexity): number of training examples that the most efficient learning algorithm needs to see in order to **learn a concept with a certain accuracy** (an arbitrarily small error of best possible function, with probability arbitrarily close to 1)\n",
        " - It's a measure of how data-efficient a learning algorithm is.\n",
        "  * A high sample complexity means that a large amount of data is needed to ensure good generalization.\n",
        "  * a learning algorithm with low sample complexity can achieve good performance with less data.\n",
        "\n",
        "* estimate how large a sample size is needed to ensure that the empirical average of a random variable is close to its true mean with high probability (especially relevant in scenarios like empirical risk minimization, where one seeks to minimize the discrepancy between the empirical risk (based on finite samples) and the true risk)\n",
        "\n",
        "* Bounding sample complexity:\n",
        "  * showing that output of learning algorithm is close to its expected value with high probability, given a sufficient number of training samples. Allows us to set a confidence bound on  accuracy of the algorithm, even if we have only seen a small number of training samples.\n",
        "  * e.g. **Bounding sample complexity of empirical risk minimization (ERM) or online learning**: (ERM=average loss on the training data). Concentration inequalities can show that ERM learns a function with a small generalization error with high probability, given a sufficient number of training samples.\n",
        "  * For only particular class of target functions (e.g. linear functions) sample complexity is finite and depends linearly on [VC dimension](https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension) on class of target functions. Sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C.\n",
        "\n",
        "* Factors that affect sample complexity of learning a target function:\n",
        "  * Size of hypothesis space: Large space = more samples\n",
        "  * Error tolerance (desired level of accuracy and confidence): Smaller error tolerance = more samples\n",
        "  * Noise: Higher noise = more samples\n",
        "  * Complexity of target function or concept to be learned: More complex target function = more samples\n",
        "\n",
        "* Difference:\n",
        "  * sample complexity focuses on the data requirements for learning,\n",
        "  * model complexity deals with the representational power of the model.\n",
        "\n",
        "* Trade-Offs:\n",
        "  * models with higher complexity (capacity) can represent more complex functions but may require more data (higher sample complexity) to generalize well and avoid overfitting.\n",
        "  * Conversely, simpler models might generalize better with less data but are limited in the complexity of functions they can learn."
      ],
      "metadata": {
        "id": "pA922WyYbUKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Learner (Model)*"
      ],
      "metadata": {
        "id": "qMfQjLMqmzuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The Learner or learning algorithm (e.g. gradient descent, logic regression, linear regression, decision trees and random forests) is **the algorithm that produces a specific model from the hypothesis class using data** - the learner is the algorithm that **chooses the best hypothesis from the hypothesis class** given the training data.*"
      ],
      "metadata": {
        "id": "tvnsAeoaoQm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAC learning**\n",
        "\n",
        "* **PAC Learning**: is a method to measure sample complexity. The Probably Approximately Correct (PAC) learning framework provides a formal definition of learning and can be used to derive bounds on the sample complexity needed to learn a function to a given degree of accuracy with a certain probability.\n",
        "\n",
        "* PAC learning is a framework in computational learning theory where an algorithm aims to learn a target function based on samples such that it performs well on unseen data with high probability.\n",
        "\n",
        "* **For linear regression to be a PAC learner, the following conditions are typically required:**\n",
        "\n",
        "  * The true relationship between the variables is (approximately) linear.\n",
        "  * The noise in the data is bounded.\n",
        "  * The features (input data) have a bounded range.\n",
        "\n",
        "* Under these conditions, with a sufficiently large sample size, the linear regression model can be shown to generalize well to unseen data with high probability, thereby satisfying the PAC learning criteria.\n",
        "\n",
        "> **However, if the true underlying relationship is non-linear or if the noise is unbounded, then linear regression might not be a PAC learner.**\n",
        "\n",
        "* Several algorithms and models in machine learning can be considered PAC learners under appropriate conditions. Here are some examples:\n",
        "\n",
        "  1. **Decision Trees**: Decision tree algorithms, like ID3, C4.5, and CART, can be PAC learners under certain conditions. They can approximate discrete-valued functions well given a sufficient number of samples.\n",
        "\n",
        "  2. **Finite Automata Learning**: Algorithms that learn regular languages, like the L* algorithm, can be seen as PAC learners for certain classes of regular languages.\n",
        "\n",
        "  3. **Neural Networks**: Under certain assumptions, feed-forward neural networks with a fixed number of hidden layers can be PAC learners. However, their sample complexity can be high, especially as the size of the network grows.\n",
        "\n",
        "  4. **k-Nearest Neighbors (k-NN)**: Under certain conditions, the k-NN algorithm can be a PAC learner, particularly when the data is uniformly distributed.\n",
        "\n",
        "  5. **Support Vector Machines (SVM)**: SVMs can be considered PAC learners when the margin between classes is reasonably large.\n",
        "\n",
        "  6. **Boosting Algorithms**: Algorithms like AdaBoost, which combine multiple weak learners to create a strong learner, can be seen as PAC learners under the right conditions.\n",
        "\n",
        "  7. **Monotone DNF**: Algorithms that learn disjunctive normal form (DNF) expressions where all literals are positive can be PAC learners.\n",
        "\n",
        "* It's worth noting that for any learning algorithm to be a PAC learner, it must be able to produce hypotheses that generalize well to unseen data with high probability, given polynomially many samples in the size of the concept/class, the error bound, and the confidence level. The specific conditions under which these algorithms are PAC learnable can vary based on the complexity of the model, the nature of the data, and the distribution from which the data is drawn.\n",
        "\n",
        "*Under which conditions is a neural network or a k-Nearest Neighbors (k-NN) not a PAC learner?*\n",
        "\n",
        "* Both neural networks and k-Nearest Neighbors (k-NN) have conditions under which they might not be PAC learners:\n",
        "\n",
        "  1. **Neural Networks**:\n",
        "\n",
        "   - **Complexity and Overfitting**: If the neural network is too complex (e.g., too many layers or neurons) relative to the amount of training data available, it can overfit. Overfitting means the network memorizes the training data rather than generalizing from it, leading to poor performance on unseen data.\n",
        "   \n",
        "   - **Non-IID Data**: Neural networks assume that data is identically and independently distributed (IID). If there's a temporal or spatial structure in the data, without appropriate handling, a standard neural network might not generalize well.\n",
        "   \n",
        "   - **Unbounded Activation Functions**: Activation functions that aren't bounded might lead to weights that grow indefinitely, causing issues with learning.\n",
        "   \n",
        "   - **Unsuitable Data Distribution**: If the data distribution is not well-suited for the neural network's assumptions or if it changes over time (non-stationary), the network might not be a PAC learner for that distribution.\n",
        "\n",
        "  2. **k-Nearest Neighbors (k-NN)**:\n",
        "\n",
        "   - **Curse of Dimensionality**: As the dimensionality of the dataset increases, the volume of the space increases exponentially, and data becomes sparse. k-NN suffers in high-dimensional spaces because the notion of \"nearness\" becomes less meaningful. This can cause k-NN to be a poor learner in high dimensions unless the dataset is exceptionally large.\n",
        "   \n",
        "   - **Non-uniform Feature Scales**: If features have different scales and aren't normalized, the distance measure used by k-NN can be dominated by certain features, making it a poor learner.\n",
        "   \n",
        "   - **Noisy Data**: k-NN is sensitive to noise in the dataset. If a significant portion of the data has errors or misclassifications, k-NN's performance can degrade, and it may not be a PAC learner for very noisy datasets.\n",
        "   \n",
        "   - **Variable Densities**: If some regions of the input space are densely populated and others are sparse, k-NN might not generalize well across all regions.\n",
        "\n",
        "* For both neural networks and k-NN, it's important to note that being a PAC learner is about the ability to generalize to unseen data with high probability given a polynomial number of samples. If either model fails to meet this criterion under certain conditions, it's not considered a PAC learner under those conditions.\n",
        "\n",
        "*Why is a query learner not a PAC learner?*\n",
        "\n",
        "* A query learner and a PAC learner are not inherently mutually exclusive. They are two different frameworks within computational learning theory that describe the manner and conditions under which learning happens. However, the methods by which they acquire data and their learning objectives are what differentiate them:\n",
        "\n",
        "  1. **Data Acquisition**:\n",
        "   - **Query Learner**: The learner can actively ask an oracle about specific inputs or hypotheses. The types of questions might include membership queries (asking if a specific instance belongs to the target concept) and equivalence queries (proposing a hypothesis and asking if it's correct). There might be other types of queries as well.\n",
        "   - **PAC Learner**: The learner passively receives random samples drawn from a distribution. It doesn't get to choose specific examples it wants to learn from.\n",
        "\n",
        "  2. **Learning Objective**:\n",
        "   - **Query Learner**: Depending on the model, the goal might be exact learning, where the learner tries to find a hypothesis that exactly matches the target concept. This is often the objective in models that use equivalence queries.\n",
        "   - **PAC Learner**: The goal is to find a hypothesis that is probably approximately correct. That is, with high probability, the hypothesis should be approximately accurate on new, unseen examples drawn from the same distribution.\n",
        "\n",
        "* Given these distinctions, a learning algorithm could theoretically be both a query learner and a PAC learner under different circumstances or with different assumptions. For instance, one might imagine a scenario where a learner uses queries to gather information and then ensures that its hypothesis is probably approximately correct for a given distribution.\n",
        "\n",
        "* However, the traditional definitions of these models focus on their distinct characteristics, which is why they are often treated as separate learning paradigms in computational learning theory.\n",
        "\n",
        "\n",
        "*Leslie Valiant’s Probably Approximately Correct (PAC) model gives a precise complexity- theoretic definition of what it means for a concept class to be (efficiently) learnable. - Source: Optimal Quantum Sample Complexity of Learning Algorithms (2017)*\n",
        "\n",
        "* [PAC Learning](https://en.m.wikipedia.org/wiki/Error_tolerance_(PAC_learning)): learner is evaluated on its predictive power of a test set.\n",
        "\n",
        "  * Dimensionality measures are important tools in machine learning because they can be used to bound the sample complexity of learning algorithms. For example, the following theorem provides an upper bound on the sample complexity of PAC learning in terms of the VC dimension:\n",
        "\n",
        "  * **Theorem:** Let H be a hypothesis class with VC dimension d. Then, the sample complexity of PAC learning H is given by:\n",
        "\n",
        "  * $n >= O(d / ε^2 * ln(1/δ))$\n",
        "\n",
        "  * where ε is the desired error tolerance and δ is the desired confidence level.\n",
        "\n",
        "  * This theorem tells us that if we know the VC dimension of a hypothesis class, then we can bound the number of training examples needed to learn a hypothesis in that class with high probability.\n",
        "  \n",
        "  * In general, the VC dimension is the most widely used dimensionality measure in PAC learning. It is simple to compute and easy to interpret. However, there are other dimensionality measures that can be more effective for certain types of learning problems.\n",
        "  \n",
        "* [PAC learning](https://en.m.wikipedia.org/wiki/Probably_approximate) (Probably approximate learner): A PAC learner is a machine learning algorithm that can learn a function from a set of labeled examples with high accuracy. The PAC learner is given a set of labeled examples, where each example is a pair of an input and its corresponding output. The PAC learner then learns a function that maps from inputs to outputs. The function learned by the PAC learner should have high accuracy, meaning that it should output the correct output for most inputs.\n",
        "\n",
        "* Hypothesis class is finite and labeling function is consistent with some hypothesis in the hypothesis class. Ideal for more simpler applications like Classification, regression, clustering.\n",
        "\n",
        "* PAC learning (Probably Approximately Correct learning) is a framework for machine learning that allows us to measure the accuracy of a learning algorithm. In PAC learning, we assume that the learner has access to a training set of labeled examples, and the goal is to learn a hypothesis that predicts the label of new, unseen examples with high accuracy.\n",
        "\n",
        "* **Proper quantum PAC learner** with optimal sample complexity, i.e., one whose output hypothesis lies in C itself\n",
        "\n",
        "* ps: sample complexity for the PAC and agnostic models, quantum examples do not provide an advantage (*Survey of Quantum Learning Theory*)\n",
        "\n",
        "* „quantum sample complexity of PAC learning“ versus „sample complexity of PAC learning n-qubit quantum states“?\n",
        "\n",
        "  * The quantum sample complexity of PAC learning refers to the number of quantum examples needed to learn a concept class with probability 1-delta, where each example is a coherent quantum state.\n",
        "\n",
        "  * The sample complexity of PAC learning n-qubit quantum states refers to the number of classical examples needed to learn a concept class of n-qubit quantum states, where each example is a classical description of an n-qubit quantum state.\n",
        "\n",
        "  * In general, the quantum sample complexity of PAC learning is lower than the sample complexity of PAC learning n-qubit quantum states. This is because quantum examples can contain more information than classical examples.\n",
        "\n",
        "  * For example, consider the task of learning a concept class of binary functions. A classical example of a binary function is a pair of inputs (x, y), where x is a binary number and y is the output of the function on x. A quantum example of a binary function is a quantum state |ψ⟩ that encodes the function. **It can be shown that the quantum sample complexity of PAC learning binary functions is O(log(d)/ε), where d is the VC dimension of the concept class and ε is the error tolerance. The classical sample complexity of PAC learning binary functions is O(d/ε), which is larger than the quantum sample complexity.**\n",
        "\n",
        "  * This shows that quantum examples can be more powerful than classical examples for some learning tasks. However, it is important to note that the quantum sample complexity of PAC learning is still an active area of research, and there are many open questions."
      ],
      "metadata": {
        "id": "IqioVhxR7yy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear regression is an example of a PAC learner. A PAC learner is a type of machine learning algorithm that can learn a concept class with high probability, given a sufficient number of training examples. The concept class is the set of all possible functions that the learner is trying to learn. In the case of linear regression, the concept class is the set of all linear functions.*\n",
        "\n",
        "* Linear regression has been shown to be PAC learnable under a variety of different conditions. For example, if the training data is drawn from a distribution where the target values are generated by a linear function, then linear regression can learn the correct function with high probability, given a sufficient number of training examples.\n",
        "\n",
        "* However, it is important to note that the PAC learnability of linear regression depends on the specific hypothesis class that is being used. For example, if the hypothesis class includes all possible linear functions, then linear regression is not PAC learnable. This is because there are infinitely many linear functions, and it is not possible to learn any infinite hypothesis class with a finite number of training examples.\n",
        "\n",
        "* In practice, linear regression is often used with a restricted hypothesis class. For example, the hypothesis class might be restricted to only include linear functions that have a certain number of parameters. This makes the problem of learning the correct function more feasible, and linear regression can often be used to learn the correct function with a relatively small number of training examples.\n",
        "\n",
        "* Here is an example of how linear regression can be used as a PAC learner:\n",
        "\n",
        "* Suppose we want to learn a linear function that predicts the weight of a person based on their height. We can collect a training dataset of people's heights and weights. Then, we can use a linear regression algorithm to learn a linear function that fits the training data.\n",
        "\n",
        "* The PAC learnability of linear regression tells us that, if the training data is drawn from a distribution where the weights are generated by a linear function of height, then the linear regression algorithm will learn the correct function with high probability, given a sufficient number of training examples.\n",
        "\n",
        "* Of course, we cannot guarantee that the linear regression algorithm will always learn the correct function. However, the PAC learnability of linear regression tells us that the algorithm will learn the correct function with high probability, if the training data is sampled from the correct distribution."
      ],
      "metadata": {
        "id": "OrqN6xS5PYJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, a PAC learner would be a learner in computational learning theory. A PAC learner is a machine learning algorithm that can learn a hypothesis that generalizes well to new examples, with high probability.\n",
        "\n",
        "The PAC model is a theoretical framework for evaluating the performance of learning algorithms. In the PAC model, the learner is given a set of training examples and is asked to learn a hypothesis that generalizes well to new examples. The hypothesis is a function that maps from inputs to outputs, and the learner's goal is to find a hypothesis that is close to the target function, which is the unknown function that generated the training examples.\n",
        "\n",
        "A PAC learner is a learner that can learn a hypothesis that generalizes well to new examples, with high probability. This means that the learner can learn a hypothesis that will make few mistakes on new examples, most of the time.\n",
        "\n",
        "PAC learners are important because they provide a way to evaluate the performance of learning algorithms in a rigorous way. The PAC model is also useful for understanding the theoretical limitations of learning algorithms.\n",
        "\n",
        "Here are some examples of PAC learners:\n",
        "\n",
        "* Linear regression\n",
        "* Logistic regression\n",
        "* Decision trees\n",
        "* Support vector machines\n",
        "* Neural networks\n",
        "\n",
        "These algorithms can all be used to learn hypotheses that generalize well to new examples, with high probability.\n",
        "\n",
        "It is important to note that the PAC model is a theoretical framework, and it is not always possible to find a PAC learner that works well in all practical settings. However, the PAC model provides a useful way to think about the problem of learning and to evaluate the performance of learning algorithms."
      ],
      "metadata": {
        "id": "lf78dXP8PpNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA stands for AutoRegressive Integrated Moving Average, and it is a statistical model that is used to forecast time series data. ARIMA models are based on the assumption that the future values of a time series can be predicted from its past values.\n",
        "\n",
        "In the context of computational learning theory, ARIMA can be considered a learner, but it is a special type of learner that is designed for time series forecasting. ARIMA models learn a hypothesis that maps from past time series values to future time series values. The hypothesis is a linear function of the past values, and it is estimated using a statistical procedure called maximum likelihood estimation.\n",
        "\n",
        "ARIMA models are widely used in a variety of applications, such as forecasting sales, stock prices, and weather patterns. They are relatively simple to implement and interpret, and they can be used to generate accurate forecasts for a variety of time series data.\n",
        "\n",
        "Here is an example of how ARIMA could be used as a learner in computational learning theory:\n",
        "\n",
        "Suppose we have a time series of daily sales data for a particular product. We want to learn a hypothesis that can be used to predict future sales. We can use an ARIMA model to learn this hypothesis.\n",
        "\n",
        "First, we would need to identify the order of the ARIMA model. The order of the model determines how many past values of the time series are used to predict future values. We can use a statistical procedure called autocorrelation to help us identify the order of the model.\n",
        "\n",
        "Once we have identified the order of the model, we can estimate the parameters of the model using maximum likelihood estimation. This procedure will give us a hypothesis that is the best fit to the training data.\n",
        "\n",
        "We can then use the hypothesis to predict future sales. To do this, we would simply plug the past values of the time series into the hypothesis. The output of the hypothesis will be the predicted value for the next time step.\n",
        "\n",
        "ARIMA models are a powerful tool for time series forecasting, and they can be used as learners in computational learning theory. However, it is important to note that ARIMA models are only suitable for forecasting time series data that is stationary. Stationary time series data is data that has a constant mean, variance, and autocorrelation over time. If the time series data is not stationary, then it may be necessary to transform the data before using an ARIMA model."
      ],
      "metadata": {
        "id": "xSVXX1H7PxnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the PAC model, the learner is given a set of training examples and is asked to learn a hypothesis that generalizes well to new examples. The hypothesis is a function that maps from inputs to outputs, and the learner's goal is to find a hypothesis that is close to the target function, which is the unknown function that generated the training examples.\n",
        "\n",
        "The PAC model is used to measure the generalization error of a learning algorithm. The generalization error is the probability that the hypothesis will make a mistake on a new example. The PAC model guarantees that, with high probability, the learning algorithm will output a hypothesis with low generalization error.\n",
        "\n",
        "And what would be an example of a learner in this context?\n",
        "\n",
        "An example of a learner in the context of computational learning theory is a machine learning algorithm, such as linear regression, decision trees, or support vector machines. These algorithms are given a set of training examples and are asked to learn a hypothesis that generalizes well to new examples.\n",
        "\n",
        "For example, a linear regression algorithm could be used to learn a hypothesis that predicts the price of a house based on its square footage, number of bedrooms, and other features. The algorithm is given a set of training examples, which are houses with known prices and features. The algorithm then learns a linear function that best fits the training data. This function can then be used to predict the price of a new house, given its features.\n",
        "\n",
        "Another example of a learner is a decision tree algorithm. Decision tree algorithms learn a hypothesis by constructing a tree structure, where each node in the tree represents a decision and each leaf node represents a prediction. The algorithm is given a set of training examples, and it constructs a tree that best separates the positive examples from the negative examples. The tree can then be used to predict the class of a new example by traversing the tree from the root node to a leaf node.\n",
        "\n",
        "These are just two examples of learners in computational learning theory. There are many other machine learning algorithms that can be used as learners in this context.\n",
        "\n",
        "It is important to note that the PAC model is a theoretical framework, and it is not always possible to find a learning algorithm that works well in all practical settings. However, the PAC model provides a useful way to think about the problem of learning and to evaluate the performance of learning algorithms."
      ],
      "metadata": {
        "id": "NkAyGxjLP8__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One example of a quantum PAC learner is the Grover's algorithm. Grover's algorithm is a quantum search algorithm that can search through a database of unsorted items in polynomial time, whereas classical search algorithms require exponential time.\n",
        "\n",
        "Grover's algorithm can be used to implement a quantum PAC learner by using it to search through the space of all possible hypotheses to find one that is consistent with the training data. This can be done by constructing a quantum oracle that takes a hypothesis as input and outputs 1 if the hypothesis is consistent with the training data and 0 otherwise.\n",
        "\n",
        "Once the oracle is constructed, Grover's algorithm can be used to search for a hypothesis that outputs 1. Grover's algorithm will find such a hypothesis in polynomial time, even if the space of all possible hypotheses is very large.\n",
        "\n",
        "Another example of a quantum PAC learner is the HHL algorithm. The HHL algorithm is a quantum algorithm for solving linear systems of equations. It can be used to implement a quantum PAC learner by using it to solve the linear system of equations that represents the training data.\n",
        "\n",
        "Once the linear system of equations is solved, the coefficients of the solution can be interpreted as the weights of a hypothesis. This hypothesis can then be used to predict the labels of new examples.\n",
        "\n",
        "Both Grover's algorithm and the HHL algorithm have been shown to achieve polynomial speedups over classical PAC learners for certain types of concepts. However, it is important to note that these algorithms are still in their early stages of development, and it is not yet clear how well they will perform on real-world problems.\n",
        "\n",
        "Here is a simple example of how Grover's algorithm could be used to implement a quantum PAC learner:\n",
        "\n",
        "\n",
        "1. Initialize a quantum register to be in the superposition of all possible hypotheses.\n",
        "2. Apply the quantum oracle to the quantum register.\n",
        "3. Measure the quantum register.\n",
        "4. If the measurement result is 1, then the hypothesis is consistent with the training data and the algorithm terminates.\n",
        "5. Otherwise, apply the Grover diffusion operator to the quantum register and repeat steps 2-4.\n",
        "\n",
        "\n",
        "The Grover diffusion operator is a quantum operator that amplifies the probability of measuring the correct hypothesis. After a few iterations of the Grover algorithm, the probability of measuring the correct hypothesis will be very high.\n",
        "\n",
        "I hope this helps!"
      ],
      "metadata": {
        "id": "Lwlt5nzkwpfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agnostic learning**:\n",
        "\n",
        "  * Hypothesis class is finite but labeling function is not assumed to be consistent with any hypothesis in the hypothesis class. Ideal for more general applications, such as natural language processing and computer vision.\n",
        "  * The PAC model assumes that the labeled examples are generated according to a target concept c ∈ C . However, in many learning situations that is not a realistic assumption, for example when the examples are noisy in some way or when we have no reason to believe there is an underlying target concept at all. The agnostic model of learning, introduced by Haussler [Hau92] and Kearns et al. [KSS94], takes this into account (Optimal Quantum Sample Complexity of Learning Algorithms).\n",
        "\n",
        "  * Agnostic learning is a more general concept than PAC learning. PAC learning is a framework for machine learning that guarantees that a learner can find a hypothesis that is approximately correct with high probability, given a finite amount of training data. The learner does this by making two assumptions:\n",
        "\n",
        "    1. The hypothesis class is finite.\n",
        "    2. The labeling function is consistent with some hypothesis in the hypothesis class.\n",
        "\n",
        "  * Agnostic learning relaxes the second assumption. In agnostic learning, the learner does not assume that the labeling function is consistent with any hypothesis in the hypothesis class. This makes agnostic learning a more challenging problem, but it also allows the learner to learn more complex concepts.\n",
        "\n",
        "  * In other words, PAC learning is a special case of agnostic learning where the labeling function is consistent with some hypothesis in the hypothesis class."
      ],
      "metadata": {
        "id": "8DexccI_oHSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Online learning**\n",
        "  * The online model can be viewed as a variant of tomography and PAC learning (Survey on the complexity of learning quantum states, page 15)\n",
        "  * [PAC Learning and Online Learning](https://courses.corelab.ntua.gr/pluginfile.php/7949/course/section/925/lecture2.pdf)"
      ],
      "metadata": {
        "id": "8vQUT-kMoE3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Active Learning**\n",
        "\n",
        "* Semi-supervised learning problems include active learning, where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels."
      ],
      "metadata": {
        "id": "WQ80qL7y_oSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exact learning** (uses queries, not sample!)\n",
        "\n",
        "* This is a specific type of query learning where the objective is to learn the target concept exactly.\n",
        "\n",
        "* Exact learning is a part of PAC learning. Exact learning is a special case of PAC learning where the learner is guaranteed to learn the correct hypothesis with certainty, given enough training examples. This is in contrast to PAC learning, where the learner is only guaranteed to learn a hypothesis that is accurate with high probability.\n",
        "* **Query complexity of exact learning**. In quantum computational learning theory, exact learning is typically studied in the context of Probably Approximately Correct (PAC) learning, where the learner is given a set of labeled examples, and is asked to learn a hypothesis that classifies new examples with high accuracy.\n",
        "* There are a number of different ways to perform exact learning in quantum computational learning theory. One approach is to use quantum algorithms to perform tomography on the target function. However, this approach is often inefficient, as it requires a large number of measurements.\n",
        "* Another approach is to use quantum algorithms to perform machine learning directly, without the need for tomography. This approach is more promising, as it can be more efficient. However, it is still a relatively new area of research, and there are many open problems.\n",
        "* (N,M)-quantum query complexity of exact learning. Page 8 under \"4.1 Query complexity of exact learning\" notation: **{s ∈ S : s_i = 0}** in this [paper](https://arxiv.org/abs/1701.06806)\n",
        "\n",
        "  * The notation {s ∈ S : s_i = 0} is a set comprehension notation that selects all elements s in a set S such that the ith index of s is equal to 0. In other words, it is the set of all elements in S whose ith digit is 0.\n",
        "    * For example, if S is the set {1, 2, 3, 4, 5}, then the set {s ∈ S : s_i = 0} is the set {0, 2, 4}.\n",
        "    * Compare in binary: 0 : **00**, 1 : **01**, 2: **10**, 3: **11**, 4 : **100**, 5: **101** (look at the last digit, the ith digit)\n",
        "\n",
        "  * The (N,M)-quantum query complexity of exact learning is **the minimum number of queries to an oracle that a quantum algorithm needs to make in order to learn a Boolean function f : {0, 1}^n → {0, 1} with error probability at most 1/M, where N is the number of input variables and M is a positive integer**.\n",
        "\n",
        "  * In other words, the (N,M)-quantum query complexity is the quantum analogue of the (N,M)-classical query complexity, which is the minimum number of queries to an oracle that a classical algorithm needs to make in order to learn f with error probability at most 1/M.\n",
        "\n",
        "  * The (N,M)-quantum query complexity of exact learning is a difficult problem to study, and it is not known for many functions f. However, there are some functions for which the (N,M)-quantum query complexity is known to be better than the (N,M)-classical query complexity.\n",
        "\n",
        "  * For example, it is known that the (N,M)-quantum query complexity of learning a k-junta with error probability at most 1/M is O(n(log n)/k) queries, while the (N,M)-classical query complexity is Ω(n^k/k^2) queries. This means that a quantum algorithm can learn a k-junta with fewer queries than a classical algorithm, if M is sufficiently large.\n",
        "\n",
        "*Exact Learning with Membership and Equivalence Queries: In this model, the learner tries to identify a target concept exactly (not approximately) by asking two types of queries:*\n",
        "\n",
        "  * Membership Queries: The learner poses an input and asks the oracle if it belongs to the target concept.\n",
        "  * Equivalence Queries: The learner proposes a hypothesis and asks the oracle if it's equivalent to the target concept. If not, the oracle provides a counterexample."
      ],
      "metadata": {
        "id": "13qWjplwoCW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Learning**\n",
        "\n",
        "* This is a broader category where the learner has access to an oracle (similar to the membership and equivalence query model) but may have various types of queries at its disposal. The nature of the queries and the information that can be extracted determine the learnability of the target concept.\n",
        "\n",
        "* both query learning and exact learning involve the use of queries. However, the distinction lies in the objective of the learning and the broader context in which the terms are used.\n",
        "\n",
        "  1. **Objective**:\n",
        "   - **Query Learning**: The term \"query learning\" refers broadly to any learning model where the learner can actively query an oracle about the target concept. The queries might include membership queries, equivalence queries, and potentially other types of queries. The goal is not necessarily to learn the target concept exactly—it could be approximate or within certain bounds.\n",
        "   - **Exact Learning**: This is a specific type of query learning where the objective is to learn the target concept exactly. Exact learning often involves equivalence queries, where the learner proposes a hypothesis and asks the oracle if it exactly matches the target concept. If not, the oracle provides a counterexample.\n",
        "\n",
        "  2. **Types of Queries**:\n",
        "   - **Query Learning**: Can encompass various types of queries, depending on the specific learning model.\n",
        "   - **Exact Learning**: Primarily uses equivalence queries but may also use membership queries to refine its hypothesis based on the counterexamples received.\n",
        "\n",
        "3. **Context**:\n",
        "   - **Query Learning**: This is a broader term in computational learning theory, covering any model that uses some form of queries to gather information about the target concept.\n",
        "   - **Exact Learning**: This is a more specific model under the umbrella of query learning. It represents a subset of query learning models where the objective is to pinpoint the target concept without any approximation.\n",
        "\n",
        "In essence, all exact learning can be considered a form of query learning, but not all query learning aims for exact learning. Some query learning models might be content with approximate solutions or may have other objectives beyond exactness.\n"
      ],
      "metadata": {
        "id": "A25h4TlTNxRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Empirical Risk Minimization (ERM)**\n",
        "\n",
        "* Bounding the sample complexity of empirical risk minimization (ERM): ERM is a popular machine learning algorithm that learns a function by minimizing the empirical risk, which is the average loss on the training data.\n",
        "\n",
        "* Concentration inequalities can be used to show that ERM learns a function with a small generalization error with high probability, given a sufficient number of training samples."
      ],
      "metadata": {
        "id": "7Bm4nqqWQEJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Occam learning**\n",
        "\n",
        "* [Occam learning](https://en.m.wikipedia.org/wiki/Occam_learning): the objective of the learner is to output a succinct representation of received training data.\n",
        "* Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables and decision lists.\n",
        "* Occam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples."
      ],
      "metadata": {
        "id": "tUzyFOZagC2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Fourier features and Classical Surrogates for QML**\n",
        "\n",
        "* [Potential and limitations of random Fourier features for dequantizing quantum machine learning](https://scirate.com/arxiv/2309.11647): we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization. On a higher level, this work contributes to delineating the boundary between quantum and classical processes.\n",
        "\n",
        "* [Classical Surrogates for Quantum Learning Models](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.100803)"
      ],
      "metadata": {
        "id": "TBU-HI_ugksq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum statistical query model (QSQ)**\n",
        "  * e.g. quantum example oracle\n",
        "  * QSQ (Quantum statistical query model) is a model of quantum machine learning that allows us to ask queries about the distribution of a quantum state. In QSQ, the learner has access to a quantum oracle that can answer queries about the distribution of a quantum state, and the goal is to learn a hypothesis that predicts the output of the oracle with high accuracy.\n",
        "  * A quantum example oracle is a black box that takes a quantum state as input and gives a quantum state as output. The quantum example oracle is not accessible to the user, and the user only knows how to interact with it through a specific set of operations.\n",
        "  * The main difference between a quantum example oracle and a PAC learner is that a quantum example oracle provides the learner with quantum states, while a PAC learner provides the learner with labeled examples. Quantum states are more powerful than labeled examples (from PAC), because they can represent a superposition of multiple inputs and outputs. This means that a quantum example oracle can provide the learner with more information about the function being learned. ***As a result, quantum example oracles can be used to learn functions that are more difficult to learn with classical PAC learners.*** For example, it has been shown that DNF formulas can be learned efficiently with a quantum example oracle, but they are not known to be efficiently learnable with a classical PAC learner.\n",
        "  * Another example of a quantum oracle is the Deutsch-Jozsa algorithm. This algorithm takes a function f:{0,1}^n->{0,1} as input, and determines whether f is constant or balanced. A constant function is one that always outputs the same value, regardless of its input. A balanced function is one that outputs 1 half of the time and 0 half of the time. The Deutsch-Jozsa algorithm works by first creating a superposition of all possible inputs to f. It then applies the oracle to this superposition. If f is constant, then the oracle will not change the superposition. However, if f is balanced, then the oracle will flip the phase of half of the states in the superposition. Finally, the algorithm measures the superposition. If the measurement result is 0, then f is constant. If the measurement result is 1, then f is balanced.\n"
      ],
      "metadata": {
        "id": "yUEN_ECDmc0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tomography**\n",
        "  * **Quantum state tomography (QST)**:\n",
        "    * State tomography is an alternative to boolean functions in quantum computational learning.\n",
        "    * To solve shadow tomography, the goal is to find a quantum state σ that satisfies Tr(σEi) ≈ Tr(ρEi) for all i (the trace distance is almost zero, the states are identical). Further, one would like to minimize the number of copies of ρ, suggesting that σ should be no more informative than matching the above expectations.\n",
        "    * In state tomography, the complete state vector or density matrix of the quantum state is reconstructed. This requires measuring the state in a complete set of bases, which can be exponentially many bases for a large quantum state.\n",
        "  * **shadow tomography**:\n",
        "    * In shadow tomography, only a subset of the bases is measured. This allows the state to be reconstructed with fewer copies of the state, but the reconstructed state may not be as accurate as the state reconstructed using state tomography.\n",
        "    * The main difference between state tomography and shadow tomography is the number of copies of the quantum state that are needed to reconstruct the state.\n",
        "  * **alternate algorithm for shadow tomography**: Max-entropy principle and Matrix Multiplicative Weight Update."
      ],
      "metadata": {
        "id": "ojKgJkAHmabx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Learners**\n",
        "* [Communication complexity models](https://en.m.wikipedia.org/wiki/Communication_complexity)\n",
        "* The perceptron algorithm.\n",
        "* The ID3 algorithm.\n",
        "* The backpropagation algorithm"
      ],
      "metadata": {
        "id": "Fw0MNiKsmNqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Backpropagation and Barren Plateaus (Overparametrization) in Quantum Machine Learning*"
      ],
      "metadata": {
        "id": "Az7EjhgzgTLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Backpropagation and Barren Plateaus (Overparametrization) in Quantum Machine Learning**\n",
        "\n",
        "*Why can backpropagation not easily scale in quantum machine learning?*\n",
        "\n",
        "1. **Non-commutativity of operators**: Quantum operations, unlike classical ones, are represented by operators that generally do not commute, meaning the order in which they are applied matters. This non-commutativity makes the computation of gradients more complex than in the classical case.\n",
        "\n",
        "2. **Measurement**: In quantum mechanics, obtaining information about the state of a system involves measurement, which is a probabilistic process that collapses the state of the system. The inherent randomness of measurement makes the direct application of backpropagation problematic.\n",
        "\n",
        "3. **Complexity of quantum states**: Quantum states live in a complex vector space and can exist in a superposition of multiple states simultaneously, further complicating the process of backpropagation.\n",
        "\n",
        "4. **Barren plateaus**: In the context of variational quantum algorithms, it's been found that the cost function landscape often suffers from the problem of \"barren plateaus\", where the function is flat almost everywhere. This makes it difficult for gradient-based methods like backpropagation to find a direction to move in to improve the function.\n",
        "\n",
        "*Are Barren Plateaus a sign for Overparametrization and too high capacity of my quantum machine learning model?*\n",
        "\n",
        "* Yes, barren plateaus can be a sign of overparametrization and too high capacity of a quantum machine learning model. Overparametrization occurs when a model has more parameters than it needs to learn the task at hand. This can lead to the model fitting the training data too well, which can cause it to perform poorly on new data.\n",
        "\n",
        "* When a model is overparametrized, the cost function landscape can become very flat. This means that there are many different parameter settings that give the same or very similar cost values. As a result, the optimization algorithm can get stuck in a local minimum, where the cost function is not very low. This is what is known as a barren plateau.\n",
        "\n",
        "* There are a few things that can be done to mitigate the problem of barren plateaus:\n",
        "\n",
        "  * Reduce the number of parameters in the model. This can be done by simplifying the model architecture or by using regularization techniques.\n",
        "  \n",
        "  * Use a more robust optimization algorithm. Some optimization algorithms are more resistant to getting stuck in local minima than others.\n",
        "\n",
        "  * Use a more informative cost function. A more informative cost function can help the optimization algorithm find a better solution.\n",
        "\n",
        "* Here are some additional resources that you may find helpful:\n",
        "\n",
        "  * Avoiding Barren Plateaus in Variational Quantum Algorithms: https://arxiv.org/abs/2204.13751\n",
        "  * Barren Plateaus in Quantum Neural Network Training Landscapes: https://www.nature.com/articles/s41467-018-07090-4\n",
        "  * Solving 'barren plateaus' is the key to quantum machine learning: https://discover.lanl.gov/news/0319-barren-plateaus/\n",
        "\n",
        "*Techniques for improving gradient scaling and thus learning in quantum machine learning systems include:*\n",
        "\n",
        "1. **Variational Quantum Algorithms:** In the field of quantum machine learning, Variational Quantum Algorithms (VQAs) like the Variational Quantum Eigensolver (VQE) or Quantum Approximate Optimization Algorithm (QAOA) are commonly used. These algorithms employ a hybrid quantum-classical approach, which allows classical optimization techniques to be used in the learning process. This means classical techniques for managing gradient scaling, like batch normalization or gradient clipping, can still be applicable in this quantum setting.\n",
        "\n",
        "2. **Parameter Shift Rule:** The parameter-shift rule is a method for computing gradients in quantum circuits, which is particularly important for variational quantum algorithms. The rule ensures that for certain types of quantum gates (those that generate rotations), the gradient of the expectation value of a quantum circuit with respect to a parameter can be computed exactly, regardless of the number of qubits or the complexity of the circuit. This rule allows the derivative of a quantum circuit output with respect to its parameters to be computed in terms of circuit evaluations. Given a parameterized quantum gate (say, a rotation gate), the derivative of the expectation value of an observable with respect to the parameter can be computed as the difference between the expectation values of the observable for two slightly different values of the parameter. These \"shifted\" parameter values are typically chosen to be a small positive or negative shift from the original parameter value. The parameter-shift rule is powerful because it allows us to compute exact gradients using only additional evaluations of the quantum circuit, with no need for complex computations involving the inner workings of the quantum operations.\n",
        "\n",
        "3. **Adjoint method**: The adjoint method, also known as the reverse-mode differentiation, is a technique borrowed from classical automatic differentiation, generalized to the context of quantum circuits. It involves running the quantum circuit forward, storing the state at each step, and then running a modified version of the circuit backward to calculate the derivatives. This method is efficient in terms of the number of quantum operations required, especially for circuits with many parameters but a single output (as is common in quantum machine learning models). However, it requires the ability to run quantum operations in reverse, as well as the ability to store quantum states, which can be challenging to implement on near-term quantum devices.\n",
        "\n",
        "4. **Randomized Layerwise Training:** Another strategy suggested for training deep quantum circuits involves training one layer at a time with a random initialization for the rest of the circuit, a technique inspired by classical machine learning strategies. This approach can help to alleviate barren plateaus -- regions in the cost function landscape where the variance of the gradients vanishes exponentially with increasing system size.\n",
        "\n",
        "5. **Natural Gradient Descent:** There have been some initial studies into quantum natural gradient descent, which is an analogue to the classical natural gradient descent algorithm and is believed to be more robust to issues with gradient scaling.\n",
        "\n",
        "6. **Quantum-aware optimizers:**\n",
        "  * shot-frugal optimizers [51–54] can employ stochastic gradient descent while adapting the number of shots (or measurements)\n",
        "  * Quantum natural gradi- ent [55, 56] adjusts the step size according to the local geometry of the landscape (based on the quantum Fisher information metric).\n",
        "\n",
        "7. **New Research: Use unbounded objective function**\n",
        "\n",
        "  * Other barren plateaus also don't apply for unbounded objective function. Almost all of QML uses bounded operators.\n",
        "  * **KL divergence** in classical, would be quantum relativ entropy, but that's too hard to compute. **better: Maximal Quantum Rényi Divergence**\n",
        "  * compute with Extended swap test (Generalizes swap test and Hadamard test)\n",
        "  * Learning thermal states: Generative algorithm to thermal state learning,Access to LCU decomposition of the Hamiltonian\n",
        "  * Abstract [Quantum Generative Training Using Rényi Divergences](https://arxiv.org/abs/2106.09567): Quantum neural networks (QNNs) are a framework for creating quantum algorithms that promises to combine the speedups of quantum computation with the widespread successes of machine learning. A major challenge in QNN development is a concentration of measure phenomenon known as a barren plateau that leads to exponentially small gradients for a range of QNNs models.\n",
        "  * In this work, **we examine the assumptions that give rise to barren plateaus and show that an unbounded loss function can circumvent the existing no-go results**. We propose a training algorithm that minimizes the maximal **Renyi divergence** of order two and present techniques for gradient computation. We compute the closed form of the gradients for Unitary QNNs and Quantum Boltzmann Machines and **provide sufficient conditions for the absence of barren plateaus in these models**. We demonstrate our approach in two use cases: thermal state learning and Hamiltonian learning. In our numerical experiments, we observed rapid convergence of our training loss function and frequently archived a 99% average fidelity in fewer than 100 epochs.\n",
        "  * Video: [Maria Kieferova - Training quantum neural networks with an unbounded loss function - IPAM at UCLA](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "8. **New Research: Algebraic solution to solve Barren Plateaus (overparametrization = too much capacity)**\n",
        "\n",
        "  * How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - **Overparamerization (too much capacity)** arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren’t needed anymore.\n",
        "\n",
        "  * Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g. Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "  * From: [QHack 2022: Marco Cerezo —Barren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "  * *Exponentiate Lie algebra to get lie groups:*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)\n",
        "\n",
        "\n",
        "*What is backpropagation scaling?*\n",
        "\n",
        "\"Backpropagation scaling\" usually refers to techniques that manage the magnitudes of gradients during the process of backpropagation, which is used to train neural networks. Proper scaling is important because it can impact the speed and effectiveness of learning.\n",
        "\n",
        "Here are two common scaling issues that may arise during backpropagation:\n",
        "\n",
        "1. **Vanishing gradients:** When deep neural networks are trained, gradients of the loss function can become very small. As a result, weight updates during training become insignificant, and the network takes a very long time to learn, if it can learn at all. This problem is particularly common with activation functions like the sigmoid or hyperbolic tangent, which squish a large input space into a small output range.\n",
        "\n",
        "2. **Exploding gradients:** Conversely, gradients can also become very large, leading to large updates to the weights and causing the model to oscillate around the optimal solution, or even to diverge entirely. This is often a problem in recurrent neural networks (RNNs).\n",
        "\n",
        "Various techniques have been proposed to mitigate these issues:\n",
        "\n",
        "1. **Gradient clipping:** This is a common technique to prevent exploding gradients. If the norm of the gradient exceeds a certain threshold, we scale it back to prevent it from getting too large.\n",
        "\n",
        "2. **Weight initialization:** Properly initializing the weights can prevent gradients from vanishing or exploding too quickly. For example, Xavier/Glorot and He initialization are strategies that consider the sizes of the input and output layers.\n",
        "\n",
        "3. **Choice of activation function:** The choice of activation function can help alleviate the vanishing gradients problem. For example, ReLU (Rectified Linear Unit) and its variants (like Leaky ReLU and Parametric ReLU) are commonly used because they do not saturate for positive inputs.\n",
        "\n",
        "4. **Batch normalization:** This technique normalizes the activations of each layer to prevent the distribution of inputs to each layer from changing too much during training, which can help mitigate both vanishing and exploding gradients.\n",
        "\n",
        "5. **Use of optimizers:** Certain optimization algorithms like RMSProp, Adam, and Nadam can adaptively scale learning rates to mitigate both exploding and vanishing gradients.\n",
        "\n",
        "In summary, backpropagation scaling techniques are important to ensure that the magnitude of updates during training is appropriate and to prevent issues related to vanishing or exploding gradients.\n",
        "\n",
        "\n",
        "*Expectation Value and Backpropagation*\n",
        "\n",
        "The average value (expectation value) of the measurement result is given by the\n",
        "Born rule:\n",
        "\n",
        "> $\\langle B\\rangle=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle$\n",
        "\n",
        "Just linear algebra! Every step is a matrix-vector or matrix-matrix multiplication\n",
        "\n",
        "Expectation values depend continuously on the gate parameters\n",
        "\n",
        "*Backpropagating Through Quantum Circuits*\n",
        "\n",
        "However, as long as we don't \"zoom in\" to what is happening in the quantum circuit, backpropagation can treat the quantum circuit as a single indivisible function\n",
        "\n",
        "The expectation value of a quantum circuit is a differentiable function\n",
        "\n",
        "> $\n",
        "f(\\theta)=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle=\\langle B\\rangle$\n",
        "\n",
        "Running on hardware and using the parameter-shift rule, we can provide both ingredients needed by backpropagation\n",
        "\n",
        "> $\n",
        "\\left(\\langle B\\rangle, \\frac{\\partial}{\\partial \\theta}\\langle B\\rangle\\right)\n",
        "$\n",
        "\n",
        "[Automatic Differentiation of Quantum Circuits](https://youtu.be/McgBeSVIGus)\n",
        "\n",
        "[Variational Quantum Algorithms](https://youtu.be/YtepXvx5zdI)\n",
        "\n",
        "[Hybrid Quantum-Classical Machine Learning](https://youtu.be/t9ytqPTij7k)\n",
        "\n",
        "**Algebraic solution to solve Barren Plateaus (overparametrization = too much capacity)**\n",
        "\n",
        "\n",
        "* How can we measure if a Barren plateau (overparametrization) will occur before running the quantum neural network? - With Lie algebra! - **Overparamerization (too much capacity)** arises when the quantum Fischer information matrices (QFIM) simultaneously saturate their achievable rank. More parameter aren’t needed anymore.\n",
        "\n",
        "* Link to Lie algebra: And the maximum rank of each QFIM is upper bounded by the dimension of the Lie algebra g. Lie Algebra: tells me where do I get when I start at a given state\n",
        "\n",
        "* From: [QHack 2022: Marco Cerezo —Barren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "*Exponentiate Lie algebra to get lie groups:*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1402.png)"
      ],
      "metadata": {
        "id": "VXWcAsxVYr3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***$\\hookrightarrow$ Model Complexity***"
      ],
      "metadata": {
        "id": "m_M1o-kVjMZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> How complex a model needs to be to accurately learn from data without overfitting? - Derive bounds on the generalization error of a learning algorithm (e.g. Inequality: bound probability of random variable deviating from its expected value / bounds probability of estimator deviating from its expected value by a certain amount = provide guarantees on accuracy of estimator).\n",
        "\n",
        "**Expressibility (Expressiveness or Representational Capacity)**:\n",
        "* Expressibility refers to the ability of a learning model or algorithm to represent a wide variety of functions or concepts. It's about the richness of the hypothesis space that the model can capture.\n",
        "* For example, a neural network with more layers and nodes has higher expressibility because it can represent more complex functions compared to a simpler network.\n",
        "* Expressibility is closely related to the concepts like VC dimension or Rademacher complexity, which measure the capacity of a model class to fit data."
      ],
      "metadata": {
        "id": "tyB6nQ3AXv8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In order to solve a specific problem, one has to find a model with sufficient capacity, but not too much, and an according sample size to the model?**\n",
        "\n",
        "\n",
        "Sufficient Capacity: The model should have enough capacity (or complexity) to capture the underlying patterns or relationships in the data. If the model's capacity is too low, it will underfit, meaning it cannot capture the complexity of the data.\n",
        "\n",
        "Avoiding Excess Capacity: On the other hand, if the model's capacity is too high, it risks overfitting, where it starts to learn the noise and random fluctuations in the training data as if they were meaningful patterns. This leads to poor generalization to new, unseen data."
      ],
      "metadata": {
        "id": "X2n0nVThRb_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Complexity I: Model Capacity / Power / Expressivity (for Generalization)**\n",
        "\n",
        "> Model complexity is the size or complexity of the function that an algorithm can learn.Bounding model complexity of learning algorithms: showing that the algorithm is unlikely to learn a function that is too complex, given a sufficient number of training samples. This helps us to prevent the algorithm from overfitting the training data. e.g. **Bounding the model complexity of neural networks**:  Concentration inequalities can be used to show that neural networks are unlikely to learn a function that is too complex, given a sufficient number of training samples and a suitable regularization scheme.\n",
        "\n",
        "* **Capacity** = **depth of functions** (for Generalization): complex functions (can be wide or narrow), also called model power or expressivity. Masure of how many possible hypotheses a learning algorithm can consider (e.g. number of parameters in ML model). Complex models can learn more complex functions, but also more likely to overfit training data and not generalize well to new data.\n",
        "\n",
        "  * (attention, double check this) It is important to distinguish between model capacity and expressivity. Capacity refers to the ability of a model to learn any function, while expressivity refers to the ability of a model to learn a specific class of functions. For example, a neural network with a large number of parameters may have high capacity, but it may not be expressive enough to learn a complex function such as the XOR function.\n",
        "\n",
        "* **Objective of Measuring Model Complexity: understanding how well model can generalize on dataset (prediction error)**\n",
        "  * No overfitting on training (high capacity networks, number of possible hypothesis not too small, but also not too large), not too high variance.\n",
        "  * **Determine bounds** on smallest possible variance, which is ultimately achievable precision, and on shattered (separated) points.\n",
        "  * Generalization (Prediction error) depends on both the training error as well as the complexity of the trained model. The prediction error is small only if training error is itself small and the **complexity of trained model is moderate** (i.e., sufficiently smaller than training data size).\n",
        "\n",
        "* Paper: [On the expressivity of embedding quantum kernels](https://arxiv.org/abs/2309.14419)\n",
        "\n",
        "*There is a relationship between Generalization and capacity (max capacity is not necessarily what we want):*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1399.png)\n",
        "\n",
        "* [Neural network capacity](https://en.m.wikipedia.org/wiki/Artificial_neural_network#Capacity): A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\n",
        "\n",
        "* **Model capacity**: complexity of the patterns a model is capable of learning from the data. Is often associated with size or complexity of model — a model with more parameters (like a deep neural network) has a higher capacity than one with fewer parameters (like a simple linear regression).\n",
        "  * A higher capacity model is theoretically able to learn more complex relationships in the data, but this also opens up the risk of overfitting, where the model learns the noise or specific quirks of the training data instead of the general underlying patterns.\n",
        "\n",
        "* **Expressivity**: how well a model can approximate a wide variety of functions? A neural network with a higher number of layers and neurons would be considered more expressive than a network with fewer layers and neurons because it can theoretically approximate a greater variety of functions [Source](https://www.youtube.com/watch?v=ETxQNIR6dAg&t=684s).\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1602.png)\n",
        "\n",
        "**Metrics for Model Complexity (Measuring Bounds)**\n",
        "\n",
        "*Different measures (metrics) of model capacity and their usefulness (ED: effective dimension):*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1644.png)\n",
        "\n",
        "*Model Complexity - Measure Metrics*\n",
        "* VC dimension\n",
        "* Rademacher complexity\n",
        "* Fisher-Rao norm, Fisher Information and Quantum Cramér-Rao Bound\n",
        "* Covering number\n",
        "* Fat-Shattering Dimension\n",
        "* Effective dimensions\n",
        "* Frobenius norm\n",
        "* Spectral norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZOQKo-v-PWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Complexity II: Model Expressibility (Trainability)**\n",
        "\n",
        "* **Expressibility** = **width of functions** (for trainability): wide array of functions (can be complex or shallow)\n",
        "\n",
        "* Expressibility of circuits: Learning an unknown unitary\n",
        "\n",
        "* **Expressibility & trainability is a tradeoff!!**\n",
        "\n",
        "* Expressibility generally refers to the ease with which a model can be trained to approximate a desired function.\n",
        "\n",
        "* Zoe Holmes: reduce expressibility to increase trainability. Find ansatz that fits the use case problem.\n",
        "\n",
        "* https://pennylane.ai/qml/demos/tutorial_haar_measure.html\n",
        "\n",
        "* Barren plateaus, and hence expressibility issues, are not so much in classical ML, because vanishing cost gradients are not so much of an issue in classical ML because we don‘t have precision limitations in the same way.  You don‘t have to evauluate your cost function using many many shots, or is so resource intensive to get gradients.\n",
        "* In quantum ML: you can use ideas from control theory to try to assess whether or not your ansatz is gonna be trainable or not in advance. Thats an important strategy.\n",
        "* The other approach: use symmetries of your problem / you gonna have to use physics to come up with whats a good ansatz. ZB: use VGQ for some system with various (particle number conserving) translational symmetries, you want to build all of those symmetries into your ansatz and hence reduce expressibility while capturing some of the solution space.\n",
        "\n",
        "* Maria schuld paper: how expressive are circuits? You can distribute circuits and how flexible are they? - identity gate maps to one point only. If you have a couple of more gates it maps to more points.\n",
        "\n",
        "https://arxiv.org/abs/1905.10876: Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1408.png)\n",
        "\n",
        "Is expressivity even important? (maria schuld) https://www.youtube.com/watch?v=8bfUMdj0-x4&t=1384s\n",
        "\n",
        "*Model Complexity in Deep Learning:*\n",
        "\n",
        "* **Expressive capacity**: model complexity may refer to capacity of deep models in expressing or approximating complicated distribution functions (seems to be **expressibility** = width, wide array of functions, complex or shallow)\n",
        "* **Effective complexity**: describes how complicated the distribution functions are with some parameterized deep models (seems to be **capacity** = depth, complex functions, wide or narrow)\n",
        "\n",
        "**There cases where expressibility is high and capacity is small and vice versa:**\n",
        "\n",
        "* You can have High expressibility, small capacity (shallow neural nets), but unlikely to have Low expressibility, high capacity.\n",
        "\n",
        "* **High expressibility, small capacity**: This could potentially occur in situations where a model has a wide range of different functions it can represent (high expressibility) but is limited in the complexity of those functions (small capacity). For example, a shallow neural network (only a few layers deep) can represent a wide variety of different functions, but it may struggle to accurately represent highly complex functions or patterns (like those present in high-dimensional data or complex tasks). This is because it lacks the depth necessary for creating intricate compositional representations.\n",
        "\n",
        "* **Low expressibility, high capacity**: This situation might be harder to come by, but one could imagine a scenario where a model has the potential to represent very complex functions (high capacity) but is restricted in the variety of functions it can actually express due to constraints on its parameters (low expressibility). An example could be a deep neural network with high capacity but with parameters constrained in such a way that it can only express a narrow range of functions. However, such a situation might be considered somewhat artificial.\n",
        "\n",
        "*Expressibility generally refers to the ease with which a model can be trained to approximate a desired function. Here some methods give you a more targeted approach to evaluating and measure model expressibility! It remains a nuanced topic with many factors at play:*\n",
        "\n",
        "1. **Training Convergence**: You can monitor the training convergence of the model, i.e., how quickly and reliably it reaches a minimum of the loss function during training.\n",
        "\n",
        "2. **Gradient-Based Metrics**: Examine the gradients during training, since the behavior of gradients (like vanishing or exploding gradients) can affect the expressibility of the model.\n",
        "\n",
        "3. **Loss Landscape Analysis**: Analyze the loss landscape of the model using techniques such as visualization of loss landscapes to understand the complexity and the potential difficulties in training the model.\n",
        "\n",
        "4. **Hessian-Based Analysis**: Use Hessian-based analysis to study the second-order properties of the loss function, which can provide insights into the local curvature of the loss landscape and potentially the expressibility of the model.\n",
        "\n",
        "5. **Optimization Trajectory**: Study the optimization trajectory of the model during training. Some models may have smoother, more predictable trajectories that suggest better expressibility.\n",
        "\n",
        "6. **Sensitivity to Initialization**: Investigate how the model's performance varies with different initialization strategies. A model that can train effectively from a wide range of initializations might be said to have good expressibility.\n",
        "\n",
        "7. **Sensitivity to Hyperparameters**: Analyze the sensitivity of the model to hyperparameter settings. If a model can only be trained effectively with a very narrow range of hyperparameter settings, it might have limited expressibility.\n",
        "\n",
        "8. **Generalization Gap**: Study the generalization gap, which is the difference between training error and test error. A smaller generalization gap might indicate better expressibility, as it suggests that the model is able to learn a more useful representation of the data.\n",
        "\n",
        "9. **Empirical Studies**: Conduct empirical studies where you train the model on a range of different tasks and datasets to see how easily it can adapt to different kinds of data and problem structures.\n",
        "\n",
        "10. **Computational Resources**: Evaluate the computational resources (time, memory) required to train the model. A model that requires less computational resources to reach a certain level of performance might be said to have better expressibility.\n"
      ],
      "metadata": {
        "id": "kuNW1Ap_Kzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Myths Buster in study of complexity notions*"
      ],
      "metadata": {
        "id": "0md9DQ34gkax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Myths Buster in study of complexity notions**\n",
        "\n",
        "https://www.inference.vc/generalization-and-the-fisher-rao-norm-2/\n",
        "\n",
        "*Myth #1: Current theory is lacking because deep neural networks have too many parameters*\n",
        "\n",
        "* P. Bartlett, “The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network,” 1998\n",
        "* Margin theory was developed to address this very problem for Boosting and NN (e.g. Koltchinskii & Panchenko ’02 and references therein)\n",
        "* Example: linear classifiers $\\left\\{x \\mapsto \\operatorname{sign}(\\langle w, x\\rangle):\\|w\\|_2 \\leq 1\\right\\}$ and assume margin. Then dimension of $w$ (num. of params in 1-layer NN) nevel appears in generalization bounds (and can be infinite). This observation already appears in the 60 's.\n",
        "* In Statistics, one often deals with infinite-dimensional models\n",
        "* Number of parameters is rarely the right notion of complexity (true, in classical statistics still the case for linear regression or simple models)\n",
        "* VC dimension is known to be a loose quantity (distribution-free, only an upper bound)\n",
        "* Our own (arguably incomplete) take on this problem:\n",
        "T. Liang, T. Poggio, J. Stokes, A.R. “Fisher-Rao Metric, Geometry, and Complexity of Neural Networks,” 2017.\n",
        "  * Fisher local norm as a common starting point for many measures of complexity currently studied in the literature (see work of Srebro’s group and Bartlett et al).\n",
        "  * Information Geometry suggests Natural Gradient as the optimization method. Appears to resolve ill-conditioned problems in Shalev-Shwartz et al ’17.\n",
        "\n",
        "*Myth #2: To prove good out-of-sample performance, we need to show uniform convergence (a la Vapnik) over some class*\n",
        "\n",
        "* The oldest counter-example: Cover and Hart, “Nearest neighbor pattern classification,” 1967.\n",
        "* Second (related) issue: uniform vs universal consistency.\n",
        "* **Uniform Consistency**: There exists a sequence $\\left\\{\\hat{y}_t\\right\\}_{t=1}^{\\infty}$ of estimators, such that for any $\\epsilon>0$, there exists $n_e$ such that for any distribution $P \\in \\mathcal{P}$ and $n \\geq n_e$, $\n",
        "\\mathbb{E} L\\left(\\widehat{y}_n\\right)-\\inf L(f) \\leq \\epsilon\n",
        "$\n",
        "* **Universal Consistency**: There exists a sequence $\\left\\{\\widehat{y}_t\\right\\}_{t=1}^{\\infty}$ of estimators, such that for any distribution $P \\in \\mathcal{P}$ and any $\\epsilon>0$, there exists $n_e$ such that for $n \\geq n_e(P)$, $\n",
        "\\mathbb{E} L\\left(\\widehat{y}_n\\right)-\\inf L(f) \\leq \\epsilon\n",
        "$\n",
        "* Importantly, can interpolate between the two notions using penalization. A few more approaches (e.g. use bracketing entropy)\n",
        "\n",
        "*Myth #3: Sample complexity of neural nets scales exponentially with depth*\n",
        "\n",
        "* A common pitfall of making conclusions based on (possibly loose) upper bounds. Mostly resolved: N. Golowich, A.R., O. Shamir, “Size-Independent Sample Complexity of Neural Networks,” 2017\n",
        "* From $2^{\\mathrm{d}}$ to $\\sqrt{\\mathrm{d}}$ dependence was simply a technical issue. From $\\sqrt{\\mathrm{d}}$ to $O(1)$ requires more work.\n",
        "\n",
        "*Myth #4: If we can fit any set of labels, then Rademacher complexity is too large and, hence, nothing useful can be concluded*\n",
        "\n",
        "* Related to Myth #2, but let’s illustrate with a slightly di↵erent technique. Bottom line: we can have a very large overall model, but performance depends on a **posteriori** complexity of the **obtained** solution.\n",
        "* Most trivial example: take a large $\\mathcal{F}=\\cup_k \\mathcal{F}_k$, where $\\mathcal{F}_k=\\left\\{f: \\operatorname{compl}_n(f) \\leq k\\right\\}$ and for simplicity assume $\\operatorname{compl}_n(f)$ is positive homogenous. Suppose (this is standard) we have that with high probability\n",
        "  * $\n",
        "\\forall f \\in \\mathcal{F}_1, \\quad \\mathbb{E f}-\\widehat{\\mathbb{E}} \\mathrm{f} \\lesssim \\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)+\\ldots\n",
        "$\n",
        "* where $\\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)$ is empirical Rademacher. Then with same probability\n",
        "  * $\n",
        "\\forall f \\in \\mathcal{F}, \\quad \\mathbb{E} f-\\widehat{\\mathbb{E}} f \\lesssim \\operatorname{compl}_n(f) \\cdot \\widehat{\\mathscr{R}}\\left(\\mathcal{F}_1\\right)+\\ldots\n",
        "$\n",
        "* Conclusion: an a posteriori data-dependent guarantee for all $\\mathrm{f}$ based on complexity of $f$, yet $\\widehat{\\mathscr{R}}(\\mathcal{F})$ never appears (huge or infinite). If complexity is not positive homogenous, use union bound instead.\n",
        "\n",
        "*Is there anything left to do? Yes, tons. Perhaps need to ask different questions*\n",
        "* What are the properties of solutions that optimization methods find in a nonconvex landscape? Is there “implicit regularization” that we can isolate? - a nice line of work by Srebro and co-authors\n",
        "* What are the salient features of the random landscape? Uniform deviations for gradients and Hessians? - nice work by Montanari and co-authors\n",
        "* How can one exploit randomness to make conclusions about optimization solutions? (e.g. see the SGLD work of Raginsky et al, as well as papers on escaping saddles)\n",
        "* What geometric notions can be associated to multi-layer neural nets? How can this geometry be exploited in optimization methods and be reflected in sample complexity?\n",
        "* Theoretical understanding of adversarial examples. etc."
      ],
      "metadata": {
        "id": "CobPNZxamooq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Model Complexity of classical deep learning*"
      ],
      "metadata": {
        "id": "jRi0tikbgikC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model Complexity of classical deep learning*\n",
        "\n",
        "* the expressive power of a model is used to bound the generalization error\n",
        "* Vapnik-Chervoneniks (VC) dimension [26]. Besides, Rademacher complexity and Gaussian complexity [8, 12] are also used to mea- sure model complexity of logistic regression models\n",
        "* Comparing to VC dimension, Rademacher complexity takes data distribution into consideration and therefore reflects finer-grained model complexity.\n",
        "* suggest that model complexity of logistic regression models is related to the number of distinguishable distributions that can be rep- resented by the models.\n",
        "* Bulso et al. [21] define a complexity measure of logistic regression models based on the determinant of the Fisher Information matrix.\n",
        "* Spiegelhater et al. [93] define a model complexity measure by the number of effective parameters. Using the information theoretic argument, they show that this complexity measure can be estimated by the difference between the posterior mean of the deviance and the deviance at the posterior estimates of the parameters of interest, and is approximately the trace of the product of Fisher’s information [33] and the posterior covariance matrix.\n",
        "* Complexity measures are often model specific and complexity measures of different model frameworks cannot be compared\n",
        "* Expressibility: A notion to capture hypothesis space complexity is Rademacher complexity [12], which measures the degree to which a hypothesis space can fit random noise. Another notion is VC dimen- sion [26], which reflects the size of the largest set that can be shattered by the hypothesis space. Exploring expressive capacity helps to obtain the guarantee of learnability of deep models and derive generalization bounds\n",
        "* Page 11: The comparison of deep and shallow sum- product networks representing the same function indicates that, to represent the same functions, the number of neurons in a shallow network has to grow exponentially but only a linear growth is needed for deep networks.\n",
        "* Page 26: First, based on the piecewise linear property, Novak et al. [81] propose the Jacobian norm to measure the local sensitivity under the assumption that the input is perturbed within the same linear region. (Includes Frobenius norm)\n",
        "* To approach the generalization problem of deep learning models, Liang et al. [60] introduce a new notion of model complexity measure, the Fisher-Rao norm.\n",
        "* In statistical learning theory, expressive capacity (i.e., hypothesis space com- plexity) is used to bound generalization error [69]\n",
        "* Page 33: Based on these desiderata, Neyshabur et al. [76] investigate several complex- ity measures including norms [79], robustness [97], and sharpness [50]. They show that, these measures can meet some of the above requirements, but not all.\n",
        "* Novak et al. [81] define two complexity measures from the perspective of model sensitivity, and identify an empirical correlation between the complexity measures and model generalization capability. They show that operations that lead to poor generalization, such as full batch training, correspond to high sen- sitivity, and in turn imply high effective model complexity. Similarly, operations that lead to good generalization, such as data augmentation, correspond to low sensitivity, and thus imply low effective model complexity.\n",
        "* In other words, many different parameterizations may lead to the same prediction. Thus, the specific parameterization of deep models should not affect the generalization and the complexity measure. They show that the Fisher-Rao norm honors this invari- ance property and thus is able to explain the generalization capability of deep learning models.\n",
        "* On one hand, making predictions with high accuracy is the essential goal of learning a model [69]. A model is expected to be able to capture the underlying patterns hidden in the training data and achieve predictions of accuracy as high as possible. In order to represent a large amount of knowledge and obtain high accuracy, a model with a high expressive capacity, a large degree of freedom and a large training set is required [13]. To this extent, a model with more parameters and higher complexity is favored.\n",
        "    * On the other hand, an overly complex model may be difficult to train and may incur unnecessary resource consumption, such as storage, computation and time cost [72]. Unnecessary resource consumption should be avoided particularly in practical large scale applications [42]. To this extent, a simpler model with comparable accuracy is preferred than a more complicated one.\n",
        "* Can we obtain a lower bound of expressive capacity of deep learning models that are sufficient for a given task? Does a narrow layer limit the expressive capacity of a model even if the model itself has a large number of parameters?"
      ],
      "metadata": {
        "id": "TLrBzX9qghLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Error Correction*"
      ],
      "metadata": {
        "id": "Apl61SPnakw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These pro- cessors are sensitive to their environment, prone to quantum decoherence, and not yet capable of continuous quantum error correction. This is improving significantly with ad- vancements in materials, and there are techniques that can be applied to refresh the information, such as, “Dynamical Decoupling”."
      ],
      "metadata": {
        "id": "ZVREhaail6V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Transmon Qubit](https://youtu.be/kbeSTcmeXCs?si=JwzvgDvBdbqWjN_r)"
      ],
      "metadata": {
        "id": "gxb4K7Dh9xXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/never-repeating-tiles-can-safeguard-quantum-information-20240223/"
      ],
      "metadata": {
        "id": "zYUg4WOs3re6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.spektrum.de/news/kosmische-strahlung-verursacht-ein-sechstel-der-quantenfehler/2208778"
      ],
      "metadata": {
        "id": "D4_hc7cp99jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resisting high-energy impact events through gap engineering in superconducting qubit arrays: https://arxiv.org/abs/2402.15644 (T1 error)"
      ],
      "metadata": {
        "id": "n5Rug5t6I12f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resisting high-energy impact events through gap engineering in superconducting qubit arrays**\n",
        "\n",
        "In a beautiful experiment, my colleagues at Google show that gap-engineered superconducting qubits are resilient to high energy (cosmic rays, gammas) impacts, and even IR irradiation. This experiment largely puts to rest the worry that you would need to build the quantum computer in a tunnel under a mountain.\n",
        "\n",
        "https://arxiv.org/abs/2402.15644"
      ],
      "metadata": {
        "id": "bK9WjmvOTPWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Dave Aasen - Quantum computation from dynamic automorphism codes - IPAM at UCLA ](https://youtu.be/Wrn9ewZaR9U?si=rUydxzOFMw03zXIH)\n",
        "\n",
        "Recorded 27 November 2023. Dave Aasen of Microsoft Station Q presents \"Quantum computation from dynamic automorphism codes\" at IPAM's Topology, Quantum Error Correction and Quantum Gravity Workshop.\n",
        "Abstract: We propose a new model of quantum computation comprised of low-weight measurement sequences that simultaneously encode logical information, enable error correction, and apply logical gates. These measurement sequences constitute a new class of quantum error-correcting codes generalizing Floquet codes, which we call dynamic automorphism codes. We construct an explicit example, the dynamic automorphism color code, which is assembled from short measurement sequences that can realize all 72 automorphisms of the 2D color code. On a stack of N triangular patches, the dynamic automorphism color code encodes N logical qubits and can implement the full logical Clifford group by a sequence of two- and, more rarely, three-qubit Pauli measurements. We also make the first step towards universal quantum computation with dynamic automorphism codes by introducing a 3D dynamic automorphism color code and showing that a non-Clifford logical gate can be realized by adaptive two-qubit measurements.\n",
        "\n"
      ],
      "metadata": {
        "id": "LN3GkaTPy2Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Calibrating large quantum processors](https://www.youtube.com/watch?v=fk2HH9M4FqA&list=WL&index=8&t=224s)"
      ],
      "metadata": {
        "id": "ixbZSt8G76I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Estimating overheads for quantum fault-tolerance in the honeycomb code](https://youtu.be/ND9OoqJ0NMw?si=y2jnMCryfqdOc4wn)\n",
        "\n",
        "video: [Pulse sequence design for crosstalk mitigation](https://youtu.be/suO1_7rZAW0?si=XSNiKQX8t23yZOe6)\n",
        "\n",
        "honeycomb: [Craig gidney: [Talk] Software and the Honeycomb Code](https://youtu.be/O3NaTGmY0Rw?si=3fp6f9ApE8Gu5sF2)\n",
        "\n",
        "honeycomb: [A short history of the honeycomb code](https://youtu.be/frnym8S5qM4?si=wrnIJ6A6_a188Xug)"
      ],
      "metadata": {
        "id": "UcLw3fA4zCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [QIP 2022 | Floquet Codes (Matthew Hastings)](https://www.youtube.com/watch?v=JTvByDNy8zE&list=WL&index=6&t=68s)\n",
        "\n",
        "video: [QIP2023 | Floquet codes without parent subsystem codes (Margarita Davydova)](https://www.youtube.com/watch?v=nq021Pw7orc&list=WL&index=7&t=40s)"
      ],
      "metadata": {
        "id": "bMmznOFT7sSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumpoet.com/superconducting-quantum-computing/"
      ],
      "metadata": {
        "id": "dSMojPGuvt7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/how-quantum-computers-will-correct-their-errors-20211116/"
      ],
      "metadata": {
        "id": "4h_aGy2Kf4UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/new-codes-could-make-quantum-computing-10-times-more-efficient-20230825/\n",
        "\n",
        "https://www.quantamagazine.org/physicists-create-elusive-particles-that-remember-their-pasts-20230509/"
      ],
      "metadata": {
        "id": "oqNWZS_EPXdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*difference between \"code space\", \"code word\" and \"stabilizer code\"?*\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/1822/what-is-the-difference-between-code-space-code-word-and-stabilizer-code"
      ],
      "metadata": {
        "id": "yRaDer6BKAp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error Correction Codes**\n",
        "\n",
        "https://www.quantamagazine.org/the-basic-algebra-behind-secret-codes-and-space-communication-20230123/"
      ],
      "metadata": {
        "id": "pvAtUauVDPN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Correction Zoo**\n",
        "\n",
        "https://errorcorrectionzoo.org/c/honeycomb\n",
        "\n",
        "https://errorcorrectionzoo.org/list/ag\n",
        "\n",
        "https://errorcorrectionzoo.org/list/homological\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0110143"
      ],
      "metadata": {
        "id": "N8zOwuitr9nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1603.02286\n",
        "\n",
        "Sometimes transversal means specifically \"the logical operation has constant depth, regardless of code distance\". For example, the transversal S gate in the folded surface code uses physical gates that aren't the S gate. It uses two qubit gates across the folded halves.\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/24269/what-is-formally-a-transversal-operator"
      ],
      "metadata": {
        "id": "8PeCMq-UIJ5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does quantum error correction work?**\n",
        "\n",
        "Quantum error correction is a set of techniques for protecting quantum information from errors due to decoherence and other quantum noise. Here's a high-level summary:\n",
        "\n",
        "1. **Encoding**: The first step in quantum error correction is to encode the quantum information. Rather than storing a quantum bit of information (a \"qubit\") in a single physical qubit, we store it in multiple physical qubits. This is done in such a way that, even if some of the physical qubits are corrupted by noise, the original quantum information can still be recovered. This encoding is done using a quantum error-correcting code. There are many different types of quantum error-correcting codes, each with its strengths and weaknesses.\n",
        "\n",
        "2. **Syndrome measurement**: Once the quantum information has been encoded, the next step is to periodically check for errors. This is done by performing a syndrome measurement, which is a special kind of quantum measurement that can detect whether an error has occurred, and if so, what kind of error it was. Importantly, this measurement does not disturb the encoded quantum information.\n",
        "\n",
        "3. **Error correction**: If the syndrome measurement indicates that an error has occurred, the next step is to perform an error correction. This involves applying a series of quantum gates to the physical qubits to correct the error, based on the result of the syndrome measurement.\n",
        "\n",
        "4. **Repeat**: Because quantum systems are always subject to noise, this process of syndrome measurement and error correction needs to be repeated periodically to keep the errors in check.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "* It's worth noting that the whole process of quantum error correction requires a significant overhead in terms of additional physical qubits and quantum operations. This is a major challenge in the development of large-scale, fault-tolerant quantum computers.\n",
        "\n",
        "* It's also worth noting that, although quantum error correction can protect against many types of errors, it cannot protect against all possible errors. In particular, **it's assumed that the errors are relatively rare and do not all occur at once, and that they are independent and identically distributed across the physical qubits**. **If these assumptions are violated, then quantum error correction may not be able to correct the errors**. This is another major challenge in the field of quantum error correction."
      ],
      "metadata": {
        "id": "JjYa1KwBkNRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) very good\n",
        "Video: [The superconducting transmon qubit](https://youtu.be/dKTNBN99xLw)\n",
        "Video: [The transmon qubit](https://youtu.be/cb_f9KpYipk)\n",
        "Video: [Making quantum error correction practical](https://youtu.be/YPFpll1NFQc)\n",
        "Video: [Steven Girvin](https://youtu.be/nhUKHf-GN_Y)\n",
        "Video: [Quantum Industry Talks](https://youtu.be/eyICn3KCUPI)\n",
        "Video: [Qiskit QEC](https://youtu.be/ZY8PddknCos), [Qiskit](https://youtu.be/SHr3uSv9Bts), [qiskit](https://youtu.be/96a0G4G5ZH8)"
      ],
      "metadata": {
        "id": "68dkfSxfVj8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Concepts:*** *Ancilla qubits, surface code, distance code, X (bit flip) and Z (phase flip) error, threshold theory, required fault-tolerance, logical vs physical qubit*\n",
        "\n",
        "*Tasks: Error detection, error mitigation, error correction, error suppression*\n",
        "\n",
        "Video [Progress Towards Quantum Error Correction with the Surface Code | Qiskit Seminar Series](https://www.youtube.com/watch?v=si5a9RJP01A)"
      ],
      "metadata": {
        "id": "uRkoUVMsuEg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use more than one qubit to represent a state, use neighboring qubit check (so you don't measure the exact state which would collapse the quantum state). You can decode with it and see that the error was on the last qubit. Then you can correct the physical qubit to get back the correct logical qubit state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1278.png)"
      ],
      "metadata": {
        "id": "l3Ja_nTmmvIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[An Introduction to Quantum Error Correction and Fault-Tolerant Quantum Computation](https://arxiv.org/pdf/0904.2557.pdf)"
      ],
      "metadata": {
        "id": "2OdDTy_mW_Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For what do you need abelian groups in quantum computing?\n",
        "\n",
        "In quantum computing, Abelian groups are used to describe the symmetry of a quantum system. Symmetry is a fundamental concept in quantum mechanics, and it refers to the idea that a physical system will remain unchanged under certain transformations. For example, the symmetry of a quantum system may be described by a group of rotations, translations, or reflections. Abelian groups are used to describe the symmetry of a quantum system because they have the useful property of being commutative, meaning that the order in which the transformations are applied does not affect the outcome. This property makes it possible to use Abelian groups to describe the symmetries of a quantum system in a way that is mathematically tractable and easy to work with."
      ],
      "metadata": {
        "id": "4KQRd-aYkmU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Error detection and correction**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Error_detection_and_correction\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Repetition_code"
      ],
      "metadata": {
        "id": "mGJ-sWNFvF47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faut-Tolerant QC (Quantum Error Correction)**\n",
        "\n",
        "* Bit flip (from 0 to 1) or dephasing (from superposition to exact state)\n",
        "\n",
        "* Challenge: we need to keep that states correct without looking at them (because then WE dephase them)\n",
        "\n",
        "* We need a method to **build relatively noiseless qubits (logical qubits)out of many noisy ones (physical qubits)**. This is quantum error correction.\n",
        "\n",
        "* Solution: one way (repetition encoding), sit our qubits on a line. We then go along and ask every pair of next-door-neighbours whether they agree or disagree with each other. This tells us nothing about whether they are 0 or 1. But repetition encoding, which protects against bit flip errors so well, actually makes dephasing more likely!\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_error_correction\n",
        "\n",
        "\n",
        "* Examples of QEC: **repetition code** (simplest QEC) and **surface code** (and color codes?)\n",
        "\n",
        "* techniques: syndrome measurements, decoding, logical operations\n",
        "\n",
        "* https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/\n",
        "\n",
        "\n",
        "* [An introduction to Fault-tolerant Quantum Computing](https://arxiv.org/abs/1508.03695)\n",
        "\n",
        "http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html\n",
        "\n",
        "* [INTRODUCTION TO\n",
        "QUANTUM ERROR\n",
        "CORRECTION](https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2327/files/2019/11/QECIntro.pdf)"
      ],
      "metadata": {
        "id": "B-hRX5qBqHdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Error Mitigation**\n",
        "\n",
        "* quantum error correction is long term goal, meanwhile we try to mitigate it\n",
        "\n",
        "* Error mitigation techniques: statistical corrections (on histogram for example)\n",
        "\n",
        "\t* https://qiskit.org/textbook/ch-quantum-hardware/measurement-error-mitigation.html\n",
        "\n",
        "\t* https://arxiv.org/abs/2005.10189"
      ],
      "metadata": {
        "id": "jX7JYnTLqNo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DiVincenzo's criteria**\n",
        "\n",
        "Video: [Introduction to Toric Code](https://youtu.be/ZRqgAbBGg40) (very good!)\n",
        "\n",
        "Video: [Gottesman 1](https://youtu.be/ltJ1jXQeDl8) and [Gottesman 2](https://youtu.be/cUqys29d0YA)\n",
        "\n",
        "[DiVincenzo's criteria](https://en.m.wikipedia.org/wiki/DiVincenzo%27s_criteria) are conditions necessary for constructing a quantum computer, conditions proposed in 2000 by the theoretical physicist David P. DiVincenzo.\n",
        "\n",
        "**1. A scalable physical system with well-characterized qubits.**\n",
        "\n",
        "**2. The ability to initialize the state of the qubits to a simple fiducial state, such as 000...).**\n",
        "\n",
        "**3. Long relevant decoherence times, much longer than the gate operation time.**\n",
        "\n",
        "**4. A \"universal\" set of quantum gates** (that approximate any unitary operation - a unitary transformation preserves the inner product, which is a property of the Hilbert space)\n",
        "\n",
        "**5. A qubit-specific measurement capability** (ability to measure individual qubits)\n",
        "\n",
        "* Trapped Ion and superconducting qubits do really well on all five criteria\n",
        "\n",
        "6. *The ability to interconvert stationary and flying qubits.*\n",
        "\n",
        "7. *The ability to faithfully transmit flying qubits between specified locations.*\n",
        "\n",
        "* The DiVincenzo criteria consist of seven conditions an experimental setup must satisfy to successfully implement quantum algorithms such as Grover's search algorithm or Shor factorization.\n",
        "\n",
        "* The first five conditions regard quantum computation itself. Two additional conditions regard implementing quantum communication, such as that used in quantum key distribution. One can demonstrate that DiVincenzo's criteria are satisfied by a classical computer.\n",
        "\n",
        "* Comparing the ability of classical and quantum regimes to satisfy the criteria highlights both the complications that arise in dealing with quantum systems and the source of the quantum speed up.\n",
        "\n",
        "*Universal quantum computing and quantum annealer: not all criteria match the quantum annealers*\n",
        "\n",
        "**Definition of Quantum Computing**\n",
        "\n",
        "[Quantum computing](https://en.m.wikipedia.org/wiki/Quantum_computing) is a type of computation whose operations can harness the phenomena of quantum mechanics, such as superposition, interference, and entanglement to perform computation. Devices that perform quantum computations are known as quantum computers.\n",
        "\n",
        "*Harnessing effects of quantum mechanics: by this definition also quantum annealers are quyantum computers.*"
      ],
      "metadata": {
        "id": "9b6pMJ8JVL_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizer Code and Parity Check Measurement**"
      ],
      "metadata": {
        "id": "v41iaGUgY8hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code**\n",
        "\n",
        "*Exkurs: Stabilizer Code & Ancilla qubits*\n",
        "\n",
        "* A [stabilizer](https://en.m.wikipedia.org/wiki/Stabilizer_code) quantum error-correcting code appends [ancilla qubits](https://en.m.wikipedia.org/wiki/Ancilla_bit) to qubits that we want to protect.\n",
        "\n",
        "**A stabilizer quantum error-correcting code appends ancilla qubits to qubits that we want to protect**. A unitary encoding circuit rotates the global state into a subspace of a larger Hilbert space. This highly entangled, encoded state corrects for local noisy errors.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Stabilizer_code\n",
        "\n",
        "In quantum computing, **a stabilizer code is a type of error-correcting cod**e that is used to protect quantum information from the effects of noise and decoherence. These codes are based on the concept of stabilizer operators, which are a special type of operator that can be used to detect and correct errors in a quantum system. The basic idea behind stabilizer codes is to encode the quantum information using a set of stabilizer operators, such that any errors that occur in the system can be detected and corrected by measuring the values of these operators. GPT\n",
        "\n",
        "\n",
        "Many quantum error correction schemes can be classified as stabilizer codes, where a single bit of **quantum information is encoded in the joint state of many physical qubits**, which we refer to as data qubits. Interspersed among the data qubits are **measure qubits**, which periodically measure the parity of chosen combinations of data qubits. https://arxiv.org/pdf/2102.06132.pdf\n",
        "\n",
        "\n",
        "https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s\n",
        "\n",
        "https://leftasexercise.com/2019/01/28/basics-of-quantum-error-correction/\n",
        "\n",
        "https://leftasexercise.com/2019/02/04/q-fault-tolerant-quantum-computing/\n",
        "\n",
        "https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "https://leftasexercise.com/2019/04/08/quantum-error-correction-the-surface-code/\n",
        "\n",
        "https://leftasexercise.com/2019/02/11/quantum-error-correction-with-stabilizer-codes/\n",
        "\n",
        "https://leftasexercise.com/2018/09/10/quantum-computing-an-overview/"
      ],
      "metadata": {
        "id": "2ns0241plUDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stabilizer code with parity check circuits (for Z and X errors)"
      ],
      "metadata": {
        "id": "4c1Ooe-Odl3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parity measurement**: is it even (correct) or odd (error)? You will have a square with one dimension for X error and another dimension for Z error measurement. You have an **ancilla qubit** to make the measurements with C-Z-gate and C-X gate (but you first put it into an equal superposition).\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1283.png)"
      ],
      "metadata": {
        "id": "iCtG8eoVtaGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You put both X and Z together and get a 2D lattice of surface code with a determined code distance. We have now data qubits, X ancilla qubits and Z ancilla qubits.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1284.png)"
      ],
      "metadata": {
        "id": "ujX9f7Wevt2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Repetition Codes**\n",
        "\n",
        "* look at majority of bits\n",
        "\n",
        "* https://qiskit.org/textbook/ch-quantum-hardware/error-correction-repetition-code.html\n",
        "\n",
        "* repetition code: redundancy (repetition) is a way to make sure the message gets delivered (i.e. with majority voting, for d repetition: $P=\\sum_{n=0}^{[ a / 2]}\\left(\\begin{array}{l}d \\\\ n\\end{array}\\right) p^{n}(1-p)^{d-n} \\sim\\left(\\frac{p}{(1-p)}\\right)^{[ d / 2]}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_100.png)"
      ],
      "metadata": {
        "id": "zEwG8U1SwWhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZX-calculus**"
      ],
      "metadata": {
        "id": "cZPcI0kqrh4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1307.7025\n",
        "\n",
        "The ZX-calculus is complete for stabilizer quantum mechanics"
      ],
      "metadata": {
        "id": "0fudQnLJODY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZX Calculus: Hopf rule\n",
        "\n",
        "Reasoning with connectivity using diagrammatic reasoning\n",
        "\n",
        "ZX Calculus: Hadamard rule\n",
        "\n",
        "Evaluating quantum circuits using diagrammatic reasoning."
      ],
      "metadata": {
        "id": "5gLhGntqCW37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/ZX-calculus\n",
        "\n",
        "https://en.wikipedia.org/wiki/Penrose_graphical_notation\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_network_theory?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Unified_field_theory\n",
        "\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_product_state?wprov=sfti1\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Density_matrix_renormalization_group?wprov=sfti1\n",
        "\n",
        "https://en.wikipedia.org/wiki/Categorical_quantum_mechanics?wprov=sfti1\n",
        "\n",
        "And finally I'll learn about fault-tolerance cause now it's in a language that I can understand, cause I co-invented it with Ross Duncan!\n",
        "\n",
        "https://lnkd.in/dX2YDzfF\n",
        "\n",
        "But the “newly introduced ZX-instruments” want to be the bastard spiders of the dodo-book. They go back a long time, even before ZX calculus itself, and published here:\n",
        "\n",
        "https://lnkd.in/e4VvrGHA\n",
        "\n",
        "Quantum In Pictures also has that stuff, and so does this paper:\n",
        "\n",
        "https://lnkd.in/eSwFWkHZ"
      ],
      "metadata": {
        "id": "KbDcHaTjrqmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity (Error Probability)**"
      ],
      "metadata": {
        "id": "f0O6pdoUYhec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Physical and Logical Qubits\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Physical_and_logical_qubits"
      ],
      "metadata": {
        "id": "j7puSMXHeUxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fidelity**: For Shor's algorithm with estimated 10^9 physical gates required, the error should be less than 10^-9, ideally 10^-10. We are still several orders of magnitude away from that accuracy / faut-tolerance.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1279.png)"
      ],
      "metadata": {
        "id": "EM0u6HMroENg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many applications call for error rates in the 10−15 regime [2–9], but state-of-the-art quantum platforms typically have physical error rates near 10−3\n",
        "https://arxiv.org/pdf/2102.06132.pdf"
      ],
      "metadata": {
        "id": "FcbSD88fkhOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run quantum algorithms perfectly we need error probability of 1 in a billion or 1 in a trillion - but we are at 1 in a thousand\n",
        "\n",
        "Video: [Suppressing quantum errors by scaling a surface code logical qubit](https://www.youtube.com/watch?v=dVkLNwSTBU0)"
      ],
      "metadata": {
        "id": "pQbRXxY779rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Fidelity of quantum states**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Fidelity_of_quantum_states\n",
        "\n",
        "* Noise transforms pure states into mixed states.\n",
        "\n",
        "  * There are also simpler ones: Fidelity between two pure states\n",
        "\n",
        "  * And there are also more complex ones: Fidelity between two mixed states\n",
        "\n",
        "* fidelity is generally defined as the quantity:\n",
        "\n",
        "> ${\\displaystyle F(\\rho ,\\sigma )=\\left(\\operatorname {tr} {\\sqrt {{\\sqrt {\\rho }}\\sigma {\\sqrt {\\rho }}}}\\right)^{2}}$\n",
        "\n",
        "* most useless state is fidelity 0,5. because fidelity = 0 means orthogonal, and =1 means exactly the same.\n",
        "\n",
        "* Video: [Fidelity](https://www.youtube.com/watch?v=GWi_HIVz2B4)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1275.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1276.png)"
      ],
      "metadata": {
        "id": "GV4K-Cl0qIIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of errors that you want to detect: **Bit-flip** (represented with Pauli X gate) and **Phase-flip** (represented with Pauli-Z gate). But 1D string physical qubits cannot protect from bit and phase flip at the same time. You need a 2D string.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1281.png)"
      ],
      "metadata": {
        "id": "In3mQfhOp0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene: Qubits können auch ganz verloren gehen**\n",
        "\n",
        "* Inzwischen können Quantencomputer mit einer gewissen Anzahl von Rechenfehlern, wie zum Beispiel Bitflip- oder Phasenflip-Fehlern, umgehen. Zusätzlich zu diesen Fehlern können jedoch auch Qubits ganz aus dem Quantenregister verloren gehen.\n",
        "\n",
        "* Je nach Art des Quantencomputers kann dies auf den tatsächlichen Verlust von Teilchen wie Atomen oder Ionen zurückzuführen sein, oder darauf, dass Quantenteilchen beispielsweise in unerwünschte Energiezustände übergehen, welche nicht mehr als Qubit erkannt werden. Wenn ein Qubit verloren geht, wird die Information in den verbleibenden Qubits unlesbar und ungeschützt. Für das Ergebnis der Berechnung kann dieser Prozess zu einem potentiell verheerenden Fehler werden.\n",
        "\n",
        "https://www.cosmos-indirekt.de/News/Neue_Methode_schützt_Quantencomputer_vor_Ausfällen.html\n",
        "\n",
        "Resolving catastrophic error bursts from cosmic rays in large arrays of superconducting qubits.\n",
        "\n",
        "https://arxiv.org/abs/2104.05219\n",
        "\n",
        "https://physicsworld.com/a/cosmic-ray-threat-to-quantum-computing-greater-than-previously-thought/"
      ],
      "metadata": {
        "id": "0N8AlBcAz4Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Syndrome Measurement**\n",
        "\n",
        "**|1> measured in instead of a |0> : The permutations of outputted |1〉’s is called the “error syndrome”.**\n",
        "\n",
        "Syndrome: what qubit the error is on\n",
        "\n",
        "Error syndrome measurement:\n",
        "\n",
        "https://www.quora.com/Error-Correcting-Codes-What-is-a-syndrome:\n",
        "\n",
        "* The syndrome measurement provides information about the error that has happened, but not about the information that is stored in the logical qubit—as otherwise the measurement would destroy any quantum superposition of this logical qubit with other qubits in the quantum computer, which would prevent it from being used to convey quantum information. (https://en.m.wikipedia.org/wiki/Quantum_error_correction)\n",
        "\n",
        "* It is the result of multiplying a parity check matrix times a vector. By convention, codewords of a code have syndrome zero, so that by linearity of the code, the syndrome of a word is the syndrome of the \"error\" vector. Typically from the syndrome you would either try to determine whether there was an error (is the syndrome nonzero?) and recover the error from it, so that in turn you can recover the data from the received word.\n",
        "\n",
        "syndrome. = error?\n",
        "\n",
        "here slide 4: https://people.engr.tamu.edu/andreas-klappenecker/689/stabilizer.pdf"
      ],
      "metadata": {
        "id": "fYAPsGZXAPsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Distance and Surface Code**"
      ],
      "metadata": {
        "id": "dpys_ybfYkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Five-qubit_error_correcting_code"
      ],
      "metadata": {
        "id": "KVjhpug-hkYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum **Threshold theory** ensure that there is a limit that helps to get error under control even with larger numbers of physical qubits. The **code distance** is then a result of the max error rate and represents the number of physical qubits for one state:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1280.png)"
      ],
      "metadata": {
        "id": "RuL0PiU_okd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Quantum Threshold Theorem**\n",
        "\n",
        "* Challenge: quantum computer will not be able to perform gate operations perfectly, some small constant error is inevitable\n",
        "\n",
        "* [quantum threshold theorem](https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem) (or quantum fault-tolerance theorem) states that a quantum computer\n",
        "  * **with a physical error rate below a certain threshold** can,\n",
        "  * **through application of quantum error correction schemes**,\n",
        "  * suppress the logical error rate to arbitrarily low levels.\n",
        "\n",
        "* This shows that quantum computers can be made fault-tolerant, as an analogue to von Neumann's threshold theorem for classical computation\n",
        "\n",
        "* The formal statement of the threshold theorem depends on the types of error correction codes and error model being considered.\n",
        "\n",
        "* for any particular error model (such as having each gate fail with independent probability p), use **error correcting codes** to build better gates out of existing gates.\n",
        "\n",
        "  * Though these \"better gates\" are larger, and so are more prone to errors within them, their error-correction properties mean that they have a lower chance of failing than the original gate (provided p is a small-enough constant).\n",
        "\n",
        "  * Then, one can use these better gates to recursively create even better gates, until one has gates with the desired failure probability, which can be used for the desired quantum circuit.\n",
        "\n",
        "* Current estimates put the threshold for the [surface code](https://en.m.wikipedia.org/wiki/Toric_code) (here: Toric code) on the order of 1%, though estimates range widely and are difficult to calculate due to the exponential difficulty of simulating large quantum systems.\n",
        "\n",
        "* At a 0.1% probability of a [depolarizing](https://en.m.wikipedia.org/wiki/Depolarization) error, the surface code would require approximately 1,000-10,000 physical qubits per logical data qubit, though more pathological error types could change this figure drastically.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1xEbTDdNwar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Distance Code / Code Distance**\n",
        "\n",
        "* https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory\n",
        "\n",
        "\n",
        "* Over all 25 cycles of error correction, the distance-5 code realises lower logi- cal error probabilities pL than the average of the subset distance-3 codes - [Paper](https://arxiv.org/pdf/2207.06431.pdf)\n",
        "\n",
        "* the distance is the shortest path in a certain \"space of errors\" which maps between two orthogonal quantum states that are in the code.\n",
        "\n",
        "* The natural space of errors is that of single qubit errors of the form 𝜎𝑋, 𝜎𝑌 or 𝜎𝑧, in the case where the Hilbert space is that of 𝑛 qubits.\n",
        "\n",
        "* So you can think of distance as the shortest path to get from one state to another by operations on single qubits, applied one at a time sequentially.\n",
        "\n",
        "* [Source](https://physics.stackexchange.com/questions/29397/what-is-the-code-distance-in-quantum-information-theory)"
      ],
      "metadata": {
        "id": "ieotWtPN7sz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Code*"
      ],
      "metadata": {
        "id": "KSaDCK89Y3_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, code distance and surface code are not the same in quantum error correction.\n",
        "\n",
        "* **Code distance** is a measure of the ability of a quantum error-correcting code to correct errors. A code with a higher code distance can correct more errors than a code with a lower code distance.\n",
        "* **Surface code** is a specific type of quantum error-correcting code that has a high code distance. Surface codes are often used in quantum computers because they are relatively easy to implement and can correct a large number of errors.\n",
        "\n",
        "In other words, code distance is a property of all quantum error-correcting codes, while surface code is a specific type of quantum error-correcting code that has a high code distance.\n",
        "\n",
        "Here is an analogy to help you understand the difference between code distance and surface code:\n",
        "\n",
        "* Code distance is like the number of lanes on a highway. A highway with more lanes can handle more traffic than a highway with fewer lanes.\n",
        "* Surface code is like a specific type of highway that is designed to handle a lot of traffic. Surface highways have more lanes than other types of highways, so they can handle more traffic.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "XJl73OAdfcoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D Surface code** protects from X error and Z error (X and Z - that's why 2 D). Errors typically arise only locally. The gate structure needs to fit the physical geometry of the quantum processor. Error per gate should be 0,5%, but overall threshold depends on case.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1282.png)"
      ],
      "metadata": {
        "id": "jQZePon1slGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Stabilizer Code $\\rightarrow$ Surface Code $\\rightarrow$ Toric Code**\n",
        "\n",
        "* https://quantumcomputing.stackexchange.com/questions/2106/what-is-the-surface-code-in-the-context-of-quantum-error-correction\n",
        "\n",
        "* **Surface codes**: family of quantum error correcting codes defined on a 2D lattice of qubits.\n",
        "\n",
        "* Each code has [stabilizers](https://en.m.wikipedia.org/wiki/Stabilizer_code) that are defined equivalently in the bulk, but differ from one another in their boundary conditions.\n",
        "\n",
        "* The members of the surface code family are sometimes also described by more specific names:\n",
        "\n",
        "  * The [toric code](https://en.m.wikipedia.org/wiki/Toric_code) is a surface code with periodic boundary conditions,\n",
        "\n",
        "  * the planar code is one defined on a plane, etc.\n",
        "\n",
        "* How many qubits are in a surface code? - While the surface code requires four-qubit measurements to encode a single logical qubit, we introduce families of quantum error correcting codes that use only three-qubit measurements. [Paper](https://www.ucl.ac.uk/quantum/news/2021/aug/subsystem-codes-outperform-surface-code)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [Topological quantum memory (paper)](https://arxiv.org/abs/quant-ph/0110143)\n",
        "\n",
        "* [Surface codes: Towards practical large-scale quantum computation (paper)](https://arxiv.org/abs/1208.0928)\n",
        "\n",
        "* [My blog series introducing surface codes](http://decodoku.blogspot.com/2016/02/5-story-so-far_57.html)\n",
        "\n",
        "* The surface codes can also be generalized to qudits. For more on that, [see here (Fault-tolerant quantum computation by anyons)](https://arxiv.org/abs/quant-ph/9707021)"
      ],
      "metadata": {
        "id": "Ur4bkT3KooVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Toric Code**\n",
        "\n",
        "* For the toric code we don’t put our qubits in a line, we put them in a grid pattern.\n",
        "\n",
        "* Video: [INTRODUCTION TO TOPOLOGICAL ORDER, DEMONSTRATION VIA THE TORIC CODE](https://www.youtube.com/watch?v=Rs2NMe4Lsbw&t=456s)\n",
        "\n",
        "* https://leftasexercise.com/2019/03/25/qec-an-introduction-to-toric-codes/\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/6-toric-code-part-2.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/03/8-toric-code-part-3.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/9-toric-code-part-4.html\n",
        "\n",
        "* http://decodoku.blogspot.com/2016/04/10-toric-code-part-5.html"
      ],
      "metadata": {
        "id": "zpXlgK2jyNPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Color Code**\n",
        "\n",
        "* [Fault-tolerant quantum computing with color codes](https://arxiv.org/abs/1108.5738)\n",
        "\n",
        "* https://physics.stackexchange.com/questions/169176/quantum-error-correction-surface-code-vs-color-code\n",
        "\n",
        "The color code and surface code are very similar. They are stabilizer codes composed of qubits arranged in two dimensions, requiring only geometrically local stabilizer measurements.\n",
        "\n",
        "From the theory point of view, the codes are very similar. In fact, with collaborators we have proven that the color code is equivalent to a surface code (paper) up to a geometrically local unitary (one which only makes nearby qubits interact). One can think by analogy of the surface code* as a napkin with two rough and two smooth sides and the color code as folding this napkin along its diagonal. Because in the folded napkin, there are new things that are now close, it is possible to do more logical gates \"transversally\". This is good because it keeps errors from propagating and is relatively easy. However, the color code needs more qubits to interact in each stabilizer so ends up leading to a lower noise threshold. So one can say that although very similar, each code has its advantages and disadvantages.\n",
        "\n",
        "At this point, only very small versions of either of these codes are being demonstrated. The Rainer Blatt group demonstrated the smallest possible color-code which also uses 7 qubits (this instance is also referred to as the Steane code). However, the underlying geometry in which the qubits are laid out in the Blatt setup is a linear chain of ions, so I would say that this is not the natural setting to extend to larger and larger system sizes.\n",
        "\n",
        "**The superconducting qubit people (Martinis, IBM, DiCarlo, ...) on the other hand, are concentrating more on surface codes**. While in principle, their architecture should allow them to go full fledge 2D, for now, they are having the classical logic come in from the sides, which is something that needs to change.\n",
        "\n",
        "*There is actually an ambiguity as to what to call surface codes, but I will refer to the quantum double of Z2 with rough and smooth boundaries defined by Bravyi and Kitaev (paper)."
      ],
      "metadata": {
        "id": "9tWyVLKvxLE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Distance 2 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "cNo9Qsp7ZTQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go down from a seven qubit surface code to a simpler two qubit surface code. Smallest meaningful is a 2x2 lattice. But it's just an error detection code, because it's too small to do error correction.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1286.png)"
      ],
      "metadata": {
        "id": "d503fihiwLGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green line is a flux line**. We put a magnetic flux through the script loop of each qubit to put a a specific frequency where we want it to be. **Pink line is a charge line** that is used for single qubit gates. All the rest (red, blue, purple box) are part of the readout. It's very important to have a good readout - we need to measure the ancilla qubits during the operation to see if there wasn't an error. You see that sort of resonator over each Qubit (die dinger die aussehen wir alte Heizungskoerper) - this is very standard, there is a harmonic oscillator.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1287.png)"
      ],
      "metadata": {
        "id": "ugRZQcwN0eoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1288.png)"
      ],
      "metadata": {
        "id": "Mbmct0-22-2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1289.png)"
      ],
      "metadata": {
        "id": "sddTSG2S7J3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*7 Qubit Surface Code (Gate Circuit)*"
      ],
      "metadata": {
        "id": "QHclLheAYhtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a 7 qubit gate circuit. You can't correct the error, but you can detect it. At the end we verify by measuring the actual state of each qubit.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1290.png)\n"
      ],
      "metadata": {
        "id": "eqXaY73s5YSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run multiple measurements in (20) microseconds for Z and X operator:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1291.png)"
      ],
      "metadata": {
        "id": "ywM2-Zqb6dQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D = 3 Surface Code Stabilizer Gate Sequence**\n",
        "\n",
        "* Red: data qubits\n",
        "* Blue: X-type ancilla qubit\n",
        "* Green Z-type ancilla qubit\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1300.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1301.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1302.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1303.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1304.png)\n"
      ],
      "metadata": {
        "id": "5sejATAN9-FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1305.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1306.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1307.png)\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1308.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1L2hW2GU_SsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Future of Quantum Error Correction*"
      ],
      "metadata": {
        "id": "0PltPkkfZ1Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future of quantum error correction**\n",
        "* we still have problems like leakage etc\n",
        "  * When they ask about thresholds, they are also derived from ideal models. Things like circular ZZ coupling can make it much harder.\n",
        "  * If you add leakage to your CZ gates, you could take a surface code that would sort of there was zero leakage below the physical error rate. But if you add leakage to that 0.1 percent degrees (the red line), which is a second. When you do the decoding (the green line), suddenly you are not below the threshold anymore\n",
        "* Also, if we want distance n=17 (mentioned in the beginning) you have an insane amount of data coming out of the device:\n",
        "  * we need ($n^2 -1$) physical qubits for error correction. So if one readout is 1 bit, we need 288 bits per microsecond ($\\mu$s) per qubit).\n",
        "  * This amounts to 288 Gbits per s for 1000 logical qubits\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1292.png)"
      ],
      "metadata": {
        "id": "yps3FbFbuHOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyons, Transmon and Fluxonium**"
      ],
      "metadata": {
        "id": "oQZJ3rU97I-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyon (Quasi Particles)**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Anyon\n",
        "\n",
        "* PBS Video on Quasiparticles: https://youtu.be/le_ORQZzkmE"
      ],
      "metadata": {
        "id": "gy8QFKOMICxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transmon and Fluxonium Qubit**\n",
        "\n",
        "* Better hardware to protect against noise orders of magnitude better (to get down to 10^-5 instead of 10^-9\n",
        "* they can show cherence times above 1 millisecond, they had single qubit errors of 0.9999 (only 4x) with Fluxonium\n",
        "\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1293.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1299.png)\n",
        "\n",
        "*Source: https://theorie.physik.uni-konstanz.de/burkard/sites/default/files/images/Seminar_3_TrFl.pdf*\n",
        "\n",
        "\n",
        "Video: [Google Keynote: Superconducting qubits for quantum computation: transmon vs fluxonium](https://www.youtube.com/watch?v=qsizrKrUZDg)"
      ],
      "metadata": {
        "id": "We68Qy_9vgEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anyons, Qubit Braiding and Topological Quantum Computing**\n",
        "\n",
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/#loop-excitations-and-error-correction\n",
        "\n",
        "https://www.quantamagazine.org/physicists-create-elusive-particles-that-remember-their-pasts-20230509/\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1568.png)\n",
        "\n",
        "\n",
        "\n",
        "https://arthurpesah.me/blog/2023-05-13-surface-code/\n",
        "\n",
        "https://dom-kufel.github.io/blog/2023-05-13-toric_code-intro/\n",
        "\n",
        "https://arthurpesah.me/blog/2023-01-31-stabilizer-formalism-1/\n",
        "\n",
        "https://arthurpesah.me/blog/2022-01-25-intro-qec-1/\n",
        "\n",
        "https://blog.google/technology/research/an-important-step-towards-improved-quantum-computers/\n",
        "\n",
        "https://phys.org/news/2023-05-google-quantum-ai-braids-non-abelian.html\n",
        "\n",
        "https://www.spektrum.de/news/nichtabelsche-anyonen-auf-quantenprozessor-simuliert/2138241\n",
        "\n",
        "\n",
        "\n",
        " I am also frustrated by us and the media representing these anyon simulations as a \"step towards fault-tolerance\". Certainly topological codes are inspired by the physics of anyons and there are direct analogies between how those codes work and are realized and these simulations. I also have no doubt that certain aspects of these simulations have some degree of error robustness. But my understanding is that once you get serious about turning such simulations into a form of fault-tolerance appropriate for our devices and then try to optimize its realization you end up with the surface code. So while thinking about the physics of error-correction from this perspective might be useful and there are myriad mathematical similarities between what is going on in these simulations and topological codes, I think it is misleading/wrong to claim that performing these simulations is taking us any closer to fault-tolerance (whether the surface code or a different form of it). People keep telling me that I am misunderstanding this (and maybe I am!) but I have not been convinced of that yet.\n",
        "\n",
        "my objection, which might be different from Cody's, is that the title of this blog post suggests that this experiment is getting us closer to realizing universal fault-tolerant quantum computers. In particular, when we say it is a step closer to improved quantum computers, it makes it sound like this approach is somehow a step on our roadmap for realizing fault-tolerant quantum computers, or that it is going to make our ultimate goal easier or something like that. That is reading between the lines a little bit, but it is how it sounds to me. Is that actually true? Or is this in the category of \"nice demonstration, but not something we're going to follow up on, and thus not a step towards us improving quantum computers other than in the very broad sense that it helps test hardware capabilities like most physics team experiments do\"? My understanding is that nothing about this experiment is likely to change anything about how we are planning to realize fault-tolerance. Maybe that is incorrect, or it is correct and I am just reading too much into the implications of statements like the title. It would be nice to understand this better.\n",
        "\n",
        "The main sense in which the non-abelian anyons are useful for fault tolerance is that they are isomorphic to twists in the surface code, and you can use twists to store logical qubits. But we already knew about twists independent of this work e.g. from https://arxiv.org/abs/1609.04673 so I don't know what it adds on the fault tolerance side.\n",
        "\n",
        "I did give a presentation to the authors of our abelian paper on how to do it fault tolerantly (they were technically \"using\" a surface code, but they were preparing it entirely unitarily which is not fault tolerant; you have to use measurements for everything). They weren't so interested I think, because the measurement version is harder and maybe also because they didn't consider it \"really doing it\" in the same way. I actually did run some quick shots of my versions of the circuits on the device back when pink was in M2 shape, and it worked, in that I got non-zero signal (the error rate was close to max but not max). I wasn't doing adept or etc or etc; if an experimentalist did it it would have been better. Also I now have much better versions of the circuit, which are described in https://arxiv.org/abs/2302.07395 .\n",
        "\n",
        "My point was that if you interpret TQC as broadly (\"you have things that act like anyons\") then this is demonstrating something interesting (multiparticle entangled EC state, FT gates on that state).  But I also agree with Ryan in that I think the blog post does not convey this well.\n",
        "\n",
        "Fair enough, I'm just trying to put forth a perspective on why it is fine to say this is an interesting experiment in topological quantum computation.  And if you want some idea of how it might lead to noise protection, yes  https://arxiv.org/abs/quant-ph/9912040 which was expanded upon in https://arxiv.org/abs/0907.3988\n",
        "\n",
        "And indeed our own team's work investigating how simulations of many body systems are related does or does not give protection of topologically protected information is in this direction.\n",
        "\n",
        "I think I agree with everything you've said Dave. But I don't think any of that really justifies the framing of the blog post title, which to me suggests that this is moving us closer to building a fault-tolerant quantum computer as opposed to exploring the error robustness of anyon simulation."
      ],
      "metadata": {
        "id": "yvGciNEzWO-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, anyons and transmons are related in quantum computing. Anyons are theoretical particles that have unique properties that make them well-suited for quantum computing. Transmons are physical implementations of qubits that can be used to build quantum computers.\n",
        "\n",
        "One of the key properties of anyons is that they can be braided together to perform quantum operations. This is because the braiding of anyons is a topological operation, meaning that it is not affected by the environment. This makes anyons very resistant to noise, which is a major challenge in quantum computing.\n",
        "\n",
        "Transmons, on the other hand, are physical particles that can be used to build qubits. Qubits are the basic unit of information in quantum computing. Transmons are made of superconducting circuits and can be manipulated using microwaves.\n",
        "\n",
        "Transmons are not as topologically protected as anyons, but they are much easier to implement in a practical quantum computer. This is because transmons can be manufactured using existing semiconductor technology.\n",
        "\n",
        "So, anyons and transmons are both promising candidates for quantum computing. Anyons are more topologically protected, but transmons are easier to implement. It is still not clear which approach will ultimately be more successful, but both anyons and transmons are active areas of research.\n",
        "\n",
        "Here is a table that summarizes the key differences between anyons and transmons:\n",
        "\n",
        "| Property | Anyons | Transmons |\n",
        "|---|---|---|\n",
        "| Topological protection | Yes | No |\n",
        "| Ease of implementation | No | Yes |\n",
        "| Current research status | Active | Active |\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "CaNGqKxZgJy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Jellium\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Cooper-Paar\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Bose–Einstein_condensate\n",
        "\n",
        "https://opg.optica.org/oe/fulltext.cfm?uri=oe-2-8-299&id=63264\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Phonon\n",
        "\n",
        "\n",
        "Part of this correlation is the formation of pairs of electrons called Cooper pairs. According to Josephson, under certain circumstances these Cooper pairs move from one superconductor to the other across the thin insulating layer. Such motion of pairs of electrons constitutes the Josephson current, and the process by which the pairs cross the insulating layer is called Josephson tunneling.\n",
        "\n",
        "https://www.britannica.com/science/Josephson-effect\n",
        "\n",
        "\n",
        "Meissner effect\n",
        "\n",
        "Meissner effect, the expulsion of a magnetic field from the interior of a material that is in the process of becoming a superconductor, that is, losing its resistance to the flow of electrical currents when cooled below a certain temperature, called the transition temperature, usually close to absolute zero. The Meissner effect, a property of all superconductors, was discovered by the German physicists W. Meissner and R. Ochsenfeld in 1933.\n",
        "\n",
        "https://slideplayer.com/slide/5010186/\n",
        "\n",
        "Squid"
      ],
      "metadata": {
        "id": "I2GgHQvDATOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Josephson Junction*"
      ],
      "metadata": {
        "id": "siEtU4-r3-Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Superconducting integrated circuits**\n",
        "\n",
        "- Conductance is not constant but varies with how much current is flowing, making it an unharmonic oscillator\n",
        "- Potential of the conductor is a cosine\n",
        "- Low energy excitations are pairs of electrons slashing back and forth between the two antenna pads\n",
        "- From ground state to first excited state: 5 gigahertz\n",
        "- From first excited state to second excited state transition: 4.9 Ghz (due to flattened curve of cosine)\n",
        "\n",
        "More details: https://www.youtube.com/watch?v=uD69GCYF9Zg&t=2023s\n"
      ],
      "metadata": {
        "id": "yx8LP-FXATe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übergangsdipolmoment (Transition dipole moment)\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Übergangsdipolmoment"
      ],
      "metadata": {
        "id": "MTADPYzI5eaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ZZ: Residual ZZ coupling, circular ZZ coupling, ZZ crosstalk**\n",
        "\n",
        "*  Noise is a significant obstacle to quantum computing, and 𝑍 𝑍 cross- talk is one of the most destructive types of noise affecting supercon- ducting qubits. Previous approaches to suppressing 𝑍𝑍 crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to sup- press 𝑍𝑍 crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved. https://arxiv.org/pdf/2202.07628.pdf\n",
        "\n",
        "* In superconductors a destructive type of noise known as 𝑍𝑍 crosstalk. This refers to an always-on 𝜎𝑧 ⊗ 𝜎𝑧 inter- action between qubits connected by couplings, which originates from the interaction between the computational and non-computational energy levels of qubits.\n",
        "\n",
        "* different types of crosstalks:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1295.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1296.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1297.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1298.png)\n",
        "\n",
        "\n",
        "*Source: https://www.youtube.com/watch?v=si5a9RJP01A&t=3645s*\n",
        "\n",
        "* second graph: top red is perfect readout, and black light is for max 10% readout\n",
        "* We need very good readout and very small ZZ error\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1294.png)"
      ],
      "metadata": {
        "id": "y9wvhg9DyWQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Notes and Algorithms*"
      ],
      "metadata": {
        "id": "BCIAafwqXMVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *notes 2*"
      ],
      "metadata": {
        "id": "sQ4SU9ZTTVqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several interesting papers at the intersection of Generative AI and  Quantum computing / Physics:\n",
        "\n",
        "https://arxiv.org/pdf/2403.07059.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2308.12013.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2311.02041.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2401.07049.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2310.00585.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2310.05866.pdf\n"
      ],
      "metadata": {
        "id": "Op9rguqJ6-8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main figure of merit characterizing a state tomography protocol is its sample complexity defined as the number of copies of $\\rho$ consumed by the protocol in order to learn $\\rho$. Of particular interest are classes of n-qubit quantum states that can be learned efficiently, such that the sample complexity grows only polynomially with n. Known examples of efficiently learnable states include Matrix Product States describing weakly entangled quantum spin chains [17], output states of Clifford circuits [36], output states of Clifford circuits with a single layer of T gates [30], and high-temperature Gibbs states of local Hamiltonians [4, 24]. Apart from their potential use in experiments, efficiently learnable quantum states are of great importance for quantum algorithm design. For example, a quantum algorithm for solving the dihedral hidden subgroup problem [7] can be viewed as a tomography protocol for learning so-called hidden subgroup states (although this protocol is efficient in term of its sample complexity, its runtime is believed to be super-polynomial [7]). https://drops.dagstuhl.de/storage/00lipics/lipics-vol266-tqc2023/LIPIcs.TQC.2023.3/LIPIcs.TQC.2023.3.pdf\n",
        "https://drops.dagstuhl.de/storage/00lipics/lipics-vol266-tqc2023/LIPIcs.TQC.2023.3/LIPIcs.TQC.2023.3.pdf"
      ],
      "metadata": {
        "id": "cql64l7KdnG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "review notes / sources:\n",
        "\n",
        "- https://www.amazon.science/blog/preskill-wins-prize-for-work-on-learning-and-quantum-computing\n",
        "\n",
        "- https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040349\n",
        "\n",
        "- https://t3n.de/news/klassische-computer-quantencomputer-nachahmen-weniger-ressourcen-1616972/\n",
        "\n",
        "- https://arxiv.org/abs/2310.19882: Learning quantum states and unitaries of bounded gate complexity\n",
        "\n",
        "- \\href{https://iopscience.iop.org/article/10.1088/2632-2153/ad2f51}{opscience.iop.org/article/10.1088/2632-2153/ad2f51 }:  On the expressivity of embedding quantum kernels\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0022000005800431\n",
        "\n",
        "Variational quantum circuits (VQCs) have been widely used in near-term quantum algorithms for various tasks [48, 49], such as ground state preparation [50, 51], eigenenergy estimation [52, 53], quantum data compression [54, 55], quantum cir- cuit compiling [56, 57].\n",
        "\n",
        "\\href{https://quantum-journal.org/papers/q-2022-10-06-828/pdf/}{quantum-journal.org/papers/q-2022-10-06-828/pdf} ...\n",
        "\\href{https://www.researchgate.net/publication/350335643_Variational_Circuit_Compiler_for_Quantum_Error_Correction}{Variational Circuit Compiler for Quantum Error Correction}\n",
        "\n",
        "Several promising error mitigation strategies have recently emerged, including zero-noise extrapolation [2], quasi-probability decomposition [2], post-selection [3, 4], noise-aware compiling [5], and machine learning for circuit-depth compression [6]. Let us consider two other strategies for error mitigation in what follows.\n",
        "https://iopscience.iop.org/article/10.1088/1367-2630/ab784c\n",
        "\n",
        "https://arxiv.org/pdf/2312.07932.pdf\n",
        "\n",
        "examples from preskyll qubit talk: Useful tasks: Predict properties of chemical compounds and materials we haven't encountered in the lab before. Recognize when a qualitatively new phase of matter (in equilibrium or out of equilibrium) has been created. Preparing target quantum states using available experimental tools (Use reinforcement learning to more precisely prepare target quantum states). Extracting a usable signal from very noisy data (metrology for verz weak signals).\n",
        "\n",
        "(Beyond the focus on concrete quantum states, one can also analyse the electronic structure of quantum systems. The electronic structure refers to the overall arrangement of electrons within an atom or molecule. It describes how electrons are distributed among different energy levels and orbitals, and provides a broad picture of how electrons are organized and how they might interact in chemical bonds.)\n",
        "\n",
        "https://www.derstandard.de/story/3000000213781/erste-schritte-auf-dem-weg-zu-einer-quanten-ki\n",
        "\n",
        "* Beispielsweise könnten Quantencomputer bei der sogenannten Hyperparameteroptimierung helfen, die Teil von Machine-Learning-Ansätzen ist. Dabei geht es um eine Art \"Einstellung\" des KI-Modells, die das Training der neuronalen Netze vorbereitet und steuert. \"Diese Parametrisierung des KI-Systems ist besonders rechenaufwendig, und es gibt bereits Forschungen zur Frage, wie die Suche nach optimalen Werten von Quantenalgorithmen übernommen werden kann\", erklärt Boeddinghaus.\n",
        "\n",
        "* Ein anderes Beispiel betrifft die sogenannte Feature-Selection im maschinellen Lernen. Dabei geht es um bestimmte statistische Merkmale, die für das Training der neuronalen Netzwerke ausgewählt werden, um ein möglichst gutes Ergebnis zu erhalten. \"Bei einem System, das Wertpapiermärkte analysiert, stellt sich etwa die Frage, wie man Preisbewegungen zusammenfasst, um sie für das Training zu verwenden. Dazu muss man wissen, welchen Einfluss verschiedene Varianten haben\", veranschaulicht Boeddinghaus. \"Auch an der Verbesserung dieser Optimierungsfrage mittels Quantenalgorithmen wird gearbeitet.\"\n"
      ],
      "metadata": {
        "id": "Jgy7IksHan0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *notes 1*"
      ],
      "metadata": {
        "id": "O_fSoE3sg4UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit in: Cases that contain \\textbf{complex quantum states with entanglement} that exhibit exponential separations. Similarly, individual product states without entanglement are present, do not exhibit an exponential separation between classical and quantum learners.\n"
      ],
      "metadata": {
        "id": "kH1IKGpayKlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/s41567-024-02393-4"
      ],
      "metadata": {
        "id": "OAtQPnDGJTZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantinuum.com/news/theoretical-work-finds-shortcut-to-solving-the-max-cut-problem-with-a-quantum-computer\n",
        "\n",
        "https://quantum-journal.org/views/qv-2020-03-17-32/\n",
        "\n",
        "https://www.spektrum.de/news/neuer-siliziumchip-bricht-rekorde-und-verbessert-quantencomputer/2216020"
      ],
      "metadata": {
        "id": "HPczSxn8AD-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our new paper, freshly on arxiv:\n",
        "\"Quantum Algorithms for Inverse Participation Ratio Estimation in multi-qubit and multi-qudit systems\"\n",
        "\n",
        "A D-dimensional many-body quantum state can be decomposed as a linear superposition of orthonormal vectors forming a given Hilbert space.\n",
        "\n",
        "One way of characterizing a quantum state is by answering the following question: \"How much is a given state smeared out on the Hilbert space?\" The Inverse Participation Ratio is defined so that it takes the value IPR = 1 for a state occupying only one state of the Hilbert space and IPR = 1/D when it is an equal superposition of all Hilbert space vectors.\n",
        "\n",
        "IPR helps in characterizing many quantum phenomena, such as ergodicity breaking, or a fractal dimension of a quantum state.\n",
        "\n",
        "Calculating IPR for the many-body quantum systems is a computationally difficult task. It requires the decomposition of a state in the eigenbasis of the operator of our interest, and in particular, full diagonalization of a many-body quantum Hamiltonian is required.\n",
        "\n",
        "In our work, we propose three quantum algorithms that allow estimating the IPR of multi-qubit and multi-qudit systems via single-qubit measurements without any need for diagonalization.\n",
        "\n",
        "We presented three examples of our algorithms showing the full agreement with exact calculations; we also prepared an experiment on an IBM quantum machine showing qualitative agreement between NISQ devices and exact results.\n",
        "\n",
        "Enjoy!\n",
        "\n",
        "https://lnkd.in/dDZbbwuF\n",
        "\n",
        "—-\n",
        "\n",
        "Excited to share my latest research on cuTN-QSVM: cuTensorNet-accelerated Quantum Support Vector Machine with cuQuantum SDK, now available on arXiv!\n",
        "\n",
        "🔗 Read the paper: https://lnkd.in/exkUiYRc\n",
        "\n",
        "This work, a collaborative effort between NVIDIA (NVAITC), National  Applied Research Laboratories, NARLabs (National HPC center), and academic partners at Technical University of Munich and Imperial QuEST, presents a significant breakthrough in quantum machine learning and its benchmarking.\n",
        "\n",
        "Quantum Support Vector Machines (QSVMs) utilize a quantum-enhanced approach to tackle complex, multidimensional classification problems, surpassing the capabilities of classical SVMs under certain conditions. However, prior to the advent of large-scale fault-tolerance quantum systems, the scalability of simulating QSVMs on CPUs was traditionally limited by the exponential growth in computational demands as qubit counts increased. By employing NVIDIA's #cuQuantum SDK and the #cuTensorNet library, cuTN-QSVM effectively reduces this computational complexity from exponential to quadratic. This enables the simulation of large quantum systems of up to 784 qubits on the NVIDIA A100 GPU within seconds.\n",
        "\n",
        "👨‍💻 Explore our code and contribute:\n",
        "GitHub Repo: https://lnkd.in/e_6SsikA\n",
        "cuQuantum SDK: https://lnkd.in/eDqcM9j2\n",
        "cuTensorNet Library: https://lnkd.in/eaX6tg6h\n",
        "\n",
        "I would like to thank my co-author, Tai Yue Li, Chun-Chieh Wang, Nan-Yow Chen, An-Cheng Yang, Chun-Yu Lin (National HPC center and NSRRC), and  Yun-Yuan Wang, Simon See (NVIDIA NVAITC) for their significant contributions. Also, I would like to thank my host supervisor, Professor Robert Wille at the Technical University of Munich for his guidance.\n",
        "\n",
        "If you would like to know more about this project, please feel free to reach out to me!"
      ],
      "metadata": {
        "id": "t1aNXZMuG2vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [quantum generative modelling](https://youtu.be/YUK-YSj0bwo?si=xeJWMTE4w1CKzG4_)"
      ],
      "metadata": {
        "id": "t0amcEIIO3zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://research.google/blog/scaling-hierarchical-agglomerative-clustering-to-trillion-edge-graphs/"
      ],
      "metadata": {
        "id": "S_Gv-lU87JX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digital-analog quantum convolutional neural networks for image classification https://arxiv.org/abs/2405.00548"
      ],
      "metadata": {
        "id": "kMdYRBRs6eW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triply efficient shadow tomography: https://arxiv.org/pdf/2404.19211"
      ],
      "metadata": {
        "id": "G0NhriE-55BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/de/blogs/quantum-computing/jpmorgan-chase-and-aws-study-the-prospects-for-quantum-speedups-with-near-term-rydberg-atom-arrays/"
      ],
      "metadata": {
        "id": "7Feg92v56j8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\subsubsection{Introduction}\n",
        "\\label{sec:qml_cml_intro}\n",
        "\n",
        "\"I think a major limitation to this paper is the amount of training data used which is considerably small. Our findings show increasing the amount of training data (10k+) stabilizes quantum model performance, in the case of kernel methods. In fact, we see contrary evidence when scaling to very large qubit space (100+) that quantum models improve with more features...paper in progress. I think the issue with feature dimension is the exclusion of a the bandwidth parameter in the IQP circuit, only including it in the projected quantum kernel method. Though, it is true that there is something about the dataset that influences model performance and entanglement isn't the primary source of efficiency gains.\" - \"Classical Machine Learning vs Quantum Machine Learning. QML Out-Gunned by Classical Models. Tested on 12 Models. Researchers from Xanadu and Chalmers University of Technology have conducted a large-scale study testing 12 quantum machine learning models on six binary classification tasks. The study found that classical machine learning models generally outperformed quantum classifiers, contradicting claims made in several previous studies.\" https://quantumzeitgeist.com/classical-machine-learning-vs-quantum-machine-learning-qml-challenged/. Source: \\cite{arxiv:2403.07059}\n",
        "\n",
        "It has been suggested that future, fault-tolerant quantum algorithms could potentially contribute to most state-of-the-art, large-scale machine-learning problems, for example by incorporating quantum computing into the classical machine-learning process can potentially help make machine learning more sustainable and efficient \\cite{10.1038/s41467-023-43957-x}. In current classical machine learning, data are often uploaded, pruned (compressed to remove redundant information), then downloaded and re-uploaded. This is a costly process and a hybrid quantum-classical approach by incorporating quantum computing into the process after the initial prune could help cut the costs of that download and re-upload without sacrificing efficiency. It has been benchmarked to large machine learning models from 7 million to 103 million parameters as a basic pipeline for solving some gradient descent equations used in machine learning models. The results are equipped with guarantees.\n",
        "\n",
        "We propose a guiding principle for how to design the architecture of a quantum neural network in order to achieve a high learning efficiency - Quantum neural networks have also been applied for regression and classification tasks in the context of optimal information scrambling to achieve a high learning efficiency. \"In both cases, we find that, for the architecture with a larger averaged operator size, the loss function decreases faster or the prediction accuracy increases faster as the training epoch increases, which means higher learning efficiency\" \\cite{10.1103/PhysRevResearch.3.L032057}.\n"
      ],
      "metadata": {
        "id": "CidU84NN8x-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Review of Barren Plateaus in Variational Quantum Computing\n",
        "\n",
        "https://arxiv.org/abs/2405.00781\n",
        "\n",
        "—-\n",
        "\n",
        "What is the quantum state of a qubit? It can be a point in the Bloch sphere or in the simplex of its phase space representation. While both are intriguing, the phase space representation could offers simplicity and intuition.\n",
        "\n",
        "Take a look at these examples showcasing qubit states in the phase space. Here is the Mathematica notebook: https://lnkd.in/gsk4Jdki\n",
        "\n",
        "As we navigate uncharted territories in quantum for applications and computing, Wolfram remains dedicated. Our toolsets not only facilitate seamless application development but also delve into the foundational aspects of quantum. This unique feature sets Wolfram Quantum Framework apart from other platforms. Explore with us.\n",
        "\n",
        "—-\n",
        "\n",
        "Achieving high-quality solutions faster than classical solvers is a challenge for quantum optimization to deliver utility. Together with collaborators at Purdue University, NASA - National Aeronautics and Space Administration, and USRA we investigate the performance of a quantum algorithm for solving optimization problems using the Rigetti Ankaa™-2 quantum system. We achieve an average solution quality of 99% on problems with thousands of variables. We benchmark the quantum solver against similarly high-performing classical heuristics, including the commercial Gurobi optimizer, simulated annealing, and the Burer-Monteiro algorithm. A runtime analysis on a projected improved 100-qubit system shows that the quantum solver on large-scale problems is competitive against Gurobi but short of others. We explore multiple leads to close the gap and discuss prospects for a practical quantum speedup. This work was supported by the ONISQ program of the Defense Advanced Research Projects Agency (DARPA).  \n",
        "\n",
        "https://arxiv.org/abs/2404.17579\n",
        "\n",
        "—-\n",
        "\n",
        "Exciting news! Our latest preprint, \"Towards (2+1)-dimensional Quantum Electrodynamics running coupling with Monte Carlo and quantum computing methods,\" is now on arXiv! 💡 We combine Monte Carlo simulations and quantum computing to present a strategy for studying the running coupling, bridging perturbative and non-perturbative regimes. Read it here: https://lnkd.in/dfps4WcZ\n",
        "\n",
        "https://arxiv.org/abs/2404.17545\n",
        "\n",
        "—-\n",
        "\n",
        "The striking difficulty of continuous-variable quantum state tomography\n",
        "\n",
        "https://lnkd.in/dkcXeuhe\n",
        "\n",
        "https://scirate.com/arxiv/2405.01431\n",
        "\n",
        "Spoiler: Continuous variable #quantumstatetomography is much more challenging concerning its sample complexity than expected.\n",
        "\n",
        "Quantum state tomography with rigorous guarantees with respect to the trace distance, the most operationally meaningful metric for distinguishing quantum states, has been studied extensively for finite-dimensional systems; however, it remains almost unexplored for #continuousvariablesystems.\n",
        "\n",
        "This work fills this gap. We prove that learning energy-constrained n-mode states without any additional prior assumption is extremely inefficient: The minimum number of copies needed for achieving an ε-approximation in trace distance scales as ∼ε−2n, in stark contrast to the n-qudit case, where the ε-scaling is ∼ε−2.\n",
        "\n",
        "Specifically, we find the optimal sample complexity of tomography of energy-constrained pure states, thereby establishing the ultimate achievable performance of tomography of continuous variable systems.\n",
        "\n",
        "Given such an extreme inefficiency, we then investigate whether more structured, yet still physically interesting, classes of quantum states can be efficiently tomographed.\n",
        "\n",
        "We rigorously prove that this is indeed the case for Gaussian states, a result previously assumed but never proved in the literature. To accomplish this, we establish bounds on the trace distance between two Gaussian states in terms of the norm distance of their first and second moments, which constitute technical tools of independent interest.\n",
        "\n",
        "This allows us to answer a fundamental question for the field of Gaussian quantum information: by estimating the first and second moments of an unknown Gaussian state with precision ε, what is the resulting trace distance error on the state?\n",
        "\n",
        "Lastly, we show how to efficiently learn t-#dopedGaussianstates, i.e., states prepared by Gaussian unitaries and at most t local non-Gaussian evolutions, unveiling more of the structure of these slightly-perturbed Gaussian systems.\n",
        "\n",
        "—-\n",
        "\n",
        "Near-Term Quantum Computers: Fault Tolerance + Benchmarking + Quantum Advantage + Quantum Algorithms\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Quantum computers promise superior computational power over classical computers for some structured problems. While this insight is not new, only in recent years, steps have been taken to actually build intermediate-sized quantum devices, creating an exciting state of affairs. For some paradigmatic problems, there is some evidence that quantum computers may outperform classical devices [1]. For practically motivated problems in machine learning [2, 3] and in optimization [4], fault tolerant quantum computers may indeed perform better than classical ones. While this is promising, actual systems to date are relatively small and noisy. The main part of the talk is actually concerned with identifying limitations to near-term quantum computing. We discuss notions of learnability of output distributions of short quantum circuits - as they can be seen as parts of variational quantum algorithms - and find that a single T-gate renders learning them hard [5]. We identify exponentially tighter bounds on limitations of quantum error mitigation [6]. We finally - and with the greatest emphasis - discuss the impact of non-unital noise on quantum computing, with quite unexpected results [7]. We end on the note that while fault tolerant quantum computers offer substantial computational benefits, the race is still open for near-term quantum devices.\n",
        "[1] Computational advantage of quantum random sampling, D. Hangleiter, J. Eisert, Rev. Mod. Phys. 95, 035001 (2023).\n",
        "[2] Towards provably efficient quantum algorithms for large-scale machine-learning models, J. Liu, M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert, L. Jiang, Nature Comm. 15, 434 (2024).\n",
        "[3] A super-polynomial quantum-classical separation for density modelling, N. Pirnay, R. Sweke, J. Eisert, J.-P. Seifert, Phys. Rev. A 107, 042416 (2023).\n",
        "[4] An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory, N. Pirnay, V. Ulitzsch, F. Wilde, J. Eisert, J.-P. Seifert, arXiv:2212.08678, Science Adv. (2024).\n",
        "[5] A single T-gate makes distribution learning hard, M. Hinsche, M. Ioannou, A. Nietner, J. Haferkamp, Y. Quek, D. Hangleiter, J.-P. Seifert, J. Eisert, R. Sweke, Phys. Rev. Lett. 130, 240602 (2023).\n",
        "[6] Exponentially tighter bounds on limitations of quantum error mitigation, Y. Quek, D. Stilck França, S. Khatri, J. Jakob Meyer, J. Eisert, arXiv:2210.11505, Nature Phys., in press (2024).\n",
        "[7] Noise-induced shallow circuits and absence of barren plateaus, A. A. Mele, A. Angrisani, S. Ghosh, A. Khatri, J. Eisert, Y. Quek, D. Stilck Franca, arXiv:2403.13927 (2024).\n",
        "\n",
        "—-\n",
        "\n",
        "(b) Circuit for an arbitrary $2 \\times 2$ gray value image using the MARY-implementation. Pixel values are converted into angles $\\theta_i$, for $i \\in\\{0,1,2,3\\}$. Hadamard gates are used for superposition, X-gates for rotation to the desired state. There is one MARY-gate for each pixel value.\n",
        "\n",
        "https://www.nature.com/articles/d41586-021-02102-8\n"
      ],
      "metadata": {
        "id": "bmS3UxXTvBQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *notes 3*"
      ],
      "metadata": {
        "id": "vyaK3FPEe2vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cramer rao bound amd Cauchy–Schwarz inequality, Frobeniusnorm:\n",
        "variance ideal for quantum Hilbert space?\n",
        "\n",
        "How much is a loss function concentrated around its mean (not partial derivative concentration)?\n",
        "\n",
        "\n",
        "**Currently: how to study Parametrized Quantum Circuits (PQCs)?**\n",
        "\n",
        "* Expressivity / Capacity\n",
        "* Trainability\n",
        "  * subset of computational complexity and circuit complexity?\n",
        "* Generalization > which model or algorithm to choose best?\n",
        "* Sample complexity\n",
        "\n",
        "**Generalization in Machine Learning:**\n",
        "\n",
        "* Generalization refers to the ability of an algorithm to **perform accurately on new, unseen data after being trained on a sample dataset**. The goal is to learn patterns that are not just specific to the training data but also applicable to new data.\n",
        "\n",
        "* Uniform Generalization Bounds: These bounds provide a measure of how well the learned model will generalize across all possible data distributions, not just the one it was trained on. They are called 'uniform' because they provide a guarantee that holds uniformly over a class of functions or models.\n",
        "\n",
        "* Role of Sample Size: One key aspect of these bounds is that they typically improve with an increase in the size of the training sample. As more data is used to train the model, we can be more confident that the model will generalize well.\n",
        "\n",
        "* Complexity of the Hypothesis Class: **The bounds are also influenced by the complexity of the hypothesis class** (the set of models being considered). More complex classes, such as those with a large number of parameters, typically have **looser bounds, meaning they need more data to ensure good generalization**.\n",
        "\n",
        "* Mathematical Formulation: **The bounds are often derived using tools from probability and statistics, such as concentration inequalities (like Hoeffding’s inequality or McDiarmid's inequality)**. These tools help quantify the likelihood that the observed performance of a model on a sample is close to its true performance on the entire population.\n",
        "\n",
        "* Practical Implication: In practical terms, **uniform generalization bounds help in understanding and choosing models and algorithms**. They provide a theoretical foundation for preferring certain models over others based on their generalization capabilities, given a finite amount of data."
      ],
      "metadata": {
        "id": "lWl0TKgLrsJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Besides model complexity, sample complexity and computational complexity, are there more dimension that need to be considered on computational learning theory?**\n",
        "\n",
        "Yes, besides model complexity, sample complexity, and computational complexity, there are several other dimensions and factors that are important in computational learning theory:\n",
        "\n",
        "1. **Algorithmic Stability**:\n",
        "    - This refers to how sensitive a learning algorithm is to small changes in the training set. Algorithms that are more stable are generally better at generalizing, as they don't overfit to the noise or specific details in the training data.\n",
        "\n",
        "2. **Noise Tolerance**:\n",
        "    - The ability of a learning algorithm to perform well in the presence of noise (errors or randomness in the training data) is crucial. This includes dealing with mislabeled data or data with measurement errors.\n",
        "\n",
        "3. **Robustness**:\n",
        "    - Robustness is the ability of the model to maintain performance when faced with input data that differs from the training data. This includes handling adversarial examples, where small, intentional changes to input data can mislead the model.\n",
        "\n",
        "4. **Learning Rate and Convergence**:\n",
        "    - This pertains to how quickly a learning algorithm converges to a good solution. Fast convergence is desirable for efficiency, but it's also important that the convergence leads to a solution that generalizes well.\n",
        "\n",
        "5. **Bias-Variance Tradeoff**:\n",
        "    - A fundamental concept in statistical learning, the bias-variance tradeoff involves balancing underfitting (bias) and overfitting (variance). It's about finding the right level of model complexity that neither oversimplifies (high bias) nor overcomplicates (high variance) the problem.\n",
        "\n",
        "6. **Data Distribution**:\n",
        "    - The nature of the data distribution, including how representative the training data is of the real-world scenarios the model will encounter, plays a crucial role in the effectiveness of learning.\n",
        "\n",
        "7. **Feature Representation**:\n",
        "    - The way data is represented or pre-processed can significantly impact the performance of a learning algorithm. Good feature representation can ease the learning process and improve generalization.\n",
        "\n",
        "8. **Scalability**:\n",
        "    - Scalability refers to how well a learning algorithm or model can handle increasing amounts of data or more complex problems. This is particularly important in big data applications.\n",
        "\n",
        "9. **Ethical and Social Considerations**:\n",
        "    - This includes considerations of fairness, privacy, and transparency. It's essential to ensure that learning algorithms do not perpetuate or exacerbate biases and are respectful of privacy and ethical norms.\n",
        "\n",
        "10. **Transferability and Adaptability**:\n",
        "    - This refers to the ability of a model to adapt to new tasks or transfer learned knowledge to different but related problems. It's a key aspect of research in areas like transfer learning and domain adaptation.\n",
        "\n",
        "11. **Interpretability and Explainability**:\n",
        "    - The ability to understand and interpret the decisions made by a model is increasingly important, especially for applications in sensitive areas like healthcare or finance.\n",
        "\n",
        "Each of these dimensions adds complexity to the design and evaluation of learning algorithms, and they are often interrelated. Balancing these factors is a key challenge in developing effective and responsible machine learning systems."
      ],
      "metadata": {
        "id": "3dgYVzt0a8lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are standard scientific methods to measure model complexity, sample complexity and computational complexity?**\n",
        "\n",
        "Measuring model complexity, sample complexity, and computational complexity in machine learning involves a blend of theoretical analysis and empirical evaluation. Each type of complexity has standard scientific methods associated with it:\n",
        "\n",
        "*1. Model Complexity*\n",
        "Model complexity refers to the ability of a model to fit a wide range of functions. It's often associated with the flexibility of the model to adapt to the underlying data structure.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **VC Dimension**: For classification models, the Vapnik-Chervonenkis (VC) dimension is a theoretical measure of model capacity. It's the size of the largest set of points that the model can shatter.\n",
        "- **Number of Parameters**: In many models, such as neural networks, the number of trainable parameters is a direct measure of complexity.\n",
        "- **Model Depth and Architecture**: For neural networks, depth (number of layers) and architecture details (like types of layers and connections) indicate complexity.\n",
        "- **Information Criteria**: Metrics like AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used in statistical modeling to balance model fit and complexity.\n",
        "\n",
        "- Number of Parameters: One of the most common measures of model complexity is the number of parameters. This is a simple and intuitive measure, as it directly reflects the number of degrees of freedom in the model. However, it is not always the most informative measure, as it does not take into account the structure of the model or the relationships between its parameters.\n",
        "\n",
        "- Effective Number of Parameters: A more sophisticated measure of model complexity is the effective number of parameters. This measure takes into account the fact that some parameters may be more important than others, and it can be used to compare models with different numbers of parameters.\n",
        "\n",
        "- VC Dimension: The VC dimension is a theoretical measure of model complexity that is based on the concept of shattering. A shattered set of points is a set of points that can be perfectly classified by a hypothesis class in all possible ways. The VC dimension of a hypothesis class is the maximum size of a shattered set. A higher VC dimension indicates a more complex hypothesis class.\n",
        "\n",
        "- Kolmogorov Complexity: Kolmogorov complexity is a measure of the information content of an object. It is based on the idea that the complexity of an object is the shortest possible program that can generate it. Kolmogorov complexity can be used to measure the complexity of models, as well as the complexity of data.\n",
        "\n",
        "*2. Sample Complexity*\n",
        "Sample complexity relates to the amount of data required for a model to achieve a certain level of performance.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Empirical Evaluation**: Experimentally determining how model performance varies with different training set sizes. This often involves plotting learning curves.\n",
        "- **PAC Learning Framework**: Probably Approximately Correct (PAC) learning provides theoretical bounds on the number of samples needed for a model to learn a concept within a certain error margin and confidence level.\n",
        "- **Cross-validation**: Techniques like k-fold cross-validation can help assess how well a model with certain complexity performs with the available data.\n",
        "\n",
        "- Learning Curves: Learning curves are a simple and effective way to measure sample complexity. A learning curve is a plot of the training and generalization error of a model as a function of the number of training examples. A steeper learning curve indicates that the model is more sensitive to the number of training examples and therefore has higher sample complexity.\n",
        "\n",
        "- Error Bounds: Error bounds are a more theoretical way to measure sample complexity. Error bounds provide an upper bound on the generalization error of a model with a given level of confidence. A smaller error bound indicates lower sample complexity.\n",
        "\n",
        "- Rademacher Complexity: Rademacher complexity is a measure of the generalization error of a model that is based on the concept of Rademacher averages. Rademacher averages are a type of random average that is used to control for the randomness of the training data. A smaller Rademacher complexity indicates lower sample complexity.\n",
        "\n",
        "*3. Computational Complexity*\n",
        "Computational complexity is about the resources (time, memory) required to train or use a model.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Big O Notation**: Theoretical analysis of algorithms in terms of time and space complexity (e.g., O(n), O(n^2)) is standard.\n",
        "- **Time Profiling**: Empirically measuring the actual time taken to train models on datasets of varying sizes.\n",
        "- **Space Profiling**: Measuring the memory requirements of a model during training and inference.\n",
        "- **Algorithm Analysis**: Analyzing the specific steps of an algorithm to determine their computational requirements.\n",
        "\n",
        "- Time Complexity: Time complexity is a measure of the amount of time that an algorithm takes to run as a function of the input size. Time complexity is typically expressed using Big O notation, which provides an upper bound on the worst-case performance of the algorithm.\n",
        "\n",
        "- Space Complexity: Space complexity is a measure of the amount of memory that an algorithm requires to run as a function of the input size. Space complexity is also typically expressed using Big O notation.\n",
        "\n",
        "- Scalability: Scalability is a measure of how well an algorithm can handle increasing input sizes. An algorithm is said to be scalable if its time and space complexity grow sublinearly with the input size.\n",
        "\n",
        "*Integrating Theoretical and Empirical Approaches*\n",
        "In practice, a combination of these methods is often used. Theoretical measures provide a guideline or a framework, while empirical methods offer practical insights based on real-world data and computing environments. The choice of method depends on the specific model, the nature of the task, and the available resources. For example, in deep learning, empirical evaluation is heavily relied upon due to the complexity and non-linearity of the models, while in more traditional machine learning or statistical models, theoretical measures might be more prominently used."
      ],
      "metadata": {
        "id": "7FbmpALiSvmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *notes 4*"
      ],
      "metadata": {
        "id": "Gz0oSYxOTkLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Locally flat distributions*"
      ],
      "metadata": {
        "id": "DyGjb0ZUwy_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/s41598-023-43404-3\n",
        "\n",
        "we use a robust local minima problem to compare state-of-the-art local optimizers (SLSQP, COBYLA, L-BFGS-B and SPSA) against DE using the Variational Quantum Eigensolver algorithm."
      ],
      "metadata": {
        "id": "Wl6tjloxxBNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.quantum.ibm.com/api/qiskit/qiskit.algorithms.optimizers.SLSQP"
      ],
      "metadata": {
        "id": "Mv_PoowCww0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is so interesting aboiut Locally flat distributions and Paulis on quantum computers?**\n",
        "\n",
        "Here's what's interesting about locally flat distributions and Paulis (Pauli operators) in the context of quantum computers, and why they matter:\n",
        "\n",
        "**Locally Flat Distributions**\n",
        "\n",
        "* **Noise Representation:** Locally flat distributions are used to model a particular type of noise in quantum circuits. This noise has the property that small errors become much more likely than large errors. Locally flat distributions capture this behavior.\n",
        "* **Error Correction and Threshold Theorems:** Understanding the properties of locally flat distributions plays a role in the development of quantum error correction codes.  Importantly, they are connected to the idea of fault-tolerance thresholds – limits on error rates below which quantum computation can be made reliable.\n",
        "* **Simulation and Analysis:** Studying locally flat distributions helps researchers design and analyze quantum algorithms that are robust to this specific type of noise.\n",
        "\n",
        "**Paulis (Pauli Operators)**\n",
        "\n",
        "* **Fundamental Building Blocks:** The Pauli operators (X, Y, Z) together with the identity operator (I) form a basis for the space of operators on a single qubit.  This means any operation on a single qubit can be expressed as a combination of Paulis.\n",
        "* **Error Representation:** Many common quantum errors can be represented as combinations of Pauli operators acting on qubits. This makes them convenient for describing noise processes\n",
        "* **Qubit Control:**  Pauli operators are used as quantum gates to manipulate the state of qubits, forming the essential toolkit for quantum computation.\n",
        "\n",
        "**Where They Intersect**\n",
        "\n",
        "* **Describing Noise with Paulis:** Locally flat noise distributions can often be well-described in terms of probabilistic combinations of Pauli errors.  This connection helps analyze how such noise affects quantum circuits.\n",
        "* **Error Correction Tailored to Paulis:** Quantum error-correcting codes can be specifically designed to combat Pauli errors, which are a common type of noise. Understanding the relationship between Paulis and noise distributions helps in creating more effective codes.\n",
        "* **Efficient Simulation:** Certain simulation techniques, such as stabilizer circuit simulations, heavily exploit the structure of Pauli operators to efficiently simulate specific classes of quantum circuits.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Locally flat distributions and Paulis provide a framework to model, analyze, and mitigate the effects of noise in quantum computers. Understanding their connection enables the design of more resilient quantum algorithms and the development of robust error-correction schemes that are crucial for the advancement of practical quantum computing.\n",
        "\n",
        "**Let me know if you'd like to dive deeper into any of these aspects, such as quantum error correction codes, noise modeling, or stabilizer circuits!**\n"
      ],
      "metadata": {
        "id": "yB16-W-bwYg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is low degree truncation in quantum computing?**\n",
        "\n",
        "Here's an explanation of low-degree truncation in quantum computing, its uses, and its significance:\n",
        "\n",
        "**What is Low-Degree Truncation?**\n",
        "\n",
        "Low-degree truncation is a technique used in quantum computing to simplify calculations involving complex quantum systems. It involves approximating a system's Hamiltonian (the operator that describes its energy) by keeping only the terms with low-degree polynomials.\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "1. **Hamiltonian Representation:**  The Hamiltonian of a quantum system can often be expressed as a sum of terms involving products of Pauli operators (X, Y, Z).  Each term has a degree determined by the number of Pauli operators in the product (e.g., the term XY has degree 2).\n",
        "\n",
        "2. **Truncation:** In low-degree truncation, you keep only the terms in the Hamiltonian up to a certain degree and discard the higher-degree terms. This simplifies the representation and makes calculations more manageable.\n",
        "\n",
        "**Why Use Low-Degree Truncation**\n",
        "\n",
        "* **Computational Complexity:** Simulating complex quantum systems on classical computers can be extremely computationally expensive. The number of terms in the Hamiltonian grows exponentially with the size of the system. Low-degree truncation reduces this complexity.\n",
        "* **Focus on Essential Interactions:** In many systems, the essential physics is often captured by the low-order interactions. Truncating higher-order terms may introduce some error but often provides a good approximation while significantly simplifying the calculations.\n",
        "* **Resource Limitations:** Quantum computers themselves have a limited number of qubits and limited gate depth they can apply. Low-degree truncation can make problems more amenable to simulation on near-term quantum devices.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* **Quantum Chemistry Simulations:** Low-degree truncation is used in simulating the electronic structure of molecules. This helps understand chemical reactions and design new materials.\n",
        "* **Material Science:**  It's used in simulations of materials to predict their properties, aiding in the discovery of new materials with tailored characteristics.\n",
        "* **Quantum Optimization:**  Some quantum optimization algorithms are amenable to low-degree truncation, making it possible to approximate solutions to complex optimization problems.\n",
        "\n",
        "**Significance**\n",
        "\n",
        "Low-degree truncation is a powerful tool for making quantum simulations more tractable, especially when dealing with large or complex systems. However, there's a trade-off:\n",
        "\n",
        "* **Accuracy vs. Efficiency:**  Truncation introduces errors, so it's important to balance accuracy requirements with computational efficiency when choosing the degree of truncation.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Consider a Hamiltonian with terms like X, Y, Z, XY, XZ, YZ, XYZ.  A low-degree truncation at degree 2 would only keep the terms X, Y, Z, XY, XZ, and YZ, and discard the XYZ term.\n",
        "\n",
        "**Let me know if you would like to explore any specific application of low-degree truncation in more detail!**\n"
      ],
      "metadata": {
        "id": "FrqzVdYqwaWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the Depolarizing channel in each qubit?**\n",
        "\n",
        "Here's a breakdown of the depolarizing channel in quantum computing, including how it works and its significance:\n",
        "\n",
        "**What is the Depolarizing Channel?**\n",
        "\n",
        "* **Definition:** The depolarizing channel is a type of quantum channel (a mathematical model of how quantum states evolve) that introduces noise into a qubit (the basic unit of quantum information).  It does this by effectively shrinking the Bloch sphere representation of the qubit's state.\n",
        "* **Behavior:** With a probability `p`, the depolarizing channel replaces the input qubit with the completely mixed state (a state of maximum uncertainty). With the remaining probability `(1-p)`, the qubit is left unchanged.\n",
        "* **Effect:** The depolarizing channel introduces randomness, making it harder to reliably store and process quantum information.\n",
        "\n",
        "**Mathematical Representation**\n",
        "\n",
        "The depolarizing channel can be represented mathematically using the Kraus operator-sum representation:\n",
        "\n",
        "```\n",
        "ℰ(ρ) = (1-p) ρ + p/3 (XρX + YρY + ZρZ)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `ρ` is the input density matrix (representing the qubit's state).\n",
        "* `p` is the depolarization probability.\n",
        "* `X`, `Y`, and `Z` are the Pauli matrices (basic quantum gates).\n",
        "\n",
        "**Geometric Interpretation**\n",
        "\n",
        "The depolarizing channel can be visualized as shrinking the Bloch sphere (a geometric representation of a qubit's state):\n",
        "\n",
        "* The completely mixed state is represented by the center of the Bloch sphere.\n",
        "* The depolarizing channel moves the qubit's state closer to the center, shrinking its representation on the Bloch sphere.\n",
        "* The stronger the depolarization (higher `p`), the more the Bloch sphere shrinks.\n",
        "\n",
        "**Significance**\n",
        "\n",
        "* **Modeling Noise:** The depolarizing channel is a useful model for representing realistic noise in quantum systems.  Real-world qubits interact with their environment, leading to decoherence that is often well-approximated by a depolarizing channel.\n",
        "* **Error Correction:** Understanding the depolarizing channel is crucial for designing quantum error correction codes. These codes help protect quantum information from the effects of noise.\n",
        "* **Capacity Limits:** The depolarizing channel affects the capacity of a quantum communication channel, which limits the amount of information that can be reliably transmitted.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose you have a qubit in the pure state |0⟩. If it passes through a depolarizing channel with probability `p = 0.25`, the output state would be:\n",
        "\n",
        "```\n",
        "(1 - 0.25) |0⟩⟨0| + 0.25/3 (I/2) = 0.75 |0⟩⟨0| + 0.083 I\n",
        "```\n",
        "\n",
        "This is a mixed state, indicating a loss of quantum information.\n",
        "\n",
        "**Let me know if you would like a deeper dive into any of these aspects or have a specific application in mind!**\n"
      ],
      "metadata": {
        "id": "3J6s2aTawcbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Quantum hadamrd test](https://youtu.be/F98jpHBQPys?si=_pH2ozWfJjwL7Ap5)\n",
        "\n",
        "[Algebra of pauli matrices](https://youtu.be/Gj9iezP89Dk?si=w_6P8vH3ONLq1a8L)\n",
        "\n"
      ],
      "metadata": {
        "id": "4zPmw5k95FR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2024-02-magnesium-tantalum-material-qubits.amp"
      ],
      "metadata": {
        "id": "CeWSjnS5DjD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www-insidequantumtechnology-com.cdn.ampproject.org/c/s/www.insidequantumtechnology.com/news-archive/news-from-infleqtion-inertial-sensors-atomic-clocks-rf-receivers-oh-and-1600-qubits-by-brian-siegelwax/amp/"
      ],
      "metadata": {
        "id": "ObzMgs60-XB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://thequantuminsider.com/2024/02/07/quantum-matters-quantum-ai-early-days-for-a-killer-combination/\n",
        "\n",
        "https://arxiv.org/abs/2402.03871: Geometric quantum machine learning of BQP^A protocols and latent graph classifiers\n",
        "\n",
        "https://www.spektrum.de/news/revolutioniert-die-fusion-von-ki-und-quantenphysik-die-wissenschaft/2203074"
      ],
      "metadata": {
        "id": "UN-M-61Vhj1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Daniel Sank: The superconducting transmon qubit as a microwave resonator](https://www.youtube.com/watch?v=dKTNBN99xLw&list=WL&index=7&t=2329s)\n",
        "\n",
        "[Quantum Industry Talks: Quantum computing hardware](https://www.youtube.com/watch?v=eyICn3KCUPI&list=WL&index=6&t=2000s)"
      ],
      "metadata": {
        "id": "nKydZaFIbSqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Course 3: How to build a quantum network/ Hardware perspective](https://youtu.be/iGdbROJfmiw?si=v7Bi6FIfY01Dobrd)\n",
        "\n",
        "[Course 4: How to build a quantum network/ Theory perspective](https://youtu.be/pZJVU_60Gd8?si=LLIgpDalf4eLx_ps)"
      ],
      "metadata": {
        "id": "TYad4CPW-EfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hamiltonian simulation and Trotterization: https://vtomole.com/blog/2019/04/07/trotter"
      ],
      "metadata": {
        "id": "1RO_b38EmEgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.golem.de/news/militaer-erfolgreiche-demonstration-von-quanten-funkkommunikation-2401-181120.amp.html"
      ],
      "metadata": {
        "id": "q8fwKeBZlQ8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pending:**\n",
        "\n",
        "* [Robert Huang: Fundamental aspects of solving quantum problems with machine learning\n",
        "](https://www.youtube.com/watch?v=VazUK13iwpQ&list=WL&index=5&t=936s)\n",
        "\n",
        "* [Vedran Dunjko - Building the case for Quantum Machine Learning](https://www.youtube.com/watch?v=td3jfKxT5TI&list=WL&index=7&t=2795s)\n",
        "\n",
        "* [Vedran Dunjko - Exponential separations between classical and quantum learners - IPAM at UCLA](https://www.youtube.com/watch?v=IlEixl4mq0o&list=WL&index=5&t=163s)\n",
        "\n",
        "* [Markus Müller - Topological Quantum Error Correction: From Theoretical Concepts to Experiments](https://www.youtube.com/watch?v=tbrTOemjxow&t=11s)\n",
        "\n",
        "* [Xanadu PennyLane Quantum Computing Training - 2023 NUG Annual Meeting](https://www.youtube.com/watch?v=gJUBBZIq8zo&list=WL&index=4&t=4450s)\n",
        "\n",
        "*Applied Reviews*\n",
        "\n",
        "* [A Look Back at 2023 and Quantum Predictions for 2024](https://www.youtube.com/watch?v=ORhwzlDHdy8&list=PLBn8lN0DcvpmcGQ1H_9YMFPew1ZsP_8Sj&index=3&t=2742s)"
      ],
      "metadata": {
        "id": "UvrswVv2QT7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NISQ and LISQ**\n",
        "\n",
        "https://dabacon.org/pontiff/2024/01/03/acronyms-beyond-nisq/\n",
        "\n",
        "https://www.linkedin.com/pulse/bye-nisq-hello-lisq-simone-severini-ybkmc"
      ],
      "metadata": {
        "id": "OzwFAIepZQMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Encoding and QSVT**\n",
        "\n",
        "* [Pennylane: Intro to QSVT](https://pennylane.ai/qml/demos/tutorial_intro_qsvt/#transforming-matrices-encoded-in-matrices)\n",
        "\n"
      ],
      "metadata": {
        "id": "f-mVvct2q9g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimental advantage in learning with noisy Quantum memory - Quantum Summer Symposium 2021**\n",
        "\n",
        "* Video: [Experimental advantage in learning with noisy Quantum memory - Quantum Summer Symposium 2021](https://www.youtube.com/watch?v=GEgJkqQNwvQ&list=WL&index=1)\n",
        "\n",
        "*details used for QML book"
      ],
      "metadata": {
        "id": "OgCGvQeGkFtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x| x\\right\\rangle$\n",
        "\n",
        "The given expression $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x|x\\right\\rangle$ represents the **squared norm of the quantum state |x⟩**. The squared norm of a quantum state is a measure of its overall size or amplitude. It is calculated by taking the inner product of the state with itself.\n",
        "\n",
        "Let's break down the expression:\n",
        "\n",
        "* **⟨0|:** This denotes the inner product of the vacuum state |0⟩ with itself. The vacuum state is the state with no particles, and it has unit norm.\n",
        "\n",
        "* **U_0^{\\dagger}(x):** This is the conjugate transpose of the identity operator I0, which is a unitary operator that acts as the identity on the vacuum state. The conjugate transpose is a common operation in quantum mechanics, and it relates the action of an operator on a quantum state to its action on the dual state.\n",
        "\n",
        "* **U_0(x):** This is the identity operator I0. The identity operator is a unitary operator that does not change the state of a quantum system. It is the simplest and most fundamental unitary operator.\n",
        "\n",
        "* **|0⟩:** This is the vacuum state |0⟩. The vacuum state is the state with no particles, and it has unit norm.\n",
        "\n",
        "* **⟨x|x⟩:** This denotes the inner product of the state |x⟩ with itself. The inner product of two quantum states is a measure of their overlap. It is calculated by taking the dot product of their corresponding complex vectors.\n",
        "\n",
        "Putting everything together, the expression $f(x)=\\left\\langle 0\\left|U_0^{\\dagger}(x)   U_0(x)\\right| 0\\right\\rangle=\\left\\langle x|x\\right\\rangle$ represents the squared norm of the quantum state |x⟩. It is a scalar quantity that indicates the overall size or amplitude of the state.\n"
      ],
      "metadata": {
        "id": "Fkcx5erfdR5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 1**\n",
        "\n",
        "* Video: [Parameter--Shift Rule Derivation — Part 1 | PennyLane Tutorial](https://www.youtube.com/watch?v=crgmwXBogx4&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=5)\n",
        "* https://pennylane.ai/qml/glossary/parameter_shift/#gaussian-gate-example-\n",
        "* Also see: [Automatic Differentiation of Quantum Circuits](https://www.youtube.com/watch?v=McgBeSVIGus&t=1s)\n",
        "* Also see: [Parameter Shift Rule for calculation of Gradients in Quantum Variational Circuits](https://www.youtube.com/watch?v=yJriVOy5l_8&list=WL&index=1&t=6s)\n",
        "* Represent certain expectation values with Fourier series, important to derive some parameter shift rules in order to compute quantum gradients\n",
        "* Fourier representation is also useful to expressivity of certain quantum models\n",
        "* **We are computing the expectation value <font color=\"green\">$E(x)$</font> of operator <font color=\"red\">$B$</font> such that we have a quantum state $\\psi$ which is evolved by some unitary <font color=\"blue\">$U(x)$</font>**\n",
        "\n",
        "> <font color=\"green\">$E(x)$</font> $=\\langle\\psi|$ <font color=\"blue\">$U^{\\dagger}(x)$</font> <font color=\"red\">$B$</font> <font color=\"blue\">$U(x)$</font> $| \\psi\\rangle$\n",
        "\n",
        "This kind of expression is common in quantum computing and quantum mechanics, **where it's used to calculate the expected outcomes of various quantum operations**, which are critical for understanding how quantum algorithms will behave.\n",
        "\n",
        "**How to solve this? $E(x) =\\langle\\psi| U^{\\dagger}(x) B U(x) | \\psi\\rangle$**\n",
        "\n",
        "* Two main approaches to calculating the eigenvalues of a Hermitian operator B:\n",
        "  1. **Diagonalization:** This method involves finding an orthonormal basis of eigenvectors of B. The eigenvalues of B are then the corresponding eigenvalues of the Hermitian matrix representing B in this basis.\n",
        "  2. **Eigenvalue Solvers:** There are various numerical methods for calculating eigenvalues of Hermitian operators, such as the Rayleigh-Ritz quotient, the Lanczos algorithm, and the Arnoldi algorithm. These methods can be used to calculate eigenvalues for large Hermitian matrices that cannot be diagonalized directly.\n",
        "* In the case of $E(x) =\\langle\\psi| U^{\\dagger}(x) B U(x) | \\psi\\rangle$, the observable B is sandwiched between the unitary operator $U(x)$ and its conjugate transpose $U^{\\dagger}(x)$.\n",
        "* This means that $B$ can be expressed as a combination of powers of $U(x)$ and $U^{\\dagger}(x)$. **If we can find the eigenvalues of this combination of operators $U(x)$ and $U^{\\dagger}(x)$, then we can calculate the eigenvalues of $E(x)$**.\n",
        "  * One way to achieve this is to use a diagonalization method - accurate and but computational expensive. This involves finding a suitable basis for the Hilbert space of quantum states, and representing U(x) and U^{\\dagger}(x) in this basis. The eigenvalues of B can then be calculated by diagonalizing the matrix representing B in this basis.\n",
        "  * Another approach is to use an eigenvalue solver - efficient, but not super accurate. This involves choosing an appropriate numerical method, and applying it to the matrix representing B in the basis defined by U(x) and U^{\\dagger}(x). The eigenvalues of B can then be calculated by using the numerical method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_89dYYPMwz0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 2**\n",
        "\n",
        "(same video, continuation, [Parameter--Shift Rule Derivation — Part 1 | PennyLane Tutorial](https://www.youtube.com/watch?v=crgmwXBogx4&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=5)\n",
        ")\n",
        "\n",
        "* <font color=\"blue\">$U(x) = e^{ixG}$</font>\n",
        "* <font color=\"orange\">$G$</font> = hermitian operator <font color=\"orange\">$G^{\\dagger}$ = $G$</font> is called the **generator** of the operator $U$ (e.g. the generator of a Y rotation gate is the Pauli Y gate with a factor of $-\\frac{1}{2}$\n",
        "\n",
        "Eigenvalues and Eigenstates of <font color=\"orange\">$G$</font>, denoted by some $\\omega_j$. We create set out of these eigenvalues, ordered in non-descending order. j is from e, which is defined as set of positive integers from 1 to d\n",
        "\n",
        "> Eigenvalue: <font color=\"orange\">$\\left\\{\\omega_j\\right\\}_{j \\in[d]}$ $\\quad$ with $[d]:=\\{1, \\cdots, d\\}$\n",
        "\n",
        "> Eigenequation: <font color=\"orange\">$G\\left|\\varphi_j\\right\\rangle=\\omega_j\\left|\\varphi_j\\right\\rangle$\n",
        "\n",
        "Eigenvalues and Eigenstates of <font color=\"blue\">$U(x)$</font>:\n",
        "\n",
        "> Eigenvalue: <font color=\"blue\">$\\left\\{e^{i x \\omega_j}\\right\\}_{j \\in[d]}$ $\\quad$ with $[d]:=\\{1, \\cdots, d\\}$\n",
        "\n",
        "> Eigenstate: <font color=\"blue\">$|\\varphi_j\\rangle_{j \\in[d]}$\n",
        "\n",
        "> Eigenequation: <font color=\"blue\">$U(x)\\left|\\varphi_j\\right\\rangle=e^{i x \\omega_j}\\left|\\varphi_j\\right\\rangle$\n",
        "\n",
        "Two more definitions:\n",
        "\n",
        "> $\\psi_j := \\left\\langle\\varphi_j \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $b_{j k} := \\left\\langle\\varphi_j|B| \\varphi_k\\right\\rangle$ with $j, k \\in[d]$\n",
        "\n",
        "Now we can re-express the expectation value <font color=\"green\">$E(x)$</font> by expanding each of these terms in the multiplication in the Eigenbasis of the unitary <font color=\"blue\">$U(x)$</font>:\n",
        "\n",
        "> $\\begin{gathered}E(x)=\\sum_{j, k=1}^d \\psi_j^*\\left[e^{i x \\omega_j}\\right]^* b_{j k} e^{i x \\omega_k} \\psi_k \\\\ \\psi_j=\\left\\langle\\varphi_j \\mid \\psi\\right\\rangle \\\\ b_{j k}=\\left\\langle\\varphi_j|B| \\varphi_k\\right\\rangle\\end{gathered}$"
      ],
      "metadata": {
        "id": "156UXYtVzz68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Circuits and Parameter-Shift Rule - Part 3**\n",
        "\n",
        "Video: [QHack 2021: Maria Schuld—Quantum Differentiable Programming](https://www.youtube.com/watch?v=cwiINWkMOvA)\n",
        "\n",
        "Are there QML problems that are proven to be superior to CML? - Yes, there are certain problems superior on QML, but none are useful. Useful QML problems with exponential separation to CML are still an open question.\n",
        "\n",
        "What model class is QML? How can it generalize? Let's describe what it is, before we look into concrete cases with only exponential speedups.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1685.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1686.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1687.png)\n"
      ],
      "metadata": {
        "id": "STeAmaKnzi8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Chemistry**\n",
        "* Video: [Introduction to Quantum Chemistry | PennyLane Tutorial](https://www.youtube.com/watch?v=khC0CCjxB7k&list=WL&index=11)\n",
        "* See also: [Aurora Clark (3/10/20): Topology in chemistry applications](https://www.youtube.com/watch?v=qSJeynJbmqU&list=WL&index=12&t=1252s)\n",
        "* Predict properties of materials\n",
        "* What chemical reactions can happen under certain circumstances\n",
        "* Molecular orbitals of electrons: how to represent them with qubits? With Jordan Wigner representation\n",
        "* Hamiltonian operator (all interactions and movements) important to know the energy levels occupied by the electrons in order to predict accurately the chemical properties of the molecules\n",
        "* Hamiltonian operator = all interactions and movements -> too complicated, so we approximate: Hartree Fock approximation\n",
        "    * Calculates molecular geometry (positions where the atomic nuclei are)\n",
        "    * Then calculate hamiltonian\n",
        "* **Jordan Wigner representation**\n",
        "  * https://learn.microsoft.com/en-us/azure/quantum/user-guide/libraries/chemistry/concepts/jordan-wigner\n",
        "  * https://en.m.wikipedia.org/wiki/Jordan–Wigner_transformation\n",
        "  * https://www.cond-mat.de/events/correl21/manuscripts/koch.pdf"
      ],
      "metadata": {
        "id": "Z6d3CddIw4Ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variational Quantum Algorithms**\n",
        "* Video: [Variational Quantum Algorithms](https://www.youtube.com/watch?v=YtepXvx5zdI&t=21s)\n",
        "* Contains **Great images of model types etc)**\n",
        "* https://pennylane.ai/qml/glossary/variational_circuit/\n",
        "* Also known as: parametrized quantum circuits, quantum neural networks\n",
        "* example: VQE (variational quantum eigensolver) for chemistry problems or QAOA for optimization and many more\n",
        "* Variational quantum circuits are already hybrid models"
      ],
      "metadata": {
        "id": "Ezjnw5L2w_ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid Quantum-Classical Machine Learning**\n",
        "* Video: [Hybrid Quantum-Classical Machine Learning](https://www.youtube.com/watch?v=t9ytqPTij7k)\n",
        "* * Variational quantum circuits are already hybrid models\n",
        "* Backprop:\n",
        "  * treat the quantum circuit as a basic atomic individual step in the computation graph.  \n",
        "  * the expectation value of a quantum circuit is a differentiable function.\n",
        "  * So if we treat that single expectation value as a single step, we can use the parameter shift rule to determine the gradient of that same function.\n",
        "  * Means: we can feed that to the backprop algorithm."
      ],
      "metadata": {
        "id": "gD0Zvm95xCNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizing a quantum circuit with PennyLane | PennyLane Tutorial**\n",
        "* Quantum circuit is a function. Think of quantum circuit as cost function and measurement will be the cost -> useful for finding minimum in optimization or ground state in chemistry.\n",
        "* Video: [Optimizing a quantum circuit with PennyLane | PennyLane Tutorial](https://www.youtube.com/watch?v=42aa-Ve5WmI)\n",
        "* https://docs.pennylane.ai/en/stable/introduction/interfaces.html\n"
      ],
      "metadata": {
        "id": "xzAcWeX8xT3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beyond-classical computing, Sergio Boixo**\n",
        "* Video: [Beyond-classical computing, Sergio Boixo](https://www.youtube.com/watch?v=cGaULUQuu1A&list=WL&index=1&t=1147s)\n",
        "* Sample problem, approximate from a sample distribution\n",
        "* Experimental Random Circuit Sampling (RCS)"
      ],
      "metadata": {
        "id": "iBn3SNx_w9qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to solve the QUBO problem | PennyLane Tutorial**\n",
        "\n",
        "* Video: [How to solve the QUBO problem | PennyLane Tutorial](https://www.youtube.com/watch?v=LhbDMv3iA9s&t=212s)"
      ],
      "metadata": {
        "id": "M72_rlVX5fsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Haar Measure | PennyLane Tutorial**\n",
        "\n",
        "* Video: [The Haar Measure | PennyLane Tutorial](https://www.youtube.com/watch?v=d4tdGeqcEZs)\n"
      ],
      "metadata": {
        "id": "QkYxR2W-5p8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Physical simulation through a quantum computational lens, Jarrod McClean**\n",
        "\n",
        "* Video: [Physical simulation through a quantum computational lens, Jarrod McClean](https://www.youtube.com/watch?v=-8fYbDwLAug&list=WL&index=2&t=383s)\n",
        "\n",
        "* What we mean with quantum chemistry simulation: we want some amount of understanding if the system. Like learn interesting things, like will a system absorb light and why? How often will it show up?\n",
        "\n",
        "* The predictive power of (free) energies: Can all physically interesting questions be answered by some reduced model? Recent, but there are undecidable, there can't be an algorithm that determines the answer:\n",
        "  * Does a system thermalize?\n",
        "  * Does a system have an electronic gap?\n",
        "  * Will molecule X ever form from constituents Y?\n",
        "\n",
        "* qualitative changes that can't be predicted ahead of time: proteins, RNA/DNA\n",
        "\n",
        "* Physical undecidability - as the system evolves in time, there are sudden, qualitative changes that cannot be predicted in any way except evolving forward in time and seeing if it happened, and no answer in finite time can indicate if it will never happen (for all systems).\n",
        "\n",
        "> Undecidability formally broken by advice in some cases - and data is a restricted form of advice!\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1688.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1689.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5K4X-OB5RiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jarrod McClean - The role of data, precomputation, and communication in a quantum learning landscape**\n",
        "\n",
        "* Video: [Jarrod McClean - The role of data, precomputation, and communication in a quantum learning landscape](https://www.youtube.com/watch?v=dOUppTONVDM&list=WL&index=8&t=819s)\n",
        "\n",
        "* Quantum data is interesting for future discovery of the universe (recall the impact of CCD cameras on telescopes - see\n",
        "\"The Perfect Theory\"), but most data we work with today, even from quantum systems, seems classical.\n",
        "\n",
        "* There are a few pieces of evidence that QC might help for classical data (sampling hard distributions, learning problems based on discrete log, linear algebra routines, ...) but a lot of pieces of evidence that it will be hard to achieve in practice\n",
        "\n",
        "* Question - What is the full class of uniquely quantum pre-computations we can do to accelerate time to solution?\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1690.png)\n"
      ],
      "metadata": {
        "id": "pDVzhkBUBGja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *notes 5*"
      ],
      "metadata": {
        "id": "k5rk1UYyjFyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pro-physik.de/nachrichten/dunkelheit-macht-makro-quanteneffekte-sichtbar\n",
        "\n",
        "M. Roda-Llordes et al.: Macroscopic Quantum Superpositions via Dynamics in a Wide Double-Well Potential, Phys. Rev. Lett. 132, 023601 (2024); DOI: 10.1103/PhysRevLett.132.023601\n",
        "\n",
        "https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.132.023601"
      ],
      "metadata": {
        "id": "JHMu1wg7jKWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cramer rao bound amd Cauchy–Schwarz inequality, Frobeniusnorm:\n",
        "variance ideal for quantum Hilbert space?\n",
        "\n",
        "How much is a loss function concentrated around its mean (not partial derivative concentration)?\n",
        "\n",
        "\n",
        "**Currently: how to study Parametrized Quantum Circuits (PQCs)?**\n",
        "\n",
        "* Expressivity / Capacity\n",
        "* Trainability\n",
        "  * subset of computational complexity and circuit complexity?\n",
        "* Generalization > which model or algorithm to choose best?\n",
        "* Sample complexity\n",
        "\n",
        "**Generalization in Machine Learning:**\n",
        "\n",
        "* Generalization refers to the ability of an algorithm to **perform accurately on new, unseen data after being trained on a sample dataset**. The goal is to learn patterns that are not just specific to the training data but also applicable to new data.\n",
        "\n",
        "* Uniform Generalization Bounds: These bounds provide a measure of how well the learned model will generalize across all possible data distributions, not just the one it was trained on. They are called 'uniform' because they provide a guarantee that holds uniformly over a class of functions or models.\n",
        "\n",
        "* Role of Sample Size: One key aspect of these bounds is that they typically improve with an increase in the size of the training sample. As more data is used to train the model, we can be more confident that the model will generalize well.\n",
        "\n",
        "* Complexity of the Hypothesis Class: **The bounds are also influenced by the complexity of the hypothesis class** (the set of models being considered). More complex classes, such as those with a large number of parameters, typically have **looser bounds, meaning they need more data to ensure good generalization**.\n",
        "\n",
        "* Mathematical Formulation: **The bounds are often derived using tools from probability and statistics, such as concentration inequalities (like Hoeffding’s inequality or McDiarmid's inequality)**. These tools help quantify the likelihood that the observed performance of a model on a sample is close to its true performance on the entire population.\n",
        "\n",
        "* Practical Implication: In practical terms, **uniform generalization bounds help in understanding and choosing models and algorithms**. They provide a theoretical foundation for preferring certain models over others based on their generalization capabilities, given a finite amount of data."
      ],
      "metadata": {
        "id": "lQTFSjJ1bWlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Besides model complexity, sample complexity and computational complexity, are there more dimension that need to be considered on computational learning theory?**\n",
        "\n",
        "Yes, besides model complexity, sample complexity, and computational complexity, there are several other dimensions and factors that are important in computational learning theory:\n",
        "\n",
        "1. **Algorithmic Stability**:\n",
        "    - This refers to how sensitive a learning algorithm is to small changes in the training set. Algorithms that are more stable are generally better at generalizing, as they don't overfit to the noise or specific details in the training data.\n",
        "\n",
        "2. **Noise Tolerance**:\n",
        "    - The ability of a learning algorithm to perform well in the presence of noise (errors or randomness in the training data) is crucial. This includes dealing with mislabeled data or data with measurement errors.\n",
        "\n",
        "3. **Robustness**:\n",
        "    - Robustness is the ability of the model to maintain performance when faced with input data that differs from the training data. This includes handling adversarial examples, where small, intentional changes to input data can mislead the model.\n",
        "\n",
        "4. **Learning Rate and Convergence**:\n",
        "    - This pertains to how quickly a learning algorithm converges to a good solution. Fast convergence is desirable for efficiency, but it's also important that the convergence leads to a solution that generalizes well.\n",
        "\n",
        "5. **Bias-Variance Tradeoff**:\n",
        "    - A fundamental concept in statistical learning, the bias-variance tradeoff involves balancing underfitting (bias) and overfitting (variance). It's about finding the right level of model complexity that neither oversimplifies (high bias) nor overcomplicates (high variance) the problem.\n",
        "\n",
        "6. **Data Distribution**:\n",
        "    - The nature of the data distribution, including how representative the training data is of the real-world scenarios the model will encounter, plays a crucial role in the effectiveness of learning.\n",
        "\n",
        "7. **Feature Representation**:\n",
        "    - The way data is represented or pre-processed can significantly impact the performance of a learning algorithm. Good feature representation can ease the learning process and improve generalization.\n",
        "\n",
        "8. **Scalability**:\n",
        "    - Scalability refers to how well a learning algorithm or model can handle increasing amounts of data or more complex problems. This is particularly important in big data applications.\n",
        "\n",
        "9. **Ethical and Social Considerations**:\n",
        "    - This includes considerations of fairness, privacy, and transparency. It's essential to ensure that learning algorithms do not perpetuate or exacerbate biases and are respectful of privacy and ethical norms.\n",
        "\n",
        "10. **Transferability and Adaptability**:\n",
        "    - This refers to the ability of a model to adapt to new tasks or transfer learned knowledge to different but related problems. It's a key aspect of research in areas like transfer learning and domain adaptation.\n",
        "\n",
        "11. **Interpretability and Explainability**:\n",
        "    - The ability to understand and interpret the decisions made by a model is increasingly important, especially for applications in sensitive areas like healthcare or finance.\n",
        "\n",
        "Each of these dimensions adds complexity to the design and evaluation of learning algorithms, and they are often interrelated. Balancing these factors is a key challenge in developing effective and responsible machine learning systems."
      ],
      "metadata": {
        "id": "h8b1e__XbWlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are standard scientific methods to measure model complexity, sample complexity and computational complexity?**\n",
        "\n",
        "Measuring model complexity, sample complexity, and computational complexity in machine learning involves a blend of theoretical analysis and empirical evaluation. Each type of complexity has standard scientific methods associated with it:\n",
        "\n",
        "*1. Model Complexity*\n",
        "Model complexity refers to the ability of a model to fit a wide range of functions. It's often associated with the flexibility of the model to adapt to the underlying data structure.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **VC Dimension**: For classification models, the Vapnik-Chervonenkis (VC) dimension is a theoretical measure of model capacity. It's the size of the largest set of points that the model can shatter.\n",
        "- **Number of Parameters**: In many models, such as neural networks, the number of trainable parameters is a direct measure of complexity.\n",
        "- **Model Depth and Architecture**: For neural networks, depth (number of layers) and architecture details (like types of layers and connections) indicate complexity.\n",
        "- **Information Criteria**: Metrics like AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used in statistical modeling to balance model fit and complexity.\n",
        "\n",
        "- Number of Parameters: One of the most common measures of model complexity is the number of parameters. This is a simple and intuitive measure, as it directly reflects the number of degrees of freedom in the model. However, it is not always the most informative measure, as it does not take into account the structure of the model or the relationships between its parameters.\n",
        "\n",
        "- Effective Number of Parameters: A more sophisticated measure of model complexity is the effective number of parameters. This measure takes into account the fact that some parameters may be more important than others, and it can be used to compare models with different numbers of parameters.\n",
        "\n",
        "- VC Dimension: The VC dimension is a theoretical measure of model complexity that is based on the concept of shattering. A shattered set of points is a set of points that can be perfectly classified by a hypothesis class in all possible ways. The VC dimension of a hypothesis class is the maximum size of a shattered set. A higher VC dimension indicates a more complex hypothesis class.\n",
        "\n",
        "- Kolmogorov Complexity: Kolmogorov complexity is a measure of the information content of an object. It is based on the idea that the complexity of an object is the shortest possible program that can generate it. Kolmogorov complexity can be used to measure the complexity of models, as well as the complexity of data.\n",
        "\n",
        "*2. Sample Complexity*\n",
        "Sample complexity relates to the amount of data required for a model to achieve a certain level of performance.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Empirical Evaluation**: Experimentally determining how model performance varies with different training set sizes. This often involves plotting learning curves.\n",
        "- **PAC Learning Framework**: Probably Approximately Correct (PAC) learning provides theoretical bounds on the number of samples needed for a model to learn a concept within a certain error margin and confidence level.\n",
        "- **Cross-validation**: Techniques like k-fold cross-validation can help assess how well a model with certain complexity performs with the available data.\n",
        "\n",
        "- Learning Curves: Learning curves are a simple and effective way to measure sample complexity. A learning curve is a plot of the training and generalization error of a model as a function of the number of training examples. A steeper learning curve indicates that the model is more sensitive to the number of training examples and therefore has higher sample complexity.\n",
        "\n",
        "- Error Bounds: Error bounds are a more theoretical way to measure sample complexity. Error bounds provide an upper bound on the generalization error of a model with a given level of confidence. A smaller error bound indicates lower sample complexity.\n",
        "\n",
        "- Rademacher Complexity: Rademacher complexity is a measure of the generalization error of a model that is based on the concept of Rademacher averages. Rademacher averages are a type of random average that is used to control for the randomness of the training data. A smaller Rademacher complexity indicates lower sample complexity.\n",
        "\n",
        "*3. Computational Complexity*\n",
        "Computational complexity is about the resources (time, memory) required to train or use a model.\n",
        "\n",
        "**Standard Methods to Measure:**\n",
        "- **Big O Notation**: Theoretical analysis of algorithms in terms of time and space complexity (e.g., O(n), O(n^2)) is standard.\n",
        "- **Time Profiling**: Empirically measuring the actual time taken to train models on datasets of varying sizes.\n",
        "- **Space Profiling**: Measuring the memory requirements of a model during training and inference.\n",
        "- **Algorithm Analysis**: Analyzing the specific steps of an algorithm to determine their computational requirements.\n",
        "\n",
        "- Time Complexity: Time complexity is a measure of the amount of time that an algorithm takes to run as a function of the input size. Time complexity is typically expressed using Big O notation, which provides an upper bound on the worst-case performance of the algorithm.\n",
        "\n",
        "- Space Complexity: Space complexity is a measure of the amount of memory that an algorithm requires to run as a function of the input size. Space complexity is also typically expressed using Big O notation.\n",
        "\n",
        "- Scalability: Scalability is a measure of how well an algorithm can handle increasing input sizes. An algorithm is said to be scalable if its time and space complexity grow sublinearly with the input size.\n",
        "\n",
        "*Integrating Theoretical and Empirical Approaches*\n",
        "In practice, a combination of these methods is often used. Theoretical measures provide a guideline or a framework, while empirical methods offer practical insights based on real-world data and computing environments. The choice of method depends on the specific model, the nature of the task, and the available resources. For example, in deep learning, empirical evaluation is heavily relied upon due to the complexity and non-linearity of the models, while in more traditional machine learning or statistical models, theoretical measures might be more prominently used."
      ],
      "metadata": {
        "id": "6ZyTxTqpbWls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensions**\n",
        "\n",
        "[Dimension](https://en.m.wikipedia.org/wiki/Dimension).Dimensionality measures are used to quantify the complexity of a set of points or a function class. They are often used in machine learning to bound the sample complexity of learning algorithms.\n",
        "* in mathematics, is a particular way of describing the size of an object (contrasting with measure and other, different, notions of size). Dimensionality measures are also used to study the relationships between different complexity classes. For example, it is known that the complexity class PSPACE is contained in the complexity class EXPTIME. This can be shown using the following theorem: Theorem: The metric entropy of any hypothesis class with VC dimension d is at most d. This theorem tells us that the complexity class PSPACE, which contains all problems that can be solved by a polynomial space Turing machine, is contained in the complexity class EXPTIME, which contains all problems that can be solved by an exponential time Turing machine. Dimensionality measures are a powerful tool for studying the complexity of computational problems. They can be used to bound the resource requirements of algorithms, and to study the relationships between different complexity classes.\n",
        "\n",
        "*Examples:*\n",
        "\n",
        "* [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension) is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting. There are several variations (various notions of effective dimension) of which the most common is effective Hausdorff dimension.\n",
        "\n",
        "  * effective dimension is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting\n",
        "\n",
        "  * to measure power / capacity of a model [Source](https://www.youtube.com/watch?v=fDIGmkq9xNE&t=2067s)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1398.png)\n",
        "\n",
        "* [Hausdorff dimension](https://en.m.wikipedia.org/wiki/Hausdorff_dimension) generalizes the well-known integer dimensions assigned to points, lines, planes, etc. by allowing one to distinguish between objects of intermediate size between these integer-dimensional objects.\n",
        "\n",
        "* [Intrinsic dimension](https://en.m.wikipedia.org/wiki/Intrinsic_dimension): The intrinsic dimension of a set of points is the smallest number of parameters needed to represent the points accurately.\n",
        "\n",
        "* [Fractal dimension](https://en.m.wikipedia.org/wiki/Fractal_dimension) provides a rational statistical index of complexity detail in a pattern. A fractal pattern changes with the scale at which it is measured. It is also a measure of the space-filling capacity of a pattern, and it tells how a fractal scales differently, in a fractal (non-integer) dimension.\n",
        "\n",
        "* [Lebesgue covering dimension](https://en.m.wikipedia.org/wiki/Lebesgue_covering_dimension) or topological dimension of a topological space is one of several different ways of defining the dimension of the space in a topologically invariant way\n",
        "\n",
        "* [Krull dimension](https://en.m.wikipedia.org/wiki/Krull_dimension)\n",
        "\n",
        "* [Inductive dimension](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "\n",
        "* [Minkowski–Bouligand dimension](https://en.m.wikipedia.org/wiki/Minkowski–Bouligand_dimension) also known as Minkowski dimension or box-counting dimension, is a way of determining the fractal dimension of a set S in a Euclidean space $\\mathbb {R} ^{n}$, or more generally in a metric space (X,d).\n",
        "\n",
        "* [Information dimension](https://en.m.wikipedia.org/wiki/Information_dimension) is a measure of the fractal dimension of a probability distribution. It characterizes the growth rate of the Shannon entropy given by successively finer discretizations of the space.\n",
        "\n",
        "  * In 2010, Wu and Verdú gave an operational characterization of [Rényi information dimension = Rényi entropy](https://en.m.wikipedia.org/wiki/Rényi_entropy) as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder.\n",
        "\n",
        "  * Enge Verbindung zwischen Entropy und Dimension measures.\n",
        "\n",
        "* [Correlation dimension](https://en.m.wikipedia.org/wiki/Correlation_dimension) is a measure of the dimensionality of the space occupied by a set of random points, often referred to as a type of fractal dimension.\n",
        "\n",
        "* [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "\n",
        "* [Equilateral dimension](https://en.m.wikipedia.org/wiki/Equilateral_dimension)\n",
        "\n",
        "* [Dimensions of commutative algebra](https://en.m.wikipedia.org/wiki/Glossary_of_commutative_algebra#dimension), like Embedding dimension"
      ],
      "metadata": {
        "id": "HwVq-y4ibWls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vapnik-Chervonenkis, Pseudo- and Fat-Shattering Dimensions are three distinct notions of “dimension”. The phrase “dimension” is rather unfortunate, as the three “dimensions” have nothing to do with the dimension of a vector space, except in very special situations. Rather, these “dimensions” are combinatorial parameters that measure the “richness” of concept classes or function classes.*\n",
        "\n",
        "* [Vapnik–Chervonenkis dimension](https://en.m.wikipedia.org/wiki/Vapnik–Chervonenkis_dimension)\n",
        "  * It quantifies the expressive power or complexity of a hypothesis class in terms of its ability to \"shatter\" sets of points. It's a scalar value that provides insight into the capacity of a class of functions or models.\n",
        "  * A hypothesis class is said to \"shatter\" a set of points if, for every possible labeling of the points, there exists a hypothesis in the class that can perfectly classify those points according to that labeling. VC dimension is defined in terms of shattering.\n",
        "  * It is the most widely used complexity metric in PAC learning, and it has been used to prove a number of important theoretical results.\n",
        "  * However, the VC dimension is not a perfect metric for the complexity of a hypothesis class. For example, the VC dimension of the class of all linear classifiers is infinite, even though linear classifiers can be learned efficiently from a small number of training examples.\n",
        "  * In PAC learning: Let H be a hypothesis class with VC dimension d. Then, the sample complexity of PAC learning H is given by: $n >= O(d / ε^2 * ln(1/δ))$, where ε is the desired error tolerance and δ is the desired confidence level.\n",
        "  \n",
        "  * [*Bias-Variance Tradeoff*](https://en.m.wikipedia.org/wiki/Bias–variance_tradeoff): This is a key principle in statistical learning that helps to understand the tradeoff between model complexity and the risk of overfitting, which impacts the number of samples needed for learning.\n",
        "\n",
        "  * Higher VC-dimension of a class = more samples are required to learn that class. Bounds on sample complexity using VC-dimension are often given in the form, where d is the VC-dimension:\n",
        "\n",
        "  * $m \\geq \\frac{8}{\\epsilon} \\ln \\left(\\frac{4}{\\delta}\\right)+\\frac{4}{\\epsilon} d \\ln \\left(\\frac{2 e m}{d}\\right)$\n",
        "\n",
        "  * measure of the capacity: The more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data (-that was assumed for neural nets).\n",
        "\n",
        "  * The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data. sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C. measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm.\n",
        "\n",
        "  * A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data. The VC Dimension can provide an **upper bound on the sample complexity in terms of the size of the hypothesis class**, the desired error rate, and the confidence level.\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1397.png)\n",
        "\n",
        "  * *Shatter points = separate points: how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity. - Shattered:  A set of points is shattered by H if for every possible labeling of the points, there exists a hypothesis in H that agrees with that labeling.*\n",
        "\n",
        "  * One of the most fundamental results in learning theory is that the sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C (Source: Optimal Quantum Sample Complexity of Learning Algorithms)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1643.png)\n",
        "\n",
        "  * VC dimension is still a useful concept in machine learning. It can be used to understand the complexity of a neural network and to choose a neural network that is not too complex, so that it can generalize well to new data.\n",
        "\n",
        "  * Limitations of the VC dimension (VC dimension theorem would seem to imply that large neural networks would overfit very badly and never generalize well):\n",
        "    * The VC dimension is only a theoretical bound, and it is not always accurate in practice. The VC dimension theorem assumes that the training data is drawn from an i.i.d. (independent and identically distributed) distribution. However, in practice, the training data is often not i.i.d., and this can make the VC dimension theorem less accurate.\n",
        "    * The VC dimension does not take into account the complexity of the target function. The VC dimension is only a bound on the generalization error. It is possible for a neural network to have a large VC dimension and still generalize well, if the training data is large enough.\n",
        "    * The VC dimension does not take into account the optimization algorithm used to train the neural network. There are a number of techniques that can be used to prevent neural networks from overfitting, such as regularization and dropout. These techniques can help to reduce the complexity of the neural network and make it more likely to generalize well.\n",
        "\n",
        "  * https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_VC_Dimension_Shatter.php\n",
        "\n",
        "  * Code example: https://www.geeksforgeeks.org/vapnik-chervonenkis-dimension/\n",
        "\n",
        "  * Video: https://youtu.be/puDzy2XmR5c?si=FTPneApRNiWQkJey\n",
        "\n",
        "  * VC dimension is a method to measure model complexity. It is a measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm. A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data.\n",
        "\n",
        "  * The VC Dimension can provide an upper bound on the sample complexity in terms of the size of the hypothesis class, the desired error rate, and the confidence level.\n",
        "\n",
        "  * In [Vapnik–Chervonenkis theory](https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory), the [Vapnik–Chervonenkis (VC) dimension](v) is a **measure of the capacity (complexity, expressive power, richness, or flexibility)** of a set of functions that can be learned by a statistical binary classification algorithm.\n",
        "\n",
        "  * It is defined as the cardinality of the largest set of points that the algorithm can [shatter](https://en.m.wikipedia.org/wiki/Shattered_set), which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points.\n",
        "\n",
        "  * Relationship:\tThe VC dimension can be used to derive bounds on the sample complexity.\tThe sample complexity can be used to estimate the VC dimension.\n",
        "\n",
        "  * The VC dimension of a set of functions can be used to bound the generalization error of a learning algorithm. The generalization error is the error that a learning algorithm makes on new data that it has not seen before. The VC dimension theorem states that the generalization error of a learning algorithm is bounded by the VC dimension of the hypothesis space divided by the number of training examples. In other words, **the more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data** (-that was assumed for neural nets). The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data.\n",
        "\n",
        "* [Sauer's Lemma (or Sauer–Shelah Lemma)](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma): A combinatorial bound that relates the growth function to the VC dimension and the number of data points. It says that if a hypothesis class has a VC dimension \\( d \\) and does not shatter any set of \\( d+1 \\) points, then the number of dichotomies it can produce on any set of \\( n \\) points is bounded by the sum of binomial coefficients from \\( i = 0 \\) to \\( d \\).\n",
        "\n",
        "* [Natarajan dimension](https://en.m.wikipedia.org/wiki/Natarajan_dimension): An extension of VC dimension to multiclass classification problems.\n",
        "\n",
        "* **Fat-shattering dimension**:\n",
        "\n",
        "  * An extension of the VC dimension that considers the ability of a hypothesis class to shatter sets of points with margins. It's useful for analyzing algorithms that make use of margins, like Support Vector Machines. / [Fat-shattering dimension](https://mlweb.loria.fr/book/en/fatshattering.html) at a certain scale of a function class is defined as the maximal number of points that can be fat-shattered by this function class at that scale. It can be bounded for popular function classes such as for the set of linear functions.\n",
        "\n",
        "  * https://mathoverflow.net/questions/307201/vc-dimension-fat-shattering-dimension-and-other-complexity-measures-of-a-clas\n",
        "\n",
        "  * Covering Numbers and Fat-Shattering Dimension: measures of complexity of function classes that can be used to **derive upper bounds on sample complexity** (bounds on expressivity of class of CPTP maps (or unitaries) that a quantum machine learning model (QMLM) can implement in terms of number of trainable elements)\n",
        "\n",
        "  * **fat-shattering dimension and VC dimension (Vapnik-Chervonenkis dimension)** fat-shattering dimension is not an example of an inequality. It is a measure of the complexity of a function class. A function class with a small fat-shattering dimension is said to be easy to learn, while a function class with a large fat-shattering dimension is said to be difficult to learn.\n",
        "\n",
        "  * The fat-shattering dimension is a complexity measure used in statistical learning theory. It's one way of quantifying the **complexity or expressive power of a class of functions, and it's used to derive bounds on the generalization error** of a learning algorithm.\n",
        "\n",
        "  * The formal definition of the fat-shattering dimension is a bit technical, but here's the general idea: suppose you have a class of functions, and you want to understand how complex this class is. One way of doing this is to look at how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity.\n",
        "\n",
        "  * **Now, for the fat-shattering dimension, you add an additional constraint: you require a certain margin (the \"fatness\") between points that are labeled differently**. The largest set of points that can be shattered with a given margin is used to define the fat-shattering dimension of the function class.\n",
        "\n",
        "  * The fat-shattering dimension is related to the VC dimension (Vapnik-Chervonenkis dimension), another complexity measure that is widely used in statistical learning theory. However, **while the VC dimension only considers exact separation of points, the fat-shattering dimension takes into account this idea of a margin, which makes it more suitable for dealing with \"noisy\" or non-separable data.**\n",
        "\n",
        "  * the concept of the fat-shattering dimension is often used in the analysis of machine learning algorithms, particularly those based on empirical risk minimization. It can help to understand the trade-off between the expressive power of a function class (which often corresponds to the complexity of a learning algorithm) and the ability of the algorithm to generalize well to unseen data.\n",
        "\n",
        "* **Pseudo-Dimension**: Similar in spirit to the VC dimension but adapted for real-valued function classes rather than binary classifiers. / [Pseudo-dimension](https://link.springer.com/chapter/10.1007/978-1-4471-3748-1_4), also Pollard dimension, is a generalization of the VC-dimension to real-valued functions. The fat-shattering dimension, unlike the Pseudo-dimension, is a “scale-sensitive” measure of richness.\n"
      ],
      "metadata": {
        "id": "FiGM7LUAbWls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Concentration inequalities**](https://en.m.wikipedia.org/wiki/Concentration_inequality) = bounds probability of estimator deviating from its expected value by a certain amount = provide guarantees on accuracy of estimator. Inequalities above just bound magnitude of deviation, bound probability. Will a learning algorithm converge to correct hypothesis with high probability, even if data is noisy or incomplete? Concentration inequalities can be used to bound VC dimension of hypothesis space to design ML algorithms that are guaranteed to generalize well to new data. And in regularization (to control complexity of ML models): analyze effects of regularization on generalization performance of ML models. Concentration inequalities don't guarantee learning a function with small error, but provide high degree of confidence that algorithm is likely to learn a good function, given a sufficient number of training samples and a suitable regularization scheme.\n",
        "\n",
        "* Matrix concentration inequalities: use by Ewin tang for dequantization, see also: https://arxiv.org/abs/1501.01571 - An Introduction to Matrix Concentration Inequalities. Some common matrix concentration inequalities that might appear in Tang's work include:\n",
        "\n",
        "  * Matrix Bernstein Inequality: Provides bounds on the deviation of the sum of random matrices from its expectation. It has variants tailored to handling sums of matrices that aren't independent.\n",
        "  * Matrix Hoeffding Inequality: Generalizes Hoeffding's inequality (which is for sums of random variables) to the matrix setting.\n",
        "  * Matrix Chernoff Bound: Gives tail bounds on how much a random matrix might deviate from its mean.\n",
        "\n",
        "\n",
        "* [Markov's inequality](https://en.m.wikipedia.org/wiki/Markov%27s_inequality): simplest concentration inequality. Bounds probability that random variable deviates from its expected value by a certain amount = bounds probability of errors in statistical estimators, proves convergence of learning algorithms, analyzes performance of randomized algorithms.\n",
        "\n",
        "* [Hoeffding's inequality](https://en.m.wikipedia.org/wiki/Hoeffding%27s_inequality):\n",
        "  * We want to guarantee that a hypothesis with a small training error will have good accuracy on unseen examples, and one way to do so is with Hoeffding bounds. This characterizes the deviation between the true probability of some event and its observed frequency over m independent trails. [Source](https://www.cis.upenn.edu/~danroth/Teaching/CS446-17/LectureNotesNew/colt/main.pdf)\n",
        "  * states that probability that average of a number of independent and identically distributed random variables deviates from its expected value by more than a certain amount is exponentially small in number of random variables.\n",
        "  * average of n independent random variables deviates from its expected value by more than ε at most $2e^{(-2nε^2)}$ probability and decays exponentially with number of random variables -> Probability of deviation becomes very small as number of random variables increases. **This inequality bounds generalization error of a learning algorithm, regardless of the model complexity or the sample complexity. Can show that sample complexity of learning a linear regression model with a certain level of accuracy is proportional to logarithm of number of features.**  \n",
        "  * Hoeffding's inequality is a **special case of the Azuma–Hoeffding inequality and McDiarmid's inequality**. It is **similar to the Chernoff bound**, but tends to be less sharp, in particular when the variance of the random variables is small. It is **similar to, but incomparable with, one of Bernstein's inequalities**.\n",
        "  * Example: bound sample complexity of online learning algorithm (with perceptron):\n",
        "    * $X_1, X_2, ..., X_n$ be independent random variables with mean $\\mu$ and variance $\\sigma^2$.\n",
        "    * For any $\\delta > 0$, $P(\\bar{X} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\bar{X}$ is average of $X_i$.\n",
        "    * $L_i$ be loss of Perceptron on $i$th training sample. Average loss of Perceptron on training data is: $\\bar{L} = \\frac{1}{n} \\sum_{i=1}^n L_i$.\n",
        "    * Objective: Use concentration inequality to show that average loss of Perceptron on training data is close to its expected value with high probability, given a sufficient number of training samples: $P(\\bar{L} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\mu$ is expected loss of Perceptron on training data.\n",
        "    * With probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$, average loss of Perceptron on training data is within $\\delta$ of its expected value.\n",
        "    * Set $\\delta$ to be a small value, such as $0.01$, to ensure that average loss of Perceptron on training data is very close to its expected value with high probability.\n",
        "    * Hoeffdings inequality bounds sample complexity: $n \\geq \\frac{2 \\sigma^2 \\ln(1 / \\delta)}{\\delta^2}$ tells us that if we have $n$ training samples, then average loss of Perceptron on training data is within $\\delta$ of its expected value with probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$.\n",
        "\n",
        "* [Azuma's inequality](https://en.m.wikipedia.org/wiki/Azuma%27s_inequality)\n",
        "\n",
        "* [Chernoff bounds](https://en.m.wikipedia.org/wiki/Chernoff_bound): Bound probability of sum of independent random variables deviating from its expected value by more than a certain amount, even if the random variables are not identically distributed. Generalization of Hoeffding's inequality, used to bound tails of probability distributions.\n",
        "\n",
        "* [Bernstein's inequality](https://en.m.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)): generalization of Chernoff bounds. Bounds probability, even if the random variables are not identically distributed and have non-identical variances (used to bound tails of sub-Gaussian distributions).\n",
        "\n",
        "* [McDiarmid's inequality](https://en.m.wikipedia.org/wiki/McDiarmid%27s_inequality): bounds deviation between sampled value and expected value of certain functions (= functions that satisfy a bounded differences property, meaning that replacing a single argument to the function while leaving all other arguments unchanged cannot cause too large of a change in the value of the function) when they are evaluated on independent random variables.\n",
        "\n",
        "* [Bennett's inequality](https://en.m.wikipedia.org/wiki/Bennett%27s_inequality): provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. **Can be used to: Bound the generalization error of learning algorithms.**\n",
        "\n",
        "* [(Bienaymé–) Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality): provide upper bounds on the probability that a random variable deviates from its expected value by more than a certain amount. Chebyshev's inequality is particularly useful for bounding the deviations of random variables with finite mean and variance. One way to think about Chebyshev's inequality is that it provides a measure of how concentrated the probability distribution of a random variable is around its expected value. The more concentrated the probability distribution is, the lower the probability is that the random variable will deviate from its expected value by more than a certain amount. **Can be used to: Bound the probability of rare events**. Design confidence intervals for population parameters. Develop efficient algorithms for statistical testing.\n",
        "\n",
        "* [Vysochanskij–Petunin inequality](https://en.m.wikipedia.org/wiki/Vysochanskij%E2%80%93Petunin_inequality):  It is a refinement of Chebyshev's inequality that provides tighter bounds on the probability that a random variable deviates from its expected value by more than a certain amount, especially for random variables with unimodal distributions."
      ],
      "metadata": {
        "id": "EyLCNuiFbWlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Inequality (complexities)](https://en.m.wikipedia.org/wiki/Inequality_(mathematics)): bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.** See [Inequalities in information theory](https://en.m.wikipedia.org/wiki/Inequalities_in_information_theory), [Information geometry](https://en.m.wikipedia.org/wiki/Information_geometry), [Information projection](https://en.m.wikipedia.org/wiki/Information_projection), and [Confidence Interval](https://en.m.wikipedia.org/wiki/Confidence_interval).\n",
        "\n",
        "Evaluate performance of estimators: bounds on Prediction Errors, Sample Complexity, Model complexity, and Deviation of Estimators. By bounding error: ensure that model is not too far from optimal solution (Regularize models by bounding complexity of model: L1 norm to bound number of parameters, L2 norm to bound sum of squared parameters).\n",
        "\n",
        "* [Triangle inequality](https://en.m.wikipedia.org/wiki/Triangle_inequality). K-means clustering: cost function is sum of squared distances between data points and cluster centroids. Squared distances can be bounded by triangle inequality.\n",
        "\n",
        "* [Cramér–Rao bound](https://en.m.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound): lower bound (min value) on variance of an unbiased estimator (but cannot tell about probability of deviation) - this value is inverse of [Fisher information](https://de.m.wikipedia.org/wiki/Fisher-Information#Verwendung)(how much information data provides about parameters being estimated). Used to design estimators that are as efficient as possible.\n",
        "\n",
        "  * [*Quantum Cramér-Rao Bound*](https://en.m.wikipedia.org/wiki/Quantum_Cramér–Rao_bound)\n",
        "\n",
        "  * QFIM and Quantum Cramér-Rao Bound used in quantum metrology to **quantify ultimate limit to precision** that can be achieved in estimating parameters (CRB (derived from FIM) tells us smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator. Fisher Information is a measure of how much information a random variable provides about an unknown parameter).\n",
        "\n",
        "  * Is an inequality: bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.\n",
        "\n",
        "  * The Cramér-Rao bound (CRB) belongs to **model complexity** = Bounds on Precision of Estimator. It gives a measure of the best possible precision that can be achieved when estimating that parameter from a given set of data. The **quantum Cramér-Rao bound provides a limit on the precision with which these properties can be estimated, given the inherent uncertainties of quantum mechanics**.\n",
        "\n",
        "  * It provides a lower bound on the covariance matrix of any unbiased quantum estimator (The covariance matrix determines the uncertainty or precision of the estimated parameter). It characterizes the best possible precision that can be achieved in estimating a parameter of interest from quantum measurements.\n",
        "\n",
        "  * The quantum Cramér-Rao bound serves as a powerful tool for understanding the fundamental limits of precision in quantum state estimation (without consoidering noise, imperfections etc).\n",
        "\n",
        "  * The quantum Cramér-Rao bound is given by the inverse of the **Fisher information matrix**, which in the quantum case is calculated using the quantum state and the POVM (Positive Operator-Valued Measure) elements describing the measurements. It can provide insights into the optimal measurement strategies for estimating the parameters of a quantum state, and can help guide the design of quantum sensors and other quantum technologies.\n",
        "\n",
        "  * The basic formula for calculating the quantum Cramér-Rao bound (QCRB) for parameter estimation:\n",
        "\n",
        "  * $QCRB = Tr(F^{-1} M)$\n",
        "\n",
        "  * where:\n",
        "\n",
        "    - QCRB is the quantum Cramér-Rao bound, which provides a lower bound on the covariance matrix of any unbiased estimator.\n",
        "    - $F$ is the Fisher information matrix, which quantifies the amount of information provided by the measurements about the parameter of interest.\n",
        "    - $M$ is the symmetric, positive semidefinite matrix representing the measurement operators' influence on the parameter estimation.\n",
        "\n",
        "  * To calculate the QCRB, you'll need to obtain the Fisher information matrix (F) and the measurement matrix (M) for the chosen estimation strategy. The Fisher information matrix can be computed based on the measurement operators and the quantum state being measured.\n",
        "\n",
        "  * Cramér-Rao is a lower bound on the variance of any unbiased estimator of a parameter, given a fixed model. The CRB depends on the Fisher information, which is a measure of how much information the data provides about the parameter. A higher Fisher information means that the data is more informative about the parameter, and therefore the CRB is lower.\n",
        "\n",
        "  * In general, the CRB is a more fundamental concept than the sample complexity. It is used to understand the fundamental limits of parameter estimation, regardless of the amount of data available. The sample complexity, on the other hand, is more practical, as it tells us how much data we need to achieve a certain level of accuracy.\n",
        "\n",
        "  * In machine learning, the Cramér-Rao Bound (CRB) is often used as a tool for **understanding the fundamental limits of learning algorithms, particularly in the field of parameter estimation and model selection**. Here are some specific applications:\n",
        "\n",
        "    1. Variance Estimation: Similar to financial economics, the CRB provides a theoretical lower limit for the variance of an unbiased estimator. This can help understand how well a learning algorithm might perform in terms of parameter estimation, given a certain amount of data.\n",
        "\n",
        "    2. Model Complexity: The CRB can provide insights into how model complexity affects estimation accuracy. A model with too many parameters may have a higher CRB (i.e., higher variance for the best possible estimator), indicating that it could be more prone to overfitting.\n",
        "\n",
        "    3. Algorithm Evaluation: Researchers might use the CRB to evaluate and compare the performance of different learning algorithms. If an algorithm's performance is close to the CRB, it might be deemed near-optimal.\n",
        "\n",
        "    4. Designing Neural Networks: In the design of neural networks, the CRB can be used to understand the limit of what the network can learn from the data, which can help in making decisions about network architecture and training strategies.\n",
        "\n",
        "* [Trace_inequality](https://en.m.wikipedia.org/wiki/Trace_inequality) bounds the trace of a matrix by the sum of the traces of its eigenvalues. Trace inequality, also known as the triangle inequality for the trace distance, states that for any three quantum states ρ, σ, and τ, the following inequality holds:\n",
        "  * $δ(ρ, σ) + δ(σ, τ) ≥ δ(ρ, τ)$\n",
        "  * where δ(ρ, σ) is the trace distance between ρ and σ.\n",
        "  * Bounding the error of quantum learning algorithms: The trace inequality can be used to derive upper bounds on the error of quantum learning algorithms. This is useful for understanding the performance of quantum learning algorithms and for comparing them to classical learning algorithms.\n",
        "  *  The trace inequality can be used to derive bounds on the error of a wide variety of quantum learning algorithms, including algorithms for learning linear classifiers, support vector machines, and neural networks.\n",
        "  * It is important to note that the bounds derived using the trace inequality are often loose. This is because the trace inequality is a very general tool, and it does not take into account the specific structure of the quantum learning algorithm or the training data. However, the trace inequality can still be used to get a rough estimate of the error of a quantum learning algorithm.\n",
        "\n",
        "* [Bohnenblust-Hille inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality):\n",
        "\n",
        "  * The Bohnenblust-Hille inequality says that the $\\ell^{\\frac{2 m}{m+1}}$-norm of the coefficients of an $m$-homogeneous polynomial $P$ on $\\mathbb{C}^n$ is bounded by $\\|P\\|_{\\infty}$ times a constant independent of $n$, where $\\|\\cdot\\|_{\\infty}$ denotes the supremum norm on the polydisc $\\mathbb{D}^n$. [Source](https://annals.math.princeton.edu/wp-content/uploads/annals-v174-n1-p13-s.pdf)\n",
        "\n",
        "\n",
        "* [Littlewood's 4/3 inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality)\n",
        "\n",
        "  * is a norm inequality for multilinear forms and polynomials. It is used to study the behavior of random variables and multilinear forms. Can be used to derive concentration inequalities in some cases. For example, it can be used to prove that the coefficients of a random homogeneous polynomial are concentrated around their expected values. This can then be used to prove concentration inequalities for other random variables, such as the output of a neural network. Used in \"Learning to predict arbitrary quantum processes\".\n",
        "\n",
        "\n",
        "* [Golden–Thompson inequality](https://en.m.wikipedia.org/wiki/Golden%E2%80%93Thompson_inequality) bounds difference between logarithm of trace of a matrix and sum of logarithms of its eigenvalues. Used in 'A survey on the complexity of learning quantum states' page 17 - difference between real state and shadow tomography state)\n",
        "\n",
        "* [Cauchy–Schwarz inequality](https://en.m.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality). Cauchy-Schwarz inequality used to bound sum of squared residuals (cost function for linear regression) by the sum of the squared predicted values and the sum of the squared actual values. Used in *A Survey of Quantum Learning Theory* page 11 to compute upper bounds of learning algorithm.\n",
        "\n",
        "* [Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality) bounds probability that a random variable deviates from its expected value by more than a certain amount (k standard deviations at most 1/k^2).\n",
        "\n",
        "* [Rademacher Complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity):\n",
        "  * upper bound on sample complexity = on learnability of function classes (deriving generalization bounds). Measures ability of functions in class to fit to random noise. [Rademacher Complexity PDF](https://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf).\n",
        "  * When the Rademacher complexity is small, it is possible to learn the hypothesis class H using [empirical risk minimization](https://en.m.wikipedia.org/wiki/Empirical_risk_minimization).\n",
        "\n",
        "* [Gaussian complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity#Gaussian_complexity):  similar complexity to Rademacher. Can be obtained from Rademacher using random variables $g_i$ instead of $\\sigma_i$, where $g_i$ are Gaussian i.i.d. random variables with zero-mean and variance 1, i.e. $g_i \\sim \\mathcal{N}(0,1)$. Gaussian and Rademacher are equivalent up to logarithmic factors.\n",
        "\n",
        "* [Fano's inequality](https://en.m.wikipedia.org/wiki/Fano%27s_inequality): is a lower bound on the average probability of error in a multiple hypothesis testing problem. It states that the average probability of error in a multiple hypothesis testing problem is at least as high as the entropy of the hypothesis distribution divided by the natural logarithm of two. It can be used to: Design efficient multiple hypothesis testing algorithms. Lower bound the generalization error of learning algorithms. Develop new algorithms for information compression and transmission. Fano's inequality is a powerful tool for analyzing the performance of multiple hypothesis testing algorithms and other machine learning algorithms.\n",
        "\n",
        "* [Data processing inequality](https://en.m.wikipedia.org/wiki/Data_processing_inequality): is a concept that states that the information content of a signal cannot be increased via a local physical operation. This can be expressed concisely as 'post-processing cannot increase information'. It is an inequality that relates the mutual information between two random variables before and after a data processing channel. It states that the mutual information between two random variables after a data processing channel cannot be greater than the mutual information between the two random variables before the channel. It can be used to: prove that certain learning problems are impossible to solve perfectly.\n",
        "Design efficient algorithms for information compression and transmission. It is a powerful tool for analyzing the flow of information through systems."
      ],
      "metadata": {
        "id": "SeHmvo96bWlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A norm inequality (like Bohnenblust-Hille inequalities) is an inequality that involves the norms of vectors and matrices. Norms are functions that measure the size of vectors and matrices. There are many different types of norms, but some of the most common include the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "Norm inequalities are useful in many different areas of mathematics, including functional analysis, numerical linear algebra, and machine learning. For example, norm inequalities can be used to prove bounds on the error of numerical algorithms, to design efficient machine learning algorithms, and to analyze the stability of differential equations.\n",
        "\n",
        "Here are some examples of norm inequalities:\n",
        "\n",
        "* **Triangle inequality:** $\\|x + y\\| \\leq \\|x\\| + \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Cauchy-Schwarz inequality:** $\\|⟨x, y⟩\\| \\leq \\|x\\| \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Hölder's inequality:** $\\left\\| \\sum_{i=1}^n a_i b_i \\right\\| \\leq \\left\\| \\sum_{i=1}^n |a_i|^p \\right\\|^{1/p} \\left\\| \\sum_{i=1}^n |b_i|^q \\right\\|^{1/q}$, where $p$ and $q$ are conjugate exponents (i.e., $1/p + 1/q = 1$).\n",
        "* **Submultiplicativity of the Frobenius norm:** $\\|AB\\|_F \\leq \\|A\\|_F \\|B\\|_F$ for all matrices $A$ and $B$.\n",
        "\n",
        "Norm inequalities can be used to prove many other important results in mathematics. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations. The Cauchy-Schwarz inequality can be used to prove the Bessel inequality, which is a fundamental result in Fourier analysis. Hölder's inequality can be used to prove the Minkowski inequality, which is another important result in functional analysis.\n",
        "\n",
        "Norm inequalities are a powerful tool in mathematics, and they have many applications in science and engineering.\n",
        "\n",
        "*Yes, the triangle inequality is a norm inequality. The triangle inequality states that the norm of the sum of two vectors is less than or equal to the sum of the norms of the two vectors. In other words, for any two vectors $x$ and $y$, we have:*\n",
        "\n",
        "```\n",
        "||x + y|| <= ||x|| + ||y||\n",
        "```\n",
        "\n",
        "This inequality holds for any norm, including the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "The triangle inequality is a fundamental property of norms, and it has many important consequences. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations, and it can also be used to design efficient machine learning algorithms.\n",
        "\n",
        "The triangle inequality is also related to the triangle inequality in Euclidean geometry. The triangle inequality in Euclidean geometry states that the sum of the lengths of any two sides of a triangle must be greater than or equal to the length of the third side. This inequality can be proven using the triangle inequality for norms.\n",
        "\n",
        "In conclusion, the triangle inequality is a norm inequality that has many important applications in mathematics, science, and engineering."
      ],
      "metadata": {
        "id": "TyeMFHRnbWlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means that a generalization bound is uniform?**\n",
        "\n",
        "A generalization bound is said to be uniform if it holds for all hypotheses in a given hypothesis class, rather than just for a specific hypothesis. This is important because it allows us to make guarantees about the performance of our learning algorithm on any new data, regardless of the specific hypothesis that it learns.\n",
        "\n",
        "Uniform generalization bounds are typically derived using a technique called **Rademacher complexity**. Rademacher complexity is a measure of the complexity of a hypothesis class, and it can be used to bound the generalization error of any learning algorithm that uses that hypothesis class.\n",
        "\n",
        "Uniform generalization bounds are particularly useful for complex hypothesis classes, such as the class of all neural networks. These hypothesis classes have infinite VC-dimension, which makes it difficult to derive generalization bounds using traditional methods. However, uniform generalization bounds based on Rademacher complexity can be applied to any hypothesis class, regardless of its VC-dimension.\n",
        "\n",
        "Some examples of uniform generalization bounds include:\n",
        "\n",
        "* The generalization bound for uniformly stable learning algorithms\n",
        "* The generalization bound for overparameterized neural networks in kernel regimes\n",
        "\n",
        "Uniform generalization bounds are an important tool for understanding and analyzing the performance of machine learning algorithms. They allow us to make guarantees about the ability of our algorithms to generalize to new data, even when the hypothesis class is complex.\n",
        "\n",
        "Here is a simple analogy to help understand uniform generalization bounds:\n",
        "\n",
        "Imagine that you are trying to train a model to predict whether a given email is spam or not. You have a dataset of labeled emails, and you use this dataset to train a machine learning algorithm.\n",
        "\n",
        "Once the algorithm is trained, you want to be able to use it to predict the spam/ham label of any new email, even if that email is not in the training dataset.\n",
        "\n",
        "A uniform generalization bound tells you that the probability that your algorithm will make a mistake on any new email is less than a certain threshold. This threshold is independent of the specific email that you are trying to predict.\n",
        "\n",
        "In other words, a uniform generalization bound guarantees that your algorithm will generalize well to any new data, even if that data is very different from the training data."
      ],
      "metadata": {
        "id": "0QgIgipSbWlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Gates and Circuits*"
      ],
      "metadata": {
        "id": "UQgau4E3u8MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Gates and Gradients (Weyl chamber of canonical non-local 2-qubit gates)**\n",
        "\n",
        "* Video: [Quantum Gates and Gradients](https://www.youtube.com/watch?v=cobp2Sf5f3o&t=880s)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1691.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1692.png)\n",
        "\n",
        "In quantum computing, the Weyl chamber of canonical non-local 2-qubit gates is a geometric representation of the space of all non-local two-qubit gates. It is a three-dimensional tetrahedron, with each vertex representing a particular type of gate. The Weyl chamber is useful for understanding the relationships between different gates, and for identifying important classes of gates.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1694.png)\n",
        "\n",
        "The Weyl chamber is constructed from the canonical decomposition of non-local two-qubit gates. This decomposition involves representing each gate as a linear combination of three Pauli matrices (X, Y, and Z) acting on the two qubits. The coefficients of these matrices are called the canonical coordinates of the gate. The Weyl chamber is then defined as the set of all vectors in three-dimensional space whose canonical coordinates satisfy certain constraints.\n",
        "\n",
        "The vertices of the Weyl chamber correspond to the following four types of gates:\n",
        "\n",
        "* **Identity:** The identity gate, which does not change the state of the two qubits.\n",
        "\n",
        "* **CNOT:** The controlled-NOT gate, which flips the state of the target qubit if and only if the control qubit is in the state 1.\n",
        "\n",
        "* **Swap:** The swap gate, which exchanges the states of the two qubits.\n",
        "\n",
        "* **√Swap:** The square root of the swap gate, which is a gate that is halfway between the identity and the swap gate.\n",
        "\n",
        "The interior of the Weyl chamber consists of a much larger number of gates, including all of the other possible two-qubit gates that can be constructed from the Pauli matrices. These gates can be classified by their entanglement properties, their ability to generate certain types of states, and their performance in quantum algorithms.\n",
        "\n",
        "The Weyl chamber is a useful tool for understanding the structure of two-qubit quantum gates. It allows us to visualize the relationships between different gates, and to identify important classes of gates. The Weyl chamber can also be used to design new gates and algorithms.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1693.png)"
      ],
      "metadata": {
        "id": "o0J_Y6zgSRNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizer Circuits (Clifford Group & Pauli Gate)**"
      ],
      "metadata": {
        "id": "2wucAxpmEbW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Stabilizer_code\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Five-qubit_error_correcting_code"
      ],
      "metadata": {
        "id": "67c8czs0O86E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Linear Growth of Quantum Circuit Complexity](https://www.youtube.com/watch?v=McfxzoMw1Ig)\n",
        "\n",
        "Video: [Estimating the Fault Tolerant Cost of Classically Intractable Quantum Computations](https://www.youtube.com/watch?v=H1UdcAdw5kg)\n",
        "\n",
        "Video: [Crypto and Privacy Village 2021 Day 1](https://www.youtube.com/watch?v=8dcl9GTPebU)"
      ],
      "metadata": {
        "id": "Dq_P0-YlMDkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is the Pauli z operator a stabilizer for state |0>?**\n",
        "\n",
        "The Pauli-Z operator \\( Z \\) is given by:\n",
        "\n",
        "$\n",
        "Z = \\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & -1 \\\\\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "A stabilizer for a quantum state \\( $|\\psi\\rangle$ \\) is an operator \\( S \\) such that:\n",
        "\n",
        "$\n",
        "S |\\psi\\rangle = |\\psi\\rangle\n",
        "$\n",
        "\n",
        "That is, applying the stabilizer leaves the state unchanged.\n",
        "\n",
        "Now, let's see if the Pauli-Z operator is a stabilizer for the state \\( |0\\rangle \\). The state \\( |0\\rangle \\) is represented as:\n",
        "\n",
        "$\n",
        "|0\\rangle = \\begin{pmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "Applying the Pauli-Z operator to this state:\n",
        "\n",
        "$\n",
        "Z |0\\rangle = \\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & -1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\end{pmatrix}\n",
        "= |0\\rangle\n",
        "$\n",
        "\n",
        "Indeed, $Z |0\\rangle = |0\\rangle$, so the Pauli-Z operator is a stabilizer for the state $|0\\rangle$.\n",
        "\n",
        "The identity operator $I$ is also a stabilizer!"
      ],
      "metadata": {
        "id": "CKvI_-fHHlhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizer Circuits:**  *Generate stabilizer states (don't change eigenvalues of Pauli matrices), but can be efficienly simulated classically + are not universal. Sometimes are transversal (=automatically fault-tolerant).*\n",
        "\n",
        "  * **Group Generators and Stabilizer States**:\n",
        "\n",
        "    * [Clifford group](https://en.m.wikipedia.org/wiki/Clifford_gates) consists of a set of n-qubit operations generated by three gates: CNOT, H and S. [Construct a single qubit gate Pauli gates X, Y, Z from S, H and Pauli gates](https://quantumcomputing.stackexchange.com/questions/29035/constructing-a-single-qubit-gate-from-s-h-and-pauli-gates)\n",
        "\n",
        "    * **The Clifford Group is the group of operators which maps the Pauli Group {I, X, Y, Z} onto itself**\n",
        "\n",
        "      * The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse): I^2 = X^2 = Y^2 = Z^2 = -i X Y Z = I\n",
        "\n",
        "      * The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example Z X = i Y = -X Z\n",
        "\n",
        "      * The [rotation operators](https://en.m.wikipedia.org/wiki/List_of_quantum_logic_gates#Rotation_operator_gates) Rx(θ), Ry(θ), Rz(θ), the [phase shift gate](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#Phase_shift_gates) P(φ) and [CNOT](https://en.m.wikipedia.org/wiki/Quantum_logic_gate#CNOT) form a universal set of quantum gates [Source](https://en.m.wikipedia.org/wiki/Quantum_logic_gate) (due to phase shift it is universal)\n",
        "\n",
        "    * **Stabilizer states: Clifford gates \"preserve\" Pauli group: when you apply a Clifford gate to a Pauli operator, the result is another Pauli operator. Clifford group generates [stabilizer states](https://en.m.wikipedia.org/wiki/Stabilizer_code)** or stabilizer circuits = circuits that only consist of gates from the normalizer of the qubit Pauli group = CNOT, Hadamard, and phase gate.\n",
        "\n",
        "    * Clifford gates can be efficiently implemented in many physical systems = they are NOT expensive (like T gate) = based on surface code\n",
        "\n",
        "    * Clifford-group elements take stabilizer groups to stabilizer groups and thus stabilizer states to stabilizer states. A Clifford unitary can be multiplied by a phase without changing how it conjugates operators. http://info.phys.unm.edu/~caves/reports/stabilizer.pdf. https://arxiv.org/pdf/1603.03999.pdf. Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)\n",
        "  \n",
        "  * **Bad thing**: Stabilizer circuits can be efficiently and perfectly simulated classically according to the [Gottesman–Knill theorem](https://en.m.wikipedia.org/wiki/Gottesman%E2%80%93Knill_theorem) (in polynomial time on a probabilistic classical computer).\n",
        "\n",
        "    1. Preparation of qubits in computational basis states,\n",
        "    2. Clifford gates (Hadamard gates, controlled NOT gates, phase gate S ), and\n",
        "    3. Measurements in the computational basis.\n",
        "\n",
        "  * **Solution**: Because there are quantum operations that don't preserve the Pauli group, we need non-Clifford gates to achieve universal quantum computing like tha T-gate). [Magic States](https://en.m.wikipedia.org/wiki/Magic_state_distillation) can help to implement them efficiently. <font color=\"red\"> Note: why can't you simulate a T-gate in polynomial time?\n",
        "\n",
        "    * Universal Quantum Gates = Any quantum operation can be constructed to any desired level of precision.\n",
        "\n",
        "    * You need: Clifford gate (H, S, CNOT) +  Non Clifford gate (T). Alternatively: Toffoli gate + H. Alternatively: Single-qubit rotation gates (Pauli gates) + CNOT gate + Phase shift (incl. Z, S, T gate).\n",
        "\n",
        "    * Classically: One can show that the AND, NOT and OR gates form a universal set which means that any logical operation on a arbitrary number of qubits can be expressed in terms of these gates. Thus, classically, we only need 1-bit and 2-bit gates to perform any operation. https://young.physics.ucsc.edu/150/gates.pdf"
      ],
      "metadata": {
        "id": "KXLuIdIBcD7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synthesizing**\n",
        "* a rotation (with a T gate): find interatively a sequence of T gates that can be used to implement a desired rotation with minimum number of T gates for a given rotation.\n",
        "* With T-count algorithm in polynomial-time to optimize circuit and make it mofe efficient + fault-tolerant. Example, the rotation that takes the state |0⟩ to the state |+⟩ can be synthesized with a single T gate.\n",
        "* https://www.nature.com/articles/s41534-022-00651-y\n"
      ],
      "metadata": {
        "id": "zm-zEdmqu2oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pauli Gates*"
      ],
      "metadata": {
        "id": "0LWkHsu1_r51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/quantum-untangled/visualizing-quantum-logic-gates-part-1-515bb7b58916\n",
        "\n",
        "Nice visualisations: https://medium.com/analytics-vidhya/quantum-gates-7fe83817b684\n",
        "\n",
        "https://pme.uchicago.edu/awschalom-group/all-optical-holonomic-single-qubit-gates\n"
      ],
      "metadata": {
        "id": "PGcrDhdstYmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-X Gate (Flip Computational States)**\n",
        "\n",
        "> $X=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\sigma_{x}=\\mathrm{NOT}$\n",
        "\n",
        "* The Pauli- $X$ gate is the quantum equivalent of the [NOT gate](https://en.m.wikipedia.org/wiki/Inverter_(logic_gate)) for classical computers (its main function is to invert the input signal applied) with respect to the standard basis $|0\\rangle,|1\\rangle$, which distinguishes the $z$ axis on the Bloch sphere. It is sometimes called a bit-flip as it maps $|0\\rangle$ to $|1\\rangle$ and $|1\\rangle$ to $|0\\rangle .$\n",
        "\n",
        "* The Pauli gates $(X, Y, Z)$ are the three Pauli matrices $\\left(\\sigma_{x}, \\sigma_{y}, \\sigma_{z}\\right)$ and act on a single qubit. The Pauli $X_{1} Y$ and $Z$ equate, respectively, to a rotation around the $x, y$ and $z$ axes of the Bloch sphere by $\\pi$ radians.\n",
        "\n",
        "* Nice visualisations: https://medium.com/analytics-vidhya/quantum-gates-7fe83817b684\n",
        "\n"
      ],
      "metadata": {
        "id": "zDXxSuLakwyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root of Pauli-X Gate**\n",
        "\n",
        "The square root of NOT gate (or square root of Pauli- $X, \\sqrt{X}$ ) acts on a single qubit. It maps the basis state $|0\\rangle$ to $\\frac{(1+i)|0\\rangle+(1-i)|1\\rangle}{2}$ and $|1\\rangle$ to $\\frac{(1-i)|0\\rangle+(1+i)|1\\rangle}{2} .$\n",
        "\n",
        "In matrix form it is given by\n",
        "\n",
        ">$\n",
        "\\sqrt{X}=\\sqrt{\\mathrm{NOT}}=\\frac{1}{2}\\left[\\begin{array}{cc}\n",
        "1+i & 1-i \\\\\n",
        "1-i & 1+i\n",
        "\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}\n",
        "e^{i \\pi / 4} & e^{-i \\pi / 4} \\\\\n",
        "e^{-i \\pi / 4} & e^{i \\pi / 4}\n",
        "\\end{array}\\right]$\n",
        "\n",
        "such that\n",
        "\n",
        ">$\n",
        "(\\sqrt{X})^{2}=(\\sqrt{\\mathrm{NOT}})^{2}=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right]=X\n",
        "$\n",
        "\n",
        "> *This operation represents a rotation of $\\pi / 2$ about $x$ -axis at the Bloch sphere*."
      ],
      "metadata": {
        "id": "PeMSkgSzlzsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Y Gate (Phase Flip between i and -i)**\n",
        "\n",
        "> $Y=\\sigma_{y}=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right]$\n",
        "\n",
        "* Similarly, the Pauli- $Y$ maps $|0\\rangle$ to $i|1\\rangle$ and $|1\\rangle$ to $-i|0\\rangle$ (NOT gate with i-multiple):\n"
      ],
      "metadata": {
        "id": "htDSfNdzJulw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-Z Gate ($\\pi$ Flip Phase between +1 and -1)**\n",
        "\n",
        "> $Z=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]=\\sigma_{z}\\quad Z=|0\\rangle\\langle 0|-| 1\\rangle\\langle 1|$\n",
        "\n",
        "* Pauli Z gate is a phase flip gate that causes rotation around the z-axis by π radians. Z flippt zwischen den Polen der X Achse (+ und -).\n",
        "\n",
        "* *Z Gate flippt zwischen den Hadamard gegenüberliegenden Richtungen auf der X-Achse. We change the phase (or sign) on a Qubit*:\n",
        "\n",
        "> $\\mathbf{Z}|0\\rangle=|0\\rangle$\n",
        "\n",
        "> $\\mathbf{Z}|1\\rangle=-|1\\rangle$\n",
        "\n",
        "* Since |0⟩ and |1⟩ lie on the z-axis, the Z-gate will not affect these states. To put it in other terms *|0⟩ and |1⟩ are the two eigenstates of the Z-gate*. On the other hand, it flips |+⟩ to |-⟩ and |-⟩ to |+⟩.\n",
        "\n",
        "* Pauli $Z$ leaves the basis state $|0\\rangle$ unchanged and maps $|1\\rangle$ to $-|1\\rangle$. Due to this nature, it is sometimes called *phase-flip* (flips sign of second entangled state)\n",
        "\n",
        "* *The Z gate is like the X gate but in the hadamard basis, flipping states*"
      ],
      "metadata": {
        "id": "qj2bMNDwLoDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli-S Gate: $\\frac{\\pi}{2}$ Flip Phase between Z and Y (with $\\mu$=i and $\\nu$=-i)**\n",
        "\n",
        "> ${S}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & \\sqrt{-1}\\end{array}\\right]=\\sqrt{\\mathbf{Z}}$\n",
        "\n",
        "* The phase gate $\\mathbf{Z}$ transforms $|+\\rangle$ to $|-\\rangle$, and we can find the gate that does \"half of\" this transformation by finding the square root of the matrix $\\mathbf{Z}$\n",
        "\n",
        "* fintuning Z gate in der Hadamard basis: S-gate is die Hälfte vom Z-Gate, und R-Gate ein viertel vom Z-Gate\n",
        "\n",
        "* Use the matrix $\\mathbf{S}$ to find the state $|\\mu\\rangle$ which is \"halfway\" between $|+\\rangle$ and $|-\\rangle$ :\n",
        "\n",
        "> $\\mathbf{S}|+\\rangle=|\\mu\\rangle =\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "If you begin with $|+\\rangle$ and apply the $\\mathbf{S}$ gate three times in a row, you find a new state, $|\\nu\\rangle$ which appears to mirror the complex state $|\\mu\\rangle$ :\n",
        "\n",
        "> $|\\mu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+i|1\\rangle)$\n",
        "\n",
        "> $|\\nu\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-i|1\\rangle) .$"
      ],
      "metadata": {
        "id": "Frcfbnlgr6d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already uncovered the gates we need to perform any rotations around the $x$ - and $z$-axes, which can bring us from an initialized qubit to any other quantum state.\n",
        "\n",
        "Unfortunately, this result comes with a pretty serious caveat: all of the gates we have used so far are discrete. They perform rotations around the Bloch sphere, but since they rotate in discrete hops (of $\\pi / 2$ or $\\pi$ ), they are unable to reach most of the intermediate states on the surface of the sphere.\n",
        "\n",
        "One solution to this problem is to define smaller and smaller rotations around these axes. The gate $\\mathbf{S}$ is equal to $\\sqrt{\\mathbf{Z}}$ and halves its rotation angle from $\\pi$ to $\\pi / 2$. Even greater division of these gates is possible, and some of them have\n",
        "common names:\n",
        "\n",
        "$\\mathbf{Z}=\\mathbf{Z}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]$ Rotation: $\\pi$\n",
        "\n",
        "$\\mathbf{S}=\\sqrt[2]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & i\\end{array}\\right]$ Rotation: $\\pi / 2$\n",
        "\n",
        "\n",
        "$\\mathbf{T}=\\sqrt[4]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 4}\\end{array}\\right] \\quad$ Rotation: $\\pi / 4$ not clifford\n",
        "\n",
        "$\\mathbf{R 8}=\\sqrt[8]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{i \\pi / 8}\\end{array}\\right] \\quad$ Rotation: $\\pi / 8 .$ not clifford"
      ],
      "metadata": {
        "id": "6jspQ2kndBZg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjqz8Hkol68T"
      },
      "source": [
        "**Swap Gate**\n",
        "\n",
        "*2 Qubits - Swap Gate (& Swap Square root): Swap two Qubits (like in Quantum Fourier Transform)*\n",
        "\n",
        "The swap gate **swaps two qubits**.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\text { SWAP }=\\left[\\begin{array}{llll}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzlOABhWmMms"
      },
      "source": [
        "**Square root of swap gate**\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate performs half-way of a two-qubit swap.\n",
        "\n",
        "It is universal such that any many-qubit gate can be constructed from only $\\sqrt{\\text { SWAP }}$ and single qubit gates.\n",
        "\n",
        "The $\\sqrt{\\text { SWAP }}$ gate is not, however maximally entangling; more than one application of it is required to produce a Bell state from product states.\n",
        "\n",
        "With respect to the basis $|00\\rangle,|01\\rangle,|10\\rangle,|11\\rangle$, it is represented by the matrix:\n",
        "\n",
        ">$\n",
        "\\sqrt{\\mathrm{SWAP}}=\\left[\\begin{array}{cccc}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & \\frac{1}{2}(1+i) & \\frac{1}{2}(1-i) & 0 \\\\\n",
        "0 & \\frac{1}{2}(1-i) & \\frac{1}{2}(1+i) & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "This gate arises naturally in systems that exploit exchange interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transversal Gate (Eastin–Knill theorem)**"
      ],
      "metadata": {
        "id": "Z5VmP1XnIHWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arthurpesah.me/blog/2023-12-25-transversal-gates/"
      ],
      "metadata": {
        "id": "vzCwuL1sZypU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transversality**\n",
        "\n",
        "* **Transversal gates**: transversal is \"the logical operation is achieved by broadcasting the same operation over the physical qubits\" [Source](https://quantumcomputing.stackexchange.com/questions/24269/what-is-formally-a-transversal-operator). Are automatically fault-tolerant.\n",
        "\n",
        "* Non-Clifford gates (like T-gate) can NOT be implemented transversally in a fault-tolerant manner in any quantum error-correcting code, see [Eastin-Knill theorem](https://en.m.wikipedia.org/wiki/Eastin%E2%80%93Knill_theorem). Because Set of transversal gates is finite group. **A finite group - unlike a finite set - cannot approximate an infinite set of all unitaries arbitrarily well**. [Source](https://quantumcomputing.stackexchange.com/questions/28057/how-to-intuitively-understand-why-t-gate-cant-be-implemented-transversally). So you need at least one non transversal / non-Clifford gate for universality.\n",
        "  \n",
        "* non-transversal gates have higher depth (dominant contributer to fault-tolerant threshold).  Goal: Simplify these implementations. [Source](https://quantumcomputing.stackexchange.com/questions/21580/why-do-we-care-about-the-number-of-t-gates-in-a-quantum-circuit) with magic state distillation.\n",
        "\n",
        "* [Transversal logical gate for Stabilizer (or at least Steane code)](https://quantumcomputing.stackexchange.com/questions/15301/transversal-logical-gate-for-stabilizer-or-at-least-steane-code) and [errorcorrectionzoo](https://errorcorrectionzoo.org/list/quantum_transversal)"
      ],
      "metadata": {
        "id": "IWKsUajQyA1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1639.png)"
      ],
      "metadata": {
        "id": "i6wGbXqJJ1aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/0811.4262\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Eastin%E2%80%93Knill_theorem"
      ],
      "metadata": {
        "id": "OhGA2GT2IKkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic State Distillation**"
      ],
      "metadata": {
        "id": "gq1Pg2IZkUmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distillation & Magic States** *Some non-Clifford gates are magic states, allow for universal quantum computing*\n",
        "  \n",
        "  * Magic states are specific quantum states that can be used to implement non-Clifford gates (like T-gate) in a fault-tolerant way. A non-Clifford gate, such as the T-gate. All magic states are non-Clifford gates, but not all non-Clifford gates are magic states.\n",
        "\n",
        "  * [Magic States](https://en.m.wikipedia.org/wiki/Magic_state_distillation) are non-Clifford gates that can be used to efficiently implement Clifford gates and perform any quantum algorithm. They are not stabilizer states (not invariant under the action of the Pauli matrices = resulting state will have different eigenvalues) and have very nice properties with respect to fault-tolerant quantum computation. <font color=\"red\"> Note: what are these properties?</font>  [what-are-magic-states](https://quantumcomputing.stackexchange.com/questions/13629/what-are-magic-states)\n",
        "  \n",
        "  * Magic States are a key component of the [\"magic state **distillation**\"](https://en.m.wikipedia.org/wiki/Magic_state_distillation) protocol, a technique used in quantum error correction. Then, using a procedure called **gate injection or state injection**, these magic states can be used to effectively implement non-Clifford gates in a fault-tolerant manner. This is done by preparing the magic state, performing some Clifford operations and measurements (which can be done fault-tolerantly), and using the outcomes to adaptively apply further Clifford operations.\n",
        "    \n",
        "  * **Distillation**: This protocol involves preparing many noisy copies of a magic state, then performing a series of operations to \"distill\" these into fewer, higher-fidelity magic states. Distillation: purifying a quantum state by combining multiple copies of the state. Goal is to produce a state with high fidelity (close to desired state, e.g. closer to ideal T gate than the input states). T gate can be distilled efficiently (relative to alternatives) because it is a **magic state** = there are a number of different distillation protocols that have high yields (high efficiency).\n",
        "      \n",
        "  * https://en.m.wikipedia.org/wiki/Five-qubit_error_correcting_code\n",
        "\n",
        "  * https://www.nature.com/articles/s41534-022-00666-5\n",
        "\n",
        "**quantum gate injection or magic state injection**\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/28481/is-this-kind-of-t-injection-correct\n",
        "\n",
        "**lattice surgery**: Lattice surgery is a method to perform quantum computation fault-tolerantly by using operations on boundary qubits between different patches of the planar code.\n"
      ],
      "metadata": {
        "id": "VnRYxkP9diSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It's a kind of magic.**\n",
        "\n",
        "Quantum states can be much less magical than they computationally appear to be.\n",
        "\n",
        "*A little magic means a lot*\n",
        "\n",
        "https://arxiv.org/abs/2308.16228\n",
        "\n",
        "Notions of so-called #magic quantify how non-classical #quantumstates are in a precise sense: high values of magic are necessary to demonstrate a quantum advantage in #quantumcomputing; they also play a key role in #quantumerrorcorrection.\n",
        "\n",
        "In this work, we introduce the phenomenon of `#pseudomagic' - wherein certain ensembles of quantum states with low magic are computationally indistinguishable from quantum states with high magic. Previously, such computational indistinguishability has been studied with respect to entanglement, by introducing the notion of #pseudoentanglement. However, we show that pseudomagic neither follows from pseudoentanglement, nor implies it.\n",
        "\n",
        "In terms of applications, pseudomagic sheds new light on the theory of #quantumchaos: it reveals the existence of states that, although built from non-chaotic unitaries, cannot be distinguished from #randomchaoticstates by any physical observer. Further applications include new lower bounds on #statesynthesis problems, #propertytesting protocols, as well as implications for #quantumcryptography.\n",
        "\n",
        "Our results have the conceptual implication that magic is a `hide-able' property of quantum states: some states have a lot more magic than meets the (computationally-bounded) eye. From the physics perspective, it advocates the mindset that the only physical properties that can be measured in a laboratory are those that are efficiently computationally detectable."
      ],
      "metadata": {
        "id": "Q4RHws1hTcz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**can you give me an example of magic state distillation in quantum computing?**\n",
        "\n",
        "Certainly! Magic state distillation is a protocol used to produce fault-tolerant, high-fidelity \"magic states\" from noisy versions of those states. These magic states can be used to perform non-Clifford gates, which are essential for universal quantum computation but are generally not directly implementable in a fault-tolerant manner.\n",
        "\n",
        "The core idea is to take multiple copies of a noisy magic state, process them together using quantum operations, and produce a smaller number of less-noisy magic states.\n",
        "\n",
        "Here's a simplified outline of the process using the $|T\\rangle$ state, a commonly used magic state:\n",
        "\n",
        "1. **Preparation**: Prepare multiple copies of the noisy $|T\\rangle$ state:\n",
        "$|T\\rangle = \\cos(\\frac{\\pi}{8})|0\\rangle + e^{i \\frac{\\pi}{4}}\\sin(\\frac{\\pi}{8})|1\\rangle$ - (Stimmt das?)\n",
        "\n",
        "2. **Encoding**: Encode these states using a specially designed quantum error-correcting code. For our purposes, think of this as a way to \"spread out\" the errors.\n",
        "\n",
        "3. **Verification**: Perform a quantum error-detection procedure on the encoded states. If the procedure detects an error, the states are discarded, and the process starts over.\n",
        "\n",
        "4. **Decoding**: If no error is detected, decode the protected information to produce a fewer number of $|T\\rangle$ states that have a higher fidelity than the initial states.\n",
        "\n",
        "The process can be iteratively repeated, using the distilled states as the input for another round of distillation until the desired fidelity is achieved.\n",
        "\n",
        "While this is a conceptual overview, the real magic state distillation involves a deeper dive into the specifics of the error-correcting codes and quantum circuits used for encoding, verification, and decoding. Researchers have proposed various protocols to optimize the trade-offs in terms of the number of initial states needed, the final fidelity achieved, and the overhead of the quantum operations."
      ],
      "metadata": {
        "id": "Ns9hFthVJwa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(2004) Fault-Tolerant Postselected Quantum Computation: Schemes (**Magic State Distillation**)*"
      ],
      "metadata": {
        "id": "AGZAnbN8AI5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/quant-ph/0402171\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0403025\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Magic_state_distillation#:~:text=Magic%20state%20distillation%20is%20a,contribute%20to%20quantum%20computers'%20power."
      ],
      "metadata": {
        "id": "y4KmoFxr5d-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1209.2426"
      ],
      "metadata": {
        "id": "zh2Dim8aNpBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Magic state distillation is a process in quantum computing which aims to produce high-quality, or \"magic\", quantum states from imperfect ones, thus helping to correct errors that arise due to the imperfections of physical qubits and quantum gates. This method is used to implement non-Clifford gates, which are a crucial building block for universal quantum computation, but are particularly prone to noise and errors.\n",
        "\n",
        "The term \"distillation\" is used to evoke the idea of refining or purifying a substance, just like in chemistry. In this context, multiple imperfect copies of a quantum state are combined and processed to produce fewer copies with higher fidelity.\n",
        "\n",
        "Magic state distillation is an important part of some quantum error correction schemes, such as the surface code. These schemes are key to creating reliable and scalable quantum computers. However, as of my knowledge cutoff in 2021, practical, large-scale implementations of magic state distillation were yet to be realized, as the process is quite resource-intensive.\n"
      ],
      "metadata": {
        "id": "pcd2jrpe5oWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic States**\n",
        "\n",
        "* if we have access to certain specific quantum states, known as magic states, then even with only Clifford gates, we can perform universal quantum computation. This is done by using these magic states as resources in a specific type of quantum protocol called magic state distillation.\n",
        "\n",
        "* These magic states are especially important in quantum error correction because they are resistant to certain types of quantum noise and can be \"distilled\" to produce high-fidelity magic states even when we start with imperfect copies. This makes them very useful in designing fault-tolerant quantum computers, as they can help us overcome the errors that inevitably occur in any real-world quantum system.\n",
        "\n",
        "Magic states are purified from n copies of a mixed state $\\rho$ . These states are typically provided via an ancilla to the circuit. A magic state for the $T$ gate is ${\\displaystyle |M\\rangle =\\cos(\\beta /2)|0\\rangle +e^{i{\\frac {\\pi }{4}}}\\sin(\\beta /2)|1\\rangle }$ where ${\\displaystyle \\beta =\\arccos \\left({\\frac {1}{\\sqrt {3}}}\\right)}$. By combining (copies of) magic states with Clifford gates, can be used to make a non-Clifford gate\n",
        "\n",
        "*The first magic state distillation algorithm, invented by Sergey Bravyi and Alexei Kitaev, is a follows.*\n",
        "\n",
        "* Input: Prepare 5 imperfect states.\n",
        "* Output: An almost pure state having a small error probability.\n",
        "* repeat\n",
        "  * Apply the decoding operation of the five-qubit error correcting code and measure the syndrome.\n",
        "  * If the measured syndrome is ${\\displaystyle |00000\\rangle }$, the distillation attempt is successful.\n",
        "  * else Get rid of the resulting state and restart the algorithm.\n",
        "* until The states have been distilled to the desired purity."
      ],
      "metadata": {
        "id": "9_FS5V9hlPyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Magic state distillation**](https://en.m.wikipedia.org/wiki/Magic_state_distillation) is a procedure in quantum computation that is used to create high-fidelity quantum states, starting from several copies of a \"noisy\" state. The name \"distillation\" is used because it's similar to the idea of distilling a liquid to increase its purity.\n",
        "\n",
        "The technique was first proposed by Emanuel Knill in 2004, and further analyzed by Sergey Bravyi and Alexei Kitaev the same year.\n",
        "\n",
        "Here's a high-level overview of how it works:\n",
        "\n",
        "1. **Preparation**: Prepare several copies of a quantum state. These states are typically \"noisy\" copies of a desired magic state, meaning they have some errors or deviations from the perfect magic state.\n",
        "\n",
        "2. **Error Detection**: Apply a quantum error-detecting code to these copies. This code is a series of quantum operations that can identify (but not correct) certain errors in the quantum states.\n",
        "\n",
        "3. **Measurement**: Measure the error-detecting code. This does not directly measure the quantum states themselves but measures whether an error has been detected.\n",
        "\n",
        "4. **Post-selection**: If the measurement indicates that no errors were detected, then (due to the properties of quantum mechanics) the quantum states are projected into a state that is closer to the perfect magic state than the original states were. If an error was detected, discard the states and start over.\n",
        "\n",
        "5. **Repeat**: Repeat this process several times. Each time, the states get closer to the perfect magic state.\n",
        "\n",
        "By repeating this process, one can \"distill\" a set of noisy magic states into a smaller number of higher-fidelity magic states. This is a crucial part of many fault-tolerant quantum computing schemes, as it allows one to perform universal quantum computation even in the presence of noise."
      ],
      "metadata": {
        "id": "iFeGkcT-lTDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T-Gate and Toffoli Gate**"
      ],
      "metadata": {
        "id": "gLYqKHKB_yS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Jens Eisert | September 27, 2022 | A Single T-Gate Makes Quantum Distribution Learning Hard](https://www.youtube.com/watch?v=HAKPulM08hs)"
      ],
      "metadata": {
        "id": "K6ehfiAJL4v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is so special about the T-gate?**\n",
        "\n",
        "* non-Clifford gate, majority of non-Clifford gates used, most expensive gate (based on surface code), for universal quantum gate, controls phase / rotation (phase shift) by $\\frac{i \\pi}{4}$ around Z-axis\n",
        "* [Craig Gidney of why T-gates are best option](https://quantumcomputing.stackexchange.com/questions/33357/universal-gate-set-magic-states-and-costliness-of-the-t-gate/33358#33358)\n",
        "* [Can someone explain to me what is T gate and tdg like im five?](https://quantumcomputing.stackexchange.com/questions/26666/can-someone-explain-to-me-what-is-t-gate-and-tdg-like-im-five)\n",
        "* [Why do we care about the number of 𝑇 gates in a quantum circuit?](https://quantumcomputing.stackexchange.com/questions/21580/why-do-we-care-about-the-number-of-t-gates-in-a-quantum-circuit)\n",
        "\n",
        "> $\\mathbf{T}=\\sqrt[4]{\\mathbf{Z}}=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & e^{\\frac{i \\pi}{4}}\\end{array}\\right]$\n",
        "\n",
        "> Applying it: $\n",
        "T|1\\rangle=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & e^{\\frac{i \\pi}{4}}\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=e^{\\frac{i \\pi}{4}}|1\\rangle\n",
        "$ Since QPE will give us $\\theta$ where: $\n",
        "T|1\\rangle=e^{2 i \\pi \\theta}|1\\rangle\n",
        "$ we expect to find theta: $\n",
        "\\theta=\\frac{1}{8}\n",
        "$\n",
        "\n",
        "*Paper [Universal Quantum Computation with ideal Clifford gates and noisy  ancillas](https://arxiv.org/abs/quant-ph/0403025): check out figure 1 that provides a nice visualization of two types of magic states and also definition 1 on page 3. confusingly, T state is the \"H-type state\" and not the \"T-type state\" in their lingo (since the density matrix of the T state is (I+1/sqrt2(X+Y))/2) - Yes, it seems to me that in the old days folks named these states by the operators of which the states were the eigenvectors (rather than by the gate you could realize using the states as we do today). the caption of figure 1 undersells its significance. the octahedron consists of states which don't boost computational power of the Clifford gateset (see Theorem 1 on the same page). it's the high-purity states outside the octahedron that are useful. I mean isn't the eigenstate the more sensible naming scheme?  Old man shakes cane. T |T> = t |T>. +1 to naming things for what they are instead of for what you can do with them (as that is subject to change)*"
      ],
      "metadata": {
        "id": "lYtm8-hjDwuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is so special about the Toffoli-gate?**\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Qcircuit_ToffolifromCNOT.svg/799px-Qcircuit_ToffolifromCNOT.svg.png)\n",
        "\n",
        "* construct a quantum mechanical Toffoli gate out of 1-qubit and 2-qubit gates.\n",
        "* [Toffoli gate](https://en.m.wikipedia.org/wiki/Toffoli_gate#), a reversible gate, and is universal in classical computing - can simulate classical computation (https://youtu.be/qs0D9sdbKPU?t=6520) - essential tool for building big classical operations like multipliers and [multiplexers](https://en.m.wikipedia.org/wiki/Multiplexer)\n",
        "* It is used in the Shor algorithm, quantum error correction, fault-tolerant computation and quantum arithmetic operations (https://www.nature.com/articles/npjqi201619)\n",
        "* In many of Google's papers talk about directly distilling Toffoli rather T = Toffoli has many clear connections to classical logic and is a more natural gate when your algorithm is bottlenecked by classical logic rather than single qubit rotations\n",
        "* [Are Toffoli gates actually used in designing quantum circuits?](https://quantumcomputing.stackexchange.com/questions/11915/are-toffoli-gates-actually-used-in-designing-quantum-circuits)\n",
        "* [A Simple Proof that Toffoli and Hadamard are Quantum Universal](https://arxiv.org/abs/quant-ph/0301040)"
      ],
      "metadata": {
        "id": "XPdY_aRRVpZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hadamard Gate**"
      ],
      "metadata": {
        "id": "Ca3fvYOc_oY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Logic Gates Rotate Qubits](https://www.youtube.com/watch?v=ZBaXPY_0TNI)\n",
        "\n",
        "Video: [Qubits vs Bits: The Kickback Effect](https://www.youtube.com/watch?v=EjdngeGXWEg)"
      ],
      "metadata": {
        "id": "d6OTWQs8Lcu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hadamard Gate (Superposition)*\n",
        "\n",
        "> $H=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)$\n",
        "\n",
        "**The Hadamard states ∣+⟩ and ∣−⟩ are considered superposition states**\n",
        "\n",
        "because they are a combination of the two computational states:\n",
        "\n",
        "> $|\\pm\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\pm \\frac{1}{\\sqrt{2}}|1\\rangle$\n",
        "\n",
        "**Apply Hadamard gate on a qubit that is in the |0> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "**Now apply Hadamard gate on a qubit that is in the |1> state**:\n",
        "\n",
        "> <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "The qubit enters a new state where the probability of measuring 0 is:\n",
        "\n",
        "* $\\left(\\frac{1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "And the probability of measuring 1 is also:\n",
        "\n",
        "* $\\left(\\frac{-1}{\\sqrt{2}}\\right)^{2}=\\frac{1}{2}$\n",
        "\n",
        "Hence, in both cases (qubit |0> or qubit |1>) applying a Hadamard Gate gives an equal chance for the qubit to be 0 or 1' when measured.\n",
        "\n",
        "Source: https://freecontent.manning.com/all-about-hadamard-gates/\n",
        "\n",
        "**Herleitung Hadamard (Wichtig!)**\n",
        "\n",
        "For an equal (or uniform) superposition of the two computational states, we can set the two coefficients equal to each other:\n",
        "\n",
        "$a_{1}=a_{2}=a$\n",
        "\n",
        "The normalization condition for a well-behaved quantum state requires that the sum of the squared magnitudes of the coefficients be equal to one; this is sufficient to find\n",
        "a\n",
        "a for a uniform superposition:\n",
        "\n",
        "$|a|^{2}+|a|^{2}=1$\n",
        "\n",
        "$2|a|^{2}=1$\n",
        "\n",
        "$|a|^{2}=\\frac{1}{2}$\n",
        "\n",
        "$a=\\frac{1}{\\sqrt{2}}$\n",
        "\n",
        "In vector form, this state can represented as\n",
        "\n",
        "> $\\left[\\begin{array}{l}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right]$ = $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right]$ + $\\frac{1}{\\sqrt{2}}$ $\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right]$\n",
        "\n",
        "This is what we can use now to understand Hadamard, where you want \"halfway\" a 50/50 chance of basis states 0 and 1 (Bloch sphere representation of superposition state):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_025.jpg)\n",
        "\n",
        "*Hadamard gate operations:*\n",
        "\n",
        "> $\\begin{aligned} H(|0\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle=:|+\\rangle \\\\ H(|1\\rangle) &=\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle=:|-\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)+\\frac{1}{2}(|0\\rangle-|1\\rangle)=|0\\rangle \\\\ H\\left(\\frac{1}{\\sqrt{2}}|0\\rangle-\\frac{1}{\\sqrt{2}}|1\\rangle\\right) &=\\frac{1}{2}(|0\\rangle+|1\\rangle)-\\frac{1}{2}(|0\\rangle-|1\\rangle)=|1\\rangle \\end{aligned}$\n",
        "\n",
        "One application of the Hadamard gate to either a 0 or 1 qubit will produce a quantum state that, if observed, **will be a 0 or 1 with equal probability** (as seen in the first two operations). This is exactly like flipping a fair coin in the standard probabilistic model of computation. However, if the Hadamard gate is applied twice in succession (as is effectively being done in the last two operations), then the final state is always the same as the initial state (because quantum operations are reversable, unlike operations on classical computers)."
      ],
      "metadata": {
        "id": "_T8yeOt7mKxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Noise**"
      ],
      "metadata": {
        "id": "xMyCxxxgo5T2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is Laplacian noise in the context of quantum computing and quantum machine learning?**\n",
        "\n",
        "Laplacian noise is a type of noise that is often used to model the effects of decoherence in quantum computing. Decoherence is the loss of quantum coherence due to interactions with the environment.\n",
        "\n",
        "Laplacian noise is a non-Gaussian noise, which means that it does not follow a normal distribution. Instead, it follows a Laplace distribution, which is a bell-shaped curve with heavier tails than a normal distribution.\n",
        "\n",
        "The Laplace noise is characterized by a parameter called the scale parameter, which controls the amount of noise. The larger the scale parameter, the more noise there is.\n",
        "\n",
        "Laplacian noise can be added to quantum states to simulate the effects of decoherence. This can be done by adding a random variable to each qubit in the state, where the random variable is drawn from a Laplace distribution.\n",
        "\n",
        "Laplacian noise can also be used to design quantum machine learning algorithms that are robust to noise. For example, Laplacian noise can be used to regularize quantum neural networks, which can help to improve their performance.\n",
        "\n",
        "Here are some of the key properties of Laplacian noise:\n",
        "\n",
        "* It is a non-Gaussian noise.\n",
        "* It follows a Laplace distribution.\n",
        "* It is characterized by a scale parameter.\n",
        "* It can be used to simulate the effects of decoherence in quantum computing.\n",
        "* It can be used to design quantum machine learning algorithms that are robust to noise.\n",
        "\n",
        "Laplacian noise is a versatile tool that can be used to study the effects of noise in quantum computing and quantum machine learning. It is a promising area of research with the potential to improve the performance of quantum computers and quantum machine learning algorithms."
      ],
      "metadata": {
        "id": "mgs7G5vRo-ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What kinds of noise can you add to a quantum computing circuit?**\n",
        "\n",
        "There are many different kinds of noise that can be added to a quantum computing circuit. Some of the most common types of noise include:\n",
        "\n",
        "* **Bit flip noise:** This is the most common type of noise. It occurs when a qubit is flipped from the |0⟩ state to the |1⟩ state or vice versa.\n",
        "* **Phase flip noise:** This type of noise occurs when the phase of a qubit is flipped. This means that the probability of measuring the qubit in the |0⟩ state or the |1⟩ state is flipped.\n",
        "* **Amplitude damping:** This type of noise occurs when the amplitude of a qubit is reduced. This means that the probability of measuring the qubit in the |0⟩ state is decreased, and the probability of measuring it in the |1⟩ state is increased.\n",
        "* **Depolarizing noise:** This is the most general type of noise. It can cause any combination of bit flip, phase flip, and amplitude damping noise.\n",
        "\n",
        "These are just a few of the many different types of noise that can be added to a quantum computing circuit. The specific type of noise that is added depends on the source of the noise.\n",
        "\n",
        "Noise is a major obstacle to the development of practical quantum computers. It can cause errors in quantum computations, which can lead to incorrect results. There are a number of techniques that can be used to reduce the effects of noise, such as quantum error correction.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions.\n",
        "\n",
        "Here are some additional details about the different types of noise mentioned above:\n",
        "\n",
        "* **Bit flip noise:** Bit flip noise can be modeled by a Pauli X gate, which flips the state of the qubit. The probability of a bit flip error occurring is determined by the strength of the noise.\n",
        "* **Phase flip noise:** Phase flip noise can be modeled by a Pauli Z gate, which flips the phase of the qubit. The probability of a phase flip error occurring is determined by the strength of the noise.\n",
        "* **Amplitude damping:** Amplitude damping can be modeled by a beam splitter, which mixes the |0⟩ and |1⟩ states of the qubit. The probability of an amplitude damping error occurring is determined by the strength of the noise.\n",
        "* **Depolarizing noise:** Depolarizing noise can be modeled by a depolarizing channel, which randomly changes the state of the qubit. The probability of a depolarizing error occurring is determined by the strength of the noise.\n",
        "\n",
        "The effects of noise can be mitigated by using quantum error correction. Quantum error correction is a technique that can be used to detect and correct errors in quantum computations. Quantum error correction can be used to make quantum computers more resistant to noise.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "-O53JXv_pEJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do you have more examples? Like Laplacian noise and more?**\n",
        "\n",
        "Sure. Here are some other examples of noise that can be added to a quantum computing circuit:\n",
        "\n",
        "* **Gaussian noise:** Gaussian noise is a type of noise that is characterized by a Gaussian distribution. This means that the probability of a certain error occurring is given by a Gaussian function. Gaussian noise can be caused by thermal fluctuations in the environment.\n",
        "* **Readout noise:** Readout noise is the noise that occurs when a qubit is measured. This noise can cause errors in the measurement results. Readout noise can be caused by a number of factors, including the environment and the measurement device.\n",
        "* **Crosstalk noise:** Crosstalk noise is the noise that occurs when one qubit affects the state of another qubit. This noise can be caused by the interaction between the qubits.\n",
        "* **Qubit loss:** Qubit loss is the loss of a qubit from the system. This can occur due to a number of factors, such as environmental noise or errors in the control system.\n",
        "\n",
        "These are just a few of the many different types of noise that can be added to a quantum computing circuit. The specific type of noise that is added depends on the source of the noise and the specific application.\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "603erkTepKMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And laplacian noise?**\n",
        "\n",
        "Yes, Laplacian noise is another type of noise that can be added to a quantum computing circuit. It is a non-Gaussian noise, which means that it does not follow a Gaussian distribution. Instead, it follows a Laplace distribution, which is a bell-shaped curve with heavier tails than a Gaussian distribution.\n",
        "\n",
        "Laplacian noise can be added to a quantum state by adding a random variable to each qubit in the state, where the random variable is drawn from a Laplace distribution. The Laplace noise is characterized by a parameter called the scale parameter, which controls the amount of noise. The larger the scale parameter, the more noise there is.\n",
        "\n",
        "Laplacian noise is often used to model the effects of decoherence in quantum computing. Decoherence is the loss of quantum coherence due to interactions with the environment. Laplacian noise is more robust to noise than Gaussian noise, which makes it a good choice for applications where noise is a major concern.\n",
        "\n",
        "Laplacian noise can also be used to design quantum machine learning algorithms that are robust to noise. For example, Laplacian noise can be used to regularize quantum neural networks, which can help to improve their performance.\n",
        "\n",
        "Here are some of the key properties of Laplacian noise:\n",
        "\n",
        "* It is a non-Gaussian noise.\n",
        "* It follows a Laplace distribution.\n",
        "* It is characterized by a scale parameter.\n",
        "* It can be added to quantum states to simulate the effects of decoherence in quantum computing.\n",
        "* It can be used to design quantum machine learning algorithms that are robust to noise.\n",
        "\n",
        "Laplacian noise is a versatile tool that can be used to study the effects of noise in quantum computing and quantum machine learning. It is a promising area of research with the potential to improve the performance of quantum computers and quantum machine learning algorithms."
      ],
      "metadata": {
        "id": "qCnnUEPHpOQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is Laplacian noise in the context of quantum computing and quantum machine learning?**\n",
        "\n",
        "Laplacian noise is a type of noise that is often used to model the effects of decoherence in quantum computing. Decoherence is the loss of quantum coherence due to interactions with the environment.\n",
        "\n",
        "Laplacian noise is a non-Gaussian noise, which means that it does not follow a normal distribution. Instead, it follows a Laplace distribution, which is a bell-shaped curve with heavier tails than a normal distribution.\n",
        "\n",
        "The Laplace noise is characterized by a parameter called the scale parameter, which controls the amount of noise. The larger the scale parameter, the more noise there is.\n",
        "\n",
        "Laplacian noise can be added to quantum states to simulate the effects of decoherence. This can be done by adding a random variable to each qubit in the state, where the random variable is drawn from a Laplace distribution.\n",
        "\n",
        "Laplacian noise can also be used to design quantum machine learning algorithms that are robust to noise. For example, Laplacian noise can be used to regularize quantum neural networks, which can help to improve their performance.\n",
        "\n",
        "Here are some of the key properties of Laplacian noise:\n",
        "\n",
        "* It is a non-Gaussian noise.\n",
        "* It follows a Laplace distribution.\n",
        "* It is characterized by a scale parameter.\n",
        "* It can be used to simulate the effects of decoherence in quantum computing.\n",
        "* It can be used to design quantum machine learning algorithms that are robust to noise.\n",
        "\n",
        "Laplacian noise is a versatile tool that can be used to study the effects of noise in quantum computing and quantum machine learning. It is a promising area of research with the potential to improve the performance of quantum computers and quantum machine learning algorithms."
      ],
      "metadata": {
        "id": "C6pxre-a4HnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Jens Eisert: What do we do with Quantum Computers?](https://youtu.be/70MD0pe6HGs) and [(2023) Disentangling Hype from Practicality](https://www.researchgate.net/publication/372075046_Disentangling_Hype_from_Practicality_On_Realistically_Achieving_Quantum_Advantage)"
      ],
      "metadata": {
        "id": "Ntf_T5a8waQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Where does a quantum computer has an (exponential) advantage over a classical computer in machine learning tasks?**\n",
        "\n",
        "* Classical Data (learn classical Boolean circuits):\n",
        "  * average error: no\n",
        "  * worse error: no\n",
        "\n",
        "* Quantum Data (learn unknown cptp maps over known observables):\n",
        "  * average error: no - classical ML can learn unknown state p or observable O very well\n",
        "  * worse error: yes (predicting ex-\n",
        "pectation values of Pauli operators in an unknown n-\n",
        "qubit quantum state ρ)"
      ],
      "metadata": {
        "id": "xdH07JFx1Szq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pending questions*\n",
        "\n",
        "what is the chernoff bound\n",
        "\n",
        "how can you learn the phase of a heisenberg-weyl operator?\n",
        "\n",
        "what is the phase of a heisenberg-weyl operator?\n",
        "\n",
        "what is a simplectic inner product?\n"
      ],
      "metadata": {
        "id": "DH8extyZD-oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by the parity of the two qubits?**\n",
        "\n",
        "The \"parity\" of two qubits refers to whether the number of qubits in the state |1⟩ is even or odd.\n",
        "\n",
        "The parity operator for two qubits, which could be denoted as Z ⊗ Z in terms of Pauli matrices, acts on a two-qubit state |ψ⟩ = α|00⟩ + β|01⟩ + γ|10⟩ + δ|11⟩ as follows:\n",
        "\n",
        "(Z ⊗ Z)|ψ⟩ = α|00⟩ - β|01⟩ - γ|10⟩ + δ|11⟩\n",
        "\n",
        "In this case, the eigenvalues of the parity operator are +1 and -1. States |00⟩ and |11⟩ have even parity (0 and 2 qubits are in the state |1⟩, respectively, both of which are even numbers), so the parity operator gives them a factor of +1. States |01⟩ and |10⟩ have odd parity (1 qubit is in the state |1⟩), so the parity operator gives them a factor of -1.\n",
        "\n",
        "Measuring the expectation value of the parity operator tells you about the relative likelihood of being in an even parity state versus an odd parity state. For example, if a two-qubit system is in the state |ψ⟩ = 1/√2(|00⟩ + |11⟩), then the expectation value of the parity operator would be +1, indicating that measurements of both qubits will always yield the same result, either both 0 or both 1. On the other hand, if the system is in the state |ψ⟩ = 1/√2(|01⟩ + |10⟩), then the expectation value of the parity operator would be -1, indicating that measurements of both qubits will always yield opposite results."
      ],
      "metadata": {
        "id": "n-lA7hp1t_Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "John Preskyll - Quantum Computation\n",
        "\n",
        "https://youtu.be/w08pSFsAZvE"
      ],
      "metadata": {
        "id": "eG1F6mn_fsDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means \"bounded fan-in (a general form of locality restriction)\" in the context of quantum computing?**\n",
        "\n",
        "In the context of quantum computing, the concept of \"fan-in\" relates to the number of input bits that a quantum gate or operation can accept.\n",
        "\n",
        "When we say a system or a circuit has \"bounded fan-in,\" it means that there's a limit to the number of input bits any single gate or operation can accept.\n",
        "\n",
        "\"Bounded fan-in\" is a kind of \"locality restriction.\" In a locality restriction, we limit how our quantum operations can act. In physical systems, this is often motivated by the fact that most physical interactions are local -- for instance, each qubit in a quantum computer might only interact directly with a small number of nearby qubits.\n",
        "\n",
        "A gate or operation with bounded fan-in can only act on a fixed number of qubits. For example, if we're working with a two-qubit bounded fan-in, our gates can only act on one or two qubits at a time -- we can't have a single gate that acts on three or more qubits simultaneously. This is a general form of locality restriction because it restricts how 'far' each operation can 'reach' in the space of qubits.\n",
        "\n",
        "This is important in quantum computing because it affects the kinds of circuits and algorithms we can use. It's also important when thinking about how to physically implement a quantum computer, as real-world quantum systems often have natural locality restrictions due to physical constraints."
      ],
      "metadata": {
        "id": "L3scapHoAYEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means strings over n bits?**\n",
        "\n",
        "In the context of computer science and mathematics, a \"string over n bits\" usually refers to a sequence or set of binary digits (bits) of length n. Each bit can either be a 0 or a 1.\n",
        "\n",
        "For instance, if n = 3, then the possible strings over 3 bits are: 000, 001, 010, 011, 100, 101, 110, 111. These are all the possible combinations of 3 bits.\n",
        "\n",
        "The total number of possible strings over n bits is 2^n, because each bit has 2 possible states (0 or 1), and there are n such bits. This principle is widely used in areas such as digital computing, information theory, cryptography, etc."
      ],
      "metadata": {
        "id": "qRrlJTQgAfHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means \"covering number bounds\" in quantum computing?**\n",
        "\n",
        "Covering number bounds in quantum computing (and more generally in the field of mathematics) refer to a metric related to how many of a certain-sized object are needed to \"cover\" or contain another object.\n",
        "\n",
        "The concept of a covering number is used in various mathematical contexts, but the general idea remains the same. The covering number of a set (in a metric space) is the smallest number of \"balls\" of a given radius that can cover the entire set.\n",
        "\n",
        "In quantum computing, this might be used in the context of quantum states or operations. For instance, if you're talking about quantum states in a Hilbert space, you might be asking: how many quantum states (treated as \"balls\" in the Hilbert space) of a certain \"size\" (or distance from each other) do you need to cover the entire space of possible quantum states?\n",
        "\n",
        "Covering numbers are important in quantum computing because they can help us understand the complexity of various tasks, including the complexity of approximating a given quantum state or operation.\n",
        "\n",
        "Bounding the covering number, or providing covering number bounds, is important because it gives us a measure of the \"size\" or complexity of the space we're dealing with. These bounds can help us understand the resources required for tasks such as quantum state preparation, quantum channel simulation, and so on."
      ],
      "metadata": {
        "id": "MOIwu35QAkuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qubitization**\n",
        "\n",
        "\"Qubitization\" is a term related to quantum computing. It is a procedure to convert a certain operation into a quantum operation that can be performed on a quantum computer. Qubitization offers an approach for performing certain computational tasks with a significant reduction in resource requirements compared to other quantum algorithms.\n",
        "\n",
        "A primary example of the use of qubitization is in simulating Hamiltonians, a fundamental problem in quantum physics, with an algorithm known as the \"qubitization of Hamiltonian.\" This algorithm allows the efficient estimation of quantities such as ground state energy, which are of significant interest in quantum chemistry and condensed matter physics.\n",
        "\n",
        "The key advantage of qubitization over other quantum simulation algorithms is that it allows a more efficient use of quantum resources. However, the technique typically requires a more complex set of quantum gates and, as of my knowledge cut-off in September 2021, is a subject of active research and development in the field of quantum computing.\n",
        "\n",
        "Please note that the understanding and implementation of qubitization may have evolved beyond my last update. For the most accurate information, it's recommended to refer to the latest literature in the field of quantum computing.\n",
        "\n",
        "https://arxiv.org/abs/1610.06546"
      ],
      "metadata": {
        "id": "Ph9a0fu1y9Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trotterization**\n",
        "\n",
        "https://vtomole.com/blog/2019/04/07/trotter\n",
        "\n",
        "**Trotterization** is a technique used in quantum computing to simulate the evolution of a quantum system under a Hamiltonian that is a sum of many terms. The idea is to break down the evolution into a sequence of smaller evolutions, each of which is under a single term of the Hamiltonian. This can be done because the exponential of a sum of matrices is approximately equal to the product of the exponentials of the individual matrices, up to a small error.\n",
        "\n",
        "The Trotterization formula is:\n",
        "\n",
        "> $e^{-iHt} \\approx \\left( e^{-iH_0t/r} \\cdot e^{-iH_1t/r} \\cdots e^{-iH_{d-1}t/r} \\right)^r$\n",
        "\n",
        "where $H$ is the Hamiltonian, $t$ is the time, $r$ is the number of Trotter steps, and $H_i$ are the individual terms of the Hamiltonian.\n",
        "\n",
        "The error in the Trotterization approximation can be controlled by increasing the number of Trotter steps. However, this also increases the complexity of the quantum circuit that is needed to implement the Trotterized evolution.\n",
        "\n",
        "Trotterization is a powerful technique that can be used to simulate a wide variety of quantum systems. However, it is important to note that the error in the Trotterization approximation can be significant, especially for long times or for Hamiltonians with terms that do not commute.\n",
        "\n",
        "Here are some additional details about Trotterization:\n",
        "\n",
        "* The Trotterization formula was first introduced by Harold Trotter in 1958.\n",
        "* The error in the Trotterization approximation is of the order of $O(t^2/r^2)$.\n",
        "* The number of Trotter steps needed to achieve a given accuracy depends on the Hamiltonian and the desired accuracy.\n",
        "* Trotterization can be used to simulate the evolution of both closed and open quantum systems.\n",
        "* Trotterization is a key technique in many quantum algorithms, such as quantum simulation, quantum chemistry, and quantum machine learning."
      ],
      "metadata": {
        "id": "5tnibSuZLcPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shadow Tomography**"
      ],
      "metadata": {
        "id": "i_mzlg6WUapR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/machine-learning-aids-classical-modeling-of-quantum-systems-20230914/"
      ],
      "metadata": {
        "id": "0RlEe49PJsfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State Tomography, Shadow Tomography and Process Tomography (Choi matrix)"
      ],
      "metadata": {
        "id": "edw279abUZ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> State tomography is an alternative to boolean functions in quantum computational learning"
      ],
      "metadata": {
        "id": "6ZVr_tksjzSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> To solve shadow tomography, the goal is to find a quantum state σ that satisfies Tr(σEi) ≈ Tr(ρEi) for all i (the trace distance is almost zero, the states are identical). Further, one would like to minimize the number of copies of ρ, suggesting that σ should be no more informative than matching the above expectations."
      ],
      "metadata": {
        "id": "I42TFae9nf2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference between state tomography and shadow tomorgraphy?**\n",
        "\n",
        "The main difference between state tomography and shadow tomography is the number of copies of the quantum state that are needed to reconstruct the state.\n",
        "\n",
        "In state tomography, the complete state vector or density matrix of the quantum state is reconstructed. This requires measuring the state in a complete set of bases, which can be exponentially many bases for a large quantum state.\n",
        "\n",
        "In shadow tomography, only a subset of the bases is measured. This allows the state to be reconstructed with fewer copies of the state, but the reconstructed state may not be as accurate as the state reconstructed using state tomography.\n",
        "\n",
        "Shadow tomography can be useful when the number of copies of the state is limited, or when it is only necessary to reconstruct a specific aspect of the state.\n",
        "\n",
        "Here is a table summarizing the key differences between state tomography and shadow tomography:\n",
        "\n",
        "| Feature | State tomography | Shadow tomography |\n",
        "|---|---|---|\n",
        "| Number of copies of state needed | Exponentially many | Polynomially many |\n",
        "| Accuracy | High | Low |\n",
        "| Applications | Reconstructing the complete state vector or density matrix | Reconstructing a specific aspect of the state |\n",
        "\n",
        "Here are some examples of when shadow tomography can be used:\n",
        "\n",
        "* To reconstruct the behavior of a quantum state on a specific set of operations.\n",
        "* To reconstruct the entanglement of a quantum state.\n",
        "* To reconstruct the purity of a quantum state.\n",
        "\n",
        "***Shadow tomography is a promising new technique for reconstructing quantum states with fewer copies of the state. As quantum technologies develop, shadow tomography is likely to become an increasingly important tool for understanding and manipulating quantum states.***\n"
      ],
      "metadata": {
        "id": "5SNoHHqcj0-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference of shadow tomography and state tomography to process tomography?**\n",
        "\n",
        "\n",
        "The main difference between shadow tomography, state tomography, and process tomography is the object that is being reconstructed.\n",
        "\n",
        "* **State tomography** reconstructs the state of a quantum system. This can be done by measuring the system in a complete set of bases and using the results to reconstruct the state vector or density matrix of the system.\n",
        "* **Shadow tomography** reconstructs a classical shadow of a quantum state. A classical shadow is a lower-dimensional representation of the quantum state that still retains some of its key properties.\n",
        "* **Process tomography** reconstructs the quantum process that transforms one quantum state into another. This can be done by preparing an ensemble of quantum states and sending them through the process, then using state tomography to identify the resultant states.\n",
        "\n",
        "In terms of the number of copies of the quantum system needed, shadow tomography and process tomography are both more efficient than state tomography. Shadow tomography can reconstruct a classical shadow of a quantum state with only polynomially many copies of the state, while process tomography can reconstruct a quantum process with only exponentially many copies of the state.\n",
        "\n",
        "However, shadow tomography and process tomography are less accurate than state tomography. The classical shadow of a quantum state may not be able to perfectly represent the quantum state, and the reconstructed quantum process may not be perfectly accurate.\n",
        "\n",
        "Here is a table summarizing the key differences between shadow tomography, state tomography, and process tomography:\n",
        "\n",
        "| Feature | Shadow tomography | State tomography | Process tomography |\n",
        "|---|---|---|---|\n",
        "| Object being reconstructed | Classical shadow of a quantum state | State vector or density matrix of a quantum state | Quantum process that transforms one quantum state into another |\n",
        "| Number of copies of quantum system needed | Polynomially many | Exponentially many | Exponentially many |\n",
        "| Accuracy | Low | High | Low |\n",
        "| Applications | Reconstructing specific aspects of a quantum state | Reconstructing the complete state of a quantum state | Reconstructing the behavior of a quantum process |\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "752lwZmzkuel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What is shadow tomography and how is it used in this context?*\n",
        "\n",
        "* Shadow tomography is a technique that assumes access to multiple copies of a quantum state and uses a hypothesis state model, often stored classically, to require a minimal number of destructive measurements.\n",
        "\n",
        "* In the context of quantum neural network training, recent developments in shadow tomography have challenged the notion that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation.\n",
        "\n",
        "* By assuming access to multiple copies of a quantum state, a modification of existing shadow tomography routines enables backpropagation scaling if one restricts costs to the quantum overhead and ignores the classical cost incurred to implement known shadow tomography schemes.\n",
        "\n",
        "*What is shadow tomography and how is it used in the context of quantum computing?*\n",
        "\n",
        "* Shadow tomography, in the context of quantum computing, is a technique used to characterize an unknown quantum state by gathering statistical information about its measurement outcomes. It is an approach that aims to reconstruct the quantum state without directly accessing it.\n",
        "\n",
        "* In quantum computing, qubits represent the fundamental units of information and are typically manipulated in superposition states. However, obtaining complete knowledge about the quantum state of a qubit is challenging due to limitations such as noise, decoherence, and the no-cloning theorem. Shadow tomography offers a way to infer information about the state without requiring full access to it.\n",
        "\n",
        "* The technique involves performing a series of measurements on a large number of identically prepared quantum systems in the same unknown state. Each measurement provides partial information about the state, revealing statistical probabilities for the different measurement outcomes. By gathering a sufficiently large set of measurement statistics, it becomes possible to infer the likely quantum state.\n",
        "\n",
        "* The reconstruction of the quantum state from the collected statistics is achieved through mathematical techniques, such as maximum likelihood estimation or compressed sensing algorithms. These methods leverage the statistical patterns observed in the measurement outcomes to estimate the underlying quantum state that generated them.\n",
        "\n",
        "* Shadow tomography provides a means to characterize unknown quantum states in a non-invasive manner, making it particularly useful for quantum systems that are difficult or impossible to access directly. It plays a crucial role in quantum information processing tasks such as quantum error correction, state verification, and characterizing the performance of quantum devices.\n",
        "\n",
        "\n",
        "*What is gentle measurement in the context of quantum computing?*\n",
        "\n",
        "* Gentle measurement techniques often employ strategies such as weak measurements, quantum non-demolition measurements, or adaptive measurements.\n",
        "\n",
        "* Weak measurements involve extracting partial information about the system while causing minimal disturbance.\n",
        "\n",
        "* Quantum non-demolition measurements are designed to measure one property of the system without affecting other compatible observables.\n",
        "\n",
        "* Adaptive measurements dynamically adjust the measurement strategy based on the intermediate measurement results to minimize disturbance.\n",
        "\n",
        "* By using gentle measurement techniques, researchers can gather information about quantum states while preserving their fragile nature. This is particularly important in quantum computing, where maintaining the coherence of quantum bits (qubits) is crucial for performing reliable computations and preventing errors."
      ],
      "metadata": {
        "id": "BkjNfD00kURa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**quantum state tomography**, and it is already an important task in experimental physics. To give some examples, tomography has been used to obtain a detailed picture of a chemical reaction (namely, the dissociation of I2 molecules) [29]; to confirm the preparation of three-photon [28] and eight-ion [20] entangled states; to test controlled-NOT gates [26]; and to characterize optical devices [15].\n",
        "Physicists would like to scale up tomography to larger systems, in order to study the many-particle entangled states that arise (for example) in chemistry, condensed-matter physics, and quantum information. But there is a fundamental obstacle in doing so. This is that, to reconstruct an n-qubit state, one needs to measure a number of observables that grows exponentially in n: in particular like 4^n, the number of parameters in a 2^n × 2^n density matrix. This exponentiality is certainly a practical problem—Ha ̈ffner et al. [20] report that, to reconstruct an entangled state of eight calcium ions, they needed to perform 656, 100 experiments! But to us it is a theoretical problem as well. For it suggests that learning an arbitrary state of (say) a thousand particles would take longer than the age of the universe, even for a being with unlimited computational power.\n",
        "Source: The Learnability of Quantum States"
      ],
      "metadata": {
        "id": "XfBOiHzuNSrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantum shadow tomography, which gives a way of predicting the values of most quantum observables using a small number of samples from a quantum system.\n",
        "\n",
        "Aaronson S 2018 Shadow tomography of quantum states Proc. of the 50th Annual ACM SIGACT Symp. on Theory of Computing\n",
        "pp 325–38"
      ],
      "metadata": {
        "id": "Aq23Xx1H2aaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1711.01053"
      ],
      "metadata": {
        "id": "-AzCvKg-59TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ***Sample complexity of quantum state tomography (e.g. shadow tomography) important for difference between coherent and incoherent measurements***\n"
      ],
      "metadata": {
        "id": "2tcJVkBR9ZB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the shadow tomography problem in quantum information theory?**\n",
        "\n",
        "In quantum information theory, the shadow tomography problem refers to the task of characterizing an unknown quantum state by gathering statistical information about its measurement outcomes. It addresses the challenge of reconstructing a quantum state without directly accessing it.\n",
        "\n",
        "The shadow tomography problem arises due to the limitations of directly measuring quantum states. In quantum systems, the act of measurement can disturb the system and potentially cause the collapse of its quantum state. Additionally, some quantum states may be inaccessible or difficult to measure directly due to experimental constraints or limitations.\n",
        "\n",
        "To overcome these challenges, shadow tomography offers a non-invasive approach to infer information about the unknown quantum state. Instead of directly measuring the state, it focuses on collecting statistical data by repeatedly measuring identically prepared copies of the state and recording the outcomes.\n",
        "\n",
        "The goal of the shadow tomography problem is to use the collected statistical data to reconstruct an estimate of the quantum state that is consistent with the observed measurement outcomes. This estimation process involves mathematical techniques such as maximum likelihood estimation, compressed sensing, or machine learning algorithms.\n",
        "\n",
        "Solving the shadow tomography problem is important in various quantum information tasks, including quantum state verification, characterization of quantum devices, and quantum error correction. By obtaining accurate reconstructions of unknown quantum states, researchers can gain insights into the properties of quantum systems and develop techniques to mitigate errors and improve quantum information processing tasks.\n",
        "\n",
        "**what is the exponential classical cost within the shadow tomography problem?**\n",
        "\n",
        "The term \"exponential classical cost\" in the context of the shadow tomography problem refers to the computational complexity associated with classical methods of characterizing or reconstructing an unknown quantum state. It signifies that the amount of computational resources required grows exponentially with the size of the quantum system.\n",
        "\n",
        "In shadow tomography, the goal is to estimate the quantum state by collecting measurement statistics. The challenge lies in finding an efficient algorithm to reconstruct the state from this statistical data. Classical methods typically involve searching through a large space of possible quantum states and optimizing a likelihood function or another measure of agreement with the observed statistics.\n",
        "\n",
        "However, for large quantum systems, the number of possible quantum states grows exponentially with the number of qubits. As a result, exhaustive classical methods that explore the entire state space become computationally infeasible as the system size increases. This exponential growth in the number of possible states leads to an exponential increase in the computational cost required for reconstruction.\n",
        "\n",
        "The exponential classical cost in shadow tomography highlights the need for alternative approaches, such as leveraging quantum computers or developing more efficient reconstruction algorithms. Quantum computers can potentially provide speedup by harnessing the principles of quantum mechanics, allowing for more efficient exploration of the state space or utilizing specialized quantum algorithms tailored for state reconstruction.\n",
        "\n",
        "Reducing the classical cost of shadow tomography is an active area of research, and various techniques, including compressed sensing, machine learning, and adaptive measurement strategies, have been explored to mitigate the computational challenges associated with large-scale quantum systems."
      ],
      "metadata": {
        "id": "8QKE7d8OvYR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by incoherent and coherent measurements in sample complexity (for shadow tomography)?**\n",
        "\n",
        "In the context of quantum computing and quantum information theory, coherent and incoherent measurements refer to two different types of quantum measurements.\n",
        "\n",
        "1. **Coherent measurements**: These are the idealized measurements usually described in basic quantum mechanics where measuring a quantum state immediately collapses it into one of the eigenstates of the measured observable. Coherent measurements are unitary and reversible.\n",
        "\n",
        "2. **Incoherent measurements**: These measurements, on the other hand, correspond to a more generalized form of measurement, described by a set of positive-operator valued measures (POVMs). POVMs need not be unitary or reversible, and they can model a broader range of real-world measurement processes, including those where the quantum system interacts with an environment and decoheres.\n",
        "\n",
        "> The difference between coherent and incoherent measurements can become ***important when we consider the sample complexity of quantum state tomography, or in your specific case, shadow tomography.***\n",
        "\n",
        "Quantum state tomography is the process of determining the unknown state of a quantum system from the outcomes of measurements made on the system. The sample complexity of this process refers to the number of measurements needed to accurately reconstruct the quantum state.\n",
        "\n",
        "When performing quantum state tomography, incoherent measurements can sometimes offer advantages in terms of reduced sample complexity, since they can provide more information per measurement about the state of the system. However, they may also be more technically challenging to implement than coherent measurements, and they may introduce additional sources of error.\n",
        "\n",
        "The concept of \"shadow tomography\" you're referring to might involve using a small number of random measurements (or \"shadows\") to estimate properties of the quantum state, and it might have different sample complexity depending on whether coherent or incoherent measurements are used. The detailed analysis of this would depend on the specific shadow tomography protocol and the properties of the quantum state and the measurement process.\n"
      ],
      "metadata": {
        "id": "5923WPqg8sS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Quantum_tomography\n",
        "\n",
        "https://en.wikipedia.org/wiki/Classical_shadow\n",
        "\n",
        "Scott Aaronson: Shadow Tomography https://arxiv.org/abs/1711.01053\n",
        "\n",
        "https://www.spektrum.de/news/quantennetzwerke-zeigen-schwarmintelligenz/2116689"
      ],
      "metadata": {
        "id": "ZstkVSpb2UOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shadow tomography, also known as X-ray phase-contrast imaging or phase tomography, is a technique used to create detailed three-dimensional images of an object's internal structure. It is primarily employed in the field of medical imaging but also finds applications in materials science and other areas.\n",
        "\n",
        "Traditional X-ray imaging relies on the attenuation of X-rays as they pass through different tissues or materials, resulting in a two-dimensional projection image that shows variations in X-ray absorption. However, some tissues or materials have similar absorption properties, making it challenging to distinguish between them based solely on their attenuation.\n",
        "\n",
        "Shadow tomography overcomes this limitation by exploiting the phase shift of X-rays as they pass through an object. When X-rays encounter a boundary between two materials with different refractive indices, they undergo a phase shift. This phase shift contains valuable information about the object's internal structure.\n",
        "\n",
        "To perform shadow tomography, a coherent X-ray source, such as a synchrotron or an X-ray tube, is used. The X-rays pass through the object and are collected by a detector placed behind it. The collected X-rays contain both intensity and phase information.\n",
        "\n",
        "The key to shadow tomography lies in extracting the phase information from the intensity measurements. This is typically achieved by using specialized techniques such as interferometry or grating-based methods. These methods create interference patterns that encode the phase information, which can then be computationally reconstructed into a three-dimensional image.\n",
        "\n",
        "The resulting shadow tomography image provides not only information about the X-ray absorption but also about the object's refractive index distribution and other structural features. This enables visualization of fine details, such as boundaries between tissues or subtle density variations, which may not be easily distinguishable in traditional X-ray images.\n",
        "\n",
        "Shadow tomography has the potential to improve diagnostic accuracy and enhance our understanding of complex biological and materials systems. However, it currently requires advanced X-ray sources and specialized techniques, limiting its widespread availability in clinical settings. Nevertheless, ongoing research and technological advancements aim to make shadow tomography more accessible and applicable in various fields."
      ],
      "metadata": {
        "id": "iMNjAZqA5-s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the confusion in my previous response. Shadow tomography can indeed be related to the characterization of quantum states, often referred to as \"quantum state tomography.\"\n",
        "\n",
        "In quantum mechanics, the state of a quantum system is described by a mathematical object known as a quantum state. Quantum state tomography aims to reconstruct or characterize the complete quantum state of a system by performing measurements on the system in different bases or using various observables.\n",
        "\n",
        "Shadow tomography of quantum states is a specific technique used for quantum state reconstruction. It involves measuring the shadows or projections of the quantum state onto a set of measurement operators or observables. These measurements provide partial information about the quantum state, similar to the way shadows reveal information about the object casting them.\n",
        "\n",
        "The procedure for shadow tomography of quantum states typically involves preparing the system in different known initial states, applying a set of measurements or observables to the system, and then collecting the measurement outcomes. By repeating this process for a sufficiently large number of initial states and measurements, it becomes possible to reconstruct the complete quantum state of the system through computational methods.\n",
        "\n",
        "The reconstructed quantum state provides valuable information about the system's properties, such as its purity, entanglement, coherence, and other relevant characteristics. This information is crucial for understanding and characterizing quantum systems, as well as for verifying the performance of quantum devices and protocols.\n",
        "\n",
        "Quantum state tomography, including shadow tomography, plays a vital role in the development, verification, and optimization of quantum algorithms, quantum error correction, quantum communication, and other applications of quantum information science.\n",
        "\n",
        "It's important to note that quantum state tomography can be a challenging task due to various factors, such as noise, imperfections in measurements, and the exponential growth of the state space with the number of quantum bits (qubits). Researchers continually work on improving the efficiency and accuracy of quantum state tomography techniques to overcome these challenges."
      ],
      "metadata": {
        "id": "bI5xQd9s6Atw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Process Tomography*"
      ],
      "metadata": {
        "id": "dgxniphi11W3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/quant-ph/0702131"
      ],
      "metadata": {
        "id": "vrfRDNCp18Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Tomography?**\n",
        "\n",
        "Quantum tomography is an experimental procedure to reconstruct a description of part of quantum system from the measurement outcomes of a specific set of experiments. In Qiskit we implement the following types of tomography:\n",
        "\n",
        "**Quantum state tomography**: Given a state-preparation circuit that prepares a system in a state, reconstruct a description of the density matrix 𝜌\n",
        " of the actual state obtained in the system.\n",
        "\n",
        "**Quantum process tomography**: Given a circuit, reconstruct a description of the quantum channel \n",
        " that describes the circuit’s operator when running on the system.\n",
        "\n",
        "**Quantum gate set tomography**: Performs process tomography on a set of gates in a self-consistent manner, meaning quantum noises on gates used by the tomography process itself is also taken into account.\n",
        "\n",
        "This notebook gives examples for how to use the ignis.verification.tomography modules.\n",
        "\n",
        "Quantum tomography refers to a suite of techniques used in quantum computing and quantum information theory to reconstruct a description of a part of a quantum system based on measurement data. The word \"tomography\" comes from medical imaging, where it's used to denote techniques that build a 3D image of an object from 2D slices.\n",
        "\n",
        "In quantum mechanics, there are two main types of tomography: state tomography and process tomography.\n",
        "\n",
        "1. **Quantum State Tomography**: This is a method used to determine the quantum state of a system. The goal is to construct the density matrix that represents the state of the system. This is done by making a series of measurements on identically prepared quantum systems and using the statistical results of these measurements to reconstruct the state. State tomography is a challenging problem in general, as the number of parameters to be estimated grows exponentially with the number of qubits.\n",
        "\n",
        "2. **Quantum Process Tomography**: This extends the concept of state tomography to quantum operations or processes. The goal here is to determine the quantum operation that a particular quantum system implements. In other words, given access to a quantum process (which could be a quantum gate, a sequence of quantum gates, or a quantum channel), the task is to identify the corresponding quantum map. This is done by preparing a set of input states, applying the process, and performing state tomography on the outputs. The data are then used to reconstruct the process matrix or Choi matrix, which fully describes the quantum operation. Again, process tomography is also a challenging problem, as the number of parameters to be estimated grows rapidly with the number of qubits.\n",
        "\n",
        "Quantum tomography is a fundamental tool for quantum characterization, verification, and validation, which are crucial tasks in quantum computing and quantum information processing. However, due to the complexity of these tasks, research is being conducted into more efficient methods, such as compressed sensing and other methods based on assumptions about the structure of the states or processes being measured.\n",
        "\n",
        "Qiskit:\n",
        "\n",
        "https://qiskit.org/documentation/stable/0.26/tutorials/noise/8_tomography.html\n",
        "\n",
        "https://qiskit.org/documentation/tutorials/simulators/2_device_noise_simulation.html\n",
        "\n",
        "[How do I change the fitter algorithm in a state tomography experiment? - 1 Minute Qiskit](https://www.youtube.com/watch?v=MsENB0n4Ac8)"
      ],
      "metadata": {
        "id": "OaOjrY5x5Cfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choi matrix = Process Matrix, which fully describes the quantum operation**\n",
        "\n",
        "The data are then used to reconstruct the process matrix or Choi matrix, which fully describes the quantum operation\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Choi%27s_theorem_on_completely_positive_maps\n",
        "\n",
        "Quantum tomography refers to a suite of techniques used in quantum computing and quantum information theory to reconstruct a description of a part of a quantum system based on measurement data. The word \"tomography\" comes from medical imaging, where it's used to denote techniques that build a 3D image of an object from 2D slices.\n",
        "\n",
        "In quantum mechanics, there are two main types of tomography: state tomography and process tomography.\n",
        "\n",
        "1. **Quantum State Tomography**: This is a method used to determine the quantum state of a system. The goal is to construct the density matrix that represents the state of the system. This is done by making a series of measurements on identically prepared quantum systems and using the statistical results of these measurements to reconstruct the state. State tomography is a challenging problem in general, as the number of parameters to be estimated grows exponentially with the number of qubits.\n",
        "\n",
        "2. **Quantum Process Tomography**: This extends the concept of state tomography to quantum operations or processes. The goal here is to determine the quantum operation that a particular quantum system implements. In other words, given access to a quantum process (which could be a quantum gate, a sequence of quantum gates, or a quantum channel), the task is to identify the corresponding quantum map. This is done by preparing a set of input states, applying the process, and performing state tomography on the outputs. The data are then used to reconstruct the process matrix or Choi matrix, which fully describes the quantum operation. Again, process tomography is also a challenging problem, as the number of parameters to be estimated grows rapidly with the number of qubits.\n",
        "\n",
        "Quantum tomography is a fundamental tool for quantum characterization, verification, and validation, which are crucial tasks in quantum computing and quantum information processing. However, due to the complexity of these tasks, research is being conducted into more efficient methods, such as compressed sensing and other methods based on assumptions about the structure of the states or processes being measured."
      ],
      "metadata": {
        "id": "fVcxQqimzDiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a quantum state tomography?**\n",
        "\n",
        "Quantum state tomography is the process of determining the state of a quantum system based on the results of measurements. In essence, it's a method used to figure out the unknown quantum state of a system. It's called \"tomography\" because, like a CT scan in medicine, it combines many different measurements to construct a full picture of something you can't directly observe in one shot.\n",
        "\n",
        "For a quantum system, the state is typically represented by a density matrix. In the simplest case, for a single qubit, the density matrix is a 2x2 Hermitian matrix that is positive semi-definite and has trace 1. The generalization to multiple qubits is a matrix that is 2^n x 2^n (where n is the number of qubits), but with the same properties.\n",
        "\n",
        "Quantum state tomography involves making various measurements on many identically-prepared copies of the quantum system and then using the results to reconstruct the density matrix. The measurements you make are usually in different bases. For a single qubit, measurements are often made in the X, Y, and Z bases, which correspond to the Pauli matrices.\n",
        "\n",
        "However, as the number of qubits increases, the number of measurements you need to make grows exponentially, which makes full quantum state tomography unfeasible for large systems. For a system of n qubits, the density matrix contains 4^n - 1 independent real parameters, which is why the number of measurements needed increases so rapidly.\n",
        "\n",
        "Therefore, in practice, people use other methods to get partial information about the state or use assumptions to reduce the number of necessary measurements. There are also methods that attempt to directly estimate quantities of interest, like the expectation value of an observable, without fully reconstructing the state."
      ],
      "metadata": {
        "id": "-Bv3caiQuGsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are these alternative methods to quantum state tomography?**\n",
        "\n",
        "Indeed, full quantum state tomography is often impractical due to the exponential number of measurements required as the number of qubits increases. Here are some alternative methods that are commonly used in practice:\n",
        "\n",
        "1. **Quantum Process Tomography**: Instead of trying to determine the state of a quantum system, quantum process tomography aims to determine the quantum operation or quantum channel that describes a quantum process. This can be more practical in many experimental scenarios where you're more interested in characterizing a quantum gate or a quantum circuit.\n",
        "\n",
        "2. **Direct Estimation of Expectation Values**: Rather than attempting to reconstruct the full quantum state, you can directly estimate the expectation value of a specific observable by making many measurements in the basis corresponding to that observable and averaging the results.\n",
        "\n",
        "3. **Quantum State Estimation**: Quantum state estimation techniques use a set of projective measurements to estimate the state of a quantum system. This is similar to quantum state tomography, but instead of trying to reconstruct the full state, it only attempts to estimate it based on the results of projective measurements.\n",
        "\n",
        "4. **Quantum Compressed Sensing**: Compressed sensing is a method that uses the sparsity of a state in a certain basis to reduce the number of measurements required. It has been applied in the context of quantum state tomography, especially for states that are known to be sparse in a certain basis.\n",
        "\n",
        "5. **Machine Learning Techniques**: Machine learning methods, including neural networks and other techniques, have been increasingly used for quantum state tomography and characterization. These methods can often handle higher-dimensional data and can help reduce the number of measurements required.\n",
        "\n",
        "6. **Randomized Benchmarking**: This is a method for characterizing the performance of quantum gates by applying sequences of random gates followed by their inverse and seeing how often the system returns to the initial state.\n",
        "\n",
        "Remember, the choice of method depends on the specific task, the number of qubits, and the resources available. Each method has its strengths and limitations and might be suited to different experimental setups or goals.\n",
        "\n"
      ],
      "metadata": {
        "id": "AcHLAsw4uvoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Encoding**\n",
        "\n",
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "\n",
        "block encodings (unitary, signal proicesing, transformation)\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "v1ou698c0kn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block Encoding (Gibbs-Sampling)"
      ],
      "metadata": {
        "id": "A5mYvARiUf-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1806.01838\n",
        "\n",
        "Quantum singular value transformation and beyond"
      ],
      "metadata": {
        "id": "GYNleGbzLh_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aaronson"
      ],
      "metadata": {
        "id": "Y86Jw3gfSQg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2203.10236"
      ],
      "metadata": {
        "id": "fA9_AEsAIPhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.wikipedia.org/wiki/Gibbs-Sampling"
      ],
      "metadata": {
        "id": "Sd2SdVyZPSrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block encoding is the framework or tool for developing a variety of quantum algorithms by encoding a matrix as a block of a unitary.There are various ways to implement the same as per requirement,one of the ways is by decomposing the matrices into linear combinations of displacement matrices.\n",
        "\n",
        "Arxiv: [Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics](https://arxiv.org/abs/1806.01838)\n",
        "\n",
        "In HHL f(x) = 1/x. Use Singular Value Transformation to approximate it! https://www.youtube.com/watch?v=L40UUDxPEbE&list=WL&index=3&t=215s\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18197/in-the-context-of-block-encoding-what-does-0-rangle-otimes-i-represent\n",
        "\n",
        "https://quantumcomputing.stackexchange.com/questions/18236/block-encoding-technique-what-is-it-and-what-is-it-used-for"
      ],
      "metadata": {
        "id": "7XFH8tKPLEJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(2021) Quantum **Krylov subspace** algorithms for ground and excited state energy estimation*"
      ],
      "metadata": {
        "id": "Drnam8uix3L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2109.06868"
      ],
      "metadata": {
        "id": "AE6k5L9fRiXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Krylov_subspace\n",
        "\n",
        "\n",
        "https://quantum-journal.org/papers/q-2023-05-23-1018/\n",
        "\n",
        "We present an algorithm that uses block encoding on a quantum computer to exactly construct a Krylov space, which can be used as the basis for the Lanczos method to estimate extremal eigenvalues of Hamiltonians. While the classical Lanczos method has exponential cost in the system size to represent the Krylov states for quantum systems, our efficient quantum algorithm achieves this in polynomial time and memory. The construction presented is exact in the sense that the resulting Krylov space is identical to that of the Lanczos method, so the only approximation with respect to the exact method is due to finite sample noise. This is possible because, unlike previous quantum Krylov methods, our algorithm does not require simulating real or imaginary time evolution. We provide an explicit error bound for the resulting ground state energy estimate in the presence of noise.\n",
        "\n",
        "https://medium.com/qiskit/a-mainstay-of-classical-ground-state-algorithms-gets-a-quantum-speedup-8003687dbb0d"
      ],
      "metadata": {
        "id": "z_W9d3IIx7NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Resource Estimation for Quantum Algorithms*"
      ],
      "metadata": {
        "id": "y7r0yPBYKf60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Roy J. Garcia | Nov. 15, 2022 | Resource theory of quantum scrambling](https://www.youtube.com/watch?v=pikuMFTDEbI&list=PLBn8lN0DcvpmcGQ1H_9YMFPew1ZsP_8Sj&index=11&t=25s)\n",
        "\n",
        "Video: [Quantum computing use cases for the pharma industry [QCT21/22, Seminar #5]](https://www.youtube.com/watch?v=CkzazWRCdWw&list=WL&index=37&t=2074s)"
      ],
      "metadata": {
        "id": "Q6eTdz4sX4Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://www.handelsblatt.com/audio/disrupt-podcast/drei-experten-drei-perspektiven-wann-werden-quantencomputer-endlich-realitaet/29571162.html\n",
        "\n",
        "https://spectrum.ieee.org/quantum-computing-skeptics\n",
        "\n",
        "https://ap-verlag.de/welchen-entwicklungsstand-hat-das-quantencomputing-erreicht/85623/\n",
        "\n",
        "https://www.riskcompliance.de/news/wie-veraendert-quantencomputing-das-banking/"
      ],
      "metadata": {
        "id": "ys8hKh3l2Va_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumcomputing.stackexchange.com/questions/21518/review-paper-on-depth-qubits-and-t-gates-number-on-cliffordt-decomposition-f"
      ],
      "metadata": {
        "id": "2UOmCkkoKjJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from Table 1 in arxiv.org/abs/2012.03819, pricing commonly used non-trivial options (autocallables and TARFs) using Monte Carlo simulations on a quantum computer takes ~10 billion gates\n",
        "\n",
        "https://arxiv.org/abs/2012.03819: A Threshold for Quantum Advantage in Derivative Pricing\n",
        "\n"
      ],
      "metadata": {
        "id": "M_o9NPECNoQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Classical Emulation of Quantum Algorithms*"
      ],
      "metadata": {
        "id": "5LYWuYGRUVyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Physicists 'entangle' individual molecules for the first time, hastening possibilities for quantum computing\n",
        "\n",
        "https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2023-12-physicists-entangle-individual-molecules-hastening.amp"
      ],
      "metadata": {
        "id": "9c4M19VFLhKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if quantum computers were not about getting faster results, but better results?\n",
        "\n",
        "This could be the case in optimization problems.\n",
        "\n",
        "Because when optimization problems become big enough, finding THE best answer is usually not practically achievable.\n",
        "\n",
        "The goal of optimization algorithms is therefore to get as close as possible to the best answer, but they usually only output a \"local optimum\", not the \"global optimum\".\n",
        "\n",
        "👉 So, if a quantum computer takes longer but finds a better optimum, this is a new form of quantum advantage, right?\n",
        "\n",
        "Well... yes and no.\n",
        "\n",
        "The thing is that you can emulate the behavior of a quantum computer with a classical computer. Even better, unlike real quantum computers, emulators running on classical computers aren't bothered by noise, so they tend to produce better results.\n",
        "\n",
        "This means that if you have a quantum algorithm whose main benefit is \"it finds better solutions\", then you also have a classical algorithm which does that.\n",
        "\n",
        "Just use your classical computer to run your quantum algorithm on an emulator and you're good!\n",
        "\n",
        "👉 So, why bother with real quantum computers?\n",
        "\n",
        "The problem of emulation is that the memory requirements grow exponentially with the number of qubits you want to emulate.\n",
        "\n",
        "Emulating a handful of qubits can be done on a sheet of paper.\n",
        "\n",
        "Emulating 40 qubits requires terabytes of memory.\n",
        "\n",
        "Emulating 100 qubits requires more than the combined storage capacity of all devices on Earth.\n",
        "\n",
        "This memory problem can be easily converted into a computing time problem. Need the coefficient of a given state? Don't get it from the memory, just recompute it.\n",
        "\n",
        "But then your computing time is the one that grows exponentially out of hand.\n",
        "\n",
        "At the end of the day, a classical computer is theoretically able to find results just as good as those found by a quantum computer. Possibly even better, if you consider that classical computers are not bothered by noise.\n",
        "\n",
        "But it would just take too much time.\n",
        "\n",
        "So, is it \"better results\" or \"faster results\"? To me, this really looks like two faces of the same coin!"
      ],
      "metadata": {
        "id": "HhE6VY_TUde-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "vCRgH1q0bWlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "zkMaiR5lbWlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "Ihzo1PqRbWlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "b3wxbJoKbWlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "-DivshtbbWlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "qQF73onwbWlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "y02BnicBbWlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "5pjrMx1KbWlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Topological Data Analysis*"
      ],
      "metadata": {
        "id": "s9R2XXEqZeYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topology of data hides in quantum thermal states\n",
        "\n",
        "If somebody would tell me that performing topological data analysis and \"cooling down\" quantum system is equivalent, I'll surely ask for some evidence. Yet, we discovered that measuring a purity of thermalized quantum systems gives us Betti numbers (topological features, aka doughnut/pretzel classifiers). For this we embed data (point clouds transformed into simplicial complexes) into jump operators (read – noise). Given that properties of thermalization are distinct from unitary evolution, we get a favorable algorithm complexity scaling with a spectral gap. There are also numerous links emergent links between topology and quantum thermodynamics\n",
        "\n",
        "https://arxiv.org/abs/2402.15633"
      ],
      "metadata": {
        "id": "3en4zxf3QthW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complexity-Theoretic Limitations on Quantum Algorithms for Topological Data Analysis\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040349"
      ],
      "metadata": {
        "id": "MEICYVfID1Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [QIP2023 | Quantifying Quantum Advantage in Topological Data Analysis (Vedran Dunjko)](https://youtu.be/T8ygjc-Lpn0?si=4W1n6iEveTXSbEsD)"
      ],
      "metadata": {
        "id": "4GJyErpahBn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tandfonline.com/doi/full/10.1080/23746149.2023.2202331"
      ],
      "metadata": {
        "id": "KJyD_mprZuNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Essential Steps in the qTDA Paper**\n",
        "\n",
        "* **Connect Laplacian with TDA**:\n",
        "  * harmonic part of the spectrum (eigenvalues, multiples of zero-th eigenvalue) of the persistent Laplacian fully captures the homological persistence and can thus be used to compute the persistent Betti numbers of filtrations of simplicial complexes. (harmonic part of a spectrum is the set of frequency components whose frequencies are whole number multiples of the fundamental frequency)\n",
        "  * The eigenvalues of the combinatorial Laplacian can be used to measure the topological features of the data. For example, the smallest non-zero eigenvalue of the Laplacian is related to the number of connected components in the data. The second smallest non-zero eigenvalue is related to the number of holes in the data.\n",
        "  * Specifically the **dimension of the kernel of the combinatorial Laplacian for k simplices is the kth Betti number**. (*The kernel is a function that measures the similarity between two data points). In other words, the number of data points that are in the kernel of the Laplacian is equal to the number of holes in the data set.\n",
        "  \n",
        "* The kernel of the combinatorial Laplacian specifically captures connectedness.: The kernel (or null space) of the Laplacian captures the number of connected components in the graph. The combinatorial Laplacian is defined for a graph \\( G \\) with vertex set \\( V \\) and edge set \\( E \\) as \\( L = D - A \\), where \\( D \\) is the degree matrix and \\( A \\) is the adjacency matrix. **The number of zero eigenvalues of the combinatorial Laplacian equals the number of connected components of the graph**. The eigenvectors associated with the zero eigenvalues (which form the kernel of the Laplacian) provide **harmonic functions on the graph**.\n",
        "\n",
        "* Aber: combinatorial laplacian vs persistent laplacian: Persistent homology provides information at multiple scales, while the kernel of the combinatorial Laplacian is typically focused on the connected components of a fixed graph.)\n",
        "\n",
        "* The persistent Laplacian is a graph signal processing tool that can be used to **analyze the harmonic part of a spectrum** (=*harmonic part of a spectrum is the set of frequency components whose frequencies are whole number multiples of the fundamental frequency. These frequencies are known as harmonics. harmonic part of the spectrum is then the set of eigenvalues that remain after all the small eigenvalues have been removed*). The persistent Laplacian is calculated by **taking the Laplacian of a graph, and then iteratively removing the smallest eigenvalues of the Laplacian**.\n",
        "  * The harmonic part of the spectrum is then the set of eigenvalues that remain after all the small eigenvalues have been removed\n",
        "  * this is why we were using the **Chebyshev polynomial to optimally filter the zero eigenvalues** (spectral projector that distinguishes the zero eigenvalue from the remaining nonzero eigenvalues of the Dirac operator)\n",
        "  * ps: spectral projector is a projection operator that projects onto the eigenspace of a self-adjoint operator corresponding to a particular eigenvalue because if you project onto the eigenspace of a matrix corresponding to a particular eigenvalue, you can develop efficient algorithms for solving systems of linear equations, or if you project onto the eigenspace of the Laplacian corresponding to a particular eigenvalue, you can study the behavior of solutions to the differential equation in the long-time limit."
      ],
      "metadata": {
        "id": "C5OhzjViVWHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel and Spectral Gap**\n",
        "\n",
        "* there is a connection between kernel and spectral gap. The spectral gap is a measure of how well-separated the eigenvalues of a matrix are. A large spectral gap means that the eigenvalues are well-separated, which means that the matrix is more stable and easier to invert.\n",
        "\n",
        "* The kernel of a matrix is the set of all vectors that are orthogonal to all the eigenvectors of the matrix with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the matrix.\n",
        "\n",
        "* The spectral gap of a matrix is related to the kernel of the matrix in the following way: **the larger the spectral gap, the smaller the dimension of the kernel**. This is because if the spectral gap is large, then the eigenvalues of the matrix are well-separated, which means that there are fewer vectors that are orthogonal to all the eigenvectors with eigenvalue 0."
      ],
      "metadata": {
        "id": "tTqWqxscIG1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Eigenvalue in TDA:\n",
        "* „In ordinary TDA applications one is typically concerned with the computation of the exact count of zero eigenvalues of combinatorial Laplacians“\n",
        "* = number of connected compontents (you cannot break it up any smaller).\n",
        "* „deciding if a combinatorial Laplacian has a trivial or non-trivial kernel (i.e., Betti number zero or non-zero)“\n",
        "\n",
        "Cheeger’s inequality demonstrates that the magnitudes of the small non-zero eigenvalues of the graph Laplacian characterises the connectedness of the graph [20], and similar results hold for combinatorial Laplacians.\n",
        "\n",
        "Higher Eigenvalues and why filter out first Eigenvalue:\n",
        "* Compute spectral gap, large gap = dense graph connectivity\n",
        "* Filtrations in Persistent Homology: In TDA, one often constructs filtrations, which are sequences of growing simplicial complexes. Monitoring how the first non-zero eigenvalue changes throughout a filtration can give insights into how the topological features of the data evolve across scales.\n",
        "* Robustness and Sensitivity: The magnitude of the first non-zero eigenvalue can indicate the robustness or sensitivity of the data's topological features. Smaller values might suggest features that are more sensitive to noise or perturbations.\n",
        "* Dimensionality Reduction: Techniques like Laplacian eigenmaps use the first few eigenvalues and their associated eigenvectors for dimensionality reduction. The first non-zero eigenvalue plays a critical role in capturing the most significant variance or structure in the data in such methods.\n",
        "* Clustering and Segmentation: The first non-zero eigenvalue and its corresponding eigenvector can be useful in clustering or segmenting data. Methods like spectral clustering leverage this property.\n"
      ],
      "metadata": {
        "id": "ziII-FeODo4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hodge theory](https://de.m.wikipedia.org/wiki/Hodge-Zerlegung): combinatorial laplacian (linear operator) which is a generalization of graph laplacian\n",
        "* The key idea in Hodge theory is that the cohomology groups of a smooth manifold **can be decomposed into a direct sum of spaces of harmonic forms.**\n",
        "* A harmonic form is a differential form that satisfies the Laplace equation. The Laplace equation is a partial differential equation that arises naturally from the study of differential forms.\n",
        "* This theorem has many important consequences, such as the Hodge theorem, which states that the integral of a harmonic form over a closed manifold is zero."
      ],
      "metadata": {
        "id": "n4L44J66OJQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Homology vs Hohomolgy*\n",
        "\n",
        "Homology and cohomology are two closely related concepts in mathematics that are used to study the topology of a space. They are both defined using chain complexes, but they differ in the way that the chain complexes are constructed.\n",
        "\n",
        "In homology, the chain complex is constructed using simplices, which are geometric objects that can be thought of as building blocks for a space. The cochain complex is constructed using cochains, which are functions on the simplices.\n",
        "\n",
        "**The homology groups of a space are the groups of equivalence classes of cycles in the chain complex**. A cycle is a collection of simplices that can be assembled to form a closed loop. The cohomology groups of a space are the groups of equivalence classes of cocycles in the cochain complex. A cocycle is a function on the simplices that satisfies certain conditions.\n",
        "\n",
        "The main difference between homology and cohomology is that homology groups are contravariant functors, while cohomology groups are covariant functors. This means that if you take the opposite of a space, the homology groups of the opposite space are the same as the cohomology groups of the original space.\n",
        "\n",
        "Intuitively, homology can be thought of as a way of counting the number of holes in a space, while cohomology can be thought of as a way of measuring the amount of \"stuff\" in a space.\n",
        "\n",
        "Here is a table that summarizes the key differences between homology and cohomology:\n",
        "\n",
        "| Feature | Homology | Cohomology |\n",
        "|---|---|---|\n",
        "| Construction | Simplicial chain complex | Cochain complex |\n",
        "| Equivalence classes | Cycles | Cocycles |\n",
        "| Functor type | Contravariant | Covariant |\n",
        "| Interpretation | Holes in a space | Stuff in a space |\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "vUEI23JLMNli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantifying Quantum Advantage in Topological Data Analysis*\n",
        "\n",
        "https://arxiv.org/abs/2209.13581\n",
        "\n",
        "QIP2023 | Quantifying Quantum Advantage in Topological Data Analysis (Vedran Dunjko)\n",
        "https://youtu.be/T8ygjc-Lpn0\n",
        "\n",
        "What means in the context of quantum computing: „Block encoding & qubitization instead of time evolution. Use filtering of the zero eigenvalue rather than phase estimation.“\n",
        "\n",
        "To understand this, let's first break down some key concepts in quantum computing:\n",
        "\n",
        "1. **Block Encoding**: In quantum computing, block encoding is a method of representing a matrix in a larger, unitary matrix so that the original matrix becomes a sub-block of this larger matrix. This technique is used to allow us to perform certain operations more easily.\n",
        "\n",
        "2. **Qubitization**: Qubitization is a quantum algorithm technique, which uses the block-encoding of a matrix to perform a quantum walk on it. The method uses significantly fewer resources than other quantum algorithms for similar problems.\n",
        "\n",
        "3. **Time Evolution**: Time evolution in quantum computing typically refers to how a quantum state changes over time according to the Schrödinger equation. It is often used to simulate physical systems in quantum simulations.\n",
        "\n",
        "4. **Phase Estimation**: Quantum Phase Estimation (QPE) is a fundamental algorithm in quantum computing which estimates the phase (or eigenvalue) of an eigenvector of a unitary operator. It's a key component in many quantum algorithms, such as Shor's algorithm and quantum simulation algorithms.\n",
        "\n",
        "5. **Filtering the Zero Eigenvalue**: This refers to the process of isolating or eliminating states with zero eigenvalues. It can be a way of refining results or simplifying calculations in quantum algorithms.\n",
        "\n",
        "Now, to the context you provided: \"Block encoding & qubitization instead of time evolution. Use filtering of the zero eigenvalue rather than phase estimation.\"\n",
        "\n",
        "This essentially suggests a strategy for tackling a quantum computing task. Instead of using the time evolution operator (a more traditional approach to quantum simulations), this is advocating the use of block encoding and qubitization. These methods can provide a more resource-efficient approach for certain types of quantum computations.\n",
        "\n",
        "Similarly, \"using filtering of the zero eigenvalue rather than phase estimation\" suggests a preferential way of isolating the states you're interested in. While phase estimation is a powerful tool, it can be resource-intensive. Filtering the zero eigenvalue might be a more efficient approach in certain contexts, particularly when zero eigenvalues represent \"unwanted\" states that might complicate or slow the computation."
      ],
      "metadata": {
        "id": "nQipY_1efAhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: how can you use the Chebyshev polynomial to optimally filter the zero eigenvalues?**\n",
        "\n",
        "The Chebyshev polynomials are a family of orthogonal polynomials that can be used to filter the zero eigenvalues of a matrix. The Chebyshev polynomials are defined by the following recursion:\n",
        "\n",
        "$$T_0(x) = 1$$\n",
        "$$T_1(x) = x$$\n",
        "$$T_n(x) = 2xT_{n-1}(x) - T_{n-2}(x)$$\n",
        "\n",
        "for all positive integers $n$.\n",
        "\n",
        "The Chebyshev polynomials have the property that they are orthogonal to the constant function on the interval $[-1,1]$. This means that for any two Chebyshev polynomials $T_n(x)$ and $T_m(x)$, the following inner product is zero:\n",
        "\n",
        "$$\\int_{-1}^1 T_n(x) T_m(x) \\, dx = 0$$\n",
        "\n",
        "where $n \\neq m$.\n",
        "\n",
        "This property can be used to filter the zero eigenvalues of a matrix $A$. The idea is to multiply $A$ by a Chebyshev polynomial $T_n(x)$, where $n$ is large enough so that the Chebyshev polynomial is non-zero at all eigenvalues of $A$ except for the zero eigenvalues. The resulting matrix will have all of its eigenvalues except for the zero eigenvalues scaled by $T_n(x)$. The zero eigenvalues will be filtered out because they will be multiplied by 0.\n",
        "\n",
        "The optimal value of $n$ for filtering the zero eigenvalues depends on the matrix $A$. A good choice for $n$ is the smallest integer such that $T_n(x)$ is non-zero for all eigenvalues of $A$ except for the zero eigenvalues.\n",
        "\n",
        "**The Chebyshev polynomial filter is a powerful tool for filtering the zero eigenvalues of a matrix**. It is simple to implement and it is very effective. The Chebyshev polynomial filter has been used in a variety of applications, including image processing, machine learning, and numerical analysis."
      ],
      "metadata": {
        "id": "soH4k5g-Wc_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">***Kernel of Combinatorial Laplacians and Betti numbers***"
      ],
      "metadata": {
        "id": "99lWWSlK_kig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The kernel of the combinatorial Laplacian is isomorphic to the first homology group of the simplicial complex. This means that the dimension of the kernel of the Laplacian is equal to the number of connected components of the simplicial complex."
      ],
      "metadata": {
        "id": "8TTHgppL-fMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persistent homology and the kernel of a combinatorial Laplacian** both arise in the study of topological and geometric properties of data, but they address different aspects and are based on somewhat different concepts.\n",
        "\n",
        "*Kernel of a Combinatorial Laplacian:*\n",
        "\n",
        "1. **Purpose**: The combinatorial Laplacian (or graph Laplacian) is a matrix associated with a graph that gives insights into the properties and structure of the graph. The kernel (or null space) of the Laplacian captures the number of connected components in the graph.\n",
        "\n",
        "2. **Construction**: The combinatorial Laplacian is defined for a graph \\( G \\) with vertex set \\( V \\) and edge set \\( E \\) as \\( L = D - A \\), where \\( D \\) is the degree matrix and \\( A \\) is the adjacency matrix.\n",
        "\n",
        "3. **Output**: The number of zero eigenvalues of the combinatorial Laplacian equals the number of connected components of the graph. The eigenvectors associated with the zero eigenvalues (which form the kernel of the Laplacian) provide harmonic functions on the graph.\n",
        "\n",
        "*Commonalities:*\n",
        "\n",
        "1. **Topology**: Both are tools used to study topological or geometric properties of spaces (or data). While persistent homology studies a wide range of topological features across scales, <font color=\"blue\">the kernel of the combinatorial Laplacian specifically captures connectedness.</font>\n",
        "\n",
        "2. **Graph-based**: Both can be discussed in the context of graphs or networks. Simplicial complexes in persistent homology can be viewed as higher-dimensional generalizations of graphs.\n",
        "\n",
        "*Differences:*\n",
        "\n",
        "1. **Scale**: Persistent homology provides information at multiple scales, while the kernel of the combinatorial Laplacian is typically focused on the connected components of a fixed graph.\n",
        "\n",
        "2. **Features**: While both can capture the number of connected components, persistent homology can also capture other features like loops and voids.\n",
        "\n",
        "3. **Mathematical machinery**: Persistent homology involves techniques from algebraic topology and often requires more sophisticated computational tools. The combinatorial Laplacian involves spectral graph theory and linear algebra.\n",
        "\n",
        "4. **Applications**: While both are used in data analysis, persistent homology has found a broader range of applications in studying the shape of data, while the Laplacian and its kernel are often used in graph theory, network analysis, and spectral clustering.\n",
        "\n",
        "In summary, while both persistent homology and the kernel of the combinatorial Laplacian deal with topological features of spaces or data, they focus on different aspects and are based on somewhat different mathematical concepts."
      ],
      "metadata": {
        "id": "PhPKkNu4RAQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The persistent Laplacian (persistent homology) is a generalization of the combinatorial Laplacian that can be used to compute persistent Betti numbers. Persistent Betti numbers are a way to track the birth and death of topological features in a simplicial complex as a function of a parameter. The persistent Laplacian is closely related to the kernel of the combinatorial Laplacian, and it can be used to compute persistent Betti numbers in a more efficient way.\n",
        "\n",
        "* persistent Laplacian, an extension of the standard combinatorial Laplacian to the setting of pairs: https://epubs.siam.org/doi/10.1137/21M1435471\n",
        "\n"
      ],
      "metadata": {
        "id": "_Av3fQr2-vUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Betti Numbers via Combinatorial Laplacians\n",
        "\n",
        "https://dl.acm.org/doi/pdf/10.1145/237814.237985\n",
        "\n",
        "omputing homology at present seems hard; currently algorithms require computing the [Smith normal form](https://en.m.wikipedia.org/wiki/Smith_normal_form) of the matrix. The latter can be done in polynomial time\n",
        "[KB79]), but the current polynomial required is still quite large.\n",
        "\n",
        "Normal form: https://en.m.wikipedia.org/wiki/Canonical_form"
      ],
      "metadata": {
        "id": "IXRFkIOG7lQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why is the kernel also the Betti number in topological data analysis?**\n",
        "\n",
        "* The kernel in topological data analysis is not the same as the Betti number. **The kernel is a function that measures the similarity between two data points**. The Betti number is a topological invariant that measures the number of holes in a data set.\n",
        "\n",
        "> However, there is a connection between the kernel and the Betti number. **The dimension of the kernel of the combinatorial Laplacian for k simplices is the kth Betti number. In other words, the number of data points that are in the kernel of the Laplacian is equal to the number of holes in the data set.**\n",
        "\n",
        "* This connection between the kernel and the Betti number can be used to develop topological data analysis methods that are based on kernel methods. Kernel methods are a type of machine learning method that uses kernels to measure the similarity between data points. By using the kernel of the Laplacian, we can develop kernel methods that are sensitive to topological features in data.\n",
        "\n",
        "* For example, we can use the kernel of the Laplacian to develop a kernel-based clustering algorithm that can cluster data points based on their topological features. We can also use the kernel of the Laplacian to develop a kernel-based classification algorithm that can classify data points based on their topological features.\n",
        "\n",
        "* The connection between the kernel and the Betti number is a powerful tool for topological data analysis. It allows us to use kernel methods to develop machine learning algorithms that are sensitive to topological features in data. This can be helpful for a variety of tasks, such as clustering, classification, and anomaly detection."
      ],
      "metadata": {
        "id": "HWu4pbDRVFQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the kernel of the combinatorial Laplacian?**\n",
        "\n",
        "* The kernel of the combinatorial Laplacian is the set of all vectors in the vector space that are orthogonal to all the eigenvectors of the Laplacian with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the Laplacian.\n",
        "\n",
        "* The kernel of the combinatorial Laplacian is important in topological data analysis because it is isomorphic to the homology groups of the underlying simplicial complex. This means that the kernel can be used to determine the Betti numbers of the simplicial complex, which are topological invariants that measure the number of connected components, holes, and voids in the complex.\n",
        "\n",
        "* For example, the kernel of the combinatorial Laplacian of a circle is one-dimensional, which corresponds to the fact that the circle has one Betti number, namely $\\beta_1=1$. This is because the only vector in the kernel of the Laplacian is the constant vector, which corresponds to the fact that the circle is a connected topological space.\n",
        "\n",
        "* In general, the dimension of the kernel of the combinatorial Laplacian is equal to the sum of the Betti numbers of the simplicial complex. **This means that the kernel can be used to count the number of connected components, holes, and voids in a simplicial complex.**\n",
        "\n",
        "* **The kernel of the combinatorial Laplacian can also be used to define a graph kernel, which is a similarity measure between graphs.** The graph kernel is based on the idea that two graphs are similar if their kernels are similar. This kernel has been shown to be effective for a variety of tasks in machine learning, such as graph classification and clustering.\n"
      ],
      "metadata": {
        "id": "EVXrfLEA43UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### optimization"
      ],
      "metadata": {
        "id": "1luzBIPqumre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/nphys3029: Quantum principal component analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "mXzaAafJadxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\xi$ be a precision parameter. Let there be efficient quantum access to the top $k$ right singular vectors $\\bar{V}^{(k)} \\in \\mathbb{R}^{m \\times k}$ of a matrix $A=U \\Sigma V^T \\in \\mathbb{R}^{n \\times m}$, such that $\\left\\|V^{(k)}-\\vec{V}^{(k)}\\right\\| \\leq \\frac{\\xi}{\\sqrt{2}}$. Given efficient quantum access to a row $a_i$ of $A$, the quantum state $\\left|\\bar{y}_i\\right\\rangle=\\frac{1}{\\left\\|\\bar{y}_i\\right\\|} \\sum_i^k \\bar{y}_k|i\\rangle$, proportional to its projection onto the PCA space, can be created in time $\\tilde{O}\\left(\\frac{\\left\\|a_i\\right\\|}{\\left\\|\\tilde{y}_i\\right\\|}\\right)$ with probability at least $1-1 / \\operatorname{poly}(m)$ and precision $\\|\\left|y_i\\right\\rangle-\\left|\\bar{y}_i\\right\\rangle \\| \\leq \\frac{\\left\\|a_i\\right\\|}{\\left\\|\\bar{y}_i\\right\\|} \\xi$. An estimate of $\\left\\|\\bar{y}_i\\right\\|$, to relative error $\\eta$, can be computed in $\\widetilde{O}(1 / \\eta)$."
      ],
      "metadata": {
        "id": "zBTjz6iladxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions\n",
        "\n",
        "https://arxiv.org/abs/1811.00414"
      ],
      "metadata": {
        "id": "VjcfkUbTadxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumalgorithms.org/dimensionality-reduction-1.html"
      ],
      "metadata": {
        "id": "o6fhjy72adxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video [QAOA in 5 min](https://youtu.be/IFIoMs7wffY?si=l-0CdnSuhMHKr7oS)"
      ],
      "metadata": {
        "id": "qZl5ZLU5adxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/UP-Zuke7IUg?si=mdlJKh-eb6sq0OOx\n",
        "\n",
        "\n",
        "https://youtu.be/RqGpnRh7rCM?si=IA5CPfisTgsDUCNj\n",
        "\n",
        "Video: [QAOA explained](https://youtu.be/tuGcPup8DjY?si=tdOyGsgvqozGIARY)\n",
        "\n",
        "Video: [QAOA for combinatorial optimization](https://youtu.be/AOKM9BkweVU?si=PrLHcdZ1wDAjYTnY)\n",
        "\n",
        "Video: [QAOA Peter Shor 2018]()https://youtu.be/HHIWUi3GmdM?si=c1hLXgeLCEkme_6R)"
      ],
      "metadata": {
        "id": "GBvEAlFTadxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/de/blogs/quantum-computing/citi-and-classiq-advance-quantum-solutions-for-portfolio-optimization/\n",
        "\n",
        "> From a computational point of view, the portfolio **optimization problem is NP-hard for minimization over binary or integer domains with additional constraints**. In simpler words, **the runtime to solve this problem grows exponentially as we increase the number of assets** (the size of ω). Therefore, finding ways of improving the runtime and the quality of the results is important when the number of assets is large.\n",
        "\n",
        "* QAOA is a hybrid algorithm, which means it combines both classical and quantum computation. The algorithm searches through all potential solutions using a mixer layer (which shuffles between alternative solutions) and a cost layer (which favors good ones). The circuit parameters are fine-tuned classically in an iterative manner, converging toward their optimal values with each run of QAOA.\n",
        "\n"
      ],
      "metadata": {
        "id": "mVMdmr7DadxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumai.google/cirq/experiments/qaoa/example_problems"
      ],
      "metadata": {
        "id": "zJpaBTGUadxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Computers for Combinatorial Optimization**\n",
        "\n",
        "https://arxiv.org/abs/2210.14936: A super-polynomial quantum-classical separation for density modelling\n",
        "\n",
        "* An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory\n",
        "\n",
        "* It is unclear to what extent quantum algorithms can outperform classical algorithms for problems of combinatorial optimization. In this work, by resorting to computational learning theory and cryptographic notions, we give a fully constructive proof that hashtag#quantumcomputers feature a super-polynomial advantage over classical computers in approximating combinatorial hashtag#optimization problems. Specifically, by building on seminal work by Kearns and Valiant, we provide special instances that are hard for classical computers to approximate up to polynomial factors. Simultaneously, we give a quantum algorithm that can efficiently approximate the optimal solution within a polynomial factor. The quantum advantage in this work is ultimately borrowed from Shor’s quantum algorithm for factoring. We introduce an explicit and comprehensive end-to-end construction for the advantage bearing instances. For such instances, quantum computers have, in principle, the power to approximate combinatorial optimization solutions beyond the reach of classical efficient algorithms.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1702.jpg)"
      ],
      "metadata": {
        "id": "SEUzNPOoadxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorization of Optimization Problem with different orders of their objective function**\n",
        "* Einführung: [Optimization_problem](https://en.m.wikipedia.org/wiki/Optimization_problem)\n",
        "* **First order**: [Linear optimization](https://en.m.wikipedia.org/wiki/Linear_programming): Many practical problems in [operations research](https://en.m.wikipedia.org/wiki/Operations_research) can be expressed as linear programming problems. See [Simplex_algorithm](https://en.m.wikipedia.org/wiki/Simplex_algorithm)\n",
        "* **Second Order**: [Quadratic optimization (Quadratic programming)](https://en.m.wikipedia.org/wiki/Quadratic_programming): finance (for portfolio optimization), machine learning (for support vector machines), and operations research. Example: In the Markowitz model, the objective function to be minimized is the portfolio variance, which is a quadratic function of the decision variables (asset weights in the portfolio), given the covariance matrix of the returns of the assets.\n",
        "* **Higher Order**: Polynomial optimization problems or non-linear optimization problems (optimize design of a machine part, where the objective function includes terms related to the volume of material used which might be cubic if we're considering three-dimensional parts. Often require more advanced techniques, such as [interior-point methods](https://de.m.wikipedia.org/wiki/Innere-Punkte-Verfahren), branch-and-bound techniques, or heuristic methods."
      ],
      "metadata": {
        "id": "c2PTGODWadxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optimization Algorithms*"
      ],
      "metadata": {
        "id": "txz-hKMQadxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Approximate Optimization Algorithm (QAOA)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Quantum_optimization_algorithms\n",
        "\n",
        "Farhi, Edward; Goldstone, Jeffrey; Gutmann, Sam (2014). \"A Quantum Approximate Optimization Algorithm\"\n",
        "\n",
        " arXiv:1411.4028: https://arxiv.org/abs/1411.4028"
      ],
      "metadata": {
        "id": "Tc9ZP_MDadxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "1cVTf0ftadxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation\n",
        "\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - clasislcayl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: glorothims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary, more: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but causes overhead increased asymptocatily like n to n^2 bits. Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practical engineer, our devices have this property. Required number of wires between qubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DKlPrRQnadxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system\n",
        "\n",
        "Imaginary time is a concept derived from quantum mechanics and statistical mechanics. It introduces the idea of replacing \"real time\" with \"imaginary time\" by a Wick rotation in the complex plane. That is, the time variable 't' is replaced with an imaginary number 'it', where 'i' is the imaginary unit.\n",
        "\n",
        "Imaginary time evolution plays a significant role in several areas of physics, including quantum field theory, statistical mechanics, and quantum computing.\n",
        "\n",
        "1. **Quantum Field Theory and Statistical Mechanics**: In these fields, the use of imaginary time is often a mathematical trick that simplifies calculations. By transforming to imaginary time, the calculations of quantum mechanics often become calculations in statistical mechanics. This is utilized in the technique called \"path integral formulation,\" where the evolution of a system in imaginary time makes the system go to its lowest energy state or the ground state.\n",
        "\n",
        "2. **Quantum Computing**: In quantum computing, the idea of imaginary time evolution can be used to design quantum algorithms for tasks such as finding the ground state of a system. This is used in the Quantum Approximate Optimization Algorithm (QAOA) and the Quantum Imaginary Time Evolution (QITE) algorithm.\n",
        "\n",
        "Remember that \"imaginary time\" is not about time in the sense that we experience it. Instead, it's a mathematical construct that physicists use to solve certain types of problems.\n",
        "\n",
        "Imaginary time is an unphysical, yet powerful, mathematical concept. It has been utilised in numerous physical domains, including quantum mechanics, statistical mechanics and cosmology. Often referred to as performing a ‘Wick rotation’\n",
        "\n",
        "https://www.nature.com/articles/s41534-019-0187-2"
      ],
      "metadata": {
        "id": "SfoNYhr6adxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Variational Quantum Eigensolver*"
      ],
      "metadata": {
        "id": "pqNjjLBMYen-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to Variational Quantum Algorithms, Michał Stęchły: https://arxiv.org/abs/2402.15879"
      ],
      "metadata": {
        "id": "2ljmkwo_KEVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Molecular Simulation with VQE](https://youtu.be/fJUzXbr5MTo?si=1JMrO5SZ0LRPjtYK)\n",
        "\n",
        "Pennylane: [A brief overview of VQE](https://pennylane.ai/qml/demos/tutorial_vqe/)\n",
        "\n",
        "Pennylane: [Accelerating VQEs with quantum natural gradient](https://pennylane.ai/qml/demos/tutorial_vqe_qng/)\n",
        "\n",
        "Science: [The Variational Quantum Eigensolver: A review of methods and best practices](https://www.sciencedirect.com/science/article/pii/S0370157322003118)"
      ],
      "metadata": {
        "id": "U17xCMOzQVGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane\n",
        "import pennylane as qml"
      ],
      "metadata": {
        "id": "qflJUgxEoaRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq -q\n",
        "import cirq"
      ],
      "metadata": {
        "id": "Y_exZOM0olAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from jax import numpy as np\n",
        "import jax\n",
        "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
        "jax.config.update('jax_enable_x64', True)\n",
        "\n",
        "symbols = [\"H\", \"H\"]\n",
        "coordinates = np.array([0.0, 0.0, -0.6614, 0.0, 0.0, 0.6614])"
      ],
      "metadata": {
        "id": "WL3XibcPop69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H, qubits = qml.qchem.molecular_hamiltonian(symbols, coordinates)\n",
        "print(\"Number of qubits = \", qubits)\n",
        "print(\"The Hamiltonian is \", H)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCxAGQMZoUC8",
        "outputId": "b4b0ccf4-c394-41e0-882b-1109b7f78836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of qubits =  4\n",
            "The Hamiltonian is    (-0.2427450126094144) [Z2]\n",
            "+ (-0.2427450126094144) [Z3]\n",
            "+ (-0.042072551947439224) [I0]\n",
            "+ (0.1777135822909176) [Z0]\n",
            "+ (0.1777135822909176) [Z1]\n",
            "+ (0.12293330449299361) [Z0 Z2]\n",
            "+ (0.12293330449299361) [Z1 Z3]\n",
            "+ (0.16768338855601356) [Z0 Z3]\n",
            "+ (0.16768338855601356) [Z1 Z2]\n",
            "+ (0.17059759276836803) [Z0 Z1]\n",
            "+ (0.1762766139418181) [Z2 Z3]\n",
            "+ (-0.044750084063019925) [Y0 Y1 X2 X3]\n",
            "+ (-0.044750084063019925) [X0 X1 Y2 Y3]\n",
            "+ (0.044750084063019925) [Y0 X1 X2 Y3]\n",
            "+ (0.044750084063019925) [X0 Y1 Y2 X3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = qml.device(\"lightning.qubit\", wires=qubits)"
      ],
      "metadata": {
        "id": "lNzMZ21Ro9KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "electrons = 2\n",
        "hf = qml.qchem.hf_state(electrons, qubits)\n",
        "print(hf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0XkmxHrpcA_",
        "outputId": "b47538a5-7ca1-459c-c68f-7819b145e1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev)\n",
        "def circuit(param, wires):\n",
        "    qml.BasisState(hf, wires=wires)\n",
        "    qml.DoubleExcitation(param, wires=[0, 1, 2, 3])\n",
        "    return qml.expval(H)"
      ],
      "metadata": {
        "id": "9D8Tz5KLpZ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fn(param):\n",
        "    return circuit(param, wires=range(qubits))"
      ],
      "metadata": {
        "id": "SC484qkipfCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library [optax](https://optax.readthedocs.io/en/latest/) offers different optimizers. Here we use a basic gradient-descent optimizer."
      ],
      "metadata": {
        "id": "lSRK_jEbpj85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "max_iterations = 100\n",
        "conv_tol = 1e-06\n",
        "\n",
        "opt = optax.sgd(learning_rate=0.4)"
      ],
      "metadata": {
        "id": "JIocB1FFpiNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.array(0.)\n",
        "\n",
        "# store the values of the cost function\n",
        "energy = [cost_fn(theta)]\n",
        "\n",
        "# store the values of the circuit parameter\n",
        "angle = [theta]\n",
        "\n",
        "opt_state = opt.init(theta)\n",
        "\n",
        "for n in range(max_iterations):\n",
        "\n",
        "    gradient = jax.grad(cost_fn)(theta)\n",
        "    updates, opt_state = opt.update(gradient, opt_state)\n",
        "    theta = optax.apply_updates(theta, updates)\n",
        "\n",
        "    angle.append(theta)\n",
        "    energy.append(cost_fn(theta))\n",
        "\n",
        "    conv = np.abs(energy[-1] - energy[-2])\n",
        "\n",
        "    if n % 2 == 0:\n",
        "        print(f\"Step = {n},  Energy = {energy[-1]:.8f} Ha\")\n",
        "\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "\n",
        "print(\"\\n\" f\"Final value of the ground-state energy = {energy[-1]:.8f} Ha\")\n",
        "print(\"\\n\" f\"Optimal value of the circuit parameter = {angle[-1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_-Pae1dpxIX",
        "outputId": "3125c07b-6da0-4347-8aa2-9d7e3ada3d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pennylane/math/utils.py:227: UserWarning: Contains tensors of types {'jax', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step = 0,  Energy = -1.12799983 Ha\n",
            "Step = 2,  Energy = -1.13466246 Ha\n",
            "Step = 4,  Energy = -1.13590595 Ha\n",
            "Step = 6,  Energy = -1.13613667 Ha\n",
            "Step = 8,  Energy = -1.13617944 Ha\n",
            "Step = 10,  Energy = -1.13618736 Ha\n",
            "Step = 12,  Energy = -1.13618883 Ha\n",
            "\n",
            "Final value of the ground-state energy = -1.13618883 Ha\n",
            "\n",
            "Optimal value of the circuit parameter = 0.2089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_figheight(5)\n",
        "fig.set_figwidth(12)\n",
        "\n",
        "# Full configuration interaction (FCI) energy computed classically\n",
        "E_fci = -1.136189454088\n",
        "\n",
        "# Add energy plot on column 1\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.plot(range(n + 2), energy, \"go\", ls=\"dashed\")\n",
        "ax1.plot(range(n + 2), np.full(n + 2, E_fci), color=\"red\")\n",
        "ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
        "ax1.set_ylabel(\"Energy (Hartree)\", fontsize=13)\n",
        "ax1.text(0.5, -1.1176, r\"$E_\\mathrm{HF}$\", fontsize=15)\n",
        "ax1.text(0, -1.1357, r\"$E_\\mathrm{FCI}$\", fontsize=15)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add angle plot on column 2\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(range(n + 2), angle, \"go\", ls=\"dashed\")\n",
        "ax2.set_xlabel(\"Optimization step\", fontsize=13)\n",
        "ax2.set_ylabel(\"Gate parameter $\\\\theta$ (rad)\", fontsize=13)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3, bottom=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "H3oayNtGp1RT",
        "outputId": "8c085e3c-52d8-4eea-9200-2d95c8328d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Dequantization and Quantum-Inspired Algorithms*"
      ],
      "metadata": {
        "id": "mL9gnsMGYrOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper: [Ewin Tang: Quantum Machine Learning Without Any Quantum](https://inspirehep.net/literature/2716091)"
      ],
      "metadata": {
        "id": "OuJiSsXIeITe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewan Tang is known for finding a classical algorithm that solves the problem of recommendation systems as efficiently as the best-known quantum algorithm. The quantum version of the problem was solved using an algorithm called Quantum Singular Value Transformation (QSVD). The classical version, however, had no known efficient algorithm, until Tang's work.\n",
        "\n",
        "Ewan Tang's dequantization approach takes advantage of the structure of the QSVD algorithm. He noted that the QSVD operates on sparse, low-rank matrices, and developed a classical algorithm that could handle this structure efficiently.\n",
        "\n",
        "Here's a simplified overview of Tang's algorithm:\n",
        "\n",
        "1. **Input Data Conversion**: Transform the input data into a sparse, low-rank format. In the context of recommendation systems, this might mean constructing a matrix where the rows correspond to users, the columns correspond to products, and each entry reflects the user's preference for the product.\n",
        "\n",
        "2. **Sampling Procedure**: Use a randomized sampling procedure to select a small number of \"landmark\" columns (products). The selection probability for each column is proportional to its contribution to the total preference scores.\n",
        "\n",
        "3. **Low-Rank Approximation**: Construct a low-rank approximation of the original preference matrix based on the sampled columns.\n",
        "\n",
        "4. **Estimation**: Use the low-rank approximation to estimate the user's preferences for all other products.\n",
        "\n",
        "This is a simplified version of the process. The actual algorithm involves a lot of linear algebra, randomization, and complexity analysis. If you're interested in the full technical details, you can look up Ewan Tang's paper \"A quantum-inspired classical algorithm for recommendation systems\".\n",
        "\n",
        "To summarize, the dequantization process involved taking a quantum algorithm and identifying the aspects of the quantum algorithm that could be implemented classically, thereby creating a classical version of the algorithm with similar efficiency."
      ],
      "metadata": {
        "id": "BNQYlE1k42Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum-inspired Algorithms**\n",
        "\n",
        "* quantum computers still exhibit an exponential speedup over all known classical algorithms for **sparse, full-rank matrix problems**, including the quantum Fourier transform, eigenvector and eigenvalue analysis, linear systems, and others https://arxiv.org/abs/1905.10415\n",
        "* Quantum algorithms for linear algebra are a flagship application of quantum computing, partic- ularly due to their relevance in machine learning. These algorithms typically [scale polylogarithmically](https://en.wikipedia.org/wiki/Polylogarithmic_function) with dimension, which, at the time they were reported, implied an asymptotic exponential speedup compared to state-of-the-art classical methods. For this reason, significant interest has been generated in the dequantization approach that led to breakthrough quantum- inspired classical algorithms for linear algebra problems with sublinear complexity https://arxiv.org/abs/1905.10415\n",
        "\n",
        "* https://crypto.stackexchange.com/questions/48638/whats-the-difference-between-polylogarithmic-and-logarithmic\n",
        "\n",
        "* For example, matrix chain ordering can be solved in polylogarithmic time on a Parallel Random Access Machine."
      ],
      "metadata": {
        "id": "mUzaGhCi1Y26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dequantization a quantum algorithm**\n",
        "* Not all problems are amenable to quantum speed-up. Quantum algorithms are specifically efficient for problems where a superposition of solutions can be created and manipulated. Linear algebra problems, especially in high dimensions, may not easily lend themselves to such superposition-based speed-ups.\n",
        "* **Output limitations**: Even if a quantum computer can perform a computation faster, it might not be able to provide the result in a way that's useful or can be checked in a reasonable time by a classical computer. This limitation is a crucial aspect in quantum linear algebra problems. For example, **you can load a vector into a quantum state exponentially faster, but you cannot read out all the entries in less than exponential time due to the 'no-cloning' theorem of quantum mechanics.** (limits ability to fully read out the state of a quantum system: each measurement only gives you a tiny piece of the picture, and you would have to repeat the process an exponential number of times to see the full state.)\n",
        "* Tang’s algorithm builds on a classical data structure called a \"tree of singular vectors,\" which is used to approximate the product of the preference matrix with a vector that represents a new user's preferences. This approximation is good enough to recommend items in a way that's nearly as good as the quantum algorithm.\n",
        "* The approach relies on a form of algorithm called \"randomized sampling algorithms\" and works by creating a probability distribution that closely matches the one that would have been created by a quantum computer. By cleverly sampling from this distribution, Tang's algorithm can estimate the answers to the recommendation problem almost as accurately as the quantum version.\n",
        "* Tang's algorithm relies heavily on the specifics of the recommendation system problem: the user-product preference matrix is \"low rank.\" Therefore, it doesn't mean that all quantum algorithms can be dequantized in the same way.\n",
        "* https://arxiv.org/abs/1905.10415 with https://www.math.cmu.edu/~af1p/Texfiles/SVD.pdf\n",
        "* There are two ways to compute the average of many values. A first method is to add all of them together and divide the result by the total number of values. An alternative approach is to select a few values at random, then compute their average using the first method. Of course, in the second case we get only an approximation of the true average, but in many cases that is good enough and can be done in significantly less time.\n",
        "* **This is the strategy of quantum-inspired algorithms: the coefficients of the solution vector are inner products between vectors, which can be expressed as the average of a collection of numbers that depend on the entries of the vectors**. Instead of directly computing this average (which takes linear time) we select a few of the values at random and compute their average instead.\n",
        "* The question is, how many values do we need to sample to get a good approximation? It turns out that **the number of samples grows with the precision of the approximation, the rank, and the condition number of the input matrix.**\n",
        "* In practice, this estimation method is only faster than a direct calculation of the coefficients if (i) precision, rank, and condition number are small, and (ii) the input matrix has a colossal dimension.\n",
        "* In practice, it pays off to be slightly more sophisticated: we keep each number with likelihood proportional to its value, such that large numbers are kept with high probability and small numbers with low probability. This is the basis of a technique called [rejection sampling](https://en.m.wikipedia.org/wiki/Rejection_sampling), **which is employed in quantum-inspired algorithms to sample from the solution vectors**. Once the approximate singular value decomposition has been obtained and the coefficients have been estimated, it is possible to query any entry of the solution vector in sublinear time.\n",
        "* https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n",
        "* https://cstheory.stackexchange.com/questions/42338/list-of-quantum-inspired-algorithms\n",
        "* https://github.com/XanaduAI/quantum-inspired-algorithms"
      ],
      "metadata": {
        "id": "w5EmlC2n2rN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tensor Networks*"
      ],
      "metadata": {
        "id": "vUaRXgHIgW2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://iopscience.iop.org/article/10.1088/2058-9565/aaea94\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_tn_circuits#huggins"
      ],
      "metadata": {
        "id": "TJ5Ir5fTM-DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor networks applied in quantum-inspired algorithms?** Solving Optimization Problems: Tensor networks can also be used to solve optimization problems. By mapping the problem onto a tensor network, we can use techniques like the Density Matrix Renormalization Group (DMRG) to find the optimal solution. This is particularly useful for problems where the number of variables is large, as is often the case in optimization problems."
      ],
      "metadata": {
        "id": "VyCQ6sLRpzRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**are Tensor Networks a form of quantum-inspired algorithm or not?**\n",
        "\n",
        "Tensor networks are a mathematical tool widely used in many-body quantum physics and quantum information theory. They provide a way to efficiently represent and manipulate high-dimensional tensors (or high-dimensional arrays of numbers), which frequently occur in these fields.\n",
        "\n",
        "Quantum many-body systems, which involve multiple interacting quantum particles, give rise to high-dimensional tensors. When these interactions are local, it turns out that the resulting state of the system can be represented more efficiently with a tensor network, allowing for computational tractability.\n",
        "\n",
        "However, tensor networks are not inherently quantum-inspired algorithms. They are more of a framework or a technique for representing complex multi-dimensional data structures, which can be used in the development of algorithms for quantum and classical computations.\n",
        "\n",
        "Where the quantum inspiration comes in is their application: tensor networks are extensively used to study quantum systems. For instance, they are used in algorithms for simulating quantum systems, understanding quantum entanglement, quantum error correction, and so on. They have also found applications beyond quantum physics, such as machine learning and artificial intelligence.\n",
        "\n",
        "In summary, tensor networks themselves are not quantum-inspired algorithms, but they are a critical tool in the development and application of algorithms (including quantum-inspired ones) used in quantum physics and quantum computing."
      ],
      "metadata": {
        "id": "Zn8-X-V23faS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tensor networks are mathematical constructs that are used to represent and manipulate quantum states, particularly those with many parts or \"qubits\".\n",
        "\n",
        "* In the context of quantum computing, each number in the tensor can represent an amplitude of a quantum state.\n",
        "\n",
        "* A tensor network is a particular way of organizing and manipulating these tensors. In a tensor network, each tensor is represented as a node, and the indices of the tensor are represented as edges connecting the nodes. By organizing the tensors in this way, **certain computations can be performed more efficiently, particularly those involving high-dimensional tensors**.\n",
        "\n",
        "* The real power of tensor networks comes from their ability to efficiently represent and manipulate certain types of quantum states, particularly those that exhibit some form of entanglement. **In many physical systems, and particularly in systems that are near their ground state, the quantum state of the system can be well approximated by a tensor network with a relatively simple structure**.\n",
        "\n",
        "* This makes **tensor networks a very useful tool for simulating such systems on a classical computer**.\n",
        "\n",
        "* For instance, the class of states that can be efficiently represented by a tensor network includes many states that arise in condensed matter physics and quantum chemistry, as well as some states that are used in quantum error correction and quantum computing.\n",
        "\n",
        "* One common type of tensor network is the [Matrix Product State](https://en.m.wikipedia.org/wiki/Matrix_product_state) (MPS), which is used in the [Density Matrix Renormalization Group](https://en.m.wikipedia.org/wiki/Density_matrix_renormalization_group) (DMRG) method, a powerful method for simulating one-dimensional quantum systems.\n",
        "\n",
        "  * The density matrix renormalization group (DMRG) is a numerical variational technique devised to obtain the [low-energy physics](https://en.m.wikipedia.org/wiki/Macroscopic_scale) of quantum many-body systems with high accuracy. As a variational method, DMRG is an efficient algorithm that attempts to find the lowest-energy matrix product state wavefunction of a Hamiltonian.\n",
        "\n",
        "( Other types of tensor networks include the Projected Entangled Pair State (PEPS), the Multi-scale Entanglement Renormalization Ansatz (MERA), and others."
      ],
      "metadata": {
        "id": "vtMw-E_BgcNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Annealer and Adiabatic Algorithms*"
      ],
      "metadata": {
        "id": "SfaztpziY2sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2403.00910: Computational supremacy in quantum simulation"
      ],
      "metadata": {
        "id": "WI3oZ_mYPQg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*QUBO*"
      ],
      "metadata": {
        "id": "egmKQRu3GHft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mdr-inc/quadratic-unconstrained-binary-optimization-qubo-on-dwave-chimera-graph-part-1-a7eb05e3f155"
      ],
      "metadata": {
        "id": "RiSZ_jhPGJL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Speedup in Combinatorial Optimization*"
      ],
      "metadata": {
        "id": "tnFQtxBD9KEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Speedup in Combinatorial Optimization: the status**\n",
        "\n",
        "A pivotal challenge in quantum information science's ever-evolving realm is designing quantum algorithms that outpace their classical counterparts. A recent experimental observation showcased a superlinear quantum speedup in solving the Maximum Independent Set problem on specific graph instances. This has spurred the development of a comprehensive theoretical framework to compare the performance of quantum adiabatic algorithms and classical Markov chain Monte Carlo algorithms.\n",
        "\n",
        "The Quantum-Classic Conundrum\n",
        "\n",
        "Combinatorial optimization problems, foundational to modern computer science, encompass NP-hard problems that remain elusive to efficient solutions through known algorithms. While classical combinatorial optimization algorithms focus on minimizing a cost function over bit strings, Quantum adiabatic algorithms (QAAs) serve as their quantum analogs, preparing low-energy states of a classical cost Hamiltonian through adiabatic evolution. However, the relative efficacy of QAA and classical counterparts like simulated annealing (SA) remains a topic of debate and exploration.\n",
        "\n",
        "From Theory to Application\n",
        "\n",
        "The recent experimental results, particularly the superlinear speedup observed using a programmable Rydberg atom array, have ignited a renewed interest in this domain. This study presents a theoretical framework that zeroes in on problem instances characterized by flat energy landscapes. Here, the quantum algorithm's performance hinges on the (de)localization of the low-energy eigenstates of the adiabatic Hamiltonian. When these eigenstates are delocalized and the quantum evolution is optimized, the QAA can achieve a significant speedup over classical algorithms like SA.\n",
        "\n",
        "Toward a Quantum Future\n",
        "\n",
        "Building on this foundation, the researchers introduce a modified QAA that showcases a quadratic speedup over SA on specific problem instances. This algorithm employs local Hamiltonians without a sign problem, making them more amenable to quantum Monte Carlo simulations. The study concludes by applying these insights to interpret experimental observations, identifying instances where quantum algorithms either outperform or underperform classical ones due to the localization properties of the low-energy eigenstates.\n",
        "\n",
        "In Summary\n",
        "\n",
        "This work offers a deep dive into the intricate dance between quantum and classical algorithms in the context of combinatorial optimization. Shedding light on the conditions under which quantum algorithms can achieve a speedup paves the way for further advancements in the quantum computing landscape.\n",
        "\n",
        "Link to the publication:\n",
        "\n",
        "https://arxiv.org/pdf/2306.13123.pdf\n"
      ],
      "metadata": {
        "id": "gh1J-ajfc5_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quantum annealing is a quantum computing algorithm that can be used to solve a variety of problems, including NP-hard problems.\n",
        "* However, the efficiency of quantum annealing is limited by the **exponential closing of the spectral gap with increasing system size.**\n",
        "* The spectral gap is a measure of the connectivity of a quantum system. **A large spectral gap means that the system is well-connected, while a small spectral gap means that the system is poorly connected**. The exponential closing of the spectral gap means that the spectral gap decreases exponentially with increasing system size.\n",
        "* This means that quantum annealing becomes exponentially slow for solving NP-hard problems as the system size increases. This is because **the quantum system becomes more and more poorly connected as the system size increases, and it takes longer and longer for the system to reach its ground state**.\n",
        "* There are a number of ways to address the problem of exponential closing of the spectral gap. One approach is to use a technique called adiabatic quantum computation. Adiabatic quantum computation is a variation of quantum annealing that uses a slower annealing schedule. This can help to prevent the spectral gap from closing too quickly.\n",
        "* Another approach to addressing the problem of exponential closing of the spectral gap is to use a technique called quantum error correction. Quantum error correction is a technique that can be used to protect quantum systems from noise. This can help to prevent the quantum system from becoming too poorly connected as the system size increases.\n",
        "* The problem of exponential closing of the spectral gap is a major challenge for quantum annealing. However, there are a number of promising approaches to addressing this problem. As quantum annealing technology continues to develop, it is possible that quantum annealing will become a viable solution for solving NP-hard problems.\n"
      ],
      "metadata": {
        "id": "8eSN4dg5Y2sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Kerr ising model*"
      ],
      "metadata": {
        "id": "_UQPJ8Oz9iXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kerr and ising gate: non linear gates\n",
        "\n",
        "* The Kerr gate is a gate that introduces nonlinearity into a quantum system. The Kerr gate can be used to implement a variety of quantum operations, such as quantum logic gates and quantum simulations.\n",
        "\n",
        "* The quartic gate is a gate that introduces even higher-order nonlinearity into a quantum system. The quartic gate can be used to implement a variety of quantum operations, such as quantum error correction and quantum machine learning.\n",
        "\n",
        "* The cubic gate is a gate that introduces cubic nonlinearity into a quantum system. The cubic gate can be used to implement a variety of quantum operations, such as quantum cryptography and quantum teleportation."
      ],
      "metadata": {
        "id": "9nkXUBDaY2sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the Kerr ising model?**\n",
        "\n",
        "\n",
        "The Kerr Ising model (KIM) is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity. The Ising model is a classical model of interacting spins, while the Kerr nonlinearity is a quantum effect that describes the interaction of light with matter.\n",
        "\n",
        "The KIM is described by the following Hamiltonian:\n",
        "\n",
        "> $H = -\\sum_{i,j} J_{ij} \\sigma^z_i \\sigma^z_j - \\sum_i \\frac{\\alpha}{2} \\sigma^x_i^2$\n",
        "\n",
        "where $\\sigma^z_i$ and $\\sigma^x_i$ are Pauli matrices that represent the spin of the $i$th qubit, $J_{ij}$ is the Ising coupling strength between the $i$th and $j$th qubits, and $\\alpha$ is the Kerr nonlinearity coefficient.\n",
        "\n",
        "The KIM has been studied in a variety of contexts, including quantum annealing, quantum simulation, and quantum information processing. It has been shown that the KIM can be used to solve certain NP-complete problems, and it can also be used to simulate the dynamics of other quantum many-body systems.\n",
        "\n",
        "Here are some of the key features of the Kerr Ising model:\n",
        "\n",
        "* It is a quantum many-body model that combines the features of the Ising model and the Kerr nonlinearity.\n",
        "* It has been shown to be useful for quantum annealing, quantum simulation, and quantum information processing.\n",
        "* It has been studied in a variety of contexts, including theoretical studies and experimental implementations.\n",
        "\n",
        "The Kerr Ising model is a promising platform for a variety of quantum applications. As our understanding of the model continues to grow, we can expect to see even more exciting applications in the future.\n",
        "\n",
        "https://iopscience.iop.org/article/10.1088/1367-2630/ab7255"
      ],
      "metadata": {
        "id": "nMEaxCxVY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*adiabatic quantum computing and quantum annealing*"
      ],
      "metadata": {
        "id": "JFVnpVvY9T1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the difference between adiabatic quantum computing and quantum annealing?**\n",
        "\n",
        "Adiabatic quantum computing (AQC) and quantum annealing (QA) are both approaches to quantum computing that use a time-dependent Hamiltonian to solve problems. However, there are some key differences between the two approaches.\n",
        "\n",
        "In AQC, the Hamiltonian is slowly varied from an initial Hamiltonian that is easy to prepare to a final Hamiltonian that encodes the solution to the problem. The system is then allowed to evolve adiabatically, meaning that it remains in its ground state throughout the evolution. This ensures that the system will end up in the ground state of the final Hamiltonian, which is the solution to the problem.\n",
        "\n",
        "In QA, the Hamiltonian is also slowly varied from an initial Hamiltonian to a final Hamiltonian. However, the final Hamiltonian does not necessarily encode the solution to the problem. Instead, the goal of QA is to find the ground state of the final Hamiltonian, which is a global minimum of the energy landscape. This can be useful for solving optimization problems, where the goal is to find the minimum value of a function.\n",
        "\n",
        "Another difference between AQC and QA is that AQC is a universal model of quantum computation, while QA is not. This means that AQC can be used to solve any problem that can be solved by a classical computer, while QA is only able to solve optimization problems.\n",
        "\n",
        "In practice, QA is often used to solve problems that are difficult for classical computers to solve. This is because the adiabatic evolution can help the system to tunnel through energy barriers that would be insurmountable for a classical computer. However, there is still some debate about whether QA is truly a quantum-mechanical phenomenon, or whether it can be explained by classical physics.\n",
        "\n",
        "Here is a table that summarizes the key differences between AQC and QA:\n",
        "\n",
        "| Feature | Adiabatic Quantum Computing | Quantum Annealing |\n",
        "|---|---|---|\n",
        "| Hamiltonian | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that encodes the solution to the problem. | Varied slowly from an easy-to-prepare initial Hamiltonian to a final Hamiltonian that is a global minimum of the energy landscape. |\n",
        "| Universality | Yes | No |\n",
        "| Applications | Any problem that can be solved by a classical computer | Optimization problems |\n",
        "| Quantum-mechanical phenomenon | Yes | Possibly |\n"
      ],
      "metadata": {
        "id": "rbIyjx5hY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104129"
      ],
      "metadata": {
        "id": "wRJv1ngPY2se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exponential Closing of Spectral Gap*"
      ],
      "metadata": {
        "id": "QnYJ0XpX82sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/math-ph/0507008: Spectral Gap and Exponential Decay of Correlations\n",
        "\n",
        "https://arxiv.org/abs/2212.13649 : Why Adiabatic Quantum Annealing is unlikely to yield speed-up"
      ],
      "metadata": {
        "id": "ZlZovX7quE9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity and Solution Finding in Quantum Annealing**\n",
        "\n",
        "Quantum annealing is a quantum computing technique used to find the minimum value of a given objective function over a defined set of possible solutions. It draws inspiration from classical annealing, a process in which a system is gradually cooled to find its lowest energy state. The hope in quantum annealing is to leverage quantum properties, such as superposition and tunneling, to more efficiently explore the solution space and find the ground state, which represents the minimum of the objective function.\n",
        "\n",
        "Connectivity plays an essential role in this process. Let's understand this with a few key points:\n",
        "\n",
        "1. **Local Minima and Global Minima**: In complex optimization problems, there are often many local minima. Finding the global minimum (or a sufficiently good approximation of it) is the main challenge. High connectivity means that the system can more easily explore the solution space, jumping from one potential solution to another and avoiding getting trapped in local minima.\n",
        "\n",
        "2. **Quantum Tunneling**: One of the primary advantages of quantum annealing over classical annealing is quantum tunneling. Tunneling allows the system to \"pass through\" barriers between states, meaning that the system can more easily escape local minima and move towards the global minimum. If the quantum system's connectivity is low, the benefit from quantum tunneling can be diminished.\n",
        "\n",
        "3. **Effective Exploration of Solution Space**: In a highly connected system, any given qubit (quantum bit) is more likely to interact with many other qubits. <font color=\"blue\">This allows for a richer exploration of the solution space. With fewer connections, the qubits are limited in their interactions, making the exploration less effective.</font>\n",
        "\n",
        "4. **Scaling Issues**: <font color=\"blue\">as the system size increases, maintaining high connectivity becomes more challenging. The difficulty in keeping high connectivity in larger systems is one reason why quantum annealing's efficiency decreases for more complex NP-hard problems</font>. The sparser the connectivity, the less able the system is to effectively leverage quantum effects.\n",
        "\n",
        "5. **Error Correction and Noise**: Highly connected quantum systems can be more susceptible to certain types of errors or noise. So, while high connectivity can aid in the exploration of solution space, it can also introduce challenges in terms of maintaining the coherence and reliability of the quantum information.\n",
        "\n",
        "In summary, high connectivity in quantum annealing is crucial for effectively exploring the solution space, leveraging quantum effects like tunneling, and ensuring the system doesn't get trapped in local minima. As system sizes increase, maintaining this connectivity becomes more challenging, which contributes to the reduced efficiency of quantum annealing for large NP-hard problems."
      ],
      "metadata": {
        "id": "IKNrTGMaY2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Does spectral gap stand in quantum annealing for difference between global and local optimum, or graph connectivity?* For graph connectivity it would be bad to have a small spectral gap, meanhwile for optimum it would be potentially good.\n",
        "\n",
        "*Spectral gap stands for graph connectivity. if that is low, then there are fewer connections in the graph, and that results that it's harder to find the global optimum*"
      ],
      "metadata": {
        "id": "7eO90_7EY2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Around the same time, it was argued that QA is exponentially slow for solving NP-hard problems due to exponential closing of the spectral gap with increasing system size*\n",
        "https://arxiv.org/pdf/2212.13649.pdf\n",
        "\n",
        "[20]  Jorg, T., Krzakala, F., Kurchan, J. & Maggs, A. Simple glass models and their quantum annealing. Physical review letters 101, 147204 (2008).\n",
        "[21]  Altshuler, B., Krovi, H. & Roland, J. Adiabatic quantum optimization fails for random in- stances of np-complete problems. arXiv preprint arXiv:0908.2782 (2009).\n",
        "\n",
        "*exponential closing of the quantum gap with increasing problem size* https://www.nature.com/articles/s41467-018-05239-9\n",
        "\n",
        "*The efficiency of Adiabatic Quantum Annealing is limited by the scaling with system size of the minimum gap that appears between the ground and first excited state in the annealing energy spectrum. In general the algorithm is unable to find the solution to an optimisation problem in polynomial time due to the presence of avoided level crossings at which the gap size closes exponentially with system size.* https://arxiv.org/pdf/2203.06779.pdf\n",
        "  * A particular problem which has been highlighted is the potential for so called perturbative crossings to form between the low energy eigenstates of the problem Hamiltonian towards the end of the anneal [11, 12]. The corresponding energy gap has been found to be exponentially small with the Hamming distance between the eigenstates, which can generally be expected to grow with the system size [11]."
      ],
      "metadata": {
        "id": "IQZSg2D4Y2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In quantum annealing, an exponential spectral gap generally presents a challenge** = energy difference between ground state and first excited state is very small. This small gap can make it harder for the quantum annealer to distinguish between the two states, especially when there's noise in the system. \"However, efficiency of quantum annealing is limited by the exponential closing of the spectral gap with increasing system size.\" Problems:\n",
        "  * In order to stay close to ground state throughout annealing process, evolution parameter \\( s \\) or \\( t \\) needs to change very slowly. This can lead to very long quenching times, making annealing process computationally inefficient.\n",
        "  * Thermal Excitations: thermal fluctuations can easily push system out of ground state because energy difference is small. This makes finding the correct solution (the ground state) less likely.\n",
        "  * Small spectral gap implies sensitivity to external noise. Small gap can cause system to be easily perturbed away from desired evolution.\n",
        "  * Techniques and strategies, like optimized annealing schedules or error-correction methods, may be needed to cope with these challenges.\n",
        "* An exponential spectral gap might mean that the first excited state is close to ground state in terms of energy, the implications for solution quality can vary. Depending on the problem, this could mean you have a nearly optimal solution, or it could mean you're still far from the best possible configuration.\n",
        "  1. **Nature of the Problem**: The importance of the energy difference can vary depending on the nature of the problem you're trying to solve. In some optimization problems, even a small energy difference might correspond to a significant difference in solution quality. In others, the difference might be negligible in practical terms.\n",
        "  2. **Many-body Systems**: Quantum annealing is often applied to complex many-body systems. In these systems, the difference in energy between the ground state and the first excited state might correspond to a complex reconfiguration of many particles or spins. So, the \"distance\" between these states in terms of system configuration might be significant even if their energy difference is small.\n",
        "  3. **Landscapes with Many Local Minima**: In complex optimization problems, there can be many local minima (sub-optimal solutions that are better than their immediate neighbors). The presence of an exponential spectral gap might indicate that the system is easily getting trapped in one of these local minima. While the first excited state might be close in energy to the ground state, there might be other states with much higher energies that are also close in terms of the system's configuration.\n",
        "  4. **Goal of Quantum Annealing**: The primary goal of quantum annealing is to find the global minimum (or a very close approximation to it). So, while the first excited state might offer a \"good\" solution, the aim is typically to find the \"best\" solution, especially in problems where small differences can have amplified outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "sDrzyqJlY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Scott Aaronson - Dismantling quantum hype](https://youtu.be/qs0D9sdbKPU)"
      ],
      "metadata": {
        "id": "A8U-2gdBY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@quantum_wa/quantum-annealing-cdb129e96601\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Annealing_(materials_science)"
      ],
      "metadata": {
        "id": "rneoSBMUY2sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Gap (and and Combinatorial (Graph) Laplacians)**\n",
        "\n",
        "* Spectral gap = **measure of connectivity of a quantum system** and **difference between second smallest and smallest eigenvalues** of normalized Laplacian matrix of graph (smallest eigenvalue is always zero for connected graphs)\n",
        "\n",
        "* Eigenvalues: $\\lambda_0 \\leq \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_{n-1}$, $n$ is number of nodes (vertices) in graph. Spectral gap: $ \\lambda_1$ = smallest non-zero eigenvalue.\n",
        "\n",
        "* Graph with large spectral gap $\\lambda_1 >> 0$ is well-connected. Graph with a small spectral gap is poorly connected.\n",
        "  * It is related to the graph's **Cheeger constant**: The Cheeger constant is a measure of the graph's connectivity that is closely related to the spectral gap.\n",
        "  * It is related to the **graph's mixing time**: The mixing time is the time it takes for a random walk on the graph to converge to its **stationary distribution**. A graph with a **large spectral gap has a short mixing time**, while a graph with a small spectral gap has a long mixing time (relevant in quantum annealing). Similar: **Markov Chains**: In the study of Markov chains, the spectral gap is related to the rate of convergence to the stationary distribution. A larger spectral gap typically means faster convergence.\n",
        "  * Cluster analysis: The spectral gap can be used to find clusters in graphs.\n",
        "  * Graph partitioning: The spectral gap can be used to partition graphs into two pieces with similar properties.\n",
        "  * Random walks: The spectral gap can be used to analyze the behavior of random walks on graphs.\n",
        "  * Diffusion processes: The spectral gap can be used to analyze the behavior of diffusion processes on graphs.\n",
        "* In TDA the smallest non-zero eigenvalue of combinatorial Laplacian is related to number of connected components in the data. The second smallest non-zero eigenvalue is related to the number of holes in the data.\n",
        "  * In gene expression data analysis, the combinatorial Laplacian can be used to identify clusters of genes that are co-expressed. This can be helpful for finding genes that are involved in the same biological process.\n",
        "  * In protein interaction data analysis, the combinatorial Laplacian can be used to identify protein complexes. This can be helpful for understanding how proteins interact with each other to carry out biological functions.\n",
        "  * In image data analysis, the combinatorial Laplacian can be used to identify objects in images. This can be helpful for image segmentation and classification."
      ],
      "metadata": {
        "id": "EeisMgf-84pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Spectral Gap**\n",
        "\n",
        "* Exponential spectral gap: spectral gap decreases exponentially with respect to some parameter of system.\n",
        "As system size grows, **difference between ground state energy (lowest eigenvalue) and first excited state (next smallest eigenvalue) might decrease exponentially**. This can have important implications for the system's behavior, especially in the context of quantum phase transitions or the computational hardness of approximating certain states.\n",
        "\n",
        "* **The exact interpretation and importance of an exponential spectral gap can vary depending on the context**. In some cases, having such a gap is favorable for certain properties or behaviors, while in other contexts, it might pose challenges or signal specific types of behavior.\n",
        "\n",
        "* In spectral graph theory, an exponential spectral gap is a property of a graph that ensures that the mixing time of a random walk on the graph is exponentially small in the number of vertices. This means that the random walk will converge to its stationary distribution very quickly. Exponential spectral gaps are important for a number of applications, including:\n",
        "  * A graph has an exponential spectral gap if the second smallest eigenvalue of its normalized Laplacian matrix is exponentially small in the number of vertices. This means that the graph is very well-connected, and that the random walk will quickly spread throughout the graph.\n",
        "  * Distributed algorithms: Exponential spectral gaps can be used to design distributed algorithms that are efficient and scalable.\n",
        "  * Communication networks: Exponential spectral gaps can be used to design communication networks that are reliable and efficient.\n",
        "  * Machine learning: Exponential spectral gaps can be used to design machine learning algorithms that are robust to noise and outliers.\n",
        "  * Here are some examples of graphs with exponential spectral gaps:\n",
        "    * Expander graphs: Expander graphs are graphs that have good connectivity properties. They are often used in the design of distributed algorithms and communication networks.\n",
        "    * Ramanujan graphs: Ramanujan graphs are a special type of expander graph that has an exponential spectral gap. They are often used in the design of cryptography protocols.\n",
        "    * Random graphs: Random graphs with a large number of vertices have an exponential spectral gap with high probability. This means that they can be used to design efficient and scalable algorithms for a variety of problems."
      ],
      "metadata": {
        "id": "sMd8jEF587Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel and Spectral Gap**\n",
        "\n",
        "* there is a connection between kernel and spectral gap. The spectral gap is a measure of how well-separated the eigenvalues of a matrix are. A large spectral gap means that the eigenvalues are well-separated, which means that the matrix is more stable and easier to invert.\n",
        "\n",
        "* The kernel of a matrix is the set of all vectors that are orthogonal to all the eigenvectors of the matrix with eigenvalue 0. In other words, it is the set of all vectors that do not change under the action of the matrix.\n",
        "\n",
        "* The spectral gap of a matrix is related to the kernel of the matrix in the following way: the larger the spectral gap, the smaller the dimension of the kernel. This is because if the spectral gap is large, then the eigenvalues of the matrix are well-separated, which means that there are fewer vectors that are orthogonal to all the eigenvectors with eigenvalue 0.\n",
        "\n",
        "* This connection between kernel and spectral gap has implications for machine learning. Kernel methods are a type of machine learning algorithm that use kernels to compute the similarity between two data points. The spectral gap of the kernel matrix is related to the generalization performance of kernel methods. In general, kernel methods with a larger spectral gap have better generalization performance.\n",
        "\n",
        "* In addition, the kernel of a matrix can be used to define a graph kernel. Graph kernels are a type of similarity measure between graphs. The graph kernel is based on the idea that two graphs are similar if their kernels are similar. The spectral gap of the kernel matrix is related to the quality of the graph kernel. In general, graph kernels with a larger spectral gap have better quality.\n",
        "\n",
        "* Overall, the kernel and spectral gap are two important concepts in machine learning. They are related to each other in a number of ways, and they both have implications for the performance of machine learning algorithms."
      ],
      "metadata": {
        "id": "toLEwiEj89GB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More on Spectral theory**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Laplacian_matrix\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Discrete_Laplace_operator\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_graph_theory\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_shape_analysis\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_clustering\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Laplace_operator"
      ],
      "metadata": {
        "id": "gUQX_Eay8_JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Neural Network*"
      ],
      "metadata": {
        "id": "nzm9UioSZQMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ddd"
      ],
      "metadata": {
        "id": "d5gl_2mFZOIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video [QAOA in 5 min](https://youtu.be/IFIoMs7wffY?si=l-0CdnSuhMHKr7oS)"
      ],
      "metadata": {
        "id": "T9g5PPUYVk1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/UP-Zuke7IUg?si=mdlJKh-eb6sq0OOx\n",
        "\n",
        "\n",
        "https://youtu.be/RqGpnRh7rCM?si=IA5CPfisTgsDUCNj\n",
        "\n",
        "Video: [QAOA explained](https://youtu.be/tuGcPup8DjY?si=tdOyGsgvqozGIARY)\n",
        "\n",
        "Video: [QAOA for combinatorial optimization](https://youtu.be/AOKM9BkweVU?si=PrLHcdZ1wDAjYTnY)\n",
        "\n",
        "Video: [QAOA Peter Shor 2018]()https://youtu.be/HHIWUi3GmdM?si=c1hLXgeLCEkme_6R)"
      ],
      "metadata": {
        "id": "zZHXaux0Vk1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/de/blogs/quantum-computing/citi-and-classiq-advance-quantum-solutions-for-portfolio-optimization/\n",
        "\n",
        "> From a computational point of view, the portfolio **optimization problem is NP-hard for minimization over binary or integer domains with additional constraints**. In simpler words, **the runtime to solve this problem grows exponentially as we increase the number of assets** (the size of ω). Therefore, finding ways of improving the runtime and the quality of the results is important when the number of assets is large.\n",
        "\n",
        "* QAOA is a hybrid algorithm, which means it combines both classical and quantum computation. The algorithm searches through all potential solutions using a mixer layer (which shuffles between alternative solutions) and a cost layer (which favors good ones). The circuit parameters are fine-tuned classically in an iterative manner, converging toward their optimal values with each run of QAOA.\n",
        "\n"
      ],
      "metadata": {
        "id": "ruDxKwP5Vk1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumai.google/cirq/experiments/qaoa/example_problems"
      ],
      "metadata": {
        "id": "t-HpPWOpVk1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Computers for Combinatorial Optimization**\n",
        "\n",
        "https://arxiv.org/abs/2210.14936: A super-polynomial quantum-classical separation for density modelling\n",
        "\n",
        "* An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory\n",
        "\n",
        "* It is unclear to what extent quantum algorithms can outperform classical algorithms for problems of combinatorial optimization. In this work, by resorting to computational learning theory and cryptographic notions, we give a fully constructive proof that hashtag#quantumcomputers feature a super-polynomial advantage over classical computers in approximating combinatorial hashtag#optimization problems. Specifically, by building on seminal work by Kearns and Valiant, we provide special instances that are hard for classical computers to approximate up to polynomial factors. Simultaneously, we give a quantum algorithm that can efficiently approximate the optimal solution within a polynomial factor. The quantum advantage in this work is ultimately borrowed from Shor’s quantum algorithm for factoring. We introduce an explicit and comprehensive end-to-end construction for the advantage bearing instances. For such instances, quantum computers have, in principle, the power to approximate combinatorial optimization solutions beyond the reach of classical efficient algorithms.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1702.jpg)"
      ],
      "metadata": {
        "id": "sagMtp2UVk1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorization of Optimization Problem with different orders of their objective function**\n",
        "* Einführung: [Optimization_problem](https://en.m.wikipedia.org/wiki/Optimization_problem)\n",
        "* **First order**: [Linear optimization](https://en.m.wikipedia.org/wiki/Linear_programming): Many practical problems in [operations research](https://en.m.wikipedia.org/wiki/Operations_research) can be expressed as linear programming problems. See [Simplex_algorithm](https://en.m.wikipedia.org/wiki/Simplex_algorithm)\n",
        "* **Second Order**: [Quadratic optimization (Quadratic programming)](https://en.m.wikipedia.org/wiki/Quadratic_programming): finance (for portfolio optimization), machine learning (for support vector machines), and operations research. Example: In the Markowitz model, the objective function to be minimized is the portfolio variance, which is a quadratic function of the decision variables (asset weights in the portfolio), given the covariance matrix of the returns of the assets.\n",
        "* **Higher Order**: Polynomial optimization problems or non-linear optimization problems (optimize design of a machine part, where the objective function includes terms related to the volume of material used which might be cubic if we're considering three-dimensional parts. Often require more advanced techniques, such as [interior-point methods](https://de.m.wikipedia.org/wiki/Innere-Punkte-Verfahren), branch-and-bound techniques, or heuristic methods."
      ],
      "metadata": {
        "id": "7l-BkCHtVk1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optimization Algorithms*"
      ],
      "metadata": {
        "id": "qaZS1UGcVk1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum Approximate Optimization Algorithm (QAOA)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Quantum_optimization_algorithms\n",
        "\n",
        "Farhi, Edward; Goldstone, Jeffrey; Gutmann, Sam (2014). \"A Quantum Approximate Optimization Algorithm\"\n",
        "\n",
        " arXiv:1411.4028: https://arxiv.org/abs/1411.4028"
      ],
      "metadata": {
        "id": "N1m2xcsVVk1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "IhDYEzzXVk10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation\n",
        "\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - clasislcayl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: glorothims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary, more: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but causes overhead increased asymptocatily like n to n^2 bits. Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practical engineer, our devices have this property. Required number of wires between qubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uKBKKElEVk10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imaginary time evolution**\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/557225/why-do-we-use-the-imaginary-time-evolution-in-simulations-of-some-quantum-system\n",
        "\n",
        "Imaginary time is a concept derived from quantum mechanics and statistical mechanics. It introduces the idea of replacing \"real time\" with \"imaginary time\" by a Wick rotation in the complex plane. That is, the time variable 't' is replaced with an imaginary number 'it', where 'i' is the imaginary unit.\n",
        "\n",
        "Imaginary time evolution plays a significant role in several areas of physics, including quantum field theory, statistical mechanics, and quantum computing.\n",
        "\n",
        "1. **Quantum Field Theory and Statistical Mechanics**: In these fields, the use of imaginary time is often a mathematical trick that simplifies calculations. By transforming to imaginary time, the calculations of quantum mechanics often become calculations in statistical mechanics. This is utilized in the technique called \"path integral formulation,\" where the evolution of a system in imaginary time makes the system go to its lowest energy state or the ground state.\n",
        "\n",
        "2. **Quantum Computing**: In quantum computing, the idea of imaginary time evolution can be used to design quantum algorithms for tasks such as finding the ground state of a system. This is used in the Quantum Approximate Optimization Algorithm (QAOA) and the Quantum Imaginary Time Evolution (QITE) algorithm.\n",
        "\n",
        "Remember that \"imaginary time\" is not about time in the sense that we experience it. Instead, it's a mathematical construct that physicists use to solve certain types of problems.\n",
        "\n",
        "Imaginary time is an unphysical, yet powerful, mathematical concept. It has been utilised in numerous physical domains, including quantum mechanics, statistical mechanics and cosmology. Often referred to as performing a ‘Wick rotation’\n",
        "\n",
        "https://www.nature.com/articles/s41534-019-0187-2"
      ],
      "metadata": {
        "id": "xdIv1xx6Vk11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *QAOA*"
      ],
      "metadata": {
        "id": "yIEGR7ZtZKfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/de/blogs/quantum-computing/citi-and-classiq-advance-quantum-solutions-for-portfolio-optimization/\n",
        "\n",
        "> From a computational point of view, the portfolio **optimization problem is NP-hard for minimization over binary or integer domains with additional constraints**. In simpler words, **the runtime to solve this problem grows exponentially as we increase the number of assets** (the size of ω). Therefore, finding ways of improving the runtime and the quality of the results is important when the number of assets is large.\n",
        "\n",
        "* QAOA is a hybrid algorithm, which means it combines both classical and quantum computation. The algorithm searches through all potential solutions using a mixer layer (which shuffles between alternative solutions) and a cost layer (which favors good ones). The circuit parameters are fine-tuned classically in an iterative manner, converging toward their optimal values with each run of QAOA.\n",
        "\n"
      ],
      "metadata": {
        "id": "iIVULjq3XGQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Computers for Combinatorial Optimization**\n",
        "\n",
        "https://arxiv.org/abs/2210.14936: A super-polynomial quantum-classical separation for density modelling\n",
        "\n",
        "* An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory\n",
        "\n",
        "* It is unclear to what extent quantum algorithms can outperform classical algorithms for problems of combinatorial optimization. In this work, by resorting to computational learning theory and cryptographic notions, we give a fully constructive proof that hashtag#quantumcomputers feature a super-polynomial advantage over classical computers in approximating combinatorial hashtag#optimization problems. Specifically, by building on seminal work by Kearns and Valiant, we provide special instances that are hard for classical computers to approximate up to polynomial factors. Simultaneously, we give a quantum algorithm that can efficiently approximate the optimal solution within a polynomial factor. The quantum advantage in this work is ultimately borrowed from Shor’s quantum algorithm for factoring. We introduce an explicit and comprehensive end-to-end construction for the advantage bearing instances. For such instances, quantum computers have, in principle, the power to approximate combinatorial optimization solutions beyond the reach of classical efficient algorithms.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1702.jpg)"
      ],
      "metadata": {
        "id": "txLRsGwpWWwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorization of Optimization Problem with different orders of their objective function**\n",
        "* Einführung: [Optimization_problem](https://en.m.wikipedia.org/wiki/Optimization_problem)\n",
        "* **First order**: [Linear optimization](https://en.m.wikipedia.org/wiki/Linear_programming): Many practical problems in [operations research](https://en.m.wikipedia.org/wiki/Operations_research) can be expressed as linear programming problems. See [Simplex_algorithm](https://en.m.wikipedia.org/wiki/Simplex_algorithm)\n",
        "* **Second Order**: [Quadratic optimization (Quadratic programming)](https://en.m.wikipedia.org/wiki/Quadratic_programming): finance (for portfolio optimization), machine learning (for support vector machines), and operations research. Example: In the Markowitz model, the objective function to be minimized is the portfolio variance, which is a quadratic function of the decision variables (asset weights in the portfolio), given the covariance matrix of the returns of the assets.\n",
        "* **Higher Order**: Polynomial optimization problems or non-linear optimization problems (optimize design of a machine part, where the objective function includes terms related to the volume of material used which might be cubic if we're considering three-dimensional parts. Often require more advanced techniques, such as [interior-point methods](https://de.m.wikipedia.org/wiki/Innere-Punkte-Verfahren), branch-and-bound techniques, or heuristic methods."
      ],
      "metadata": {
        "id": "zsVPCyezZKfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QAOA** (Ryan)\n",
        "\n",
        "https://arxiv.org/pdf/2011.04149.pdf\n",
        "\n",
        "https://arxiv.org/pdf/2004.04197.pdf\n",
        "\n",
        "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031015\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.1.020312\n",
        "\n",
        "10^53\n",
        "\n",
        "Research team has done a lot of optimisation\n",
        "\n",
        "Killer app - almost all business have logistics, scheduling, load balancing, portfolio optimisation - holy grail application, because it’s broad\n",
        "\n",
        "Team has some attention on this topic, 2013 had most focus, at that time annealing /adbatic QC at that time\n",
        "\n",
        "In real world: problem only on a couple of hundred bits, dicciutl to find good solution classically, formlery NP hard,determistic algorithm, formerly exponentially scaling, - classicl herutsics algorithm (simulated annealing) very good, have exponential runtime, contrast: algorithims that run exactly: but run in practice too long, in practice more heuristics,\n",
        "\n",
        "Just hundreds of bits: classical easy. Thousands bits to have truly challenging instances. Optimisation variable surely binary. But normally: real variable or multiple instances, need to map to binary qubits is difficult.\n",
        "\n",
        "The other things why so difficult: how dense , cost function being expressed as sum of products of bits, sum of pairwise products bits, like quadratic optimisation problem - quantum annealing: can just solve quadratic optimisation problem. There or four body (bits) problems, can be mapped to qudrtica function with 2 in qubits, but this causes overhead that increase asymptocatily like n to n^2 bits.\n",
        "\n",
        "Connectivity of graph - some optimisation have direst neuhbors on grid - turns out to be csolved easily classical. If local nature of connections. So you need problems, where the graph is not sparse and non local. If you use annealing: map problem hmailtoian to hardware graph, causes tons of overhead and qubits to get that plane. If you have digital approach like for qAOA or fault tolerant, increases number of swap gates (but not bits).\n",
        "\n",
        "contrast: quantum chemistry simulation: true quantum nature not easy to solve on 16 or 18 bits, solution is single bit string (expressing). Optimisation: finding the solution is easy, but you can’t express it easily n^80, you can’t write down the quantum wave function. You need 10^100 more bits than you would for other applications in simulation, like quantum chemist than on optimisation.\n",
        "\n",
        "Annealer some advantages for long range couples, sparse d of graph is increasing: but not practicalto engineer, our devices have this property. Requires number of wires  between wqubits increases with more qubits, which is impracticable.\n",
        "\n",
        "For error corrected otpimuzation : e don’t have the computers, but what we can prove? The problems are non oracular -\n",
        "\n",
        "You have to go to astronomical sizes to have advantages in error corrected with quadratic speedups\n",
        "\n",
        "Introduce energy landscape to cquamtum computer - diagonal hamitlionaina under the cost function, in all forms of quantum optimisation. Every cost function has to have order n term in it (edges), you need high connectivity\n",
        "\n",
        "Many hard problems are not even graphs, n^3 don’t need any edges, but n^1,5 you can’t have a sparse graphs. N needs to be order of thousands, number of bits, number of edges n^1,5 = 160.000, each of those edges needs at least x seizes. 300.000 edges (with rotations?), QC has 0.995 fidelity: 300.000 gates with that, raise that to power of 300.000 -> going against zero. 10^-220. 5 times n edges, we have 5000 edges, 2 seizes er edge=. 10..000 gates. I get 10^-22 fidelity. Number of circuit repetition required before seeing one error is over that, 5^25, repeated QC so many times, not going to happen.\n",
        "\n",
        "NISQ: is not scalable to 2000 qubits, not simulation, not optimisation, you have to have error correction. 50 -150 bits would be classically intractable, 0.995^300 = 0.2 fidelity, that sounds reasonable. Or 0.995^3000, we used a hundred bits, plus edges 2 seizes per edge, 2000 gates as result with 0.995 dfeiltiy, 4* 10^-5 = 22.000 samples in 2 seconds, something with hundred bit is ok, with thousands of bits is incredible.\n",
        "\n",
        "Runtime of exact classuical alrogrhtm with heuristics algorithms compare - wrong!!\n",
        "\n",
        "Maybe future for optimisation, but better approach apply QAOA to new problem is not going change speedup, you really need a close match between structure of algorithm and problem."
      ],
      "metadata": {
        "id": "5NxB9AolItWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *T-Gate Depth*"
      ],
      "metadata": {
        "id": "vysHiG6QtWbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [T1 and T2 Error](https://youtu.be/b7XSD2Eqb0s?si=CX3myisYAi0ReJQF)\n",
        "\n",
        "Video: [Quantum Algorithms for Hamiltonian Simulation | Quantum\n",
        "Colloquium ](https://youtu.be/X4gegxIuh1o?si=tMX01yfnfvv2-kLn)\n",
        "\n",
        "Video: [Classical shadows - Richard Küng | TQC 2023](https://youtu.be/1n9O0biAUWk?si=8o3HhLx75e89bZtL)"
      ],
      "metadata": {
        "id": "zAx2WQ8qKksn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is t-count in quantum computing and why is it relevant?**\n",
        "\n",
        "In quantum computing, T-count is a critical metric that influences the feasibility and efficiency of quantum algorithms. Here's a breakdown of what it means and its significance:\n",
        "\n",
        "**What is T-Count?**\n",
        "\n",
        "* **Definition:** The T-count refers to the number of T-gates present within a quantum circuit. T-gates are fundamental quantum gates, but they are among the most difficult to implement reliably in fault-tolerant quantum computers.\n",
        "* **Universal Gate Sets:** Quantum circuits are built from sets of basic gates. A common universal gate set includes the Clifford group gates (H, S, CNOT) and the T-gate. While Clifford gates are relatively easier to implement, any quantum algorithm, in theory, can be constructed using a combination of Clifford gates and T-gates.\n",
        "\n",
        "**Why is T-Count Relevant?**\n",
        "\n",
        "1. **Fault Tolerance:**  Current quantum computers are susceptible to noise and errors.  Fault-tolerant quantum computation techniques are needed for reliable, large-scale quantum algorithms.  T-gates are a major source of error in fault-tolerant systems due to their complexity.  Minimizing T-count reduces the places where errors can occur, improving the chances of successful computation.\n",
        "\n",
        "2. **Hardware Constraints:** Implementing T-gates in real quantum hardware is more resource-intensive than other gates. Excessive T-gates in a circuit design can place high demands on the underlying quantum hardware architecture.\n",
        "\n",
        "3. **Algorithm Efficiency:**  Often, the T-count of a quantum circuit is directly correlated to its execution time and the overall computational cost. Reducing the T-count generally leads to faster and more efficient quantum algorithms.\n",
        "\n",
        "4. **Quantum Supremacy Comparison:** T-count can be a useful metric for comparing the efficiency of different quantum algorithms and benchmarking the progress of quantum computers. A lower T-count might suggest one algorithm is more practical to implement than another.\n",
        "\n",
        "**Optimization Efforts**\n",
        "\n",
        "Due to the importance of T-count, a significant amount of research in quantum computing is dedicated to developing techniques to reduce the T-count of quantum circuits. This includes:\n",
        "\n",
        "* **Algorithm Design:** Designing new quantum algorithms that inherently require fewer T-gates.\n",
        "* **Circuit Optimization:** Developing tools and methods to automatically optimize existing quantum circuits to reduce their T-count while maintaining functional correctness.\n",
        "* **Hardware Improvements:**  Research into physical implementations of quantum computers that can implement the T-gate with higher fidelity and lower resource cost.\n",
        "\n",
        "**Let me know if you'd like a specific example of how T-count optimization is applied in a quantum algorithm!**\n"
      ],
      "metadata": {
        "id": "WbZI6nPYtaXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ok, and what means \"T-depth of 46 million\"?**\n",
        "\n",
        "The phrase \"T-depth of 46 million\" refers to a quantum circuit's complexity and its impact on hardware requirements. Here's what it means:\n",
        "\n",
        "* **T-depth:** T-depth, in quantum computing,  indicates the number of sequential layers of T-gates in a quantum circuit.  If a circuit needs multiple T-gates to be executed simultaneously, that only counts as a single layer in the T-depth.\n",
        "\n",
        "* **Significance of \"T-depth of 46 million\":**\n",
        "   * **Error Potential:** A circuit with a T-depth of 46 million implies that it requires 46 million execution cycles where T-gates must be applied faultlessly in succession. Considering the high error rates of current T-gate implementations, this highlights extreme challenges in running such a circuit reliably.\n",
        "   * **Resource Demands:**  T-depth directly influences the number of qubits and overall resources needed for execution. A  large T-depth may require holding quantum states (qubits) in a coherent and error-free state for an extended period while waiting for sequential T-gate applications, adding significantly to hardware complexity.   \n",
        "\n",
        "**Why it Matters:**\n",
        "\n",
        "A T-depth of 46 million reveals:\n",
        "\n",
        "1. **Impracticality with Present Hardware:**  The circuit is likely beyond the capabilities of currently available fault-tolerant quantum computers.\n",
        "2. **Need for Optimization:**  Significant algorithm and quantum circuit optimization would be necessary to reduce T-depth before the algorithm becomes feasible to run\n",
        "3. **Benchmarking:** This value could be a benchmark to track advancements in quantum hardware. As fidelity and the ability to perform sequential T-gates with fewer errors improve, circuits with large T-depths become more viable.\n",
        "\n",
        "**Let me know if you'd like to explore the connection between T-depth and specific quantum algorithms!**\n"
      ],
      "metadata": {
        "id": "wgmWgucftdGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "pT2hUkxuVAta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "3qQq_Q5zVAta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "yRJ7VzEiVAtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "RsobYwvCVAtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "jK22xuPIVAtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "VW4f6KEDVAtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "g30EunKfVAtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "fgI1B-5tVAtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *embedding*"
      ],
      "metadata": {
        "id": "7aVt8gJY03nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "heXFYWBUU7bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "5SP4IAxoU7bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "3Rqq0s2vU7bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "FZos4zOfU7bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "-aEXhlLdU7bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "HyYdKDVQU7bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "KgqMQmPgU7bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "o0P3t2_IU7bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Symmetries*"
      ],
      "metadata": {
        "id": "ZVxXcD-mYYhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hamiltonian Symmetries\n",
        "\n",
        "With this new knowledge we are now able to understand what is meant when some Hamiltonian models are said to be symmetric under some symmetry group.\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_liealgebra/"
      ],
      "metadata": {
        "id": "t4ojzBkXZRYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pennylane.ai/qml/demos/tutorial_geometric_qml/"
      ],
      "metadata": {
        "id": "_CgA0vF-Zu2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporating Symmetries:\n",
        "\n",
        "This is a nice article published by Smik Patel and Artur F. Izmaylov. They basically represent the Hamiltonian as a direct sum of Lie algebras that are solvable by Lie group rotations. Hence, by expressing a given Hamiltonian in this particular sum, one would obtain part of the solution for each term and then sum over all terms.\n",
        "\n",
        "In this work, we applied algebraic methods to develop new decompositions of the electronic Hamiltonian for VQE. The exact solvability of the fragments is based on combining commutativity and anti-commutativity conditions for Pauli products in a way consistent with Lie algebraic structures.\n",
        "These algebraic structures are re-\n",
        "vealed once the Pauli symmetries are measured.\n",
        "The re-\n",
        "sulting effective Hamiltonian is an element of a compact Lie algebra, and can therefore be measured by applying a unitary in the corresponding Lie group.\n",
        "\n",
        "https://arxiv.org/abs/2402.09376"
      ],
      "metadata": {
        "id": "NMOxh06aYWi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "by9rgZMBU4rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "O4j7EjXaU4rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "H9PV5VsVU4rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "rHXX4jImU4rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "WEe-6rw9U4rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "324O51glU4ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "bku_DNhJU4ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "FCqRCJaxU4ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Encoding*"
      ],
      "metadata": {
        "id": "91Gi3EMaYlBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fourier is all you need**: positional and number encoding using periodic signals\n",
        "\n",
        "I just realized during a conversation with Vanio Markov the similarity between the sinusoidal position encoding in the transformer architecture and what we’ve been calling “value encoding” in quantum computing. This concept was the main idea in encoding a (polynomial) function in a quantum state (inspired by phase estimation), used for optimization purposes (and contributed to Qiskit): https://lnkd.in/ea7QU77\n",
        "\n",
        "The “Attention is all you neeed” paper (https://lnkd.in/gP-y4xUE ) explains the positional encoding, but other references, like the one below, may be more accessible.\n",
        "\n",
        "https://sair.synerise.com/fourier-feature-encoding/\n",
        "\n",
        "This is a video showing frequency/value encoding for various frequencies: https://github.com/QuState/cqc_code/blob/master/videos/three_qubit_frequency_encoding.mp4 . The interactive tool also shows both the complex sinusoid (geometric sequence)  for a frequency and its  Fourier transform."
      ],
      "metadata": {
        "id": "PB2uSkU6JIZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding (Embedding)\n",
        "\n",
        "**Quantum computers are Kernel methods** of a very specific kind: https://www.youtube.com/watch?v=pe1d0RyCNxY&t=2655s\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1406.png)\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n",
        "\n",
        "**Data Encoding is the most important!**\n",
        "\n",
        "https://youtu.be/pe1d0RyCNxY?t=3008\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1407.png)\n",
        "\n",
        "The Helstrom measurement is the measurement that has the minimum error probability when trying to distinguish between two states.05.09.2018\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state_discrimination\n",
        "\n",
        "After we embedded / encoded our data, and if we encode it in a way that it separates one data type from another in the hilbert space, we already know which measurement is the best one to do. We know quite a bit about the measurements to distinguish data. **Maybe after encoding the data, we are already done**. We could even train the encoding!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1410.png)\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2001.03622 - Quantum embeddings for machine learning\n",
        "\n"
      ],
      "metadata": {
        "id": "reC2GT6fUkEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The easiest way for a parameter to enter a circuit is through a rotation of a single qubit, in proportion to the value of a single datapoint, so a single scalar value:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_190.png)\n",
        "\n",
        "> **You can also use a sequence of rotations to embedd data (reuploding).** And maybe there is free parameters in between as well. Can make a more complex function available than if you upload only once in a single rotation.\n",
        "\n",
        "> **Learnable embeddings**: The other idea is to actually have a trainable embedding layer. Not to worry about training the unitary of the circuit, but worry about training the embedding and then use standard quantum information metrics to classify the data.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_191.png)"
      ],
      "metadata": {
        "id": "xYRmG4v9vwCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis (State) Encoding**\n",
        "\n",
        "* We gave data points x12 and x2\n",
        "\n",
        "* We first need to represent data in binary form, like 00 and 10\n",
        "\n",
        "* Then we encode it into a QC in a way, such that we have basis states that represents them like |00> and |10>\n",
        "\n",
        "* with all other basis states having probability zero - represented in the amplitude vector\n",
        "\n",
        "* So we encode data in quantum state that is aligned with basis states\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_856.png)"
      ],
      "metadata": {
        "id": "Fxjw-h8ox4Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amplitude Encoding**\n",
        "\n",
        "* we want to encode our classical information into an amplitude vector\n",
        "\n",
        "* you have a classical data vector with 4 entries (features) x1\n",
        "\n",
        "* now construct a circuit, so that we have an amplitude vector that corresponds to the values in the classical data vector:\n",
        "\n",
        "\t* we have 2 qubits initialized in the ground state\n",
        "\n",
        "\t* then we apply some operations U (x1) on these qubits\n",
        "\n",
        "\t* and then we get a quantum state that corresponds to an amplitude vector that exactly represents our classical data points\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_857.png)"
      ],
      "metadata": {
        "id": "F2UqpWiN0Dv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Angle Encoding**\n",
        "\n",
        "* we have 2 dimension data that we can write in a two dimensional vector\n",
        "\n",
        "* then I take the number of qubits equal to the number of features (rows / entries in a classical vector)\n",
        "\n",
        "* then I apply rotations to each of these qubits that are equal to the value of the features\n",
        "\n",
        "\t* for example I rotate the first qubit about some axis Z. The rotation value / rotation angle is equal to the first classical feature value\n",
        "\n",
        "\t* the I take my second qubit and rotate it, for example again by the Z axis, and the angle of the rotation is equal to the second feature value of my data point\n",
        "\n",
        "* for higher dimensional data, for example a third dimension classical vector, then I simply add more qubits to my system to encode this information\n",
        "\n",
        "* For example:  [z feature map (Qiskit)](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZFeatureMap.html#qiskit.circuit.library.ZFeatureMap):\n",
        "\t* apply Hadamard operator first to each of the qubits, and then encode data values in rotations\n",
        "\t* and then repeat this as many times as you want (stacking operations sequentially like in the image) to encode data multpiple times in a row\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_858.png)"
      ],
      "metadata": {
        "id": "GbtZMigG2mij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher Order Encoding**\n",
        "\n",
        "* there is no theoretical reason why doing this or if it's better or not\n",
        "\n",
        "* the idea comes from the paper [Supervised learning with quantum enhanced feature spaces](https://arxiv.org/abs/1804.11326)\n",
        "\n",
        "* Basic idea: let's do an encoding that is hard to reproduce classically and simulate, and then maybe we get some quantum advantage in doing this\n",
        "\n",
        "* we have some two dimensional data with a vector with 2 entries\n",
        "\n",
        "\t* choose number of qubits = number of feature values\n",
        "\n",
        "\t* then apply an hadarmard on each qubit\n",
        "\n",
        "\t* and then do rotations about some axis Z, and the first angle is the first feature value, and the same with the second qubit\n",
        "\n",
        "\t* and then we apply some entanglement gates between these qubits\n",
        "\n",
        "\t* then we do another rotation (for example again Z axis), but this rotation angle depends on some function of the product of the feature values R(x^1 * x^2)\n",
        "\n",
        "\t* this is where the name comes from: we encode in a higher order product space\n",
        "\n",
        "\t* and this whole block of this encoding can be repeated, which is called the depth of the feature maps\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_860.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_861.png)"
      ],
      "metadata": {
        "id": "T8NOeUKI5dOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Encodings**\n",
        "\n",
        "* Hamiltonian evolution ansatz encoding\n",
        "\n",
        "* Displacement Encoding\n",
        "\n",
        "* IQP Encoding (Instantaneous quantum polynomial)\n",
        "\n",
        "* Squeezing Encoding\n",
        "\n",
        "* QAOA Encoding"
      ],
      "metadata": {
        "id": "BtmFtmBU6JFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"Blue\">**Computational Complexity**"
      ],
      "metadata": {
        "id": "vLLaN6mPXKth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Keep Stuff*"
      ],
      "metadata": {
        "id": "QlLUjXlCD2jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model vs Circuit Capacity (Complexity / Expressivity)* - Adding more layers = Increase the frequency of the cosine kernel?? - Min 27: [QML Meetup: Dr Maria Schuld, Taking stock of quantum machine learning - a critical perspective](https://youtu.be/8bfUMdj0-x4), then just repeating these layers of encoding would be better. In many cases making an embedding and then repeating it makes the model class richer.\n",
        "\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_magic_state_distillation\n",
        "\n",
        "https://pennylane.ai/qml/demos/tutorial_How_to_optimize_QML_model_using_JAX_and_JAXopt/"
      ],
      "metadata": {
        "id": "41pMV7jgDpU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1698.jpg)"
      ],
      "metadata": {
        "id": "WwHCBjDeD432"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hidden Subgroup Problem*"
      ],
      "metadata": {
        "id": "g9-cNnkyYxVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hidden subgroup problem**\n",
        "\n",
        "The dihedral hidden subgroup problem https://arxiv.org/abs/2106.09907\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem"
      ],
      "metadata": {
        "id": "K5mtRvKj1f07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Examples of HSP over non-Abelian groups](https://www.youtube.com/watch?v=D-pfWjGqENg)\n",
        "\n",
        "* We describe the HSP over symmetric and dihedral groups and their interesting applications.\n",
        "\n",
        "* example 2 graphs A and B: it's not sufficient if they have same number of edges and vertices\n",
        "\n",
        "* it's also important to know if they have the same connectivity (ismorphic): edge 5 and 10 in graph A are connected as well as edge 5 and 10 in graph B\n",
        "\n",
        "* compelixity of graph ismorphism is NP: if you have the solution, it's easy to check if it's true.\n",
        "\n",
        "* Graph automorpphism: you relabel the graph and it looks the same (non trivial besides identity map)\n"
      ],
      "metadata": {
        "id": "idQjSalFVKtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hidden Subgroup Problem](https://en.m.wikipedia.org/wiki/Hidden_subgroup_problem) (HSP) generalizes many problems on which quantum computers offer a potential exponential speed-up over classical computers and derives its significance from two important and related questions. First, the study of the problem sheds some light on what it is that quantum computers are really good at. Second, the ability to solve HSP for non-Abelian groups in polynomial time would enable us to extend the set of problems on which quantum computers offer substantial performance advantage. In this introductory talk, I will recap essential concepts from group theory before defining the Hidden Subgroup Problem and describing the quantum algorithm that solves it quickly. I will end by showing how to express a number of well-known problems, such as Simon's problem, discrete logarithm and graph isomorphism, as instances of HSP.\n"
      ],
      "metadata": {
        "id": "X1R28Zo3W95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hidden Subgroup Problem (HSP) is a computational problem in the field of quantum computing and cryptography. It is a generalization of the well-known problem called the Discrete Logarithm Problem (DLP) and the Integer Factorization Problem (IFP), which are both important in modern cryptography.\n",
        "\n",
        "In the Hidden Subgroup Problem, the input is a group G and a function f: G -> X, where G is a finite group and X is a finite set. The goal is to find a subgroup H of G such that f(x) is constant for all elements x in the same coset of H. In simpler terms, the problem involves finding a hidden structure within a group based on the information provided by the function.\n",
        "\n",
        "The Hidden Subgroup Problem has important implications in cryptography because certain mathematical problems, such as factoring large numbers, can be reduced to it. If an efficient quantum algorithm were developed to solve the Hidden Subgroup Problem, it would have significant consequences for public-key cryptosystems like RSA and elliptic curve cryptography, which rely on the difficulty of factoring large numbers or solving the DLP.\n",
        "\n",
        "Efforts have been made to find efficient quantum algorithms for solving the Hidden Subgroup Problem, particularly using quantum Fourier transform-based algorithms like Shor's algorithm. However, no general efficient quantum algorithm has been discovered yet for arbitrary groups, although there are some specific cases where efficient algorithms have been developed.\n",
        "\n",
        "The study of the Hidden Subgroup Problem and its related algorithms is an active area of research in quantum computing and cryptography, with the aim of better understanding the limits and possibilities of quantum computing's impact on cryptographic systems."
      ],
      "metadata": {
        "id": "Se1miG3Q2qH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the challenge in the hidden subgroup problem?\n",
        "\n",
        "The main challenge in the Hidden Subgroup Problem (HSP) is to efficiently find the hidden subgroup within a given group based on the information provided by a function. The difficulty lies in identifying the underlying structure or pattern that the function reveals about the subgroup.\n",
        "\n",
        "One of the key aspects that makes the HSP challenging is that the function only provides partial information about the subgroup. The function maps elements of the group to a set of values, but it does not directly reveal the subgroup itself. Instead, the function is designed in such a way that it distinguishes elements of different cosets of the hidden subgroup while keeping elements within the same coset indistinguishable. This limited information makes it difficult to directly extract the subgroup.\n",
        "\n",
        "Another challenge is that the HSP encompasses a wide range of group structures. While some specific cases of the problem have been efficiently solved using quantum algorithms, finding a general solution that works for arbitrary groups remains an open question. The difficulty varies depending on the properties and structure of the group involved.\n",
        "\n",
        "Additionally, the HSP is of particular interest in the field of quantum computing because it has the potential to break certain cryptographic protocols. Efficiently solving the HSP for specific groups could lead to the development of quantum algorithms that can break widely used encryption schemes based on problems like factoring large numbers or solving the Discrete Logarithm Problem.\n",
        "\n",
        "Despite efforts to develop quantum algorithms for the HSP, no general solution has been found yet. The challenge lies in understanding the underlying mathematical structures, devising efficient algorithms, and leveraging the power of quantum computation to solve the problem in a scalable and practical manner."
      ],
      "metadata": {
        "id": "r6qrUX10Wh-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you have a concrete example for a hidden subgroup?\n",
        "\n",
        "\n",
        "Certainly! One of the well-known examples of the Hidden Subgroup Problem involves the symmetric group, denoted as S<sub>n</sub>. The symmetric group consists of all possible permutations of n elements. In this case, the hidden subgroup problem involves finding the hidden subgroup based on the information provided by a function.\n",
        "\n",
        "Let's consider an example with n = 3, where the elements of the symmetric group S<sub>3</sub> are permutations of three objects, let's say {1, 2, 3}. The hidden subgroup problem would involve finding a hidden subgroup within S<sub>3</sub> based on the function f: S<sub>3</sub> -> X, where X is some set.\n",
        "\n",
        "In this scenario, the function f could be defined as follows:\n",
        "f(1, 2, 3) = 'A'\n",
        "f(1, 3, 2) = 'A'\n",
        "f(2, 1, 3) = 'B'\n",
        "f(2, 3, 1) = 'B'\n",
        "f(3, 1, 2) = 'C'\n",
        "f(3, 2, 1) = 'C'\n",
        "\n",
        "Here, the function assigns labels 'A', 'B', and 'C' to certain permutations in S<sub>3</sub>. The goal of the Hidden Subgroup Problem would be to determine the hidden subgroup, which, in this case, corresponds to the permutations that are labeled with the same letter.\n",
        "\n",
        "In this particular example, the hidden subgroup consists of the identity permutation (1, 2, 3), the permutation (2, 1, 3), and the permutation (3, 2, 1). The function f labels these permutations with the letter 'A'. So, the hidden subgroup in this case is { (1, 2, 3), (2, 1, 3), (3, 2, 1) }.\n",
        "\n",
        "The challenge is to find the hidden subgroup based on the given function, which may involve clever algorithms and techniques to deduce the structure of the subgroup from the function's output."
      ],
      "metadata": {
        "id": "8XCw9wUqWk3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would be a potential algorithm in this case?\n",
        "\n",
        "In the specific case of the Hidden Subgroup Problem (HSP) for the symmetric group S<sub>3</sub> with the given function f: S<sub>3</sub> -> X, there is a known efficient algorithm called the Coset Sampling Algorithm. This algorithm allows us to find the hidden subgroup by exploiting the structure of the symmetric group.\n",
        "\n",
        "The Coset Sampling Algorithm for S<sub>3</sub> proceeds as follows:\n",
        "\n",
        "1. Start with an initial candidate subgroup H = {identity permutation}.\n",
        "2. For each element g in S<sub>3</sub> - H:\n",
        "   a. Compute the coset gH.\n",
        "   b. Apply the function f to each element in the coset gH and record the results.\n",
        "3. Analyze the recorded results to determine if there is a consistent pattern.\n",
        "   a. If the results show that all elements in a coset map to the same value, update H to include the elements of that coset.\n",
        "   b. If the results do not exhibit a consistent pattern, return to step 2 with the next element in S<sub>3</sub> - H.\n",
        "4. Repeat steps 2 and 3 until no new cosets are added to H.\n",
        "5. The final H obtained is the hidden subgroup.\n",
        "\n",
        "In the case of S<sub>3</sub>, the Coset Sampling Algorithm will correctly identify the hidden subgroup { (1, 2, 3), (2, 1, 3), (3, 2, 1) } based on the given function f.\n",
        "\n",
        "It's important to note that the Coset Sampling Algorithm is specific to the symmetric group S<sub>3</sub> and may not generalize to other groups. The Hidden Subgroup Problem for other groups often requires different algorithms and techniques tailored to the specific group's structure."
      ],
      "metadata": {
        "id": "Y38g40y_WosB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you write this as a python function?\n",
        "\n",
        "Certainly! Here's an example of a Python function that implements the Coset Sampling Algorithm for the Hidden Subgroup Problem in the case of the symmetric group S<sub>3</sub>:\n",
        "\n",
        "In this example, the hidden_subgroup_algorithm_S3 function implements the Coset Sampling Algorithm for S<sub>3</sub>. The f parameter represents the given function that maps permutations to certain values. The algorithm iterates through the elements of S<sub>3</sub> and updates the candidate subgroup H based on the function's output until no new cosets are added to H. Finally, the function returns the discovered hidden subgroup.\n",
        "\n",
        "You can run this code to find the hidden subgroup in the provided example function example_function. Feel free to modify the function example_function or provide your own function f to experiment with different scenarios."
      ],
      "metadata": {
        "id": "nixfuFWvWrEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Random Circuit Sampling*"
      ],
      "metadata": {
        "id": "-91jSrDIX7jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantumcomputing.stackexchange.com/questions/4005/what-exactly-is-random-circuit-sampling"
      ],
      "metadata": {
        "id": "wN00xqCQFZdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[New Algorithm Closes Quantum Supremacy Window](https://www.quantamagazine.org/new-algorithm-closes-quantum-supremacy-window-20230109/)\n",
        "\n",
        "*Random circuit sampling, a popular technique for showing the power of quantum computers, doesn’t scale up if errors go unchecked.*\n",
        "\n",
        "If you imagine continually increasing the number of qubits as complexity theorists do, and you also want to account for errors, you need to decide whether you’re also going to keep adding more layers of gates — increasing the circuit depth, as researchers say. Suppose you keep the circuit depth constant at, say, a relatively shallow three layers, as you increase the number of qubits. You won’t get much entanglement, and the output will still be amenable to classical simulation. On the other hand, if you increase the circuit depth to keep up with the growing number of qubits, the cumulative effects of gate errors will wash out the entanglement, and the output will again become easy to simulate classically."
      ],
      "metadata": {
        "id": "N_8INTIQe7Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80 qubits: fidelyt decreases"
      ],
      "metadata": {
        "id": "JD-K1jPIy-6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nature.com/articles/s41586%20019%201666%205\n",
        "\n",
        "* Original experiment from Aaronson: boson sampling. Google: random circuit sampling (RCS). In both it is exponentially hard to compute the output distribution (also for quantum -> limits of quantum computing). But a quantum computer can sample efficiently from this distribution\n",
        "* Challenge: you can‘t go to 80 qubits or so, because then it is impossible for a classical computer to verify. Google used 53 qubits, because then you can at least verify (with 10,000 years on classical computer, doing extrapolation). But this opens the door to spoof the results classically (-> what did aaronson mean by that?)\n",
        "* Google did two things badly:\n",
        "    * They didn‘t properly estimate the 10.000 years on a supercomputer (breaking down and then extrapolate)\n",
        "    * google didn‘t use the best classical algorithm to compare with: later people used tensor networks to speed up the classical calculation and now it‘s down to 1 few seconds.\n",
        "* But this raises a question of what means supremacy: in terms of electrticity usage, quantum is still exponentially better than classical in this example: kilowatt vs megawatt or so\n",
        "* In terms of clockspeed though, classical is equally fast for this expoeriement, because classical computing is highly parallelizable (many operations in parallel)\n"
      ],
      "metadata": {
        "id": "Sie8LRdbgjj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spoofing: technical term, do something fraudently, how do i know that they ran the computation and got a result?\n",
        "easy to sample random bits from porter thomas distribution.\n",
        "\n",
        "page 3: https://www.cs.cmu.edu/~odonnell/quantum18/lecture25.pdf\n",
        "\n",
        "for RCS we dont have an euqivalent way of verfying this? KL divergence between what it should look like\n",
        "\n",
        "spoofing: clasical ciruti doesnt smiluate, but does anything up to simulaotyon. it can fool a verifyer."
      ],
      "metadata": {
        "id": "U_wyYl8ExRxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"Blue\">*Computational Complexity*"
      ],
      "metadata": {
        "id": "efzpvSJocU3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [**Computational complexity theory**](https://en.m.wikipedia.org/wiki/Computational_complexity_theory) = <u>**Is it possible to compute a function efficiently?**</u>\n",
        "\n",
        "* Read: [A Short History of Computational Complexity](https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.190/Mitarbeiter/toran/beatcs/column80.pdf)\n",
        "\n",
        "* **Definition**: Computational complexity theory asks \"How efficiently can a problem be solved\"? (NP-complete, NP-hard, P problems, etc. on Turing machine). It studies the resources (time, space, etc.) required to solve different computational problems. For **Quantum Computing: understand the ultimate physical limits to computation**\n",
        "\n",
        "* **Origin**: Hartmanis and Stearns showed that computational problems have an inherent complexity, which can be quantified in terms of the number of steps needed on a simple model of a computer, the multi-tape Turing machine. efficient reductions... collapse in equivalent classes, to natural complexity classes. motion of reducability. [Yablonsky](https://en.m.wikipedia.org/wiki/Sergey_Yablonsky) one of first to raise issues of potentially **inherent unavoidability of brute force search for some problems**, precursor of P = NP problem\n",
        "\n",
        "* **Special - Quantum**: [Quantum circuit complexity](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory): subfield of computational complexity theory that deals with complexity classes defined using quantum computers. Studies hardness of computational problems and relationship between complexity classes. Two important: BQP and QMA.\n",
        "\n",
        "* **Ausgangspunkt**: [Quantum many body problem](https://en.m.wikipedia.org/wiki/Many-body_problem): seems exponentially difficult for classical computers to simulate quantum systems ([Source](https://en.m.wikipedia.org/wiki/Quantum_threshold_theorem#Notes)). Quantum computers can simulate many Hamiltonians in [polynomial time with bounded errors](https://en.m.wikipedia.org/wiki/BQP) (BQP): chemical simulations, drug discovery, energy production, [climate modeling](https://en.m.wikipedia.org/wiki/Climate_model) and [FeMoco](https://en.m.wikipedia.org/wiki/FeMoco). [Finally, a Problem That Only Quantum Computers Will Ever Be Able to Solve](https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/). **Iterative improvements in classical algorithm are not enough**. Quantum computing: an “easy” problem can be solved on QC in polynomial time is class BQP (Bounded-error Quantum Polynomial time), and a hard problem which can only be verified in polynomial time is class QMA (the playfully named [Quantum Merlin Arthur](https://en.m.wikipedia.org/wiki/QMA)).  Hope in field is that there is overlap between NP and BQP: a hard problem can be transformed into an easy one.\n",
        "\n",
        "* **Limitations**: [\"The Limits of Quantum Computers\"](https://www.scientificamerican.com/article/the-limits-of-quantum-computers/): Quantum computers may provide a massive speedup for problems in the BQP complexity class. Nothing more. They are no more powerful than a Turing machine (in terms of the problems that can be solved). [The_Limits_of_Quantum_Computers](https://www.cs.virginia.edu/~robins/The_Limits_of_Quantum_Computers.pdf). [How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html).\n",
        "\n",
        "\n",
        "* **Computational Problems**: Decision, Sampling, Counting, Verifying, Optimization, Function etc. [Decision Problems](https://en.m.wikipedia.org/wiki/Decision_problem) see also [Search Problem](https://en.m.wikipedia.org/wiki/Search_problem) wie PH oder RP, Sampling Problem wie RP oder Boson Sampling, Proof Verifier Problems wie P, IP oder MIP* = RE, [Counting Problems](https://en.m.wikipedia.org/wiki/Counting_problem_(complexity)), wie Sharp-P oder P#P.\n",
        "\n",
        "* **Further reading:**\n",
        "  * Wiki: [Quantum complexity theory](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory). [Quantum_supremacy: Computational_complexity](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Computational_complexity). [Computational_problem](https://en.m.wikipedia.org/wiki/Computational_problem).\n",
        "  * Five worlds of Hardness: [Which Computational Universe Do We Live In?](https://www.quantamagazine.org/which-computational-universe-do-we-live-in-20220418/)\n",
        "  * [A Short Guide to Hard Problems](https://www.quantamagazine.org/a-short-guide-to-hard-problems-20180716/). [A New Map Traces the Limits of Computation](https://www.quantamagazine.org/edit-distance-reveals-hard-computational-problems-20150929/).\n",
        "  * Diagonalization remains one of key tools in complexity theorists’ arsenal [Alan Turing and the Power of Negative Thinking](https://www.quantamagazine.org/alan-turing-and-the-power-of-negative-thinking-20230905/).\n",
        "  * [Complexity Theory’s 50-Year Journey to the Limits of Knowledge](https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/).\n",
        "  * [Avi Wigderson, Complexity Theory Pioneer, Wins Turing Award](https://www.quantamagazine.org/avi-wigderson-complexity-theory-pioneer-wins-turing-award-20240410/)"
      ],
      "metadata": {
        "id": "E-jqmAnSXhmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Meta-complexity**](https://simons.berkeley.edu/programs/Meta-Complexity2023) (Example: Minimum Circuit Size Problem)\n",
        "\n",
        "* Video: [How Complex Is Complexity? Or What’s a ‘Meta’ for?](https://www.youtube.com/watch?v=7cgcPNUNgxQ) by Eric Allender at Somons Institute\n",
        "\n",
        "* **Meta-complexity**: [Complexity Theory’s 50-Year Journey to the Limits of Knowledge](https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/). It is concerned with difficulty of computational problems, both directly and indirectly. **How hard is it to prove that problems are hard to solve?**\n",
        "\n",
        "  * **Computational complexity theory**:\tDifficulty of computational problems\n",
        "  * **Meta-complexity theory**:\tDifficulty of determining the difficulty of computational problems\n",
        "\n",
        "* Example: [Minimum Circuit Size Problem](https://en.m.wikipedia.org/wiki/Circuit_complexity) (MCSP - Circuit Complexity), is concerned with complexity of Boolean functions. Bit string $2^n$ is a truth table of a function, and a number s. Does that function has a small circuit (of size at most s)?\n",
        "\n",
        "  * (f,s) : f has a circuit of size ≤ s, where f is represented by a bit string of length $2^n$ (seems intractable, is hard)\n",
        "\n",
        "  * **Complexity question**: Show f is hard\n",
        "\n",
        "  * **Meta-Complexity question**: show that it is hard to show that f is hard\n",
        "\n",
        "* Difference between computational complexity and algorithmic information theory on MCSP:\n",
        "\n",
        "  * In **computational complexity theory**, MCSP asks whether a given Boolean function can be computed by a circuit of a certain size (NP-hard, but not known whether NP-complete) - Is it possible to **compute efficiently** a given Boolean function?\n",
        "\n",
        "  * In **algorithmic information theory**, MCSP asks for Kolmogorov complexity of a given Boolean function (how difficult it is to describe object using a computer program?) - How difficult it is to **describe concisely** a given Boolean function?\n",
        "\n"
      ],
      "metadata": {
        "id": "_h99ix-0Tshq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit Complexity**\n",
        "* Quantum circuit complexity is a measure of the minimum number of quantum gates needed to implement a given unitary transformation\n",
        "* circuit complexity is a lower bound on computational capacity.\n",
        "* (run)time complexity of a quantum algorithm depends on a number of factors, **including the circuit complexity of the algorithm**, the architecture of the quantum computer, and the error rate of the quantum computer.\n",
        "* For [quantum circuit complexity](https://en.m.wikipedia.org/wiki/Circuit_complexity) see also:\n",
        "  * [Linear growth of quantum circuit complexity](https://arxiv.org/abs/2106.05305): Consider constructing deeper and deeper circuits for an n-qubit system, by applying random two-qubit gates. At what rate does the circuit complexity increase?\n",
        "  * [Quantum Computation as Geometry](https://arxiv.org/abs/quant-ph/0603161): Determining the quantum circuit complexity of a unitary operation is closely related to the problem of finding minimal length paths in a particular curved geometry.\n",
        "  * [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004)\n",
        "  * [Quantum Computation as Geometry](https://arxiv.org/abs/quant-ph/0603161) (Nielsen geometry)\n",
        "* [Quantum circuit complexity](https://en.m.wikipedia.org/wiki/Circuit_complexity) is a fundamental concept in quantum computation: widespread applications ranging from determining the running time of quantum algorithms to understanding the physics of black holes. By understanding the complexity of quantum circuits, we can develop more efficient quantum algorithms and better understand the capabilities of quantum computers.\n",
        "  * **Circuit complexity is a measure of the difficulty of computing a function using a Boolean circuit.** A Boolean circuit is a network of logic gates that computes a Boolean function, which is a function that takes a Boolean input vector and produces a Boolean output. The size of a circuit is the number of gates it contains, and its depth is the longest path from an input gate to an output gate.\n",
        "  * Quantum circuit complexity is a **measure of the minimum number of quantum gates needed to implement a given unitary transformation**. However, it is important to note that not all gates are created equal. Some gates are more complex than others, and it may take multiple gates to implement a single unitary transformation.\n",
        "  * Measure the complexity of a quantum gate:\n",
        "    * by its depth: The depth of a gate is the longest path from the input to the output of the gate.\n",
        "    * by its size: The size of a gate is the number of qubits it acts on.\n",
        "  * Quantum circuits are a graphical representation of quantum algorithms. They consist of a sequence of quantum gates, which are unitary operations that act on quantum qubits. The output of a quantum circuit is a unitary transformation on the input state. Here are some examples of quantum circuit complexity:\n",
        "  * The quantum Fourier transform can be implemented with a circuit of depth O(log n), where n is the number of qubits. The quantum Fourier transform can be implemented with a circuit of depth O(log n), where n is the number of qubits. However, the time complexity of the quantum Fourier transform depends on the architecture of the quantum computer. For example, if the quantum computer can implement the quantum Fourier transform using a parallel circuit, then the time complexity will be O(log n). However, if the quantum computer can only implement the quantum Fourier transform using a sequential circuit, then the time complexity will be O(n log n).\n",
        "  * Shor's algorithm for factoring integers can be implemented with a circuit of depth O(log n). Shor's algorithm for factoring integers can be implemented with a circuit of depth O(log n). However, the time complexity of Shor's algorithm depends on the error rate of the quantum computer. If the error rate is low enough, then the time complexity of Shor's algorithm will be polynomial in the size of the integer being factored.\n",
        "  * Grover's algorithm for searching unsorted databases can be implemented with a circuit of depth O(sqrt(N)), where N is the size of the database. Grover's algorithm for searching unsorted databases can be implemented with a circuit of depth O(sqrt(N)), where N is the size of the database. However, the time complexity of Grover's algorithm depends on the number of queries that can be made to the database. If the number of queries is limited, then the time complexity of Grover's algorithm will be higher than O(sqrt(N))\n",
        "* Research papers:\n",
        "  * [Complexity and order in approximate quantum error-correcting codes](https://arxiv.org/abs/2310.04710): We establish rigorous connections between quantum circuit complexity and approximate quantum error correction (AQEC) properties, covering both all-to-all and geometric scenarios including lattice systems. Approximate quantum error correction is a mostly unexplored land, of which we know only a few landmarks. Here they give us a map, and show interesting connections to other areas of physics.\n",
        "  * [Linear growth of quantum circuit complexity](https://arxiv.org/abs/2106.05305): Consider constructing deeper and deeper circuits for an n-qubit system, by applying random two-qubit gates. At what rate does the circuit complexity increase?\n",
        "  * [Quantum Computation as Geometry](https://arxiv.org/abs/quant-ph/0603161): Determining the quantum circuit complexity of a unitary operation is closely related to the problem of finding minimal length paths in a particular curved geometry.\n",
        "  * [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004)\n",
        "  * [Quantum Computation as Geometry](https://arxiv.org/abs/quant-ph/0603161) (Nielsen geometry)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKpQ47BNMI_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Complexity Classes**](https://en.m.wikipedia.org/wiki/Complexity_class)\n",
        "\n",
        "The relationship of BQP to essential classical complexity classes [(Source)](https://en.m.wikipedia.org/wiki/Quantum_complexity_theory):\n",
        "\n",
        "> ${\\mathsf {P\\subseteq BPP\\subseteq BQP\\subseteq PP\\subseteq PSPACE}}$\n",
        "\n",
        "Relationships between fundamental time and space complexity classes [(S1)](https://en.m.wikipedia.org/wiki/PSPACE) and [(S2)](https://en.m.wikipedia.org/wiki/Complexity_class):\n",
        "\n",
        "> ${\\mathsf {NL\\subseteq P\\subseteq NP\\subseteq PH\\subseteq PSPACE\\subseteq EXPTIME\\subseteq EXPSPACE}}$\n",
        "\n",
        "\n",
        "* [P](https://en.m.wikipedia.org/wiki/P_(complexity)): Can be solved by a deterministic classical computer in polynomial time. P $\\subseteq$ BQP (i.e. anything you can do with a classic computer you can do with a quantum computer). We don't know if that is a strict inequality! [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "\n",
        "* [RP](https://en.m.wikipedia.org/wiki/RP_(complexity)) und [ZPP](https://en.m.wikipedia.org/wiki/ZPP_(complexity)). Unsolved problem in computer science: $\\displaystyle {\\mathsf {P}}{\\overset {?}{=}}{\\mathsf {RP}}$. [RP](https://en.m.wikipedia.org/wiki/RP_(complexity)): Sampling Problem. Randomized polynomial time (RP) is the complexity class of problems for which a [probabilistic Turing machine](https://en.m.wikipedia.org/wiki/Probabilistic_Turing_machine) exists with these properties. Class of problems for which a randomized algorithm can give the correct answer in polynomial time, with a probability of at least 1/2 for \"yes\" instances. Ask for samples from probability distributions. [Source](https://en.m.wikipedia.org/wiki/Quantum_supremacy).\n",
        "  * [Boson sampling](https://en.m.wikipedia.org/wiki/Boson_sampling) Boson sampling is believed to be an RP complexity problem (randomized polynomial time - not proven): solve it in polynomial time with a high probability of success. Randomized algorithm is quantum computer that sample from Boson sampling distribution, which exponentially hard to sample from classically, but maybe polynomial for QC. Showing that Boson sampling is RP -> demonstrate quantum supremacy.\n",
        "  * [Sampling the output distribution of random quantum circuits\n",
        "  ](https://en.m.wikipedia.org/wiki/Quantum_supremacy#Sampling_the_output_distribution_of_random_quantum_circuits) (Google experiment)\n",
        "  * [Solving the sampling problem of the Sycamore quantum circuits](https://arxiv.org/abs/2111.03011)\n",
        "  * [Quantum Sampling Problems, Boson Sampling and Quantum Supremacy](https://arxiv.org/abs/1702.03061)\n",
        "  * [Computational advantage of quantum random sampling](https://arxiv.org/abs/2206.04079)\n",
        "  \n",
        "* [BPP](https://de.m.wikipedia.org/wiki/BPP_(Komplexitätsklasse)): Decision problems, that can be solved by a probabilistic classical computer in polynomial time\n",
        "\n",
        "* [BQP](https://en.m.wikipedia.org/wiki/BQP): Decision problems, that can be solved by a quantum computer in polynomial time (with quantum probability).\n",
        "  * BQP are not in [BPP](https://de.m.wikipedia.org/wiki/BPP_(Komplexitätsklasse)): [Factorization](https://de.m.wikipedia.org/wiki/Faktorisierung) with [Shor's algorithm](https://de.m.wikipedia.org/wiki/Shor-Algorithmus). BQP = P? - Open Question. Dequantized algorithms for such problems – high-rank matrix inversion, for example – would imply that classical computers can efficiently simulate quantum computers, i.e., BQP = P, which is **not** currently considered to be likely. [Source](https://arxiv.org/abs/1905.10415)\n",
        "\n",
        "* [PP](https://de.m.wikipedia.org/wiki/Probabilistische_Polynomialzeit) - Decision problems, die in von einer probabilistischen Turingmaschine in Polynomialzeit lösbar ist und die Antwort in mindestens der Hälfte der Fälle richtig ist. [PostBQP](https://en.m.wikipedia.org/wiki/PostBQP) = [PP](https://de.m.wikipedia.org/wiki/Probabilistische_Polynomialzeit).\n",
        "\n",
        "* [PSPACE](https://de.m.wikipedia.org/wiki/PSPACE) - Problems that can be solved using a polynomial amount of memory, and possibly exponential time.\n",
        "\n",
        "* [IP](https://en.m.wikipedia.org/wiki/IP_(complexity)) interactive proof) is the class of problems solvable by an interactive proof system. It is equal to the class PSPACE.\n",
        "\n",
        "* [NP](https://en.m.wikipedia.org/wiki/NP_(complexity)): Solution can be checked by a deterministic classical computer in polynomial time. [List of NP problems](https://en.m.wikipedia.org/wiki/List_of_NP-complete_problems).\n",
        "  * [NP-Hard](https://en.m.wikipedia.org/wiki/NP-hardness): [travelling salesman](https://en.m.wikipedia.org/wiki/Travelling_salesman_problem). [Sign Problem](https://en.m.wikipedia.org/wiki/Numerical_sign_problem), zB [Sign-Problem-Free Fermionic Quantum Monte Carlo](https://arxiv.org/pdf/1805.08219.pdf)\n",
        "    * [PCP theorem](https://en.m.wikipedia.org/wiki/PCP_theorem): states that every decision problem in the NP complexity class has probabilistically checkable proofs (proofs that can be checked by a randomized algorithm) of constant query complexity and logarithmic randomness complexity (uses a logarithmic number of random bits). The PCP theorem is the cornerstone of the theory of computational [hardness of approximation](https://en.m.wikipedia.org/wiki/Hardness_of_approximation), which investigates the inherent difficulty in designing efficient [approximation algorithms](https://en.m.wikipedia.org/wiki/Approximation_algorithm) for various [optimization problems](https://en.m.wikipedia.org/wiki/Computational_problem).\n",
        "  * [NP-Complete](https://en.m.wikipedia.org/wiki/NP-completeness): hardest of problems to which solutions can be verified quickly, like [Halting problem](https://en.m.wikipedia.org/wiki/Halting_problem) or [3SAT](https://en.m.wikipedia.org/wiki/Boolean_satisfiability_problem) (except, one manages to create a reduction of Grovers algorithm on this NP-Complete algorithm). Can quantum computer solve NP-complete problems? - if you solve any NP-complete problem, all other NP problems come as a 'freebie' (not just the NP-complete ones). In that sense, it would be a huge milestone. It is widely believed that quantum computers cannot solve NP-complete problems, but it has never been proven [Source](https://quantumcomputing.stackexchange.com/questions/16506/can-quantum-computer-solve-np-complete-problems)\n",
        "  * [Co-NP](https://de.m.wikipedia.org/wiki/Co-NP): Complement of NP. Problems for which a \"no\" answer can be verified in polynomial time\n",
        "  * [P versus NP](https://en.m.wikipedia.org/wiki/P_versus_NP_problem): Open question, [video1](https://youtu.be/EHp4FPyajKQ), [video2](https://youtu.be/YX40hbAHx3s)\n",
        "  * [BQNP: The quantum analogue of NP](https://medium.com/mit-6-s089-intro-to-quantum-computing/bqnp-the-quantum-analogue-of-np-486ed2469c1d) and [What is the relationship between BQP and NP?](https://www.quora.com/What-is-the-relationship-between-BQP-and-NP-1)\n",
        "  * Kolmogorov suggested, even before the notions of P, NP, and NP-completeness existed, that lower bound efforts might best be focused on sets that are relatively devoid of simple structure. That is, the NP-complete problems are probably too structured to be good candidates for separating P from NP. One should rather focus on the intermediate less-structured sets that somehow are complex enough to prove separations. As a candidate of such a set he proposed to look at the set of what we call nowadays the **resource-bounded Kolmogorov random strings.** His student Levin looked at \"Time-bounded Kolmogorov Complexity\"\n",
        "\n",
        "* [PH](https://en.m.wikipedia.org/wiki/PH_(complexity)) union of all complexity classes in polynomial hierarchy. Generalizations of NP.\n",
        "\n",
        "\n",
        "* [QMA](https://en.m.wikipedia.org/wiki/QMA): Solution can be checked by a quantum computer in polynomial time.\n",
        "  * QMA is the quantum analog of the NP complexity class.\n",
        "  * HeurBQP/qpoly ⊆ HeurQMA/poly (The Learnability of Quantum States, 2004)\n",
        "  * Many interesting classes are contained in QMA, such as P, BQP and NP, all problems in those classes are also in QMA. However, there are problems that are in QMA but not known to be in NP or BQP. A [list of known QMA-complete problems](https://arxiv.org/abs/1212.6312)\n",
        "    * Quantum circuit/channel property verification (V)\n",
        "    * Hamiltonian ground state estimation (H), icl. Quantum k-SAT (S)\n",
        "    * Density matrix consistency (C)\n",
        "  * The **Local Hamiltonian problem** is a complexity class in quantum computing. It is the problem of determining the ground state energy of a local Hamiltonian. It is QMA-complete, which means that there exists a quantum algorithm that can solve the problem with a polynomial number of queries to a quantum oracle, and no classical algorithm can solve the problem with a polynomial number of queries to a classical oracle, unless P=BQP. The Local Hamiltonian problem is a fundamental problem in quantum computing. It is a key problem in the study of quantum algorithms for solving optimization problems.\n",
        "* [RE](https://en.m.wikipedia.org/wiki/RE_(complexity)) recursively enumerable, is the class of decision problems for which a 'yes' answer can be verified by a Turing machine in a finite amount of time. RE-complete problem: Halting problem.\n",
        "* [MIP* = RE ?](https://medium.com/mit-6-s089-intro-to-quantum-computing/mip-re-6e903720c82f), bzw [MIP* = RE (arXiv)](https://arxiv.org/abs/2001.04383) (aus Spektrum der Wissenschaft 7/20): enthält MIP* sämtliche berechenbaren Probleme der Informatik! Dem Beweis zufolge ist MIP identisch mit der riesigen Komplexitätsklasse RE. Sie umfasst alle Entscheidungsprobleme (solche, deren Antwort Ja oder Nein lautet), die ein Computer in endlicher Zeit bejahen kann. Darunter fällt unter anderem die hartnäckigste aller Aufgaben, das berühmte Halteproblem. Dabei geht es darum, zu bestimmen, ob ein Computer bei einer Berechnung jemals anhalten kann – oder für immer weiterrechnet.\n",
        "\n",
        "*Further important complexity classes*\n",
        "\n",
        "* [DQC1](https://en.m.wikipedia.org/wiki/One_Clean_Qubit): Deterministic quantum computation with one clean qubit is the class of decision problems solvable by a one clean qubit machine in polynomial time, upon measuring the first qubit, with an error probability of at most 1/poly(n) for all instances\n",
        "* [Optimization problems](https://en.m.wikipedia.org/wiki/Optimization_problem), see [Complexity Classes for Optimization Problems](https://home.in.tum.de/~kugele/files/JoBSIS.pdf): [PLS](https://en.m.wikipedia.org/wiki/PLS_(complexity)): a local optimal solution can be found in polynomial time, but it might not be the global optimal solution. **NPO** is the optimization equivalent to NP (candidate solution can be checked in polynomial time). [APX](https://en.m.wikipedia.org/wiki/APX) for problems that can be approximated within a constant factor in polynomial time.\n",
        "* [Function problems](https://en.m.wikipedia.org/wiki/Function_problem): [FP](https://en.m.wikipedia.org/wiki/FP_(complexity)): The function problem equivalent of P, where the task is to compute a specific output rather than just deciding yes/no.\n",
        "* [Sharp-P](https://de.m.wikipedia.org/wiki/Sharp-P) How many solutions are there? Sharp P complete problems: #SAT oder Anzahl der perfekten Matchings eines bipartiten Graphen\n",
        "* More: https://complexityzoo.net/Complexity_Zoo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B8Sn02WrzXmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1684.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1655.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1646.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1274.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1646.jpg)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1647.png)\n",
        "\n",
        "*Source [here](https://www.researchgate.net/figure/Computability-hierarchy-and-computational-complexity-classes_fig5_341817215)*"
      ],
      "metadata": {
        "id": "vQZjqUGyNu2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computer Efficiency & Spacetime Complexity**\n",
        "\n",
        "*How to calculate Computer Performance?*: [Frontier supercomputer](https://en.m.wikipedia.org/wiki/Frontier_(supercomputer)) is capable of making 1,102,000 [TFLOPs](https://en.m.wikipedia.org/wiki/FLOPS) (1.1 quintillion calculations per second).  [Exascale_computing](https://en.m.wikipedia.org/wiki/Exascale_computing). [Floating-point_arithmetic](https://en.m.wikipedia.org/wiki/Floating-point_arithmetic). [Instructions_per_second](https://en.m.wikipedia.org/wiki/Instructions_per_second). [Gleitkommazahl](https://de.m.wikipedia.org/wiki/Gleitkommazahl) (floating point). [Floating_point_numbers](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers). [Double-precision_floating-point_format](https://en.m.wikipedia.org/wiki/Double-precision_floating-point_format)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1273.png)\n",
        "\n",
        "*Time Complexity of 10 Most Popular ML Algorithms:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1739.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1683.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1740.png)\n",
        "\n",
        "New Breakthrough Brings Matrix Multiplication Closer to Ideal: https://www.quantamagazine.org/new-breakthrough-brings-matrix-multiplication-closer-to-ideal-20240307/\n",
        "\n",
        "\n",
        "[Researchers Approach New Speed Limit for Seminal Problem](https://www.quantamagazine.org/researchers-approach-new-speed-limit-for-seminal-problem-20240129/): Integer linear programming can help find the answer to a variety of real-world problems. Now researchers have found a much faster way to do it.\n",
        "\n",
        "Source: [Youtube](https://youtu.be/A5QTukT1VS8?si=tpAqnNWsgS9aWAEm), https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/wcms.1481\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1630.png)\n",
        "\n",
        "* Examples: [Computational complexity of mathematical operations](https://en.m.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra) and [Computational complexity of matrix multiplication](https://en.m.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication)\n",
        "\n",
        "* **Time complexity vs Space complexity**: [Time Complexity](https://en.m.wikipedia.org/wiki/Time_complexity): Big O notation.\n",
        "  * The time complexity is the [computational complexity](https://en.m.wikipedia.org/wiki/Computational_complexity) that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm. See also [Time hierarchy theorem](https://en.m.wikipedia.org/wiki/Time_hierarchy_theorem). [Space complexity](https://en.m.wikipedia.org/wiki/Space_complexity) is generally expressed as the amount of memory required by an algorithm on an input of size n. See also [Space hierarchy theorem](https://en.m.wikipedia.org/wiki/Space_hierarchy_theorem).\n",
        "  * [Time complexity](https://en.m.wikipedia.org/wiki/Time_complexity), Space Complexity = Auxiliary Space + Space used for input values, [Space time tradeoff](https://en.m.wikipedia.org/wiki/Space–time_tradeoff), [Garbage collection](https://de.m.wikipedia.org/wiki/Garbage_Collection), [Time and Space Complexity Analysis of Algorithm](https://afteracademy.com/blog/time-and-space-complexity-analysis-of-algorithm), [How to compute Time Complexity or Order of Growth of any program](https://www.rookieslab.com/posts/how-to-compute-time-complexity-order-of-growth-of-any-program)\n",
        "\n",
        "* Time Complexity: The amount of time it takes a learning algorithm to learn a concept. **\"quantum time complexity, defined as the total number of gates used by the algorithm**\". (Source: Survey on the complexity of learning quantum states)\n",
        "  * low sample complexity is a necessary condition for efficient learning, but information-theoretic sufficiency of a small sample is not much help in practice if finding a good hypothesis still takes much time (time complexity of best quantum learner vs best known classical learner)\n",
        "  * \"despite several distribution-specific speedups, quantum examples do not signifi- cantly reduce sample complexity if we require our learner to work for all distributions D. This should be contrasted with the situation when considering the time complexity of learning\"\n",
        "  * \"Exponential sample complexity when data is from quantum sensors. Time complexity is more subtle.\"\n",
        "\n",
        "* **Worst-case vs Average-case complexity**: [Worst-case_complexity](https://en.m.wikipedia.org/wiki/Worst-case_complexity) and [Average-case_complexity](https://en.m.wikipedia.org/wiki/Average-case_complexity)\n",
        "\n",
        "* [Big O notation (Landau)](https://en.m.wikipedia.org/wiki/Big_O_notation): Big (O) worste case / Big Ω (Omega) best case / Big θ (Theta), [Examples of runtime with big O](https://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly), [Brilliant: Complexity Theory](https://brilliant.org/wiki/complexity-theory/), [Theory of computation](https://en.m.wikipedia.org/wiki/Theory_of_computation)"
      ],
      "metadata": {
        "id": "m13Q5iLjReWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beispiele:**\n",
        "\n",
        "Video: [The Complexity of Dynamic Least-Squares Regression](https://www.youtube.com/watch?v=GLE3hjDRbQw) by Shunhua Jiang (Columbia University)\n",
        "\n",
        "Video: [Fast Algorithms for Regression Problems](https://www.youtube.com/watch?v=FqLCImQNpeg)\n",
        "\n",
        "\n",
        "* Does the complexity increase with larger p-norms, or not?*\n",
        "\n",
        "The complexity of linear regression with regularization using different p-norms (such as in Ridge regression for \\( p = 2 \\) or Lasso for \\( p = 1 \\)) does indeed vary, but not necessarily in a direct relationship with the value of \\( p \\).\n",
        "\n",
        "1. **L2 Norm (Ridge Regression - \\( p = 2 \\)):**\n",
        "   - Ridge regression adds a regularization term equal to the square of the magnitude of the coefficients (L2 norm) to the loss function. The computational complexity of ridge regression is similar to ordinary linear regression. The solving process involves matrix operations that are computationally intensive (like inversion), but these operations don't significantly increase in complexity with the addition of the L2 term.\n",
        "\n",
        "2. **L1 Norm (Lasso Regression - \\( p = 1 \\)):**\n",
        "   - Lasso regression uses the absolute value of the coefficients (L1 norm) as the regularization term. The main computational challenge with Lasso is that the absolute value function is not differentiable at zero, which complicates the optimization process. Specialized algorithms like coordinate descent are used, and the complexity can be higher compared to ridge regression, especially for high-dimensional data. However, this increased complexity is more due to the nature of the L1 norm rather than its size.\n",
        "\n",
        "3. **Higher p-norms:**\n",
        "   - In practice, norms higher than L2 are rarely used in linear regression. Theoretically, as \\( p \\) increases beyond 2, the optimization problem can become more challenging due to the increasing non-linearity and non-convexity of the norm. This could potentially lead to higher computational complexity, but such cases are not common in standard linear regression problems.\n",
        "\n",
        "In summary, the increase in computational complexity with different p-norms in linear regression is less about the size of \\( p \\) and more about the mathematical and computational characteristics of the norms themselves. L1 and L2 norms are the most commonly used, with L1 (Lasso) generally posing more computational challenges due to its non-differentiability at zero. Higher p-norms are not standard in linear regression and could potentially introduce more complex optimization challenges.\n",
        "\n",
        "*Why do higher p-norms show increasing non-linearity and non-convexity of the norm?*\n",
        "\n",
        "Higher p-norms exhibit increasing non-linearity and non-convexity due to the mathematical properties of the norms themselves. To understand this, let's first define the p-norm for a vector \\( \\mathbf{x} \\) in \\( \\mathbb{R}^n \\):\n",
        "\n",
        "\\[ \\|\\mathbf{x}\\|_p = \\left( \\sum_{i=1}^{n} |x_i|^p \\right)^{\\frac{1}{p}} \\]\n",
        "\n",
        "As \\( p \\) increases, the behavior of this norm changes:\n",
        "\n",
        "1. **Increasing Non-Linearity:**\n",
        "   - For \\( p < 1 \\), the p-norm is non-convex. As \\( p \\) approaches 1, the function becomes linear in nature (the L1 norm or Manhattan norm). Beyond \\( p = 1 \\), including the commonly used \\( p = 2 \\) (Euclidean norm), the function is convex but exhibits increasing non-linearity as \\( p \\) increases.\n",
        "   - The non-linearity arises because higher powers increasingly magnify larger components of the vector more than smaller ones. For instance, in the case of \\( p = 2 \\), squaring each component treats all components equally in terms of their sign but not in terms of their magnitude. As \\( p \\) increases further, this effect becomes more pronounced, leading to a function that responds very non-linearly to changes in the input vector.\n",
        "\n",
        "2. **Non-Convexity for \\( p > 1 \\):**\n",
        "   - For \\( p > 1 \\), the p-norms are always convex. This convexity is a crucial property for many optimization problems, including those in machine learning, as it ensures that any local minimum is also a global minimum.\n",
        "   - However, as \\( p \\) increases, the shape of the function becomes more \"peaky\" or \"sharp\" around the axes. This can be intuitively understood: for very large \\( p \\), the p-norm is heavily influenced by the largest component of the vector, almost ignoring the smaller components. This behavior can lead to a function that, while still convex, has very steep gradients near the axes, making optimization more challenging.\n",
        "\n",
        "3. **Practical Implications:**\n",
        "   - In optimization problems, such as those in machine learning, these properties have significant implications. Non-linear, sharp functions can lead to optimization challenges, such as difficulty in finding the global minimum due to steep gradients or flat regions.\n",
        "   - Moreover, the computational cost of evaluating higher p-norms can be significant, as it involves raising each component of the vector to a high power and then taking a root, operations that are more computationally intensive than those required for lower p-norms.\n",
        "\n",
        "In summary, the increase in non-linearity and the specific convexity shape of higher p-norms are inherent to their mathematical definition. These properties influence their practical use in fields like machine learning, where lower p-norms (like L1 and L2) are preferred due to their balance of computational feasibility and desirable optimization properties. Higher p-norms, while theoretically interesting, often present practical challenges for optimization.\n",
        "\n",
        "*How can you quantify the computational cost?*\n",
        "\n",
        "Quantifying the computational cost of an algorithm or a computational process involves assessing several factors, such as time complexity, space complexity, and the actual execution time and memory usage in practice. Here's a breakdown of how these are typically quantified:\n",
        "\n",
        "1. **Time Complexity:**\n",
        "   - Time complexity is a theoretical measure of the execution time of an algorithm as a function of the size of the input data. It's usually expressed in Big O notation (like O(n), O(n^2), O(log n)), which describes the upper limit of the growth rate of the runtime.\n",
        "   - This measure helps to understand how the execution time will increase as the input size grows. It's crucial for comparing different algorithms, especially in terms of their scalability.\n",
        "\n",
        "2. **Space Complexity:**\n",
        "   - Space complexity measures the amount of memory an algorithm needs in terms of the size of the input data. Like time complexity, it's also expressed in Big O notation.\n",
        "   - This aspect is important in scenarios where memory is a limiting factor, such as in embedded systems or when dealing with extremely large data sets.\n",
        "\n",
        "3. **Actual Execution Time:**\n",
        "   - The actual execution time is the real-time an algorithm takes to complete. This can be measured by running the algorithm and timing its execution.\n",
        "   - The actual execution time can vary based on several factors, including the hardware used, the efficiency of the implementation, and the specific characteristics of the input data.\n",
        "\n",
        "4. **Actual Memory Usage:**\n",
        "   - This is a practical measurement of the amount of memory used by an algorithm when it runs. It can be measured using profiling tools.\n",
        "   - Actual memory usage can vary from the theoretical space complexity due to factors like the efficiency of the memory allocation in the implementation and overheads imposed by the system or the programming environment.\n",
        "\n",
        "5. **Algorithmic Efficiency:**\n",
        "   - Efficiency takes into account both time and space complexity. An efficient algorithm is one that has both a lower time complexity and a lower space complexity.\n",
        "   - In practice, there's often a trade-off between time and space efficiency (known as the time-space trade-off). Optimizing for one can lead to increased usage of the other.\n",
        "\n",
        "6. **Profiling and Benchmarking:**\n",
        "   - In practice, profiling and benchmarking are used to quantify the computational cost of an algorithm. Profiling involves running the algorithm with specific tools to measure various performance metrics like execution time and memory usage.\n",
        "   - Benchmarking involves comparing these performance metrics under different conditions or against other algorithms to assess relative performance.\n",
        "\n",
        "In summary, quantifying computational cost involves a mix of theoretical analysis (time and space complexity) and practical measurements (actual execution time and memory usage). This assessment helps in understanding the efficiency of an algorithm and its suitability for a particular application, especially when resources are limited or when dealing with large-scale problems.\n",
        "\n",
        "*could you quantify the Time Complexity in advance without running the function?*\n",
        "\n",
        "Yes, you can quantify the time complexity of an algorithm in advance without actually running the function. This is typically done through theoretical analysis based on the algorithm's structure and operations. Here's how you might approach this:\n",
        "\n",
        "1. **Understand the Algorithm's Operations:**\n",
        "   - Analyze each part of the algorithm and identify the basic operations (like comparisons, assignments, arithmetic operations) and how often they are performed relative to the input size.\n",
        "   - Pay special attention to loops and nested loops, as they often play a significant role in determining time complexity.\n",
        "\n",
        "2. **Counting Basic Operations:**\n",
        "   - For each line of code or each operation, estimate how many times it will be executed in terms of the size of the input. For instance, a single loop from 1 to \\( n \\) suggests \\( n \\) iterations, while a nested loop might suggest \\( n^2 \\) iterations.\n",
        "\n",
        "3. **Big O Notation:**\n",
        "   - Express the total number of basic operations in Big O notation. This involves finding the term with the fastest-growing rate and ignoring constants and less significant terms. For example, if the total operations are \\( 5n^2 + 3n + 8 \\), the time complexity is \\( O(n^2) \\).\n",
        "\n",
        "4. **Consider the Worst, Average, and Best Cases:**\n",
        "   - Analyze the time complexity for different scenarios: worst-case (upper bound), average case (expected complexity for a random input), and best case (lower bound). Sometimes, the complexity might be the same for all cases, but often it varies.\n",
        "\n",
        "5. **Comparing Different Parts of the Algorithm:**\n",
        "   - If the algorithm has different components or phases, analyze each part separately and then combine the results. The overall complexity is dominated by the part with the highest complexity.\n",
        "\n",
        "6. **Recursion:**\n",
        "   - For recursive algorithms, establish the recurrence relation describing the algorithm and solve it to find the time complexity.\n",
        "\n",
        "7. **Amortized Analysis:**\n",
        "   - In some cases, especially with algorithms that have occasional expensive operations, an amortized analysis might be more appropriate. This looks at the time complexity over a sequence of operations, providing an average time per operation.\n",
        "\n",
        "8. **Empirical Validation (Optional):**\n",
        "   - While not necessary for theoretical analysis, sometimes running the algorithm with different input sizes and measuring the execution time can validate your theoretical complexity analysis.\n",
        "\n",
        "It's important to note that this analysis provides an upper bound on the growth rate of the execution time and does not give the exact execution time, which can depend on factors like hardware, compiler optimizations, and specifics of the input data.\n",
        "\n",
        "*for linear regression using L2 norm the time complexity is n^2, or not?*\n",
        "\n",
        "The time complexity of linear regression using the L2 norm (Ridge Regression) isn't simply \\( n^2 \\); it depends on various factors including the number of features (denoted as \\( m \\)) and the number of training samples (denoted as \\( n \\)). Let's break it down:\n",
        "\n",
        "1. **Using the Normal Equation:**\n",
        "   - The normal equation for Ridge Regression is \\((X^TX + \\lambda I)^{-1}X^Ty\\), where \\( X \\) is the feature matrix, \\( y \\) is the target vector, \\( \\lambda \\) is the regularization parameter, and \\( I \\) is the identity matrix.\n",
        "   - The computationally intensive step here is the matrix multiplication \\( X^TX \\) and the matrix inversion \\((X^TX + \\lambda I)^{-1}\\).\n",
        "   - The multiplication \\( X^TX \\) has a complexity of \\( O(m^2n) \\), as it involves multiplying an \\( n \\times m \\) matrix with its transpose \\( m \\times n \\).\n",
        "   - Matrix inversion typically has a complexity of \\( O(m^3) \\) using standard algorithms like Gaussian elimination.\n",
        "   - So, the overall complexity would be dominated by the higher of \\( O(m^2n) \\) and \\( O(m^3) \\).\n",
        "\n",
        "2. **Using Gradient Descent:**\n",
        "   - If Ridge Regression is solved using gradient descent, the time complexity per iteration is \\( O(mn) \\), since each update requires computing gradients over all \\( n \\) samples and \\( m \\) features.\n",
        "   - However, the total number of iterations needed to converge can vary widely depending on the learning rate and the specifics of the dataset.\n",
        "\n",
        "In conclusion, the time complexity of linear regression using the L2 norm is not simply \\( n^2 \\); it depends on the method used for solving the regression and the relationship between the number of features \\( m \\) and the number of samples \\( n \\). For the normal equation method, it's generally the higher of \\( O(m^2n) \\) and \\( O(m^3) \\), while for gradient descent, it's \\( O(mn) \\) per iteration."
      ],
      "metadata": {
        "id": "JkiTC1xozlAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Physical Limits of Moore's Law**\n",
        "\n",
        "https://www.derstandard.de/story/3000000201716/auf-dem-weg-zu-graphen-chips-der-zukunft\n",
        "\n",
        "https://www.derstandard.at/story/2000143050519/chipherstellung-2d-kristalle-sollen-moores-law-zurueckbringen\n",
        "\n",
        "* [Moore's Law](https://en.m.wikipedia.org/wiki/Moore%27s_law), *Keywords: Wafer, semiconductor, microchips, silicon, transistors*\n",
        "\n",
        "* We have 1000x1000x1000 atoms distance. for quantum tunneling we would need 10x10x10\n",
        "\n",
        "* exponential improvement is the result of 1000 innovations at the same time\n",
        "\n",
        "* [Jim Keller: Moore's Law, Microprocessors, and First Principles | Lex Fridman Podcast #70](https://www.youtube.com/watch?v=Nb2tebYAaOA)\n",
        "\n",
        "* **Silicium** ist [Halbleiter](https://www.halbleiter.org/waferherstellung/silicium/): Leitfähigkeit zwischen der von Leitern und Nichtleitern. [Mikroelektronik](https://de.m.wikipedia.org/wiki/Mikroelektronik) und [Computerchips](https://de.m.wikipedia.org/wiki/Integrierter_Schaltkreis). Reinheitsgrade elementares Silicium: **Sieg** (electronic grade, Halbleitersilicium, Verunreinigungen kleiner 10^−9).\n",
        "\n",
        "* **Alternative: 2D waferswith molten sodium molybdate salt (Na₂MoO₄)** [Source (dt)](https://efahrer.chip.de/news/china-entwickelt-ultraduenne-halbleiter-revolution-in-der-technologiebranche_1014836) & [Source (en)](https://www.scmp.com/news/china/science/article/3232116/revolutionising-semiconductor-industry-chinese-scientists-unveil-12-inch-wafer-groundbreaking-2d)\n",
        "  * Ultra-thin 2D semiconductor: thickness is hardly higher than **thickness of an atom**. The layer is almost two-dimensional.\n",
        "  * succeeded in making 12-inch wafers (300 millimeters in diameter) from the material (standard measure for wafers in semiconductors).\n",
        "  * \"When silicon transistors become thinner, control of voltage deteriorates. Electricity flows even when device is not working. This brings additional energy costs and heat development\"\n",
        "  * For 2D wafers: molten sodium molybdate salt (Na2MoO4) - makes easier to control tension.\n",
        "\n",
        "* ***The size of today's transistors and microchips is measured in nanometers (nm). As of 2023, the smallest transistors that have been produced are around 5nm in size. This means that 5 billion transistors can fit on a single square millimeter of chip.***\n",
        "\n",
        "* The size of microchips is measured in square millimeters (mm2). The largest microchips that are currently being produced are around 1,000mm2 in size. These chips can contain tens of billions of transistors.\n",
        "\n",
        "* Moore's law states that the number of transistors on a microchip doubles every two years. This has held true for over 50 years, but there are signs that it is starting to slow down. This is because it is becoming increasingly difficult to manufacture transistors that are smaller and smaller.\n",
        "\n"
      ],
      "metadata": {
        "id": "DnUtKY5hSAgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://t3n.de/news/supercomputer-rekord-groesster-computerchip-transistoren-ki-kerne-1614241/\n",
        "\n",
        "https://www.cerebras.net/press-release/cerebras-announces-third-generation-wafer-scale-engine"
      ],
      "metadata": {
        "id": "rFyJidPsBQAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [1.2 - Racing Down the Slopes of Moore’s Law (Bram Nauta)](https://www.youtube.com/watch?v=THJP_HB5HEk&list=WL&index=5&t=596s)\n",
        "\n",
        "Video: [Why We're Reaching the Theoretical Limit of Computer Power](https://www.youtube.com/watch?v=Qlv5pB6u534&list=WL&index=6&t=1s)\n",
        "\n",
        "Video: [Integrated Circuit Design – EE Master Specialisation](https://www.youtube.com/watch?v=jZIxNIzi-I8&list=WL&index=7)\n",
        "\n",
        "Video: [Jim Keller: Moore's Law, Microprocessors, and First Principles | Lex Fridman Podcast #70](https://www.youtube.com/watch?v=Nb2tebYAaOA)"
      ],
      "metadata": {
        "id": "ZgToug2OXUfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transistor: [New Microchip Breakthrough: New Era in Electronics?](https://youtu.be/wGzBuspS9JI?si=AxNQFBEttoOzeSDX)\n"
      ],
      "metadata": {
        "id": "LQ-KyX_3u3iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chips und Transistoren**\n",
        "\n",
        "https://www.heise.de/news/Intel-und-TSMC-arbeiten-an-Transistoren-der-Zukunft-9568367.html\n",
        "\n",
        "https://www.golem.de/news/smic-3-nm-prozessoren-aus-china-auch-ohne-westliche-unterstuetzung-2312-180619.amp.html\n",
        "\n",
        "https://www.pcgameshardware.de/CPU-CPU-154106/News/Doppelte-Dichte-TSMC-CFET-Fertigung-90-Prozent-Yield-1436998/\n",
        "\n",
        "https://www.tomshardware.com/tech-industry/semiconductors/intels-ceo-says-moores-law-is-slowing-to-a-three-year-cadence-but-its-not-dead-yet\n"
      ],
      "metadata": {
        "id": "wzyPXaZ2R53L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***How many atoms distance have today‘s smallest transistors?***\n",
        "\n",
        "* The smallest transistors that have been produced as of 2023 are around 5nm in size. A silicon atom is about 0.117nm in diameter. This means that the smallest transistors are about 43 atoms wide.\n",
        "\n",
        "* However, it is important to note that the gate length of a transistor is not the only factor that determines its size. The transistor also has other components, such as the source, drain, and body. These components also take up space.\n",
        "\n",
        "* As a result, the actual size of a transistor is typically larger than the gate length. For example, a 5nm transistor might actually be 10nm or more in width.\n",
        "\n",
        "* This means that the smallest transistors today are made up of hundreds or even thousands of atoms.\n",
        "\n",
        "* It is also important to note that the size of transistors is constantly shrinking. Researchers are working on developing new materials and manufacturing techniques that will allow them to produce even smaller transistors.\n",
        "\n",
        "* So, the answer to the question of how many atoms distance today's smallest transistors have is that it depends on how you measure it. If you measure the gate length, then the smallest transistors are about 43 atoms wide. However, if you measure the overall size of the transistor, then the smallest transistors are made up of hundreds or even thousands of atoms.\n",
        "\n",
        "* ***And is it true that transistors need a distance of 10 atoms ao that for quantum tunneling takes effect and limits further shrinking physically?***\n",
        "\n",
        "* Yes, it is true that transistors need a distance of about 10 atoms in order for quantum tunneling to take effect and limit further shrinking physically.\n",
        "\n",
        "* Quantum tunneling is a phenomenon in which particles can pass through barriers that they would not be able to overcome classically. This is because particles have a wave-like nature, and they can have a certain probability of being found on the other side of a barrier, even if they do not have enough energy to overcome it.\n",
        "\n",
        "* In the case of transistors, quantum tunneling can cause electrons to leak from the source to the drain, even when the transistor is turned off. This can lead to increased power consumption and reduced performance.\n",
        "\n",
        "* The distance at which quantum tunneling becomes a problem depends on a number of factors, including the height of the barrier and the energy of the particles. However, it is generally believed that quantum tunneling becomes a significant problem when the distance between the source and drain is less than about 10 atoms.\n",
        "\n",
        "* This means that the smallest transistors that can be made using current technology are around 10nm in size. However, researchers are working on new materials and manufacturing techniques that could allow them to produce even smaller transistors in the future.\n",
        "\n",
        "* One way to reduce the effects of quantum tunneling is to use new materials that have a higher bandgap. The bandgap is the energy difference between the valence band and the conduction band. Electrons in the valence band are bound to atoms, while electrons in the conduction band are free to move around. The higher the bandgap, the more energy it takes for an electron to move from the valence band to the conduction band.\n",
        "\n",
        "* Another way to reduce the effects of quantum tunneling is to use new manufacturing techniques that create sharper interfaces between the different layers of the transistor. This helps to create a higher barrier that electrons need to tunnel through.\n",
        "\n",
        "* Researchers are also investigating new transistor designs that are less susceptible to quantum tunneling. For example, some researchers are developing transistors that use vertical channels instead of horizontal channels. Vertical channels are more difficult for electrons to tunnel through than horizontal channels.\n",
        "\n",
        "* It is still too early to say when or if transistors that are smaller than 10nm will be possible to manufacture. However, the research that is being done in this area is very promising.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeuqaZABR9i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Physical Limits of CPU-Clockspeed**\n",
        "* There are a few reasons why CPU clock speed is not increasing anymore. See article: [why-cpu-clock-speed-isnt-increasing](https://www.maketecheasier.com/why-cpu-clock-speed-isnt-increasing/)\n",
        "* **The physical limits of transistors.** Transistors are the basic building blocks of a CPU, and they can only switch so fast. As transistors get smaller, they also get slower. This is because the distance between the transistors gets smaller, and the electrons have to travel a shorter distance to switch them on and off.\n",
        "* **The power consumption of CPUs.** As the clock speed of a CPU increases, so does its power consumption. This is because the transistors have to switch more often, and this requires more energy.\n",
        "* **The heat generated by CPUs.** When a CPU consumes more power, it also generates more heat. This heat can cause the CPU to throttle its clock speed, or even shut down, to prevent damage.\n",
        "* As a result of these factors, CPU manufacturers have shifted their focus to other ways to improve CPU performance, such as increasing the number of cores and using new technologies like multithreading.\n",
        "* Here are some of the ways that CPU manufacturers are increasing performance without increasing clock speed:\n",
        "\n",
        "  * **Adding more cores.** A CPU with multiple cores can run multiple tasks at the same time, which can significantly improve performance.\n",
        "  * **Using multithreading.** Multithreading allows a single core to run multiple tasks by dividing each task into smaller parts that can be executed independently.\n",
        "  * **Using new technologies.** New technologies such as 3D stacking and chiplets are allowing CPU manufacturers to pack more transistors into a smaller space, which can improve performance without increasing clock speed.\n"
      ],
      "metadata": {
        "id": "TN3MLVdwSCzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Multiplication on Classical Computers**\n",
        "\n",
        "Video 1: [How do GPUs speed up Neural Network training?](https://www.youtube.com/watch?v=EKD1kEMNeeU&list=WL&index=27)\n",
        "\n",
        "Video 2: [EE5332 L11.3 - Matrix Multiplication on NVidia GPUs](https://www.youtube.com/watch?v=fpwq5zDBO2o&list=WL&index=28&t=362s)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1633.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1634.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1635.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1636.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1637.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1638.png)"
      ],
      "metadata": {
        "id": "ZZPqdWX4RiZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero power computing**:\n",
        "* a perfect fredkin gate doesnt cost energy! (Information theory & Entropy Limits) [How to Perform Calculations Using Zero Power](https://medium.com/hackernoon/zero-power-computing-how-to-perform-calculations-using-zero-power-e2b4bfcd4d7e).\n",
        "* **What costs the energy is not the computation itself, but‘s the raising information**. Take a look at this original Fredkin/Toffoli paper on the subject, section 5 (**nondissipative computation**): https://cqi.inf.usi.ch/qic/82_Fredkin.pdf.\n",
        "* Fredkin: 3 in 3 out, reversable?[Fredkin gate](https://www.wikiwand.com/en/Fredkin_gate). Stackexchange: [Why are reversible gates not used?](https://cs.stackexchange.com/questions/38049/why-are-reversible-gates-not-used/38053#38053).\n",
        "* Science: [A quantum Fredkin gate](https://www.science.org/doi/10.1126/sciadv.1501531). Paper: Thermodynamic [The Cost of computation](https://arxiv.org/abs/1905.05669).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7P8TyPKLSga0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flow of Compilation in Programming Languages**\n",
        "\n",
        "* **First generation (1GL)**:\n",
        "  * Machine code (binary befehle), written in binary code\n",
        "  * machine-specific = each language designed for a specific type of computer\n",
        "\n",
        "* **Second generation (2GL)**:\n",
        "  * Assembly Language, mit [Hexadezimalsystem](https://de.m.wikipedia.org/wiki/Hexadezimalsystem), wie Intel x86, still machine-specific, but easier to write and read than binary code. Assembly languages used mnemonics (=abbreviations for machine instructions)\n",
        "  * Shell: command-line interpreter to control operating system (automate tasks). Book: [Assembler-Programmierung für x86-Prozessoren\n",
        "  ](https://de.m.wikibooks.org/wiki/Assembler-Programmierung_für_x86-Prozessoren/_Druckversion)\n",
        "\n",
        "* **Third generation (3GL)**:\n",
        "  * high-mid level progamming language, like BASIC, COBOL, FORTRAN, Pascal, C, Python, C++, Java, Perl\n",
        "  * (high-mid level progamming language):\n",
        "  * were a major breakthrough in programming: 3GL languages were not machine-specific anymore\n",
        "  * 3GL languages also used English-like keywords = easier to learn and use than assembly languages\n",
        "\n",
        "* **Fourth generation (4GL)**:\n",
        "  * SQL, PL/SQL, Visual Basic, PowerBuilder, TensorFlow\n",
        "  * also known as non-procedural languages, were designed to make programming even easier.\n",
        "  4GL languages use natural language statements to describe what the program should do, rather than how to do it. This makes 4GL languages ideal for business applications.\n",
        "\n",
        "* **Fifth generation (5GL)**:\n",
        "  * [Prolog](https://en.m.wikipedia.org/wiki/Prolog), OPS5, Mercury\n",
        "  * also known as logic programming languages, are based on AI: use logic to solve problems, rather than traditional programming techniques\n",
        "  * 5GL languages are still in their early stages of development\n",
        "\n",
        "**Modern flow of compilation involves following steps**:\n",
        "\n",
        "1. **Preprocessing:** This step removes comments and other non-essential text from the source code.\n",
        "2. **Lexical analysis:** This step breaks the source code into tokens, which are the basic building blocks of the language.\n",
        "3. **Parsing:** This step constructs a parse tree, which is a graphical representation of the syntactic structure of the source code.\n",
        "4. **Semantic analysis:** This step checks the source code for errors in its meaning.\n",
        "5. **Code generation:** This step generates machine code from the parse tree.\n",
        "6. **Optimization:** This step can improve the performance of the machine code by removing unnecessary instructions or by rearranging the instructions in a more efficient order."
      ],
      "metadata": {
        "id": "yAO-PIoC6Atc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FLOPS**\n",
        "\n",
        "Eine derart gigantische Simulation konnte nur mit entsprechender Rechenleistung bewältigt werden, die der Supercomputer HPE SGI ICE XA (Cheyenne) dankenswerterweise zur Verfügung gestellt hat.\n",
        "* **Mit 145.152 Prozessoren, 40 Petabyte Speicherplatz und insgesamt 5,3 Petaflops** waren die nötigen Modellierungen realisierbar. Und die detaillierten Ergebnisse sind beachtenswert.\n",
        "\n",
        "https://www.notebookcheck.com/Aufforstung-kein-Allheilmittel-Waelder-mit-komplexem-Einfluss-auf-Klima-und-Umwelt.807845.0.html"
      ],
      "metadata": {
        "id": "0ycT8U37bhB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binär- und Terniärsysteme**\n",
        "\n",
        "Gleitkommazahl: https://www.golem.de/news/bitnet-statt-gleitkommazahlen-forscher-versprechen-massive-effizienzsteigerung-bei-ki-2403-182763.html\n",
        "\n",
        "FP8\n",
        "https://en.wikipedia.org/wiki/Minifloat\n",
        "\n",
        "half precision (sometimes called FP16 or float16)\n",
        "https://en.wikipedia.org/wiki/Half-precision_floating-point_format\n",
        "\n",
        "bfloat16 (brain floating point with 16 bits). In TPUs.\n",
        "https://de.wikipedia.org/wiki/Bfloat16\n",
        "\n",
        "Double-precision floating-point format (sometimes called FP64 or float64)\n",
        "https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n",
        "\n",
        "Terniärsystem\n",
        "https://de.wikipedia.org/wiki/Tern%C3%A4rsystem\n",
        "\n",
        "https://en.wikipedia.org/wiki/Floating-point_arithmetic\n",
        "\n",
        "https://de.wikipedia.org/wiki/Gleitkommazahl"
      ],
      "metadata": {
        "id": "0vY8qYEH5lc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Pre-electronic computing machines (why are we working with binary?)**\n",
        "- Greece: [Antikythera](https://de.m.wikipedia.org/wiki/Mechanismus_von_Antikythera) to predict solar esclipses\n",
        "- Charles Babbage and [Difference engine and Analytical Engine](https://de.m.wikipedia.org/wiki/Analytical_Engine)\n",
        "- Fur Dezimalsystem: kann man ein Mechanisches Zahnrad erstellen, zehn Zähne schleifen and jeweils equal distances apart um genau zu rechnen?\n",
        "- But Charles had complains from their gear cutters. He had to pay a lot, because manufacturing wasn’t advance at that time yet (no large-scale automation in manufacturing).\n",
        "- Later: electro-mechanical era of relays\n",
        "- Then: early electronic era (1930/40s): the first electronic logic elements. you talked about [thermionic valves (Röhrencomputer)](https://de.m.wikipedia.org/wiki/Röhrencomputer)\n",
        "    -  Röhrenrechner stellen den Übergang zwischen den [Analogrechnern](https://de.m.wikipedia.org/wiki/Analogrechner) (=Berechnungen mit Hilfe von kontinuierlichen mechanischen oder elektrischen Vorgängen) zu den auf [Halbleitertechnik](https://de.m.wikipedia.org/wiki/Halbleitertechnik) basierenden [Minirechnern](https://de.m.wikipedia.org/wiki/Minirechner) dar.\n",
        "- How they works: vacuum tubes as a logic element on and off. Cathode with boils off electrons and anode with a big voltage on it, sucking off the electrons up, and in the middle there is a grid. By putting a bias voltage on the grid. If you put a heavy negative voltage on the grid, it repelled the electrons back to the cathode plate and didn’t let them through. So you switched it off. But you needed heaters to boil the electrons off the cathode. Heaters, power consumption was huge.\n",
        "- One reason to use binary was that it’s perfect for the logic. They are simpler to build!\n",
        "- But: For decimal you need fewer digits (binary is a lot longer)\n",
        "- How many more circuitry and components do you need if you go for a binary computing than if you go for a decimal one (how many more digits we need?): log_2 10 = 3.322 (= you need 3.3 times as much binary circuitry if you did binary)\n",
        "    - e.g. how many bits do you need to represent 99 in binary?\n",
        "    - 8 bits = 256, 7 = 128. So 99 requires 7 bits\n",
        "    - Taking 99 in decimal are 2 bits, using the formula above: 2 x 3.322 = 6.644 ~ 7 bits\n",
        "- Alternaive approach in 1930s: bi-quinary: base 5 ([“Colossus”](https://de.m.wikipedia.org/wiki/Colossus))\n",
        "- [Why Use Binary? - Computerphile](https://www.youtube.com/watch?v=thrx3SBEpL8) by [Tommy Flowers](https://de.m.wikipedia.org/wiki/Tommy_Flowers)\n",
        "\n",
        "*Warum sind 8 Bit 1 Byte?*\n",
        "* Bit = Datentransferrate (Geschwindigkeit).\n",
        "* Byte = Speichergröße (Kapazität)\n",
        "* 1 Byte in Bits umgerechnet sind 8 Bits.\n",
        "* 1 Byte kann somit 2^8 (256 Bits) verschiedene Zustände darstellen. Somit ist 1 Byte meist die kleinste, adressierbare Speichereinheit, um Zeichen wie einen Buchstaben abzubilden."
      ],
      "metadata": {
        "id": "jVPHK8KmrZTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"Blue\">*Algorithmic Complexity*"
      ],
      "metadata": {
        "id": "Gj-d2EYc2e7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Algorithmic Information (Algorithmic Complexity)](https://en.m.wikipedia.org/wiki/Algorithmic_information_theory) = <u>**How difficult is it to describe a function concisely?**</u>\n",
        "\n",
        "* Rayo's number is the smallest number that cannot be expressed anymore- an example??\n",
        "\n",
        "* One motivation: Boltzmann's famous example of monkeys typing on typewriters will eventually end up writing a book by Shakespear - but chances are vanishing small, much longer than our universe.\n",
        "  * Same is for chance that monkey type first x digits of Pi correct - very small\n",
        "  * But going down to program level: what's the chance of writing a program / function that spits out all numbers of Pi? That is much more likely by monkeys.\n",
        "  * Then you can also ask: what things you can NOT describe with short programs anymore? where you need an instruction that is the same size of it's output (Kolmogorof complexity)? Example: Any real number!\n",
        "\n",
        "* [Mathematical Simplicity May Drive Evolution’s Speed](https://www.quantamagazine.org/computer-science-and-biology-explore-algorithmic-evolution-20181129/): for some outputs, it’s computationally easier to describe how to generate something than to actually generate it. The probability of producing some types of outputs is far greater when randomness operates at the level of the program describing it rather than at the level of the output itself, because that program will be short.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1679.png)\n",
        "\n",
        "* Algorithmic Information Theory (AIT) (Algorithmic Complexity): minimum amount of information, or minimum size of algorithm that you need to express a problem or function unambigously. AIT is study of relationship between computation and information, and is concerned with [Kolmogorov Complexity](https://en.m.wikipedia.org/wiki/Kolmogorov_complexity) of objects (Algorithmic Complexity or Solomonoff–Kolmogorov–Chaitin complexity)\n",
        "  * Is a measure of complexity of a particular string in terms of all algorithms that generate it.\n",
        "  * Kolmogorov Complexity is **the length of the shortest computer program that can generate the object**.\n",
        "  * Complexity of X is the length of the shortest description d such that if you feed it into the universal Turing machine, it spits out the string X.\n",
        "  * When a string has no short description (means the complexity is about the length of X and I need to feed the entire string into a universal Turing machine), that means the string is random.\n",
        "  * So Kolmogorov complexity is equal to their length = shortest program that can solve a problems is just as long as the input to the problem.\n",
        "* AIT is typically concerned with the complexity of individual objects, such as strings or Turing machines (as opposed to Computational Complexity Theory which looks at entire classes of problems)\n",
        "\n",
        "\n",
        "> <font color=\"Blue\">**Algorithmic Complexity Theory and Algorithmic Information Theory: It‘s useful to talk about complexity of models or structures in terms of minimum amount of information, or minimum size of algorithm that you need to express that unambigously.**</font>\n",
        "\n",
        "* **Example 1: What is simpler: a single integer or the set of all integers?** (from Kolmogorov complexity)\n",
        "  * Answer is counter-intuitive: it’s not a single integer, because the vast majority of all integers are way more complicated than the set of all possible integers.\n",
        "  * Because you can write down a simple algorithm, rule or mathematical expression that unambigously defines the set of all possible integers. But a single integer can require an arbitrarily large amount of information to express.\n",
        "  * So from a Kolmogorov complexity standpoint or algorithmic information complexity standpoint most integers are more complicated than the set of all integers.\n",
        "  * **This is been used as an argument in favor of the multiverse**. As an ontology: what‘s simpler: just our un universe exists, or the space of all possible universes exist? The latter is more plausible [Source](https://youtu.be/DaKR-UiYd6k?si=hZ-qbpiRU4gDxt9E)\n",
        "\n",
        "* **Example 2:  Is the universe predictable and definable by a finite set of symbols (Computational Irreducability)?**\n",
        "  * Assuming universe is computable, is it also predictable (=computationally reducable), or you need to run it?\n",
        "  * [Computational Irreducability](https://en.m.wikipedia.org/wiki/Computational_irreducibility) theory: no model can predict using only initial conditions, exactly what will occur in a given physical system before an experiment is conducted. It is a topic in both computational complexity theory (=problems cannot be solved efficiently by any algorithm) and algorithmic information theory (=how difficult it is to describe the object using a computer program).\n",
        "  * **Wolfram argues that many natural systems are computationally irreducible, which means that they cannot be simulated efficiently by any computer program**. Examples of problems that are thought to be computationally irreducible:\n",
        "    * The halting problem: determining whether a given computer program will halt or run forever. The factoring problem: factoring a large number into its prime factors.\n",
        "    * <font color=\"Blue\">These problems are thought to be computationally irreducible **because their Kolmogorov complexity is equal to their length. In other words, the shortest program that can solve these problems is just as long as the input to the problem**.</font>\n",
        "  * Universe: It seems that **there is no simplification that you can make. you just have to do the computation from beginning to end to work it out how the universe is going to evolve** (we have to explicitely simulate every step, there is no equation that spits out the answer, because it‘s fundamentally irredusable). Examples fluid mechanics: where is every molecule? We have to simulate it, e.g. position of individual molecules, that is completely unknown, would require arbitrary amounts of computational effort to determine. You need to use Coarse grain to make bulk statements: navier stokes or euler equation, Partition functions, Boltzmann equatio, Ergodicity: there is no net movement of particles in any direction. One particle distribution function, chapman-enskog expansion. From [Video 1](https://youtu.be/EIyjaCwbYXQ?si=-rmxgdj7Bpz945Fm) and [Video 2](https://youtu.be/DaKR-UiYd6k?si=pOSreHgxonkg4Wr9).\n",
        "  * Parts of universe seem to be computable and definable with a finite set of symbols, at least based on our current understanding of physics and mathematics. However, the complete computability and definability of the universe remain open questions, subject to ongoing scientific and philosophical inquiry. It is an area where new discoveries and insights can potentially reshape our understanding in fundamental ways.\n",
        "\n",
        "* **Further problems and examples in AIT**\n",
        "\n",
        "  * **Assembly Theory**: [Assembly theory](https://en.m.wikipedia.org/wiki/Assembly_theory) is a hypothesis that characterizes object complexity - molecules produced by biological processes must be more complex than those produced by non-biological processes. [Article](https://www.quantamagazine.org/a-new-theory-for-the-assembly-of-life-in-the-universe-20230504/). It studies complexity of constructing objects from smaller components and is **based on idea that complexity of an object is equal to minimum number of steps required to construct it from a set of primitive components**. (AT is more focused on construction of objects, while AIT is more focused on description of objects)\n",
        "\n",
        "  * **Constructor Theory** (Chiara Marletto and David Deutsch). [Constructor_theory](https://en.m.wikipedia.org/wiki/Constructor_theory). [How to Rewrite the Laws of Physics in the Language of Impossibility](https://www.quantamagazine.org/with-constructor-theory-chiara-marletto-invokes-the-impossible-20210429/). [Physicists Rewrite the Fundamental Law That Leads to Disorder](https://www.quantamagazine.org/physicists-trace-the-rise-in-entropy-to-quantum-information-20220526/). It studies complexity of constructing objects from smaller components and is **based on idea that complexity of an object is equal to minimum number of steps required to construct it from a set of primitive components**. (CT is more focused on construction of objects, while AIT is more focused on the description of objects)\n",
        "\n",
        "  * Quantamagazine: [Mathematical Simplicity May Drive Evolution’s Speed](https://www.quantamagazine.org/computer-science-and-biology-explore-algorithmic-evolution-20181129/)\n",
        "\n",
        "* **Difference to Computational Complexity Theory (CCT):**\n",
        "\n",
        "  * CCT: **Can a problem be solved** in polynomial time? - Complexity of **classes** of computational problems.\n",
        "\n",
        "  * AIT: **How complex** is a given string? Complexity of **individual objects**.\n",
        "\n",
        "  * Algorithmic complexity (Kolmogorov): Length of the shortest program that can generate a given output. How difficult it is to describe an object using a computer program?\n",
        "\n",
        "* **Connections to Computational Complexity Theory (CCT):**\n",
        "\n",
        "  * AIT provides a theoretical foundation for CCT. CCT is study of amount of resources (time and space) required to solve computational problems. CCT is concerned with complexity of classes of problems (P, NP).\n",
        "\n",
        "  * **AIT can prove lower bounds on time and space complexity**: define complexity classes P and NP (with Kolmogorov complexity) or study computational complexity of certain problems (sorting list of numbers). [Minimum Description Length (MDL) principle](https://en.m.wikipedia.org/wiki/Minimum_description_length) is based on AIT and used in ML."
      ],
      "metadata": {
        "id": "yy8Hru9l28pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problems at intersection between Computability Theory and Algorithmic Information Theory (Algorithmic Complexity)**\n",
        "\n",
        "> <font color=\"blue\">**Three dimensions of complexity of function or problem:**</font>\n",
        "* **Is it computable (solvable, or undecidable)?** (<font color=\"blue\">computability theory</font>)\n",
        "* **Is it computable efficiently (in finite spacetime)?** (<font color=\"blue\">computational complexity theory</font>)\n",
        "* **Is it describable concisely (with finite set of symbols)?** (<font color=\"blue\">algorithmic complexity / information theory</font>)\n",
        "\n",
        "*Special: **Is it predictable = simulatable efficiently** by any computer program? (computationally irreducibility theorem)  (a topic in computational complexity theory=problems cannot be solved efficiently by any algorithm, and algorithmic information theory=how difficult it is to describe the object using a computer program)*\n",
        "\n",
        "**Example: Number Theory and Computability: Difference between (in)finite value and (in)finite instruction within ZFC?**\n",
        "\n",
        "<font color=\"red\">**Computable and definable with finite set of symbols**\n",
        "* Algorithms for Prime Numbers, Fibonacci sequence\n",
        "* Graham's number is the biggest number used constructively (is not transfinite) - it is definable using a finite set of symbols, as evidenced by its representation in Knuth's up-arrow notation.\n",
        "* Tree3. Loaders number. SCG(13) and SSCG(3)\n",
        "\n",
        "<font color=\"red\">**Not Computable but still definable with finite set of symbols**\n",
        "* **Busy Beaver function**: ZFC can define the Busy Beaver function itself, in less than, say, a billion symbols. But ZFC can't pin down the precise value of even BB(7918) = can not determine specific values of the function. Closely related to undecidable Halting problem.\n",
        "* **Rayo's number**: largest number nameable by an expression in first-order set theory with a given finite number of symbols. The number it represents is so vast that it transcends ordinary concepts of computability and representation in mathematics.\n",
        "* **Chaitin's number**: is transfinite and it is not computable, which means that there is no algorithm to compute its digits exactly. Closely related to undecidable Halting problem.\n",
        "* **Transfinite numbers**: are numbers that are greater than any finite number. They represent different \"sizes\" of infinity. The concept was introduced by Georg Cantor. Two most well-known transfinite numbers are ℵ₀ (aleph-null) and ℵ₁ (aleph-one). Transfinite numbers are not computable in the traditional sense because they represent infinite values. Despite their infinite nature, transfinite numbers can be defined using a finite set of symbols. For example, ℵ₀ is defined as the cardinality (size) of the set of natural numbers, and ℵ₁ is defined as the next larger infinite cardinal number.\n",
        "\n",
        "<font color=\"red\">**Not Computable and not definable with finite set of symbols**\n",
        "* **Most real numbers**: Fast alle reelle Zahlen sind nicht berechenbar, [gehorchen nicht einmal einer Rechenvorschrift](https://www.spektrum.de/kolumne/die-meisten-reellen-zahlen-sind-nicht-berechenbar/2133762), there are uncountably many real numbers, but only countably many finite strings of symbols with which to define numbers. This means that almost all real numbers are not definable with a finite set of symbols.\n",
        "* Computing **very large and precise value of the busy beaver function** can be either undefinable by finite set of symbols, or not practically / efficently feasible but still possible with a very very large, finite set of symbols. This isn't cut-clear.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1631.png)\n",
        "\n",
        "Source: [All the Numbers - Numberphile](https://www.youtube.com/watch?v=5TkIe60y2GI&t=458s). See also: [Large_numbers](https://en.m.wikipedia.org/wiki/Large_numbers), [Names of large numbers](https://en.m.wikipedia.org/wiki/Names_of_large_numbers), [Liste besonderer Zahlen](https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen), [Constructible numbers](https://en.m.wikipedia.org/wiki/Constructible_number), [Algebraic numbers](https://en.m.wikipedia.org/wiki/Algebraic_number), [Transcendental numbers](https://en.m.wikipedia.org/wiki/Transcendental_number), [Computable numbers](https://en.m.wikipedia.org/wiki/Computable_number). **Non-Computable numbers**: [Chaitin constant](https://en.m.wikipedia.org/wiki/Chaitin%27s_constant). [Die meisten reellen Zahlen kennen wir nicht](https://www.spektrum.de/kolumne/die-meisten-reellen-zahlen-sind-nicht-berechenbar/2133762) (gehorchen nicht einmal einer Rechenvorschrift). **Special cross-section**: [Normal numbers](https://en.m.wikipedia.org/wiki/Normal_number): [Champernowne’s constant](https://en.m.wikipedia.org/wiki/Champernowne_constant) (whole numbers) - normal and transendental. [Copeland-Erdös-number](https://de.m.wikipedia.org/wiki/Copeland-Erdős-Zahl) (primes). **Empty section: normal and Non-Computable numbers (we have no examples)**: We have no examples. But proofs have shown: this is the greatest amount of numbers: most numbers are normal and most numbers are uncomputable. So this section should be full, but we have no example."
      ],
      "metadata": {
        "id": "6431aTWzeYh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"Blue\">*Computability Theory*"
      ],
      "metadata": {
        "id": "gNGVQC0OENoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Computability_theory](https://en.m.wikipedia.org/wiki/Computability_theory) = <u>**Can a function be computed (solved) on a Turing machine?**</u>\n",
        "\n",
        "* What problems can be solved? (Halting, Entscheidungsproblem, etc. with Turing machine, finite automata..)\n",
        "\n",
        "* [An Easy-Sounding Problem Yields Numbers Too Big for Our Universe](https://www.quantamagazine.org/an-easy-sounding-problem-yields-numbers-too-big-for-our-universe-20231204/) - Researchers prove that navigating certain systems of vectors is among the most complex computational problems.\n",
        "\n",
        "* Video: [Extended Church Turing thesis](https://youtu.be/gs2Pv2vHqn8?si=bkcy04UGegfa7_yd)\n",
        "\n",
        "* COMPUTABILITY: [How to Build an Origami Computer](https://www.quantamagazine.org/how-to-build-an-origami-computer-20240130/) (quantamagazine)\n",
        "\n",
        "* Computability_theory **studies the theoretical limits of what can be computed by an idealized computing device**, such as a [Turing machine](https://en.m.wikipedia.org/wiki/Turing_machine). [Church Turing Thesis](https://en.m.wikipedia.org/wiki/Church%E2%80%93Turing_thesis): all algorithms may be thought of as Turing machines. apply to all function. you cannot short cut computation: Halting problem.\n",
        "\n",
        "  * **Turing**: functions that can be computed by Turing machine is computable by humans using paper and pencil\n",
        "\n",
        "  * **Gödel**: there are certain statements that cannot be proven or disproven within any formal system (incompleteness theorem). Implications for computability theory: **there are some functions that cannot be computed by any Turing machine (Halting problem, Collatz conjecture, Turing degrees)**\n",
        "\n",
        "* Video: [Church-Turing Thesis Cannot Possibly Be True\n",
        "](https://www.youtube.com/watch?v=egK4xhuWsVY&t=61s):\n",
        "  * The thesis asserts this: If an algorithm A computes a partial function f from natural numbers to natural numbers then f is partially recursive, i.e., the graph of f is recursively enumerable.\n",
        "  * The thesis has been formulated in 1930s. The only algorithms at the time were sequential algorithms. Sequential algorithms were axiomatized in 2000. This axiomatization was used in 2008 to prove the thesis for sequential algorithms, i.e., for the case where A ranges over sequential algorithms.\n",
        "  * These days, in addition to sequential algorithms, there are parallel algorithms, distributed algorithms, probabilistic algorithms, quantum algorithms, learning algorithms, etc.\n",
        "  * The question whether the thesis is true in full generality is actively discussed from 1960s. We argue that, in full generality, the thesis cannot possibly be true.\n",
        "\n",
        "* Computability theory provides the theoretical foundation for computational complexity theory (CCT), and many of the problems studied in CCT are motivated by questions in computability theory.\n",
        "\n",
        "* A computable **number** a real number that can be calculated to any desired precision by finite, terminating algorithm. But: almost no real numbers are computable.\n",
        "* A computable **function** requires a finite number of steps to produce the output. The Busy Beaver function Σ(n) grows faster than any computable function. Hence, it is not computable; only a few values are known.\n",
        "\n",
        "*See [Computability (Berechenbarkeit)](https://en.m.wikipedia.org/wiki/Computability), [Analysis of algorithms](https://en.m.wikipedia.org/wiki/Analysis_of_algorithms), [Model of Computation](https://en.m.wikipedia.org/wiki/Model_of_computation), [Theory of Computation](https://en.m.wikipedia.org/wiki/Theory_of_computation), [Algorithmic Information Theory](https://en.m.wikipedia.org/wiki/Algorithmic_information_theory), [Undecidable problem](https://en.m.wikipedia.org/wiki/Undecidable_problem), [List of undecidable problems](https://en.m.wikipedia.org/wiki/List_of_undecidable_problems), [Limits of Computation](https://en.m.wikipedia.org/wiki/Limits_of_computation), [Pfeilschreibweise](https://de.m.wikipedia.org/wiki/Pfeilschreibweise), [Hyper-Operator](https://de.m.wikipedia.org/wiki/Hyper-Operator), [Potenzturm](https://de.m.wikipedia.org/wiki/Potenzturm), [Long_and_short_scales](https://en.m.wikipedia.org/wiki/Long_and_short_scales)*"
      ],
      "metadata": {
        "id": "ERjGs_IMvI3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Extended Church Turing thesis (qECTT)**\n",
        "* Quantum Extension: The qECTT proposes that any physically realizable computational device can be efficiently simulated by a quantum Turing machine. **In essence, it posits that quantum computers, though vastly more powerful than classical computers, still operate within limits that can be, in principle, simulated**.\n",
        "* Most researchers think it's valid. If yes, it would forbid \"supertasks\" like solving the halting problem (Quantum computers remain fundamentally powerful but seem unlikely to enable true hypercomputation).\n",
        "* Open Question: However, the door isn't fully closed. Exotic models of quantum mechanics or yet-undiscovered physics could offer surprises, and philosophical debates over the definition of computation itself contribute to the ongoing discussion.\n",
        "\n",
        "* The **quantum extended Church-Turing thesis** (qECTT) states that any physical system that can compute any function computable by a quantum Turing machine can be efficiently simulated by a quantum Turing machine.\n",
        "\n",
        "> **In other words, the qECTT asserts that quantum computers are the most powerful physical computers possible, and that no other physical system can compute any function that a quantum computer cannot efficiently compute.**\n",
        "\n",
        "The qECTT is based on the following two premises:\n",
        "\n",
        "1. Quantum computers are more powerful than classical computers, in the sense that they can efficiently solve certain problems that are intractable for classical computers.\n",
        "  * The first premise is well-established, and has been demonstrated by the development of quantum algorithms that can efficiently solve certain problems that are intractable for classical computers, such as Shor's algorithm for factoring large numbers and Grover's algorithm for searching unsorted databases.\n",
        "2. Any physical system that can compute any function can be efficiently simulated by a quantum Turing machine.\n",
        "  * The second premise is more speculative, but it is supported by the fact that quantum Turing machines are a universal model of computation, meaning that they can simulate any other physical system of computation.\n",
        "\n",
        "> <font color=\"blue\">**The qECTT is still an open question**, but it is a very important one, as it has **implications for the limits of what is computable in the physical world**</font>. If the qECTT is true, then it means that quantum computers are the most powerful physical computers possible, and that there are certain problems that cannot be efficiently solved by any physical computer.\n",
        "\n",
        "* Interesting example and contradiction of the qECTT:\n",
        "\n",
        "  * Suppose that there is a physical system, such as a black hole, that can compute some function that cannot be efficiently computed by a quantum Turing machine. Then, the qECTT predicts that there is a quantum Turing machine that can efficiently simulate the black hole, and thus also compute the function.\n",
        "\n",
        "  * This is a very powerful prediction, and it is still not known whether it is true. However, <font color=\"blue\">**if the qECTT is true, then it would have implications for our understanding of the nature of computation and the limits of what is possible in the universe**</font>.\n",
        "\n",
        "  * This is a contradiction because it implies that there is a quantum Turing machine that can compute any function, which is not possible. The Church-Turing thesis states that there are some functions that cannot be computed by any Turing machine, and the qECTT is an extension of the Church-Turing thesis to quantum computers.\n",
        "\n",
        "  * So, the contradictory example of the qECTT shows that the qECTT itself is not a valid thesis. However, it is still an interesting and important question to ask whether there are any physical systems that can compute functions that cannot be efficiently computed by quantum computers.\n",
        "\n",
        "* ***One possible resolution to the contradiction is to say that the qECTT only applies to physical systems that are consistent with the laws of physics***. If there is a physical system that can compute a function that cannot be efficiently computed by a quantum Turing machine, then it must be a system that violates the laws of physics.\n",
        "\n",
        "* ***Another possible resolution to the contradiction is to say that the qECTT only applies to physical systems that are efficient***. If there is a physical system that can compute a function that cannot be efficiently computed by a quantum Turing machine, then it must be a system that is very inefficient.\n",
        "\n",
        "> Ultimately, the question of whether the qECTT is valid or not is an open one. It is a very important question, as it has implications for the limits of what is computable in the physical world."
      ],
      "metadata": {
        "id": "zltUffjrn2_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chomsky-Hierarchy and Automata Theory**\n",
        "\n",
        "* [Formal language](https://en.m.wikipedia.org/wiki/Formal_language) consists of words from letters from an alphabet and are well-formed acc. to specific set of rules\n",
        "* Automata theory and [Chomsky hierarchy](https://en.m.wikipedia.org/wiki/Chomsky_hierarchy) are used to **classify the complexity of languages**.\n",
        "* [Automata theory](https://en.m.wikipedia.org/wiki/Automata_theory) is study of abstract machines and automata, and computational problems that can be solved\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1651.png)\n",
        "\n",
        "\n",
        "**Beyond Turing Machines**: [Undecidable function](https://en.m.wikipedia.org/wiki/Undecidable_problem) with [List of undecidable problems](https://en.m.wikipedia.org/wiki/List_of_undecidable_problems)\n",
        "\n",
        "  * Undecidable functions can be used to show that certain languages are not in the Chomsky hierarchy. For example, the language of all strings that encode a Turing machine that halts on its own input is not context-sensitive. This is because if there were a context-sensitive grammar for this language, then we could use it to solve the halting problem by simply constructing a Turing machine from the grammar.\n",
        "\n",
        "  * In general, undecidable functions can be used to show that certain problems are not solvable by any algorithm. This is a powerful tool for understanding the limits of computation.\n",
        "\n",
        "\n",
        "**0. Type:** [Recursively enumerable](https://en.m.wikipedia.org/wiki/Recursively_enumerable_language) grammar = [Turing Machine](https://en.m.wikipedia.org/wiki/Turing_machine)\n",
        "\n",
        "  * Unlimited RAM, Recursively and enumerable are unrestricted, most general grammar and automata and allow general computation, the Turing machines = machines that can remember an unlimited number of states. See also: [Nondeterministic Turing machine](https://en.m.wikipedia.org/wiki/Nondeterministic_Turing_machine) and [Probabilistic_Turing_machine](https://en.m.wikipedia.org/wiki/Probabilistic_Turing_machine). Also: [Quantum Turing Machine (QTM](https://en.m.wikipedia.org/wiki/Quantum_Turing_machine), [quantum circuit](https://en.m.wikipedia.org/wiki/Quantum_circuit) is computationally equivalent. QTM can be related to classical and probabilistic Turing machines with [transition (Stochastic_matrix) matrices](https://en.m.wikipedia.org/wiki/Stochastic_matrix).\n",
        "\n",
        "    *  A quantum Turing machine (QTM) with postselection was defined by Scott Aaronson, who showed that class of polynomial time on such a machine (PostBQP) is equal to classical complexity class PP.\n",
        "    * A way of understanding the Quantum Turing machine (QTM) is that it generalizes the classical Turing machine (TM) in the same way that the quantum finite automaton (QFA) generalizes the deterministic finite automaton (DFA). In essence, the internal states of a classical TM are replaced by pure or mixed states in a Hilbert space; the transition function is replaced by a collection of unitary matrices that map the Hilbert space to itself.\n",
        "\n",
        "  * **Type a**: [Recursively enumerable function](https://en.m.wikipedia.org/wiki/Recursively_enumerable_language): [Busy beaver](https://en.m.wikipedia.org/wiki/Busy_beaver) (does not terminate)for some arguments you put into the function they will stop and give an answer and for others they will go on forever)\n",
        "\n",
        "  * **Type b**: [Recursive](https://en.m.wikipedia.org/wiki/Recursive_language) language and [Recursive functions](https://en.m.wikipedia.org/wiki/General_recursive_function): is [Ackermann function](https://en.m.wikipedia.org/wiki/Ackermann_function) (terminates). Not every total recursive function is a primitive recursive function—the most famous example is the [Ackermannfunktion](https://de.m.wikipedia.org/wiki/Ackermannfunktion): extrem schnell wachsende Funktion, mit deren Hilfe in der theoretischen Informatik Grenzen von Computer- und Berechnungsmodellen aufgezeigt werden können. Die [Sudanfunktion](https://de.wikipedia.org/wiki/Sudanfunktion) ist eine rekursive berechenbare Funktion, die total μ-rekursiv, **jedoch nicht primitiv rekursiv** ist, was sie mit der bekannteren Ackermannfunktion gemeinsam hat.\n",
        "\n",
        "  * **Type c**: [Primitive Recursive function](https://en.m.wikipedia.org/wiki/Primitive_recursive_function), incl. every other program that isn’t recursive, like something going through a Sequence, for loop and nested for loops. 1926 vermutete David Hilbert, dass jede [berechenbare](https://de.m.wikipedia.org/wiki/Berechenbarkeit) Funktion [primitiv-rekursiv](https://de.m.wikipedia.org/wiki/Primitiv-rekursive_Funktion) sei, siehe auch [Berechenbarkeitstheorie](https://de.m.wikipedia.org/wiki/Berechenbarkeitstheorie): **lässt sich jede durch einen Computer berechenbare Funktion aus einigen wenigen, sehr einfachen Regeln zusammensetzen und die Dauer der Berechnung im Voraus abschätzen?**. Ackermann und Sudan haben das widerlegt. Die Sudanfunktion und die Ackermannfunktion waren so die ersten veröffentlichten, nicht primitiv rekursiven Funktionen. ps: [Enumeration algorithm](https://en.m.wikipedia.org/wiki/Enumeration_algorithm). [Recursive function](https://en.m.wikipedia.org/wiki/Recursion_(computer_science)), also [Computable function](https://en.m.wikipedia.org/wiki/Computable_function), calls itself again to repeat code. An [iterative function](https://en.m.wikipedia.org/wiki/Iteration) repeatedly executes set of statements (code) without overhead of function calls and stack memory (simpler, faster).\n",
        "\n",
        "\n",
        "**1. Type:** [Context-sensitive](https://en.m.wikipedia.org/wiki/Context-sensitive_language) grammars =  [Linear bounded automata](https://en.m.wikipedia.org/wiki/Linear_bounded_automaton)\n",
        "\n",
        "  * Limited RAM needed, but you can predict how much RAM (turing machines with predictable and finite amount of RAM). Linear bounded automata can remember a stack of states and a counter. Context-sensitive languages are used to model the set of all strings that are syntactically correct in a programming language.\n",
        "\n",
        "  * On Turing machines the tape has unbounded length (unlimited tape). An LBA can access only a finite portion of the tape by the read/write head. **This makes an LBA a more accurate model of a real-world computer than a Turing machine.** A linear bounded automaton is a [nondeterministic Turing machine](https://en.m.wikipedia.org/wiki/Nondeterministic_Turing_machine)\n",
        "\n",
        "**2. Type:** [Context-free](https://en.m.wikipedia.org/wiki/Context-free_grammar) grammars = [Pushdown Automata](https://en.m.wikipedia.org/wiki/Pushdown_automaton)\n",
        "  \n",
        "  * No RAM needed, for example for parsing. Pushdown automata can recognize context-free languages (e.g. set of all balanced parentheses) because pushdown automata have a stack, which they can use to store information about the input string. This allows them to keep track of the context of the input string, which is necessary for recognizing context-free languages.\n",
        "\n",
        "**3. Type:** [Regular](https://en.m.wikipedia.org/wiki/Regular_language) grammars = [Finite State Machine](https://en.m.wikipedia.org/wiki/Finite-state_machine)\n",
        "\n",
        "  * Finite state automata: machines that can only remember a finite number of states. Regular languages are used to model simple patterns, such as the set of all strings of even length.\n",
        "\n",
        "  * z.B. [Deterministic Finite Automaton](https://en.m.wikipedia.org/wiki/Deterministic_finite_automaton) and [Quantum Finite Automaton](https://en.m.wikipedia.org/wiki/Quantum_finite_automaton). Finite automata: pattern recognition, regular expressions.\n",
        "\n",
        "\n",
        "*Exkurs: Combinational Logic vs Sequential logic (Automata theory): [Sequential logic](https://en.m.wikipedia.org/wiki/Sequential_logic): type of logic circuit whose output depends on present value of its input signals and on sequence of past inputs (history). Sequential logic has state (memory). Sequential logic is used to store the state of the automaton, which is necessary to track the current position of the tape head and the symbols that have been read (more complex, because requires a larger number of memory elements, and the logic to update the state of the automaton can be more complicated). The sequential logic is implemented using flip-flops, which are memory elements that can store a single bit of information. [Combinational Logic](https://en.m.wikipedia.org/wiki/Combinational_logic): output is a function of only the present input. Combinational logic does not have a state (memory). Combinatorial logic is used to determine the next state of the automaton and the output symbol to be written to the tape, based on the current state, the input symbol, and the contents of the tape. Combinational logic is used in computer circuits to perform Boolean algebra on input signals and on stored data. The ALU is constructed using combinational logic, also half adders, full adders, half subtractors, full subtractors, multiplexers, demultiplexers, encoders and decoders, AND, OR, and NOT. The combinational logic is typically much simpler than the sequential logic. This is because the next state of the automaton and the output symbol to be written to the tape can be determined by a relatively small number of input variables.*\n",
        "\n"
      ],
      "metadata": {
        "id": "3cQ2ILoRRcAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beyond Automata: Computational irreducibility and graphs**\n",
        "\n",
        "https://www.spektrum.de/news/stephen-wolfram-sucht-nach-der-weltformel-der-physik/2203229\n",
        "\n",
        "Die Zoologie der Graphensubstitutionsregeln ist um Klassen schwieriger als die der zellulären Automaten. Vor allem sieht man einer Regel im Allgemeinen nicht an, welche Graphen sie auf die Dauer produzieren wird. Von Ausnahmen abgesehen gibt es keine andere Möglichkeit, das herauszufinden, als die Regel ihre Arbeit machen zu lassen, ein Phänomen, das Wolfram »rechnerische Irreduzibilität« (computational irreducibility) nennt. Das berüchtigte deterministische Chaos (das Verhalten des Systems ist bis in alle Zukunft vorherbestimmt, aber unvorhersagbar) ist die Regel und das, wovon die ganze Physik handelt (Systeme, deren zukünftiges Verhalten man vorhersagen kann), die Ausnahme: einsame Inseln der Reduzibilität im großen Ozean des Chaos."
      ],
      "metadata": {
        "id": "uPH-C3qz4l7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why are most decision problems uncomputable?**  \n",
        "* [Turing and the Halting problem (computerphile)](https://youtu.be/macM_MtS_w4)\n",
        "* Decision problems answer is binary (chess, tetris, halting problem, negative weight cycle detection)\n",
        "* Decision problems are as hard as optimisation problems\n",
        "* Why are most decision problems uncomputable. Proof with theory from [MIT Lecture 23: Computational Complexity](https://youtu.be/moPtwq_cVH8):\n",
        "    * Define a progam: space of all possible programs ≈ you can think of it as binary strings (reduced). You can also think of numbers represented as binary strings ≈ natural number element N\n",
        "    * Define a Decision problem: function that maps inputs to yes (1) or no (0). Input ≈ is a binary string element of N (natural numbers). It‘s a function from N to 0/1. **Every infinite string of bits represents a decision problem.** Output is infite! **A program is a fintie string of bits.**\n",
        "    * You can write down a table of all answers: **a decision problem is an infite string of bits:** =110001010100010111. A program is a finite string of bits.** So they are different.\n",
        "    * One way to see the difference is to add a decimal point: .110001010100010111 - now this infinite string of bits in the output of a decision problem is a real number between 0 and 1 (written in binary). Any real number can be represented by an infinite string of bits.\n",
        "    * A decision problem is element of R, meanwhile a program is element of N (set of all integers).\n",
        "    * But R >> N. R (uncountably infinite) >> N (countable infinite), <font color=\"red\">**there are way more problems than there are programs to solve them**</font>, almost every problem unsolvable by any program.\n"
      ],
      "metadata": {
        "id": "2lw0UapfQk5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Non-computable numbers**\n",
        "* [Large_numbers](https://en.m.wikipedia.org/wiki/Large_numbers), [Names of large numbers](https://en.m.wikipedia.org/wiki/Names_of_large_numbers), [Liste besonderer Zahlen](https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1631.png)\n",
        "\n",
        "* [Constructible numbers](https://en.m.wikipedia.org/wiki/Constructible_number)\n",
        "* [Algebraic numbers](https://en.m.wikipedia.org/wiki/Algebraic_number)\n",
        "* [Transcendental numbers](https://en.m.wikipedia.org/wiki/Transcendental_number)\n",
        "* [Computable numbers](https://en.m.wikipedia.org/wiki/Computable_number)\n",
        "* **Non-Computable numbers**\n",
        "  * [Chaitin constant](https://en.m.wikipedia.org/wiki/Chaitin%27s_constant)\n",
        "  * [Die meisten reellen Zahlen kennen wir nicht](https://www.spektrum.de/kolumne/die-meisten-reellen-zahlen-sind-nicht-berechenbar/2133762) (gehorchen nicht einmal einer Rechenvorschrift)\n",
        "* Special cross-section: [Normal numbers](https://en.m.wikipedia.org/wiki/Normal_number)\n",
        "  * [Champernowne’s constant](https://en.m.wikipedia.org/wiki/Champernowne_constant) (whole numbers) - normal and transendental\n",
        "  * [Copeland-Erdös-number](https://de.m.wikipedia.org/wiki/Copeland-Erdős-Zahl) (primes)\n",
        "\n",
        "Source: [All the Numbers - Numberphile](https://www.youtube.com/watch?v=5TkIe60y2GI&t=458s)\n",
        "\n",
        "**Empty section: normal and Non-Computable numbers (we have no examples)**: We have no examples. But proofs have shown: this is the greatest amount of numbers: <font color=\"blue\">**most numbers are normal and most numbers are uncomputable**</font>. So this section should be full, but we have no example.\n"
      ],
      "metadata": {
        "id": "1f0R8e-ts52g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"Blue\">*Physical Boundaries of Computation*"
      ],
      "metadata": {
        "id": "hlzi59BP8aqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Physical Boundaries of Computation** ([Limits of Computation](https://en.m.wikipedia.org/wiki/Limits_of_computation))\n",
        "\n",
        "> Video: [Quantum Computing](https://youtu.be/gsxeKg41yMw?si=-B5mx5y0jhQGLUpR)\n",
        "\n",
        "*Limits from **Physics** (thermodynamics, quantum mechanics), from **Computer Science** (Computability theory, Complexity theory, Information science) and from **Mathematics** (number theory). Limits are derived from simple questions:*\n",
        "\n",
        "1. <font color=\"blue\">What is the max limit of information capacity?</font> (per given volume and in the observable universe) - [Beckenstein Bound](https://en.m.wikipedia.org/wiki/Bekenstein_bound) 10$^{43}$ bits per kg\n",
        "2. <font color=\"blue\">What is the min amount of heat per erased bit that is dissipated when information is destroyed?</font> - $10^{-21}$ x 2.9 Joule: One limit is the [Landauer’s principle](https://en.m.wikipedia.org/wiki/Landauer%27s_principle), which states that the minimum energy required to perform a single logical operation is equal to the Boltzmann constant times the temperature of the system. This limit implies that the maximum number of computations that can be performed in a given amount of time is limited by the total energy available.\n",
        "2. <font color=\"blue\">What is the max physical speed limit of computation?</font> -\n",
        "  * [Bremermann's Limit](https://en.m.wikipedia.org/wiki/Bremermann%27s_limit) 10$^{50}$ operations per second (speed of light and Planck length > fundamental limit to how fast information can be processed, even in a perfect computer). Bremermann's Limit states that the maximum computational speed of a self-contained system is limited by the speed of light and the Planck length. This limit implies that there is a fundamental limit to how fast information can be processed, even in a perfect computer. **1.3563925 × 10^50 bits per second per kilogram**\n",
        "  * The [Margolus-Levitin theorem](https://de.m.wikipedia.org/wiki/Margolus-Levitin-Theorem) states that the maximum computational speed per unit of energy is limited by the Planck constant. This limit implies that there is a fundamental limit to how much computation can be performed with a given amount of energy. (Margolus-Levitin-Theorem, the processing rate of all forms of computation (including quantum computation) cannot be higher than about **6 × 10^33 operations per second per joule of energy.** 'Black Hole Computers' from Seth Lloyd: Margolus-Levitin theorem: operations take place in the minimum time allowed. The theorem says that the time it takes to flip a bit, t, depends on the amount of energy you apply, E. The more energy you apply, the shorter the time can be. Mathematically, the rule is t h/4E, where h is Planck's constant.\n",
        "3. <font color=\"blue\">What is the physical limit of computation (max number of operations) of the universe in its entire lifetime?</font> - 10$^{229}$ operations (if all matter in the observable would turn into a black hole computer). Calculated combining Margolus-Levitin theorem and Landauer’s principle.\n",
        "4. <font color=\"blue\">What is the max physical time limit of computation?</font> - [Poincare Recurrence time](https://en.m.wikipedia.org/wiki/Poincaré_recurrence_theorem) (universe resets itself, e.g. before finishing Graham's number)\n",
        "5. <font color=\"blue\">What is the max physical resolution limit of the universe?</font> - 10$^{185}$ Planck volume. Anything bigger than this number cannot be explained in physical terms.\n",
        "6. <font color=\"blue\">Can I find another me of myself in this universe?</font> - No! $10^{{10}^{70}}$ is the number of all possible quantum states a person can occupy (roughly a 1 m$^3$ of space) - the same arrangement of atoms that makes you. If you would walk $10^{{10}^{70}}$ meters, you would start to see repetitions of yourself. But the max number of protons of the observable universe is only $10^{80}$ [Eddington number](https://en.m.wikipedia.org/wiki/Eddington_number) and the max resolution is only 10$^{185}$ (Planck volume).\n",
        "\n",
        "Papers: [Ultimate physical limits to computation (Seth Lloyd, 2000)](https://arxiv.org/abs/quant-ph/9908043), [Computational capacity of the universe (Seth Lloyd, 2001)](https://arxiv.org/abs/quant-ph/0110141), [NP-complete Problems and Physical Reality (Scott Aaronson, 2005)](https://arxiv.org/abs/quant-ph/0502072), [Estimation of the information contained in the visible matter of the universe](https://arxiv.org/abs/2112.04473), [The Cost of computation](https://arxiv.org/abs/1905.05669), [Article: From 1,000,000 to Graham’s Number](https://waitbutwhy.com/2014/11/1000000-grahams-number.html), Video: [The Boundary of Computation](https://www.youtube.com/watch?v=kmAc1nDizu0&list=WL&index=7), Video: [The Limits of Computation](https://www.youtube.com/watch?v=ZDfaXJRtOoM), [Amdahl's law](https://en.m.wikipedia.org/wiki/Amdahl%27s_law) predict theoretical speedup when using multiple processors"
      ],
      "metadata": {
        "id": "c1x-N4GYUfEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selection of physical limits relevant to computation:**\n",
        "\n",
        "> See also [Orders of magnitude (numbers)](https://en.wikipedia.org/wiki/Orders_of_magnitude_(numbers))\n",
        "\n",
        "* <font color=\"blue\">$10^{-44}$ seconds max speed to write a single symbol - [Planck time](https://de.m.wikipedia.org/wiki/Planck-Zeit). Important to know the limits how to compute large numbers: </font>\n",
        "\n",
        "  * if you can write a single symbol only at this time max, then it would take you more time to write down **Graham's number** than one period within **Poincare's recurrence time** = when the universe would reset itself before you finished writing Graham's number\n",
        "\n",
        "  * to write down the **Googol number** you need $10^{56}$ seconds = $10^{48}$ years - Have we enough time to write that down? - Depends on nature of dark energy. In $10^{48}$ years we have the **era of Black Hole dominance** (all matter disappeared and we live in a supermassive black hole or is all matter is unimaginable far apart) [Video](https://www.youtube.com/watch?v=X3l0fPHZja8)\n",
        "\n",
        "* <font color=\"blue\">$10^{-21}$ x 2.9 Joule: [Landauer’s principle](https://en.m.wikipedia.org/wiki/Landauer%27s_principle). Min amount of heat per erased bit that is dissipated when information is destroyed [Source](https://physicsworld.com/a/wiping-data-will-cost-you-energy). See also [Entropy in thermodynamics and information theory](https://en.m.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory)</font>\n",
        "\n",
        "\n",
        "* **1 bit is in quantum communication complexity the communication complexity of the OR problem, even if the parties share entanglement**. It shows that there are some problems that cannot be solved using less communication than their classical counterparts, even if the parties share entanglement. The Quantum OR lemma was first published in 1999 by David Deutsch and Artur Ekert. It has been used to prove lower bounds on the communication complexity of a variety of other problems, including the AND problem, the XOR problem, and the equality problem. **The Quantum OR lemma is a powerful tool for studying the power of quantum communication. It has helped to shed light on the fundamental limits of what can be achieved with quantum communication.**\n",
        "\n",
        "* $10^{10}$ factor: Almost all information in black holes 10^10 factor more information stored in BH than in anything else, 25:40 Leonard Susskind https://youtu.be/CQAcLW6qdQY?si=947fUVP4UqhEP3qU\n",
        "\n",
        "* $10^{11}$ years (100 billion) other galaxies than Andromeda and Milky way are outside of the visible universe\n",
        "\n",
        "* $10^{12}$ years (1 trillion) the galaxies will be depleted of gas clouds, and thus the formation of new stars will be impossible (all hydrogen in stars cores are exhausted)\n",
        "\n",
        "\n",
        "* $12^{12}$ years (1,2 trillion) all stars in the universe will have exhausted (no more stars). [Source](https://www.youtube.com/watch?v=dsWfGzxjs0w), remaining only white dwarfs, neutron stars and black holes.\n",
        "\n",
        "* $10^{14}$ years: In 100 trillion years (100 x 10^12) the last star will die and the universe is ony dominated by dark matter [Source](https://www.youtube.com/watch?v=4Stzj2_Rlo4). Nothing more interesting will happen for next decillions (10^60), vigintillions (10^120) and googols of years dominated by dark matter. Then you have a visible universe of 36 bn light years diameter and is a black hole inside out. But quantum fluctuations at the event horizon will fill the inside up with new particles.\n",
        "\n",
        "* <font color=\"blue\">$10^{15}$ (1 quadrillion) – all planets are detached from their solar systems\n",
        "\n",
        "* <font color=\"blue\">$10^{16}$ bit is the total amount of information a typical human observer can possibly absorb during his lifetime [arXiv:0910.1589](https://arxiv.org/abs/0910.1589)</font>\n",
        "\n",
        "\n",
        "* <font color=\"blue\">$10^{17}$ (100 quadrillion) – The number of seconds since the Big Bang (important to estimate max limit of computation since beginning of universe)\n",
        "\n",
        "* <font color=\"blue\">$10^{18}$ max operations per second in human brain (and floating point operations, flops in 2020)</font>\n",
        "\n",
        "* $10^{18}$ max operations per second in human brain (and floating point operations, flops in 2020)\n",
        "\n",
        "* $10^{23}$ x 6,022 - Avogadro Zahl. Multipliziert mit mol$^{-1}$ ergibt die [Avogadro Konstante](https://de.m.wikipedia.org/wiki/Avogadro-Konstante) - die Anzahl der Teilchen, die in einem Mol eines Stoffes enthalten sind (602 Trilliarden Teilchen pro Mol). For computation: see [Fredkin gate paper](https://cqi.inf.usi.ch/qic/82_Fredkin.pdf), section 5: An isolated physical system consisting of a substantial amount (say, 1 g) of matter possesses an enormous number of degrees of freedom, or modes, of the order of magnitude of Avogadro's number (..).\n",
        "\n",
        "* <font color=\"blue\">$10^{30}$ years: all remnant of stars will have fallen in the central galactic supermassive black hole\n",
        "\n",
        "* <font color=\"blue\">$10^{30}$ x 3 years: if also Protons decay and all atmic nuclei are decayed. Black hole era of the universe starts. Black holes are the only celestial objects in the entire universe.\n",
        "\n",
        "* <font color=\"blue\">$10^{33}$ x 6 : [Margolus-Levitin-Theorem](https://en.m.wikipedia.org/wiki/Margolus–Levitin_theorem), the processing rate of all forms of computation (including quantum computation) cannot be higher than about 6 × 10^33 operations per second per joule of energy. The maximum computational speed per unit of energy is limited by the Planck constant.\n",
        "\n",
        "* <font color=\"blue\">$10^{43}$ = [Beckenstein bound](https://en.m.wikipedia.org/wiki/Bekenstein_bound): maximal amount of information that can be contained within a given volume. Genauer: Using mass–energy equivalence, the informational limit may be reformulated as follows where M is the mass (in kg), and R is the radius (in meter) of the system ${\\displaystyle H\\leq {\\frac {2\\pi cRM}{\\hbar \\ln 2}}\\approx 2.5769082\\times 10^{43}\\ {\\frac {\\text{bit}}{{\\text{kg}}\\cdot {\\text{m}}}}\\cdot M\\cdot R,}$. You can also consider this as $10^{43}$ Hertz: a computer that operated at these operations per second would use so much energy that it would simply collapse to a black hole - Moore’s law: ultimate limits imposed by quantum gravity (min 55:00 [video](https://www.youtube.com/watch?v=uX5t8EivCaM&t=2492s))</font>\n",
        "  * It implies that the information of a physical system, or the information necessary to perfectly describe that system, must be finite if the region of space and the energy are finite.\n",
        "  * In computer science this implies that non-finite models such as [Turing machines](https://en.m.wikipedia.org/wiki/Turing_machine) are not realizable as finite devices.\n",
        "\n",
        "* <font color=\"blue\">$10^{50}$ bits per second per kilogram x 1.3563925 = [Bremermann limits](https://en.m.wikipedia.org/wiki/Bremermann%27s_limit) - Max rate of data processing by isolatad material system. Physical limit of computation. It is derived from Einstein's mass-energy equivalency and the Heisenberg uncertainty principle, and is c2/h.</font>\n",
        "\n",
        "  * A computer at the size of earth and operating at Bremermann's limits could do $10^{74}$ computations per second. It could break a 128 bit cryptographic key in less than $10^{-36}$ of a second and a 256 bit key in 2 minutes. However for 512 bit key working this Bremermann's computational speed limit would ta e$10^{72}$ years. [Source at min 2:30](https://www.youtube.com/watch?v=ZDfaXJRtOoM). Das Universum wird aber vrs “nur” noch max Trillionen (10^12) oder Quadrillionen (10^15) Jahre existieren.\n",
        "\n",
        "* <font color=\"blue\">$10^{50}$ x 5 operations per second. The \"ultimate laptop\": 1 kg black hole with width = 3 x $10^{27}$ meters it would perform 5 x $10^{50}$ operations /sec, but has only a lifetime = $10^{-22}$ sec (due to Hawking radiation). Source: [Seth Lloyd: Black hole computer](https://www.scientificamerican.com/article/black-hole-computers-2007-04/). It's also the **final limit of computation is at Heisenberg uncertainty** at $10^{50}$ operations per second (flops, not clock speed) [Source](https://www.youtube.com/watch?v=jv2H9fp9dT8) even when accounting for a Black Hole</font>\n",
        "\n",
        "* * <font color=\"blue\">$10^{68}$ years: black hole of the size of the sun will have decayed due to Hawking radiation\n",
        "\n",
        "* $10^{80}$ [Eddington number](https://en.m.wikipedia.org/wiki/Eddington_number): Number of protons in the observable universe. genauer: 1.57 × $10^{79}$\n",
        "\n",
        "* <font color=\"blue\">$10^{80}$ x ∼6: number of bits of information stored in all the matter particles of the observable universe (where each particle in the observable universe contains 1.509 bits of information) [arxiv:0110141](https://arxiv.org/abs/quant-ph/0110141) and [arxiv:2112.04473](https://arxiv.org/abs/2112.04473)</font>\n",
        "\n",
        "* $10^{90}$: Fill the universe with grains of sand (.5mm in diameter)\n",
        "\n",
        "* <font color=\"blue\">$10^{90}$ - max number of bits the universe can have processed [arxiv:0110141](https://arxiv.org/abs/quant-ph/0110141), based on the amount of information the Universe can register and the number of elementary operations that it can have performed over its history</font>\n",
        "\n",
        "* <font color=\"blue\">$10^{90}$ operations per second: if all matter in the observable would turned into a black hole computer it could perform $10^{90}$ operations per second but it has a life time of 2.8 x $10^{139}$ seconds before hawking radiation cause it to evaporate. In that time it could perform 2.8 x $10^{229}$ operations. Source: [Seth Lloyd: Black hole computer](https://www.scientificamerican.com/article/black-hole-computers-2007-04/)</font>\n",
        "\n",
        "\n",
        "* $10^{106}$ - $10^{108}$ years it takes for Black holes to evaporate due to Hawking radiation. Source: [The Crazy Future If Protons Don't Decay](https://www.youtube.com/watch?v=5XuBIyGqE1w). The universe enters the dark era.All physical objects have decayed into subatomic particles. If protons don't decay, look at $10^{1500}$ years.\n",
        "\n",
        "\n",
        "* $10^{100}$: [Googol](https://en.wikipedia.org/wiki/Googol), from American mathematician Edward Kasner in 1938\n",
        "\n",
        "* $10^{113}$  The number of hydrogen atoms it would take to pack the universe full of them.\n",
        "\n",
        "* $10^{116}$ x 1.57: possible ways of arrangements a 6x6x6 rubik's cube can have\n",
        "\n",
        "* <font color=\"blue\">$10^{120}$ - max number of operations the universe can have performed [arxiv:0110141](https://arxiv.org/abs/quant-ph/0110141), based on the amount of information the Universe can register and the number of elementary operations that it can have performed over its history</font>\n",
        "\n",
        "* <font color=\"blue\">$10^{120}$ max number of bits of data that could be computes in the amount of time that has elapsed so far in the universe based on the maximum entropy of the universe, the speed of light and the minimum time taken to move information across the Planck length. Anything that requires more than this amount of data cannot have been computed yet [Source, min 2](https://youtu.be/nJObMJLweCs) - Limit on the computational power of the universe (and why Laplace demon cannot exist)</font>\n",
        "\n",
        "* $10^{122}$ The number of protons you could fit in the universe (incl, non-observable universe??) -> for this I need a source\n",
        "\n",
        "* $10^{183}$ number of Planck length' ($10^{-35}$ meters) cubes in the observable universe\n",
        "\n",
        "* <font color=\"blue\">$10^{185}$ x 4: Without being able to go smaller, we’ve reached the largest number where the physical world can be visualize: take a Planck length (10^(-35) meter) and fill it with universe - largest number where the physical world can be used to visualize it. **Anything bigger than this number cannot be explained in physical terms.**</font>\n",
        "\n",
        "* $10^{200}$ pieces to cut a circle and fill with it a square - from circle to square: [Tarski's circle-squaring problem](https://en.m.wikipedia.org/wiki/Tarski%27s_circle-squaring_problem). Achtung: From square to circle: [Squaring the circle](https://en.m.wikipedia.org/wiki/Squaring_the_circle) has been proven to be impossible.\n",
        "\n",
        "* <font color=\"blue\">$10^{229}$ x 2.8: max number of operations if all matter in the observable would turned into a black hole computer Source: [Seth Lloyd: Black hole computer](https://www.scientificamerican.com/article/black-hole-computers-2007-04/)</font>\n",
        "\n",
        "\n",
        "* $10^{495}$ x 6.8: possible proteins the cells in a body can create from the 375 amino acids (but it just unfolds a small subset of all these combinations). Video: [The Most Complex Language in the World](https://www.youtube.com/watch?v=TYPFenJQciw&list=WL&index=13)\n",
        "\n",
        "\n",
        "* $10^{1500}$ years: In case proton decay is not possible, all the particles will have fused together to form iron-56 isotopes, and thus creating something called 'Iron stars'.\n",
        "\n",
        "* 10$^{272.000}$ number of possible universes in string theory [video](https://youtu.be/k_TEoUF12Yk)\n",
        "\n",
        "* $10^{{10}^{7}}$ bits: the amount of information that goes into the Black Hole to build it up for a black hole of the mass of our sun (contradiction because we have only Black Holes observables: mass, electric charge, momentum (no hairs theorem from Bekenstein) = very few bits). Black holes have an enormous number of microstates (hidden configurations - translates to an enormous entropy). Entropy formula from Jacob Bekenstein: $S_{B H}=\\frac{A}{4 L_P^2}=\\frac{c^3 A}{4 G \\hbar}$. Source: [Are Black Holes actually fuzzballs?](https://youtu.be/351JCOvKcYw?si=CtlKQtLkoSx0hRBc)\n",
        "\n",
        "* $10^{{10}^{16}}$: Number of universes that a human observer may distinguish = number of different configurations a typical human brain can have [arXiv:0910.1589](https://arxiv.org/abs/0910.1589)\n",
        "\n",
        "* $10^{{10}^{29}}$ times the size of the observable universe = the chance to meeting an exact copy of yourself (=same constellation of particles) in an infinite universe. Source: [The paradox of an infinite universe](https://youtu.be/isdLel273rQ?si=zvg0qvbC5pAHtssg)\n",
        "\n",
        "* $10^{10^{40}}$ where [Merten‘s Conjecture](https://en.m.wikipedia.org/wiki/Mertens_conjecture) can be disproven\n",
        "\n",
        "* $10^{10^{40}}$ years: all particles will have collapes into black holes (which would evaporate almost instantaneously due to timescale). Now the universe is an absolute void with nothing inside of it. Now, the Universe has reached its final energy state, its maximum entropic value.\n",
        "\n",
        "* $10^{10^{40}}$ years: [Boltzmann brain](https://de.m.wikipedia.org/wiki/Boltzmann-Gehirn), a self-aware entity that appeared due to random quantum fluctuations.\"\n",
        "\n",
        "* <font color=\"blue\">$10^{{10}^{70}}$ - All possible quantum states a person can occupy (roughly a meter ^3 of space). If you would walk $10^{{10}^{70}}$ meters, you would start to see repetitions of yourself.[Source](https://youtu.be/8GEebx72-qs?t=287)</font>\n",
        "\n",
        "* $10^{{10}^{76}}$ years: iron stars will slowly collapse into black holes. Source: [The Crazy Future If Protons Don't Decay](https://www.youtube.com/watch?v=5XuBIyGqE1w)\n",
        "\n",
        "* $10^{{10}^{100}}$ or $10^{Googol}$ - [Googolplex](https://en.m.wikipedia.org/wiki/Googolplex): you cannot write this number in the (observable) universe, since there is not enough space: it has more digits than there are atoms in the observable universe. <font color=\"blue\">**From here start numbers that are incomprehensible. If i walk a googoplex far (the universe is as big as a googolplex meters across), I would see repetitions of myself** (aka same arrangements of atoms like me).</font> if i go further i would see the entire observable universe repeating (size of the universe: $(10^{26})^3$ meters. <font color=\"blue\">**This means there is probably not another me in the universe**.</font> but if you live in a universe that is a googolplex across, then you would by chance run into the same arrangements of atoms that matches you. [numberphile](https://www.youtube.com/watch?v=8GEebx72-qs&t=0s)\n",
        "\n",
        "* $10^{{10}^{{10}^{7}}}$: Number of possible universes, but assume that we are not limited as observers to distinguish more universes. $10^{{10}^{16}}$ is number of universes that a human observer may distinguish [arXiv:0910.1589](https://arxiv.org/abs/0910.1589)\n",
        "\n",
        "* $10^{{10}^{{10}^{34}}}$ bzw. $e^{{e}^{{e}^{79}}}$: untere Schranke der [Skewes Number](https://de.m.wikipedia.org/wiki/Skewes-Zahl)\n",
        "\n",
        "\n",
        "* $10^{{10}^{{10}^{56}}}$ years: random quantum fluctuations and quantum tunneling is predicted to give birth to another universe [Source](https://www.youtube.com/watch?v=dsWfGzxjs0w)\n",
        "\n",
        "* $10^{{10}^{{10}^{76}}}$ years: next checkpoint for a new universe to give birth via a big bang (if protons don't decay), if we are looking at the probability of quantum fluctuations. Source: [The Crazy Future If Protons Don't Decay](https://www.youtube.com/watch?v=5XuBIyGqE1w)\n",
        "\n",
        "* $10^{{10}^{{10}^{100}}}$ - Googolplexian. Number 10 raised to the power of one Googolplex. 10^1000000..... - line is probably trillions of light years long\n",
        "\n",
        "* $10^{{10}^{{10}^{1000}}}$: untere Schranke der [Skewes Number](https://de.m.wikipedia.org/wiki/Skewes-Zahl) unter Nicht-Annahme der Riemann Hypothese. Unter Annahme der Riemann Hypothese: $10^{{10}^{{10}^{961}}}$ bzw. $e^{{e}^{{e}^{7703}}}$\n",
        "\n",
        "* <font color=\"blue\">$10^{{10}^{{10}^{{10}^{{10}^{1.1}}}}}$ **Poincare recurrence time**: Time until a system resets itself (particle of gas go back in the corner, because phase space is finite). you can apply that to the whole universe. Largest finite time ever been calculated by a physicist in a published paper: [arxiv:9411193](https://arxiv.org/abs/hep-th/9411193). He calculated [Poincare recurrence time](https://en.m.wikipedia.org/wiki/Poincar%C3%A9_recurrence_theorem) (for a certain type of universe with a certain cosmological number). [Video](https://www.youtube.com/watch?v=1GCf29FPM4k).</font>\n"
      ],
      "metadata": {
        "id": "6tjmyhKJX2FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Here we cannot simply write down the numbers anymore $\\downarrow \\downarrow$**\n",
        "\n",
        "> See also: Video: [Numbers too big to imagine](https://youtu.be/u1x_FJZX6Vw?si=4bqa_0_nbdd4DquX)\n",
        "\n",
        "**g64 number [Grahams Zahl](https://de.m.wikipedia.org/wiki/Grahams_Zahl)**\n",
        "\n",
        "* **Graham's number is the biggest number used constructively** = used in a mathematical proof, rather than just being a theoretical concept. There is a specific mathematical problem that can be solved by using Graham's number.\n",
        "\n",
        "* $g_{64}$ [Grahams Zahl](https://de.m.wikipedia.org/wiki/Grahams_Zahl) $g_{64} = 3 \\uparrow \\underbrace{\\uparrow \\ldots \\uparrow}_{g_{63}} \\uparrow 3 = 3(G63\\uparrow)3$ = the biggest number used constructively. If you count that number in your head, the total amount of information would turn your head into a black hole. Graham's number is **not** transfinite. But it is an astronomically large number that is defined using a recursive formula. The formula is so large that it cannot be written out in standard mathematical notation.\n",
        "\n",
        "> $3 \\uparrow \\uparrow 3=3 \\uparrow 3 \\uparrow 3 = 3^{3^3}=7,625,597,484,987$\n",
        "\n",
        "\n",
        "> $3 \\uparrow \\uparrow \\uparrow 3=3 \\uparrow \\uparrow 3 \\uparrow \\uparrow 3$ = $3^{3^{3^{(..)^3}}}$ - This means the number 3 is 7,625,597,484,987 (7.6 trillion) times exponentiated!\n",
        "\n",
        "> $g_1 = 3 \\uparrow \\uparrow \\uparrow \\uparrow 3 = 3 \\uparrow \\uparrow \\uparrow 3 \\uparrow \\uparrow \\uparrow 3$\n",
        "\n",
        "> $g_2 = 3 \\uparrow \\underbrace{\\uparrow \\ldots \\uparrow}_{g_1} \\uparrow 3  = 3(G1\\uparrow)3$\n",
        "\n",
        "> $g_3 = 3 \\uparrow \\underbrace{\\uparrow \\ldots \\uparrow}_{g_2} \\uparrow 3 $\n",
        "\n",
        "> ....\n",
        "\n",
        "> $g_{64} = 3 \\uparrow \\underbrace{\\uparrow \\ldots \\uparrow}_{g_{63}} \\uparrow 3 = 3(G63\\uparrow)3$ = [Grahams Zahl](https://de.m.wikipedia.org/wiki/Grahams_Zahl) the biggest number used constructively\n",
        "\n",
        "> Siehe: [Hyperoperation](https://en.m.wikipedia.org/wiki/Hyperoperation) und [Knuth‘s Up Arrow Notation](https://en.m.wikipedia.org/wiki/Knuth%27s_up-arrow_notation)\n",
        "\n",
        "* you can't say how many digits it has. If you imagine Graham's number in your head, then your head would collaps into a black hole. It is estimated that the observable universe is not large enough to contain an ordinary digital representation of Graham's number.\n",
        "\n",
        "* Upper bound: used in combinatorics, graph theory, as an upper bound for coloring graphs that are linked to higher dimensional cubes: for [\"Ramsey number for hypergraphs\"](https://en.m.wikipedia.org/wiki/Ramsey%27s_theorem) (This problem is concerned with the number of edges that need to be added to a hypergraph in order to make it so that any two distinct subsets of the hypergraph have either the same number of edges between them, or the opposite number of edges between them.). Graham's number is the max number for this to be true (and 6 or 11 is the lowest)\n",
        "\n",
        "* https://waitbutwhy.com/2014/11/1000000-grahams-number.html\n",
        "\n",
        "\n",
        "**tree(3)** [Kruskal's tree theorem](https://en.m.wikipedia.org/wiki/Kruskal%27s_tree_theorem)\n",
        "\n",
        "* tree theorems are larger numbers for mathematical proofs than graham's number\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Kruskal%27s_tree_theorem\n",
        "\n",
        "https://towardsdatascience.com/how-big-is-the-number-tree-3-61b901a29a2c\n",
        "\n",
        "- Inf-embedabble\n",
        "- Ackermann numbers as lower bound\n",
        "- Proof theory: kruskals tree theory\n",
        "    - Well quasi ordering\n",
        "    - Poincare recurrence, universe will reset itself due to entropy (limited number of steps) or proof doesn‘t fit in our universe\n",
        "    - Ordinals, trans-finite arithmetic\n",
        "- Video 1: https://www.youtube.com/watch?v=3P6DWAwwViU\n",
        "- Video 2: https://www.youtube.com/watch?v=IihcNa9YAPk\n",
        "\n",
        "\n",
        "**Loaders number** $D^{(5)}(99)$\n",
        "\n",
        "* It's the fifth iteration of a certain function  D  on the value 99. TREE(3) is indistinguishable from zero compared to Loader's number. Code was limited to 512 characters, otherwise it would get bigger\n",
        "\n",
        "* https://googology.fandom.com/wiki/Loader%27s_number\n",
        "\n",
        "* The interesting point is that  D , the function, is a very fast-growing function with a particularly compact representation, and it’s valuable to compare its growth rate with that of other computable functions.\n",
        "\n",
        "* I’m not sure much is known about the place of  D  in the fast-growing hierarchy. The Googology entry seems to indicate that, for example, finite promise games produce a faster-growing computable function.\n",
        "\n",
        "* **TREE(3) is indistinguishable from zero compared to Loader's number[1]**, which basically takes every bit pattern up to some n and expresses this as a program in the Calculus of Constructions. This system is a bit weaker than being Turing complete, but the programs do always terminate (this makes the number computable compared with, say, the Busy Beaver number which does a similar thing with Turing complete programs).\n",
        "It also has the geek cred of being represented by an obfuscated C program (the unobfuscated verson is also available[2]).\n",
        "[1] http://googology.wikia.com/wiki/Loader%27s_number\n",
        "[2] https://github.com/rcls/busy\n",
        "\n",
        "\n",
        "**SCG(13) and SSCG(3)** [Friedman's SSCG function](https://en.m.wikipedia.org/wiki/Friedman's_SSCG_function)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Friedman%27s_SSCG_function\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Friedman's_SSCG_function\n",
        "\n",
        "* SCG(13) and SSCG(3) - [Friedman's SSCG function](https://en.m.wikipedia.org/wiki/Friedman's_SSCG_function). SCG(13) is computable, whereas Rayo's number is uncomputable. Rayo's number >> SCG(13).\n",
        "\n",
        "* The SSCG sequence begins slower than SCG, SSCG(0) = 2, SSCG(1) = 5, but then grows rapidly.\n",
        "\n",
        "* This number is known for being able to surpass TREE(3) and is defined with SSCG. It is praised for being much larger than TREE(TREE(TREE(TREE… (over TREE(3) times …TREE(TREE(TREE(TREE(3)…).\n",
        "\n",
        "* SSCG(3) is much larger than both TREE(3)\n",
        "\n",
        "* SCG(13) is computable, whereas Rayo's number is uncomputable. From here we can already say Rayo's number >> SCG(13). For large numbers beyond g(64), we can only use boundaries to define them, hence the uncertainty.\n",
        "\n",
        "* Adam P. Goucher claims there is no qualitative difference between the asymptotic growth rates of SSCG and SCG. He writes \"It's clear that SCG(n) ≥ SSCG(n), but I can also prove SSCG(4n + 3) ≥ SCG(n).\"\n",
        "\n",
        "\n",
        "* SCG(13) and SSCG(3) - [Friedman's SSCG function](https://en.m.wikipedia.org/wiki/Friedman's_SSCG_function). SCG(13) is computable, whereas Rayo's number is uncomputable. Rayo's number >> SCG(13).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bek6CfuxWq1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**$\\uparrow \\uparrow$ Here separates the computable from the uncomputable $\\downarrow \\downarrow$**\n",
        "\n",
        "**Busy Beaver - Separating the computable from the uncomputable**\n",
        "\n",
        "* [Busy Beaver](https://en.m.wikipedia.org/wiki/Busy_beaver): endliche, aber im Allgemeinen nicht berechenbare Funktion. The Busy Beaver function Σ(n) grows faster than any computable function. Hence, it is not computable; only a few values are known. some mathematical system loose the ability to prove its values beyond a point. ps: Rayo's number is much bigger than the Beaver - but the Busy Beaver isn't relevant to Rayo. Rayo is fundamentally about truth rather than provability, and so the relevant \"logical obstacle\" is Tarski's undefinability theorem rather than the incomputability of the halting problem\n",
        "\n",
        "* Difference between (in)finite value and (in)finite instruction: ZFC can't pin down the precise value of even BB(7918). However, ZFC can define the Busy Beaver function itself, in less than, say, a billion symbols.\n",
        "\n",
        "* Die [Fleißiger-Biber-Funktion bzw. Radó-Funktion Σ](https://de.m.wikipedia.org/wiki/Flei%C3%9Figer_Biber) ist in der theoretischen Informatik ein Standardbeispiel für eine **endliche, aber im Allgemeinen nicht berechenbare Funktion**\n",
        "\n",
        "* **Open Questions**: Open Questions BB($10^{100}$) > Tree($10^{100}$) ??.\n",
        "\n",
        "* Die Rado-Funktion Σ ist nicht Turingmaschinen-berechenbar. Obwohl es sehr viele nicht-berechenbare Funktionen gibt - sogar mehr als berechenbare -, muss man sich doch Außergewöhnliches einfallen lassen, um eine nicht-berechenbare Funktionen möglichst konkret zu beschreiben. [Source](https://www.inf-schule.de/algorithmen/berechenbarkeit/grenzenderberechenbarkeit/station_fleissigebiber)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Busy_beaver\n",
        "\n",
        "* $\\sum(n)$ : The Busy Beaver function\n",
        "  * Consider all n-state Turing machines.\n",
        "  * Run each on a tape of all 0's.\n",
        "  * Of all machines that halted,\n",
        "    * $\\sum(n)$ = max count of 1's\n",
        "\n",
        "* **$\\Sigma(n)$ is not a computable function!** (for all n, but for a specific n it's computable). **The Busy Beaver function Σ(n) grows faster than any computable function. Hence, it is not computable; only a few values are known**\n",
        "\n",
        "* Some mathematical system loose the ability to prove its values beyond a point. A computable function requires a finite number of steps to produce the output. Siehe [Berechenbarkeit](https://de.m.wikipedia.org/wiki/Berechenbarkeit)\n",
        "\n",
        "* this function grows faster than any computable function! For: If f(n) : $\\mathbb{N}$ => $\\mathbb{N}$ for any computable function, then there exists $n_f$ such that: $f(n) < \\Sigma(n)$ for all $n$ ≥ $n_f$\n",
        "\n",
        "* **Means: The busy beaver function beyond some value of $\\mathbb{N}$ will grow faster than $\\mathbb{N}$**\n",
        "\n",
        "* There are true statements like: \"$\\Sigma$(1000) = k\" that cannot be proved in our normal axiomatic systems. Mathematics looses the ability to make claims about these numbers.\n",
        "\n",
        "* the nineteenth busy beaver number is greater than grahams number\n",
        "\n",
        "* [Aaronson: Busy Beaver Updates: Now Even Busier](https://scottaaronson.blog/?p=6673)\n",
        "\n",
        "* Source Video: [The Boundary of Computation](\n",
        "https://www.youtube.com/watch?v=kmAc1nDizu0):\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1627.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1628.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1629.png)\n",
        "\n",
        "\n",
        "**Rayo($10^{100}$)** [Rayo's number](https://en.m.wikipedia.org/wiki/Rayo).\n",
        "\n",
        "* Rayo's number is the smallest number that cannot be expressed anymore. even grahams number would look like a 0 next to rayos number. [Video](https://www.youtube.com/watch?v=X3l0fPHZja8).\n",
        "\n",
        "* Rayo's number is defined as the smallest number that is larger than any number that can be named by an expression in the language of first-order set theory with a googol symbols or less. This means that Rayo's number is much larger than a Gogol.\n",
        "\n",
        "* Compared to Busy beaver: Rayo is way way way way WAY bigger than the Beaver - but the Busy Beaver just isn't relevant to Rayo.) Rayo is fundamentally about truth rather than provability, and so the relevant \"logical obstacle\" is Tarski's undefinability theorem rather than the incomputability of the halting problem [Source](https://math.stackexchange.com/questions/3910468/a-confusion-regarding-rayos-number-and-busy-beaver-function)\n",
        "\n",
        "**Chaitin's constant / Chaitin's Omega number**\n",
        "\n",
        "* Chaitin’s omega number [4], **which is interpretable as the halting probability of a universal computer**, can be “computed in the limit” (without any computable radius of convergence) by a finite-size pro- gram in infinite time and with infinite space. Just as for the difference being the absence of any computable radius of convergence the first digits of omega are well known [5], yet omega has been proved to be algorithmically incompressible and thus random (doi:10.1016/j.amc.2005.09.076)\n",
        "\n",
        "* Are chaitin‘s constant and chaitin‘s omega number the same?\n",
        "\n",
        "  * Yes, Chaitin's constant and Chaitin's Omega number refer to the same concept in algorithmic information theory. Here's a breakdown of why:\n",
        "\n",
        "  * **Chaitin's Omega Number (Ω)**\n",
        "    * **Definition:** The Chaitin Omega number (Ω) is a real number representing the probability that a randomly constructed self-delimiting program will halt when run on a universal Turing machine.\n",
        "    * **Significance:**  It demonstrates the fundamental limits of formal systems and the existence of uncomputable truths.\n",
        "  * **Key Properties of Chaitin's Omega Number**\n",
        "    * **Uncomputable:** There's no algorithm that can compute all the digits of Ω.\n",
        "    * **Transcendental:** It's not the root of any polynomial equation with integer coefficients.\n",
        "    * **Normal:** Its digits are statistically random, meaning any finite sequence of digits appears with the expected frequency.\n",
        "  * **Why the Names Are Used Interchangeably**\n",
        "    * The terms \"Chaitin's constant\" and \"Chaitin's Omega number\" are used synonymously because:\n",
        "    * **Gregory Chaitin:** Both terms refer to the work of mathematician and computer scientist Gregory Chaitin.\n",
        "    *  **Halting Probability:**  They represent the same fundamental idea of the probability of a random program halting.\n",
        "\n",
        "* [Chaitin's constant](https://en.m.wikipedia.org/wiki/Chaitin%27s_constant) or halting probability is a real number that, informally speaking, represents the probability that a randomly constructed program will halt.\n",
        "\n",
        "* ***Chaitin's number is transfinite. It is an uncomputable number that measures the randomness of a computer program***. The higher the Chaitin's number, the more random the program is. Chaitin's number is uncomputable (because it is impossible to determine the halting probability of a Turing machine.)\n",
        "\n",
        "* Each halting probability is a normal and transcendental real number that **is not computable**, which means that there is no algorithm to compute its digits. Each halting probability is [Martin-Löf random](https://en.m.wikipedia.org/wiki/Algorithmically_random_sequence), meaning there is not even any algorithm which can reliably guess its digits.\n",
        "\n",
        "**Fish number 7**\n",
        "\n",
        "* Fish number 7 belongs to the family of the fish numbers, which were defined by a Japanese googologist. **Fish numbers were often classified as “above Rayo's number”**, and they're pretty incomprehensible. [More info](https://googology.fandom.com/wiki/Fish_number_7).\n",
        "\n",
        "**Merten‘s Conjecture** $10^{10^{40}}$\n",
        "\n",
        "* [Merten‘s Conjecture](https://en.m.wikipedia.org/wiki/Mertens_conjecture) is disproven: at some point the sum will surpass the square root of the number. [Video](https://youtu.be/uvMGZb0Suyc).\n",
        "* If it had been true, that it would proof the Riemann hypothesis\n",
        "* But if we knew it, we cannot write it down, because **we would need more atoms than exist in the universe to write it down**\n",
        "    * PRIZE FOR SOLVING RIEMANN HYPOTHESIS $ 10€\n",
        "    * STARS IN THE UNIVERSE 10^22\n",
        "    * ATOMS IN THE UNIVERSE 10^80\n",
        "    * MERTENS CONJECTURE FAILS $10^{10^{40}}$\n",
        "* We have no way to describe the first that it happens, but we know it exists\n",
        "\n"
      ],
      "metadata": {
        "id": "DRzsUXULYvpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infinity $\\infty$ and Transfinite numbers**\n",
        "\n",
        "*überabzählbare und abzählbare Unendlichkeit*\n",
        "\n",
        "* Tatsächlich gibt es nicht nur eine Unendlichkeit, sondern gleich unendlich viele. So unterscheidet man beispielsweise zwischen der Unendlichkeit der natürlichen Zahlen und der reellen Zahlen: Während man natürliche Zahlen wie 1, 2, 3, … lückenlos auflisten kann, ist das mit reellen Zahlen unmöglich. Eine Aufzählung existiert nicht, selbst wenn die Liste unendlich lang ist. Denn zwischen zwei reellen Zahlen findet man immer eine weitere, die dazwischensteckt. Auch wenn man vermuten würde, dass das bei Bruchzahlen ebenso ist, lassen diese sich dennoch wie die natürlichen Zahlen aufzählen, zum Beispiel, indem man sie nach der Größe ihres Nenners ordnet: 1⁄1, 1⁄2, 1⁄3, 2⁄3, 1⁄4, 3⁄4, 1⁄5, 2⁄5, 3⁄5, 4⁄5, … Damit haben wir zwei Kategorien von Unendlichkeiten ausgemacht: abzählbare Unendlichkeit, wie die der natürlichen oder rationalen Zahlen, und überabzählbare Unendlichkeit, wie die der reellen Zahlen. Und auch wenn beides unvorstellbare Größen sind, ist ihr Unterschied erheblich.\n",
        "\n",
        "https://www.spektrum.de/kolumne/masstheorie-eine-wahrscheinlichkeit-von-null-heisst-nicht-unmoeglich/2092452\n",
        "\n",
        "*In mathematics, transfinite numbers are numbers that are greater than all finite numbers. They are often used in set theory and topology.*\n",
        "\n",
        "There are two main types of transfinite numbers: ordinal numbers and cardinal numbers. Ordinal numbers are used to order sets, while cardinal numbers are used to count the number of elements in a set.\n",
        "\n",
        "Some examples of transfinite numbers are:\n",
        "\n",
        "* **$\\omega$** (omega): The smallest transfinite ordinal number. It is the order type of the natural numbers.\n",
        "* **$\\omega^2$** (omega squared): The number of all possible orderings of the natural numbers.\n",
        "* **$\\aleph_0$** (aleph null): The smallest transfinite cardinal number. It is the cardinality of the set of natural numbers.\n",
        "* **$\\aleph_1$** (aleph one): The cardinality of the set of all countable ordinal numbers.\n",
        "* **$\\aleph_2$** (aleph two): The cardinality of the set of all uncountable ordinal numbers.\n",
        "\n",
        "Transfinite numbers can be used to solve problems that are impossible to solve with finite numbers. For example, the problem of determining whether a given set is countable or uncountable can only be solved using transfinite numbers.\n",
        "\n",
        "The study of transfinite numbers is a complex and fascinating area of mathematics. It has applications in many different areas of mathematics, including set theory, topology, and logic.\n",
        "\n",
        "**what is the relationship between transfinite numbers and infinity?**\n",
        "\n",
        "Transfinite numbers are numbers that are greater than all finite numbers. Infinity is a concept that refers to something that is boundless or endless.\n",
        "\n",
        "The relationship between transfinite numbers and infinity is that transfinite numbers are a way of quantifying infinity. In other words, transfinite numbers allow us to talk about different sizes of infinity.\n",
        "\n",
        "For example, the natural numbers (1, 2, 3, ...) are finite numbers. The set of all natural numbers is countable, which means that it can be put into a one-to-one correspondence with the set of all positive integers.\n",
        "\n",
        "The set of all real numbers is uncountable, which means that it cannot be put into a one-to-one correspondence with the set of all positive integers. This means that the set of all real numbers is larger than the set of all natural numbers.\n",
        "\n",
        "Transfinite numbers can be used to quantify the size of the set of all real numbers. The smallest transfinite cardinal number is $\\aleph_0$, which is the cardinality of the set of all natural numbers. The cardinality of the set of all real numbers is $\\aleph_1$, which is larger than $\\aleph_0$.\n",
        "\n",
        "There are many different transfinite numbers, and they can be used to quantify the size of different sets. The study of transfinite numbers is a complex and fascinating area of mathematics.\n",
        "\n",
        "Here are some other ways to think about the relationship between transfinite numbers and infinity:\n",
        "\n",
        "* Transfinite numbers are a way of making infinity more precise.\n",
        "* Transfinite numbers allow us to compare different infinities.\n",
        "* Transfinite numbers can be used to solve problems that are impossible to solve with finite numbers.\n",
        "\n",
        "The study of transfinite numbers is a complex and fascinating area of mathematics. It has applications in many different areas of mathematics, including set theory, topology, and logic.\n",
        "\n",
        "http://peter-ripota.de/mathe/ordinalzahlen-einfach-weiterzaehlen/von-%CF%89-zu-%CE%B50-von-klein-omega-zu-epsilon-null/\n",
        "\n",
        "\n",
        "Infinity is not a number, it's an idea, a concept. Hence you cannot equal something to infinity.\n",
        "\n",
        "Aleph Null\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Aleph-Funktion\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/amp-aleph-0-aleph-null/443\n",
        "\n",
        "[Counting beyond infinity](https://youtu.be/SrU9YDoXE88)\n",
        "\n",
        "https://www.quantamagazine.org/how-many-numbers-exist-infinity-proof-moves-math-closer-to-an-answer-20210715/\n",
        "\n",
        "* Kontinuumshypothese (Cantor) kann man nicht beweisen oder widerlegen (Gödel). Die Mathematik kann funktionieren, wenn man sowohl davon ausgeht, dass die Kontiuumshypothese gilt, als auch, dass sie nicht gilt.\n",
        "\n",
        "* term of size (how many?) is: **Cardinality**\n",
        "\n",
        "* Rules for comparing cardinalities: injection, subjection, bijection\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_319.png)\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite\n",
        "\n",
        "* [How Big are All Infinities Combined? (Cantor's Paradox) | Infinite Series](https://www.youtube.com/watch?v=TbeA1rhV0D0&list=WL&index=12&t=86s)\n",
        "\n",
        "**Smallest Sizes of Infinity**\n",
        "\n",
        "* intuition is often misleading in mathematics!\n",
        "\n",
        "* There are infinitely many sizes of infinity\n",
        "\n",
        "* Smallest infinity: natural numbers (counting numbers). Here: even numbers and natural numbers are the same size!\n",
        "\n",
        "  * Natural numbers: $\\aleph$ \"Aleph-naught\" (the least infinity cardinality)\n",
        "\n",
        "* We need to find a way to tell which infinity is bigger without counting them.\n",
        "\n",
        "* You use something called Bijection: if you can pair up two sets, they are the same size\n",
        "\n",
        "\t* even numbers and natural numbers are the same size, because there is a bijection between the two sets! Each natural number is [aired with 2 times itself: 1 with 2, 2 with 4, 4 with 6 etc. Damit sind alles odd numbers out, but the set didn't get smaller\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_318.png)\n",
        "\n",
        "\t* the natural numbers are also the same size as the integers: Integers include all natural numbers + all the negative whole numbers. We can pair them up exactly, so they must be the same size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_317.png)\n",
        "\n",
        "* Above the natural numbers are the real numbers in terms of infinity size. After that the real numbers.\n",
        "\n",
        "\t* an interval on the real number line is also an infinity. And an interval on the real line between 0 and 5 has the same size as an interval between 0 and 10. They are ll as big as the real numbers!!\n",
        "\n",
        "\t* You can show that any interval is the same size as the entire real number line !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_316.png)\n",
        "\n",
        "* Cantor wondered if there is an infinity between the natural and the real numbers?\n",
        "\n",
        "\t* CONTINUUM HYPOTHESIS: there is no size of infinity between the natural numbers and the real numbers\n",
        "\n",
        "\t* Decades later, mathematicians found out that the Continuum hypothesis is independent of the Zermela-Fraenkel set theory with choice (ZFC) - the Continuum hypothesis can not be proved or disproved using the standard rules of mathematics\n",
        "\n",
        "* So in one model of the tower of infinities the real numbers sit directly above the natural numbers\n",
        "\n",
        "* But in other models there are many infinities in between\n",
        "\n",
        "* The rules of maths don't say that one tower is correct and the other wrong.\n",
        "\n",
        "> So it seems that mathematics seems to be surprisingly agnostics with regards to which hierarchy of infinities is correct\n",
        "\n",
        "* How does the cardinality of a set A compares to that of its power set (=set of all subsets of A including itself)\n",
        "\n",
        "* The cardinality of the power set of A is strictly greater than the cardinality of its original set A, even when it's infinite"
      ],
      "metadata": {
        "id": "6AKEytfrRpL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eine Wahrscheinlichkeit von null heißt nicht unmöglich**\n",
        "\n",
        "*$\\hookrightarrow$ 'Improbable but not impossible'*\n",
        "\n",
        "Wenn man also die Wahrscheinlichkeit berechnet, welche Art von reeller Zahl man antrifft, wenn man zufällig eine zieht, erhält man ein eindeutiges Ergebnis: In 100 Prozent der Fälle ist diese Zahl nicht berechenbar. Das heißt aber nicht, dass man keine andere Zahl ziehen kann – bei unendlichen Ereignismengen bedeutet eine Wahrscheinlichkeit von null nicht unmöglich. Das ist umso erstaunlicher, als dass nicht allzu viele nicht berechenbare Zahlen bekannt sind. [Source](https://www.spektrum.de/kolumne/die-meisten-reellen-zahlen-sind-nicht-berechenbar/2133762)\n",
        "\n",
        "https://www.spektrum.de/kolumne/masstheorie-eine-wahrscheinlichkeit-von-null-heisst-nicht-unmoeglich/2092452\n",
        "\n",
        "Dieses Problem wird auch **Dartscheiben-Paradoxon** genannt. Gelöst wird es durch die Erkenntnis, dass eine Wahrscheinlichkeit von null nicht zwangsläufig bedeutet, dass ein Ereignis niemals eintritt – sondern nur, dass es »fast sicher« nicht eintritt.\n",
        "\n",
        "Woher weiß man nun, ob man es mit einem tatsächlich unmöglichen Ereignis (etwa, eine Acht mit einem W6-Würfel zu würfeln) oder einem fast unmöglichen Ereignis (einen bestimmten Punkt auf der Dartscheibe treffen) zu tun hat? Das hängt von der Anzahl der möglichen Ereignisse ab, dem so genannten Ereignisraum: Ist der Ereignisraum endlich (bei einem Würfel besteht er aus sechs Elementen: den sechs Seiten, auf denen er liegen bleiben kann), dann bedeutet eine Wahrscheinlichkeit von null, dass das betrachtete Ereignis niemals eintreten wird. Betrachtet man hingegen einen unendlich großen Ereignisraum (wie die möglichen Treffpunkte auf einer Dartscheibe), dann bedeutet eine Wahrscheinlichkeit von null nicht zwangsläufig, dass das Ereignis nicht eintritt – sondern nur, dass es sehr unwahrscheinlich ist."
      ],
      "metadata": {
        "id": "_YFS2_Vt1_JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"Blue\">*Beyond Quantum Computing (Hypercomputation)*"
      ],
      "metadata": {
        "id": "2rAAm4JiTDN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental Limitations to Quantum Computation**\n",
        "\n",
        "* https://www.birs.ca/events/2024/5-day-workshops/24w5259\n",
        "* https://www.birs.ca/events/2024/5-day-workshops/24w5259/schedule\n",
        "\n",
        "* Yifan Jia: Hay from the haystack: explicit examples of exponential quantum circuit complexity: The vast majority of quantum states and unitaries have circuit complexity exponential in the number of qubits. In a similar vein, most of them also have exponential minimum description length, which makes it difficult to pinpoint examples of exponential complexity. In this work, we construct examples of constant description length but exponential circuit complexity. We provide infinite families such that each element requires an exponential number of two-qubit gates to be generated exactly from a product and where the same is true for the approximate generation of the vast majority of elements in the family. The results are based on sets of large transcendence degree and discussed for tensor networks, diagonal unitaries, and maximally coherent states.\n",
        "\n",
        "\n",
        "* Jens Eisert: Fundamental limits to quantum computation: Quantum computers promise superior computational power over classical computers for some structured problems. While this insight is not new, only in recent years, steps have been taken to actually build intermediate-sized quantum devices, creating an exciting state of affairs. For some paradigmatic problems, there is some evidence that quantum computers may outperform classical devices [1]. For practically motivated problems in machine learning [2, 3] and in optimization [4], fault tolerant quantum computers indeed perform better than classical ones. While this may be promising, actual systems to date are relatively small and noisy. The main part of the talk is concerned with identifying *limitations* to quantum computing in this realm. We discuss notions of learnability of output distributions of short quantum circuits - as they can be seen as parts of variational quantum algorithms - and find that a single T-gate renders learning them hard [5]. We identify exponentially tighter bounds on limitations of quantum error mitigation [6]. We finally discuss the impact of non-unital noise on quantum computing, with quite unexpected results [7]. We end on the note that while fault tolerant quantum computers offer substantial computational benefits, the race is still open for near-term quantum devices. [1] Computational advantage of quantum random sampling, D. Hangleiter, J. Eisert, Rev. Mod. Phys. 95, 035001 (2023). [2] A super-polynomial quantum-classical separation for density modelling, N. Pirnay, R. Sweke, J. Eisert, J.-P. Seifert, Phys. Rev. A 107, 042416 (2023). [3] Towards provably efficient quantum algorithms for large-scale machine-learning models, J. Liu, M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert, L. Jiang, Nature Comm. 15, 434 (2024). [4] An in-principle super-polynomial quantum advantage for approximating combinatorial optimization problems via computational learning theory, N. Pirnay, V. Ulitzsch, F. Wilde, J. Eisert, J.-P. Seifert, arXiv:2212.08678, Science Advances (2024). [5] A single T-gate makes distribution learning hard, M. Hinsche, M. Ioannou, A. Nietner, J. Haferkamp, Y. Quek, D. Hangleiter, J.-P. Seifert, J. Eisert, R. Sweke, Phys. Rev. Lett. 130, 240602 (2023). [6] Exponentially tighter bounds on limitations of quantum error mitigation, Y. Quek, D. Stilck França, S. Khatri, J. Jakob Meyer, J. Eisert, arXiv:2210.11505, Nature Physics (2024). [7] Non-unital noise, friend of foe, A. A. Mele, A. Angrisani, A Ghosh, A. Khatri, J. Eisert, Y. Quek, D. Stilck Franca, in preparation (2024).\n"
      ],
      "metadata": {
        "id": "szGHIoNv-vxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Hypercomputation?**\n",
        "* Beyond Turing Machines: Hypercomputation refers to theoretical models of computation that can solve problems that are uncomputable by a standard Turing machine. Examples include machines that can decide the halting problem or compute functions with infinite information content.\n",
        "* Mostly Theoretical: No universally accepted, physically realizable model of hypercomputation exists. They remain predominantly concepts within theoretical computer science.\n",
        "* <font color=\"blue\">**Solving the Halting problem would be an example of hypercomputation, but it would not be covered by the qECTT!**</font>\n",
        "  * **The Halting Problem:**  The Halting problem asks whether it's possible to create a universal algorithm that can definitively determine if any given computer program will eventually stop running or continue indefinitely.  Alan Turing famously proved this problem is undecidable by a standard Turing machine.\n",
        "  * **Why it's Hypercomputation:** <font color=\"blue\">If you could solve the Halting problem, you'd possess a machine (often envisioned as an \"oracle\") capable of doing something no Turing machine (classical or quantum) can achieve</font>. This computational superpower fits most definitions of hypercomputation.\n",
        "  * **qECTT Violation:** Since the qECTT asserts that a quantum computer is no more powerful than a theoretical quantum Turing machine, and a quantum Turing machine still can't solve the Halting problem, this type of  hypercomputation directly clashes with the qECTT.\n",
        "  * **Important Note:** While conceptually clear, hypercomputation is tricky because it's largely defined by what it isn't.  Building a hypercomputer in the real world might be fundamentally impossible.\n",
        "\n",
        "> It is well known that certain extensions to the Turing machine such as additional memory tapes or non-determinism do not allow it to compute any new functions. **Indeed this is often cited as a reason to accept the recursive functions as being the natural class of computable functions**. However, it has been shown that by adding the ability to perform additional primitive functions, to acquire input from the outside world, to perform infinite precision operations on real numbers, or to perform an infinite number of computational steps can all increase the power of the Turing machine and allow it to compute non-recursive functions. doi:10.1016/j.amc.2005.09.076\n",
        "\n",
        "**Forms of [Hypercomputation](https://en.m.wikipedia.org/wiki/Hypercomputation)**\n",
        "\n",
        "* **Hypercomputers and Quantum Gravity Computers**:\n",
        "  * see description in seth lloyds: black hole computers\n",
        "  * [Black holes as tools for quantum computing by advanced extraterrestrial civilizations](https://arxiv.org/abs/2301.09575)\n",
        "  * [Quantum Gravity Computers](https://arxiv.org/abs/quant-ph/0701019)\n",
        "  * [A simple quantum system that describes a black hole](https://arxiv.org/abs/2303.11534)\n",
        "  * [Quantum computers could simulate a black hole in the next decade](https://www.newscientist.com/article/2370695-quantum-computers-could-simulate-a-black-hole-in-the-next-decade/)\n",
        "  * [Black Hole as a model of computation](https://www.sciencedirect.com/science/article/pii/S2211379719304036)\n",
        "\n",
        "* **Infinity Machines**\n",
        "  * **Infinity Machines**: with geometrically squeezed time cycles, such as the ones envisioned by Weyl [7] and others [8-18], are they physically feasible? Motivated by recent proposals to utilize quantum computation for trespassing the Turing barrier [19-22], these accelerating Turing machines have been intensively discussed [23] among other forms of hypercomputation [24-26].\n",
        "\n",
        "* **Quantum Gravity Computers / Black Holes as Computers**\n",
        "  * **Quantum Gravity Computer with CTCs**: no one knows how to combine quantum mechanics with general theory of relativity. Quantum gravity: breakdowns of causality itself, if closed timelike curves CTCs (i.e., [time machines to the past](https://www.pbs.org/wgbh/nova/article/do-time-travelers-tweet/) ) are possible. David Deutsch, John Watrous and Aaronson: “A time machine is definitely the sort of thing that might let us tackle problems too hard even for a quantum computer.”\n",
        "  * With closed timelike curves, then under fairly mild assumptions, one could “force” Nature to solve hard combinatorial problems, just to keep the universe’s history consistent (i.e., to prevent things like the grandfather paradox from arising). Notably, the problems you could solve that way include the NP-complete problems : a class that includes hundreds of problems of practical importance (airline scheduling, chip design, etc.), and that’s believed to scale exponentially in time even for quantum computers.\n",
        "  * According to a 1992 paper (Hogarth, Mark L. (1992). [\"Does general relativity allow an observer to view an eternity in a finite time?\"](https://link.springer.com/article/10.1007/BF00682813)), a computer operating in a [Malament–Hogarth spacetime](https://en.m.wikipedia.org/wiki/Malament%E2%80%93Hogarth_spacetime) or in **orbit around a rotating black hole could theoretically perform non-Turing computations for an observer inside the black hole**.\n",
        "  * Access to a CTC may allow the rapid solution to PSPACE-complete problems, a complexity class which, while Turing-decidable, is generally considered computationally intractable. - [Computability Theory of Closed Timelike Curves](https://www.scottaaronson.com/papers/ctchalt.pdf). There are spacetimes in which the [CTC (closed timelike curves)](https://en.m.wikipedia.org/wiki/Closed_timelike_curve) region can be used for relativistic hypercomputation. [Closed Timelike Curves Make Quantum and Classical Computing Equivalent](https://arxiv.org/abs/0808.2669). While quantum formulations of CTCs have been proposed,[5][6] a strong challenge to them is their ability to freely create entanglement,[7] which quantum theory predicts is impossible. If Deutsch's prescription holds, the existence of these CTCs implies also equivalence of quantum and classical computation (both in PSPACE).[8] If Lloyd's prescription holds, quantum computations would be PP-complete. https://en.m.wikipedia.org/wiki/Closed_timelike_curve.\n",
        "  * **Achtung:** CTCs brauchen exotische Teilchen, um rückwärts in der zeit zu reisen (laut Hawking), und diesr exotischen Teilchen wurden noch nicht gefunden [Source](https://www.quora.com/If-two-black-holes-collide-does-the-matter-around-them-travel-back-in-time) - <font color=\"red\">So CTCs don't seem to exist, and with that no np-hard computation!</font>\n",
        "  * Videos: [Limits of computation](https://youtu.be/ZDfaXJRtOoM) - [Limits of computation](https://youtu.be/gV12PS19YL8) - [Physical limits of computation](https://youtu.be/ZVj93b0pa2o) - [What is the computational power of the universe](https://youtu.be/ROdv1v_YsAw) - [Is the universe a Turing machine?](https://youtu.be/VY6TzB_xH-k) (Lex Fridman: Lee Cronin vs Joscha Bach) - [The universe is not hypercomputational](https://youtu.be/VV_kArap5TM) (min 2). [Computing Limit](https://youtu.be/jv2H9fp9dT8). [Is There Anything Beyond Quantum Computing? ](https://www.pbs.org/wgbh/nova/article/is-there-anything-beyond-quantum-computing/) (Aaronson).\n",
        "\n",
        "* **Omega Machines**\n",
        "  * These are machines that can solve the halting problem, which is a problem that is known to be unsolvable by a Turing machine. (from chaitin's (omega) number):\n",
        "  * omega appears to have two features which are normally consid- ered contradictory: it is one of the most informative mathematical numbers imaginable, yet at the same time this information is so com- pressed that it cannot be deciphered. Thus omega appears to be totally structureless and random.\n",
        "  * In this sense, for omega, total information and total randomness seem to be “two sides of the same coin”. On a more pragmatic level, it seems impossible here to differen- tiate between order and chaos, or between knowledge and chance. **This gives a taste of what can be expected from any “hyper-computation” beyond universal computability** as defined by Turing. Source: [How to Acknowledge Hypercomputation?](https://content.wolfram.com/uploads/sites/13/2019/03/18-1-6.pdf)\n",
        "  * [Non-Turing Computers and Non-Turing Computability](https://www.jstor.org/stable/193018)\n",
        "  * Adamyan, Calude, Pavlov: Transcending the Limits of Turing Computability. Kieu: Computing the Noncomputable. Ord: The Many Forms of Hypercomputation. Davis: The Myth of Hypercomputation. Doria and Costa: Introduction to the Special Issue on Hypercomputation. Davis: Why There Is No Such Discipline as Hypercomputation\n",
        "  * Church–Kalmár–Kreisel–Turing theses theoretical concerning (necessary) limitations of future computers and of deductive sciences, in view of recent results of classical general relativity theory.  https://arxiv.org/abs/gr-qc/0104023\n",
        "\n"
      ],
      "metadata": {
        "id": "hojYWzd_AAMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Motivation: Beyond Limits of Quantum Computers - Scientific Questions**\n",
        "\n",
        "1. **Computational Complexity and Black Holes**: The ultimate computers? AdS/CFT and information paradox? Are Black Holes (Non-)Turing machines (describable and computable)? Are there Unknown laws of quantum gravity?\n",
        "2. **Computational Complexity and Universe**: Is it describable and computable (simulatable), or a non-Turing machine? Can we not simulate all problems (including quantum gravity) on standard QC's? And are there problems that really require computers that are more complex than QC?\n",
        "  * Where is actually the computational limit of (standard) quantum computers?  Do we really need more powerful computers?  What actually means more powerful than quantum computers? - Are there systems that are hard to simulate on standard, qubit-based quantum computers, then those systems themselves could be thought of as more powerful kinds of quantum computers, which solve at least one problem—the problem of simulating themselves —faster than is otherwise possible.\n",
        "  * Benefits of thining about Limits of quantum computing: Minimum eigenvalue gap of my hamiltonian (spectral gap decrease polynomial or exponentially as a function of the number of particles?). Eigenvalue gap can become exponentially small. **Limits of quantum computing have explanatory power: why spectral gaps behave the way they do?** Why they help to protect geometry of spacetime. Scott Aaronson: [Black Holes, Firewalls, and the Limits of Quantum Computers](https://www.youtube.com/watch?v=cstKRACrMQY&t=2446)\n",
        "  * Lloyd also postulates that the Universe can be fully simulated using a quantum computer; however, in the absence of a theory of quantum gravity, such a simulation is not yet possible. \"Particles not only collide, they compute.\" https://en.m.wikipedia.org/wiki/Programming_the_Universe\n",
        "  * **It from Qubit - Everything is computation (The universe as a computer)**\n",
        "    * But to a physicist, all physical systems are computers. Rocks, atom bombs and galaxies may not run Linux, but they, too, register and process information. Every electron, photon and other elementary particle stores bits of data, and every time two such particles interact, those bits are transformed. Physical existence and information content are inextricably linked. As physicist John A. Wheeler of Princeton University says, It from bit.\n",
        "    * What is the universe computing? Instead the universe is computing itself. Powered by Standard Model software, the universe computes quantum fields, chemicals, bacteria, human beings, stars and galaxies. As it computes, it maps out its own spacetime geometry to the ultimate precision allowed by the laws of physics. **Computation is existence.**\n",
        "    * **It from Qubit** from [Simons Collaboration on Quantum Fields, Gravity, and Information](https://web.stanford.edu/~phayden/simons/overview.pdf):\n",
        "      * Does spacetime emerge from entanglement?\n",
        "      * Can quantum computers simulate all physical phenomena?\n",
        "      * Do black holes have interiors? Does the universe exist outside our horizon?\n",
        "3. **Computational Complexity and Consciousness**: Is consciousness reducable to computation?\n",
        "\n"
      ],
      "metadata": {
        "id": "FQOMlaEP1y6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Case why Turing Machines are powerful enough**\n",
        "\n",
        "*No need for hypercomputers*\n",
        "\n",
        "**Type 1: Power of Standard Quantum Computers**\n",
        "\n",
        "* Refers to non-relativistic quantum mechanics quantum computers. Fun Fact: there is a line of research how to efficiently simulate quantum circuits on classical computers.\n",
        "\n",
        "* **Standard QCs can simulate all of quantum chemistry and atomic physics efficiently**: Could Nature allow more powerful kinds of quantum computers than the “usual” qubit-based kind? - Strong evidence that answer is “no” comes from work by Richard Feynman in the 1980s, and by Seth Lloyd and many others starting in the 1990s. They showed how to take a wide range of realistic quantum systems and simulate them using nothing but qubits. (..) it looks likely that a single device, a quantum computer, would in the future be able to simulate all of quantum chemistry and atomic physics efficiently.\n",
        "\n",
        "* **Standard QCs can simulate Quantum Field Theory efficiently**: Stephen Jordan, Keith Lee, and John Preskill gave the first detailed, efficient simulation of a “realistic” quantum field theory using a standard quantum computer (“adiabatic state preparation”).\n",
        "\n",
        "* **Standard QCs can simulate Quantum Gravity efficiently**: hint that a standard quantum computer could efficiently simulate even quantum-gravitational processes, like the formation and evaporation of black holes. Most notably, the AdS/CFT correspondence (assuming the translation is also computationally efficient, see more below under Black Hole Computing)\n",
        "  * [A simple quantum system that describes a black hole](https://arxiv.org/abs/2303.11534)\n",
        "  * [Quantum computers could simulate a black hole in the next decade](https://www.newscientist.com/article/2370695-quantum-computers-could-simulate-a-black-hole-in-the-next-decade/)\n",
        "\n",
        "**Type 2: Quantum Field Theory Computers**\n",
        "\n",
        "* Includes special relativity (quantum field theory).\n",
        "\n",
        "* Challenge: It’s not clear what we should program our quantum computer to simulate. Also, in most quantum field theories, even a vacuum is a complicated object.\n",
        "\n",
        "* A \"Quantum field theory computer” like from Michael Freedman, Alexei Kitaev, and Zhenghan Wang showed how to simulate a “toy” class of quantum field theories, called **topological quantum field theories** (TQFTs). What kinds of QFT can be simulated on standard QC:\n",
        "\n",
        "  * Simulations of QFT on QC to study chiral symmetry breaking, confinement, and deconfinement phase transition. Bigger QCs: simulate more complex QFTs to understand dark matter and dark energy. Potential to revolutionize our understanding of the fundamental forces of nature, but still in early stage\n",
        "  * **Lattice field theory**: simulate dynamics of field that is discretized onto lattice\n",
        "  * **Schwinger model** (simple QFT to study behavior of electrons in strong electric field, Google used 53-qubit QC in 2022, simulation reproduced known results from analytical calculations and provided new insights into behavior of Schwinger model)\n",
        "  * More QFTs simulatable: **Ising model, XY model, Heisenberg model, U(1) gauge theory, QCD (quantum chromo dynamics)**\n",
        "\n",
        "* But Stephen Jordan, Keith Lee, and John Preskill gave the first detailed, efficient simulation of a “realistic” quantum field theory using a standard quantum computer (“adiabatic state preparation”)\n",
        "\n",
        "**Is Nature actually np-hard? Are there really physical problems we cannot solve on standard QC's efficiently?**\n",
        "  * Penrose: speculated that quantum gravity is literally impossible to simulate using either an ordinary computer or a quantum computer, even with unlimited time and memory at your disposal. Penrose: is the brain a quantum gravitational computer? He wants the brain to be exploiting as yet unknown laws of quantum gravity. Which would be uncomputable.\n",
        "  * No, nature is not np-hard, makes mistakes also in protein folding: Nature can <u>not</u> solve all np problems (like soap bubbles is a myth). Scott aaronson: doing the soap experiment: you need to repeat the trial several times, it does not always find the best solution. it seems to find a local optimum mostly. Prions in protein: local optima! Scott Aaronson: [Black Holes, Firewalls, and the Limits of Quantum Computers](https://www.youtube.com/watch?v=cstKRACrMQY&t=2446). And it cannot scale to large number of nails. **In the case of soap bubbles, nature just seems to apply a really fast approximation which is accurate for small cases but breaks down at larger instances** (due to actual physical properties... i.e. it is physically impossible to build arbitrarily thin rods to dip into soapy water.) Many other NP-problems can be modeled as physical systems in which a lowest-energy-state would correspond to a solution to the combinatorial problem. You can find that \"DNA computing\" solves the clique problem, for instance. [Source](https://groups.google.com/g/comp.theory/c/11lY926-P7M?pli=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "KI9Gpnnj20st"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computational Complexity and Consciousness**\n",
        "\n",
        "* Penrose further speculates that human brain is sensitive to quantum gravity effects, and that this gives humans ability to solve problems that are fundamentally unsolvable by computers. However, no other expert in relevant fields agrees with arguments that lead Penrose to this provocative position. [Source from Aaronson](https://www.pbs.org/wgbh/nova/article/is-there-anything-beyond-quantum-computing/)\n",
        "* [Hard problem of consciousness](https://en.m.wikipedia.org/wiki/Hard_problem_of_consciousness) and [Pretty hard problem](https://forum.effectivealtruism.org/posts/Qiiiv9uJWLDptH2w6/the-pretty-hard-problem-of-consciousness)\n",
        "* [p Zombie (Stanford)](https://plato.stanford.edu/entries/zombies/) and [Philosophical_zombie](https://en.m.wikipedia.org/wiki/Philosophical_zombie)\n",
        "* [Integrated information theory](https://en.m.wikipedia.org/wiki/Integrated_information_theory) - Is consciousness reducable to computation?\n",
        "  * Aaaronson: [Why I Am Not An Integrated Information Theorist (or, The Unconscious Expander)](https://scottaaronson.blog/?p=1799)\n",
        "  * Tegmark has also tried to address the problem of the computational complexity behind the calculations. According to Max Tegmark “the integration measure proposed by IIT is computationally infeasible to evaluate for large systems, growing super-exponentially with the system’s information content.”\n",
        "  * As a result, Φ can only be approximated in general. However, different ways of approximating Φ provide radically different results.\n",
        "  * “Which physical states are associated with consciousness, and which are not?” This question is what Scott Aaronson (2014) has dubbed the term “Pretty Hard Problem” [Source](https://forum.effectivealtruism.org/posts/Qiiiv9uJWLDptH2w6/the-pretty-hard-problem-of-consciousness)\n",
        "  * thought experiments such as Mary the super-scientist, color spectrum inversion, or p-zombies, which are meant to draw our attention to the alleged gap between physical explanations and consciousness.\n",
        "  * Source: [Neuroscience Readies for a Showdown Over Consciousness Ideas](https://www.quantamagazine.org/neuroscience-readies-for-a-showdown-over-consciousness-ideas-20190306/)\n",
        "* Consciousness is compression: [Will GPT-5 achieve consciousness? | Joscha Bach and Lex Fridman](https://www.youtube.com/watch?v=YDkvE9cW8rw&t=139s)\n",
        "\n",
        "https://www.heise.de/hintergrund/Anzeichen-von-Bewusstsein-bei-ChatGPT-und-Co-9295425.html\n",
        "\n",
        "https://www.spektrum.de/news/hat-kuenstliche-intelligenz-wie-chatgpt-ein-bewusstsein/2193018\n",
        "\n",
        "Was Bewusstseinstheorien zu künstlicher Intelligenz sagen\n",
        "Das wurde in einer noch nicht begutachteten Arbeit, die Ende August 2023 erschienen ist, getan. 19 führende KI-Forscher und Forscherinnen haben darin anhand von fünf bekannten Bewusstseinstheorien geprüft, ob heutige KI-Systeme (oder Systeme, die man in naher Zukunft herstellen könnte) demnach ein Bewusstsein haben. Dazu haben die Fachleute jeweils »notwendige Bedingungen« aus den Theorien extrahiert, die ein System besitzen müsste, um ein Bewusstsein zu besitzen. Je mehr dieser notwendigen Anforderungen ein System erfülle, so die Argumentation, desto wahrscheinlicher sei es bewusst.\n",
        "\n",
        "* arxiv: [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/abs/2308.08708)\n",
        "\n",
        "| Theorie des <br> Bewusstseins | Künstliches Bewusstsein, falls: |\n",
        "| :---: | :---: |\n",
        "| Recurrent <br> Processing <br> Theory | Eingabemodule besitzen elnen Rekursionsalgorithmus, der <br> es ermöglicht, strukturierte und integrierte Repräsentationen <br> von sensorischen Signalen erzeugen. |\n",
        "| Global <br> Workspace <br> Theory. | Spezialisierte Module Können parallel arbeiten, und ein <br> globaler Arbeitsraum repräsentiert nur den Input eines der <br> Module, wobei die Informationen im Workspace fưr alle <br> Module verfügbar sind. |\n",
        "| (Computational) <br> Higher Order <br> Theory | Top-down-Module überwachen und unterscheiden <br> zuverlässige Repräsentationen von sensorischen Signalen. |\n",
        "| Attention <br> Schema Theory | enthält ein prädiktives Modell, das die Kontrolle über den <br> aktuellen Zustand der Aufmerksamkeit ermöglicht und <br> diesen repräsentiert |\n",
        "| Predictive <br> Processing <br> Theory | Bewusste Rechenvorgänge tragen zur Sicherung der <br> fortwährenden Existenz des Systems bei. Der kausale Fluss <br> der bewussten Berechnungen entspricht dem kausalen <br> Fluss der physischen Dynamik des Systems. |\n",
        "| Integrated <br> Information <br> Theory | Das System integriert mehr Information als seine <br> Teilsysteme. |\n",
        "\n",
        "*Hyperdimensional computing*\n",
        "\n",
        "https://www.quantamagazine.org/a-new-approach-to-computation-reimagines-artificial-intelligence-20230413/?mc_cid=ad9a93c472&mc_eid=506130a407\n",
        "\n",
        "* Instead, Olshausen and others argue that information in the brain is represented by the activity of numerous neurons. So the perception of a purple Volkswagen is not encoded as a single neuron’s actions, but as those of thousands of neurons. The same set of neurons, firing differently, could represent an entirely different concept (a pink Cadillac, perhaps).\n",
        "\n",
        "* This is the starting point for a radically different approach to computation known as hyperdimensional computing. The key is that each piece of information, such as the notion of a car, or its make, model or color, or all of it together, is represented as a single entity: a hyperdimensional vector.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Hyperdimensional_computing\n",
        "\n"
      ],
      "metadata": {
        "id": "NQjv96Zo5uag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"Blue\">**Black Hole Complexity**"
      ],
      "metadata": {
        "id": "2Xmk4oUjXRHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Physics of Black Holes (Schwarzschild Solution)*"
      ],
      "metadata": {
        "id": "D9VH8cY6Ia40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Supermassive Black Holes*"
      ],
      "metadata": {
        "id": "KR_qJb-WbsTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abell 2744**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Abell_2744\n",
        "\n",
        "https://t3n.de/news/gigantisches-schwarzes-loch-verstaendnis-kosmos-1610772/\n",
        "\n",
        "A high black hole to host mass ratio in a lensed AGN in the early Universe, Nature: https://www.nature.com/articles/s41586-024-07184-8"
      ],
      "metadata": {
        "id": "NyCWq-wUbuqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Back Holes*"
      ],
      "metadata": {
        "id": "Zd3SbuXPo-mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video: [The Map of Black Holes](https://youtu.be/Wf0uxjWGwPk)\n",
        "\n",
        "\n",
        "*Kann ein schwarzes Loch seine ganze umgebende Galaxie verschlucken?* Nein!\n",
        "\n",
        "1. [Schwarzschildradius (Ereignishorizont)](https://de.m.wikipedia.org/wiki/Ereignishorizont)\n",
        "\n",
        "2. [Eddington-Leuchtkraft](https://de.m.wikipedia.org/wiki/Eddington-Grenze)\n",
        "\n",
        "3. Long run: [*Hawking Radiation (Black Hole Radiation)*](https://de.m.wikipedia.org/wiki/Hawking-Strahlung)"
      ],
      "metadata": {
        "id": "F8bg3biAyAkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/jwst-spots-giant-black-holes-all-over-the-early-universe-20230814/\n",
        "\n"
      ],
      "metadata": {
        "id": "sj6ZzMxCSft_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/math-proof-draws-new-boundaries-around-black-hole-formation-20230816/"
      ],
      "metadata": {
        "id": "_zyvus7cf5vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supermassive Black Hole in Cygnus A, M87\n",
        "\n",
        "Ultramassive Black Hole: OJ 287, TON 618 (11 solar systems would fit)"
      ],
      "metadata": {
        "id": "MUARabl5Yynl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TON 618](https://de.m.wikipedia.org/wiki/TON_618) - ist ein ferner, lichtstarker Quasar im Sternbild der Jagdhunde. Er enthält das mit vermutlich 66 bis 70 Milliarden Sonnenmassen massereichste derzeit bekannte Schwarze Loch.\n",
        "\n",
        "- Stellar blackholes: up to 3-99 solar masses (maybe millions in Milky Way)\n",
        "- Intermediate blackholes 100-99.999 solar masses, via binary star systems\n",
        "- Supermassive blackholes: 100.000 solar masses up to millions, like Sagittarius A*\n",
        "- Ultramassive blackholes / SLABs: 10 billion solar masses and more, like IC 1101* (Abell 2029 BCG) with 40 billion suns (event horizon thousands of times large than Neptune), 50-60 billion solar masses maybe a physical limit\n",
        "- TON-618 (The 618th entry in the Tonantzintla Catalogue refers a radio-loud Quasar-galaxy, z=2.29): 66 billion solar masses, event horizon (Schwarzschild radius): 400 billion kilometers (40x the distance between Neptune and sun, 40x 30AU = 1300 AU radius). Probably it’s only 40 billion masses\n",
        "- Phoenix A* (seyfert type 2 galaxy), 5,6 min light years away, most x-rays , most star formation, highest luminous gas. Brightest cluster galaxies (BCG): maybe 100 billion solar masses (24,000 time larger than sagittarius a*, and 7% of entire Milky Way, and 3x larger than Messier 33. Triangulum Galaxy). Vent horizon of half a trillion kilometers = 100x more than median distance between Pluto and sun (40AU). Same size also for IC1101, and Holnberg 15A. But seems all three are ore around 40 billion solar masses.\n",
        "- Another big one: Abell 1201 BCG, cD galaxy with around 32 billion solar masses"
      ],
      "metadata": {
        "id": "qJYWRAnO7uUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Ton 618 vs phoenix A Black Hole Size Comparison](https://youtu.be/vtqWnq4RZUE?si=phRqddq9Vjp5v4-d)"
      ],
      "metadata": {
        "id": "gelqAIuAGg3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The largest black hole http://youtu.be/dx53GHSHrSA"
      ],
      "metadata": {
        "id": "uvJpGb4vSIWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1550.png)"
      ],
      "metadata": {
        "id": "gJF8H851pCEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Penrose-Diagram*"
      ],
      "metadata": {
        "id": "DA5HMxDY9iHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Penrose-Diagramm**](https://de.m.wikipedia.org/wiki/Penrose-Diagramm)\n",
        "\n",
        "Video [Was passiert am Ereignishorizont? | Space Time | PBS Digital Studios](https://www.youtube.com/watch?v=mht-1c4wc0Q)\n",
        "\n",
        "> Mit diesem Diagramm kann die globale Struktur der Lösungen der allgemeinen Relativitätstheorie (wie schwarze Löcher und andere Singularitäten, Ereignishorizonte, asymptotische Flachheit) graphisch dargestellt werden.\n",
        "\n",
        "* In der theoretischen Physik ist ein Penrose-Diagramm ein zweidimensionales Diagramm, das den kausalen Zusammenhang von verschiedenen Punkten in einer Raumzeit darstellt.\n",
        "\n",
        "* Es ist eine **Erweiterung des Minkowski-Diagramms**, bei dem horizontal der Raum und vertikal die Zeit eingetragen sind und ein Lichtkegel den kausalen Zusammenhang zwischen unterschiedlichen Ereignissen der Raumzeit zeigt.\n",
        "\n",
        "* Die im Penrose-Diagramm darzustellende Metrik wird mittels konformer Transformation kompaktifiziert, sodass eine unendliche Zeit- und eine unendliche Raumkoordinate als zweidimensionaler endlicher Unterraum dargestellt werden.\n",
        "\n",
        "> Mit diesem Diagramm kann die globale Struktur der Lösungen der allgemeinen Relativitätstheorie (wie schwarze Löcher und andere Singularitäten, Ereignishorizonte, asymptotische Flachheit) graphisch dargestellt werden.\n",
        "\n",
        "*Penrose-Diagramm als konforme Abbildung der (asymptotisch) flachen Minkowski-Raumzeit (siehe Abb. 2). Beide Diagramme repräsentieren die identischen kausalen Zusammenhänge zwischen Ereignissen, wobei in beiden Diagrammen die Weltlinien der Lichtstrahlen mit $\\textstyle \\pm 45^{\\circ }$ geneigte Geraden sind. Die bernsteinfarbene Kurve zeigt eine zeitartige Weltlinie, die im Penrose-Diagramm, wie im Minkowski-Diagramm, immer innerhalb der lilafarbenen Vorwärtslichtkegel verläuft.*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1220.png)"
      ],
      "metadata": {
        "id": "_k6uxeYQ7-va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Schwarzschild Metric (Schwarzschild Coordinates)*"
      ],
      "metadata": {
        "id": "8APTJDua9uNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgangspunkt: Spherically symmetric spacetime**\n",
        "\n",
        "* [Spherically symmetric spacetime](https://en.m.wikipedia.org/wiki/Spherically_symmetric_spacetime) are commonly used to obtain analytic and numerical solutions to Einstein's field equations in the presence of radially moving matter or energy.\n",
        "\n",
        "* Because spherically symmetric spacetimes are by definition irrotational, they are not realistic models of black holes in nature.\n",
        "\n",
        "* However, their metrics are considerably simpler than those of rotating spacetimes, making them much easier to analyze.\n",
        "\n",
        "* Spherical symmetry is a characteristic feature of many solutions of Einstein's field equations of general relativity, especially the Schwarzschild solution and the Reissner–Nordström solution."
      ],
      "metadata": {
        "id": "VFvJh1U23J9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Schwarzschild Metric allows us to compare two points or events in spacetime around a massive object from the perspective of different observers.\n",
        "\n",
        "> $\\Delta s^2=-\\left(1-\\frac{r_s}{r}\\right) \\Delta t^2+\\frac{1}{\\left(1-\\frac{r_s}{r}\\right)} \\Delta r^2$\n",
        "\n",
        "Video: [The Phantom Singularity | Space Time](https://www.youtube.com/watch?v=-q7EvLhOK08)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1255.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1256.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1257.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1258.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1259.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1260.png)\n",
        "\n",
        "For objects not traveling at speed of light and being at event horizon, time and space switch. The objects falls towards the central singularity:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1261.png)\n",
        "\n",
        "Light from event horizon never reaches us - has infinite wavelength:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1262.png)\n",
        "\n",
        "Remove coordinate singularity with other coordinates at event horizon:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1263.png)"
      ],
      "metadata": {
        "id": "FqIBuvFaepsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Schwarzschild Solution (Schwarzschild Coordinates)*\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1235.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1237.png)"
      ],
      "metadata": {
        "id": "rNxDekLPOL4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schwarzschild-Koordinaten (Linienelement):**\n",
        "\n",
        "> ${\\displaystyle \\mathrm {d} s^{2}=g_{\\mu \\nu }\\mathrm {d} x^{\\mu }\\mathrm {d} x^{\\nu }=-c^{2}\\left(1-{\\frac {r_{\\mathrm {s} }}{r}}\\right)\\mathrm {d} t^{2}+{\\frac {1}{1-{\\frac {r_{\\mathrm {s} }}{r}}}}\\mathrm {d} r^{2}+r^{2}\\mathrm {d} \\theta ^{2}+r^{2}\\sin ^{2}(\\theta )\\mathrm {d} \\phi ^{2}}$\n",
        "\n",
        "In einem natürlichen Einheitensystem mit ${\\displaystyle G=c=1,r_{\\mathrm {s} }=2M}$ wird das Linienelement zu\n",
        "\n",
        "> ${\\displaystyle \\mathrm {d} s^{2}=-\\left(1-{\\frac {2M}{r}}\\right)\\mathrm {d} t^{2}+{\\frac {1}{1-{\\frac {2M}{r}}}}\\mathrm {d} r^{2}+r^{2}\\mathrm {d} \\theta ^{2}+r^{2}\\sin ^{2}(\\theta )\\;\\mathrm {d} \\phi ^{2}}$\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1228.png)"
      ],
      "metadata": {
        "id": "W8GkCu3xdqDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schwarzschild Metric as a solution to the Einstein Field Equations (Birkhoff'sche Theorem)**\n",
        "\n",
        "* Schwarzschild black hole, which is often termed as the most basic type, namely non-rotating and non-charged\n",
        "\n",
        "* One of the few analytic solution to the Einstein Field equations (Gravitationsfeld einer kugelsymmetrischen Masseverteilung - Herleitung der entsprechenden Metrik dafur gelang Schwarzschild 1916).\n",
        "\n",
        "  * Schwarzschild Metrik ist die eindeutige Lösung der Vakkum-Feldgleichungen (mit sphärischer Symmetrie und verschwindender kosmologischer Konstante). [Exact solutions in general relativity > Types of exact solution](https://en.m.wikipedia.org/wiki/Exact_solutions_in_general_relativity#Types_of_exact_solution)\n",
        "\n",
        "  * Siehe auch [Birkhoff'sche Theorem](https://de.m.wikipedia.org/wiki/Birkhoff-Theorem): „*Eine sphärisch symmetrische [Vakuumlösung](https://de.m.wikipedia.org/wiki/Vakuumlösung) der einsteinschen Feldgleichungen außerhalb einer sphärisch symmetrischen Massenverteilung muss statisch sein und diese Lösung muss die Schwarzschild-Lösung sein*.“\n",
        "\n",
        "* [Schwarzschild-Metrik](https://de.m.wikipedia.org/wiki/Schwarzschild-Metrik) ist für ein statisches und ungelades Schwarzes Loch\n",
        "\n",
        "* **Die Schwarzschild-Metrik gilt für alle sphärisch-symmetrischen Massen-verteilungen mit verschwindender kosmologischer Konstante**. Befindet sich diese Massenverteilung vollständig unterhalb des Schwarzschild-Radius, so beschreibt sie auch eines der außergewöhnlichsten Objekte in der ART - ein statisches Schwarzes Loch.\n",
        "\n",
        "* Schwarzschild-Metrik **drescribes the most simplified Black hole which can exist: a single unit of a compressed mass within a completely empty plane of spacetime, and hence unaffected by surroundings** (It's an eternal black hole)\n",
        "\n",
        "* **The [Schwarzschild metric](https://de.m.wikipedia.org/wiki/Schwarzschild-Metrik) describes the gravitational field outside a spherical mass**.\n",
        "\n",
        "* Nur Aussenrand betrachtet erstmal. Innenraum: interessant bei Behandlung Neutronensterne\n",
        "\n",
        "\n",
        "* Video: [Schwarzschild metric - part 1](https://www.youtube.com/watch?v=UJmAl10srHs)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1160.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1159.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MZfJ-Rft5yrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singularities**\n",
        "\n",
        "* Some singularities come from choice of reference frame or coordinate system.\n",
        "\n",
        "  * Like event horizon is a frame-dependent singularity.\n",
        "\n",
        "  * Or on earth, North and south pole are ‘coordinate singularities’, because you can pass time zones infinitely quickly, but only because of choice of spherical coordinates.\n",
        "\n",
        "* But gravitational singularity at the center of a black hole is a real singularity, because curvature and density are infinite from any reference frame. You can change to any coordinate system and you get the same result.\n",
        "\n",
        "* However, the reality of the black hole singularity may give reason to **doubt the theory** that predicts such a thing.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1254.png)"
      ],
      "metadata": {
        "id": "NgRkZKXTdW2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Event Horizon and Schwarzschild Radius**\n",
        "\n",
        "* A Schwarzschild black hole is characterized by a surrounding spherical boundary called the [event horizon](https://en.m.wikipedia.org/wiki/Event_horizon) / [Ereignishorizont](https://de.m.wikipedia.org/wiki/Ereignishorizont) which occurs at the Schwarzschild radius.\n",
        "\n",
        "\n",
        "* Bei statischen Schwarzen Löchern ist der Ereignishorizont eine Kugeloberfläche, deren Radius [Schwarzschild-Radius](https://en.m.wikipedia.org/wiki/Schwarzschild_radius) genannt wird. [Source](https://de.wikipedia.org/wiki/Allgemeine_Relativitätstheorie#Metriken_Schwarzer_Löcher)\n",
        "\n",
        "  * The sun, for example, has a radius of 700,000km\n",
        "but has a Schwarzschild radius of just 3km.\n",
        "\n",
        "  * The earth's radius is 6371 km but its Schwarzschild radius is just 9mm, about the size of a peanut.\n",
        "\n",
        "* <font color=\"blue\">Either of these would have to collapse to their respective Schwarzschild radius before the force of gravity would overcome the nuclear forces and they would continue to become a black hole</font>, something that will never happen to either of them because they don't have the mass required.\n",
        "\n",
        "\n",
        "Video: [Do Events Inside Black Holes Happen?](https://www.youtube.com/watch?v=vNaEBbFbvcY)\n",
        "\n",
        "* Events at and in the black hole don't happen to an observer form outside. Events = Where and <strike>When</strike>? A \"when\" cannot not consistently be assigned!\n",
        "\n",
        "> **And a black hole is exactly that set of events (that don't happen to an outside observer)**\n",
        "\n",
        "* black hole is not just a set of locations. Black hole = all events that take place at these locations.\n",
        "\n",
        "* it's not a visibility issue (thaty things happen but we simply cant see them). a black hole is the collection of happenings that we say dont happen at all. **And the black blob is just how it looks like in ordinary temporal and spatial terms when you delete these occurrences from all observers from the history of the universe**.\n",
        "\n",
        "* the last thing we can see is the called 'event horizon'. but it's not just a spherical surface in space. **An event horizon is a surface in spacetime. It represents the last event to which you can even assign a \"when\"**\n",
        "\n",
        "* Fun fact: a black hole is not because all light is sucked in it\n",
        "\n",
        "  * the photons are simply falling radially inwards and the geometry inside a black hole is so distored, that going out is not a valid option anymore\n",
        "\n",
        "  * Moreover, from our external point the photon never entered the black hole. everyhting freezes on its surface to us. The wavelength of the last photon is stretch until infinity (redshifted to undetectable low frequencies).\n",
        "\n",
        "  > **So to us a black hole looks black because light that gets emitted just outside the horizon is redshifted into invisibility.**\n",
        "\n",
        "  * the infinite redshift keeps you from seeing it at all.\n",
        "\n",
        "If the sun is a perfect sphere\n",
        "\n",
        "* It determines the spacetime geometry in its neighbourhood, the resulting geodesics of which correspond to radial freefall, obrits etc\n",
        "\n",
        "* If I replace the sun with a 6 km (3) black hole, the geodesics beyond where the sun's edge used to be, remain change. Earth's orbit won't be any different = that black hole generates the same spacetime geometry out here than the sun does\n",
        "\n",
        "* the black hole is not a vaccum cleaner that suck stuff in. The idea comes from the region that used to be inside the sun but outside the black hole (event horizon edge)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1245.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1246.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "V8VF1RCIsK41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penrose–Hawking singularity theorems & Gravitational singularity**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Gravitational_singularity\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Penrose–Hawking_singularity_theorems\n",
        "\n",
        "*Gravitational singularity*\n",
        "\n",
        "* A [gravitational singularity](https://en.m.wikipedia.org/wiki/Gravitational_singularity), spacetime singularity or simply singularity is a condition in which gravity is so intense that spacetime itself breaks down catastrophically.\n",
        "\n",
        "* As such, a singularity is by definition no longer part of the regular spacetime and cannot be determined by \"where\" or \"when\"."
      ],
      "metadata": {
        "id": "LDb92b8nynXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why space and time switch in a black hole?**"
      ],
      "metadata": {
        "id": "_mBEZRzjJbIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Switch means what flows unidirectional forward: time outside, and space (all paths) inside)**\n",
        "\n",
        "> Once crossing the event horizon, all paths lead to singularity - **space flows unidirectional downwards** and leading only to the central singularity (faster than speed of light), just like **time flows unidirectional forward** outside the black hole. [Source](https://www.youtube.com/watch?v=mht-1c4wc0Q)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1270.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "lFqJcKjaQVKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Event Horizon and Kretschmann-Skalar / Singularität**\n",
        "\n",
        "* Der [Kretschmann-Skalar](https://de.m.wikipedia.org/wiki/Kretschmann-Skalar) $K$ (auch Kretschmann-Invariante oder Riemannsche Invariante) bezeichnet eine skalare Invariante im Bereich der Lorentzschen Mannigfaltigkeiten.\n",
        "\n",
        "* Er kann als Maß für die Krümmung der Raumzeit in der allgemeinen Relativitätstheorie gedeutet werden.\n",
        "\n",
        "* Für die Schwarzschild-Metrik ist der Kretschmann-Skalar mit dem Schwarzschild-Radius $r_{s}$ gegeben durch:\n",
        "\n",
        "> ${\\displaystyle K={\\frac {12{r_{s}}^{2}}{r^{6}}}={\\frac {48G^{2}M^{2}}{c^{4}r^{6}}}}$\n",
        "\n",
        "* Dieser Krümmungsskalar verhält sich im Gegensatz zum Ricci-Skalar bei $r$ = $r_s$ regulär und hat keine Singularität - es ist nur eine [Coordinate_singularities](https://en.m.wikipedia.org/wiki/Singularity_(mathematics)#Coordinate_singularity)\n",
        "\n",
        "* **Bei $r = r_s$ (am Ereignishorizont) erreicht die [Fluchtgeschwindigkeit](https://de.m.wikipedia.org/wiki/Fluchtgeschwindigkeit_(Raumfahrt)) des schwarzen Loches die Lichtgeschwindigkeit**\n",
        "\n",
        "\n",
        "* Es hat dennoch eine physikalische Bedeutung, denn die Metrikkomponenten $g_{tt}$ und $g_{rr}$ in der Schwarzschild-Metrik wechseln die Vorzeichen.\n",
        "\n",
        "* Das bedeutet die Koordinaten Zeit t und Raum r tauschen hier ihren raum- bzw. zeitartigen Charakter. Im Fall der Schwarzschild-Metrik wird diese Kugeloberfläche auch Ereignishorizont genannt. Nichts dahinter kann von aussen beobachtet werden.\n",
        "\n",
        "\n",
        "> Die Lichtkegel verengen sich bei Annäherung an den Schwarzschild-Radius. Für r > r_s sind sie entlang der Zeitachse geöffnet, **für r < r_s öffnen sich die Lichtkegel entlang der Raumachse, d. h. r wird eine zeitartige und t eine raumartige Koordinate**. Unterhalb des Horizonts bewegt sich alles unausweichlich auf die Singularität bei r = 0 zu.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1213.png)\n"
      ],
      "metadata": {
        "id": "Af7lNN5vtAV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Why space and time switch in a black hole? - Spacetime Singularity & Curvature Singularity (Gravitational singularity)](https://en.m.wikipedia.org/wiki/Gravitational_singularity)**\n",
        "\n",
        "Video:[**Why space and time switch in a black hole?**](https://www.youtube.com/watch?v=GQZ3R81iyE0)\n",
        "\n",
        "Video: [Wie Zeit in einem schwarzen Loch zu Raum wird | Space Time](https://www.youtube.com/watch?v=KePNhUJ2reI)\n",
        "\n",
        "\n",
        "* At a  center of every black hole lies a **spacetime singularity**; a point so deformed by mass compression that it punches through the fabric of spacetime to become a physical boundary of the universe, a place where all light and matter meet their eternal end.\n",
        "\n",
        "* In addition, a black hole has a **curvature singularity**, which is the part where the curvature of deformed spacetime begins to take on infinite values. It is here where science begins to depart from General relativity.\n",
        "\n",
        "* If we were somehow able to survive up until this point, then we would experience something very strange: **time and space, as connected entities, would switch roles, as the black hole's interior becomes infinite**.\n",
        "\n",
        "> **The singularity is suddenly no longer a physical point, but rather an inevitable event in the future.**\n",
        "\n",
        "* Space is so warped that all paths will inevitably lead us to this singularity, but time is so that we would require an ever-increasing amount of it to get there, as time on the outside slows down and eventually stops, forever.\n",
        "\n",
        "* And so, at this point, our cosmic fate would be sealed. We would be frozen in that moment of time forever more. The singularity now represents the infinite future of a featureless, timeless dimension, from which nothing will ever be able to escape. You would never reach the bottom of this sinkhole.. you would just continue cruising deeper into your eternal cosmic chamber, unless you were to hit another part of spacetime."
      ],
      "metadata": {
        "id": "F0XISHhtq4Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the mathematical explanation:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1235.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1237.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1238.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1239.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ge36WXVaQIkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here the graphical explanation based on Penrose diagrams:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1240.png)\n",
        "\n",
        "We need to switch to Penrose diagram:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1241.png)\n",
        "\n",
        "> <font color=\"blue\">See above: in left picture time is Y axis (going only forward) and space is X axis. In right picture (in black hole) the time is now X axis and space is Y axis (going only forward) </font>\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1242.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1243.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1244.png)"
      ],
      "metadata": {
        "id": "3YOkZqCrVjAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Why time stops in the Black Hole and we can't see inside a Black Hole? - Tidal Forces](https://en.m.wikipedia.org/wiki/Tidal_force), Speed of Light and Event Horizon**\n",
        "\n",
        "* Tidal forces: The strength of gravity is inversely proportional to the square of the distance away between two objects.\n",
        "\n",
        "* In the black hole, space is increasing faster then the speed of light, due to the gravitational pull, hence light cannot escape. <font color=\"blue\">**Reaching the centre of the black hole would take forever, and the photon would never reach the centre, because the gravitation is increasing faster than the speed of light (otherwise light would be able to escape the black hole)**</font>\n",
        "\n",
        "* To a photon, a journey from one side of the\n",
        "universe to the other is instant because at the speed of light time stops even though\n",
        "to us as the outside observers it takes a hundred billion years or more allowing for\n",
        "the expanding universe.\n",
        "\n",
        "* <font color=\"blue\">Because the force of gravity goes from very little away from the black hole to infinity at its centre, the tidal forces generated could be huge over distances at short as a couple of meters.</font>\n",
        "\n",
        "* Once beyond the horizon, everything including space itself is travelling faster than the speed of light if you could measure it against a fixed point outside. This is allowed, although nothing can travel faster than the speed of light in a vacuum, there is nothing to say that space itself can't travel faster.\n",
        "\n",
        "> <font color=\"blue\">That's what's happening with a black hole: **the extreme pull of gravity is distorting space-time so much that its moving faster towards the singularity than the light inside and that's why we can never see inside a black hole and once over the horizon, we would never see our astronaut again**.</font>\n",
        "\n",
        "Crossing the event horizon:\n",
        "\n",
        "* there is\n",
        "no absolute time, time is relative to each person and their circumstances depending on\n",
        "how warped the piece of spacetime where they are.\n",
        "* This leads us to a rather strange duality\n",
        "of possibilities:\n",
        "* Whilst our astronaut was moving closer to the event horizon in maybe a few just minutes or seconds of their time, to us on the outside\n",
        "they were already travelling close to the speed of light, so seconds to them would become\n",
        "months and ears to us. By the time they cross the horizon, it could be in the far future for us as time for them essentially grinds to a halt from our perspective.\n",
        "* To our astronaut looking out at the universe,\n",
        "it would seem as though the rest of it was speeding up exponentially and even though\n",
        "the amount of time they have left to live could be measured in minutes or seconds, they\n",
        "will have outlived all the life on earth and whatever humans became either by design or\n",
        "evolution.\n",
        "* The astronaut would be somehow dead and alive at the same time: Whilst the rest of the crew would return to live their lives, our astronaut would be frozen in time, alive in their version of reality, yet dead to us\n",
        "\n",
        "Temperature and density in a black hole\n",
        "* Before they reach the centre of the black hole they will enter the most extreme place in the universe this side of the big bang. The density of matter at the centre of the singularity is thought to reach the plank density or higher.\n",
        "* Planck density: if you take all matter of the observable universe and compress it into a space of the size of an atom\n",
        "* As you compress matter the it gets hotter\n",
        "as the particles collide together ever more energetically and as it approaches the centre\n",
        "of the black hole the temperature would rise to trillions of times greater than the centre\n",
        "of the sun but then at the actual singularity, the point of infinity, it would be about just\n",
        "a millionth of a degree above absolute zero because heat is energy is light and it can't\n",
        "escape, so there is nothing to measure, therefore there is essentially no temperature, except\n",
        "the tiny amount from the hawking radiation that leaks out.\n",
        "* end of a black hole maybe: information in form of Hawking radiation in 10^100 years during the heat death of the universe as the black holes eventually evaporates away\n",
        "\n",
        "Source: https://www.youtube.com/watch?v=JqyX26y6g2A&list=WL&index=8"
      ],
      "metadata": {
        "id": "3KRczKRz67yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Maximally-Extended Schwarzschild Solution (Kruskal-Szekeres Coordinates)*"
      ],
      "metadata": {
        "id": "OQhOPkypmczT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [PBS: How new coordinates help solve the coordinate singularity](https://youtu.be/T4oYvSH6jJ8) (eddington finkelstein and kruskal szekeres)"
      ],
      "metadata": {
        "id": "pzWO-UNuPXJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Kruskal-Szekeres-Koordinaten](https://de.m.wikipedia.org/wiki/Kruskal-Szekeres-Koordinaten)\n",
        "\n",
        "* Penrose-Diagramm eines statischen Schwarzen Loches. **Grundlage ist die Schwarzschild-Metrik mit [Kruskal-Szekeres-Koordinaten](https://de.m.wikipedia.org/wiki/Kruskal-Szekeres-Koordinaten)** als maximale analytische Erweiterung der Schwarzschildlösung [Source](https://de.m.wikipedia.org/wiki/Penrose-Diagramm)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0711.png)\n",
        "\n",
        "* Kruskal-Szekeres-Koordinaten sind Koordinaten für die Schwarzschild-Metrik, die Metrik, die den Außenraum einer kugelsymmetrischen, nicht rotierenden und elektrisch neutralen Massenverteilung beschreibt.\n",
        "\n",
        "* Die [Kruskal-Lösung](https://de.m.wikipedia.org/wiki/Kruskal-Lösung) ist eine maximale Erweiterung der Schwarzschild-Lösung. Sie weist intrinsische Singularitäten auf, weshalb sie nicht vollständig ist.\n",
        "\n",
        "* Die Lösung kann als eine Beschreibung von Einstein-Rosen-Brücken bzw. Wurmlöchern angesehen werden.\n",
        "\n",
        "* Kruskal-Szekeres-Koordinaten werden am Ereignishorizont $r=2M$ nicht singulär und werden deswegen gerne für die Beschreibung Schwarzer Löcher eingesetzt (präziser: für die Beschreibung durch mitbewegte, interne Beobachter im Gegensatz zu externen Beobachtern, die zum Beispiel im Außenbereich an einen Stern „fixiert“ sind.)\n",
        "\n",
        "* In general relativity [Kruskal–Szekeres coordinates](https://en.m.wikipedia.org/wiki/Kruskal–Szekeres_coordinates) are a coordinate system for the Schwarzschild geometry for a black hole. **These coordinates have the advantage that they cover the entire spacetime manifold** of the maximally extended Schwarzschild solution and are well-behaved everywhere outside the physical singularity. There is **no misleading coordinate singularity at the horizon**.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1216.png)\n",
        "\n",
        "*The [Kruskal–Szekeres diagram](https://en.m.wikipedia.org/wiki/Kruskal–Szekeres_coordinates), illustrated for 2GM=1. The quadrants are the black hole interior (II), the white hole interior (IV) and the two exterior regions (I and III). The dotted 45° lines, which separate these four regions, are the event horizons. The darker hyperbolas which bound the top and bottom of the diagram are the physical singularities. The paler hyperbolas represent contours of the Schwarzschild r coordinate, and the straight lines through the origin represent contours of the Schwarzschild t coordinate:*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1224.png)"
      ],
      "metadata": {
        "id": "7erd6uybQIce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximally-Extended Schwarzschild Metric with [White Holes](https://en.m.wikipedia.org/wiki/White_hole) and [Einstein-Rosen-bridges (Wormholes)](https://en.m.wikipedia.org/wiki/Wormhole)**\n",
        "\n",
        "*Schwarzschild Metric with Eternal Black Holes*\n",
        "\n",
        "* A few months after Einstein presented his Theory of General Relativity, in which he first predicted the black hole, a German scientist named Karl Schwarzschild solved the field equations for their first exact solution; giving us the Schwarzschild Metric.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0700.jpg)\n",
        "\n",
        "* The Schwarzschild Metric describes the most simplified black hole which can exist; a single unit of compressed mass within a completely empty plane of spacetime, which is therefore unaffected by its surroundings. One which was never born and one that will never die, never growing or shrinking- an Eternal Black Hole.\n",
        "\n",
        "* It was from this earliest of case studies for General Relativity that the white hole was extrapolated, almost half a century later. **In addition to admitting positive square root value solutions, the Schwarzschild Metric also admits negative square root solutions.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0701.jpg)\n",
        "\n",
        "* In simple terms, we can extend the Eternal Black Hole model to account for a new dimension where we find the white hole.\n",
        "\n",
        "* To make this easier to understand, we can use a diagram. Here is our universe, with us at the centre- you have the infinite past and future running along the y-axis, and distance running along the x-axis, marked by the cosmic horizon.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0702.jpg)\n",
        "\n",
        "* Spacetime is represented by curling lines which flow across the diagram, and light travels diagonally.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0703.jpg)\n",
        "\n",
        "* Now, if we add in the eternal black hole- one of the cosmic horizons becomes the black hole's event horizon- the point of no return and a boundary of the universe. Anything within this event horizon will need to travel for an infinite amount of time to escape, i.e., it will never happen.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0704.jpg)\n",
        "\n",
        "* If we go beneath this horizon, we extend the diagram into the black hole's interior. This is the area we spoke about where space and time switch roles; the place where time stops and the singularity becomes the infinite future.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0705.jpg)\n",
        "\n",
        "* The thing is, however, that this black hole is eternal- and so as well as accounting for the infinite future, we must account for the infinite past. And so, we can extend our diagram on the past side. What we get is a time-reversed black hole- as in, a black hole viewed backwards in time, and thus, it ejects its matter and energy.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0706.jpg)\n",
        "\n",
        "* The singularity is now an event in the infinite past which can never be reached trying to get there would be like trying to travel backwards in time.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0707.jpg)\n",
        "\n",
        "* And so instead, light and matter can only flow outwards, away from the singularity and into the universe.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0708.jpg)\n",
        "\n",
        "* And thus, we have our white hole- which works inversely to the black hole, by ejecting matter into space.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0709.jpg)\n",
        "\n",
        "* And connecting the horizons is our Einstein-Rosen Bridge, which may lead us to another part of space, another point in time, or perhaps to a new dimension entirely..\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0710.jpg)\n",
        "\n",
        "**White Holes as New Universes**\n",
        "\n",
        "* Big bang = White hole? This idea gives us the perspective we need to complete our cosmic diagram. Once we've added in our past and future black hole extensions, the Einstein-Rosen Bridge does not lead us back into another place within our universe, like a wormhole would.\n",
        "\n",
        "* Instead, if we follow the negative square root solution of the Schwarzschild Metric beyond the black hole's interior, past it leads to its own universe- a parallel universe Where time runs backwards, thus giving us: our maximally-extended Schwarzschild Solution, the [Kruskal–Szekeres coordinates](https://en.m.wikipedia.org/wiki/Kruskal–Szekeres_coordinates).\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0711.png)\n",
        "\n",
        "> Video: [Black Hole Cosmology (Schwarzschild cosmology) - other side of a black hole](https://youtu.be/ZVPsNDonA84)"
      ],
      "metadata": {
        "id": "FhYVACl3pHGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Killing vector and a Killing horizon*"
      ],
      "metadata": {
        "id": "ypsYD62SW6N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Killing-Vektorfeld\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Killing_horizon\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Null_hypersurface"
      ],
      "metadata": {
        "id": "0H2cDGKOXUj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of a \"killing vector\" and \"killing horizon\" is a theoretical concept in physics that describes the boundary between regions of spacetime where it is possible to escape to a distant observer and regions where it is not. The concept was introduced by the physicist Roger Penrose in the 1960s.\n",
        "\n",
        "A killing vector is a vector field that leaves the spacetime metric (which describes the curvature of spacetime) invariant. In other words, if you move along a killing vector, the spacetime metric around you does not change. This means that killing vectors are geodesics, which are the straightest possible paths in spacetime.\n",
        "\n",
        "A killing horizon is a surface that is orthogonal to all killing vectors. In other words, if you cross a killing horizon, you can no longer move along a killing vector. This means that killing horizons are the boundaries of regions of spacetime where it is not possible to escape to a distant observer.\n",
        "\n",
        "The concept of killing vectors and killing horizons is important in understanding the formation of black holes. Black holes are regions of spacetime where gravity is so strong that not even light can escape. The boundary of a black hole is called the event horizon. According to Penrose's theory, the event horizon of a black hole is a killing horizon.\n",
        "\n",
        "The concept of killing vectors and killing horizons is also important in understanding the Big Bang. The Big Bang was the event that marked the beginning of the universe. According to Penrose's theory, the Big Bang singularity is a killing horizon. This means that it is not possible to escape from the Big Bang singularity, even in principle.\n",
        "\n",
        "The concept of killing vectors and killing horizons is a complex one, and it is still not fully understood. However, it is a powerful tool for understanding some of the most fundamental phenomena in physics, such as black holes and the Big Bang."
      ],
      "metadata": {
        "id": "SrtoCEpMXORB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"Killing vector\" in the context of mathematics and physics, particularly in general relativity and differential geometry, is a concept that describes symmetries in spacetime or other geometric spaces. Here's a more detailed explanation:\n",
        "\n",
        "1. **Killing Vector**: A Killing vector field on a Riemannian manifold (which is a space where you can define distances and angles) is a vector field that represents a symmetry of the manifold. Essentially, if you move along a Killing vector, the geometry of the space does not change. In general relativity, Killing vectors are used to identify symmetries of spacetime. For example, in a spacetime that is symmetric in time, there will be a Killing vector corresponding to this time symmetry. Similarly, in a spherically symmetric space, there are Killing vectors corresponding to rotational symmetries.\n",
        "\n",
        "2. **Killing Horizon**: A Killing horizon is a hypersurface in spacetime on which a Killing vector field becomes null (or lightlike). This is a concept that is particularly important in the study of black holes. In simple terms, a Killing horizon is often associated with the event horizon of a black hole. It is the region where the Killing vector, which represents a certain symmetry (like time translation symmetry), becomes null. This means that at the horizon, the concepts of time and space as understood in normal circumstances become intertwined in a way that is characteristic of the relativistic effects near a black hole.\n",
        "\n",
        "Both concepts are crucial in the study of general relativity and play a significant role in understanding the geometry of spacetime and the behavior of objects like black holes."
      ],
      "metadata": {
        "id": "0QcVrW6XW-sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Singularitäten-Theorem*"
      ],
      "metadata": {
        "id": "HX_Y0zpTYD8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [How The Penrose Singularity Theorem Predicts The End of Space Time](https://www.youtube.com/watch?v=z4odQd8q3xY)\n",
        "\n",
        "[Singularitäten-Theorem](https://de.m.wikipedia.org/wiki/Singularitäten-Theorem): Penrose–Hawking singularity theorems: a set of results in general relativity that attempt to answer the question of when gravitation produces singularities\n",
        "\n",
        "A singularity in solutions of the Einstein field equations is one of two things:\n",
        "1. a situation where matter is forced to be compressed to a point (a space-like singularity)\n",
        "2. a situation where certain light rays come from a region with infinite curvature (a time-like singularity)\n",
        "\n",
        "Space-like singularities are a feature of non-rotating uncharged black holes as described by the Schwarzschild metric, while time-like singularities are those that occur in charged or rotating black hole exact solutions. Both of them have the property of geodesic incompleteness, in which either some light-path or some particle-path cannot be extended beyond a certain proper time or affine parameter (affine parameter being the null analog of proper time).\n",
        "\n",
        "> **In a Schwarzschild black hole, time ceases at the point-like central singularity, while in Kerr black holes space ends at the ring singularity.**\n",
        "\n",
        "[Geodesic incompleteness](https://en.m.wikipedia.org/wiki/Geodesic_manifold): In the theory of general relativity, which describes gravity in terms of a pseudo-Riemannian geometry, many important examples of geodesically incomplete spaces arise, e.g. non-rotating uncharged black-holes or cosmologies with a Big Bang. The fact that such incompleteness is fairly generic in general relativity is shown in the Penrose–Hawking singularity theorems.\n",
        "\n"
      ],
      "metadata": {
        "id": "PdxyWChlYIs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Spinning Black holes (Kerr Solution & Boyer–Lindquist & Kerr-Schild coordinates)*"
      ],
      "metadata": {
        "id": "T3lxJ4d7BriG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blandford-Zjanek process or Black hole bomb with mirrors: [How Black holes spin space time](https://youtu.be/UjgGdGzDFiM?si=i3q0LFySMKns9CV9)"
      ],
      "metadata": {
        "id": "hT_t4X2USOr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1696.png)"
      ],
      "metadata": {
        "id": "KycDWFFzcOFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1697.png)"
      ],
      "metadata": {
        "id": "6bICQVNJcQGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extremal Kerr black holes could amplify new physics**\n",
        "\n",
        "* [Theoretical study shows that Kerr black holes could amplify new physics](https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2023-09-theoretical-kerr-black-holes-amplify.amp)\n",
        "\n",
        "* Researchers have shown that extremal Kerr black holes, a type of rapidly spinning black hole, could be sensitive probes of new physics. These black holes have a unique characteristic: their horizons are affected by infinite tidal forces in the presence of a cosmological constant. This means that if anything were to fall into the black hole, it would be crushed by gravity before it moved even remotely close to the black hole's center.\n",
        "\n",
        "* However, the researchers showed that if the cosmological constant is zero, as it is assumed to be in many astrophysical scenarios, this effect vanishes. They then considered Einstein gravity coupled to its leading quantum corrections, and found that these corrections make the singularity jump all the way from the center of the black hole to the horizon. This suggests that the spacetime geometry near the horizon of these black holes is sensitive to new physics at higher energies.\n",
        "\n",
        "  * Class of black holes known as **extremal Kerr black holes**, which are **uncharged stationary black holes with a coinciding inner and outer horizon**. These black holes' unique characteristics could make them ideal \"amplifiers\" of new, unknown physics.\n",
        "\n",
        "  * in the presence of a cosmological constant extremal black holes are affected by infinite tidal forces. This means that if living beings were to fall into the black hole, they would be crushed by gravity before they moved even remotely close to the black hole's center. Yet the team showed that if the cosmological constant is zero, as it is assumed to be in many astrophysical scenarios, this effect vanishes.\n",
        "\n",
        "  * work on black hole horizon singularities, I asked whether other effects could give rise to such phenomena. My previous work on effective field theories (EFTs), particularly development of physics models with quantum corrections, gave me an idea. Talking with Horowitz, I wondered whether the higher-derivative terms in a gravitational EFT (i.e., quantum corrections to the Einstein equations) could themselves lead to singularities on the horizons of extreme black holes.\"\n",
        "\n",
        "  * the researchers considered Einstein gravity coupled to its leading quantum corrections: \"The Einstein equations are linear in the Riemann tensor, a mathematical object describing the curvature of spacetime,\" Remmen explained. \"In three space dimensions, the leading corrections to Einstein are terms that are cubic (third power) and quartic (fourth power) in the curvature. Because curvature is a measure of derivatives of the spacetime geometry, such terms are called 'higher-derivative terms.' We calculated the effect of these higher-derivative terms on rapidly spinning black holes.\"\n",
        "\n",
        "  * \"Surprisingly, EFT corrections make the singularity jump all the way from the center of the black hole out to the horizon, where you wouldn't expect it to be,\" Remmen said. \"The value of the coefficient in front of a given EFT term—the 'dial settings' in the laws of physics—are dictated by the couplings and types of particle that are present at high energies and short distances. In this sense, EFT coefficients are sensitive to new physics.\"\n",
        "\n",
        "  * Kolanowski, Horowitz, Remmen and Santos also found that the strength of the divergence in tides at the horizon of extremal black holes, and the possible occurrence of tidal singularity, heavily depends on the EFT coefficients. The results of their calculations thus suggest that the spacetime geometry near the horizon of these black holes is sensitive to new physics at higher energies.\n",
        "\n",
        "  * \"Interestingly, this unexpected singularity is present for the values of these EFT coefficients generated by the Standard Model of particle physics,\" Remmen said.\n",
        "\n",
        "  * \"Our results are surprising, since they imply that the low-energy description of physics can break down in a situation where you wouldn't expect that to happen. In physics, there's usually a sense of 'decoupling' between different distance scales. For example, you don't need to know the details of water molecules to describe waves using hydrodynamics. Yet for rapidly spinning black holes, that's precisely what happens: the low-energy EFT breaks down at the horizon.\"\n",
        "\n",
        "  * Overall, the calculations carried out by this team of researchers hint at the promise of extremal Kerr black holes for probing new physical phenomena. While the horizon of these black holes can be very large, it was not expected to have an infinitely large curvature (i.e., infinite tidal forces) in the EFT. Their results show that it does.\n",
        "\n",
        "  * \"In future work, we are interested in exploring whether the singularities can be resolved by ultraviolet physics,\" Remmen added. \"A pressing question is whether the sensitivity of the horizon to new physics persists all the way to the Planck scale, or whether the horizon 'smooths out' at the short-distance scale associated with the EFT. We are also looking for other situations in which short distance effects might show up unexpected at large distances.\"\n",
        "\n",
        "* Gary T. Horowitz et al, [Extremal Kerr Black Holes as Amplifiers of New Physics](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.091402), Physical Review Letters (2023). DOI: 10.1103/PhysRevLett.131.091402"
      ],
      "metadata": {
        "id": "_Ij60GralS7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Kerr_metric"
      ],
      "metadata": {
        "id": "7z7w6ETYB15u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kerr solution\n",
        "\n",
        "Mass and rotstion, but no charge: kerr black hole\n",
        "\n",
        "https://youtu.be/UjgGdGzDFiM"
      ],
      "metadata": {
        "id": "wlPIlr6hJdOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rotating / Spinning Black holes\n",
        "\n",
        "* [Ergosphere](https://de.m.wikipedia.org/wiki/Ergosphäre)\n",
        "* [Ringularity](https://en.m.wikipedia.org/wiki/Ring_singularity)\n",
        "* [Penrose process](https://de.m.wikipedia.org/wiki/Penrose-Prozess)\n",
        "* [Super radiant scattering](https://en.m.wikipedia.org/wiki/Superradiance)\n",
        "* Video: [Black hole bombs (kurzgesagt)](https://youtu.be/ulCdoCfw-bY?si=NJptNw0Pm9wHRKQd)"
      ],
      "metadata": {
        "id": "t7lJPdkcZLmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Black Hole Cosmology (Einstein–Cartan Theory)*"
      ],
      "metadata": {
        "id": "jBlZmRcPPd8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Alan Guth: big bang and black hole\n",
        "* Lee Smolin: black hole here is big bang in another universe\n",
        "* Andrei Linde: eternal inflation"
      ],
      "metadata": {
        "id": "QX_vqN5b6gVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Any such model requires that the **Hubble radius of the observable universe be equal to its Schwarzschild radius**, that is, the product of its mass and the Schwarzschild proportionality constant. This is indeed known to be nearly the case; at least one cosmologist, however, considers this close match to be a coincidence."
      ],
      "metadata": {
        "id": "HEU41Z5U77Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0711.png)\n",
        "\n",
        "> Video: [Black Hole Cosmology (Schwarzschild cosmology) - other side of a black hole](https://youtu.be/ZVPsNDonA84)"
      ],
      "metadata": {
        "id": "lVco1sIYPojc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Cosmological Natural Selection (Lee Smolin)"
      ],
      "metadata": {
        "id": "WcMMue0zbajh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosmological event horizon: no events that happen NOW beyond that horizon can never be seen\n",
        "\n",
        "The obvious difference is that the black hole singularity seems to us to be a point of infinite density in space, while the big bang singularity is a time of infinite density that included all of space.\n",
        "\n",
        "Both the big bang and black hole singularities do occupy all of space - the difference is that the big bang singularity exists in the past for all of space, while the black hole singularity exists in the future for all of the black hole space.\n",
        "\n",
        "Oppenheimer-Snyder solution: until singularity, a black hole is flat, also homogenous andzero pressure (like FLRW metric and what Friedman discivered for our universe)\n",
        "\n",
        "Raj Pathria: turn time around to get white hole and it looks like our universe from inside (black hole cosmology 1972)\n",
        "\n",
        "Lee Smolin - black hole natural selection\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1320.jpg)\n",
        "\n",
        "Video: PBS: https://youtu.be/jeRgFqbBM5E"
      ],
      "metadata": {
        "id": "OO1DZ5_QkuJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sites.imsa.edu/hadron/2021/11/07/schwarzschild-cosmology-and-black-hole-ception/"
      ],
      "metadata": {
        "id": "y0n8dfhk71bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einstein–Cartan theory and Black Hole Cosmology**\n",
        "\n",
        "Die [Einstein-Cartan-Theorie](https://de.m.wikipedia.org/wiki/Einstein-Cartan-Theorie) (ECT, auch Einstein-Cartan-Sciama-Kibble-Theorie, ECSK-Theorie) ist eine Verallgemeinerung der Allgemeinen Relativitätstheorie auf die Riemann-Cartan-Geometrie.\n",
        "\n",
        "* In der Cartan-Geometrie taucht die Torsion als zusätzlicher Freiheitsgrad auf, was in der ECT eine zusätzliche Feldgleichung ergibt. Diese zweite Feldgleichung koppelt die Torsion mit dem Spindichtetensor.\n",
        "\n",
        "A [black hole cosmology](https://en.m.wikipedia.org/wiki/Black_hole_cosmology) (also called Schwarzschild cosmology or black hole cosmological model) (also called Schwarzschild cosmology or black hole cosmological model) **is a cosmological model in which the observable universe is the interior of a black hole**. Such models were originally proposed by theoretical physicist Raj Pathria, and concurrently by mathematician I. J. Good.\n",
        "\n",
        "Brian Cox: Do we live inside of a Black Hole? https://youtu.be/4013hHZHf0I\n",
        "\n",
        "* According to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge, or wormhole. Schwarzschild wormholes and Schwarzschild black holes are different mathematical solutions of general relativity and the Einstein–Cartan theory. Yet for observers, the exteriors of both solutions with the same mass are indistinguishable.\n",
        "\n",
        "* The Einstein–Cartan theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin-spin interaction which is significant in fermionic matter at extremely high densities.\n",
        "\n",
        "* Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of an Einstein-Rosen bridge, which grows as a new universe. Accordingly, the Big Bang was a nonsingular Big Bounce at which the universe had a finite, minimum scale factor. Or, the Big Bang was a supermassive white hole that was the result of a supermassive black hole at the heart of a galaxy in our parent universe.\n",
        "\n",
        "*[Black Hole Cosmology](https://en.m.wikipedia.org/wiki/Black_hole_cosmology): Drehimpuls (Spin) mit Verkruemmung (Torsion) with Antigravitation*\n",
        "\n",
        "* *Black Hole Cosmology: Spin & Torsion (Nikodem Poplawski)*\n",
        "\n",
        "* [Arte: Leben wir in einem Schwarzen Loch?](https://youtu.be/mgXv3aE5eQc) - **Nikodem Poplawski**\n",
        "\n",
        "> Accordingly, our Universe may be closed and may have born in the interior of a black hole existing in a parent universe. [arxiv:0911.0334](https://arxiv.org/abs/0911.0334)\n",
        "\n",
        "* Drehimpuls (Spin) mit Verkruemmung (Torsion) with Antigravitation. Matter never reaches singularity, because torsion prevents that, matter spins faster and faster. Durch Torsion entsteht Abstossung, was ein neues expandierendes Universum erzeugt. - Big Bounce in Black Hole - Singularity-free universe\n",
        "\n",
        "* http://www.nikodempoplawski.com/ULA2020.pdf\n",
        "\n",
        "* https://www.pbs.org/wgbh/nova/article/big-bounce/\n",
        "\n",
        "* https://www.insidescience.org/news/every-black-hole-contains-new-universe\n"
      ],
      "metadata": {
        "id": "MbD8C5B1zk1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Planck Relic*"
      ],
      "metadata": {
        "id": "c1GtjXg4e2yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Planck relics: **tiny black holes at the end of the hawking radiation process in the size of the Planck length**.\n",
        "\n",
        "Smallest black hols would need 10^66 years, but primordial black holes could be candidate now. Important for dark matter explanation. Planck relics would also solve black hole information paradox.\n",
        "\n",
        "Black hole information paradox: if black holes radiate a perfect thermal spectrum then,  by definition, that radiation has maximum entropy and contains no information about whatever fell into the black hole. The black holes evaporates and all information that went into is deleted. This breaks rule of conservation of quantum information.\n",
        "\n",
        "What if all the information they eat is trapped forever in the tiny Planck relic? However that would break another rule: the Bekenstein bound (max of information that a region of space can contain).\n",
        "\n",
        "A way around this has been proposed - what if space inside black holes actually expands to a region larger than the event horizon? What if at the singularity of a black hole a new inflation is triggered. Then there is enough space.\n",
        "\n",
        "To explain dark matter there is one Planck Eric every 30 km^3 in space. (This is how empty space is)\n",
        "\n",
        "Video: [What If (Tiny) Black Holes Are Everywhere?](https://www.youtube.com/watch?v=srVKjWn26AQ&list=WL&index=4)"
      ],
      "metadata": {
        "id": "zePptyFTe7Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Eddington-Leuchtkraft (Eddington luminosity)*"
      ],
      "metadata": {
        "id": "Oz7oDSQ5Bm6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Eddington-Leuchtkraft](https://de.m.wikipedia.org/wiki/Eddington-Grenze)"
      ],
      "metadata": {
        "id": "_F-zucqUBto0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Gibbons-Hawking-Effekt*"
      ],
      "metadata": {
        "id": "La_aZKRrBaXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gibbons-Hawking-Effekt**\n",
        "\n",
        "* Der [Gibbons-Hawking-Effekt](https://de.m.wikipedia.org/wiki/Gibbons-Hawking-Effekt) besagt, dass jeder Lösung der Einsteinschen Feldgleichungen, die über einen kausalen Horizont verfügt, eine Temperatur zugeordnet werden kann;\n",
        "\n",
        "* der kausale Horizont erweitert den Begriff des Ereignishorizonts von Schwarzen Löchern auf kosmologische Dimensionen, er ist diejenige Fläche in der Raum-Zeit, jenseits der Ereignisse den Beobachter nicht mehr beeinflussen können."
      ],
      "metadata": {
        "id": "bv5MxZ86y_8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Extremal Surfaces*"
      ],
      "metadata": {
        "id": "-wgjwv48EPNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUANTUM EXTREMAL SURFACES IN ISOLATED BLACK HOLES\n",
        "* Netta Engelhardt\n",
        "* https://www.youtube.com/watch?v=LWfbZp5vJts&list=WL&index=3&t=761s\n",
        "* https://www.quantamagazine.org/the-most-famous-paradox-in-physics-nears-its-end-20201029/"
      ],
      "metadata": {
        "id": "fufuZarjCgyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Naked singularity*"
      ],
      "metadata": {
        "id": "pgvi8Yztfy7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naked singularity: https://de.m.wikipedia.org/wiki/Nackte_Singularität\n",
        "\n",
        "Cosmic censorship hpyothesis: https://en.m.wikipedia.org/wiki/Cosmic_censorship_hypothesis"
      ],
      "metadata": {
        "id": "b3oIovTcf1bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Holography of the Photon Ring (Sphere)*"
      ],
      "metadata": {
        "id": "XyPztHZDog1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Article: [A Black Hole’s Orbiting Ring of Light Could Encrypt Its Inner Secrets](https://www.quantamagazine.org/black-holes-ring-of-light-could-encrypt-its-inner-secrets-20220908/)\n",
        "\n",
        "* A [photon sphere or photon circle](https://en.m.wikipedia.org/wiki/Photon_sphere) is an area or region of space where gravity is so strong that photons are forced to travel in orbits, which is also sometimes called the last photon orbit\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1680.png)\n",
        "\n",
        "\n",
        "* In [a paper posted online](https://arxiv.org/abs/2205.05064) in May and recently accepted for publication in Classical Quantum Gravity, Strominger and his collaborators revealed that the photon ring around a spinning black hole has an unexpected kind of symmetry — a way that it can be transformed and still stay the same. **The symmetry suggests that the ring may encode information about the hole’s quantum structure.**\n",
        "\n",
        "* “This symmetry smells like something to do with the central problem of understanding the quantum dynamics of black holes,” he said. The discovery has led researchers to debate whether the photon ring might even be part of a black hole’s “holographic dual” — a quantum system that’s exactly equivalent to the black hole itself, and which the black hole can be thought of as emerging out of like a hologram.\n",
        "\n",
        "* Much more theoretical study is needed before researchers can say for sure whether, or in what way, the photon ring encodes a black hole’s inner contents. But: “It’s a target for a holographic description,” (Juan Maldacena)\n",
        "\n",
        "* Light from the inner subrings has made more orbits and was therefore captured before the light from outer subrings, resulting in a series of time-delayed snapshots of the surrounding universe. **“Together, the set of subrings are akin (ähnlich) to the frames of a movie, capturing the history of the visible universe as seen from the black hole,”**. - ‘Hey, there’s an infinite number of copies of the universe right there at that screen? Couldn’t that be where the holographic dual lives?’”\n",
        "\n",
        "* The researchers realized that **the ring’s concentric structure is suggestive of a group of symmetries called conformal symmetry. A system that has conformal symmetry exhibits “scale invariance,” meaning it looks the same when you zoom in or out**. In this case, each photon subring is an exact, demagnified copy of the previous subring. Moreover, a conformally symmetric system stays the same when translated forward or backward in time and when all spatial coordinates are inverted, shifted and then inverted again.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJOF5vuwondV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Computational Complexity of Black Holes*"
      ],
      "metadata": {
        "id": "mUfHuBR4Nhcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Chaos*"
      ],
      "metadata": {
        "id": "MmDqHiWSaNdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "revelant for black holes and other things\n",
        "\n",
        "Absolutely! There's a fascinating connection between quantum chaos and black holes. Here's why:\n",
        "\n",
        "**Black Holes as Chaotic Systems**\n",
        "\n",
        "* **Scrambling:** Black holes are believed to be the fastest \"scramblers\" of information in the universe. Anything falling into a black hole essentially has its information rapidly encoded and spread across the black hole's event horizon. This scrambling is seen as analogous to chaos in classical systems.\n",
        "* **Entropy & Thermodynamics:** Black holes possess entropy, a thermodynamic property often associated with disorder and chaos. The connection between a black hole's entropy and its microscopic characteristics remains a key mystery.\n",
        "\n",
        "**Quantum Chaos & Black Holes**\n",
        "\n",
        "* **Information Paradox:**  Black holes challenge a fundamental law of quantum mechanics – information can't be destroyed. Quantum chaos may help resolve this paradox. It offers possible mechanisms on how information could be 'hidden' within seemingly random features on the event horizon and potentially retrieved through subtle effects associated with Hawking radiation.\n",
        "* **Holographic Duality:** Some theoretical models, like the AdS/CFT correspondence, suggest a deep connection between the physics of black holes in specific spacetimes and certain quantum field theories without gravity. These theories may use notions from quantum chaos to establish this link.\n",
        "* **Understanding the Early Universe:** The extremely hot and dense state of the early universe might have had properties related to quantum chaos.  \n",
        "\n",
        "**Current Research**\n",
        "\n",
        "* **Out-of-Time-Order-Correlators (OTOCs):**  OTOCs are tools used to measure how quickly information scrambles or spreads in a system. They're applied to study quantum chaos and are showing promise in understanding black hole dynamics.\n",
        "* **Spectral Statistics:** Researchers examine the statistics of energy levels in systems related to black holes, looking for signatures of quantum chaos with features like 'level repulsion'.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The complex nature of black holes makes them a testing ground for ideas connected to quantum chaos. The research aims to use the chaotic nature of black holes to potentially resolve fundamental issues in physics like the information paradox, and even gain insights into how physics worked during the earliest moments of our universe.\n",
        "\n",
        "**Let me know if you want more details on any of these connections!**\n"
      ],
      "metadata": {
        "id": "6QG9S913aNdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "video: [Quantum learning for quantum chaos](https://www.youtube.com/watch?v=d1iZ-ov5QOI&list=WL&index=9&t=163s)"
      ],
      "metadata": {
        "id": "lPG-Har-aNdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of quantum chaos, including the key ideas and why it matters:\n",
        "\n",
        "**What is Quantum Chaos?**\n",
        "\n",
        "Quantum chaos is a field of physics that explores the connections between quantum mechanics (the physics of the very small, like atoms and particles) and classical chaos (the behavior of systems that are highly sensitive to starting conditions).\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Classical Chaos:** Systems like the weather or a double pendulum are \"chaotic\" because tiny changes in the initial conditions can lead to wildly different outcomes over time. This is the famous \"butterfly effect\".\n",
        "* **Quantum Mechanics:** In the quantum world, things like energy and position aren't precisely defined. Instead, they exist in fuzzy probabilities and wave-like behavior.\n",
        "* **The Correspondence Principle:**  In general, quantum mechanics should align with classical mechanics in the limit of large systems. Quantum chaos asks: if classical systems can be chaotic, can their quantum counterparts exhibit similar behavior?\n",
        "\n",
        "**How Does Quantum Chaos Manifest?**\n",
        "\n",
        "It turns out that while true chaos can't fully exist in quantum systems, there are interesting ways the \"fingerprint\" of chaos can show up:\n",
        "\n",
        "* **Energy Levels:** In chaotic classical systems, the energy levels tend to be more irregular and bunched up. Similar patterns can be found in the energy levels of certain quantum systems.\n",
        "* **Wavefunctions:** Instead of chaotic trajectories like in classical systems, we look at quantum wavefunctions (which describe the probability of finding a particle somewhere). Wavefunctions of chaotic quantum systems tend to be highly irregular and spread out.\n",
        "* **Level Repulsion:**  In chaotic quantum systems, energy levels tend to \"repel\" each other, in contrast to more predictable, evenly spaced levels seen in regular quantum systems.\n",
        "\n",
        "**Why Does Quantum Chaos Matter?**\n",
        "\n",
        "Quantum chaos research has applications in:\n",
        "\n",
        "* **Understanding Complex Systems:** Studying the ways chaos emerges in the quantum world helps us understand the behavior of complex systems like atoms with many electrons, or the properties of certain materials.\n",
        "* **Quantum Computing:** The properties of quantum chaos could have implications for designing more robust quantum computers.\n",
        "* **Fundamental Physics:** Quantum chaos helps us explore the boundaries between quantum and classical physics, potentially leading to new insights about the fundamental nature of reality.\n",
        "\n",
        "**Let me know if you want a deeper dive into any of these aspects, or if you'd like some examples!**\n"
      ],
      "metadata": {
        "id": "vhEolOizaNdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how quantum chaos impacts the world of quantum computing:\n",
        "\n",
        "**The Challenge of Decoherence**\n",
        "\n",
        "* Quantum computers rely on fragile quantum states for computation. These states are easily disrupted by outside interactions (think of them as very delicate superpositions). This loss of quantum information is called \"decoherence\".\n",
        "* Decoherence leads to errors in computation and is one of the major obstacles to making large-scale, reliable quantum computers.\n",
        "\n",
        "**How Chaos Plays a Role**\n",
        "\n",
        "* **Sensitivity to Errors:** Systems exhibiting quantum chaos can be incredibly sensitive to even tiny errors or perturbations. This means they could be much more prone to decoherence.\n",
        "* **Scrambling of Information:** A central feature of quantum chaos is the rapid \"scrambling\" of quantum information. This makes the recovery of information and correction of errors significantly harder.\n",
        "\n",
        "**Implications for Quantum Computing**\n",
        "\n",
        "* **Limits of Fault Tolerance:**  Quantum chaos may pose limits on how effective traditional error correction techniques can be in quantum computers.\n",
        "* **Designing Robust Gates:** Understanding quantum chaos could aid in designing quantum gates (the building blocks of quantum computations) and algorithms that are less susceptible to chaotic effects and decoherence.\n",
        "* **Hardware Design:** The implications of quantum chaos could even influence the physical design of quantum computer hardware to try and minimize its impact.\n",
        "\n",
        "**Ongoing Research**\n",
        "\n",
        "* **Quantifying Chaos:** Researchers are developing rigorous ways to measure and quantify quantum chaos in actual quantum computing systems. This will help us to understand its impact better.\n",
        "* **Chaos as a Tool (Possibly):** Some scientists are exploring whether aspects of quantum chaos can be harnessed for new kinds of quantum algorithms. Although speculative, this is a fascinating new field.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "Quantum chaos poses a significant challenge for building large-scale quantum computers by making them inherently more susceptible to errors and information loss. Research focused on understanding and potentially mitigating quantum chaos is critical for the advancement of quantum computing.\n",
        "\n",
        "**Let me know if you'd like to explore any of these aspects further!**\n"
      ],
      "metadata": {
        "id": "kX8vW7WhaNdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Notes*"
      ],
      "metadata": {
        "id": "9IpTffRSg1w4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Infinite Future and Infinite Past near a Black Holes Event Horizon ](https://youtu.be/ZKk9bdLLavA?si=9j5EQpqj1psJbQrY)"
      ],
      "metadata": {
        "id": "B0JV1SMNJTrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFORMATIONSVERNICHTUNG: Konkurrenz für kosmische Quantenmixer\n",
        "Schwarze Löcher sind die ultimativen »Quantenmixer« - doch sie bekommen Konkurrenz! Selbst einfache chemische Reaktionen können Information ähnlich effektiv vernichten.\n",
        "* https://www.spektrum.de/news/chemie-vernichtet-information-fast-so-schnell-wie-schwarze-loecher/2213490\n",
        "* https://www.pnas.org/doi/10.1073/pnas.2321668121"
      ],
      "metadata": {
        "id": "Uqr7Ok-MXZoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/never-repeating-tiles-can-safeguard-quantum-information-20240223/\n",
        "\n",
        "https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/"
      ],
      "metadata": {
        "id": "aGogY7Omn0nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Islands and Black Hole Information](https://youtu.be/ZCIho8geEfI?si=A9qrjmZ8qZaPy-11)"
      ],
      "metadata": {
        "id": "x3sTbKWRoAE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Quantengravitation"
      ],
      "metadata": {
        "id": "8JQND4dvHkja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantum chaos"
      ],
      "metadata": {
        "id": "1MOoIWpurMu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does quantum theory imply the entire Universe is preordained?\n",
        "https://www.nature.com/articles/d41586-023-04024-z"
      ],
      "metadata": {
        "id": "sV0gYoKcCQKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strominger Vafa black hole\n",
        "\n",
        "* Bekenstein-Hawking area-entropy\n",
        "* https://arxiv.org/abs/hep-th/9601029: Microscopic Origin of the Bekenstein-Hawking Entropy\n",
        "* https://arxiv.org/abs/1904.03232: Conceptual Analysis of Black Hole Entropy in String Theory"
      ],
      "metadata": {
        "id": "INcfc_vURqEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forscher:innen ist es erstmals gelungen, die Schwerkraft in der Quantenwelt zu messen**\n",
        "\n",
        "* Jetzt ist es uns gelungen, Gravitationssignale bei der kleinsten jemals aufgezeichneten Masse zu messen\n",
        "* Durch das Verständnis der Quantengravitation könnten wir einige der Rätsel unseres Universums lösen, beispielsweise, wie alles begann oder was in schwarzen Löchern passiert\n",
        "* Gelungen ist das den Wissenschaftler:innen mit einer neuen Technik, bei der schwebende Magnete zum Einsatz kommen, die klein genug sind, um in den Quantenbereich einzudringen. Damit waren sie in der Lage, die schwache Anziehungskraft eines winzigen Teilchens nachzuweisen.\n",
        "* Bislang konnte entsprechend nur angenommen werden, dass Teilchen und Kräfte auf mikroskopischer Ebene anders interagieren als Objekte normaler Größe.\n",
        "\n",
        "\n",
        "* https://t3n.de/news/schwerkraft-quantenwelt-quantengravitation-1610416/\n",
        "* https://www.science.org/doi/10.1126/sciadv.adk2949"
      ],
      "metadata": {
        "id": "lAW0fbMJb7sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complexity of Black Holes*"
      ],
      "metadata": {
        "id": "ql8hl9HHzAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Peter Shor - Scrambling Time and black holes - Green Family Lecture at IPAM at UCLA](https://www.youtube.com/watch?v=GPz8iCzlMTY)\n",
        "\n",
        "Video: [Virtual Seminar: Shira Chapman \"On the Complexity of Black Holes\"](https://www.youtube.com/watch?v=G_YdsmH6SBE)\n",
        "\n",
        "Video: [Causality and closed time like curves - 1](https://www.youtube.com/watch?v=MzAvr8Zau8Q)\n",
        "\n",
        "Video: [Erik Verlinde \"Emergence of Gravity from Quantum Information: a Progress Report\"](https://youtu.be/rj6-bq55ccQ?si=6BYQNujmWt4nCe9V)"
      ],
      "metadata": {
        "id": "aY1n53q90eXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Circuit Complexity of Black Holes**\n",
        "\n",
        "* **Why is that interesting?**\n",
        "  * By understanding the quantum circuit complexity of black holes, we can gain insights into the physics of black holes and the nature of quantum gravity.\n",
        "  * Machine learning of circuits: is the ultimate physical limit of computation and data science, and we want to know how far you can push that (good framework of gravitational computer error-correction)\n",
        "  * But: What is the circuit complexity of a black hole? And  What is the computational capacity (complexity) of a black hole?\n",
        "  * Can you solve (quantum or classical) exponential or even undecidable problems in a black hole (efficiently)? Is the black hole a more powerful computer than a quantum computer or not?\n",
        "\n",
        "* Approaches: **One way**: holographic principle. Information in spacetime volume is encoded on boundary -> quantum circuit complexity of black hole can be studied by studying quantum circuit complexity of its boundary. **Another way**: AdS/CFT correspondence -> quantum circuit complexity of black hole in AdS spacetime can be studied by studying the quantum circuit complexity of CFT on boundary of AdS spacetime. **Another way**: Kerr/CFT.\n",
        "\n",
        "* **Background**: Connection between Quantum information and quantum gravity: Hawking 1970s: What happens to quantum information dropped into a black hole? Black holes radiate, photons slowly come out (takes long 10^76 years for black holes of size of the sun, but does it), output is thermal / uncorrelated what came in, so information is gone. That contradicts quantum mechanics.\n",
        "  * **Dilemma:** Stays in black hole forever =› Violates quantum mechanics. Comes out in Hawking radiation =› if there's also a copy inside the black hole, seems to violate the \"No-Cloning Theorem\".\n",
        "  * **Black Hole Complementarity (modern view):** Inside is not a different state than outside, inside is just a \"re-encoding\" of exterior, so no cloning is needed to have (y) in both places. - Jumping into a black hole is just a weird way of measuring the same quantum information that you could have measured if you had stayed outside the black hole.\n",
        "  * **2012: firewall paradox**. (Almheiri et al. 2012): Qubit coming out of black hole cannot be entangled with a qubit inside the blackhole. But Hawking said if you want to see a smooth geometry of spacetime at the event horizon, the qubits need to be entangled. (In QFT spacetime is built up of a huge amount of local entanglement. No entanglement = no smooth spacetime, which means space and time end at the event horizon = the firewall). But this end of spacetime is at the singularity, not at the event horizon already).\n",
        "\n",
        "  * You can simulate gravity possibly on a standard QCs: “Even if a quantum gravity theory seems ‘wild’—even if it involves nonlocality, wormholes, and other exotica—there might be a dual description of the theory that’s more ‘tame,’ and that’s more amenable to simulation by a quantum computer.” If we wanted to simulate quantum gravity phenomena in AdS space, we might be able to do so by first translating to the CFT side, then simulating the CFT on our quantum computer, and finally translating the results back to AdS. The key point here is that, since the CFT doesn’t involve gravity, the difficulties of simulating it on a quantum computer are “merely” the relatively prosaic difficulties of simulating quantum field theory on a quantum computer. - For this to work, **the translation between the AdS and CFT descriptions also needs to be computationally efficient**—and it’s possible that there are situations where it isn’t.\n",
        "  * This means: **nature seems computable** (Penrose disagrees seems): you can build the universe out of NAND gates, see 1:00:00 [Video](https://www.youtube.com/watch?v=nAMjv0NAESM&t=2240s) and **Black holes seems describable** - <font color=\"red\">but to answer the final answer - if universe and black holes describable and computable - we need a theory of quantum gravity, what we don't have!</font>\n",
        "\n",
        "* Paper 1: [Computational Complexity and Black Hole Horizons](https://arxiv.org/abs/1402.5674) and [The Second Law of Quantum Complexity](https://arxiv.org/abs/1701.01107) (Susskind)\n",
        "  * Quantamagazine article: [In New Paradox, Black Holes Appear to Evade Heat Death](https://www.quantamagazine.org/in-new-paradox-black-holes-appear-to-evade-heat-death-20230606) with Video: [Can a New Law of Physics Explain a Black Hole Paradox?](https://www.youtube.com/watch?v=yLOHdW7dLug&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=14)\n",
        "  * Video: [Black Holes and the Quantum-Extended Church-Turing Thesis | Quantum Colloquium (Leonard Susskind)](https://www.youtube.com/live/1CpzigpEJnU?si=C5mB8zppTHXU8XO7)\n",
        "  * Susskind proposed that holographic principle can be used to relate black hole entropy to quantum circuit complexity. Holographic principle states that information contained in volume of spacetime can be encoded on lower-dimensional boundary of that volume. Susskind and his colleagues propose that the black hole entropy is proportional to the quantum circuit complexity of the state that is encoded on the black hole's event horizon.\n",
        "  * Proposal has led to a number of interesting insights into the relationship between black holes and quantum circuit complexity: growth of black hole entropy is consistent with  growth of quantum circuit complexity over time. And relationship between black hole entropy and quantum circuit complexity can be used to derive the laws of thermodynamics for black holes.\n",
        "  * One of the most important aspects of black holes is their entropy. Black hole entropy is a measure of the number of possible microstates that a black hole can be in. Susskind. proposed that black hole entropy is related to complexity of information that is encoded on the black hole's event horizon:\n",
        "    * Black hole entropy is proportional to the area of the black hole's event horizon.\n",
        "    * The area of the black hole's event horizon is proportional to the number of bits of information that can be encoded on the event horizon.\n",
        "    * Therefore, black hole entropy is proportional to the complexity of the information that is encoded on the black hole's event horizon.\n",
        "  * **Susskind's argument suggests that black holes may be able to store and process information in a very efficient way. This could lead to new insights into the nature of information and the laws of physics.**\n",
        "  * **Complexity grows linearly and then stops growing**\n",
        "  - Black hole: from outside in thermal equilibrium, but inside it still grows linearly with time towards the singularity (see penrose diagram) - quantum (circuit) complexity growing? - number of gates, grows until it‘s constant.\n",
        "  - Computational complexity of black holes?\n",
        "  - Two quantum states: how complex to distinguish them? (Metacomplexity / complexiy of complexity)\n",
        "  - Quantum extended chruch turing thesis\n",
        "  - Quantum gravity extneded church turing thesis QGECTT applies to observers outside the black hole. Information inside black hole can be read for someone inside, but cannot be sent outside, which protects the QGECTT (reading from outside would / gaining information which would violate thesis)\n",
        "  - New complexity class: JI/poly (jumping in)\n",
        "  * Different versions of Circuit complexity: with and without ancilla qubits\n",
        "  * Geofffrey Penington: 3SAT solvable by jumping in a black hole?\n",
        "  * Edward Witten: Gravity doesn‘t let us do something, that we can‘t do without gravity.\n",
        "  * Scott Aaronson: Is there some analog for fault tolerance for gravitational or black hole computing?\n",
        "  * **It shouldn‘ let us answer any classical decision problems more efficiently, because:**\n",
        "    * Assuming we can do these things no errors,\n",
        "    * So we make an obviscated circuit that is exponentially long to prepraring some state that has some simply observable thing, but you cannot tell because it ios obviscated\n",
        "    * Then from lenny‘ point of view we ignore the cost of carrying out the instructions of doing that circuit (would take us an exponentially long time, but we assume we get that for free)\n",
        "    * Then we ask how much time does it take us to figure out the problem? by jumping into the black hole we can find the answer immediately. If we are not allowed to jump in, we are still sort of at sqaure one, or it will still take us a long time to do from the outside.\n",
        "    * That is a very different problem from if you could have some efficient obviscated circuit, something that could efficiently be done by jumping in. A classical problem, phrased in classical terms and has an answer in classical terms, a classical decision problem you could solve by jumping in.\n",
        "    * Is that not about asking aboiut the boundaries of this complexity class? Asking whether it actually includes any interesting computations that actually can be done inside the black hole. But we don‘t know.\n",
        "    * Vazirani: Creating this obviscated circuit for this state, and then create it homomorphically so the state can be acted on homorphically by the shockwaves that are coming in. You fall in with this classicla description and then you act on this hidden quantum state and you eventuelly end up answering this question.\n",
        "    * Penington‘s objection: in QFT they think the quantum extended church turing thesis is true (lenny made possible conditions for it). Assuming everything we do in the quantum gravity theory is well described by quantum field theory in a semi-classical background, then it also shouldn‘t be able to break the quantum church turing thesis.\n",
        "    * So we either need to be given as part of the problem - a quantum state that has a non-trivial holographic dual geometry that didn‘t make from scratch (that‘s the case in lenny‘s setup). Or we need to somehow make that semi-classical evolution break down.\n",
        "    * I would say from what we know about quantum gravity to make the latter happen, we either need planck-length curvature or we need exponential complexity (time or number of things involved). It would be very problematic for semi-classical spacetime if we could make it breakdown without either of these conditions being true.\n",
        "    * That‘s why I‘m inclided to say: It shouldn‘ let us answer any classical decision problems more efficiently.\n",
        "    * …\n",
        "    * Sciott: You need some actual phyiscal resource to resolve some porblme beyond what a quantum computer can solve. And the question is simpky: are black holes such a resource ot not?\n",
        "    * Peningoit: my claim (without evidence) is i don’t think that you can make semi-classical gravity break down  without either by planck-lenght curvature oir by exponential times. There shoulndt be anything you do\n",
        "  * **Established: circuit complexity and physics**\n",
        "    * How do you decode information from hawikin radiation about infalling matter\n",
        "    * how does ads/cft dictionary work? (Circuit complexity is the fundamental quantity)\n",
        "    * But depends on input: is iot maximally random (exponential time to solve), or only pseudorandom (solvable in ploynomal time\n",
        "    * Geoffrey Penington: Black hole information: [YouTube · Galileo Galilei Institute (GGI)620+ Aufrufe  ·  vor 1 JahrGeoff Penington: \"Black holes and holographic entanglement entropy - II\" - YouTube](https://m.youtube.com/watch?v=kl6GE10jD9s)\n",
        "  * Computation = spacetime volume, when following Susskind‘s logic as he sees the space to the singularity growing in penrose‘s diagram. But volume cannot be measured, that‘s why he didn‘t use it\n",
        "  * Seth Lloyd: Black Hole Quantum computer (2004): howe many operations can you perform with stuff falling into the black hole?\n",
        "  * Black holes are not different to ordinary rindler space\n",
        "\n",
        "* Paper 2: [Computational pseudorandomness, the wormhole growth paradox, and constraints on the AdS/CFT duality](https://arxiv.org/abs/1910.14646) (Adam Bouland, Bill Fefferman, Umesh Vazirani)\n",
        "  * A fundamental issue in the AdS/CFT correspondence is the wormhole growth paradox. Susskind's conjectured resolution of the paradox was to equate the volume of the wormhole with the circuit complexity of its dual quantum state in the CFT. We study the ramifications of this conjecture from a complexity-theoretic perspective.\n",
        "  \n",
        "* Paper 3: [The Complexity of Quantum States and Transformations: From Quantum Money to Black Holes](https://arxiv.org/abs/1607.05256) (Scott Aaronson)\n",
        "  * [The event horizon’s involved, but the singularity is committed](https://scottaaronson.blog/?p=213) Aaronson on Lenny Susskind's “Black Holes and Holography.”\n",
        "  * **'The favorite measure, at the moment, is known as circuit complexity'** - role of complexity in the black-hole information paradox and the AdS/CFT correspondence (through connections made by Harlow-Hayden, Susskind, and others).\n",
        "\n",
        "* Paper 4: [Black Holes Produce Complexity Fastest](https://physics.aps.org/articles/v9/49)\n",
        "\n",
        "* Paper 5: [Quantum Computational Complexity -- From Quantum Information to Black Holes and Back](https://arxiv.org/abs/2110.14672)\n",
        "  * Quantum computational complexity estimates the difficulty of constructing quantum states from elementary operations, a problem of prime importance for quantum computation.\n",
        "  * Surprisingly, this quantity can also serve to study a completely different physical problem - that of information processing inside black holes.\n",
        "\n",
        "* Paper 6: [Quantum Computation as Gravity](https://arxiv.org/abs/1807.04422) - Optimal quantum computation linked from Gravity.\n",
        "    * Over the years, using holography and Anti-de Sitter/conformal field theories, we have been learning that gravity is intimately related to quantum information.\n",
        "    * The lesson from our findings is that gravity may also teach us how to perform quantum computation in physical systems in the most efficient way.\" [Article](https://phys.org/news/2019-06-optimal-quantum-linked-gravity.html)\n",
        "\n",
        "* Articles:\n",
        "  * [What is complexity in particle physics?](https://www.aei.mpg.de/120719/what-is-complexity-in-particle-physics)\n",
        "  * [nlab: computational complexity and physics](https://ncatlab.org/nlab/show/computational+complexity+and+physics)\n",
        "\n",
        "* Videos:\n",
        "  * [Black Holes and the Quantum-Extended Church-Turing Thesis](https://www.youtube.com/watch?v=1CpzigpEJnU&list=WL&index=4&t=2205s)\n",
        "  * [Black holes and computational complexity](https://www.youtube.com/live/1CpzigpEJnU)\n",
        "  * [Quantum information tools between quantum theory and gravity - Lecture 1 | Flaminia Giacomini](https://www.youtube.com/watch?v=KP09NR3qP8g&list=WL&index=6&t=653s)\n",
        "  * [Quantum Tasks in Holography - Alex May](https://www.youtube.com/watch?v=twa08og43DI)\n",
        "  * [Quantum Black Holes: a Status Report | Steve Giddings](https://www.youtube.com/watch?v=eVjbL1wOMOU)\n",
        "  * [The AdS/CFT Correspondence: a Status Report | Rajesh Gopakumar](https://www.youtube.com/watch?v=UVVfpNR8ISg)\n",
        "  * [Pseudorandomness and the AdS/CFT Correspondence - Adam Bouland](https://www.youtube.com/watch?v=WQ25Srp1zJI)\n",
        "  * [An Introduction to Celestial Holography and the flat space limit of AdS/CFT](https://youtu.be/Wbkj_2KqMoQ)\n"
      ],
      "metadata": {
        "id": "WpyK1qGu3A_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computational Complexity and High Energy Physics\n",
        "* Complexity of AdS,CTF, Firewalls and CTCs\n",
        "* Suppose CTCs exist, how would they change the theory of computation? - Problem: where is the fix point?\n",
        "\n",
        "Video: [Chaos, Black Holes, and Quantum Mechanics - Stephen Shenker, Stanford University](https://www.youtube.com/watch?v=vC0x-QXM8Qg&list=WL&index=5&t=1333s)\n",
        "\n",
        "Video: [Scott Aaronson: Computability Theory of Closed Timelike Curves](https://www.youtube.com/watch?v=fUGjv44_X4Q&list=PLUz_4vZOI0H1XnMEj9uk4Ezp9AUJBVAPE&index=4)\n",
        "\n",
        "Kurt Gödel ist bekannt für seinen bedeutenden Beitrag zur theoretischen Physik in Form der sogenannten „Gödel-Metrik“, welche die Möglichkeit von geschlossenen zeitartigen Kurven (englisch: Closed Timelike Curves, CTCs) in der Allgemeinen Relativitätstheorie aufzeigt. Diese Arbeit leistete er im Rahmen der Feierlichkeiten zu Einsteins 70. Geburtstag im Jahr 1949.\n",
        "\n",
        "Die Gödel-Metrik beschreibt eine Lösung der Feldgleichungen der Allgemeinen Relativitätstheorie, die ein rotierendes Universum darstellt. In einem solchen Universum können geschlossene zeitartige Kurven existieren, was bedeutet, dass ein Pfad durch die Raumzeit theoretisch in seine eigene Vergangenheit zurückkehren könnte. Dies impliziert die Möglichkeit von Zeitreisen innerhalb des theoretischen Rahmens der Allgemeinen Relativitätstheorie.\n",
        "\n",
        "Abstract: In a seminal 1991 paper, David Deutsch proposed a formal model of closed timelike curves (CTCs), or time travel into the past, which used linear algebra to \"resolve the grandfather paradox.\"  In 2008, John Watrous and I showed that, under Deutsch's model, both a classical computer and a quantum computer with access to a polynomial-size CTC could solve exactly the problems in PSPACE.  In this talk, I'll review this result and then give a new extension to the setting of computability theory.  Namely, I'll show that a classical or quantum computer with access to a Deutschian CTC (with no bound on its size) could solve exactly the problems that are Turing-reducible to the halting problem.  Just like in the complexity setting, the most technically interesting part is the upper bound on the power of quantum computers with access to a Deutschian CTC.\n"
      ],
      "metadata": {
        "id": "ZbLeuiw4zEp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Black Holes Surface and Quantum Error Correction: AdS/CFT*\n",
        "\n",
        "* \"Black holes surface behaving like a quantum computer\"\n",
        "* [From black holes to quantum computing - with Marika Taylor](https://www.youtube.com/watch?v=861coSFLOvk&t=193)\n",
        "* [Black Holes and Quantum Error Correction - part 1](https://www.youtube.com/watch?v=uNYVQo9RH4Q)\n",
        "\n",
        "\n",
        "**Emergent Space-Time and Quantum Error Correction**: Recent theoretical work suggests that the space-time geometry itself, especially in the context of black holes, might emerge from a kind of quantum error-correcting code. The idea is that the way information is encoded on a black hole's surface (its event horizon) might be analogous to how information is protected in a quantum error-correcting code. This means that the fabric of space-time itself could be understood as a quantum error-correcting code, which is a radical and exciting concept blending quantum mechanics, general relativity, and quantum computing."
      ],
      "metadata": {
        "id": "VeuipFGXvjfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by \"Black holes are the fastest scramblers in Nature\"?**\n",
        "\n",
        "The phrase \"Black holes are the fastest scramblers in Nature\" refers to a concept in theoretical physics that describes how black holes process information. In this context, \"scrambling\" refers to the way in which information that enters a black hole gets mixed up or dispersed throughout the black hole's horizon.\n",
        "\n",
        "Here are the key points to understand this concept:\n",
        "\n",
        "1. **Information Theory and Black Holes**: In the realm of physics, especially quantum mechanics and general relativity, information is a fundamental concept. It's often considered in terms of quantum information, which deals with the quantum states of particles.\n",
        "\n",
        "2. **Scrambling in Black Holes**: When information falls into a black hole, it doesn't get destroyed (as per the principle of quantum information conservation). Instead, it gets scrambled. This means the information is spread out across the event horizon of the black hole in a highly mixed state. It's like shuffling a deck of cards so thoroughly that the original order is completely lost.\n",
        "\n",
        "3. **Fastest Scramblers**: The assertion that black holes are the fastest scramblers suggests that they are extremely efficient at this process. This idea comes from the holographic principle in string theory, where the time it takes for information to scramble in a black hole is related to its size. The larger the black hole, the faster it scrambles information.\n",
        "\n",
        "4. **Implications for Quantum Physics and Gravity**: This concept is important in the study of quantum gravity and the information paradox. It suggests a deep connection between gravity (black holes) and quantum mechanics (information theory).\n",
        "\n",
        "5. **Chaos Theory and Black Holes**: The scrambling effect is also associated with chaos theory in physics, where small changes in initial conditions can lead to vastly different outcomes. In a black hole, the scrambling of information is an ultimate form of chaotic behavior.\n",
        "\n",
        "This idea is a topic of ongoing research and debate in theoretical physics, particularly in the context of understanding the fundamental nature of black holes, quantum mechanics, and the fabric of spacetime."
      ],
      "metadata": {
        "id": "cuu9KesIzKj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**is it possible to recover the information, that is spread out across the event horizon of the black hole in a highly mixed state?**\n",
        "\n",
        "The question of whether information that has been spread out across the event horizon of a black hole can be recovered is a major topic in theoretical physics, particularly in the context of the black hole information paradox. This paradox arises from the clash between quantum mechanics, which says information cannot be destroyed, and general relativity, which suggests that information falling into a black hole is lost forever.\n",
        "\n",
        "Here are some key points in this discussion:\n",
        "\n",
        "1. **Hawking Radiation and Information Loss**: Stephen Hawking initially proposed that black holes emit radiation (now known as Hawking radiation) and can eventually evaporate completely. This led to the paradox: if the black hole disappears, what happens to the information that fell into it? Hawking initially argued that this information is lost forever, which contradicts the principles of quantum mechanics.\n",
        "\n",
        "2. **Information Scrambling vs. Destruction**: While black holes are believed to scramble information almost beyond recognition, scrambling is not the same as destruction. Theoretical work, particularly in the context of the holographic principle and string theory, suggests that while information is highly scrambled, it is not necessarily destroyed.\n",
        "\n",
        "3. **AdS/CFT Correspondence**: A concept from string theory called the Anti-de Sitter/Conformal Field Theory (AdS/CFT) correspondence provides a framework where the physics inside a black hole (including the fate of information) might be understood in terms of a lower-dimensional boundary, like the event horizon. This has led some physicists to propose that information is somehow encoded on the horizon and could, in principle, be recovered.\n",
        "\n",
        "4. **Black Hole Complementarity**: Another proposed solution is the concept of black hole complementarity, which suggests that information is both reflected at the event horizon and passes through, but no observer can confirm both realities simultaneously. This tries to reconcile the information loss with quantum mechanics without requiring information to actually be duplicated.\n",
        "\n",
        "5. **Firewall Hypothesis**: Some researchers have suggested the existence of a \"firewall\" at the event horizon, which would destroy any information. This hypothesis aims to resolve the paradox but is controversial and contradicts the idea that crossing a black hole's event horizon is uneventful (as predicted by general relativity).\n",
        "\n",
        "6. **Recent Developments**: Recent theoretical developments, including the use of quantum error correction in the context of the holographic principle, suggest that information may indeed be recoverable in principle, although the exact mechanism remains unclear.\n",
        "\n",
        "In summary, while it's theoretically possible that information spread out across a black hole's event horizon might be recoverable, this remains a topic of active research and debate in the fields of quantum mechanics, quantum gravity, and cosmology. The true nature of information processing by black holes continues to be one of the most intriguing puzzles in modern theoretical physics."
      ],
      "metadata": {
        "id": "CJ2pyl5mzOXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Articles, Papers and Videos*"
      ],
      "metadata": {
        "id": "-OrLJOuEP4GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://twitter.com/Andercot/status/1741837072632332649"
      ],
      "metadata": {
        "id": "kZLYtY05aMJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Articles**\n",
        "\n",
        "* Causality doesn't exist? https://www.quantamagazine.org/quantum-mischief-rewrites-the-laws-of-cause-and-effect-20210311/\n",
        "\n",
        "* https://news.berkeley.edu/2019/03/06/can-entangled-qubits-be-used-to-probe-black-holes\n",
        "\n",
        "  * recovering quantum information falling into a black hole is possible if the information is scrambled rapidly inside the black hole.\n",
        "  * The more thoroughly it is mixed throughout the black hole, the more reliably the information can be retrieved via teleportation.\n",
        "  * His group implemented the protocol proposed by Yoshida and Yao and effectively measured an out-of-time-ordered correlation function. Called OTOCs, these peculiar correlation functions are created by comparing two quantum states that differ in the timing of when certain kicks or perturbations are applied.\n",
        "  * can be used in quantum computers to detect more complicated noise. “One possible application for our protocol is related to the benchmarking of quantum computers, where one might be able to use this technique to diagnose more complicated forms of noise and decoherence in quantum processors,” Yao said. “The ability to diagnose how noise affects quantum simulations is key to building better fault-tolerant algorithms and getting accurate answers from current noisy quantum computers.”\n",
        "  * **the fact that we can relate it to cosmology is because we believe the dynamics of quantum information is the same - nderstanding the dynamics of quantum information connects many areas of research within this initiative: quantum circuits and computing, high energy physics, black hole dynamics, condensed matter physics and atomic, molecular and optical physics. The language of quantum information has become pervasive for our understanding of all these different systems.**\n",
        "  * but: OTOCs do not generally discriminate between quantum scrambling and ordinary decoherence. https://www.nature.com/articles/s41586-019-0952-6\n",
        "\n",
        "* https://www.wired.com/story/black-holes-will-destroy-all-quantum-states/\n",
        "\n",
        "* https://www.quantamagazine.org/new-calculations-show-how-to-escape-hawkings-black-hole-paradox-20230802/\n",
        "\n",
        "* https://www.quantamagazine.org/the-most-famous-paradox-in-physics-nears-its-end-20201029/\n",
        "\n",
        "* https://www.quantamagazine.org/black-holes-ring-of-light-could-encrypt-its-inner-secrets-20220908/\n",
        "\n",
        "* https://www.quantamagazine.org/mathematicians-find-an-infinity-of-possible-black-hole-shapes-20230124/\n",
        "\n",
        "* https://www.quantamagazine.org/how-space-and-time-could-be-a-quantum-error-correcting-code-20190103/\n",
        "\n",
        "* https://www.quantamagazine.org/new-codes-could-make-quantum-computing-10-times-more-efficient-20230825/\n",
        "\n",
        "* https://www.quantamagazine.org/a-century-later-new-math-smooths-out-general-relativity-20231130/\n",
        "\n",
        "* https://phys-org.cdn.ampproject.org/c/s/phys.org/news/2023-12-theory-einstein-gravity-quantum-mechanics.amp\n",
        "\n",
        "* https://www.quantamagazine.org/alan-turing-and-the-power-of-negative-thinking-20230905/\n",
        "\n",
        "* https://t3n.de/news/radikale-neue-theorie-quanentmechanik-schwerkraft-1594368/\n",
        "\n",
        "* https://www.quantamagazine.org/mathematicians-prove-symmetry-of-phase-transitions-20210708/\n",
        "\n",
        "* https://www.quantamagazine.org/computer-scientists-inch-closer-to-major-algorithmic-goal-20230623/\n",
        "\n",
        "* https://scottaaronson.blog/?p=7651\n",
        "\n",
        "* https://www.quantamagazine.org/the-quest-to-quantify-quantumness-20231019/\n",
        "\n",
        "* https://www.quantamagazine.org/quantum-computers-struggle-against-classical-algorithms-20180201/\n",
        "\n",
        "* https://www.quantamagazine.org/a-new-map-of-the-universe-painted-with-cosmic-neutrinos-20230629/\n",
        "\n",
        "* https://www.derstandard.de/story/3000000198164/astronom-falcke-schwarze-loecher-sind-wie-ein-wundersamer-raum-voller-suessigkeiten\n"
      ],
      "metadata": {
        "id": "s72DQTORSAEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Youtube Videos**\n",
        "\n",
        "[Geoffrey Penington (UC Berkeley): Black Holes and Quantum Computers](https://www.youtube.com/watch?v=FtpWt4MexMU&list=WL&index=4&t=1560s)\n",
        "\n",
        "[Virtual Seminar: Shira Chapman \"On the Complexity of Black Holes\"](https://www.youtube.com/watch?v=G_YdsmH6SBE&t=1329s)\n",
        "\n",
        "[Peter Shor - Scrambling Time and black holes - Green Family Lecture at IPAM at UCLA](https://www.youtube.com/watch?v=GPz8iCzlMTY&t=5s)\n",
        "\n",
        "[Quantum Complexity Inside Black Holes | Leonard Susskind](https://www.youtube.com/watch?v=FpSriHE1r4E&list=WL&index=6)\n",
        "\n",
        "[The Physics of Black Holes - with Chris Impey](https://www.youtube.com/watch?v=roM1QPr8lNo&list=WL&index=1&t=1542s)\n",
        "\n",
        "[Information scrambling: a path-integral perspective](https://www.youtube.com/watch?v=oA9fMN8aWBQ&list=WL&index=2&t=415s)\n",
        "\n",
        "[Chaos, Black Holes, and Quantum Mechanics - Stephen Shenker, Stanford University](https://www.youtube.com/watch?v=vC0x-QXM8Qg&list=WL&index=3&t=3162s)\n",
        "\n",
        "[Quantum learning for quantum chaos](https://www.youtube.com/watch?v=d1iZ-ov5QOI&list=WL&index=5)\n",
        "\n",
        "[Entanglement and Complexity: Gravity and Quantum Mechanics](https://www.youtube.com/watch?v=9crggox5rbc&list=WL&index=8&t=3073s)\n",
        "\n",
        "[Conservation and scrambling of quantum information](https://www.youtube.com/watch?v=BJ1Teu-ZK8M&list=WL&index=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "5HgnJHpsWbAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Research with QML and QKD for Black Holes*"
      ],
      "metadata": {
        "id": "qOXTdQWbu6wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Key Distribution + Quantum Machine Learning + Black Holes**\n",
        "\n",
        "1. **Simulate dynamics of black holes**\n",
        "  \n",
        "  * **Hawking radiation**: Use QML to develop better simulations of black hole evaporation (Hawking radiation)\n",
        "\n",
        "  * **Black hole states**: Use QML to develop new representations and evolution of black hole states, typically done using a quantum circuit, that are more efficient to simulate than traditional representations (the entanglement of the Hawking radiation particles is thought to be encoded in the complexity of the black hole state. This means that the quantum circuit that represents a black hole state would need to be very complex in order to capture the information that is encoded in the Hawking radiation)\n",
        "\n",
        "2. **Extract information from black holes**\n",
        "\n",
        "  * **Scan Hawking radiation (black hole evaporation)**\n",
        "  \n",
        "    * Use QML to develop new methods for detecting quantum correlations in Hawking radiation (that may contain information about objects that have fallen into)\n",
        "\n",
        "    * Use QML to develop new algorithms for learning symmetries and dynamics of black holes (better understand known ones, and learn new ones) to reduce complexity (Black holes are believed to have many symmetries, but these symmetries are not fully understood).\n",
        "    \n",
        "      * Study the entanglement structure of black hole states: Hawking radiation or electromagnetic radiation\n",
        "      \n",
        "      * Study the dynamics of black holes: gravitational waves or neutrinos\n",
        "\n",
        "  * **Send messages across black hole event horizons (black hole complementarity)**: Develop new methods (protocols) to extract information from entangled states in Hawking radiation that are more robust to interferences with QKD signals (QKD could be used to create and distribute entangled states. Interferences come from high energy particles or gravitational effects on polarization states of entangled photons)\n",
        "\n",
        "> <font color=\"blue\">**Use QML to develop new algorithms for learning symmetries and dynamics of black holes with more robust QKD protocols**\n",
        "\n",
        "Use QML to develop automatic symmetry-learning techniques (find best methods to learn symmetries) that assist in designing more robust QKD protocols against various noise and attack scenarios\n"
      ],
      "metadata": {
        "id": "6pfK42egHjTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use QML to unscramble information from Black Holes which are quantum encryption devices**\n",
        "\n",
        "https://www.quantamagazine.org/new-calculations-show-how-to-escape-hawkings-black-hole-paradox-20230802/\n",
        "\n",
        "**They treated the black hole as a quantum encryption device — something that takes in legible information (normal matter) and spits out what appears to be scrambled information (the radiation).** In this context, one could imagine carrying out the AMPS experiment by using a **machine to unscramble the information — a machine like a quantum computer**. And with a key result from Aaronson’s doctoral thesis on the limits of quantum computation, they discovered something curious.\n",
        "\n",
        "A black hole pulverizes infalling matter so thoroughly that if an astronaut actually tasked a quantum computer with unscrambling the radiation, the task would take eons. It would take so long that the black hole would be long gone before the progress bar reached even a fraction of 1%. And by then, the astronaut wouldn’t be able to jump in to catch outside information moonlighting on the inside, because the inside wouldn’t exist.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QD3z_denetrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetries in Black Holes**\n",
        "\n",
        "*Symmetries can also be used to relate the complexity of black hole states to other quantities of interest, such as the entropy or the information content of the black hole*.\n",
        "\n",
        "* For example, it has been shown that the complexity of a black hole state is equal to the entropy of the black hole up to a logarithmic correction. This suggests that complexity may be a more fundamental measure of a black hole's information content than entropy.\n",
        "\n",
        "* **Conformal symmetry:** By using conformal transformations of the coordinates, it is possible to map a black hole to a state with a simpler circuit description. This can be used to simplify the complexity calculations. This symmetry arises from the fact that black holes are solutions to the Einstein equation, which is conformally invariant. Conformal symmetry implies that the complexity of black hole states should be independent of certain transformations of the coordinates. This can be used to simplify the complexity calculations by fixing the coordinates in a particular way.\n",
        "\n",
        "* **Entanglement structure:** By understanding the entanglement structure of black hole states, it is possible to identify certain operations that can be performed efficiently. This can be used to reduce the computational complexity of certain tasks. Black hole states are highly entangled, and this **entanglement can be exploited to reduce the computational complexity of certain tasks**. For example, it has been shown that the complexity of the state of a black hole that is created by collapsing a massive particle is proportional to the entanglement entropy of the particle.\n",
        "\n",
        "* **Symmetry between black hole formation and evaporation:** There is a symmetry between the complexity of a black hole state that is created by collapsing a massive particle and the complexity of the state that is emitted from the black hole's horizon. This symmetry can be used to relate the complexity of black holes to their entropy and information content."
      ],
      "metadata": {
        "id": "5p98JYDcRzpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can quantum machine learning be used to quantum computational complexity of black holes?**\n",
        "\n",
        "Quantum machine learning, an emerging field combining quantum computing with machine learning techniques, could potentially be applied to explore complex phenomena such as black holes, particularly in understanding their quantum computational complexity. Here's how this could work:\n",
        "\n",
        "1. **Quantum Computational Complexity of Black Holes:** This concept relates to understanding how complex the quantum states of black holes can be, especially in the context of the information paradox and the nature of the event horizon (as in the firewall hypothesis). Quantum computational complexity in this context refers to the number of quantum operations needed to create a particular state from a simple reference state.\n",
        "\n",
        "2. **Quantum Machine Learning Algorithms:** Quantum machine learning algorithms could potentially model and analyze the quantum states associated with black holes. These algorithms can process and analyze information in ways that are fundamentally different from classical algorithms, possibly offering new insights into the highly complex quantum states of black holes.\n",
        "\n",
        "3. **Simulation and Modeling:** Quantum computers could simulate aspects of black hole physics that are otherwise extremely challenging for classical computers, such as the entanglement structure of Hawking radiation or the dynamics of spacetime near the event horizon.\n",
        "\n",
        "4. **Pattern Recognition and Data Analysis:** Quantum machine learning can excel in identifying patterns in large datasets. If applicable to data obtained from astronomical observations or theoretical simulations related to black holes, it could uncover new insights into their properties and behavior.\n",
        "\n",
        "5. **Exploring Quantum Gravity Theories:** Insights gained from quantum machine learning could potentially contribute to theories of quantum gravity, a field that seeks to reconcile general relativity (which describes black holes) with quantum mechanics.\n",
        "\n",
        "6. **Challenges and Limitations:** It's important to note that practical quantum computing, especially at a scale necessary to tackle such complex problems, is still in its infancy. Furthermore, the theoretical understanding required to directly apply quantum machine learning to black hole computational complexity is highly advanced and speculative.\n",
        "\n",
        "7. **Interdisciplinary Collaboration:** This kind of research would likely require close collaboration between quantum physicists, computer scientists, and astrophysicists, given the highly interdisciplinary nature of the problem.\n",
        "\n",
        "In summary, while theoretically possible, the application of quantum machine learning to understand the quantum computational complexity of black holes is still largely in the realm of speculative and cutting-edge research. As both fields of quantum computing and black hole physics evolve, more practical applications might emerge in the future.\n",
        "\n",
        "****\n",
        "\n",
        "Yes, quantum machine learning (QML) has the potential to be used to calculate the quantum computational complexity of black holes. QML is a new field that combines the principles of quantum mechanics with machine learning. It has the potential to solve problems that are intractable for classical computers.\n",
        "\n",
        "One way that QML could be used to calculate the quantum computational complexity of black holes is to develop algorithms that can learn the structure of black hole states from data. For example, an algorithm could be trained on a dataset of black hole states, and then it could be used to calculate the complexity of new black hole states.\n",
        "\n",
        "Another way that QML could be used to calculate the quantum computational complexity of black holes is to develop algorithms that can efficiently simulate the evolution of black hole states. This could be done by using QML to develop new representations of black hole states that are more efficient to simulate than traditional representations.\n",
        "\n",
        "Researchers are currently exploring a number of different ways to use QML to calculate the quantum computational complexity of black holes. These efforts are still in their early stages, but they have the potential to lead to new insights into the nature of black holes and quantum gravity.\n",
        "\n",
        "Here are some specific examples of how QML could be used to calculate the quantum computational complexity of black holes:\n",
        "\n",
        "* **Train a machine learning model on a dataset of black hole states, and then use the model to calculate the complexity of new black hole states.**\n",
        "\n",
        "* **Develop a new quantum circuit representation for black hole states that can be efficiently simulated by a quantum computer, and then use QML to develop algorithms for simulating these circuits.**\n",
        "\n",
        "* **Use QML to develop new algorithms for estimating the probability of a particular outcome of a quantum computation, and then use these algorithms to estimate the complexity of black hole states.**\n",
        "\n",
        "Research in this area is still in its early stages, but it has the potential to revolutionize our understanding of black holes and quantum gravity. As quantum computers become more powerful, QML is likely to play an increasingly important role in our quest to understand the universe at its most fundamental level."
      ],
      "metadata": {
        "id": "RbqvlS1PvGeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Black Holes as (Quantum) Computers*"
      ],
      "metadata": {
        "id": "GwAZXBb_beZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Powerful X-rays emitted near these black holes have properties that make them ideal information carriers for quantum computing\n",
        "\n",
        "* If the star that forms it rotates, as most stars do, the black hole will also spin.\n",
        "* Material that gets close to a rotating black hole but does not fall into it will aggregate into a circular structure known as an accretion disk. Powerful forces acting on accretion disks raise their temperature so they emit X-rays, which can act as carriers of quantum information.\n",
        "* The photons that make up the X-rays have two properties: polarisation and orbital angular momentum. Each of these can encode a qubit (quantum bit) of information, the standard information unit in quantum computing. \"Lab-based researchers already use beam splitters and prisms to entangle these properties in X-ray photons and process quantum information,\" says Racorean. \"It now seems that the curvature of spacetime around a black hole will play the same role as this apparatus.\"\n",
        "* Thus far, however, this process is only a prediction. The final proof will come when the properties of X-rays near spinning black holes are observed, which could happen in the next decade.\n",
        "* Two space probes with the same mission will be launched around 2022: the Imaging X-ray Polarimetry Explorer (IXPE) by NASA, and the X-ray Imaging Polarimetry Explorer (XIPE) by the European Space Agency. These will investigate the polarisation of all X-rays found in space, including those emitted close to black holes. \"If we find that the X-ray polarisation changes with distance from the black hole, with those in the central region being least polarised, we will have observed entangled states that can carry quantum information,\" says Racorean.\n",
        "* This topic may seem esoteric, but it could have practical applications. **\"One day, we may even be able to use rotating black holes as quantum computers by sending [X-ray] photons on the right trajectory around these ghostly astronomical bodies,\"** Racorean concludes.\n",
        "* Additionally, scientists believe that simulation of unusual states of matter will be an important early application of quantum computing, and there are few more unusual states of matter than those found in the vicinity of black holes.\n",
        "\n",
        "https://phys.org/news/2017-11-black-holes-spacetime-quantum.amp"
      ],
      "metadata": {
        "id": "7fPBrXRFasEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Symmetries (equivariances or invariances) in quantum computational complexity of black holes*"
      ],
      "metadata": {
        "id": "8fYo65BMPmqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In quantum computational complexity of black holes, are there any symmetries (equivariances or invariances) that can be exploited?**\n",
        "\n",
        "Yes, there are several symmetries (equivariances or invariances) that can be exploited in the quantum computational complexity of black holes. These symmetries can be used to simplify the problem of calculating the complexity of black hole states and to gain new insights into the relationship between black holes and quantum information theory.\n",
        "\n",
        "One of the most important symmetries of black holes is their conformal symmetry. This symmetry arises from the fact that black holes are solutions to the Einstein equation, which is conformally invariant. Conformal symmetry implies that the complexity of black hole states should be independent of certain transformations of the coordinates. This can be used to simplify the complexity calculations by fixing the coordinates in a particular way.\n",
        "\n",
        "Another important symmetry of black holes is their entanglement structure. Black hole states are highly entangled, and this entanglement can be exploited to reduce the computational complexity of certain tasks. For example, it has been shown that the complexity of the state of a black hole that is created by collapsing a massive particle is proportional to the entanglement entropy of the particle.\n",
        "\n",
        "Symmetries can also be used to relate the complexity of black hole states to other quantities of interest, such as the entropy or the information content of the black hole. For example, it has been shown that the complexity of a black hole state is equal to the entropy of the black hole up to a logarithmic correction. This suggests that complexity may be a more fundamental measure of a black hole's information content than entropy.\n",
        "\n",
        "The study of symmetries in the quantum computational complexity of black holes is an active area of research. As we learn more about these symmetries, we will be able to gain a better understanding of the relationship between black holes and quantum information theory. This could ultimately lead to new insights into the nature of black holes and quantum gravity.\n",
        "\n",
        "Here are some specific examples of how symmetries can be exploited in the quantum computational complexity of black holes:\n",
        "\n",
        "* **Conformal symmetry:** By using conformal transformations of the coordinates, it is possible to map a black hole to a state with a simpler circuit description. This can be used to simplify the complexity calculations.\n",
        "\n",
        "* **Entanglement structure:** By understanding the entanglement structure of black hole states, it is possible to identify certain operations that can be performed efficiently. This can be used to reduce the computational complexity of certain tasks.\n",
        "\n",
        "* **Symmetry between black hole formation and evaporation:** There is a symmetry between the complexity of a black hole state that is created by collapsing a massive particle and the complexity of the state that is emitted from the black hole's horizon. This symmetry can be used to relate the complexity of black holes to their entropy and information content.\n",
        "\n",
        "Symmetries play a crucial role in understanding the quantum computational complexity of black holes. By exploiting these symmetries, we can gain new insights into the nature of black holes and quantum information theory."
      ],
      "metadata": {
        "id": "cSAnOF4gPxVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you represent conformal symmetry in a quantum circuit?**\n",
        "\n",
        "Yes, conformal symmetry can be represented in a quantum circuit using a specific type of gate called a conformal gate. Conformal gates are unitary operators that preserve the conformal group, which is the group of transformations that leave the conformal structure of a spacetime invariant.\n",
        "\n",
        "**Conformal Gates**\n",
        "\n",
        "Conformal gates are typically represented as a combination of two-qubit gates, such as the controlled-Z gate (CZ) and the controlled-Hadamard gate (CH). These gates can be used to implement the conformal group, which consists of dilations, translations, and special conformal transformations.\n",
        "\n",
        "**Dilation**\n",
        "\n",
        "A dilation is a transformation that increases or decreases the size of a system. In quantum mechanics, this corresponds to a scaling of the Hamiltonian. A dilation can be implemented using a CZ gate followed by a Hadamard gate (H), as shown below:\n",
        "\n",
        "```\n",
        "CZ |q0, q1⟩ ⇒ |q0, q0⟩\n",
        "H |q0⟩ ⇒ |+⟩\n",
        "```\n",
        "\n",
        "This circuit transforms the state |q0, q1⟩ into |+, +⟩, which is the maximally mixed state on two qubits. This represents a dilation of the system, as the entropy of the state has increased.\n",
        "\n",
        "**Translation**\n",
        "\n",
        "A translation is a transformation that moves a system to a different location. In quantum mechanics, this corresponds to adding a constant offset to the Hamiltonian. A translation can be implemented using a CZ gate followed by a Pauli X gate (X), as shown below:\n",
        "\n",
        "```\n",
        "CZ |q0, q1⟩ ⇒ |q1, q0⟩\n",
        "X |q1⟩ ⇒ |-⟩\n",
        "```\n",
        "\n",
        "This circuit transforms the state |q0, q1⟩ into |-, -⟩, which is the orthogonal complement of the maximally mixed state. This represents a translation of the system, as the state has moved to a different point on the Bloch sphere.\n",
        "\n",
        "**Special Conformal Transformation**\n",
        "\n",
        "A special conformal transformation is a combination of a translation and a dilation. In quantum mechanics, this corresponds to a squeezing of the system. A special conformal transformation can be implemented using a CZ gate followed by a combination of H and X gates, as shown below:\n",
        "\n",
        "```\n",
        "CZ |q0, q1⟩ ⇒ |q0, q1⟩\n",
        "H |q1⟩ ⇒ |-⟩\n",
        "X |-⟩ ⇒ |+⟩\n",
        "```\n",
        "\n",
        "This circuit effectively transforms the state |q0, q1⟩ back to itself, but it has introduced some non-classical correlations between the qubits. This represents a squeezing of the system, as the state has become more entangled.\n",
        "\n",
        "**Combining Conformal Gates**\n",
        "\n",
        "Conformal gates can be combined to implement more complex conformal transformations. For example, the following circuit implements a combination of a dilation and a special conformal transformation:\n",
        "\n",
        "```\n",
        "CZ |q0, q1⟩ ⇒ |q0, q0⟩\n",
        "H |q0⟩ ⇒ |+⟩\n",
        "X |+⟩ ⇒ |-⟩\n",
        "X |-⟩ ⇒ |+⟩\n",
        "```\n",
        "\n",
        "This circuit transforms the state |q0, q1⟩ into |+, -⟩, which is a partially entangled state. This represents a combination of a dilation and a squeezing of the system.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Conformal gates can be used to represent conformal symmetry in a quantum circuit. These gates can be used to implement the conformal group, which consists of dilations, translations, and special conformal transformations. Conformal gates have applications in various fields, including quantum field theory, quantum gravity, and condensed matter physics."
      ],
      "metadata": {
        "id": "gQT79SohR-JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Black Hole Information Paradox*"
      ],
      "metadata": {
        "id": "P4NJL0kOLbfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution: Quantengravitationskorrekturen\n",
        "\n",
        "https://www.derstandard.de/story/2000145317520/schwarze-loecher-zeigen-ihr-weiches-haar-jetzt-oeffentlich\n",
        "\n",
        "* Noch ist unklar, wie sich die Theorie überprüfen lassen könnte. Angesichts vieler neuer Informationen über Schwarze Löcher, vor allem durch die neuen Gravitationswellenexperimente, scheint eine künftige Überprüfung zumindest nicht ganz aussichtslos.\n",
        "\n",
        "* Calmet selbst regt an, Modelle Schwarzer Löcher im Labor nachzubauen. Der Bereich der \"Quantensimulation\" ist tatsächlich eine der spannendsten Anwendungen moderner Quantentechnologie.\n",
        "\n",
        "* Auch die Simulation von Effekten der Teilchenphysik, wie sie hier nötig wären, [gelang bereits](https://www.derstandard.at/story/2000039596758/innsbrucker-quantencomputer-simuliert-phaenomen-der-teilchenphysik). Es besteht also Hoffnung, dass die Aussage der Forscher, im Gegensatz zu vielen anderen, die den Grenzbereich von Quantenphysik und Relativitätstheorie betreffen, doch experimentell überprüfbar ist. (Reinhard Kleindl, 8.4.2023)\n",
        "\n",
        "> Science: [Quantum gravitational corrections to particle creation by black holes](https://www.sciencedirect.com/science/article/pii/S0370269323001545)"
      ],
      "metadata": {
        "id": "layD8eDpdwkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Black hole information paradox: **if black holes radiate a perfect thermal spectrum then, by definition, that radiation has maximum entropy and contains no information about whatever fell into the black hole**. The black holes evaporates and all information that went into is deleted. This breaks rule of conservation of quantum information.\n",
        "\n",
        "What if all the information they eat is trapped forever in the tiny Planck relic? However that would break another rule: the Bekenstein bound (max of information that a region of space can contain)."
      ],
      "metadata": {
        "id": "clcxWo4_eQbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1481.jpg)"
      ],
      "metadata": {
        "id": "6kL2b_sbTvYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The [black hole information paradox](https://en.m.wikipedia.org/wiki/Black_hole_information_paradox) is a puzzle that appears when the predictions of quantum mechanics and general relativity are combined. The theory of general relativity predicts the existence of black holes that are regions of spacetime from which nothing — not even light — can escape.\n",
        "\n",
        "* In the 1970s, Stephen Hawking applied the rules of quantum mechanics to such systems and found that an isolated black hole would emit a form of radiation called Hawking radiation. Hawking also argued that the detailed form of the radiation would be **independent of the initial state of the black hole and would depend only on its mass, electric charge and angular momentum (=No hair conjecture)**.\n",
        "\n",
        "* The information paradox appears when one considers a process in which a black hole is formed through a physical process and then evaporates away entirely through **= Hawking radiation**.\n",
        "\n",
        "* Hawking's calculation suggests that the final state of radiation would retain information only about the total mass, electric charge and angular momentum of the initial state. Since many different states can have the same mass, charge and angular momentum this suggests that many initial physical states could evolve into the same final state. Therefore, information about the details of the initial state would be permanently lost.\n",
        "\n",
        "* However, this violates a core precept of both classical and quantum physics—that, in principle, the state of a system at one point in time should determine its value at any other time. Specifically, in quantum mechanics the state of the system is encoded by its wave function. The evolution of the wave function is determined by a unitary operator, and unitarity implies that the wave function at any instant of time can be used to determine the wave function either in the past or the future.\n",
        "\n",
        "* It is now generally believed that information is preserved in black-hole evaporation."
      ],
      "metadata": {
        "id": "SjA9CngZDwbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Black hole information paradox**\n",
        "\n",
        "[What Survives Inside A Black Hole?](https://www.youtube.com/watch?v=GscfuQWZFAo)\n",
        "\n",
        "Conservation of quantum information, Gateway to blackhole thermodynamics and holographic principle\n",
        "\n",
        "- Mass, electric charge and angular momentum: only three properties to describe a black hole: no hair conjecture (Jakob bekenstein)\n",
        "- the inside of the black hole is disconnected from the outside, cannot influence. We know only the three above information\n",
        "- These properties are remembered in the curvature of spacetime, in the gravitational field, according to Gauss’s law for gravity (like in electric fields), that’s why we at least have these information about the black hole\n",
        "- Gauss’s law for gravity: THE GrAVITATIONAL FLUX ThROUGh ANY CLOSED SURFACE IS PROPORTIONAL т O tHE ENciOSED MASS. the distribution of the matter is irrelevant (as a point, or along a region)\n",
        "- Since graviational force and electric force are infinite, the mass and charge of any region in space are remembered in the field (see inverse inverse square law)\n",
        "- Same for angular momentum: you get a magnetic field when an electric field changes. Frame dragging changes the shape of the event horizon and the orbit of anything nearby. If material with angular momentum (spinning star, rotating gas) falls in a black hole it will add or subtract from the flow of space above the event horizon. Its angular momentum is remembered in the frame dragging!! (Gravity probe B)\n",
        "- But something seems too get lost: number of particles of different types, like balance between quarks and antiquarks, represented by Baryon number (which is a conserved quantity)"
      ],
      "metadata": {
        "id": "uIBWis-eN7yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Closed Timeline Curves (CTC), Ergosphere, Retrocausality and Superdeterminism*"
      ],
      "metadata": {
        "id": "Dgy31jigwHId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.scinexx.de/news/physik/ein-quantentornado-als-raumzeit-simulator/"
      ],
      "metadata": {
        "id": "Id2e6ssMRN4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How time and space switch in the ergosphere [How Black holes spin space time](https://youtu.be/UjgGdGzDFiM?si=i3q0LFySMKns9CV9)"
      ],
      "metadata": {
        "id": "q-23-lCWRXKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Ergosphäre\n",
        "\n",
        "https://www.astro.umd.edu/~richard/ASTRO350/Astro350_2020_class09.pdf\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0550321320301747"
      ],
      "metadata": {
        "id": "skb7hbj3a-L2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Physiker simulieren Zeitreisen mit Quantenpartikeln\n",
        "\n",
        "* https://t3n.de/news/physiker-simulieren-zeitreisen-quantenpartikeln-1585445/\n",
        "\n",
        "* [APS - Nonclassical Advantage in Metrology Established via Quantum Simulations of Hypothetical Closed Timelike Curves](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.150202)"
      ],
      "metadata": {
        "id": "4-NUqdJCZu9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Closed_timelike_curve\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Retrocausality\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Superdeterminism"
      ],
      "metadata": {
        "id": "kn3NToI5wNFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Holographic Principle, Ads/CFT and Kerr/CFT correspondence*"
      ],
      "metadata": {
        "id": "DOOx_crf83HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SYK model: https://en.m.wikipedia.org/wiki/Sachdev–Ye–Kitaev_model"
      ],
      "metadata": {
        "id": "r1ZxrtXNQBHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Holographic Principle, Ads/CFT and Kerr/CFT correspondence**\n",
        "\n",
        "1. **Holographic Principle**: This is a fundamental idea that suggests that all of the information contained within a given volume of space can be represented by information on the boundary of that space. Think of it as a kind of \"hologram\" where a 2D surface can contain all the information about a 3D volume. This principle was inspired by the behavior of black holes, where the entropy (a measure of information) of a black hole is proportional to its surface area and not its volume.\n",
        "\n",
        "2. **AdS/CFT correspondence**: This is a specific realization of the holographic principle. It's a conjectured relationship between two types of physical theories:\n",
        "    - AdS: Anti-de Sitter space, which is a type of space in gravity theories.\n",
        "    - CFT: Conformal Field Theories, which are quantum field theories defined on the boundary of the AdS space.\n",
        "\n",
        "  * The AdS/CFT correspondence posits that a quantum field theory on the boundary (CFT) is equivalent to a gravity theory in the bulk (AdS). This provides a powerful duality that allows for the study of difficult problems in one theory using the tools of the other. In essence, while the holographic principle is a general idea about the nature of information and space, AdS/CFT is a specific, concrete example that ties together quantum mechanics and gravity.\n",
        "\n",
        "3. **Kerr/CFT Correspondence**\n",
        "  * [Kerr/CFT Correspondence (wiki)](https://en.m.wikipedia.org/wiki/Kerr/CFT_correspondence)\n",
        "  * [The Kerr/CFT Correspondence](https://arxiv.org/abs/0809.4266)"
      ],
      "metadata": {
        "id": "HRuHg-4_81ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Überraschende Entdeckung: Gleichartiges Leuchten von schwarzen Löchern stellt Theorien infrage\n",
        "\n",
        "https://t3n.de/news/ueberraschende-entdeckung-schwarze-loecher-geichartiges-leuchten-bisherige-theorie-1594532/\n",
        "\n",
        "https://arxiv.org/abs/2307.13872"
      ],
      "metadata": {
        "id": "2HXUAGyzPAq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Landauer-Prinzip vs Nernst-Theorem*"
      ],
      "metadata": {
        "id": "I2xPw5u9vGWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wie man Daten am Quantencomputer zuverlässig löscht\n",
        "\n",
        "Die Frage beim Löschen eines Qubits sei nun, zu welchen Kosten das erfolgt: \"Sind diese endlich, wie es das [Landauer-Prinzip](https://de.m.wikipedia.org/wiki/Landauer-Prinzip) sagt, oder dem [Nernst-Theorem](https://de.m.wikipedia.org/wiki/Dritter_Hauptsatz_der_Thermodynamik) folgend unendlich?\", so Huber. Wie der Quantenphysiker mit seinem Team nun herausfand, ist im Prinzip der unendliche Energieeinsatz nur notwendig, wenn man in endlicher Zeit kühlen bzw. löschen will. Würde man unendlich lang Zeit haben, könnte man auf diesen erforderlichen unendlichen Energieeinsatz verzichten.\n",
        "\n",
        "https://futurezone.at/amp/science/daten-quantencomputer-loeschen-qubit-wien-tu-technische-universitaet-nullpunkt/402414446\n",
        "\n",
        "https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.010332"
      ],
      "metadata": {
        "id": "Kp2FOzw0vOQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Unruh-Effect (Unruh Radiation and Rindler horizon)*"
      ],
      "metadata": {
        "id": "y4YO0LQ4A4IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unruh-Effect (Unruh Radiation)**\n",
        "\n",
        "* Je schneller man sich bewegt, desto mehr Teilchen mit erhöhter Temperatur entstehen vor einem bewegenden Beobachter (befindet sich im Wärmebad). Diese Teilchen kann ein ruhender Beobachter nicht sehen.\n",
        "\n",
        "* **Ist ein relativistisches Phänomen: nicht nur Zeit und Distanz hängen vom Beobachter ab, sondern auch die Existenz von Teilchen**\n",
        "\n",
        "* ist rein theoretisch, hat noch nieman beobachtet.\n",
        "\n",
        "* Schwarze Löcher strahlen Teilchen aus. Unruh-Effekt interessant fur komplizierte Phänomene wie Hawking-Strahlung oder expandierendes Universum\n",
        "\n",
        "* **sowohl bei Hawking-Strahlung als auch bei Unruh-Effekt scheinen Teilchen aus dem Nichts zu entstehen, wodurch das Vakuum eine endliche Temperatur erhält**."
      ],
      "metadata": {
        "id": "iC7r8vzaiiO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Unruh-Effekt](https://de.m.wikipedia.org/wiki/Unruh-Effekt) ist eine Vorhersage der Quantenfeldtheorie: Ein im Vakuum gleichmäßig beschleunigter Beobachter oder Detektor sieht anstelle des Vakuums ein Gas von Teilchen (Photonen, Elektronen, Positronen, …) mit einer Temperatur T, die proportional zur Beschleunigung ist.\n",
        "\n",
        "* nature of quantum fields seems to change as the observer is accelerating\n",
        "* accelerating cuts of causal access to a region in the universe and **creates a type of event horizon ([Rindler horizon](https://en.m.wikipedia.org/wiki/Rindler_coordinates))**\n",
        "* the presence of horizons distorts the quantum vacuum in a way that can create particles (Hawking radiation) -> Unruh effect\n",
        "* created even horizon behind you can't catch you, but its Unruh radiation can!\n",
        "* accelerating observers find themselves in a warm bath of particles\n",
        "* Unruh-deWitt-detector to detect particles - **the existence of partices is observer-dependent!**\n",
        "* [The Unruh Effect](https://www.youtube.com/watch?v=7cj6oiFDEXc&list=WL&index=11)\n",
        "\n",
        "\n",
        "https://physicsworld.com/a/black-holes-could-reveal-their-quantum-superposition-states-new-calculations-reveal/"
      ],
      "metadata": {
        "id": "k3Jd1qq8fX7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *No hair conjecture (Jakob Bekenstein)*"
      ],
      "metadata": {
        "id": "aZ3xS4t7CWRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The [no-hair theorem](https://en.m.wikipedia.org/wiki/No-hair_theorem) states that **all stationary black hole solutions** of the Einstein–Maxwell equations of gravitation and electromagnetism in general relativity can be **completely characterized by only three independent externally observable classical parameters: mass, electric charge, and angular momentum**.\n",
        "\n",
        "* Other characteristics (such as geometry and magnetic moment) are uniquely determined by these three parameters, and all other information (for which \"hair\" is a metaphor) about the matter that formed a black hole or is falling into it \"disappears\" behind the black-hole event horizon and **is therefore permanently inaccessible to external observers after the black hole \"settles down\" (by emitting gravitational and electromagnetic waves)**.\n",
        "\n",
        "* Physicist John Archibald Wheeler expressed this idea with the phrase \"black holes have no hair\", which was the origin of the name.\n",
        "\n",
        "\n",
        "Mass, electric charge and angular momentum: only three properties to describe a black hole: no hair conjecture (Jakob bekenstein)\n",
        "the inside of the black hole is disconnected from the outside, cannot influence. We know only the three above information"
      ],
      "metadata": {
        "id": "hf4RmooSCYnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hawking Radiation*"
      ],
      "metadata": {
        "id": "gj5tJ7BnBLC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Hawking Radiation (Black Hole Radiation)*](https://de.m.wikipedia.org/wiki/Hawking-Strahlung)\n",
        "\n",
        "* Schwarze Löcher verlieren Energie\n",
        "\n",
        "* Schwarze Löcher geringer Masse sind von geringer Ausdehnung, d. h., sie haben einen kleineren Schwarzschildradius.\n",
        "\n",
        "* Die den Ereignishorizont umgebende Raumzeit ist entsprechend stärker gekrümmt. Je größer und damit massereicher ein Schwarzes Loch ist, desto weniger strahlt es also.\n",
        "\n",
        "* Je kleiner ein Schwarzes Loch ist, umso höher ist seine Temperatur und aufgrund stärkerer Hawking-Strahlung verdampft es umso schneller.\n",
        "\n",
        "* [Nature: Quantum simulation of black-hole radiation](https://www.nature.com/articles/d41586-019-01592-x)"
      ],
      "metadata": {
        "id": "sNuPQnPNB0jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hawking Radiation & Destruction of Information (No hair theorem)**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/No-hair_theorem\n",
        "\n",
        "Destruction of Information by Hawking radiation\n",
        "\n",
        "https://www.youtube.com/watch?v=9XkHBmE-N34\n",
        "\n",
        "[Warum Quanteninformationen niemals zerstört wird](https://www.youtube.com/watch?v=HF-9Dy6iB_4)\n",
        "\n",
        "**No hair theorem**: black hole shields every information away except mass, electrci charge and angular momentu -> informatoon is still there, just not accessible. but Hawking radiation seems to evaporate all information..\n",
        "\n",
        "\n",
        "Video: [Hawking Radiation](https://www.youtube.com/watch?v=qPKj0YnKANw)\n",
        "\n",
        "[Hawking-Strahlung](https://de.m.wikipedia.org/wiki/Hawking-Strahlung): postulierte Strahlung Schwarzer Löcher\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1264.png)\n",
        "\n",
        "Hawking radiation has wavelengths of the size of the event horizon (the size of the entire black hole). These are the de Broglie wavelenghts of created particles:\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1265.png)\n",
        "\n",
        "**Important**:\n",
        "\n",
        "* large black holes (event horizon several kilometers long) have very long wavelenght of Hawking radiation, just photons - kilometer-long electromagnetic, low energy radio-waves  - they are cold and leak energy very slowly.\n",
        "\n",
        "* the smaller the black hole gets, the shorter the wavelengths and the higher the energy, and the thermal enegry heats up. Evaporation rate increases. It's a runnaway process\n",
        "\n",
        "* the end of probably a Planck relic (tiny black holes in size of planck length) that either create an own inflation with a new universe, or evaporate\n",
        "\n",
        "* Video: [What If (Tiny) Black Holes Are Everywhere?](https://www.youtube.com/watch?v=srVKjWn26AQ&list=WL&index=4)\n",
        "\n",
        "They tell us that there is an enormous uncertainty in the location of these particles. Hawking radiation must appear to come from the global black hole, not from specific points at the event horizon\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1266.png)\n",
        "\n",
        "This radiation is visible only to distant observers, but not if you fall into the black hole (then you see flat vaccum space). Only if you hover fixed distanced above the horizon, then you see partices. This is due to the [Unruh radiation](https://de.m.wikipedia.org/wiki/Unruh-Effekt).\n",
        "\n"
      ],
      "metadata": {
        "id": "jTf6z42jo76R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1342.jpg)"
      ],
      "metadata": {
        "id": "2dj913yMCipe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Bekenstein-Bound, Bekenstein-Hawking-Entropie, and Holografische Prinzip*"
      ],
      "metadata": {
        "id": "XW6H46KpBVW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bekenstein bound**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Bekenstein_bound\n",
        "\n",
        "Black hole information paradox: if black holes radiate a perfect thermal spectrum then,  by definition, that radiation has maximum entropy and contains no information about whatever fell into the black hole. The black holes evaporates and all information that went into is deleted. This breaks rule of conservation of quantum information.\n",
        "\n",
        "What if all the information they eat is trapped forever in the tiny Planck relic? However that would break another rule: the Bekenstein bound (max of information that a region of space can contain).\n",
        "\n",
        "\n",
        "*According to the Bekenstein bound, the entropy of a black hole is proportional to the number of Planck areas that it would take to cover the black hole's event horizon.*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Bekenstein-Hawking_entropy_of_a_black_hole.jpg/640px-Bekenstein-Hawking_entropy_of_a_black_hole.jpg)\n",
        "\n",
        "A way around this has been proposed - what if space inside black holes actually expands to a region larger than the event horizon? What if at the singularity of a black hole a new inflation is triggered. Then there is enough space."
      ],
      "metadata": {
        "id": "fq2o4mluelgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bekenstein-Hawking-Entropie & Holografische Prinzip**\n",
        "\n",
        "[*Bekenstein-Hawking-Entropie*](https://de.m.wikipedia.org/wiki/Bekenstein-Hawking-Entropie)\n",
        "\n",
        "* Heuristische Überlegungen führten J. D. Bekenstein bereits 1973 zu der Hypothese, dass die Oberfläche des Ereignishorizontes ein Maß für die Entropie eines Schwarzen Loches sein könnte, siehe Bekenstein-Hawking-Entropie.\n",
        "\n",
        "* Die Bekenstein-Hawking-Entropie war eine Motivation für das [Holografische Prinzip](https://de.m.wikipedia.org/wiki/Holografisches_Prinzip).\n"
      ],
      "metadata": {
        "id": "BAJVWziCxtWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The inverse square law**\n",
        "\n",
        "* Gauss's law is a more general form of this. It applies to any shaped surface surrounding any shaped mass charge.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1269.png)"
      ],
      "metadata": {
        "id": "A2R7_zGXLYl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Killing Horizons Decohere Quantum Superpositions: Do Black Holes and the Cosmic Event Horizon create physical reality?*"
      ],
      "metadata": {
        "id": "nMRTXb5a56Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rindler horizin, kiling horizon\n",
        "\n",
        "Event horizon produce decoherence of quantum states, producing reality?\n",
        "Video: [Strange New Explanation for Why Quantum World Collapses Into Reality](https://www.youtube.com/watch?v=3_hi48l-cj8&list=WL&index=8)\n",
        "\n",
        "https://arxiv.org/abs/2301.00026 Killing Horizons Decohere Quantum Superpositions\n",
        "\n"
      ],
      "metadata": {
        "id": "Top6h_dkRvNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suggestion that horizons from black holes and the expansion of the universe cause the quantum collapse into reality\n",
        "\n",
        "* Ausgangspunkt: two entangled particles, one falls in back hole - according to general relativity they cannot decohere, because no information gets out, but that‘s not what happens according to quantum theory. maybe the edge of a black hole (event horizon) acts like an observer and decoheres the entangled state\n",
        "* Bringing this idea to the next level: The edge of the universe acts like an observer of everything that happens inside of it\n",
        "* Observation means here decoherence and brings things from quantum state into real physical state\n",
        "* Edge of universe is an event horizon (just opposite side to black holes): Rindler horizon (particularly the Killing horizon in this case)\n",
        "\n",
        "Video: https://youtu.be/3_hi48l-cj8\n",
        "\n",
        "Paper: Killing Horizons Decohere Quantum Superpositions  https://arxiv.org/abs/2301.00026"
      ],
      "metadata": {
        "id": "u8oJFM3N53sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Gravity - Quantum Cosmology & Quantum Complexity (Dump)*"
      ],
      "metadata": {
        "id": "AbTRWe0r27UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entanglement: Gravity’s long-distance connection Wormhole links between black holes could broker quantum-general relativity merger\n",
        "\n",
        "https://www.sciencenews.org/article/entanglement-gravitys-long-distance-connection"
      ],
      "metadata": {
        "id": "NGwdlAhRQhog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/renate-loll-blends-universes-to-unlock-quantum-gravity-20230525/"
      ],
      "metadata": {
        "id": "Cdf_URbpqfmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noncommutative geometry and cosmology\n",
        "\n",
        "there is also a book!\n",
        "\n",
        "http://www.its.caltech.edu/~matilde/CosmoBeijing2013.pdf\n",
        "\n",
        "https://arxiv.org/abs/gr-qc/0305077\n"
      ],
      "metadata": {
        "id": "WczJMFxaGXf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Physicists Think Gravity Creates Light**\n",
        "\n",
        "* Gravity into light: oscillations of inflaton field can produce this resonance in any field, including the gravitational fields of the primordial universe. Oscillating graviational fields are better known as gravitational waves.\n",
        "\n",
        "* If the inflation resonance coupled to the creation of gravitational waves in the early universe, this could create gravitational standing waves of incredibly high energy\n",
        "\n",
        "* in places where the refractive index of light is greater than 1 (= meaning that gravitational waves move faster than light waves), this can result in gravitational waves driving oscillations in the electromagnetic field, essentially creating a shock wave that generates spontaneous particles of light - light out of the fabric of spacetime. (Somehow analogous to Cherenkov radiation)\n",
        "\n",
        "* For a particle physics description, rather than a quantum field description: * think of a standing gravitational wave as a collective behavior of two groups of massless gravitons - particles of gravity - traveling in opposite directions. At high enough energies and densities, these can collide and decay into other particles, in this case photons. Just like a pair of high energy photons can collide and decay into an electron positron pair.\n",
        "\n",
        "* the process is very inefficient and probably only occured in early universe. Or (gravitational waves) exist today only in binary black hole systems, but the refractive index isn't usually greater than 1 in these systems, so it's difficult to create these shockwave-like environments.\n",
        "\n",
        "Video: [Why Physicists Think Gravity Creates Light](https://www.youtube.com/watch?v=LtTIE7C7me0)\n",
        "\n",
        "Paper: [Graviton to photon conversion via parametric resonance](https://www.sciencedirect.com/science/article/pii/S2212686423000365)"
      ],
      "metadata": {
        "id": "VPk6KpkSsPwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Group Field Theories for Quantum Gravity](https://youtu.be/aO3sNENdV1k)\n",
        "\n",
        "[Effective Spin Foam for quantum gravity](https://youtu.be/6n1iaT1bw1Y)"
      ],
      "metadata": {
        "id": "s35ijjuCHacG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sensing between particle’s intrinsic quantum spin and Earth’s gravitational field**\n",
        "\n",
        "how does a quantum spin interact with a gravitational field\n",
        "\n",
        "https://physics.aps.org/articles/v16/80\n",
        "\n",
        "A hunt for a spin-gravity interaction has implications for the existence of hypothetical forces of nature and for the origin of the Universe’s matter-antimatter asymmetry.\n",
        "\n",
        "https://mriquestions.com/why-at-larmor-frequency.html\n"
      ],
      "metadata": {
        "id": "ju1r0FkIziC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concepts**\n",
        "\n",
        "* A theory of the cosmological quantum state is an objective of quantum cosmology (but we've only got one system). A theory of a quantum state is a necessary part of any final theory [Source](https://www.youtube.com/watch?v=xcJ7diTtURc).\n",
        "\n",
        "* Hawking's [no-boundary wave function (NBWF)](https://web.physics.ucsb.edu/~quniverse/nbwf-form.html) is a theory of the quantum state of the universe. Such a state is a necessary part of any final theory along with a theory of quantum dynamics. In principle all predictions depend on both, and predictions of the large scale universe are observationally sensitive to its details. NBWF fluctuations start in their ground state. Essentially the [Bunch-Davies vacuum](https://en.m.wikipedia.org/wiki/Bunch–Davies_vacuum)\n",
        "\n",
        "* [Bunch–Davies vacuum](https://en.m.wikipedia.org/wiki/Bunch–Davies_vacuum): In quantum field theory in curved spacetime, there is a whole class of quantum states over a background de Sitter space which are invariant under all the isometries: the alpha-vacua. Among them there is a particular one whose associated Green functions verify a condition (Hadamard condition) consisting to behave on the light-cone as in flat space. This state is usually called the Bunch–Davies vacuum or Euclidean vacuum\n",
        "\n",
        "* [quantum field theory in curved spacetime (QFTCS)](https://en.m.wikipedia.org/wiki/Quantum_field_theory_in_curved_spacetime)"
      ],
      "metadata": {
        "id": "FolSJUgxM3q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.einstein-online.info/en/spotlight/quantum_cosmo_path_integrals/"
      ],
      "metadata": {
        "id": "Nn__-qpARqWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [APS Physics 2015 - Quantum Gravity and Quantum Cosmology](https://www.youtube.com/watch?v=xcJ7diTtURc)\n",
        "\n",
        "* Quantum theory of gravity means a quantum theory of spacetime geometry\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1482.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1483.png)\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1484.png)\n"
      ],
      "metadata": {
        "id": "mhyFfc16LGoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [PBS - Quantum Gravity and the Hardest Problem in Physics](https://www.youtube.com/watch?v=YNEBhwimJWs)\n",
        "\n",
        "* Non-renormalizability of general relativity: precise localized particles produce black holes (you need so much energy to localize below one planck length or one planck time, that it produces black holes. the more precise, the larger the black hole gets"
      ],
      "metadata": {
        "id": "NWEVty9lUJCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Event: Quantum Gravity and Quantum Cosmology\n",
        "\n",
        "https://meetings.aps.org/Meeting/APR15/Session/W1.2"
      ],
      "metadata": {
        "id": "g43DMitiCBp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Quantengravitation"
      ],
      "metadata": {
        "id": "Yq_BfEMg-na6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Quantum_cosmology"
      ],
      "metadata": {
        "id": "U_d9X5mHN3po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://link.springer.com/book/10.1007/978-3-642-33036-0"
      ],
      "metadata": {
        "id": "GQCbRUEx-2uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://hyperspace.uni-frankfurt.de/2020/01/29/phd-position-in-quantum-cosmology-and-quantum-gravity-sheffield-uk/"
      ],
      "metadata": {
        "id": "rfbwN8e0A7am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosmological consequences of Quantum Gravity proposals\n",
        "\n",
        "https://arxiv.org/abs/1804.02262"
      ],
      "metadata": {
        "id": "69rhcTNQA-mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The central object of interest in quantum cosmology is the wave function of a closed universe\n",
        "\n",
        "> $\n",
        "\\Psi\\left[h_{i j}(\\mathbf{x}), \\Phi(\\mathbf{x}), B\\right]\n",
        "$\n",
        "\n",
        "https://arxiv.org/pdf/0909.2566v1.pdf"
      ],
      "metadata": {
        "id": "0kb6n6uzO3dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/why-gravity-is-not-like-the-other-forces-20200615/"
      ],
      "metadata": {
        "id": "u07V5uko_KhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <font color=\"blue\">***Quantum Cosmology and Quantum Gravitation*** *(AdS/CFT Correspondence)*\n",
        "\n",
        "> <font color=\"blue\">***Theory of Everything*** *(Electroweak + Strong Nuclear Force + Gravitation)*"
      ],
      "metadata": {
        "id": "L2nptK1EqBuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quora.com/What-is-Quantum-Cosmology-Is-it-a-real-area-of-study-What-is-its-current-status"
      ],
      "metadata": {
        "id": "7MTjftEoOvgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Weltformel"
      ],
      "metadata": {
        "id": "_T3m-BQeqfRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRONG = 1\n",
        "\n",
        "ELECTROMAGNETIC = 1/137\n",
        "\n",
        "WEAK = 10^-6\n",
        "\n",
        "GRAVITY = 10^-38"
      ],
      "metadata": {
        "id": "rDpZ1cFET_s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/AdS/CFT_correspondence\n",
        "\n",
        "\n",
        "\"According to the AdS/CFT correspondence, the geometries of certain spacetimes are fully determined by quantum states that live on their boundaries-indeed, by the von Neumann entropies of portions of those boundary states. This work investigates to what extent the geometries can be reconstructed from the entropies in polynomial time. Bouland, Fefferman, and Vazirani (2019) argued that the AdS/CFT map can be exponentially complex if one wants to reconstruct regions such as the interiors of black holes. Our main result provides a sort of converse: we show that, in the special case of a single 1D boundary, if the input data consists of a list of entropies of contiguous boundary regions, and if the entropies satisfy a single inequality called Strong Subadditivity, then we can construct a graph model for the bulk in linear time. Moreover, the bulk graph is planar, it has O(N?) vertices (the information-theoretic minimum), and it's \"universal,\" with only the edge weights depending on the specific entropies in question. From a combinatorial perspective, our problem boils down to an \"inverse\" of the famous min-cut problem: rather than being given a graph and asked to find a min-cut, here we're given the values of min-cuts separating various sets of vertices, and need to find a weighted undirected graph consistent with those values. Our solution to this problem relies on the notion of a \"bulkless\" graph, which might be of independent interest for AdS/CFT. We also make initial progress on the case of multiple 1D boundaries-where the boundaries could be connected via wormholes- including an upper bound of O (N4) vertices whenever an embeddable bulk graph exists (thus putting the problem into the complexity class\n",
        "NP).\"\n",
        "-Scott Aaronson and Jason Pollack\n",
        "\n",
        "Discrete Bulk Reconstruction, arxiv: 2210.15601v2 [quant-ph] (2022). https://arxiv.org/pdf/2210.15601.pdf"
      ],
      "metadata": {
        "id": "DHHvq0bsJh2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Neil Turok - Some progress towards the Path Integral for Gravity](https://youtu.be/L17Cx-iD8uU)"
      ],
      "metadata": {
        "id": "4VPAPdz4YVeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Quantengravitation\n",
        "\n",
        "https://www.spektrum.de/news/quantengravitation-woraus-besteht-die-raumzeit/2007706\n",
        "\n",
        "Quantengravitationstheoretikerin Alejandra Castro von der Universität Amsterdam.\n",
        "\n",
        "*Supersymmetry (String Theory & AdS/CFT correspondence): Electroweak + Strong Nuclear Force + Gravitation ('Theory of Everything')*\n",
        "\n",
        "> **Quantum Field Theory and General Relativity are extremely well tested, but cannot be combined. This is a problem at things like Black Holes and Big Bang.**\n",
        "\n",
        "* Two electrons a centimeter apart would feel a repulsion from their electric charges, but attract each other due to their mutual gravity.\n",
        "\n",
        "* But the **force from the electric charge is $10^{24}$ higher than the attraction they feel from gravity**. That's 24 orders of magnitude bigger.\n",
        "\n",
        "* So any effect of gravity is lost down in the tenth forth decimal place of the electrostatic force. An incredible small effect.\n",
        "\n",
        "* So to test quantum gravity you need places with a huge amount of matter squeezed into very small volumes, which are always very high energy situations, like back holes or the Big Bang.\n",
        "\n",
        "* These are things we cannot create experimentally with our current technology.  To get to these energies we would need a particle accelerator like CERN, but the size of the solar system and with detectors the size of Jupiter. And to create enough energy to test quantum gravity, the experiment would actually end up turning into a black hole.\n",
        "\n",
        "* Currently we study this by using Gravitational Wave Astronomy (for Black Holes) and study the Cosmic Microwave Background (for Big Bang).\n",
        "\n",
        "**Quantum field theory in curved space time**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Physical_cosmology\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_cosmology\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Causal_sets\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_gravity\n",
        "\n",
        "* It is conceivable that, in the correct theory of quantum gravity, the infinitely many unknown parameters will reduce to a finite number that can then be measured. One possibility is that normal perturbation theory is not a reliable guide to the renormalizability of the theory, and that there really is a UV fixed point for gravity.\n",
        "\n",
        "* Since this is a question of non-perturbative quantum field theory, finding a reliable answer is difficult, pursued in the asymptotic safety program.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Background_independence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Composite_gravity\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Effective_field_theory\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Quantum_field_theory_in_curved_spacetime\n",
        "\n",
        "* Video: [Quantum Gravity | The Search For a Theory of Everything](https://www.youtube.com/watch?v=d-86tNCSJsg&t=241s)\n",
        "\n",
        "* Video: [Quantum Gravity: How quantum mechanics ruins Einstein's general relativity](https://www.youtube.com/watch?v=S3Wtat5QNUA)\n",
        "\n",
        "* Video: [Quantum Gravity and the Hardest Problem in Physics | Space Time](https://www.youtube.com/watch?v=YNEBhwimJWs)\n",
        "\n",
        "* Video: [Quantum Gravity / Fermilab](https://www.youtube.com/watch?v=CbPWYjnQIO8)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Higher-dimensional_supergravity\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Supergravity\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Weltformel\n",
        "\n",
        "**Graviton**\n",
        "\n",
        "* [Gravitons](https://de.m.wikipedia.org/wiki/Graviton) are tiny compared to a gravitational wave. Currently too small to be seen by Ligo and Virgo gravitational wave detectors\n",
        "\n",
        "* perhaps it will be impossible because we need to detect changes smaller than a Planck length 1.616255(18)x$10^{-35}$ meters, which is impossible according to quantum mechanics\n",
        "\n",
        "**Supersymmetry**\n",
        "\n",
        "* [Supersymmetry](https://en.m.wikipedia.org/wiki/Supersymmetry) is an integral part of string theory, a possible theory of everything.\n",
        "\n",
        "> Supersymmetrie: Vereinigung der Bosonen mit den Fermionen\n",
        "\n",
        "* Die [Supersymmetrie](https://de.m.wikipedia.org/wiki/Supersymmetrie) ist eine hypothetische Symmetrie der Teilchenphysik, die Bosonen (Teilchen mit ganzzahligem Spin) und Fermionen (diese haben halbzahligen Spin) ineinander umwan. delt und die dadurch das gleiche Energiespektrum haben.\n",
        "\n",
        "* Die Supersymmetrie verbindet ein Teilchen mit seinem Superpart. ner. Die einfachste Realisierung einer Supersymmetrie findet man in eindimensionalen quantenmechanischen Systemen mit Hamilton-Operatoren:\n",
        "\n",
        "> $\n",
        "\\hat{H}_{+}=\\hat{A}^{\\dagger} \\hat{A}, \\quad \\hat{H}_{-}=\\hat{A} \\hat{A}^{\\dagger}\n",
        "$ (Theoretical Physics book, Page 876)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Minimal_Supersymmetric_Standard_Model\n",
        "\n",
        "* The Minimal Supersymmetric Standard Model (MSSM) **is an extension to the Standard Model** that realizes supersymmetry.\n",
        "\n",
        "*  [Supersymmetry](https://en.m.wikipedia.org/wiki/Supersymmetry) is a [spacetime symmetry](https://en.m.wikipedia.org/wiki/Spacetime_symmetries) between two basic classes of particles:\n",
        "* bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and\n",
        "* fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Supersymmetry_algebra\n",
        "\n",
        "**ADS-CFT Correspondance & Holografisches Prinzip**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/AdS/CFT_correspondence\n",
        "\n",
        "**String Theory / M Theory** (incl. Supersymmetry and AdS/CFT correspondence)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/String_theory\n",
        "\n",
        "* Im Gegensatz zum Standardmodell der Teilchenphysik sind bei der Stringtheorie die bei fundamentalen Bausteine der Welt keine punktförmigen Teilchen, sondern vibrierende eindimensionale Objekte. Diese werden als „Strings\" bezeichnet. Die Elementarteilchen sin Schwingungsanregungen der Strings.\n",
        "\n",
        "**Loop Quantum Gravity**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Schleifenquantengravitation\n",
        "\n",
        "**Topological quantum field theory**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Topological_quantum_field_theory\n",
        "\n",
        "* Quantum gravity is believed to be background-independent (in some suitable sense), and TQFTs provide examples of background independent quantum field theories. This has prompted ongoing theoretical investigations into this class of models.\n"
      ],
      "metadata": {
        "id": "YgboF2J3nKOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Are gravity and spacetime emergent phanomena?*"
      ],
      "metadata": {
        "id": "fP_EL_HY_Xew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Are gravity and spacetime emergent phanomena?*\n",
        "\n",
        "* **It from Qubit**: Does spacetime emerge from entanglement? Can quantum computers simulate all physical phenomena? [Simons Collaboration on Quantum Fields, Gravity, and Information](https://web.stanford.edu/~phayden/simons/overview.pdf)\n",
        "* **Does gravity come from quantum information?** Paper: [Gravity from Quantum Information](https://arxiv.org/abs/1001.5445). The Einstein equation describes an information-energy relation during this process, which implies that entropic gravity is related to the quantum entanglement of the vacuum and has a quantum information theoretic origin.\n",
        "* **Can Gravity be emergent?** Video: [Ads/CFT correspondence and holographic Principle](https://www.youtube.com/watch?v=uj6wrT0nSZg): is entanglement on the boundary (since it determines  geometry within) causing gravity in 3d holographic universe? Idea comes from holographic Principle and Ads/CFT correspondence. It suggests that gravity in anti de sitter space might emerge from entanglement structure on its boundary.\n",
        "* **Spacetime is maybe emergent phenomena** (Unitarity 2.0: Isometry): [Physicists Rewrite a Quantum Rule That Clashes With Our Universe](https://www.quantamagazine.org/physicists-rewrite-a-quantum-rule-that-clashes-with-our-universe-20220926/): Isometrie, Amplituhedron und quantenphysik: [amplituhedron](https://de.m.wikipedia.org/wiki/Amplituhedron ) does not explicitly contain any notion of spacetime. Instead, it is a purely mathematical object. This suggests that spacetime may be an emergent phenomenon, arising from the geometry of the amplituhedron.\n",
        "  * There are many different amplituhedra that can be used to describe the scattering of the same particles. This suggests that spacetime may not be a fundamental property of the universe, but rather a way of organizing our observations of the amplituhedron.\n",
        "* Nota bene: AdS/CFT correspondence, which emerged from string theory, posits a “duality” between two extremely different-looking kinds of theories. On one side of the duality is AdS (Anti de Sitter): a theory of quantum gravity for a hypothetical universe that has a negative cosmological constant, effectively causing the whole universe to be surrounded by a reflecting boundary. On the other side is a CFT (Conformal Field Theory): an “ordinary” quantum field theory, without gravity, that lives only on the boundar y of the AdS space. The [AdS/CFT correspondence](https://www.pbs.org/wgbh/nova/article/holograms-black-holes-and-the-nature-of-the-universe/) , for which there’s now overwhelming evidence (though not yet a proof), says that any question about what happens in the AdS space can be translated into an “equivalent” question about the CFT, and vice versa. “Even if a quantum gravity theory seems ‘wild’—even if it involves nonlocality, wormholes, and other exotica—there might be a dual description of the theory that’s more ‘tame,’ and that’s more amenable to simulation by a quantum computer.”\n",
        "\n",
        "*Non-locality in Bell-theorem is proven. Does at least superdeterminism exist?*\n",
        "\n",
        "* Violation Bell ineuqlity experiment in 2015. it closed Locatility loophole and detection loophole - physicao reality of entsnglemenr proven. open: [superdeterminism](https://en.m.wikipedia.org/wiki/Superdeterminism)."
      ],
      "metadata": {
        "id": "hrzdeyoD_Z8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mixer of Experts(MOE) — Modern architecture for divide and concur learning\n",
        "\n",
        "https://medium.com/@sthanikamsanthosh1994/the-mixer-of-experts-moe-modern-architecture-for-divide-and-concur-learning-dbe10ffa8436"
      ],
      "metadata": {
        "id": "_J_MRflWh0Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum in High Energy Physics*"
      ],
      "metadata": {
        "id": "wagDIGtWD_f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantum computing could improve the accuracy and speed of these simulations, allowing astrophysicists to better understand the origins of the universe. Developing quantum sensors for astronomy: Quantum sensors, such as atom interferometers, can be used to measure gravity and acceleration with high precision.\n",
        "\n",
        "https://www.linkedin.com/pulse/quantum-computing-astrophysics-ajay-gokhale#:~:text=Quantum%20computing%20could%20improve%20the,and%20acceleration%20with%20high%20precision."
      ],
      "metadata": {
        "id": "AGZafQvaEFp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2101.05821"
      ],
      "metadata": {
        "id": "PsZSMfcZFI9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bootcamp.uxdesign.cc/quantum-computing-in-space-science-88d88f505bbb"
      ],
      "metadata": {
        "id": "bChWoIMqE37A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum Black Holes*"
      ],
      "metadata": {
        "id": "lzzwwpi2DUGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://news.umich.edu/whats-inside-a-black-hole-u-m-physicist-uses-quantum-computing-machine-learning-to-find-out/"
      ],
      "metadata": {
        "id": "wOwwsajZDeya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.106.062434"
      ],
      "metadata": {
        "id": "61nFnqzMDmaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2306.14817.pdf"
      ],
      "metadata": {
        "id": "rF_-dj_fDXoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Quantum High-Energy Physics*"
      ],
      "metadata": {
        "id": "XI4x12wxCMwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2307.03236"
      ],
      "metadata": {
        "id": "-Fh0dk8VCRhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">**Mathematics**"
      ],
      "metadata": {
        "id": "s2KbaipaE5zT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-1lpMAgXzD9"
      },
      "source": [
        "#### <font color=\"blue\">**Algebra 250**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Linear Algebra*"
      ],
      "metadata": {
        "id": "04vVZZU1Q4rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Linear algebra for Quantum Mechanics](https://www.youtube.com/watch?v=FF05fXg03A0)"
      ],
      "metadata": {
        "id": "ihAL0jVFhV5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *SVD vs PCA*"
      ],
      "metadata": {
        "id": "Ak7yEH2CwIV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a low rank matrix?**\n",
        "\n",
        "In linear algebra, the rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. This concept is important because it provides insight into the structure of the matrix, its null space, column space, etc.\n",
        "\n",
        "A \"low-rank\" matrix is a matrix where the rank is significantly less than the number of rows or columns. For example, if you have a 1000x1000 matrix (which could potentially have a rank up to 1000), but its rank is only 10, that matrix would be considered low-rank.\n",
        "\n",
        "Low-rank matrices are useful in many applications, including machine learning and data analysis. They are often used in techniques like principal component analysis (PCA) or singular value decomposition (SVD), where a high-dimensional dataset is approximated by a lower-dimensional one. This is possible because the data can often be accurately represented in a lower-dimensional space (i.e., with a low-rank matrix), which simplifies analysis and reduces computational complexity.\n",
        "\n",
        "In the context of quantum computing, low-rank matrices are often easier to work with because they require fewer quantum resources to implement in quantum algorithms. For example, certain operations might require a number of qubits proportional to the rank of a matrix, meaning low-rank matrices can be more efficiently processed on a quantum computer."
      ],
      "metadata": {
        "id": "HCWS6hviwLHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the difference between pca and svd?**\n",
        "\n",
        "Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are both techniques used in linear algebra and data analysis, but they are used for somewhat different purposes and their computations are based on different principles. Here's a comparison:\n",
        "\n",
        "1. **PCA (Principal Component Analysis)** is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. The transformation is done in such a way that the first principal component has the largest possible variance (i.e., accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. PCA is often used in exploratory data analysis and for making predictive models.\n",
        "\n",
        "2. **SVD (Singular Value Decomposition)** is a method of decomposing a matrix into three other matrices. Given a matrix A, you can decompose it into three matrices U, Σ, and V^T such that A = UΣV^T. The U and V matrices contain orthogonal vectors (the left and right singular vectors), and Σ contains the singular values, which can be thought of as the \"strength\" or \"importance\" of the corresponding singular vectors. The SVD is a fundamental operation in linear algebra, and it has a wide range of applications. It's used in solving systems of linear equations, inverting matrices, and is the backbone of many data reduction techniques.\n",
        "\n",
        "The relationship between PCA and SVD is as follows: PCA can be solved by applying the SVD on the data matrix (more precisely, on the covariance or correlation matrix of the data). The principal components resulting from PCA are equivalent to the right singular vectors resulting from the SVD (after some scaling). In other words, PCA can be considered a special case of SVD.\n",
        "\n",
        "Despite their relationship, the methods are typically used in different contexts: **PCA is mainly used in the context of data analysis, for tasks like dimensionality reduction and extracting useful features from data, while SVD is a more general tool in linear algebra with a wider range of applications**."
      ],
      "metadata": {
        "id": "fi7ujY932UC1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8I3t063LsOf"
      },
      "source": [
        "###### *Eigenwerte*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQKdBUmBB1w"
      },
      "source": [
        "**Kriterien fur die Existenz von Eigenwerten**\n",
        "\n",
        "(*Wenn eine dieser Aussagen wahr ist, dann alle. Und wenn eine falsch, dann sind alle falsch*)\n",
        "\n",
        "1. $\\operatorname{rg}(B) < n$\n",
        "\n",
        "2. $\\operatorname{det}(B) = 0$ -> dieses Kriterium zu prufen ist am einfachsten und daher am haufigsten!\n",
        "\n",
        "3. $B^{-1}$ existiert nicht (nicht invertierbar)\n",
        "\n",
        "4. $B \\vec{X}$ = 0 hat mehr als nur die Losung $\\vec{x}$ = 0\n",
        "\n",
        "5. $\\lambda$ = 0 ist ein Eigenwert von $B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RIOxr9LuLh"
      },
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1192.png)\n",
        "\n",
        "*Calculating Eigenvalues via determinant: The tweaked transformation squishes space into a lower dimension (Daher muss rang < n sein)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rayleigh-Quotient & Satz von Courant-Fischer*"
      ],
      "metadata": {
        "id": "30ncah90297J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), auch Rayleigh-Koeffizient genannt, ist ein Objekt aus der linearen Algebra.\n",
        "\n",
        "Der Rayleigh-Quotient wird insbesondere zur numerischen Berechnung von Eigenwerten einer quadratischen Matrix $A$ verwendet.\n",
        "\n",
        "Sei $A \\in {\\mathbb K}^{n \\times n}$ eine reelle symmetrische oder komplexe hermitesche Matrix und $x\\in {\\mathbb {K} }^{n}$ mit $x\\neq 0$ ein Vektor, dann ist der Rayleigh-Quotient von $A$ zum Vektor $x$ definiert durch:\n",
        "\n",
        "> $R_{A}(x)={\\frac  {x^{*}Ax}{x^{*}x}}$\n",
        "\n",
        "Der Rayleigh-Quotient hat eine enge Beziehung zu den Eigenwerten von $A$. Ist $v$ ein Eigenvektor der Matrix $A$ und $\\lambda$  der zugehörige Eigenwert, dann gilt:\n",
        "\n",
        "> $R_{A}(v)={\\frac  {v^{*}Av}{v^{*}v}}={\\frac  {v^{*}\\lambda v}{v^{*}v}}=\\lambda$\n",
        "\n",
        "Durch den Rayleigh-Quotienten wird also jeder Eigenvektor von $A$ auf den dazugehörigen Eigenwert $\\lambda$ abgebildet. Diese Eigenschaft wird unter anderem in der numerischen Berechnung von Eigenwerten benutzt.\n",
        "\n",
        "Insbesondere gilt für eine symmetrische oder hermitesche Matrix $A$ mit dem kleinsten Eigenwert $\\lambda _{{{\\rm {min}}}}$ und dem größten Eigenwert $\\lambda _{{{\\rm {max}}}}$ nach dem [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) (Der Satz von Courant-Fischer stellt die Eigenwerte einer symmetrischen oder hermiteschen Matrix als minimale beziehungsweise maximale Rayleigh-Quotienten):\n",
        "\n",
        "> $\\lambda _{{{\\rm {min}}}}\\leq R_{A}(x)\\leq \\lambda _{{{\\rm {max}}}}$\n",
        "\n",
        "Die Berechnung des kleinsten bzw. größten Eigenwerts ist damit äquivalent zum Auffinden des Minimums bzw. Maximums des Rayleigh-Quotienten. Das lässt sich unter geeigneten Voraussetzungen auch noch auf den unendlichdimensionalen Fall verallgemeinern und ist als Rayleigh-Ritz-Prinzip bekannt.\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 × 3)-Matrix über Extrempunkte auf einem Ellipsoid (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ über bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "q7aw0uQL3AnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kernel*"
      ],
      "metadata": {
        "id": "YfOrrAOFvF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel = Nullspace**\n",
        "\n",
        "The [kernel of this linear map (linear algebra)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) is the set of solutions to the equation Ax = 0, where 0 is understood as the zero vector."
      ],
      "metadata": {
        "id": "nHdImLnovH1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In algebra, the [kernel (algebra)](https://en.m.wikipedia.org/wiki/Kernel_(algebra)) of a homomorphism (function that preserves the structure) is generally the inverse image of 0.\n",
        "\n",
        "* The kernel of a homomorphism is reduced to 0 (or 1) if and only if the homomorphism is injective, that is if the inverse image of every element consists of a single element (jedes element im ziel hat nur ein element im ursprung, es kann aber mehr elemente im ziel ohne ein element im ursprung geben).\n",
        "\n",
        "* This means that the kernel can be viewed as a measure of the degree to which the homomorphism fails to be injective.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/320px-KerIm_2015Joz_L2.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "hLbt81t-PVPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "der Kern einer linearen Abbildung f genau dann nichttrivial ist, wenn eine linear unabhängige Menge (bzw. genauer Familie) S existiert sodass f(S) linear abhängig ist. Durch Übergang zu den Negationen erhalten wir dann die äquivalente Aussage:\n",
        "\n",
        "- kerf = {0}\n",
        "- ...genau dann, wenn gilt...\n",
        "- für alle linear unabhängigen S ist auch f(S) linear unabhängig.\n",
        "\n",
        "https://www.youtube.com/watch?v=GNf3StvaiFA&list=WL&index=9"
      ],
      "metadata": {
        "id": "UbcK-n4ZJ5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Diagonalisierbarkeit*"
      ],
      "metadata": {
        "id": "-wUJznaApPv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-5FB4sQ_XI0"
      },
      "source": [
        "**[Diagonalisierbarkeit](https://en.m.wikipedia.org/wiki/Diagonalizable_matrix) von Matrizen**:\n",
        "\n",
        "* **Eigenwerte liegen auf der Diagonalen**. Dabei: Spur = Summer aller Eigenwerte. Determinante = Produkt aller Eigenwerte = 0.\n",
        "\n",
        "* Square matrix $A$ into **invertible matrix** $P$ and **diagonal matrix** $D$ such that  ${\\displaystyle P^{-1}AP=D}$. Approach: [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix).\n",
        "\n",
        "> $P^{-1} A P=\\left[\\begin{array}{cccc}\n",
        "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right]$\n",
        "\n",
        "* Ein rotierender Körper ohne äußere Kräfte verbleibt in seiner Bewegung, wenn er um seine Symmetrieachse rotiert. Wenn eine Basis aus Eigenvektoren existiert, so ist die Darstellungsmatrix bezüglich dieser Basis eine Diagonalmatrix\n",
        "\n",
        "* *The diagonalization of a symmetric matrix can be interpreted as a rotation of the axes to align them with the eigenvectors:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/4/4e/Diagonalization_as_rotation.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Determinant (Permanent, Immanant)*"
      ],
      "metadata": {
        "id": "yIoPrdtMpRd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Determinante](https://de.m.wikipedia.org/wiki/Determinante)**:\n",
        "\n",
        "* [Determinante](https://de.m.wikipedia.org/wiki/Determinante) (nur fur quadratischen Matrix - bei $n \\cdot m$ Matrizen nutzt man zB SVD)\n",
        "\n",
        "  * gibt an, wie sich Fläche bzw. Volumen durch linearen Abbildung ändert. det(A) = 4 $\\rightarrow$ Matrix vervierfacht Flächeninhalt.\n",
        "\n",
        "  * gibt an, ob lineares Gleichungssystem lösbar ist (Lösung dann mit der Cramerschen Regel)\n",
        "\n",
        "  * Determinante ist Produkt aller Eigenwerte: $\\prod_{i=1}^{n} \\lambda_{i}=\\operatorname{det}(A) = 0$\n",
        "\n",
        "* Berechnung: $\\operatorname{det}(A)$: $\n",
        "A=\\left(\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right)\n",
        "$ $\\rightarrow$ $\n",
        "\\operatorname{det} A=\\left|\\begin{array}{ll}\n",
        "a & c \\\\\n",
        "b & d\n",
        "\\end{array}\\right|=a d-b c\n",
        "$. *Die 2x2-Determinante **ist gleich dem orientierten Flächeninhalt** des von ihren Spaltenvektoren aufgespannten Parallelogramms:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Area_parallellogram_as_determinant.svg/220px-Area_parallellogram_as_determinant.svg.png)\n",
        "\n",
        "* [Entwicklungssatz](https://de.m.wikipedia.org/wiki/Determinante#Laplacescher_Entwicklungssatz) oder  [Leibniz-Formel](https://de.m.wikipedia.org/wiki/Determinante#Leibniz-Formel) (geschlossene Form, theoretisch). [Gauß-Algorithmus](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren) in Dreiecksform - Determinante ist Produkt der [Hauptdiagonale](https://de.m.wikipedia.org/wiki/Hauptdiagonale). Computeralgorithmus: [LU-Zerlegung](https://de.m.wikipedia.org/wiki/Gau%C3%9Fsches_Eliminationsverfahren#LR-Zerlegung).\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) (=has the eigenvalues as roots, and it has the determinant and the trace / sum o fEigenvalues of the matrix among its coefficients):\n",
        "\n",
        "> <font color=\"red\">$f(\\lambda) = \\operatorname{det}(A) = 0$</font> $\\quad (= \\prod_{i=1}^{n} \\lambda_{i})$\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, <font color=\"red\">**the solutions of $f(\\lambda) = 0$** (determinant is equal to zero, Eigenvalues are \"coefficients in characteristic polynomial\" (it's trace to be precise))</font> are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "J1Ir4QyrBark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Permanent_(mathematics)\n",
        "\n",
        "https://en.wikipedia.org/wiki/Immanant"
      ],
      "metadata": {
        "id": "U3D3fAGop0ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Characteristics polynomial (Fundamental theorem of algebra)*"
      ],
      "metadata": {
        "id": "xemut5KHpjBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Characteristics polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) = has the Eigenvalues as [roots](https://en.m.wikipedia.org/wiki/Zero_of_a_function) (zero of a function)\n",
        "\n",
        "Root (Zero of a function): The function f attains the value of 0 at x, or equivalently, x is the solution to the equation f(x) = 0. A root of a polynomial is a zero of the corresponding polynomial function. The [fundamental theorem of algebra](https://en.m.wikipedia.org/wiki/Fundamental_theorem_of_algebra) shows that any non-zero polynomial has a number of roots at most equal to its degree.\n",
        "\n",
        "For example, the polynomial f of degree two, defined by $f(x)=x^{2}-5x+6$ has the two roots (or zeros) that are $\\lambda_1=2$ and $\\lambda_2=3$:\n",
        "\n",
        "> ${\\displaystyle f(2)=2^{2}-5\\times 2+6=0{\\text{ and }}f(3)=3^{2}-5\\times 3+6=0.}$\n",
        "\n",
        "<font color=\"red\">Zero of a function is important because every equation in the unknown x may be rewritten as $f(x)=0$ by regrouping all the terms in the left-hand side. Hence the study of zeros of functions is exactly the same as the study of solutions of equations.</font>\n",
        "\n",
        "Siehe [Nullstellen](https://de.m.wikipedia.org/wiki/Nullstelle) sind bei einer Funktion diejenigen Werte der Ausgangsmenge (des Definitionsbereichs D), bei denen das im Rahmen der Abbildung zugeordnete Element der Zielmenge (des Wertebereichs W) die Null ist (${\\displaystyle 0\\in W}$). Nullstellen von Polynomfunktionen werden auch als Wurzeln bezeichnet.\n",
        "\n",
        "[Kern](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) is Lösungsmenge der [homogenen linearen Gleichung](https://de.m.wikipedia.org/wiki/Lineare_Gleichung) f(x)=0 und wird hier auch Nullraum genannt (denjenigen Vektoren in V, die auf den Nullvektor in W abgebildet werden)\n",
        "\n",
        "Example: A graph of the function $\\cos (x)$ for $x$ in $[-2 \\pi, 2 \\pi]$, with zeros at $-\\frac{3 \\pi}{2},-\\frac{\\pi}{2}, \\frac{\\pi}{2}$, and $\\frac{3 \\pi}{2}$, marked in red.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/X-intercepts.svg/480px-X-intercepts.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "0jYqsd3QptJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Invertible Matrix*"
      ],
      "metadata": {
        "id": "cLeG1PswpSaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Invertible Matrix**\n",
        "\n",
        "* ($B^{-1}$ existiert nicht (Matrix nicht invertierbar) $\\rightarrow$ dann existieren Eigenwerte)\n",
        "\n",
        "* [Invertible matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix) is used in methods to solve systems of linear equations and **help to get Eigenvalues**\n",
        "\n",
        "> Apply Eigendecomposition to matrix (or other method) $\\rightarrow$ Get matrix inverse (with Eigenvectors and a diagonal with Eigenvalues) $\\rightarrow$ Solve systems of linear equations\n",
        "\n",
        "* You can also use pseudo-inverse [Moore-Penrose](https://en.m.wikipedia.org/wiki/Moore–Penrose_inverse) with [matrix solution](https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution) to solve systems of linear equations or in [curve fitting](https://de.wikipedia.org/wiki/Ausgleichungsrechnung) (like regression or ML). Methods: QR, Cholesky, Rank decomposition, SVD. 'The pseudoinverse provides a [Linear Least Squares](https://en.m.wikipedia.org/wiki/Linear_least_squares) solution to a system of linear equations.'\n",
        "\n",
        "  * Exkurs: [Linear least squares (LLS)](https://en.wikipedia.org/wiki/Linear_least_squares) for solving systems of linear equations: is the least squares approximation of linear functions to data (linear regression). **Numerical methods include inverting the matrix of the normal equations and orthogonal decomposition methods**.\n",
        "\n",
        "  * **Overdetermined case**: A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution. See under [Applications](https://en.m.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Applications)\n",
        "\n",
        "  * **Underdetermined case**: Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. [Underdetermined system](https://en.m.wikipedia.org/wiki/Underdetermined_system): there are **fewer equations than unknowns**. Has an infinite number of solutions, if any. In optimization problems that are subject to linear equality constraints, only one of the solutions is relevant, namely the one giving the **highest or lowest value of an objective function**. The use of the **(Moore Pensore) pseudoinverse is to find the minimum (Euclidean) norm solution** to a system of linear equations with multiple solutions. Can be computed using the singular value decomposition. *Example: The solution set for two equations in three variables is, in general, a line ([Source](https://en.m.wikipedia.org/wiki/System_of_linear_equations#General_behavior)):*\n",
        "\n",
        "  ![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Intersecting_Planes_2.svg/240px-Intersecting_Planes_2.svg.png)\n",
        "\n",
        "* [Methods to compute inverse of matrix](https://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion): Gaussian elimination, Newton's method, Cayley–Hamilton method, Eigendecomposition, Cholesky decomposition, Analytic solution (Cramer's rule), Blockwise inversion.\n",
        "\n",
        "* Example: If matrix A can be [eigendecomposed](https://en.m.wikipedia.org/wiki/Invertible_matrix), and if none of its eigenvalues are zero, then A is invertible and its inverse is given by\n",
        "\n",
        ">${\\displaystyle \\mathbf {A} ^{-1}=\\mathbf {Q} \\mathbf {\\Lambda } ^{-1}\\mathbf {Q} ^{-1}}$\n",
        "\n",
        "* where $\\mathbf {Q}$  is the square ($N×N$) matrix whose i-th column is the **eigenvector** $q_{i}$ of $\\mathbf {A}$\n",
        "* ${\\displaystyle \\mathbf {\\Lambda } }$ is the diagonal matrix whose diagonal elements are the corresponding **eigenvalues** ${\\displaystyle \\Lambda _{ii}=\\lambda _{i}}$."
      ],
      "metadata": {
        "id": "49Uc9N-uV6_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Rank*"
      ],
      "metadata": {
        "id": "919bw_THpTYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a low rank matrix?**\n",
        "\n",
        "In linear algebra, the rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. This concept is important because it provides insight into the structure of the matrix, its null space, column space, etc.\n",
        "\n",
        "A \"low-rank\" matrix is a matrix where the rank is significantly less than the number of rows or columns. For example, if you have a 1000x1000 matrix (which could potentially have a rank up to 1000), but its rank is only 10, that matrix would be considered low-rank.\n",
        "\n",
        "Low-rank matrices are useful in many applications, including machine learning and data analysis. They are often used in techniques like principal component analysis (PCA) or singular value decomposition (SVD), where a high-dimensional dataset is approximated by a lower-dimensional one. This is possible because the data can often be accurately represented in a lower-dimensional space (i.e., with a low-rank matrix), which simplifies analysis and reduces computational complexity.\n",
        "\n",
        "In the context of quantum computing, low-rank matrices are often easier to work with because they require fewer quantum resources to implement in quantum algorithms. For example, certain operations might require a number of qubits proportional to the rank of a matrix, meaning low-rank matrices can be more efficiently processed on a quantum computer."
      ],
      "metadata": {
        "id": "XTQWXLTu9FyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a matrix**\n",
        "\n",
        "\n",
        "The rank of a matrix is the number of linearly independent vectors (columns).\n",
        "\n",
        "So **low rank is a low number** of linearly independent vectors. A low rank matrix can be this:\n",
        "\n",
        "1\n",
        "\n",
        "2\n",
        "\n",
        "3\n",
        "\n",
        "\n",
        "* a low rank matrix (whether approximation or not) is simply a matrix for which the number of linearly independent row or columns is much smaller than the actual number of rows or columns.\n",
        "\n",
        "* Viewed as a linear transformation, the span of its range is small or the span of its null space is large.\n",
        "\n",
        "* Quantum computers give exponential speedups for high rank matrices"
      ],
      "metadata": {
        "id": "t9uYK8aS8akn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Sparse Matrix vs Low Rank Matrix**\n",
        "\n",
        "Sparse matrices and low-rank matrices are two very different types of objects. The matrix\n",
        "\n",
        "$\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4\\end{array}\\right]$\n",
        "\n",
        "\n",
        "is sparse (meaning it has a lot of zero entries) but not low-rank (as a matter of fact it’s full rank). On the other hand, the matrix\n",
        "\n",
        "$\\left[\\begin{array}{cccc}1 & 2 & 3 & 4 \\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 9 & 12 \\\\ 4 & 8 & 12 & 16\\end{array}\\right]$\n",
        "\n",
        "is low-rank but not sparse!\n",
        "\n",
        "There's however a connection to be made between the two concepts: **a low-rank matrix has a sparse set of singular values**.\n",
        "\n",
        "Take the following singular value decomposition for a general matrix $\\mathrm{X}$\n",
        "\n",
        "> $\n",
        "\\mathrm{X}=\\mathrm{USV}^T\n",
        "$\n",
        "\n",
        "Then the number of non-zero entries in (the diagonal of) $S$ is precisely equal to the rank of $\\mathbf{X}$. Thus, a sparse $\\mathbf{S}$ leads to a low-rank $\\mathbf{X}$ and vice-versa.\n"
      ],
      "metadata": {
        "id": "4eqnSMcl83mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Algebraic and geometric multiplicity of eigenvalues*"
      ],
      "metadata": {
        "id": "h0hcBLukpUcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic and geometric multiplicity of eigenvalues**\n",
        "\n",
        "> Two or more distinct eigenvalues = algebraic multiplicity\n",
        "\n",
        "> By how many linearly independent vectors is the Eigenspace of $\\lambda_1$ (which is an Eigenvalue with multiplicity 1 or more) formed? = geometric multiplicity\n",
        "\n",
        "> Geometric multiplicity is max equal or less than its algebraic multiplicity\n",
        "\n",
        "> When the geometric multiplicity of a repeated eigenvalue is **strictly less** than its algebraic multiplicity, then that eigenvalue is said to be **defective**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multiplicity_(mathematics)#Multiplicity_of_a_root_of_a_polynomial\n",
        "\n",
        "*algebraic multiplicity*\n",
        "\n",
        "The **algebraic multiplicity** of an eigenvalue is the number of times it appears as a root of the characteristic polynomial (i.e., the polynomial whose roots are the eigenvalues of a matrix).\n",
        "\n",
        "\n",
        "Take this $2 \\times 2$ matrix:\n",
        "\n",
        "> $\n",
        "A=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Its characteristic polynomial is:\n",
        "\n",
        "> $\\begin{aligned} f(\\lambda) & =\\operatorname{det}\\left(\\lambda\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]-\\left[\\begin{array}{ll}1 & 0 \\\\ 2 & 1\\end{array}\\right]\\right) \\\\ & =\\operatorname{det}\\left(\\left[\\begin{array}{cc}\\lambda-1 & 0 \\\\ -2 & \\lambda-1\\end{array}\\right]\\right) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)-0 \\cdot(-2) \\\\ & =(\\lambda-1) \\cdot(\\lambda-1)\\end{aligned}$\n",
        "\n",
        "The roots of the polynomial, that is, the solutions of $f(\\lambda) = 0$  are:\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\lambda_1=1 \\\\\n",
        "& \\lambda_2=1\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Thus, $A$ has one repeated eigenvalue whose algebraic multiplicity is\n",
        "\n",
        ">$\n",
        "\\mu\\left(\\lambda_1\\right)=\\mu\\left(\\lambda_2\\right)=2\n",
        "$\n",
        "\n",
        "*geometric multiplicity*\n",
        "\n",
        "The **geometric multiplicity** of an eigenvalue is the dimension of the linear space of its associated eigenvectors (i.e., its eigenspace).\n",
        "\n",
        "If the Eigenspace of $\\lambda_1$ is generated only by a single vector, it has dimension 1. As a consequence, the geometric multiplicity of $\\lambda_1$ is 1, less than its algebraic multiplicity, which is equal to 2.\n",
        "\n",
        "See complete example in [this pdf document](https://raw.githubusercontent.com/deltorobarba/repo/master/multiplicity.pdf)."
      ],
      "metadata": {
        "id": "VR06jupDflnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Condition Numbers*"
      ],
      "metadata": {
        "id": "0p0KFlAypVZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Condition Numbers**\n",
        "\n",
        "https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/"
      ],
      "metadata": {
        "id": "0JY7J0768m4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What means low condition number in linear algebra?\n",
        "\n",
        "The condition number of a matrix in linear algebra is a measure of the matrix's sensitivity to numerical errors. This number is used in the context of numerical analysis to understand the stability of a problem, specifically in the context of linear systems of equations.\n",
        "\n",
        "The condition number is calculated as the ratio of the largest singular value of a matrix to the smallest singular value. In other words, it gives us a measure of how much the output value of the function can change for a small change in the input.\n",
        "\n",
        "If a matrix has a low condition number, it means the matrix is well-conditioned, implying that the system of equations has a unique solution, and this solution can be found accurately by numerical methods. The solution will not significantly change even if the input is perturbed slightly.\n",
        "\n",
        "On the other hand, if the condition number is high, the matrix is said to be ill-conditioned. This indicates that the system is sensitive to small changes in the input, and even a slight change in the input could produce a large change in the result. Thus, the solution may not be accurately computable due to rounding errors and other numerical errors.\n",
        "\n",
        "So a low condition number in linear algebra is generally a good thing, as it indicates a stable and accurate system."
      ],
      "metadata": {
        "id": "b4djrIKuG-Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here's an example of a well-conditioned matrix, meaning it has a low condition number.\n",
        "\n",
        "Let's consider a simple 2x2 matrix A:\n",
        "\n",
        "    A = [4 2]\n",
        "        [2 1]\n",
        "\n",
        "The singular values of a matrix are the square roots of the eigenvalues of the matrix A^T*A. For matrix A, we can compute A^T*A and find its eigenvalues.\n",
        "\n",
        "    A^T * A = [20 10]\n",
        "              [10  5]\n",
        "\n",
        "The eigenvalues of this matrix are the solutions to the characteristic equation:\n",
        "\n",
        "    det(A - λI) = 0\n",
        "\n",
        "For the above 2x2 matrix, this gives the equation\n",
        "\n",
        "    λ^2 - λ*25 + 100 = 0\n",
        "\n",
        "Solving this equation gives us the eigenvalues 0 and 25. Therefore, the singular values of matrix A are sqrt(0) = 0 and sqrt(25) = 5.\n",
        "\n",
        "The condition number of the matrix is the ratio of the maximum singular value to the minimum singular value (ignoring zero). Therefore, in this case, the condition number of the matrix A is 5/0 = infinity. This suggests that the matrix is ill-conditioned, which means small perturbations in the input could lead to large changes in the output.\n",
        "\n",
        "But let's consider another matrix B:\n",
        "\n",
        "    B = [2 1]\n",
        "        [1 2]\n",
        "\n",
        "The singular values of matrix B are approximately 0.68 and 3.62. Hence, the condition number of matrix B is 3.62 / 0.68 ≈ 5.3, which is considerably low. So this matrix is well-conditioned, and small perturbations in the input will only cause small changes in the output.\n",
        "\n",
        "Please note that in practice, you'd typically use a numerical library or software to compute singular values and condition numbers of matrices, especially for larger and more complex matrices.\n"
      ],
      "metadata": {
        "id": "nkREvcshHBC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where is the border between low and high condition number (which value)?\n",
        "\n",
        "There's no universally accepted threshold for when a condition number is considered \"high\" or \"low\" as it can often depend on the context or specific application. That being said, some guidelines are typically used.\n",
        "\n",
        "Generally, a matrix with a condition number close to 1 is considered to be well-conditioned, which means that it is not sensitive to changes in the input values. As the condition number increases, the matrix becomes increasingly ill-conditioned.\n",
        "\n",
        "As a rough rule of thumb, if the condition number κ(A) exceeds 1/ε, where ε is the machine precision (around 10^-16 for double-precision floating-point arithmetic), then the matrix is considered ill-conditioned. This is because the precision of the numerical computations might be less than the number of digits in the condition number. For example, a condition number of 10^12 would indicate that up to 12 digits of precision could be lost in the computation.\n",
        "\n",
        "In practice, condition numbers in the range of 1-10 are generally considered good, 10-100 might be cause for some concern, and 100+ is usually considered problematic, although these thresholds are rather arbitrary and can depend heavily on the specific context and precision of the computations.\n",
        "\n",
        "However, these are general guidelines and can vary significantly depending on the context and the specific application. Therefore, it's crucial to interpret the condition number in the context of the specific problem you're trying to solve."
      ],
      "metadata": {
        "id": "Ep7rQFwfHEo1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwJ8QbNvEbX"
      },
      "source": [
        "###### *Get Eigenvalues: **Algorithms** (Power & Inverse Iteration, Charakteristisches Polynom) & **Matrix Decomposition** (Eigendecomposition/Spectrum, Schur, SVD)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> [Eigenvalue algorithm](https://en.m.wikipedia.org/wiki/Eigenvalue_algorithm) - matrices are diagonalized numerically using computer software. See [List of Eigenvalue algorithms](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Eigenvalue_algorithms)."
      ],
      "metadata": {
        "id": "cjzaJcIsQrtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbDi3hDN8p4"
      },
      "source": [
        "**Small Matrices: Ermittlung der Eigenwerte der Matrix $A$ mit Determinante und charakteristischem Polynom**\n",
        "\n",
        "In practice, eigenvalues of large matrices are not computed using the characteristic polynomial [Source](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Numerical_computations)\n",
        "\n",
        "> $A=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)$\n",
        "\n",
        "**Step 1: Bilde mit der Einheitsmatrix $E_{n}$ die Matrix $\\left(A-\\lambda E_{n}\\right)$**\n",
        "\n",
        "> $\\left(A-\\lambda E_{n}\\right)=\\left(\\begin{array}{lll}2 & 1 & 2 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 3\\end{array}\\right)-\\left(\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda\\end{array}\\right)=\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "**Step 2: Berechne die Determinante von $\\operatorname{det}\\left(A-\\lambda E_{n}\\right) = \\chi_{A}(\\lambda)$ $\\rightarrow$ [charakteristisches Polynom](https://de.m.wikipedia.org/wiki/Charakteristisches_Polynom)**\n",
        "\n",
        "> $\\operatorname{det}\\left(A-\\lambda E_{n}\\right)=\\operatorname{det}\\left(\\begin{array}{ccc}2-\\lambda & 1 & 2 \\\\ 1 & 2-\\lambda & 2 \\\\ 1 & 1 & 3-\\lambda\\end{array}\\right)$\n",
        "\n",
        "$=(2-\\lambda)^{2} \\cdot(3-\\lambda)+2+2-2 \\cdot(2-\\lambda)-2 \\cdot(2-\\lambda)-(3-\\lambda)$\n",
        "$=-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5$ $\\quad$ (= Polynom)\n",
        "\n",
        "**Step 3: Bestimme die Nullstellen des charakteristischen Polynoms, weil das sind die Eigenwerte der Matrix $A$ (und Determinante wird Null)**\n",
        "\n",
        "> $(A-\\lambda E_{n}) \\cdot v=0$\n",
        "\n",
        "* Durch Ausprobieren: erste Nullstelle $\\lambda_{1}=1$.\n",
        "\n",
        "* Klammern wir dann den Faktor $(\\lambda-1)$ aus, erhalten wir:\n",
        "$-\\lambda^{3}+7 \\lambda^{2}-11 \\lambda+5=(\\lambda-1) \\cdot\\left(-\\lambda^{2}+6 \\lambda-5\\right)\n",
        "$.\n",
        "\n",
        "* Anwendung der [Mitternachtsformel](https://de.m.wikipedia.org/wiki/Quadratische_Gleichung#Lösungsformel_für_die_allgemeine_quadratische_Gleichung_(a-b-c-Formel)): $\n",
        "\\lambda_{2,3}=\\frac{-6 \\pm \\sqrt{36-20}}{-2}=3 \\mp 2$\n",
        "\n",
        "* Somit lauten die drei Eigenwerte der Matrix $\\lambda_{1}=\\lambda_{2}=1$ sowie $\\lambda_{3}=5$\n",
        "\n",
        "*Gibt es eine Zahl $\\lambda$ und einen Vektor $v$, sodass dieser durch Multiplikation mit der Matrix $\\left(A-\\lambda E_{n}\\right)$ auf den Nullvektor abgebildet wird, so ist diese Matrix nicht von vollem Rang. Das bedeutet, dass ihre Determinante Null ist. Vektoren wie $v$ und $w$ sind linear abhängig.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooWX-rgS12w"
      },
      "source": [
        "**Eigendecomposition (spectral decomposition)**\n",
        "\n",
        "* [Eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or sometimes spectral decomposition, see [spectral theorem](https://en.m.wikipedia.org/wiki/Spectral_theorem), is the factorization of a matrix into a canonical form (Normalform) - the matrix is represented in terms of its eigenvalues and eigenvectors. Matrix must be diagonalizable. See also [Summary of Eigendecomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition#Decompositions_based_on_eigenvalues_and_related_concepts).\n",
        "\n",
        "> ${\\displaystyle A=VDV^{-1}}A=VDV^{{-1}}$\n",
        "\n",
        "* $D$ = eigenvalues of $A$ (diagonal)\n",
        "* $V$ = eigenvectors of $A$\n",
        "\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur decomposition**\n",
        "\n",
        "* [Schur decomposition](https://en.m.wikipedia.org/wiki/Schur_decomposition) is a [matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition).\n",
        "\n",
        "* Take complex square matrix and **get an upper triangular matrix whose diagonal elements are the eigenvalues** of the original matrix."
      ],
      "metadata": {
        "id": "wqlosIz9egUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singular Value Decomposition**\n",
        "\n",
        "* [Singular Value Decomposition](https://de.m.wikipedia.org/wiki/Singulärwertzerlegung) is used in calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.\n",
        "\n",
        "* SVD can also be used in least squares linear regression, image compression, and denoising data. Siehe auch [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm)\n"
      ],
      "metadata": {
        "id": "lCrTpBOyevh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Decomposition** (Gaussian Elimination, Cholesky, QR, LU)*"
      ],
      "metadata": {
        "id": "0ChC-FyHZ41O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classical: [Gaussian Elimination](https://en.m.wikipedia.org/wiki/Gaussian_elimination) (row reduction). See [List of numerical algorithms to solve systems of linear equation](https://en.m.wikipedia.org/wiki/List_of_numerical_analysis_topics#Solving_systems_of_linear_equations)"
      ],
      "metadata": {
        "id": "KzcuDJOLhCGd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc8EITyoKDXA"
      },
      "source": [
        "**Cholesky Decomposition** (Hermitian / squared)\n",
        "\n",
        "* **alternative to Eigendecomposition** to get matrix inverse for solving linear equations\n",
        "\n",
        "* [Cholesky decomposition](https://en.m.wikipedia.org/wiki/Cholesky_decomposition) of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose,\n",
        "\n",
        "* useful for efficient numerical solutions, e.g., Monte Carlo simulations (solving linear least squares for linear regression, as well as simulation and optimization methods)\n",
        "\n",
        "* When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.\n",
        "\n",
        "> $A = LL^T$\n",
        "\n",
        "* $L$ is the lower triangular matrix and $L^T$ is the transpose of L. Decompose as the product of upper triangular matrix $U$ is also possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NVXqSfFlBt"
      },
      "source": [
        "**QR Decomposition**\n",
        "\n",
        "* We can use QR decomposition to [find the determinant of a square matrix](https://en.m.wikipedia.org/wiki/QR_decomposition#Connection_to_a_determinant_or_a_product_of_eigenvalues)\n",
        "\n",
        "* The [QR decomposition](https://en.m.wikipedia.org/wiki/QR_decomposition) is for m x n matrices (not limited to square matrices) and decomposes a matrix into $Q$ (orthogonal $Q^T Q = I$ or unitary $Q \\cdot Q = I$, size m x m) and $R$ (upper triangle matrix with the size m x n) components.\n",
        "\n",
        "> $A = Q R$\n",
        "\n",
        "* Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, **although is <u>not</u> limited to square matrices**.\n",
        "\n",
        "* There are several methods for actually computing the QR decomposition, such as by [Householder transformations](https://de.m.wikipedia.org/wiki/Householdertransformation), [Givens rotations](https://de.m.wikipedia.org/wiki/Givens-Rotation) and  [Gram-Schmidtsch's orthogonalization method](https://de.m.wikipedia.org/wiki/Gram-Schmidtsches_Orthogonalisierungsverfahren)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDrmirVPSXe"
      },
      "source": [
        "**LU Decomposition**\n",
        "\n",
        "* The [LU decomposition](https://en.m.wikipedia.org/wiki/LU_decomposition) is often used to simplify the **solving of systems of linear equations**, such as **finding the coefficients in a linear regression**, as well as in **calculating the determinant and inverse** of a matrix.\n",
        "\n",
        "* Lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix.\n",
        "\n",
        "* The product sometimes includes a permutation matrix as well. LU decomposition can be viewed as the matrix form of Gaussian elimination.\n",
        "\n",
        "* Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix.\n",
        "\n",
        "The **LU decomposition is for square matrices** and decomposes a matrix into L and U components. Let A be a square matrix. An LU factorization refers to the factorization of A, with proper row and/or column orderings or permutations, into two factors – a **lower triangular matrix L** and an **upper triangular matrix U**:\n",
        "\n",
        "> A = L U\n",
        "\n",
        "* The LU decomposition is found using an <u>iterative numerical process</u> and **can fail for those matrices that cannot be decomposed or decomposed easily**.\n",
        "\n",
        "* In the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero. For example, for a 3 × 3 matrix A, its LU decomposition looks like this:\n",
        "\n",
        "> $\\left[\\begin{array}{lll}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{array}\\right]\\left[\\begin{array}{ccc}\n",
        "u_{11} & u_{12} & u_{13} \\\\\n",
        "0 & u_{22} & u_{23} \\\\\n",
        "0 & 0 & u_{33}\n",
        "\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Solving Systems of Linear Equations: **Matrix Type** (Squared & Non-Squared)*"
      ],
      "metadata": {
        "id": "PREcYcRRPEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Solving Systems of Linear Equations: <u>Squared Matrix</u> (for HHL) - Eigendecomposition to get Pseudo-inverse**\n",
        "\n",
        "> https://towardsdatascience.com/from-eigendecomposition-to-determinant-fundamental-mathematics-for-machine-learning-with-1b6b449a82c6\n",
        "\n",
        "\n",
        "<font color=\"red\">**If A is squared is a matrix (and has [full rank](https://de.m.wikipedia.org/wiki/Rang_(Mathematik))) in a linear system of equations: Getting the matrix inverse via Eigendecomposition (pseudoinverse Moore-Penrose provides a least squares solution to a system of linear equations.')**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "then you can take the [inverse](https://de.m.wikipedia.org/wiki/Inverse_Matrix) if A to solve for x:\n",
        "\n",
        "> $x = A^{-1} b$\n",
        "\n",
        "<font color=\"red\">*This part is important for HHL:*\n",
        "\n",
        "* $\\hat{A} = \\hat{A}^{\\dagger}$ $\\quad$ - Hermitian operators are [Self-adjoint operators](https://en.m.wikipedia.org/wiki/Self-adjoint_operator)\n",
        "\n",
        "* Adjungierte Matrix = transponiert + complex konjugiert (Vorzeichen umgekehrt). Hermetian: selbstadjunktiert = symmetrisch\n",
        "\n",
        "* Following from this, in bra-ket notation: $\n",
        "\\left\\langle\\phi_{i}|\\hat{A}| \\phi_{j}\\right\\rangle=\\left\\langle\\phi_{j}|\\hat{A}| \\phi_{i}\\right\\rangle^{*}\n",
        "$\n",
        "\n",
        "\n",
        "<font color=\"red\">*Since $A$ is Hermitian, it has a spectral decomposition : $\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$*\n",
        "\n",
        "<font color=\"red\">*You need the Eigendecomposition (spectral decomposition) to get the inverse of a matrix (=here unitary and hence normal)*\n",
        "\n",
        "Getting the [Matrix inverse via eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition): If a matrix $\\mathbf{A}$ can be eigendecomposed and if none of its eigenvalues are zero, then $\\mathbf{A}$ is invertible and its inverse is given by\n",
        "\n",
        ">$\n",
        "\\mathbf{A}^{-1}=\\mathbf{Q}^{-1} \\mathbf{\\Lambda}^{-1} \\mathbf{Q}\n",
        "$\n",
        "\n",
        "If $\\mathbf{A}$ is a symmetric matrix, since $\\mathbf{Q}$ is formed from the eigenvectors of $\\mathbf{A}, \\mathbf{Q}$ is guaranteed to be an orthogonal matrix, therefore $\\mathbf{Q}^{-1}=\\mathbf{Q}^{\\mathrm{T}}$. Furthermore, because $\\mathbf{\\Lambda}$ is a diagonal matrix, its inverse is easy to calculate:\n",
        "\n",
        ">$\n",
        "\\left[\\Lambda^{-1}\\right]_{i i}=\\frac{1}{\\lambda_{i}}\n",
        "$\n",
        "\n",
        "**Example - the matrix:**\n",
        "\n",
        "$A=\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]$\n",
        "\n",
        "has the eigenvectors:\n",
        "\n",
        "$\\mathbf{u}_{1}=\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{1}=4$\n",
        "\n",
        "and:\n",
        "\n",
        "$\\mathbf{u}_{2}=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] \\quad$ with eigenvalue $\\quad \\lambda_{2}=-1$\n",
        "\n",
        "We can verify (as illustrated in Figure 1) that only the length of $\\mathbf{u}_{1}$ and $\\mathbf{u}_{2}$ is changed when one of these two vectors is multiplied by the matrix $\\mathbf{A}$ :\n",
        "\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=4\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 8\\end{array}\\right]$\n",
        "\n",
        "and\n",
        "\n",
        "$\\left[\\begin{array}{ll}2 & 3 \\\\ 2 & 1\\end{array}\\right]\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=-1\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{r}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "For most applications we normalize the eigenvectors (i.e. transform them such that their length is equal to one):\n",
        "\n",
        "$\n",
        "\\mathbf{u}^{\\top} \\mathbf{u}=1 \\text {. }\n",
        "$\n",
        "\n",
        "For the previous example we obtain:\n",
        "\n",
        "$\n",
        "\\mathbf{u}_{1}=\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "Exkurs: wie man einen Vektor normiert:\n",
        "\n",
        "1) Betrag von $\\left(\\begin{array}{l}3 \\\\ 2\\end{array}\\right)$ ist gleich $\\sqrt{3^{2}+2^{2}}=3.6055$ Vektor normieren, also mit 1/Betrag malnehmen:\n",
        "\n",
        "2) $\n",
        "\\frac{1}{3.6055} \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right)= 0.27735 \\cdot\\left(\\begin{array}{l}\n",
        "3 \\\\\n",
        "2\n",
        "\\end{array}\\right) =\\left(\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\n",
        "We can check that:\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "3.3284 \\\\\n",
        "2.2188\n",
        "\\end{array}\\right]=4\\left[\\begin{array}{l}\n",
        ".8331 \\\\\n",
        ".5547\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "and\n",
        "\n",
        "$\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 3 \\\\\n",
        "2 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
        ".7071 \\\\\n",
        "-.7071\n",
        "\\end{array}\\right]=-1\\left[\\begin{array}{r}\n",
        "-.7071 \\\\\n",
        ".7071\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5z-Ez-u4RWhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Solving Systems of Linear Equations: <u>Non-Squared Matrix</u> - Singular value decomposition (SVD) to get Pseudo-inverse**\n",
        "\n",
        "<font color=\"red\">**If a matrix A is not squared: singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.**\n",
        "\n",
        "> $A x = b$ $\\quad$ ($A$ is not regular)\n",
        "\n",
        "You multiply both sides by the $A^{T}$, which is the transpose of A:\n",
        "\n",
        "> $A^{T} A x = A^{T} b$\n",
        "\n",
        "Then move $A^{T} A$ on right side (by taking their inverse). This is not the original x anymore, but the [least squares](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate#Lineare_Modellfunktion) $\\hat{x}$\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "And that term <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> is know as (Moore–Penrose) pseudo-inverse <font color=\"blue\">$A^{+}$</font>:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$A^{+}$</font> $b$\n",
        "\n",
        "And a pseudo-inverse is nothing else than our least-squares solutions. See more details under [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)\n",
        "\n",
        "*In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.*\n",
        "\n",
        "* non squared matrix\n",
        "\n",
        "* to solve you need the inverse, but normal eigendecomposition doesn work\n",
        "\n",
        "> you need to compute mooore penrose pseudo-inverse, and here you need to apply singular value decomposition to get eigendecomposition\n",
        "\n",
        "* SVD is a type of [Matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition), specicially one based on eigenvalue concepts. and SVD is a full rank decomposition (besides broader [Rank factorization methods](https://en.m.wikipedia.org/wiki/Rank_factorization))\n",
        "\n",
        "* matrix decompositions in general are used to take a large matrix apart (i.e. factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U) so the new matrices require fewer additions and multiplications to solve, compared with the original system $A\\mathbf {x} =\\mathbf {b}$\n",
        "\n",
        "<font color=\"red\">**Example:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right] x=\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "* We can immediately see that matrix A is of rank 2 because the first two rows are multiples of each other (2 and -2 and -2 and 2), just the last row numbers are not multiples of each other (5 and 3).\n",
        "\n",
        "* Now we can apply the pseudo-inverse to find the least-squares solution:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "> $\\hat{x}=$ <font color=\"blue\">$\\left(\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]$</font>$\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "> $\\hat{x}=$$\\left[\\begin{array}{l}-4 \\\\ -2\\end{array}\\right]$\n",
        "\n",
        "\n",
        "*Inserting the least-squares $\\hat{x}$ into the original equation:*\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\left[\\begin{array}{l}-4 \\\\ -3\\end{array}\\right]$ = $\\left[\\begin{array}{c}2 \\cdot(-4)+(-2) \\cdot(-3) \\\\ (-2)\\cdot (-4)+2 \\cdot (-3) \\\\ 5\\cdot (-4)+3 \\cdot (-3)\\end{array}\\right]$ = $\\left[\\begin{array}{c}-2 \\\\ 2 \\\\ -29\\end{array}\\right]$ $\\approx$ $\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$"
      ],
      "metadata": {
        "id": "g9VGZpsMZff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Graph Theory*"
      ],
      "metadata": {
        "id": "f4lHqJyMa24i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any oscillator — a pendulum, a spring, a firefly, a human heart cell — wants to match up with its neighbors. Mathematicians recently showed that synchronization is inevitable in expander graphs, a type of network found in many areas of science. https://www.quantamagazine.org/new-proof-shows-that-expander-graphs-synchronize-20230724/\n"
      ],
      "metadata": {
        "id": "PfuO6OMuA4uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/finally-a-fast-algorithm-for-shortest-paths-on-negative-graphs-20230118/"
      ],
      "metadata": {
        "id": "g3MrZudH-Nc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a combinatorial Laplacian?**\n",
        "\n",
        "\n",
        "The combinatorial Laplacian (often simply referred to as the Laplacian) is a matrix associated with a graph, which captures certain structural properties of the graph. Specifically, it's the matrix difference between the degree matrix and the adjacency matrix of the graph. Here's how to construct it:\n",
        "\n",
        "1. **Adjacency Matrix (A)**:\n",
        "For a graph \\( G \\) with \\( n \\) vertices, its adjacency matrix \\( A \\) is an \\( n \\times n \\) matrix where:\n",
        "\\[ A_{ij} =\n",
        "  \\begin{cases}\n",
        "   1 & \\text{if vertex } i \\text{ is adjacent to vertex } j \\\\\n",
        "   0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\]\n",
        "\n",
        "2. **Degree Matrix (D)**:\n",
        "The degree matrix is a diagonal matrix where the diagonal entries correspond to the degrees of the vertices. If \\( d_i \\) is the degree of the \\( i \\)-th vertex, then:\n",
        "\\[ D_{ij} =\n",
        "  \\begin{cases}\n",
        "   d_i & \\text{if } i = j \\\\\n",
        "   0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\]\n",
        "\n",
        "3. **Combinatorial Laplacian (L)**:\n",
        "The combinatorial Laplacian is then defined as:\n",
        "\\[ L = D - A \\]\n",
        "\n",
        "So, the entry \\( L_{ij} \\) of the Laplacian is:\n",
        "\\[ L_{ij} =\n",
        "  \\begin{cases}\n",
        "   d_i & \\text{if } i = j \\\\\n",
        "   -1 & \\text{if vertex } i \\text{ is adjacent to vertex } j \\\\\n",
        "   0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\]\n",
        "\n",
        "The combinatorial Laplacian has various properties and applications in graph theory, spectral graph theory, network analysis, and other areas. For example, the smallest eigenvalue of the Laplacian is always zero, and its multiplicity is related to the number of connected components in the graph. Another important application is in graph spectral clustering, where the eigenvalues and eigenvectors of the Laplacian are used to cluster the vertices of a graph."
      ],
      "metadata": {
        "id": "26wwGekK-KIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, graph theory plays a significant role in quantum computing and quantum information theory. Here are a few ways in which it's important:\n",
        "\n",
        "1. **Quantum walks**: Quantum walks are the quantum analogs of classical random walks. In a classical random walk, a \"walker\" moves randomly around a graph. Quantum walks extend this idea to the quantum realm, and their properties have been used to design new quantum algorithms. The structure of the graph can greatly impact the behavior of the quantum walk.\n",
        "\n",
        "2. **Quantum error correction**: Graph-based quantum error correction codes, such as the Toric code and surface code, are critical for protecting quantum information from errors due to environmental noise. These codes can be represented using mathematical objects from graph theory.\n",
        "\n",
        "3. **Graph states**: In quantum computing, graph states are a particular kind of multi-qubit state which is associated with a graph. They are used in quantum error correction, quantum communication, and quantum computing. For instance, measurement-based quantum computation often makes use of graph states.\n",
        "\n",
        "4. **Quantum algorithms**: Certain quantum algorithms, like the Quantum Search Algorithm (a generalized version of Grover's Algorithm), can be understood in terms of graph theory. They can be interpreted as searching a graph for a marked node.\n",
        "\n",
        "5. **Network Theory and Quantum Entanglement**: Network theory, which is a part of graph theory, can be used to understand and quantify entanglement in quantum systems. This is particularly important in understanding quantum many-body systems and the concept of quantum teleportation.\n",
        "\n",
        "So, while not all aspects of quantum computing require knowledge of graph theory, many of the key concepts and methods are deeply interconnected with it.\n"
      ],
      "metadata": {
        "id": "zW9gyJkTMCGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=M9F7zT9Gg-k\n",
        "\n",
        "https://www.youtube.com/watch?v=skaXrTtQyJA\n",
        "\n",
        "https://www.youtube.com/watch?v=2h12m-3zQ0M\n",
        "\n",
        "https://www.youtube.com/watch?v=-Afa1WI3iug"
      ],
      "metadata": {
        "id": "X13DUDyLt9db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Graph Theory**\n",
        "\n",
        "* In [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory), the [characteristic polynomial](https://en.m.wikipedia.org/wiki/Characteristic_polynomial) of a [graph](https://en.m.wikipedia.org/wiki/Graph_(discrete_mathematics)) is the characteristic polynomial of its [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix).\n",
        "\n",
        "* Spectral graph theory is the study of the properties of a graph in relationship to the **characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph**, such as its adjacency matrix or [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix) = Edge Matrix - Adjacency Matrix\n",
        "\n",
        "* One of the reasons that the eigenvalues of matrices have meaning is that they arise as the solution to natural optimization problems\n",
        "\n",
        "> Der größte Eigenwert eines k-regulären Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen. (The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**.)\n",
        "\n",
        "\n",
        "\n",
        "http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf\n",
        "\n",
        "https://www.cs.cmu.edu/~venkatg/teaching/15252-sp20/notes/Spectral-graph-theory.pdf"
      ],
      "metadata": {
        "id": "Zxze3ZZQpmmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian matrix:**\n",
        "\n",
        "> $L(G) = D - A = B^T B \\quad$ for some matrix $B$\n",
        "\n",
        "* $L$ = [Laplacian matrix](https://en.m.wikipedia.org/wiki/Laplacian_matrix)\n",
        "* $D$ = [degree matrix](https://en.m.wikipedia.org/wiki/Degree_matrix), entries contain degrees of each vertex\n",
        "* $A$ = [adjacency matrix](https://en.m.wikipedia.org/wiki/Adjacency_matrix): 1 if edge, 0 if no edge\n",
        "* $B$ = the „directed“ node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "**Description of Laplacian matrix**\n",
        "* Laplacian matrix is an operator mapping from graph to the real numbers\n",
        "* Diagonals have degrees of vertices\n",
        "* Off-diagonal contain edges marked as -1\n",
        "* Contains all information about graph (you can recreate the graph just from laplacian).\n",
        "* And you can nicely apply linear algebra to analyse the graph\n",
        "\n",
        "**Properties of Laplacian matrix:**\n",
        "1. L(G) is symmetric\n",
        "2. L(G) has:\n",
        "  * real-valued, non-negative eigenvalues and\n",
        "  * real-valued, orthogonal eigenvectors\n",
        "* Is always positive-semidefinite matrix\n",
        "* $\\lambda$ = 0 is always an Eigenvalue with Eigenvector 1 (because all rows add to zero and multiplied by vector with 1‘s leads to zero)\n"
      ],
      "metadata": {
        "id": "yk02m201n6n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of calculating Eigenvalues and Eigenvectors of Laplacian matrix**\n",
        "* second smallest Eigenvalue (**spectral gap**) is a solution (i.e. minimal energy) for many problems, if computable.\n",
        "* You can use the spectrum of the Laplacian matrix to solve\n",
        "  * **optimization problems** (i.e. shortest path, Hamiltonain circuit), like traveling salesman problem\n",
        "  * **graph partitioning problem** (optimal cut). reveal sparse connections and where there are more clusters, find bottlenecks (in social media it relates to communities, in networks it's a critical point\n",
        "  * both problems are np-hard\n",
        "* In physics: **Fundamental modes (harmonics)** are given by the Eigenvectors of the graph laplacian [Spectral Partitioning Part 3 Algebraic Connectivity](https://www.youtube.com/watch?v=Vng9lkibGEE)"
      ],
      "metadata": {
        "id": "9MumAIvqp8cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 1: Graph laplacian spectrum, meaning it's eigenvalues, tells us something about the underlying connectivity of the graph**\n",
        "* = Graph has k connected components if and only if the k-smallest Eigenvalues are identically zero: $\\lambda_0$ = $\\lambda_1$ = .. $\\lambda_{k-1}$ = 0 (Fiedler 1973).\n",
        "* Spectrum of L(G) $\\rightleftharpoons$ Connectivity of G\n",
        "* = The number of (connected) components in the graph is the dimension of the nullspace (=kernel / set of solutions) of the Laplacian and the algebraic multiplicity (=how many times the same Eigenvalue) of the 0 eigenvalue.\n",
        "* Laplacian: its Eigenvectors, which do not rotate, you can see an ellipse like section of space they draw.\n",
        "* **Spectrograph theory asked questions about this ellipse to determine graph behavior, what the fastest way from dot-to-dot is**, how many pathways there are dot to dot [Source](https://www.youtube.com/watch?v=njatNunHC_o)\n",
        "\n",
        "*Der [Satz von Courant-Fischer](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer) charakterisiert die Eigenwerte einer symmetrischen positiv definiten (3 × 3)-Matrix über Extrempunkte auf einem **Ellipsoid** (Der Satz von Courant-Fischer charakterisiert nun die Eigenwerte von $A$ über bestimmte Extrempunkte auf diesem Ellipsoid) - Siehe auch [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Ellipsoid_Quadric.png/434px-Ellipsoid_Quadric.png)"
      ],
      "metadata": {
        "id": "cZ4ROEDbn9zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special 2: $L(G)$ contains $B^T B$ for some matrix $B$**\n",
        "* the „directed“ node-edge [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) of G\n",
        "\n",
        "> $\\min _{y \\in \\Re^{\\prime \\prime}} f(y)=\\sum_{(i, j) \\in E^{\\prime}}\\left(y_i-y_j\\right)^2=y^T L y$\n",
        "\n",
        "* [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient) with Rayleigh theorem:\n",
        "  * $\\lambda_2=\\min f(y)$ : The minimum value of $f(y)$ is given by the $2^{\\text {nd }}$ smallest eigenvalue $\\lambda_2$ of the Laplacian matrix $\\boldsymbol{L}$\n",
        "  * $\\mathrm{x}=\\arg \\min _{\\mathrm{y}} \\boldsymbol{f}(\\boldsymbol{y})$ : The optimal solution for $y$ is given by the corresponding eigenvector $\\boldsymbol{x}$, referred as the [Fiedler vector (Algebraic_connectivity)](https://en.m.wikipedia.org/wiki/Algebraic_connectivity)\n",
        "\n",
        "* see how to create Laplacian matrix from incidence matrix: [Spectral Partitioning, Part 1 The Graph Laplacian](https://www.youtube.com/watch?v=rVnOANM0oJE)\n",
        "* B captures relationship between different nodes and edges, rows are nodes and columns are edges of G (pair start-sink)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1390.png)\n",
        "\n",
        "Source: [(Lecture 14) Graph Laplacians](https://www.youtube.com/watch?v=M9F7zT9Gg-kI)\n",
        "\n",
        "**How can you use this property? - The number of cut edges equals $\\frac{1}{4} x^T L(G) x$ under specific constraints**.\n",
        "  * Means: if you want to minimize edge cuts, you should try minimizing this product (it’s a combinatorial optimization problem).\n",
        "  * Cut edges: find minimal number of connections, bottleneck, borders between two clusters.\n",
        "  * This problem is however np complete. You need to relax sum contraints.\n",
        "\n",
        "*If you relax it you can combine it with the [Courant-Fisher minimax theorem (Satz von Courant-Fischer)](https://de.m.wikipedia.org/wiki/Satz_von_Courant-Fischer):*\n",
        "* if we allowed to use any vector y, where y is normalized in a certain way, and it’s elements sum to 0, then the vector y that minimizes this quantity is actually q1.\n",
        "* And q1 is the Eigenvector corresponding to the second smallest Eigenvalue.\n",
        "* And in fact the minimum value simplifies to something that is proportional to that eigenvalue.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1388.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1387.png)"
      ],
      "metadata": {
        "id": "Nw6nUk4bb4ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application example: Problem statement for using the Laplacian matrix of a graph:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1384.png)\n",
        "\n",
        "Source: [Lecture 30 — The Graph Laplacian Matrix (Advanced) | Stanford University](https://www.youtube.com/watch?v=FRZvgNvALJ4)\n",
        "\n",
        "\n",
        "Sum over all neighbouring edges:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1385.png)\n",
        "\n",
        "\n",
        "**Solution siehe** [Rayleigh-Quotient](https://de.m.wikipedia.org/wiki/Rayleigh-Quotient), um den zweitkleinsten Eigenwert der Laplacian matrix zu finden (spectral gap):\n",
        "\n",
        "> $(R_{A}(x)=) \\quad \\lambda_2 = {min \\frac {x^{T} M x}{x^{T}x}}$ with matrix $M$ being the Laplacian matrix $L$\n",
        "\n",
        "Meaning of (min $x^T L x$): I take the label of one endpoint edge, subtract the value of the other endpoint of an edge, then square it up, and sum this over all the edges. (see also in green on the bottom why quadratic forms are relevant here!)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1381.png)\n",
        "\n",
        "Source: [Lecture 33 — Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1382.png)\n",
        "\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1383.png)\n",
        "\n",
        "Source: [Lecture 33 — Spectral Graph Partitioning Finding a Partition (Advanced) | Stanford](https://www.youtube.com/watch?v=siCPjpUtE0A)\n",
        "\n",
        "> **Note that the largest eigenvalue of the adjacency matrix corresponds to the smallest eigenvalue of the Laplacian.** But: Where the smallest eigenvector of the Laplacian is a constant vector, the largest eigenvector of an adjacency matrix, called the Perron vector, need not be. The Perron-Frobenius theory tells us that the largest eigenvector of an adjacency matrix is non-negative, and that its value is an upper bound on the absolute value of the smallest eigenvalue. These are equal precisely when the graph is bipartite.\n"
      ],
      "metadata": {
        "id": "CYNJpwY7BkvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Quantum Application with Graphs: https://medium.com/quandela/exploring-graph-problems-with-single-photons-and-linear-optics-4f3d5848add8"
      ],
      "metadata": {
        "id": "Yye9sR8Zq-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph & Graph Theory**\n",
        "\n",
        "* In [discrete mathematics](https://en.m.wikipedia.org/wiki/Discrete_mathematics), and more specifically in [graph theory](https://en.m.wikipedia.org/wiki/Graph_theory), a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\".\n",
        "\n",
        "* The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called link or line).\n"
      ],
      "metadata": {
        "id": "b-7FPIb42kSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Graph Theory**\n",
        "\n",
        "* [Algebraic graph theory](https://en.m.wikipedia.org/wiki/Algebraic_graph_theory) is a branch of mathematics in which algebraic methods are applied to problems about graphs. This is in contrast to geometric, combinatoric, or algorithmic approaches.\n",
        "\n",
        "* There are three main branches of algebraic graph theory, involving the use of linear algebra, the use of group theory, and the study of graph invariants.\n",
        "\n",
        "* **Using linear algebra**:\n",
        "\n",
        "  * The first branch of algebraic graph theory involves the study of graphs in connection with linear algebra. Especially, it studies the [spectrum (Eigendecomposition of a matrix)](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the adjacency matrix, or the Laplacian matrix of a graph - this part of algebraic graph theory is also called [spectral graph theory](https://en.m.wikipedia.org/wiki/Spectral_graph_theory).\n",
        "\n",
        "  * **Several theorems relate properties of the spectrum to other [graph properties (invariant)](https://en.m.wikipedia.org/wiki/Graph_property).** As a simple example, a connected graph with diameter D will have at least D+1 distinct values in its spectrum. Aspects of graph spectra have been used in analysing the synchronizability of networks.\n",
        "\n",
        "* Video: [Daniel Spielman “Miracles of Algebraic Graph Theory”](https://www.youtube.com/watch?v=CDMQR422LGM)\n",
        "\n",
        "* Video: [The Unreasonable Effectiveness of Spectral Graph Theory: A Confluence of Algorithms, Geometry and Physics](https://www.youtube.com/watch?v=8XJes6XFjxM)"
      ],
      "metadata": {
        "id": "sg75zioGELCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Types of Graphs*"
      ],
      "metadata": {
        "id": "9ELeT7od--k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete graph**\n",
        "\n",
        "* In graph theory, a [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge.\n",
        "\n",
        "* A complete digraph is a directed graph in which every pair of distinct vertices is connected by a pair of unique edges (one in each direction).\n",
        "\n",
        "*K7, a complete graph with 7 vertices with $n$ vertices and ${\\displaystyle \\textstyle {\\frac {n(n-1)}{2}}}$ edges*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Complete_graph_K7.svg/245px-Complete_graph_K7.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "suoc9GoR3FCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete bipartite graph**\n",
        "\n",
        "* [Complete bipartite graph](https://en.m.wikipedia.org/wiki/Complete_bipartite_graph) (or biclique), a special [bipartite graph](https://en.m.wikipedia.org/wiki/Bipartite_graph) where every vertex on one side of the bipartition is connected to every vertex on the other side\n",
        "\n",
        "* A complete bipartite graph with m = 5 and n = 3:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Biclique_K_3_5.svg/320px-Biclique_K_3_5.svg.png)\n",
        "\n",
        "Bipartite Graph https://www.youtube.com/watch?v=HqlUbSA9cEY\n",
        "\n"
      ],
      "metadata": {
        "id": "p0YwqBrL3qCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directed vs Undirected Graphs**\n",
        "\n",
        "The applications for directed graphs are many and varied. They can be used to analyze electrical circuits, develop project schedules, find shortest routes, analyze social relationships, and construct models for the analysis and solution of many other problems.\n",
        "\n",
        "https://faculty.cs.niu.edu/~freedman/241/241notes/241graph.htm\n",
        "\n",
        "\n",
        "Undirected graphs are more restrictive kinds of graphs. They represent only whether or not a relationship exists between two vertices. They don't however represent a distinction between subject and object in that relationship. One type of graph can sometimes be used to approximate the other.\n",
        "\n",
        "https://www.baeldung.com/cs/graphs-directed-vs-undirected-graph"
      ],
      "metadata": {
        "id": "V0ExwcX52H9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hamiltonian path**\n",
        "\n",
        "A [Hamiltonian path](https://en.m.wikipedia.org/wiki/Hamiltonian_path) (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once. A Hamiltonian cycle around a network of six vertices:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/b/be/Hamiltonian.png)"
      ],
      "metadata": {
        "id": "E26jLergoS4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Graph Properties*"
      ],
      "metadata": {
        "id": "oAiyy65E-8Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Length** is the number of edges in a graph.\n",
        "\n",
        "A **closed walk** occurs when x = y.\n",
        "\n",
        "A **trail** is a walk with no repeated edges.\n",
        "\n",
        "A closed trail is a **circuit**.\n",
        "\n",
        "A **path** is a walk with no repeated vertices (A path is a sequence of distinct edges you can follow through the graph)\n",
        "\n",
        "A closed path is a **cycle** (and circuit)\n",
        "\n",
        "A **simple graph** is loop-free, undirected and has no multiple edges.\n",
        "\n",
        "[INTRODUCTION to GRAPH THEORY - DISCRETE MATHEMATICS](https://www.youtube.com/watch?v=HkNdNpKUByM)\n",
        "\n",
        "[Introduction to Graph Theory: A Computer Science Perspective](https://www.youtube.com/watch?v=LFKZLXVO-Dg)\n"
      ],
      "metadata": {
        "id": "yOjUf8ACsQ8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wichtige Zyklen und Pfade in der Graphentheorie**\n",
        "\n",
        "* [**Eulerian path** (Eulerkreisproblem)](https://de.m.wikipedia.org/wiki/Eulerkreisproblem) = every edge only once\n",
        "  * **Euler path** (every edge once only, vertices can be multiple: exists if either 0 or 2 odd degree vertices exist with all else even degree\n",
        "  * **Euler circuit** (more constraint): same as Euler path, but return to starting point (das Haus vom weihachtsmann). Use also Fleurys algorithm: works if all vertices have even degrees\n",
        "\n",
        "* [**Hamiltonian circuit / cycle** (Hamiltonian path problem)](https://de.m.wikipedia.org/wiki/Hamiltonkreisproblem): every vertex only once\n",
        "  * every vertex only once, but edges don‘t matter how often. And return to starting point.\n",
        "  * **Minimum cost hamiltonian circuit = [traveling salesman problem](https://de.m.wikipedia.org/wiki/Problem_des_Handlungsreisenden).**. Is np-complete. Look for [shortest path](https://de.m.wikipedia.org/wiki/Kürzester_Pfad), siehe auch [Pathfinding](https://de.m.wikipedia.org/wiki/Pathfinding)\n",
        "  * https://www.quora.com/Why-are-quantum-computers-unable-to-solve-the-travelling-salesman-problem-in-polynomial-time: *Quantum computation offers speed improvements for some very specialized problems like integer factorization, but it isn’t known or expected to offer polynomial-time algorithms for generic NP-complete problems like the Traveling Salesman Problem. If it does then NP ⊆ BQP, and most experts don’t regard this as likely at all $^{*}$*.\n",
        "  * Heuristic algorithms as alternative to brute force:\n",
        "    * [Dijkstra algorithmus](https://de.m.wikipedia.org/wiki/Dijkstra-Algorithmus)\n",
        "    * Nearest neighbor algorithm is a heuristic, non optimal, but feasible - [greedy algorithm](https://de.m.wikipedia.org/wiki/Greedy-Algorithmus), cheapest route\n",
        "    * Repeated Nearest neighbor algorithm\n",
        "    * Sorted edges algorithm\n",
        "    * Kruskal algorithm (spanning tree) for minimum cost spanning tree (optimal and efficient). For example energy lines\n",
        "\n",
        "If you have a fully connected [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) and and want to compute unique Hamiltonain circuits:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1389.png)\n",
        "\n",
        "Source: [Graph theory full course for Beginners](https://www.youtube.com/watch?v=sWsXBY19o8I)\n",
        "\n",
        "$^{*}$ *Why can’t a quantum computer solve NP complete problems in a polynomial running time (relative to their input)?\n",
        "Quantum computers, like classical computers, are not currently believed to be able to solve NP-complete problems in polynomial time.\n",
        "\n",
        "Contrary to Kurt Behnke’s answer, this is not true by definition. It’s a profound open question in mathematics.\n",
        "\n",
        "As Vadim Yakovlevich correctly points out, it’s partly a matter of no one having found a polynomial-time quantum algorithm for NP-complete problems after decades of trying—just like no one has found a polynomial-time classical algorithm.\n",
        "\n",
        "But it’s possible to say more than that. From reading popular articles, many people have the vague impression that a quantum computer could just try every possible solution in parallel. And if that’s all you know about them, then it’s indeed a mystery why they couldn’t solve NP-complete problems in polynomial time!\n",
        "\n",
        "The central difficulty is that, for a computer to be useful, at some point you need to measure it to observe an answer. And if you just measured an equal superposition over all possible answers, not having done anything else, the rules of QM dictate that you’ll just see a random answer. And of course, if you’d just wanted a random answer, you could’ve picked one yourself with a lot less trouble!\n",
        "\n",
        "So the name of the game, with every quantum algorithm, is somehow orchestrating a pattern of constructive and destructive quantum interference that boosts the probability of the correct answer (even though you yourself don’t know in advance which answer is the correct one!), and that does so efficiently. Famously, in 1994 Peter Shor showed how to do that for the problem of factoring integers, and a few related problems in number theory—but he was able to do so only by exploiting extremely special structure in those problems.\n",
        "\n",
        "For more “generic” search problems, like NP-complete problems, the best we generally know is Grover’s algorithm: a quantum algorithm that’s able to find the right answer (i.e., concentrate a large fraction of the amplitude on that answer) in roughly the square root of the number of steps that would be needed classically. So you get some speedup, but not an exponential one.\n",
        "\n",
        "But we also know that, if your search problem is a “black box”—i.e., you can just pick candidate solutions and evaluate if they’re correct, and you don’t know anything more about the problem’s structure—then Grover’s algorithm is the best that even a quantum computer can do. This is the content of the so-called BBBV (Bennett, Bernstein, Brassard, Vazirani) Theorem, which has several proofs, all of them crucially relying on the linearity of unitary evolution. The BBBV Theorem gives a fundamental explanation for why any fast quantum algorithm for NP-complete problems would have to look very different from anything that we know today—much like a fast classical algorithm would have to*."
      ],
      "metadata": {
        "id": "6M6WUYqnfgAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph Properties (Invariants)**\n",
        "\n",
        "* A [graph property (invariant)](https://en.m.wikipedia.org/wiki/Graph_property) is a property of graphs that depends only on the abstract structure, not on graph representations such as particular labellings or drawings of the graph.\n",
        "\n",
        "* An example graph, with the properties of\n",
        "\n",
        "  * being [planar](https://en.m.wikipedia.org/wiki/Planar_graph) (it can be drawn on the plane in such a way that its edges intersect only at their endpoints / no edges cross each other)\n",
        "\n",
        "  * and being [connected](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)),\n",
        "\n",
        "  * and with order 6,\n",
        "\n",
        "  * size 7,\n",
        "\n",
        "  * [diameter (distance](https://en.m.wikipedia.org/wiki/Distance_(graph_theory)) 3 (number of edges in a shortest path, also called a graph geodesic, connecting them)\n",
        "\n",
        "  * [girth](https://en.m.wikipedia.org/wiki/Girth_(graph_theory)) 3 (girth of an undirected graph is the length of a shortest cycle contained in the graph)\n",
        "\n",
        "  * vertex [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) 1, and\n",
        "\n",
        "  * [degree sequence](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (3, 3, 3, 2, 2, 1):\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/333px-6n-graf.svg.png)"
      ],
      "metadata": {
        "id": "03Ad29FMyOx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph enumeration**\n",
        "\n",
        "* In combinatorics, an area of mathematics, [graph enumeration](https://en.m.wikipedia.org/wiki/Graph_enumeration) describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph.\n",
        "\n",
        "* These problems may be solved either exactly (as an [algebraic enumeration](https://en.m.wikipedia.org/wiki/Algebraic_enumeration) problem) or asymptotically.\n",
        "\n",
        "* The number of labeled n-vertex simple undirected graphs is $2^{\\frac{n(n −1)}{2}}$\n",
        "\n",
        "* The number of labeled n-vertex simple directed graphs is $2^{n(n −1)}$\n",
        "\n",
        "* **Number of connected labeled graphs with n nodes**: The number of distinct connected labeled graphs with n nodes is tabulated in the [On-Line Encyclopedia of Integer Sequences](https://en.m.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences) as sequence [A001187](https://oeis.org/A001187). The first few non-trivial terms are\n",
        "\n",
        "$\\begin{array}{|l|l|}\n",
        "\\hline {\\text { n }} & {\\text { graphs }} \\\\\n",
        "\\hline 1 & 1 \\\\\n",
        "\\hline 2 & 1 \\\\\n",
        "\\hline 3 & 4 \\\\\n",
        "\\hline 4 & 38 \\\\\n",
        "\\hline 5 & 728 \\\\\n",
        "\\hline6 & 26704 \\\\\n",
        "\\hline 7 & 1866256 \\\\\n",
        "\\hline 8 & 251548592 \\\\\n",
        "\\hline\n",
        "\\end{array}$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9u6fmPYO1EZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genus of a Graph**\n",
        "\n",
        "[genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (plural genera) has a few different, but closely related, meanings. Intuitively, the genus is the number of \"holes\" of a surface.[1] A sphere has genus 0, while a torus has genus 1.\n",
        "\n",
        "A genus-2 surface:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Double_torus_illustration.png/219px-Double_torus_illustration.png)\n",
        "\n",
        "**The genus of a graph is the minimal integer n such that the graph can be drawn without crossing itself on a sphere with n handles** (i.e. an oriented surface of the genus n). Thus, a planar graph has genus 0, because it can be drawn on a sphere without self-crossing.\n",
        "\n"
      ],
      "metadata": {
        "id": "4eb_PjW92TEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph embedding**\n",
        "\n",
        "In topological graph theory, an [embedding](https://en.m.wikipedia.org/wiki/Graph_embedding) (also spelled imbedding) of a graph G on a surface $\\Sigma$ is a representation of G on $\\Sigma$ in which points of $\\Sigma$ are associated with vertices and simple arcs (homeomorphic images of $[0,1])$ are associated with edges in such a way that:\n",
        "\n",
        "the endpoints of the arc associated with an edge $e$ are the points associated with the end vertices of e, no arcs include points associated with other vertices, two arcs never intersect at a point which is interior to either of the arcs.\n",
        "\n",
        "> Often, an embedding is regarded as an equivalence class (under homeomorphisms of $\\Sigma$) of representations of the kind just described."
      ],
      "metadata": {
        "id": "UiscJVPa4BY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spanning tree**\n",
        "\n",
        "A [spanning tree](https://en.m.wikipedia.org/wiki/Spanning_tree) T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/4x4_grid_spanning_tree.svg/252px-4x4_grid_spanning_tree.svg.png)\n",
        "\n",
        "A special kind of spanning tree, the Xuong tree, is used in topological graph theory to find graph embeddings with maximum [genus](https://en.m.wikipedia.org/wiki/Genus_(mathematics)) (is the number of \"holes\" of a surface)"
      ],
      "metadata": {
        "id": "Z72Y0JpX169N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of random walks on graphs**\n",
        "\n",
        "Cover times: A cover time is the number of steps needed for a random walk to visit all vertices in a given graph.\n",
        "\n",
        "http://uu.diva-portal.org/smash/get/diva2:319078/FULLTEXT01.pdf"
      ],
      "metadata": {
        "id": "ChVvi_4nAOHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cover times of graphs and the Gaussian free field**\n",
        "\n",
        "Max point of a gaussian free field is the max number of time steps to visit each vertex in a graph\n",
        "\n",
        "https://tcsmath.wordpress.com/2010/12/09/open-question-cover-times-and-the-gaussian-free-field/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Gaussian_free_field\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/9/93/Discrete_Gaussian_free_field_on_60_x_60_square_grid.png)"
      ],
      "metadata": {
        "id": "6gIbaAJjACax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Circuit rank**\n",
        "\n",
        "* [Circuit rank](https://en.m.wikipedia.org/wiki/Circuit_rank), cyclomatic number, cycle rank, or nullity of an undirected graph is the **minimum number of edges that must be removed from the graph to break all its cycles, making it into a tree or forest**.\n",
        "\n",
        "* It is equal to the number of independent cycles in the graph (the size of a cycle basis).\n",
        "\n",
        "* This graph has circuit rank r = 2 because it can be made into a tree by removing two edges, for instance the edges 1–2 and 2–3, but removing any one edge leaves a cycle in the graph:\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/6n-graf.svg/320px-6n-graf.svg.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wJ0FciA8LKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Degree**\n",
        "\n",
        "* Degree of a matrix = how many neighbours in each vertex?\n",
        "\n",
        "* [Degree](https://en.m.wikipedia.org/wiki/Degree_(graph_theory)#Degree_sequence) (or valency) of a vertex of a graph is the number of edges that are incident to the vertex; in a multigraph, a loop contributes 2 to a vertex's degree, for the two ends of the edge.\n",
        "\n",
        "* The maximum degree of a graph G, denoted by $\\Delta (G)$, and the minimum degree of a graph, denoted by ${\\displaystyle \\delta (G)}$, are the maximum and minimum of its vertices' degrees. In the multigraph shown on the right, the maximum degree is 5 and the minimum degree is 0.\n",
        "\n",
        "* In a [regular graph](https://en.m.wikipedia.org/wiki/Regular_graph) (=each vertex has the same number of neighbors), every vertex has the same degree, and so we can speak of the degree of the graph.\n",
        "\n",
        "* A [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) (denoted $K_{n}$, where n is the number of vertices in the graph) is a special kind of regular graph where all vertices have the maximum possible degree, $n-1$.\n",
        "\n",
        "* Degree sum formula states that, given a graph $G=(V,E)$: ${\\displaystyle \\sum _{v\\in V}\\deg(v)=2|E|\\,}$ (Handshaking lemma)\n",
        "\n",
        "*Two non-isomorphic graphs with the same degree sequence (3, 2, 2, 2, 2, 1, 1, 1):*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Conjugate-dessins.svg/240px-Conjugate-dessins.svg.png)"
      ],
      "metadata": {
        "id": "iMnZwQUq6F8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Connected) Component (isolated subgraph) = Betti number $B_0$**\n",
        "\n",
        "* The number of (connected) [components](https://en.m.wikipedia.org/wiki/Component_(graph_theory)) of a topological space is an important topological invariant, the zeroth Betti number $B_0$, and the number of components of a graph is an important graph invariant, and **in topological graph theory it can be interpreted as the zeroth Betti number of the graph**.\n",
        "\n",
        "* The number of components arises in other ways in graph theory as well: **In algebraic graph theory number of components equals the multiplicity of 0 as an eigenvalue of the Laplacian matrix of a finite graph**.\n",
        "\n",
        "* bzw: The largest eigenvalue of a k-regular graph is k (Frobenius' theorem), **its multiplicity is the number of connected components of the graph**. (Der größte Eigenwert eines k-regulären Graphen ist k (Satz von Frobenius), seine Vielfachheit ist die Anzahl der Zusammenhangskomponenten des Graphen.) Taken from: https://de.m.wikipedia.org/wiki/Spektrum_(Graphentheorie)\n",
        "\n",
        "* Let G be a d-regular graph. The algebraic multiplicity of eigenvalue 0 for the Laplacian matrix is exactly 1 iff G is connected. https://math.uchicago.edu/~may/REU2013/REUPapers/Marsden.pdf\n",
        "\n",
        "* https://www.sciencedirect.com/science/article/pii/S0024379518300156"
      ],
      "metadata": {
        "id": "0tvO21EovW33"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgO45hovFfpL"
      },
      "source": [
        "**Connected Components**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Component_(graph_theory)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Connected_space\n",
        "* How the graph can be used to capture connected component\n",
        "* Any graph gives us a topological space (like joining a square as a connected component)\n",
        "* Graph can capture connected components via putting an equivalence relation on the set of vertices.\n",
        "* **equivalence relation is generated by tail of an edge (vertex 1) is equivalent to a head of an edge (vertex 2) / between the vertices of an edge (which together forms a connected component)**.\n",
        "* To generate equivalence relation we have to make sure that we enforce reflexivity, symmetry and transivity.\n",
        "* You get equivalence classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank of a graph**\n",
        "\n",
        "* the [rank of an undirected graph](https://en.m.wikipedia.org/wiki/Rank_(graph_theory)) has two unrelated definitions. Let n equal the number of vertices of the graph.\n",
        "\n",
        "* In the [matrix theory](https://en.m.wikipedia.org/wiki/Matrix_(mathematics)) of graphs the rank r of an undirected graph is defined as the rank of its adjacency matrix. Analogously, the [nullity](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) of the graph is the nullity of its adjacency matrix, which equals n − r.\n",
        "\n",
        "* In the [matroid theory](https://en.m.wikipedia.org/wiki/Matroid) of graphs the rank of an undirected graph is defined as the number n − c, where c is the number of connected components of the graph.\n",
        "\n",
        "  * Equivalently, the rank of a graph is the rank of the oriented [incidence matrix](https://en.m.wikipedia.org/wiki/Incidence_matrix) associated with the graph.\n",
        "\n",
        "  * Analogously, the [nullity of the graph](https://en.m.wikipedia.org/wiki/Nullity_(graph_theory)) is the [nullity (Kernel)](https://en.m.wikipedia.org/wiki/Kernel_(linear_algebra)) of its oriented incidence matrix, given by the formula m − n + c, where n and c are as above and m is the number of edges in the graph.\n",
        "\n",
        "  * The nullity is equal to the first [Betti number](https://en.m.wikipedia.org/wiki/Betti_number) of the graph. The sum of the rank and the nullity is the number of edges.\n"
      ],
      "metadata": {
        "id": "Hju_JaTe8iLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connectivity** (Menger's theorem, max-flow min-cut)\n",
        "\n",
        "* In mathematics and computer science, [connectivity](https://en.m.wikipedia.org/wiki/Connectivity_(graph_theory)) is one of the basic concepts of graph theory: **it asks for the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more [isolated subgraphs (= Connected component)](https://en.m.wikipedia.org/wiki/Component_(graph_theory)).**\n",
        "\n",
        "* It is closely related to the theory of network flow problems. The connectivity of a graph is an important measure of its resilience as a network.\n",
        "\n",
        "* One of the most important facts about connectivity in graphs is [Menger's theorem](https://en.m.wikipedia.org/wiki/Menger%27s_theorem): for any two vertices u and v in a connected graph G, the numbers κ(u, v) and λ(u, v) can be determined efficiently using the [max-flow min-cut theorem](https://en.m.wikipedia.org/wiki/Max-flow_min-cut_theorem) algorithm.\n",
        "\n",
        "* The vertex- and edge-connectivities of a disconnected graph are both 0.\n",
        "\n",
        "* 1-connectedness is equivalent to connectedness for graphs of at least 2 vertices.\n",
        "\n",
        "* The [complete graph](https://en.m.wikipedia.org/wiki/Complete_graph) on n vertices has edge-connectivity equal to n − 1. Every other simple graph on n vertices has strictly smaller edge-connectivity.\n",
        "\n",
        "\n",
        "*This graph becomes disconnected when the right-most node in the gray area on the left is removed:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/389px-Network_Community_Structure.svg.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8L5-Pd_FzCPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique**\n",
        "\n",
        "* Eine [Clique](https://de.m.wikipedia.org/wiki/Clique_(Graphentheorie)) bezeichnet in der Graphentheorie **eine Teilmenge von Knoten in einem ungerichteten Graphen, bei der jedes Knotenpaar durch eine Kante verbunden ist**.\n",
        "\n",
        "* Zu entscheiden, ob ein Graph eine Clique einer bestimmten Mindestgröße enthält, wird [Cliquenproblem](https://de.m.wikipedia.org/wiki/Cliquenproblem) genannt und gilt, wie das Finden von größten Cliquen, als algorithmisch schwierig (NP-vollständig).\n",
        "\n",
        "* Das Finden einer Clique einer bestimmten Größe in einem Graphen ist ein [NP-vollständiges Problem](https://de.m.wikipedia.org/wiki/NP-Vollständigkeit) (= schwierigsten Problemen in der Klasse NP gehört, also sowohl in NP liegt als auch NP-schwer ist) und somit auch in der Informationstechnik ein relevantes Forschungs- und Anwendungsgebiet.\n",
        "\n",
        "*Ein Graph mit einer Clique der Größe 3:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/6n-graf-clique.svg/350px-6n-graf-clique.svg.png)\n"
      ],
      "metadata": {
        "id": "3CZpsY2coXHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clique complex (Whitney complexes)**\n",
        "\n",
        "[Clique complexes](https://en.m.wikipedia.org/wiki/Clique_complex), independence complexes, flag complexes, Whitney complexes and conformal hypergraphs are closely related mathematical objects in graph theory and geometric topology that each describe the cliques (complete subgraphs) of an undirected graph.\n",
        "\n",
        "Siehe auch: [Topological Graph Theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory)"
      ],
      "metadata": {
        "id": "NENSroXr7Oh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CW Complex**\n",
        "\n",
        "A [CW complex](https://en.m.wikipedia.org/wiki/CW_complex) (also called cellular complex or cell complex) is a kind of a topological space that is particularly important in algebraic topology.[1] It was introduced by J. H. C. Whitehead[2] to meet the needs of homotopy theory. This class of spaces is broader and has some better categorical properties than simplicial complexes, but still retains a combinatorial nature that allows for computation (often with a much smaller complex). The C stands for \"closure-finite\", and the W for \"weak\" topology\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))"
      ],
      "metadata": {
        "id": "XdMET4Yk8PJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract simplicial complex**\n",
        "\n",
        "In combinatorics, an [abstract simplicial complex](https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex) (ASC), often called an abstract complex or just a complex, is a family of sets that is closed under taking subsets, i.e., every subset of a set in the family is also in the family. It is a purely combinatorial description of the geometric notion of a simplicial complex.[1]\n",
        "\n",
        "For example, in a 2-dimensional simplicial complex, the sets in the family are the triangles (sets of size 3), their edges (sets of size 2), and their vertices (sets of size 1).\n",
        "\n",
        "Geometric realization of a 3-dimensional abstract simplicial complex:\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Simplicial_complex_example.svg/247px-Simplicial_complex_example.svg.png)\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Let G be an undirected graph. The [clique complex](https://en.m.wikipedia.org/wiki/Clique_complex) (flag complexes, Whitney complexes) of G is an ASC whose faces are all cliques (complete subgraphs) of G. The independence complex of G is an ASC whose faces are all independent sets of G (it is the clique complex of the complement graph of G). Clique complexes are the prototypical example of flag complexes. A flag complex is a complex K with the property that every set of elements that pairwise belong to faces of K is itself a face of K.\n",
        "\n",
        "* Let M be a metric space and δ a real number. The [Vietoris–Rips complex](https://en.m.wikipedia.org/wiki/Vietoris–Rips_complex) is an ASC whose faces are the finite subsets of M with diameter at most δ. It has applications in homology theory, hyperbolic groups, image processing, and mobile ad hoc networking. It is another example of a flag complex (clique complex).\n",
        "\n"
      ],
      "metadata": {
        "id": "wAytmxq5-zzX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbqQuo7L2du"
      },
      "source": [
        "**Simplices**\n",
        "\n",
        "A [simplex](https://de.m.wikipedia.org/wiki/Simplex_(Mathematik)) consists of 3 components: vertices, edges and faces\n",
        "\n",
        "* 0-simplex: point\n",
        "* 1-simplex: edge (line)\n",
        "* 2-simplex: 3 connected points\n",
        "* 3-simplex: solid (3 dimensions)\n",
        "\n",
        "k-simplex is k-dimensional is formed using (k+1) vertices ('convex hull')\n",
        "\n",
        "* Complete graphs (every vertice is connected to all others) can be interpreted as simplices\n",
        "\n",
        "* you can interpret any graph as simplicial complex\n",
        "\n",
        "**Simplicial Complex**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Abstract_simplicial_complex\n",
        "\n",
        "* Mit einem Simplizialkomplex können die entscheidenden Eigenschaften von triangulierbar topologischen Räumen algebraisch charakterisiert werden können\n",
        "\n",
        "* Warum? Definition von Invarianten im topologischen Raum. Simplicial complexes can be seen as higher dimensional generalizations of neighboring graphs.\n",
        "\n",
        "* Wie? Untersuchung eines topologischen Raums durch Zusammenfügen von Simplizes womit eine Menge im d-dimensionalen euklidischen Raum konstruiert wird, die [homöomorph](https://de.m.wikipedia.org/wiki/Homöomorphismus) ist zum gegebenen topologischen Raum.\n",
        "\n",
        "* Die „Anleitung zum Zusammenbau“ der Simplizes, das heißt die Angaben darüber, wie die Simplizes zusammengefügt sind, wird dann in Form einer Sequenz von [Gruppenhomomorphismen](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus) rein algebraisch charakterisiert.\n",
        "\n",
        "* Cells can have various dimensions: vertices, edges, triangles, tetrahedra and their higher dimensional analogues. complexes reflect the correct topology of the data\n",
        "\n",
        "* If we glue many simplices together in such a way that the intersection is also a simplex (along an edge for example), we obtain a simplicial complex. If we see three points connected by edges that form a triangle, we fill in the triangle with a 2-dimensional face. Any four points that are all pairwise connected get filled in with a 3-simplex etc. The resulting simplicial complex is called (Vietoris) Rips complex.\n",
        "\n",
        "* The persistence diagram is not computed directly from L􏰑ε. Instead, one forms an object called a Cech complex. Simplicial and cubical complexes are examples of cell complexes. Cech complex is an example of a simplicial complex. - in practice, often used Vietoris-Rips complex Vε - the persistent homology defined by Vε approximates the persistent homology defined by Cε.\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_03.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hSqsZ5LLdvz"
      },
      "source": [
        "**Triangulation**\n",
        "\n",
        "* In der Topologie ist eine [Triangulierung](https://en.m.wikipedia.org/wiki/Triangulation_(topology)) oder Triangulation eine **Zerlegung eines Raumes in Simplizes** (Dreiecke, Tetraeder oder deren höher-dimensionale Verallgemeinerungen).\n",
        "\n",
        "* Mannigfaltigkeiten bis zur dritten Dimension sind stets triangulierbar.\n",
        "\n",
        "* Ursprüngliche Motivation für die Hauptvermutung war der Beweis der topologischen Invarianz kombinatorisch definierter Invarianten wie der [simplizialen Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie). Trotz des Scheiterns der Hauptvermutung lassen sich Fragen dieser Art oftmals mit dem [simplizialen Approximationssatz](https://de.m.wikipedia.org/wiki/Simpliziale_Approximation) beantworten.\n",
        "\n",
        "* Triangulation ist eine Zerlegung eines Raumes in Simplizes (Dreiecke, Tetraeder oder höher-dimensionale Verallgemeinerungen) to tease out properties of manifolds\n",
        "\n",
        "* **A triangulation of a topological space X is a simplicial complex K, homeomorphic to X, together with a homeomorphism h: K → X**\n",
        "\n",
        "* triangulation offers a concrete way of visualizing spaces that are difficult to see, and helps computing an invariant.\n",
        "\n",
        "* Ist gegeben durch einen (abstrakten) Simplizialkomplex K und Homöomorphismus h : | K | → X der geometrischen Realisierung | K | auf X.\n",
        "https://en.wikipedia.org/wiki/Triangulation_(topology)\n",
        "\n",
        "* a two-dimensional sphere (surface of a solid ball) can be approximated by gluing together two-dimensional triangles, and a three-dimensional sphere can be approximated by gluing together three-dimensional tetrahedra.\n",
        "\n",
        "* Triangles and tetrahedra are examples of more general shapes called simplices, which can be defined in any dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRA0yDZJGOjK"
      },
      "source": [
        "**[Filtration](https://de.m.wikipedia.org/wiki/Filtrierung_(Mathematik)) & Inclusion Maps**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtered_algebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Filtration_(mathematics)\n",
        "\n",
        "* **With increasing size of d, we are dealing with a sequence of simplicial complexes, each a sub-complex of the next. That is, a simplicial complex constructed from data for some small distance is a subset of the simplicial complex constructed for a larger distance**.\n",
        "\n",
        "* Equally there is an **[inclusion map](https://en.m.wikipedia.org/wiki/Inclusion_map) from each simplicial complex to the next**.\n",
        "\n",
        "* **This sequence of simplicial complexes, with inclusion maps, is called a filtration**.\n",
        "\n",
        "* When we apply homology to a filtration, we obtain an algebraic structure called persistent modul.\n",
        "\n",
        "* So if we want to compute **ith homology with coefficients from a field k**.\n",
        "\n",
        "* The **homology of any complex Cj is a vector space**, and the **inclusion maps between complexes induce linear maps between homology vector spaces**.\n",
        "\n",
        "* The direct sum of the homology vector spaces is an algebraic module - in fact a **graded module over the polynomial ring** k[x]. The variable x acts as a **shift map**, taking each homology generator to its image in the next vector space.\n",
        "\n",
        "* Furthermore, a structure theorem tells us that **a persistent module decomposes nicely into a direct sum of simple modules**, each corresponding to a bar in the barcode. This means: a barcode reall is an algebraic structure.\n",
        "\n",
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS9TpoyGEmjj"
      },
      "source": [
        "![ccc](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_01.jpg)\n",
        "\n",
        "![cscsvs](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_02.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Combinatorics*"
      ],
      "metadata": {
        "id": "DLTQm1K2gqMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Combinatorics"
      ],
      "metadata": {
        "id": "FJezNJ6gguxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Permutation\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Galoistheorie"
      ],
      "metadata": {
        "id": "NFJlTOQeCFov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permutationen und Kombinatorik sind beide Begriffe, die in der Wahrscheinlichkeitstheorie und Statistik verwendet werden. Sie befassen sich mit der Anordnung von Objekten und der Anzahl der Möglichkeiten, wie Dinge arrangiert oder ausgewählt werden können. Hier sind die Grunddefinitionen:\n",
        "\n",
        "1. **Permutationen**: Bei einer Permutation geht es um die Anordnung von Objekten, wobei die Reihenfolge wichtig ist. Eine Permutation ist also eine Anordnung von Objekten in einer bestimmten Reihenfolge. Zum Beispiel, wenn wir die Zahlen 1, 2 und 3 haben, gibt es 6 mögliche Permutationen: 123, 132, 213, 231, 312 und 321.\n",
        "\n",
        "2. **Kombinatorik**: Die Kombinatorik ist ein breiterer Begriff, der die Studie der Möglichkeiten der Anordnung und Auswahl von Objekten beinhaltet. Sie umfasst sowohl Permutationen als auch Kombinationen. Kombinationen sind eine Unterkategorie der Kombinatorik, bei der es um die Auswahl von Objekten geht, wobei die Reihenfolge nicht wichtig ist. Wenn wir zum Beispiel aus den Zahlen 1, 2 und 3 zwei Zahlen auswählen, gibt es 3 mögliche Kombinationen: {1,2}, {1,3}, {2,3}.\n",
        "\n",
        "Somit ist die Kombinatorik ein umfassenderes Konzept, das Permutationen, Kombinationen und andere ähnliche Konzepte umfasst. Sie ist das Studium der Methoden zur Zählung, Anordnung und Auswahl von Objekten. Permutationen sind ein spezifischer Teil dieses Studiums, wo die Reihenfolge der ausgewählten Objekte von Bedeutung ist."
      ],
      "metadata": {
        "id": "uEM9FO3ZCEDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Category Theory*"
      ],
      "metadata": {
        "id": "jNQs-MXfBmVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://categoricaldeeplearning.com/"
      ],
      "metadata": {
        "id": "u9d2ZFbxQE_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[A monad is a monoid in the category of endofunctors](https://youtu.be/ENo_B8CZNRQ)"
      ],
      "metadata": {
        "id": "zEr2NHhF-n1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Category theory and group theory are both branches of abstract algebra, but they are quite different in focus and scope.\n",
        "\n",
        "1. **Group Theory**: Group theory studies the algebraic structures known as groups, which are sets equipped with an operation that combines any two of its elements to form a third element in a way that satisfies four conditions: closure, associativity, identity, and invertibility. Group theory is a very concrete branch of abstract algebra, and its concepts are used throughout mathematics.\n",
        "\n",
        "2. **Category Theory**: Category theory is a higher level of abstraction. In category theory, the emphasis is not on the individual objects (like groups, rings, vectors spaces, etc.) but rather on the morphisms or maps between these objects and the structure-preserving transformations that can be defined. Categories can be thought of as an abstract way to represent mathematical concepts and their relationships.\n",
        "\n",
        "The relationship between these two branches of mathematics is that group theory can be viewed through the lens of category theory:\n",
        "\n",
        "- A group itself can be viewed as a category. Specifically, it's a category with a single object in which every morphism (which represent the group elements) is invertible.\n",
        "\n",
        "- Furthermore, the concept of a functor in category theory (a map between categories that preserves their structure) corresponds to the concept of a group homomorphism in group theory (a function between two groups that preserves the group operation).\n",
        "\n",
        "So, category theory provides a unifying framework in which group theory and many other branches of mathematics can be studied. It's a way of looking at mathematical structures and their relationships at a higher level of abstraction.\n"
      ],
      "metadata": {
        "id": "_lPCBI7TDmRo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eLPSGAeDW0j"
      },
      "source": [
        "[Category](https://en.m.wikipedia.org/wiki/Category_(mathematics)) and [Category theory](https://en.m.wikipedia.org/wiki/Category_theory)\n",
        "\n",
        "* Eine Kategorie besteht aus Objekten und Morphismen. Man möchte jedem Objekt in einer Kategorie ein Objekt in der anderen Kategorie zuordnen, und das gleiche mit den Morphismen zwischen den Objekten.\n",
        "\n",
        "* [Objects](https://ncatlab.org/nlab/show/object)\n",
        "\n",
        "* [Product und Coproduct](https://de.m.wikipedia.org/wiki/Produkt_und_Koprodukt)\n",
        "\n",
        "* [Anfangsobjekt (initiales), Endobjekt (terminales, finales) und Nullobjekt](https://de.m.wikipedia.org/wiki/Anfangsobjekt,_Endobjekt_und_Nullobjekt)\n",
        "\n",
        "* [Functor](\n",
        "https://ncatlab.org/nlab/show/functor): A homomorphism between categories is a functor. Zuordnung zwischen zwei Kategorien\n",
        "\n",
        "  * Funktoren werden auch Diagramme genannt (mitunter nur in bestimmten Kontexten), da sie eine formale Abstraktion [kommutativer Diagramme](https://de.m.wikipedia.org/wiki/Kommutatives_Diagramm) darstellen.\n",
        "\n",
        "  * functors must preserve [identity morphisms](https://en.m.wikipedia.org/wiki/Morphism#Definition) and [composition of morphisms](https://en.m.wikipedia.org/wiki/Function_composition)\n",
        "\n",
        "* [Natural Transformations](https://en.m.wikipedia.org/wiki/Natural_transformation) are Maps between functors (and functors are maps between categories)\n",
        "\n",
        "* [Duality](https://en.m.wikipedia.org/wiki/Dual_(category_theory)) is a correspondence between the properties of a category C and the dual properties of the opposite category C<sup>op</sup>.\n",
        "\n",
        "* Topos: Category theorists have proposed [topos theory](https://en.m.wikipedia.org/wiki/Topos) as an alternative to traditional [axiomatic set theory](https://de.m.wikipedia.org/wiki/Axiomatische_Mengenlehre). Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory.\n",
        "\n",
        "> [Outline of category theory](https://en.m.wikipedia.org/wiki/Outline_of_category_theory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlwPUfSMCiu"
      },
      "source": [
        "**Category Theory and Higher Category Theory**\n",
        "\n",
        "0) A category is a collection of objects and morphisms between those objects that satisfy some rules.\n",
        "\n",
        "1) A functor is a morphism in the category of categories.\n",
        "\n",
        "2) A natural transformation is a morphism in the category of functors.\n",
        "\n",
        "But they all stop right there. What about:\n",
        "\n",
        "3) the morphisms in the category of natural transformations?\n",
        "\n",
        "4) Or the \"morphisms in the category of the morphisms in the category of natural transformations\"\n",
        "\n",
        "*Definition*\n",
        "\n",
        "* [higher category theory](https://en.m.wikipedia.org/wiki/Higher_category_theory) bzw. [higher category theory](https://ncatlab.org/nlab/show/higher+category+theory) is the part of category theory at a higher order, which means that some equalities are replaced by explicit arrows in order to be able to explicitly study the structure behind those equalities.\n",
        "\n",
        "* Higher category theory is often applied in algebraic topology (especially in homotopy theory), where one studies algebraic invariants of spaces, such as their fundamental weak ∞-groupoid.\n",
        "\n",
        "* *From 2-category to Higher Order Category*: The concept of 2-category generalizes further in higher category theory to n-categories, which have k-morphisms for all\n",
        "k\n",
        "≤\n",
        "n\n",
        ". The morphisms can be composed along the objects, while the 2-morphisms can be composed in two different directions: along objects – called horizontal composition – and along morphisms – called vertical composition. The composition of morphisms is allowed to be associative only up to coherent associator 2-morphisms.\n",
        "\n",
        "* See also: [infinity-category](https://ncatlab.org/nlab/show/infinity-category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsmc7LDx_K-"
      },
      "source": [
        "*Morphismen (Linear Maps)*\n",
        "\n",
        "Ein [Morphismus](https://de.m.wikipedia.org/wiki/Morphismus) ist eine Funktion in Kategorientheorie. Man schreibt: $f\\colon X\\to Y$. Image Source: [Morphismen](https://youtu.be/0wKsFNLR15g)\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/morphismus2.jpg)\n",
        "\n",
        "* [Automorphismus](https://de.m.wikipedia.org/wiki/Automorphismus): bijektiv, Definitionsmenge = Zielmenge\n",
        "\n",
        "* [Isomorphismus](https://de.m.wikipedia.org/wiki/Isomorphismus): bijektiv, Definitionsmenge ≠ Zielmenge\n",
        "\n",
        "* [Endomorphismus](https://de.m.wikipedia.org/wiki/Endomorphismus): Definitionsmenge = Zielmenge. aber Bildmenge umfasst nicht den ganzen Vektorraum (Ziele < Usprünge)\n",
        "\n",
        "* [Monomorphismus](https://de.m.wikipedia.org/wiki/Monomorphismus): injektiv (jedes Element des Usprungs hat ein exklusives Element)\n",
        "\n",
        "* [Epimorphismus](https://de.m.wikipedia.org/wiki/Epimorphismus): surjektiv (jedes Element im Ziel ist mind 1 mal getroffen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNkozbcHgYo"
      },
      "source": [
        "**Deep Dive: Homomorphismus**\n",
        "\n",
        "> Eine lineare Abbildung ist ein (Homo-)Morphismus zwischen Vektorräumen.\n",
        "\n",
        "*Unterschied zwischen Isomorphismus und Homomorphismus:*\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/isomorphismus.JPG)\n",
        "\n",
        "Ein [Vektorraum-Homomorphismus](https://de.m.wikipedia.org/wiki/Homomorphismus) (\"Lineare Abbildung\") ist eine Abbildung $\\varphi: V \\rightarrow W$ zwischen $K$ -Vektorräumen $V$ und $W$ (gemeinsamer Grundkörper $K$) mit den folgenden Eigenschaften:\n",
        "\n",
        "\n",
        "1. **Additivität**: $\\varphi(v+u)=\\varphi(v)+\\varphi(u)$ für alle $u$ und $v \\in V$\n",
        "\n",
        "2. **Homogenität**: $\\varphi(\\lambda \\cdot v)=\\lambda \\cdot \\varphi(v) \\quad$ für alle $\\lambda \\in K$ und $v \\in V$ (Skalarmultiplikation)\n",
        "\n",
        "* Beispiel: Bei einer linearen Abbildung ist es unerheblich, ob man zwei Vektoren zuerst addiert und dann deren Summe abbildet oder zuerst die Vektoren abbildet und dann die Summe der Bilder bildet. Gleiches gilt für die Multiplikation mit einem Skalar aus dem Grundkörper.\n",
        "\n",
        "* In der Funktionalanalysis, bei der Betrachtung unendlichdimensionaler Vektorräume, die eine Topologie tragen, spricht man meist von linearen Operatoren statt von [linearen Abbildungen](https://de.m.wikipedia.org/wiki/Lineare_Abbildung).\n",
        "\n",
        "\n",
        "![xyz](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Reflection_of_a_triangle_about_the_y_axis.svg/320px-Reflection_of_a_triangle_about_the_y_axis.svg.png)\n",
        "\n",
        "*Achsenspiegelung als Beispiel einer linearen Abbildung*\n",
        "\n",
        "**Beispiele für Vektorraumhomomorphismus**\n",
        "\n",
        "* Für $V=W=\\mathbb{R}$ hat jede lineare Abbildung die Gestalt $f(x)=m x$ mit $m \\in \\mathbb{R}$\n",
        "\n",
        "* Es sei $V=\\mathbb{R}^{n}$ und $W=\\mathbb{R}^{m}$. Dann wird für jede $m \\times n$ -Matrix $A$ mit Hilfe der Matrizenmultiplikation eine lineare Abbildung $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ durch\n",
        "\n",
        "$f(x)=A x=\\left(\\begin{array}{ccc}a_{11} & \\cdots & a_{1 n} \\\\ \\vdots & & \\vdots \\\\ a_{m 1} & \\cdots & a_{m n}\\end{array}\\right)\\left(\\begin{array}{c}x_{1} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right)$\n",
        "definiert.\n",
        "\n",
        "Jede lineare Abbildung von $\\mathbb{R}^{n}$ nach $\\mathbb{R}^{m}$ kann so dargestellt werden.\n",
        "\n",
        "Siehe auch: [Gruppenhomomorphismus](https://de.m.wikipedia.org/wiki/Gruppenhomomorphismus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Morphismen:**\n",
        "\n",
        "Für manche Kategorien gibt es besondere Bezeichnungen für Morphismen.\n",
        "\n",
        "* Ein [Homöomorphismus](https://de.m.wikipedia.org/wiki/Homöomorphismus) ist ein Isomorphismus zwischen topologischen Räumen. Sind (beispielsweise) die [Fundamentalgruppen](https://de.m.wikipedia.org/wiki/Fundamentalgruppe) zweier Räume isomorph, so sind die Räume homöomorph.\n",
        "\n",
        "* Ein [Diffeomorphismus](https://de.m.wikipedia.org/wiki/Diffeomorphismus) ist ein Isomorphismus zwischen differenzierbaren Mannigfaltigkeiten.\n",
        "\n",
        "* Eine [Isometrie](https://de.m.wikipedia.org/wiki/Isometrie) ist ein Isomorphismus in der Kategorie der metrischen Räumen mit den nichtexpansiven stetigen Abbildungen."
      ],
      "metadata": {
        "id": "p_vf2S1FOc2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Group Theory*"
      ],
      "metadata": {
        "id": "_OexAhQsM_ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary*"
      ],
      "metadata": {
        "id": "sANBVESIRpMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Galois Group: https://youtu.be/Ct2fyigNgPY"
      ],
      "metadata": {
        "id": "qO3q1326RpMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Group_theory"
      ],
      "metadata": {
        "id": "lyHEyEvuRpMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representation theory & Group representation\n",
        "\n",
        "[Representation theory: The ‘Useless’ Perspective That Transformed Mathematics](https://www.quantamagazine.org/the-useless-perspective-that-transformed-mathematics-20200609/)\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Representation_theory\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Group_representation"
      ],
      "metadata": {
        "id": "ae45CfASRpMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kongruenzklassen, Äquivalenzklassen & Restklassen (Modulorechnung)**\n",
        "\n",
        "* **Kongruenz**: [Kongruenz (Zahlentheorie)](https://de.m.wikipedia.org/wiki/Kongruenz_(Zahlentheorie))) Die Kongruenz zwischen zwei ganzen Zahlen ist in Bezug auf einen Teiler definiert. **Der Teiler heißt in diesem Zusammenhang Modul.**\n",
        "  * Gegeben sei ein Modul $m \\in \\mathbb{N}$. Zwei ganze Zahlen $a$ und $b$ heißen kongruent modulo $m$, **wenn die Division von $a$ und $b$ durch $m$ den gleichen Rest $r$ lässt**.\n",
        "  * Kongruenz ist eine Art Erweiterung der Modulorechnung: **Modulorechnung**: 17 mod 3 = 2.\n",
        "  * **Kongruenz: 11 $\\equiv$ 17 mod 3 (weil bei beiden der Rest 2 ist - beide sind in der der gleichen Restklasse)**\n",
        "  * Man kann alternativ zur [**Restklassenermittlung**](https://de.m.wikipedia.org/wiki/Restklasse) auch sagen, dass zwei Zahlen kongruent sind (modulo der natürlichen Zahl n), wenn ihre Differenz durch n teilbar ist. Hier: 17 - 11 = 6, ist teilbar durch 3.\n",
        "\n",
        "* [**Äquivalenz und Äquivalenzklassen** (Congruence Classes)](https://de.m.wikipedia.org/wiki/Äquivalenzrelation). Äquivalenz: Objekte, die sich in einem bestimmten Zusammenhang gleichen, als gleichwertig bzw. äquivalent angesehen. The result of the modulo operation is an equivalence class (Äquivalenzklassen). Any member of the class may be chosen as representative.\n",
        "\n",
        "  * Von besonderem Interesse sind jedoch solche Äquivalenzrelationen $\\equiv$ , deren Quotientenabbildung $\\mathrm{q}_{\\mathrm{z}}: A \\rightarrow A / \\equiv, a \\mapsto[a]_{\\equiv}$ **mit der Struktur auf $A$ verträglich bzw. ein Homomorphismus ist**, weil dann die von $\\mathrm{q}_{=}$ erzeugte Struktur auf der [Quotientenmenge](https://de.wikipedia.org/wiki/Äquivalenzrelation#Quotientenmenge_und_Partition) $A / \\equiv$ von der gleichen Art ist wie die von $A$. **Eine solche Äquivalenzrelation $\\equiv$ nennt man eine Kongruenzrelation auf der strukturierten Menge $A$.** <font color=\"blue\">Die Quotientengruppe G/N ist homomorph zur Gruppe G.</font>\n",
        "\n",
        "* **[Restklassen](https://de.wikipedia.org/wiki/Restklasse)** sind die Äquivalenzklassen in der Kongruenzrelation. Eine Zahl a modulo einer Zahl m die Menge aller Zahlen, die bei Division durch m denselben Rest lassen wie a. See also [Modulo Operation](https://en.m.wikipedia.org/wiki/Modulo_operation) (Restklassenrechnung).\n",
        "\n",
        "* In der Gruppentheorie werden Äquivalenzklassen als [Nebenklassen (Cosets)](https://de.m.wikipedia.org/wiki/Gruppentheorie#Nebenklassen) bezeichnet."
      ],
      "metadata": {
        "id": "JWNcshgSRpMS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8et109dRpMT"
      },
      "source": [
        "**Subgroups, Normal Subgroups & Cosets**\n",
        "\n",
        "* **Subgroups (Unterguppen):**\n",
        "\n",
        "  * Gegeben sei $\\mathbb{Z}$ die Gruppe der ganzen Zahlen mit der Addition als Gruppenoperation. Man kann fur diese Gruppe verschiedene (unendlich viele) **[Untergruppen](https://de.m.wikipedia.org/wiki/Untergruppe) (Subgroups)** bilden: 2$\\mathbb{Z}$, 3$\\mathbb{Z}$, 4$\\mathbb{Z}$, 5$\\mathbb{Z}$, 6$\\mathbb{Z}$, 7$\\mathbb{Z}$.\n",
        "\n",
        "  * Beispiel: Die ganzen Zahlen $\\mathbb {Z} $ sind bezüglich der Addition eine Untergruppe der rationalen Zahlen $\\mathbb {Q} $.\n",
        "\n",
        "  * Two standard subgroups / every group has at least 2 subgroups: the identity element {e} and the entire group G. These are technically **normal subgroups (Normalteiler)**.\n",
        "\n",
        "  * If a group has no other normal subgroups then these two, than it's called a **simple group**. A simple group does not have any factor (quotient) groups, but they are the building blocks of other groups.\n",
        "\n",
        "* **Normal Subgroups (Normalteiler)**:\n",
        "\n",
        "  * <font color=\"blue\">**Die Äquivalenzklasse mit dem Rest Null ist der Normalteiler**. Normalteiler sind spezielle Untergruppen, und ihre Bedeutung liegt vor allem darin, **dass sie genau die Kerne von Gruppenhomomorphismen sind** (=Lösungsmenge ist Null, also Null als Rest).</font> Normal Subgroups heissen auch \"invariant or self-conjugate subgroups\".\n",
        "\n",
        "  * Normal subgroups determine what kinds of homomorphisms are possible from a group $G$ to other groups $f : G -> H$.\n",
        "\n",
        "  * Trivial examples: Standard subgroups identity element {e} and the entire group G.\n",
        "\n",
        "  * Die Gruppe $\\mathbb{Z}$ ist **abelsch** (kommutativ) und somit ist jede Untergruppe auch ein **[Normalteiler](https://de.wikipedia.org/wiki/Normalteiler) bzw. [Normal Subgroup](https://en.wikipedia.org/wiki/Normal_subgroup)**.\n",
        "\n",
        "* **Nebenklassen (Cosets)**:\n",
        "\n",
        "  * Nebenklassen werden benutzt, um den [Satz von Lagrange](https://de.m.wikipedia.org/wiki/Satz_von_Lagrange) zu beweisen, um die Begriffe [Normal Subgroup (Normalteiler)](https://de.m.wikipedia.org/wiki/Normalteiler) und [Quotient Group (Faktorgruppe)](https://de.m.wikipedia.org/wiki/Faktorgruppe) zu erklären und um Gruppenoperationen zu studieren.\n",
        "\n",
        "  * In contrast to Subgroups, the Coset (Nebenklasse) are not closed under addition, have no inverse and don't contain the identity element\n",
        "\n",
        "  * Cosets are there to define how many (finite) possible subgroups exist in a group. There are **two types of cosets**: left cosets and right cosets.\n",
        "\n",
        "  * A subgroup $H$ of a group $G$ may be used to decompose the underlying set of $G$ into disjoint, equal-size subsets called [cosets](https://en.m.wikipedia.org/wiki/Coset)\n",
        "\n",
        "* <font color=\"blue\">**Example: Use Normal Subgroup 5$\\mathbb{Z}$ von $\\mathbb{Z}$ (integers mod 5) to divide a Group into Cosets. We get 5 sets of remainders (the congruence classes) which are the [Quotient Group (Faktorgruppe)](https://en.m.wikipedia.org/wiki/Quotient_group), a group with 5 elements: $\\mathbb{Z}$ mod 5 = {$\\overline{0}$, $\\overline{1}$, $\\overline{2}$, $\\overline{3}$, $\\overline{4}$}.**</font>\n",
        "\n",
        "    * 5$\\mathbb{Z}$ + Rest 0 = $\\overline{0} : \\{\\ldots-10,-5,0,5,10 \\ldots\\}$ **(Normal) Subgroup**, which is technically also a Coset 0+ 5 $\\mathbb{Z}$\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 1 = $\\overline{1} : \\{\\ldots-9,-4,1,6,11 \\ldots\\}$ **Coset 1+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 2 = $\\overline{2} : \\{\\ldots-8,-3,2,7,12 \\ldots\\}$ **Coset 2+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 3 = $\\overline{3} : \\{\\ldots-7,-2,3,8,13 \\ldots\\}$ **Coset 3+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 4 = $\\overline{4} : \\{\\ldots-6,-1,4,9,14 \\ldots\\}$ **Coset 4+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "* **Quotient Group (Faktorgruppe)**:\n",
        "\n",
        "  * Die Faktorgruppe oder Quotientengruppe wird mit $G / N$ bezeichnet und ist die Menge der Nebenklassen (Cosets). Aus einer Gruppe $G$ und jedem ihrer Normalteiler $N$ lässt sich eine Faktorgruppe $G/N$ bilden.\n",
        "\n",
        "    * Die Quotientengruppe unterteilt eine Menge in Äquivalenzklassen bzw. eine Gruppe in Restklassen / Nebenklassen. Diese Menge der Restklassen (Äquivalenzklassen) heisst **[Quotientenmenge](https://de.wikipedia.org/wiki/Äquivalenzrelation#Quotientenmenge_und_Partition) bzw. Faktormenge**.\n",
        "\n",
        "  * Diese [Quotientengruppen](https://de.wikipedia.org/wiki/Faktorgruppe) sind homomorphe Bilder von G, **und jedes homomorphe Bild von G ist zu einer solchen Quotientengruppe G/N isomorph**. See fundamental theorem on homomorphisms of groups as \"**Every homomorphic image of a group is isomorphic to a quotient group**\". [Source](https://en.m.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms)\n",
        "\n",
        "  * For finite groups you can find a chain of normal subgroups called a \"composition series\" which acts as a kind of 'prime factorization' of the group (1 ◃ N1, ◃ N2 ◃ ... ◃ Nr ◃ G ). Normal subgroups can also be used to study fields (Körper), i.e. in Galois theory (Field extension K / F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Centralizer and Normalizer**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Centralizer_and_normalizer\n",
        "\n",
        "In group theory, a branch of mathematics, the normalizer of a subset of a group is a certain associated subgroup.\n",
        "\n",
        "Let's suppose we have a group G and a subset S of G. The normalizer of S in G is the set of all elements in G that commute with S in a certain sense. More specifically, the normalizer is the largest subgroup of G in which S is a subset that commutes with each element of the subgroup under the group operation of conjugation.\n",
        "\n",
        "Formally, the normalizer of S in G, denoted by N_G(S), is defined as:\n",
        "\n",
        "$N_G(S)$ = $g$ in $G$ : $gSg^{(-1)} = S$\n",
        "\n",
        "Here, $gSg^{(-1)})$ = S means that for every element s in S, there exists some s' in S such that gsg^(-1) = s'. In words, the normalizer of S in G consists of all elements g of G such that when any element of S is conjugated by g (i.e., you multiply it by g on the left and by the inverse of g on the right), the result is still an element of S.\n",
        "\n",
        "In simple terms, the normalizer of a subset S of a group G is the largest subgroup of G in which S \"behaves nicely\", in the sense that the action of conjugation by elements of the subgroup leaves S unchanged. The concept of the normalizer is particularly important in the study of the structure and symmetries of groups.\n"
      ],
      "metadata": {
        "id": "NMh7R-ldRpMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizer: conjugating with elements of the group results still in same group**\n",
        "\n",
        "Sure, let's consider a simple example with a finite group. Let's take the symmetric group S_3 (the group of all permutations of three elements), which has six elements: {e, (12), (13), (23), (123), (132)} where e is the identity permutation, (12) swaps elements 1 and 2, etc.\n",
        "\n",
        "Now let's consider the subset S = {(12), e}.\n",
        "\n",
        "We want to find the normalizer of this set. **By the definition of the normalizer, it consists of all elements g in S_3 such that when any element of S is conjugated by g, the result is still an element of S**. So we need to find all elements g in S_3 such that g(12)g^(-1) is in S and g(e)g^(-1) is in S.\n",
        "\n",
        "Well, conjugating the identity e by any element gives e back, so that part is trivial.\n",
        "\n",
        "So let's look at the conjugation of (12) by each element of S_3:\n",
        "\n",
        "- Conjugating by e gives (12), which is in S.\n",
        "- Conjugating by (12) gives (12), which is in S.\n",
        "- Conjugating by (13) gives (23), which is not in S.\n",
        "- Conjugating by (23) gives (13), which is not in S.\n",
        "- Conjugating by (123) gives (13), which is not in S.\n",
        "- Conjugating by (132) gives (23), which is not in S.\n",
        "\n",
        "So, the normalizer of S in S_3 is {e, (12)}. This is the largest subgroup of S_3 in which S is a subset that commutes with each element of the subgroup under the operation of conjugation."
      ],
      "metadata": {
        "id": "H-ZQ6TG2RpMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generators of a group*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0942.jpg)\n",
        "\n",
        "> **Each group SU(3) x SU(2) x U(1) leads to a symmetry resulting in a conservation law**"
      ],
      "metadata": {
        "id": "cpVZs5agRpMT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_Bw2XURpMU"
      },
      "source": [
        "**Group Operations: Zusammenfassung algebraischer Strukturen mit Operationen**\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ is a Group\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ is a Ring\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ $÷$ $\\quad$ is a Field\n",
        "\n",
        "\n",
        "A group is a set $G$ with an operation $*$ such that\n",
        "\n",
        "1. **Closure** [Abgeschlossenheit](https://de.m.wikipedia.org/wiki/Abgeschlossenheit_(algebraische_Struktur)): If $x$ and $y$ are in $G$ then $x * y$ is in $\\mathrm{G}$\n",
        "  * until here it is a [Magma (Groupoid)](https://de.m.wikipedia.org/wiki/Magma_(Mathematik))\n",
        "  * Beispiele (Magmen, die keine Halbgruppen sind):\n",
        "    * $(\\mathbb{Z},-):$ die ganzen Zahlen mit der Subtraktion\n",
        "    * (R $\\backslash\\{0\\}, /)$ : die reellen Zahlen ungleich 0 mit der Division\n",
        "    * Die natürlichen Zahlen mit der Exponentiation, also mit der Verknüpfung $a * b=a^{b}$\n",
        "    * Die reellen Zahlen mit der Bildung des arithmetischen Mittels als Verknüpfung\n",
        "\n",
        "2. **Associativity** [Assoziativität](https://de.m.wikipedia.org/wiki/Assoziativgesetz): For all $x, y, z$ in $G$ $\\text { we have }(x * y) * z=x *(y * z)$\n",
        "  * until here it is a [Halbgruppe](https://de.m.wikipedia.org/wiki/Halbgruppe)\n",
        "  * Beispiel: Die Menge $\\mathbb  N$ $_0$ = {0, 1, 2 ..} der natürlichen Zahlen bildet mit der gewöhnlichen Addition eine kommutative und kürzbare Halbgruppe ($\\mathbb  N$ $_0$,+), die keine Gruppe ist. Da hier die negativen Zahlen fehlen, also die „Hälfte“ der abelschen Gruppe ($\\mathbb Z,+$) der ganzen Zahlen, lag der Name Halbgruppe für diese mathematische Struktur nahe.\n",
        "\n",
        "3. **Identity Element** [Neutrales Element](https://en.m.wikipedia.org/wiki/Identity_element): There is an element $e$ in $G$ such that $e * x=x * e=x$ for all $x$ in $G$\n",
        "  * until here it is a [Monoid](https://de.m.wikipedia.org/wiki/Monoid)\n",
        "  * Beispiel: die natürlichen Zahlen mit der Addition und der Zahl 0 als neutralem Element.\n",
        "\n",
        "4. **Inverse Elements** [Inverse Elemente](https://de.m.wikipedia.org/wiki/Inverses_Element): For each element $x$ in $G,$ there is an element $x^{-1}$ such that $x * x^{-1}=x^{-1} * x=e$\n",
        "  * until here it is a [Group](https://de.m.wikipedia.org/wiki/Gruppe_(Mathematik))\n",
        "  * Beispiel: die Menge der ganzen Zahlen zusammen mit der Addition\n",
        "  * Ringe, Körper (Field), Moduln und Vektorräume sind Gruppen mit zusätzlichen Strukturen und Eigenschaften\n",
        "\n",
        "5. **Kommutative** [Kommutativität](https://de.m.wikipedia.org/wiki/Kommutativgesetz): Für alle $a, b \\in G$ gilt: $a * b=b * a$\n",
        "  * until here it is an [Abelian Group](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe)\n",
        "  * Beispiel:\n",
        "    * $(\\mathbb {Z} ,+)$ ist die wichtigste abelsche Gruppe. Dabei ist Z die Menge der ganzen Zahlen und + die gewöhnliche Addition.\n",
        "    * $(\\mathbb {Q}^{\\cdot} , \\cdot)$ ist eine abelsche Gruppe. Dabei ist $\\mathbb {Q}^{\\cdot}$ die Menge der rationalen Zahlen ohne die 0 und ⋅ ist die gewöhnliche Multiplikation. Die Null muss hierbei ausgeschlossen werden, da sie kein inverses Element besitzt: „1/0“ ist nicht definiert\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Verschiebungen.\n",
        "    * Die Menge der Drehungen in einer Ebene um einen Punkt bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Drehungen.\n",
        "    * Die Menge der Drehstreckungen in einer Ebene bilden eine abelsche Gruppe.\n",
        "    * Die Menge der endlichen Dezimalzahlen sind bezüglich der Multiplikation keine abelsche Gruppe. Zum Beispiel hat die Zahl 3 kein Inverses bezüglich der Multiplikation. $\\displaystyle {\\frac {1}{3}}$ lässt sich nicht als endlicher Dezimalbruch schreiben. Bezüglich der normalen Addition bilden die endlichen Dezimalbrüche eine abelsche Gruppe.\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Verschiebungen.\n",
        "  * [Free Abelian Group](https://de.m.wikipedia.org/wiki/Freie_abelsche_Gruppe): eine abelsche Gruppe, die als $\\mathbb {Z}$-Modul eine Basis hat. Ist G eine freie abelsche Gruppe ist, so wird man eine Basis wählen und alle Elemente als Linearkombinationen von Elementen dieser Basis ausdrücken. Allerdings sollte man betonen, dass es meist keine ausgezeichnete Basis geben wird.\n",
        "\n",
        "6. **Distributive law** [Distributivgesetz](https://de.m.wikipedia.org/wiki/Distributivgesetz): a(b + c) = ab + ac.\n",
        "  * Gilt zusätzlich Distributivgesetz gilt für Ring (da man hier zwei Operationen benötigt). Gruppen haben immer nur eine Operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Group Theory**\n",
        "\n",
        "* Solutions to polynomial equations, like find 2 roots of a quadratic equation. there is also q cubic formula and a quartic for degree 4 polynomial. Find formula to solve degree 5 polynomial - group theory showed it doesnt exist. Has to do with permutation group S5.\n",
        "\n",
        "* Connection to Physics - Noether's theorem: Conservation law - symmetry. Momentum - translation in space. Energy - translation in time.\n",
        "\n",
        "> [Researchers Use Group Theory to Speed Up Algorithms — Introduction to Groups](https://www.youtube.com/watch?v=KufsL2VgELo&list=WL&index=3&t=1579s)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/groups.png)"
      ],
      "metadata": {
        "id": "s6W7h780RpMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDf3JlzmRpMU"
      },
      "source": [
        "**Trivial Group (Zero Group)**\n",
        "\n",
        "Die [triviale Gruppe](https://de.m.wikipedia.org/wiki/Triviale_Gruppe) ist in der Gruppentheorie eine Gruppe, deren Trägermenge genau ein Element enthält. Die triviale Gruppe ist bis auf Isomorphie eindeutig bestimmt. **Jede Gruppe enthält die triviale Gruppe als Untergruppe**.\n",
        "\n",
        "Die triviale Gruppe $(\\{e\\}, *)$ ist eine Gruppe, die aus der einelementigen Menge $\\{e\\}$ besteht und versehen ist mit der einzig möglichen Gruppenoperation\n",
        "\n",
        "$\n",
        "e * e=e\n",
        "$\n",
        "\n",
        "Das Element $e$ ist damit das **neutrale Element** der Gruppe.\n",
        "\n",
        "Alle trivialen Gruppen sind zueinander isomorph. Beispiele für triviale Gruppen sind:\n",
        "\n",
        "* die zyklische Gruppe $C_{1}$ vom Grad 1\n",
        "\n",
        "* die alternierende Gruppe $A_{2}$ vom Grad 2\n",
        "\n",
        "* die symmetrische Gruppe $S_{1}$ einer einelementigen Menge\n",
        "\n",
        "*Eigenschaften trivialer Gruppen*:\n",
        "\n",
        "* Da die Gruppenoperation $\\ast$ kommutativ ist, ist die triviale Gruppe eine abelsche Gruppe.\n",
        "\n",
        "* Die einzige Untergruppe der trivialen Gruppe ist die triviale Gruppe selbst.\n",
        "\n",
        "* Die triviale Gruppe wird von der leeren Menge erzeugt:\n",
        "$\\{e\\}=\\langle \\emptyset \\rangle$ . Hierbei ergibt das leere Produkt nach üblicher Konvention das neutrale Element.\n",
        "\n",
        "* Jede Gruppe enthält die triviale Gruppe und sich selbst als (triviale) Normalteiler. **Die triviale Gruppe wird daher meistens nicht als einfache Gruppe angesehen**, die aus genau 2 Normalteilern besteht).\n",
        "\n",
        "* In der Kategorie der Gruppen Grp fungiert die triviale Gruppe als Nullobjekt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ouOByzRpMU"
      },
      "source": [
        "**Simple Group**\n",
        "\n",
        "* [Simple Groups](https://en.wikipedia.org/wiki/Simple_group) bzw. [einfache Gruppen](https://de.wikipedia.org/wiki/Einfache_Gruppe_(Mathematik)) **are the fundamental building blocks of finite groups** (just like Prime numbers are fundamental building blocks in number theory)\n",
        "\n",
        "* Just as you can factor integers into prime numbers, you can break apart some groups into a direct product of simpler groups.\n",
        "\n",
        "*  **a simple group is a <u>nontrivial</u> group whose only normal subgroups are the trivial group and the group itself.**\n",
        "\n",
        "* **Jede Gruppe hat sich selbst und die nur das neutrale Element enthaltende Menge als Normalteiler.**\n",
        "\n",
        "Damit stellt sich die Frage, welche Gruppen keine weitere Normalteiler besitzen. Bei diesen handelt es sich per definitionem gerade um die einfachen Gruppen.\n",
        "\n",
        "  * Eine Gruppe $G$ heisst einfach, falls sie als Normalteiler nur $G$ und $\\{e\\}$ mit dem neutralen Element $e$ hat.\n",
        "  * Außerdem wird zusătzlich $G \\neq\\{e\\}$ gefordert, wonach man knapper sagen kann:\n",
        "  * **Eine Gruppe heißt einfach, wenn sie genau zwei Normalteiler besitzt.**\n",
        "\n",
        "* A group that is not simple can be broken into two smaller groups, namely a nontrivial [normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup) and the corresponding quotient group. This process can be repeated, and for finite groups one eventually arrives at uniquely determined simple groups, by the [Jordan–Hölder theorem (Composition series)](https://en.wikipedia.org/wiki/Composition_series).\n",
        "\n",
        "* The complete classification of finite simple groups, completed in 2004, is a major milestone in the history of mathematics.\n",
        "\n",
        "**Seit 1982 sind die endlichen einfachen Gruppen vollständig klassifiziert, die Liste besteht aus**\n",
        "\n",
        "* den zyklischen Gruppen von Primzahlordnung,\n",
        "\n",
        "* den alternierenden Gruppen $A_{n}$ mit $n\\geq 5$,\n",
        "\n",
        "* den Gruppen vom Lie-Typ (16 jeweils unendliche Serien)\n",
        "\n",
        "* 26 sporadischen Gruppen (Es handelt sich um die endlichen einfachen Gruppen, die sich nicht in eine der (18) systematischen Familien mit unendlich vielen Mitgliedern (von endlichen einfachen Gruppen) einordnen lassen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0J85k4yRpMV"
      },
      "source": [
        "**Finite Groups**\n",
        "\n",
        "* Eine Gruppe ($G$,*) heißt [endliche Gruppe](https://de.m.wikipedia.org/wiki/Endliche_Gruppe), wenn $G$ eine endliche Menge ist, also eine endliche Anzahl von Elementen hat.\n",
        "\n",
        "* Die Annahme der Endlichkeit ermöglicht ein vereinfachtes Axiomensystem\n",
        "\n",
        "Ein Paar $(G, *)$ mit einer endlichen Menge $G$ und einer inneren zweistelligen Verknüpfung $*: G \\times G \\rightarrow G$ heißt Gruppe, wenn folgende Axiome erfüllt sind:\n",
        "\n",
        "* Assoziativität: Für alle Gruppenelemente $a, b, c$ gilt $(a * b) * c=a *(b * c)$,\n",
        "\n",
        "* [Kürzungsregel](https://de.m.wikipedia.org/wiki/Kürzbarkeit): Aus $a * x=a * x^{\\prime}$ oder $x * a=x^{\\prime} * a$ folgt $x=x^{\\prime}$\n",
        "\n",
        "Aus der Kürzungsregel folgt, dass die Links- und Rechtsmultiplikationen $x \\mapsto a * x$ und $x \\mapsto x * a$\n",
        "injektiv sind, woraus wegen der Endlichkeit auch die Surjektivität folgt. Daher gibt es ein $x$ mit\n",
        "$a * x=a,$ was zur Existenz des neutralen Elementes $e$ führt, und dann ein $x$ mit $a * x=e$, was\n",
        "die Existenz der inversen Elemente zeigt.\n",
        "\n",
        "* The [List of small groups](https://en.wikipedia.org/wiki/List_of_small_groups) contains finite groups of small [order](https://en.wikipedia.org/wiki/Order_(group_theory)) [up to](https://en.wikipedia.org/wiki/Up_to) [group isomorphism](https://en.wikipedia.org/wiki/Group_isomorphism).\n",
        "\n",
        "* Die folgende Liste enthält eine Auswahl [endlicher Gruppen kleiner Ordnung](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen).\n",
        "\n",
        "  * Diese Liste kann benutzt werden, um herauszufinden, zu welchen bekannten endlichen Gruppen eine Gruppe G isomorph ist.\n",
        "\n",
        "  * Als erstes bestimmt man die Ordnung von G und vergleicht sie mit den unten aufgelisteten Gruppen gleicher Ordnung.\n",
        "\n",
        "  * Ist bekannt, ob G abelsch (kommutativ) ist, so kann man einige Gruppen ausschließen. Anschließend vergleicht man die Ordnung einzelner Elemente von G mit den Elementen der aufgelisteten Gruppen, wodurch man G bis auf Isomorphie eindeutig bestimmen kann.\n",
        "\n",
        "In der nachfolgenden Liste werden folgende Bezeichnungen verwendet:\n",
        "\n",
        "- $\\mathbb{Z}_{n}$ ist die zyklische Gruppe der Ordnung $n$ (die auch als $C_{n}$ oder $\\mathbb{Z} / n \\mathbb{Z}$ geschrieben wird).\n",
        "\n",
        "- $D_{n}$ ist die Diedergruppe der Ordnung $2 n$.\n",
        "\n",
        "- $S_{n}$ ist die symmetrische Gruppe vom Grad $n$, mit $n !$ Permutationen von $n$ Elementen.\n",
        "\n",
        "- $A_{n}$ ist die alternierende Gruppe vom Grad $n$, mit $n ! / 2$ Permutationen von $n$ Elementen für $n \\geq 2$.\n",
        "\n",
        "- Dic $_{n}$ ist die dizyklische Gruppe der Ordnung $4 n$.\n",
        "\n",
        "- $V_{4}$ ist die [Klein'sche Vierergruppe](https://de.m.wikipedia.org/wiki/Kleinsche_Vierergruppe) der Ordnung $4 .$\n",
        "\n",
        "- $Q_{4 n}$ ist die Quaternionengruppe der Ordnung $4 n$ fur $n \\geq 2$.\n",
        "\n",
        "[Liste aller Gruppen bis Ordnung 20](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen#Liste_aller_Gruppen_bis_Ordnung_20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.spektrum.de/kolumne/endliche-einfache-gruppen-das-monster-und-der-laengste-beweis/2137146"
      ],
      "metadata": {
        "id": "CCwYyAFMRpMV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fzQBGwwRpMV"
      },
      "source": [
        "**Finite Simple Groups**\n",
        "\n",
        "* [Endliche einfache Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe) gelten in der Gruppentheorie als die Bausteine der [endlichen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_Gruppe).\n",
        "\n",
        "* Die endlichen einfachen Gruppen spielen für die endlichen Gruppen eine ähnliche Rolle wie die Primzahlen für die natürlichen Zahlen: Jede endliche Gruppe lässt sich in ihre einfachen Gruppen „zerteilen“ (für die Art der Eindeutigkeit siehe den Satz von Jordan-Hölder).\n",
        "\n",
        "* Die Rekonstruktion einer endlichen Gruppe aus diesen ihren „Faktoren“ ist aber nicht eindeutig.\n",
        "\n",
        "* Es gibt jedoch keine „noch einfacheren Gruppen“, aus denen sich die endlichen einfachen Gruppen konstruieren lassen.\n",
        "\n",
        "Obwohl die endlichen einfachen Gruppen seit 1982 als vollständig klassifiziert galten, schlossen Mathematiker um Aschbacher die Klassifikation erst im Jahre 2002 mit einem 1200 Seiten langen Beweis ab:\n",
        "\n",
        "* Fast alle dieser Gruppen lassen sich einer von 18 Familien endlicher einfacher Gruppen zuordnen.\n",
        "\n",
        "* Es existieren 26 Ausnahmen. Diese Gruppen werden als **sporadische Gruppen** bezeichnet (Zu den sporadischen Gruppen zählen die Conway-Gruppe, das Babymonster und die [**Monstergruppe**](\n",
        "https://de.m.wikipedia.org/wiki/Monstergruppe) (mit fast 1054 Elementen die größte sporadische Gruppe).\n",
        "\n",
        "* Die [sporadischen Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe) sind 26 spezielle Gruppen in der Gruppentheorie. Es handelt sich um die [endlichen einfachen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe), die sich nicht in eine der [(18) systematischen Familien mit unendlich vielen Mitgliedern](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Familien_endlicher_einfacher_Gruppen) (von endlichen einfachen Gruppen) einordnen lassen.\n",
        "\n",
        "> https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/\n",
        "\n",
        "\n",
        "*Klassifikation der endlichen einfachen Gruppe*\n",
        "\n",
        "Die endlichen einfachen Gruppen [lassen sich einteilen in](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Klassifikation) bzw [Classification of finite simple groups](https://en.m.wikipedia.org/wiki/Classification_of_finite_simple_groups), Every finite simple group is isomorphic to one of the following groups:\n",
        "\n",
        "* a member of one of three infinite classes of such, namely:\n",
        "\n",
        "  * (1) [zyklische Gruppen](https://de.m.wikipedia.org/wiki/Zyklische_Gruppe) von Primzahlordnung,\n",
        "\n",
        "  * (1) [alternierende Gruppen](https://de.m.wikipedia.org/wiki/Alternierende_Gruppe) $A_{n}$ mit $n>4$,\n",
        "\n",
        "  * (16) [Gruppen vom Lie-Typ](https://de.m.wikipedia.org/wiki/Gruppe_vom_Lie-Typ) über einem [endlichen Körper](https://de.m.wikipedia.org/wiki/Endlicher_Körper) (16 jeweils unendliche Familien),\n",
        "\n",
        "* (26) one of 26 groups called the \"sporadic groups\" / [26 sporadische Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe).\n",
        "\n",
        "* (1) the [Tits group](https://en.m.wikipedia.org/wiki/Tits_group) (which is sometimes considered a 27th sporadic group)\n",
        "\n",
        "\n",
        "[2004: Classification of Quasithin group](https://en.m.wikipedia.org/wiki/Quasithin_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Structures (Moduln, Ring, Field)*"
      ],
      "metadata": {
        "id": "1XZPa1w8RpMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/%C3%9Cbersicht_K%C3%B6rper.svg/775px-%C3%9Cbersicht_K%C3%B6rper.svg.png)"
      ],
      "metadata": {
        "id": "nbHmK_i3RpMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Modul](\n",
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)) ist ein n-dimensionaler Ring.**\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativität)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where “scalar multiplication” refers to multiplication by integer scalars.\n",
        "\n",
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$\n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$\n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ],
      "metadata": {
        "id": "BsRlopFvRpMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Ring](https://en.m.wikipedia.org/wiki/Ring_theory) ist eine Menge R mit <u>zwei inneren binären Verknüpfungen</u> „+“ und „∙“, sodass gilt:**\n",
        "\n",
        "1. **Addition: (R, +) ist eine abelsche Gruppe**\n",
        "\n",
        "* Addition is associative and commutative;\n",
        "\n",
        "* There is an additive identity, zero;\n",
        "\n",
        "* Every element has an additive inverse;\n",
        "\n",
        "2. **Multiplikation: (R, ∙) ist eine Halbgruppe**, das bedeutet:\n",
        "\n",
        "* Halbgruppe in der Multiplikation im Ring: **nur nur die Assoziativität, aber keine Inverse, neutrales element oder kommutativität**)\n",
        "\n",
        "> **Das bedeutet: -> Sowohl Ringe als auch Körper verlangen, dass bzgl. der Addition eine kommutative Gruppe vorliegt (abelsch!). Bei der Multiplikation erfolgt der Übergang vom Ring zum Körper durch die Verschärfung der Forderungen**\n",
        "\n",
        "* Unlike a field, a ring is not required to have multiplicative inverses, and the multiplication is not required to be commutative.\n",
        "\n",
        "> **A good example of a ring is the set of all n×n matrices under the operations of matrix addition and matrix multiplication.** [Matrix-Multiplication is non-commutative!](https://en.m.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity)\n",
        "\n",
        "$\\mathbf{A B} \\neq \\mathbf{B A}$\n",
        "For example\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "but\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "> **The integers Z also form a ring under the operations of addition and multiplication.**\n",
        "\n",
        "3. **Die Distributivgesetze a*(b+c)=a*b+a*c und (a+b)*c = a*c+b*c sind für alle a,b,c ε $R$ erfüllt.**\n",
        "\n",
        "4. **Das neutrale Element 0 von (R, +) heißt Nullelement von R.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Ring_(Algebra)\n",
        "\n",
        "**Ein Ring heißt kommutativ**, falls er bezüglich der Multiplikation kommutativ ist (Ein Ring heißt kommutativ, falls er bezüglich der Multiplikation kommutativ ist, ansonsten spricht man von einem nicht-kommutativen Ring.)\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* 2×2 Real matrices.\n",
        "\n",
        "* Das wichtigste Beispiel eines Ringes sind die Integers / ist die Menge (􏰁$\\mathbb Z$,+,∙) der ganzen Zahlen mit der üblichen Addition und Multiplikation. Es handelt sich dabei um einen nullteilerfreien kommutativen Ring mit Einselement, also einen Integritätsring.\n",
        "\n",
        "* the Integers modulo some Natural number greater than one;\n",
        "\n",
        "* Ebenso bildet ($\\mathbb Q$,+,∙) der rationalen Zahlen mit der üblichen Addition und Multiplikation einen Ring. Da in diesem Fall nicht nur ($\\mathbb Q$,+), sondern auch ($\\mathbb Q$ \\ {0},∙) eine abelsche Gruppe bildet, liegt sogar ein Körper vor; es handelt sich dabei um den Quotientenkörper des Integritätsringes (􏰁$\\mathbb Z$,+,∙).\n",
        "\n",
        "* Kein Ring ist die Menge ($\\mathbb N$􏰀,+,∙) der natürlichen Zahlen mit der üblichen Addition und Multiplikation, da die Addition über den natürlichen Zahlen nicht invertierbar ist.\n",
        "\n",
        "https://www.quora.com/What-are-the-differences-between-rings-and-fields"
      ],
      "metadata": {
        "id": "1KyxVCelRpMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Körper (Field)](https://de.m.wikipedia.org/wiki/K%C3%B6rper_(Algebra)) ist eine spezielle Form von Ring**:\n",
        "\n",
        "> Man nennt die Elemente im Körper nicht Vektoren, sondern Skalare. Über dem Skalarkörper betrachtet man einen Vektorraum und dessen Elemente heißen Vektoren.\n",
        "\n",
        "* **A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)**\n",
        "\n",
        "* Ein kommutativer unitärer Ring, der nicht der Nullring ist, heißt ein Körper, wenn in ihm jedes von Null verschiedene Element multiplikativ invertierbar ist.\n",
        "Anders formuliert, ist ein Körper ein kommutativer unitärer Ring K, in dem die Einheitengruppe K* gleich K \\ {0}, also maximal groß, ist.\n",
        "\n",
        "* Ein kommutativer unitärer Ring, der nicht der Nullring ist, ist ein Körper, wenn in ihm jedes von Null verschiedene Element ein Inverses bezüglich der Multiplikation besitzt. Anders formuliert, ist ein Körper ein kommutativer unitärer Ring $K$, in dem die Einheitengruppe $K^{*}$ gleich $K \\backslash\\{0\\}$ ist.\n",
        "\n",
        "\n",
        "Ein Tripel (K,+,•), bestehend aus einer Menge K und zwei binären Verknüpfungen „+“ und „•“ (die üblicherweise Addition und Multiplikation genannt werden), ist genau dann ein Körper, wenn folgende Eigenschaften erfüllt sind:\n",
        "\n",
        "* $(K,+)$ ist eine abelsche Gruppe (mit Neutralelement 0)\n",
        "\n",
        "* $(K \\backslash\\{0\\}, •)$ ist eine abelsche Gruppe (mit Neutralelement 1)\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ und $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ (Distributivgesetz)\n",
        "\n",
        "Additive Eigenschaften:\n",
        "\n",
        "* $a+(b+c)=(a+b)+c$ (Assoziativgesetz)\n",
        "\n",
        "* $a+b=b+a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $0 \\in K$ mit $0+a=a$ (neutrales Element)\n",
        "\n",
        "* Zu jedem $a \\in K$ existiert das additive Inverse $(-a)$ mit $(-a)+a=0$\n",
        "\n",
        "Multiplikative Eigenschaften:\n",
        "\n",
        "* $\\cdot a \\cdot(b \\cdot c)=(a \\cdot b) \\cdot c$ (Assoziativgesetz)\n",
        "\n",
        "* $a \\cdot b=b \\cdot a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $1 \\in K$ mit $1 \\cdot a=a$ (neutrales Element), und es ist $1 \\neq 0$.\n",
        "\n",
        "* Zu jedem $a \\in K \\backslash\\{0\\}$ existiert das multiplikative Inverse $a^{-1}$ mit $a^{-1} \\cdot a=1$\n",
        "\n",
        "Zusammenspiel von additiver und multiplikativer Struktur:\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ (Links-Distributivgesetz)\n",
        "\n",
        "* Das Rechts-Distributivgesetz $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ folgt dann aus den übrigen Eigenschaften:\n",
        "$(a+b) \\cdot c=c \\cdot(a+b)=c \\cdot a+c \\cdot b=a \\cdot c+b \\cdot c$\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* The most familiar form of algebra is the elementary algebra that you learned in high school, namely the algebra of the real numbers. From an abstract point of view, this is the algebra of fields.\n",
        "\n",
        "* Note that the axioms for a field are precisely the axioms for algebra on the real numbers. As a result, the real numbers R form a field under the usual operations of addition and multiplication. However, the real numbers are not the only possible field. Indeed, you are already familiar with a few other examples:\n",
        "\n",
        "* set of rational numbers under addition and multiplication. The rational numbers Q form a field under the usual operations of addition and multiplication. In particular, we can add or multiply two elements of Q to obtain another element of Q, and these operations obey all of the axioms listed above.\n",
        "\n",
        "* The complex numbers C form a field under the commonly defined operations of addition and multiplication. Complex numbers do obey all of the listed axioms for a field, which is why elementary algebra works as usual for complex numbers.\n",
        "\n",
        "* The Integers modulo a Prime number.\n",
        "\n",
        "*An example of a set of numbers that is **not a field** is the set of integers. It is an \"integral domain.\" It is not a field because it lacks multiplicative inverses. Without multiplicative inverses, division may be impossible.*\n",
        "\n",
        "* Both are algebraic objects with a notion of addition and multiplication, **but the multiplication in a field is more specialized**: it is necessarily commutative and every nonzero element has a multiplicative inverse.\n"
      ],
      "metadata": {
        "id": "F0SPuQK-RpMW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZpMzOn3RpMX"
      },
      "source": [
        "**Siehe auch**\n",
        "\n",
        "* [Algebraische Struktur](https://de.m.wikipedia.org/wiki/Algebraische_Struktur)\n",
        "\n",
        "* [Outline_of_algebraic_structures](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures)\n",
        "\n",
        "* [Mathematische Strukturen](https://de.m.wikipedia.org/wiki/Mathematische_Struktur) (neben topologischen Strukturen, geometrischen Strukturen und Zahlbereichen)\n",
        "\n",
        "* [Geometrische Gruppentheorie](https://de.wikipedia.org/wiki/Geometrische_Gruppentheorie) (Gruppenoperationen auf Graphen und metrischen Räumen, letztlich werden die Gruppen selbst zu solchen geometrischen Objekten)\n",
        "\n",
        "* [Verknüpfungen](https://de.m.wikipedia.org/wiki/Verknüpfung_(Mathematik))\n",
        "\n",
        "* [Gruppenoperation](https://de.wikipedia.org/wiki/Gruppenoperation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Icosahedral Symmetry](https://youtu.be/B-WI8JZR140)"
      ],
      "metadata": {
        "id": "A2j4uHa-RpMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Kongruenzklassen, Äquivalenzklassen & Restklassen*"
      ],
      "metadata": {
        "id": "gS-QUpcFmXh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Group_theory"
      ],
      "metadata": {
        "id": "nCPfqdkENfK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kongruenzklassen, Äquivalenzklassen & Restklassen (Modulorechnung)**\n",
        "\n",
        "* **Kongruenz**: [Kongruenz (Zahlentheorie)](https://de.m.wikipedia.org/wiki/Kongruenz_(Zahlentheorie))) Die Kongruenz zwischen zwei ganzen Zahlen ist in Bezug auf einen Teiler definiert. **Der Teiler heißt in diesem Zusammenhang Modul.**\n",
        "  * Gegeben sei ein Modul $m \\in \\mathbb{N}$. Zwei ganze Zahlen $a$ und $b$ heißen kongruent modulo $m$, **wenn die Division von $a$ und $b$ durch $m$ den gleichen Rest $r$ lässt**.\n",
        "  * Kongruenz ist eine Art Erweiterung der Modulorechnung: **Modulorechnung**: 17 mod 3 = 2.\n",
        "  * **Kongruenz: 11 $\\equiv$ 17 mod 3 (weil bei beiden der Rest 2 ist - beide sind in der der gleichen Restklasse)**\n",
        "  * Man kann alternativ zur [**Restklassenermittlung**](https://de.m.wikipedia.org/wiki/Restklasse) auch sagen, dass zwei Zahlen kongruent sind (modulo der natürlichen Zahl n), wenn ihre Differenz durch n teilbar ist. Hier: 17 - 11 = 6, ist teilbar durch 3.\n",
        "\n",
        "* [**Äquivalenz und Äquivalenzklassen** (Congruence Classes)](https://de.m.wikipedia.org/wiki/Äquivalenzrelation). Äquivalenz: Objekte, die sich in einem bestimmten Zusammenhang gleichen, als gleichwertig bzw. äquivalent angesehen. The result of the modulo operation is an equivalence class (Äquivalenzklassen). Any member of the class may be chosen as representative.\n",
        "\n",
        "  * Von besonderem Interesse sind jedoch solche Äquivalenzrelationen $\\equiv$ , deren Quotientenabbildung $\\mathrm{q}_{\\mathrm{z}}: A \\rightarrow A / \\equiv, a \\mapsto[a]_{\\equiv}$ **mit der Struktur auf $A$ verträglich bzw. ein Homomorphismus ist**, weil dann die von $\\mathrm{q}_{=}$ erzeugte Struktur auf der [Quotientenmenge](https://de.wikipedia.org/wiki/Äquivalenzrelation#Quotientenmenge_und_Partition) $A / \\equiv$ von der gleichen Art ist wie die von $A$. **Eine solche Äquivalenzrelation $\\equiv$ nennt man eine Kongruenzrelation auf der strukturierten Menge $A$.** <font color=\"blue\">Die Quotientengruppe G/N ist homomorph zur Gruppe G.</font>\n",
        "\n",
        "* **[Restklassen](https://de.wikipedia.org/wiki/Restklasse)** sind die Äquivalenzklassen in der Kongruenzrelation. Eine Zahl a modulo einer Zahl m die Menge aller Zahlen, die bei Division durch m denselben Rest lassen wie a. See also [Modulo Operation](https://en.m.wikipedia.org/wiki/Modulo_operation) (Restklassenrechnung).\n",
        "\n",
        "* In der Gruppentheorie werden Äquivalenzklassen als [Nebenklassen (Cosets)](https://de.m.wikipedia.org/wiki/Gruppentheorie#Nebenklassen) bezeichnet."
      ],
      "metadata": {
        "id": "0a8DDz9Vbn1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Subgroups, Normal Subgroups & Cosets*"
      ],
      "metadata": {
        "id": "DxxVSdYniUxt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfs10URbe-RG"
      },
      "source": [
        "**Subgroups, Normal Subgroups & Cosets**\n",
        "\n",
        "* **Subgroups (Unterguppen):**\n",
        "\n",
        "  * Gegeben sei $\\mathbb{Z}$ die Gruppe der ganzen Zahlen mit der Addition als Gruppenoperation. Man kann fur diese Gruppe verschiedene (unendlich viele) **[Untergruppen](https://de.m.wikipedia.org/wiki/Untergruppe) (Subgroups)** bilden: 2$\\mathbb{Z}$, 3$\\mathbb{Z}$, 4$\\mathbb{Z}$, 5$\\mathbb{Z}$, 6$\\mathbb{Z}$, 7$\\mathbb{Z}$.\n",
        "\n",
        "  * Beispiel: Die ganzen Zahlen $\\mathbb {Z} $ sind bezüglich der Addition eine Untergruppe der rationalen Zahlen $\\mathbb {Q} $.\n",
        "\n",
        "  * Two standard subgroups / every group has at least 2 subgroups: the identity element {e} and the entire group G. These are technically **normal subgroups (Normalteiler)**.\n",
        "\n",
        "  * If a group has no other normal subgroups then these two, than it's called a **simple group**. A simple group does not have any factor (quotient) groups, but they are the building blocks of other groups.\n",
        "\n",
        "* **Normal Subgroups (Normalteiler)**:\n",
        "\n",
        "  * <font color=\"blue\">**Die Äquivalenzklasse mit dem Rest Null ist der Normalteiler**. Normalteiler sind spezielle Untergruppen, und ihre Bedeutung liegt vor allem darin, **dass sie genau die Kerne von Gruppenhomomorphismen sind** (=Lösungsmenge ist Null, also Null als Rest).</font> Normal Subgroups heissen auch \"invariant or self-conjugate subgroups\".\n",
        "\n",
        "  * Normal subgroups determine what kinds of homomorphisms are possible from a group $G$ to other groups $f : G -> H$.\n",
        "\n",
        "  * Trivial examples: Standard subgroups identity element {e} and the entire group G.\n",
        "\n",
        "  * Die Gruppe $\\mathbb{Z}$ ist **abelsch** (kommutativ) und somit ist jede Untergruppe auch ein **[Normalteiler](https://de.wikipedia.org/wiki/Normalteiler) bzw. [Normal Subgroup](https://en.wikipedia.org/wiki/Normal_subgroup)**.\n",
        "\n",
        "* **Nebenklassen (Cosets)**:\n",
        "\n",
        "  * Nebenklassen werden benutzt, um den [Satz von Lagrange](https://de.m.wikipedia.org/wiki/Satz_von_Lagrange) zu beweisen, um die Begriffe [Normal Subgroup (Normalteiler)](https://de.m.wikipedia.org/wiki/Normalteiler) und [Quotient Group (Faktorgruppe)](https://de.m.wikipedia.org/wiki/Faktorgruppe) zu erklären und um Gruppenoperationen zu studieren.\n",
        "\n",
        "  * In contrast to Subgroups, the Coset (Nebenklasse) are not closed under addition, have no inverse and don't contain the identity element\n",
        "\n",
        "  * Cosets are there to define how many (finite) possible subgroups exist in a group. There are **two types of cosets**: left cosets and right cosets.\n",
        "\n",
        "  * A subgroup $H$ of a group $G$ may be used to decompose the underlying set of $G$ into disjoint, equal-size subsets called [cosets](https://en.m.wikipedia.org/wiki/Coset)\n",
        "\n",
        "* <font color=\"blue\">**Example: Use Normal Subgroup 5$\\mathbb{Z}$ von $\\mathbb{Z}$ (integers mod 5) to divide a Group into Cosets. We get 5 sets of remainders (the congruence classes) which are the [Quotient Group (Faktorgruppe)](https://en.m.wikipedia.org/wiki/Quotient_group), a group with 5 elements: $\\mathbb{Z}$ mod 5 = {$\\overline{0}$, $\\overline{1}$, $\\overline{2}$, $\\overline{3}$, $\\overline{4}$}.**</font>\n",
        "\n",
        "    * 5$\\mathbb{Z}$ + Rest 0 = $\\overline{0} : \\{\\ldots-10,-5,0,5,10 \\ldots\\}$ **(Normal) Subgroup**, which is technically also a Coset 0+ 5 $\\mathbb{Z}$\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 1 = $\\overline{1} : \\{\\ldots-9,-4,1,6,11 \\ldots\\}$ **Coset 1+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 2 = $\\overline{2} : \\{\\ldots-8,-3,2,7,12 \\ldots\\}$ **Coset 2+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 3 = $\\overline{3} : \\{\\ldots-7,-2,3,8,13 \\ldots\\}$ **Coset 3+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "    * 5 $\\mathbb{Z}$ + Rest 4 = $\\overline{4} : \\{\\ldots-6,-1,4,9,14 \\ldots\\}$ **Coset 4+ 5 $\\mathbb{Z}$**\n",
        "\n",
        "* **Quotient Group (Faktorgruppe)**:\n",
        "\n",
        "  * Die Faktorgruppe oder Quotientengruppe wird mit $G / N$ bezeichnet und ist die Menge der Nebenklassen (Cosets). Aus einer Gruppe $G$ und jedem ihrer Normalteiler $N$ lässt sich eine Faktorgruppe $G/N$ bilden.\n",
        "\n",
        "    * Die Quotientengruppe unterteilt eine Menge in Äquivalenzklassen bzw. eine Gruppe in Restklassen / Nebenklassen. Diese Menge der Restklassen (Äquivalenzklassen) heisst **[Quotientenmenge](https://de.wikipedia.org/wiki/Äquivalenzrelation#Quotientenmenge_und_Partition) bzw. Faktormenge**.\n",
        "\n",
        "  * Diese [Quotientengruppen](https://de.wikipedia.org/wiki/Faktorgruppe) sind homomorphe Bilder von G, **und jedes homomorphe Bild von G ist zu einer solchen Quotientengruppe G/N isomorph**. See fundamental theorem on homomorphisms of groups as \"**Every homomorphic image of a group is isomorphic to a quotient group**\". [Source](https://en.m.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms)\n",
        "\n",
        "  * For finite groups you can find a chain of normal subgroups called a \"composition series\" which acts as a kind of 'prime factorization' of the group (1 ◃ N1, ◃ N2 ◃ ... ◃ Nr ◃ G ). Normal subgroups can also be used to study fields (Körper), i.e. in Galois theory (Field extension K / F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Centralizer and Normalizer*"
      ],
      "metadata": {
        "id": "iEPXBs9Yx37x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Centralizer_and_normalizer\n",
        "\n",
        "In group theory, a branch of mathematics, the normalizer of a subset of a group is a certain associated subgroup.\n",
        "\n",
        "Let's suppose we have a group G and a subset S of G. The normalizer of S in G is the set of all elements in G that commute with S in a certain sense. More specifically, the normalizer is the largest subgroup of G in which S is a subset that commutes with each element of the subgroup under the group operation of conjugation.\n",
        "\n",
        "Formally, the normalizer of S in G, denoted by N_G(S), is defined as:\n",
        "\n",
        "$N_G(S)$ = $g$ in $G$ : $gSg^{(-1)} = S$\n",
        "\n",
        "Here, $gSg^{(-1)})$ = S means that for every element s in S, there exists some s' in S such that gsg^(-1) = s'. In words, the normalizer of S in G consists of all elements g of G such that when any element of S is conjugated by g (i.e., you multiply it by g on the left and by the inverse of g on the right), the result is still an element of S.\n",
        "\n",
        "In simple terms, the normalizer of a subset S of a group G is the largest subgroup of G in which S \"behaves nicely\", in the sense that the action of conjugation by elements of the subgroup leaves S unchanged. The concept of the normalizer is particularly important in the study of the structure and symmetries of groups.\n"
      ],
      "metadata": {
        "id": "GvmH89qHx5_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizer: conjugating with elements of the group results still in same group**\n",
        "\n",
        "Sure, let's consider a simple example with a finite group. Let's take the symmetric group S_3 (the group of all permutations of three elements), which has six elements: {e, (12), (13), (23), (123), (132)} where e is the identity permutation, (12) swaps elements 1 and 2, etc.\n",
        "\n",
        "Now let's consider the subset S = {(12), e}.\n",
        "\n",
        "We want to find the normalizer of this set. **By the definition of the normalizer, it consists of all elements g in S_3 such that when any element of S is conjugated by g, the result is still an element of S**. So we need to find all elements g in S_3 such that g(12)g^(-1) is in S and g(e)g^(-1) is in S.\n",
        "\n",
        "Well, conjugating the identity e by any element gives e back, so that part is trivial.\n",
        "\n",
        "So let's look at the conjugation of (12) by each element of S_3:\n",
        "\n",
        "- Conjugating by e gives (12), which is in S.\n",
        "- Conjugating by (12) gives (12), which is in S.\n",
        "- Conjugating by (13) gives (23), which is not in S.\n",
        "- Conjugating by (23) gives (13), which is not in S.\n",
        "- Conjugating by (123) gives (13), which is not in S.\n",
        "- Conjugating by (132) gives (23), which is not in S.\n",
        "\n",
        "So, the normalizer of S in S_3 is {e, (12)}. This is the largest subgroup of S_3 in which S is a subset that commutes with each element of the subgroup under the operation of conjugation."
      ],
      "metadata": {
        "id": "3OEGqwka1zGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Group Generators*"
      ],
      "metadata": {
        "id": "bTmTARGQiRbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Generators of a group*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0942.jpg)\n",
        "\n",
        "> **Each group SU(3) x SU(2) x U(1) leads to a symmetry resulting in a conservation law**"
      ],
      "metadata": {
        "id": "Pp3Z0FI7ipEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Group Operations*"
      ],
      "metadata": {
        "id": "-6UGcA85iMH0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVfPVtLVGzQ"
      },
      "source": [
        "**Zusammenfassung algebraischer Strukturen mit Operationen**\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ is a Group\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ is a Ring\n",
        "\n",
        "$+$ $\\quad$ $-$ $\\quad$ $\\cdot$ $\\quad$ $÷$ $\\quad$ is a Field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvvUdIJw3cu"
      },
      "source": [
        "A group is a set $G$ with an operation $*$ such that\n",
        "\n",
        "1. **Closure** [Abgeschlossenheit](https://de.m.wikipedia.org/wiki/Abgeschlossenheit_(algebraische_Struktur)): If $x$ and $y$ are in $G$ then $x * y$ is in $\\mathrm{G}$\n",
        "  * until here it is a [Magma (Groupoid)](https://de.m.wikipedia.org/wiki/Magma_(Mathematik))\n",
        "  * Beispiele (Magmen, die keine Halbgruppen sind):\n",
        "    * $(\\mathbb{Z},-):$ die ganzen Zahlen mit der Subtraktion\n",
        "    * (R $\\backslash\\{0\\}, /)$ : die reellen Zahlen ungleich 0 mit der Division\n",
        "    * Die natürlichen Zahlen mit der Exponentiation, also mit der Verknüpfung $a * b=a^{b}$\n",
        "    * Die reellen Zahlen mit der Bildung des arithmetischen Mittels als Verknüpfung\n",
        "\n",
        "2. **Associativity** [Assoziativität](https://de.m.wikipedia.org/wiki/Assoziativgesetz): For all $x, y, z$ in $G$ $\\text { we have }(x * y) * z=x *(y * z)$\n",
        "  * until here it is a [Halbgruppe](https://de.m.wikipedia.org/wiki/Halbgruppe)\n",
        "  * Beispiel: Die Menge $\\mathbb  N$ $_0$ = {0, 1, 2 ..} der natürlichen Zahlen bildet mit der gewöhnlichen Addition eine kommutative und kürzbare Halbgruppe ($\\mathbb  N$ $_0$,+), die keine Gruppe ist. Da hier die negativen Zahlen fehlen, also die „Hälfte“ der abelschen Gruppe ($\\mathbb Z,+$) der ganzen Zahlen, lag der Name Halbgruppe für diese mathematische Struktur nahe.\n",
        "\n",
        "3. **Identity Element** [Neutrales Element](https://en.m.wikipedia.org/wiki/Identity_element): There is an element $e$ in $G$ such that $e * x=x * e=x$ for all $x$ in $G$\n",
        "  * until here it is a [Monoid](https://de.m.wikipedia.org/wiki/Monoid)\n",
        "  * Beispiel: die natürlichen Zahlen mit der Addition und der Zahl 0 als neutralem Element.\n",
        "\n",
        "4. **Inverse Elements** [Inverse Elemente](https://de.m.wikipedia.org/wiki/Inverses_Element): For each element $x$ in $G,$ there is an element $x^{-1}$ such that $x * x^{-1}=x^{-1} * x=e$\n",
        "  * until here it is a [Group](https://de.m.wikipedia.org/wiki/Gruppe_(Mathematik))\n",
        "  * Beispiel: die Menge der ganzen Zahlen zusammen mit der Addition\n",
        "  * Ringe, Körper (Field), Moduln und Vektorräume sind Gruppen mit zusätzlichen Strukturen und Eigenschaften\n",
        "\n",
        "5. **Kommutative** [Kommutativität](https://de.m.wikipedia.org/wiki/Kommutativgesetz): Für alle $a, b \\in G$ gilt: $a * b=b * a$\n",
        "  * until here it is an [Abelian Group](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe)\n",
        "  * Beispiel:\n",
        "    * $(\\mathbb {Z} ,+)$ ist die wichtigste abelsche Gruppe. Dabei ist Z die Menge der ganzen Zahlen und + die gewöhnliche Addition.\n",
        "    * $(\\mathbb {Q}^{\\cdot} , \\cdot)$ ist eine abelsche Gruppe. Dabei ist $\\mathbb {Q}^{\\cdot}$ die Menge der rationalen Zahlen ohne die 0 und ⋅ ist die gewöhnliche Multiplikation. Die Null muss hierbei ausgeschlossen werden, da sie kein inverses Element besitzt: „1/0“ ist nicht definiert\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Verschiebungen.\n",
        "    * Die Menge der Drehungen in einer Ebene um einen Punkt bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Drehungen.\n",
        "    * Die Menge der Drehstreckungen in einer Ebene bilden eine abelsche Gruppe.\n",
        "    * Die Menge der endlichen Dezimalzahlen sind bezüglich der Multiplikation keine abelsche Gruppe. Zum Beispiel hat die Zahl 3 kein Inverses bezüglich der Multiplikation. $\\displaystyle {\\frac {1}{3}}$ lässt sich nicht als endlicher Dezimalbruch schreiben. Bezüglich der normalen Addition bilden die endlichen Dezimalbrüche eine abelsche Gruppe.\n",
        "    * Die Menge der Verschiebungen in der euklidischen Ebene bilden eine abelsche Gruppe. Die Verknüpfung ist die Hintereinanderausführung der Verschiebungen.\n",
        "  * [Free Abelian Group](https://de.m.wikipedia.org/wiki/Freie_abelsche_Gruppe): eine abelsche Gruppe, die als $\\mathbb {Z}$-Modul eine Basis hat. Ist G eine freie abelsche Gruppe ist, so wird man eine Basis wählen und alle Elemente als Linearkombinationen von Elementen dieser Basis ausdrücken. Allerdings sollte man betonen, dass es meist keine ausgezeichnete Basis geben wird.\n",
        "\n",
        "6. **Distributive law** [Distributivgesetz](https://de.m.wikipedia.org/wiki/Distributivgesetz): a(b + c) = ab + ac.\n",
        "  * Gilt zusätzlich Distributivgesetz gilt für Ring (da man hier zwei Operationen benötigt). Gruppen haben immer nur eine Operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/groups.png)"
      ],
      "metadata": {
        "id": "y12kc1i6O7GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Group Theory**\n",
        "\n",
        "* Solutions to polynomial equations, like find 2 roots of a quadratic equation. there is also q cubic formula and a quartic for degree 4 polynomial. Find formula to solve degree 5 polynomial - group theory showed it doesnt exist. Has to do with permutation group S5.\n",
        "\n",
        "* Connection to Physics - Noether's theorem: Conservation law - symmetry. Momentum - translation in space. Energy - translation in time.\n",
        "\n",
        "> [Researchers Use Group Theory to Speed Up Algorithms — Introduction to Groups](https://www.youtube.com/watch?v=KufsL2VgELo&list=WL&index=3&t=1579s)"
      ],
      "metadata": {
        "id": "wwGvCcMgGFMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Representation theory & Group representation*"
      ],
      "metadata": {
        "id": "8rq0lQprf66n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Representation_theory"
      ],
      "metadata": {
        "id": "M3KVD40qgDAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Group_representation"
      ],
      "metadata": {
        "id": "YPIs-HK3DMJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Trivial Group*"
      ],
      "metadata": {
        "id": "0pnekZtsh-ON"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCaLVo_nP2aQ"
      },
      "source": [
        "**Trivial Group (Zero Group)**\n",
        "\n",
        "Die [triviale Gruppe](https://de.m.wikipedia.org/wiki/Triviale_Gruppe) ist in der Gruppentheorie eine Gruppe, deren Trägermenge genau ein Element enthält. Die triviale Gruppe ist bis auf Isomorphie eindeutig bestimmt. **Jede Gruppe enthält die triviale Gruppe als Untergruppe**.\n",
        "\n",
        "Die triviale Gruppe $(\\{e\\}, *)$ ist eine Gruppe, die aus der einelementigen Menge $\\{e\\}$ besteht und versehen ist mit der einzig möglichen Gruppenoperation\n",
        "\n",
        "$\n",
        "e * e=e\n",
        "$\n",
        "\n",
        "Das Element $e$ ist damit das **neutrale Element** der Gruppe.\n",
        "\n",
        "Alle trivialen Gruppen sind zueinander isomorph. Beispiele für triviale Gruppen sind:\n",
        "\n",
        "* die zyklische Gruppe $C_{1}$ vom Grad 1\n",
        "\n",
        "* die alternierende Gruppe $A_{2}$ vom Grad 2\n",
        "\n",
        "* die symmetrische Gruppe $S_{1}$ einer einelementigen Menge\n",
        "\n",
        "*Eigenschaften trivialer Gruppen*:\n",
        "\n",
        "* Da die Gruppenoperation $\\ast$ kommutativ ist, ist die triviale Gruppe eine abelsche Gruppe.\n",
        "\n",
        "* Die einzige Untergruppe der trivialen Gruppe ist die triviale Gruppe selbst.\n",
        "\n",
        "* Die triviale Gruppe wird von der leeren Menge erzeugt:\n",
        "$\\{e\\}=\\langle \\emptyset \\rangle$ . Hierbei ergibt das leere Produkt nach üblicher Konvention das neutrale Element.\n",
        "\n",
        "* Jede Gruppe enthält die triviale Gruppe und sich selbst als (triviale) Normalteiler. **Die triviale Gruppe wird daher meistens nicht als einfache Gruppe angesehen**, die aus genau 2 Normalteilern besteht).\n",
        "\n",
        "* In der Kategorie der Gruppen Grp fungiert die triviale Gruppe als Nullobjekt."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Simple Groups*"
      ],
      "metadata": {
        "id": "YW49URUwh8mn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXVFg6eAKyF0"
      },
      "source": [
        "**Simple Group**\n",
        "\n",
        "* [Simple Groups](https://en.wikipedia.org/wiki/Simple_group) bzw. [einfache Gruppen](https://de.wikipedia.org/wiki/Einfache_Gruppe_(Mathematik)) **are the fundamental building blocks of finite groups** (just like Prime numbers are fundamental building blocks in number theory)\n",
        "\n",
        "* Just as you can factor integers into prime numbers, you can break apart some groups into a direct product of simpler groups.\n",
        "\n",
        "*  **a simple group is a <u>nontrivial</u> group whose only normal subgroups are the trivial group and the group itself.**\n",
        "\n",
        "* **Jede Gruppe hat sich selbst und die nur das neutrale Element enthaltende Menge als Normalteiler.**\n",
        "\n",
        "Damit stellt sich die Frage, welche Gruppen keine weitere Normalteiler besitzen. Bei diesen handelt es sich per definitionem gerade um die einfachen Gruppen.\n",
        "\n",
        "  * Eine Gruppe $G$ heisst einfach, falls sie als Normalteiler nur $G$ und $\\{e\\}$ mit dem neutralen Element $e$ hat.\n",
        "  * Außerdem wird zusătzlich $G \\neq\\{e\\}$ gefordert, wonach man knapper sagen kann:\n",
        "  * **Eine Gruppe heißt einfach, wenn sie genau zwei Normalteiler besitzt.**\n",
        "\n",
        "* A group that is not simple can be broken into two smaller groups, namely a nontrivial [normal subgroup](https://en.wikipedia.org/wiki/Normal_subgroup) and the corresponding quotient group. This process can be repeated, and for finite groups one eventually arrives at uniquely determined simple groups, by the [Jordan–Hölder theorem (Composition series)](https://en.wikipedia.org/wiki/Composition_series).\n",
        "\n",
        "* The complete classification of finite simple groups, completed in 2004, is a major milestone in the history of mathematics.\n",
        "\n",
        "**Seit 1982 sind die endlichen einfachen Gruppen vollständig klassifiziert, die Liste besteht aus**\n",
        "\n",
        "* den zyklischen Gruppen von Primzahlordnung,\n",
        "\n",
        "* den alternierenden Gruppen $A_{n}$ mit $n\\geq 5$,\n",
        "\n",
        "* den Gruppen vom Lie-Typ (16 jeweils unendliche Serien)\n",
        "\n",
        "* 26 sporadischen Gruppen (Es handelt sich um die endlichen einfachen Gruppen, die sich nicht in eine der (18) systematischen Familien mit unendlich vielen Mitgliedern (von endlichen einfachen Gruppen) einordnen lassen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Finite Groups*"
      ],
      "metadata": {
        "id": "Qwm0Keqzh64L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSRFOXm2QgM"
      },
      "source": [
        "**Finite Groups**\n",
        "\n",
        "* Eine Gruppe ($G$,*) heißt [endliche Gruppe](https://de.m.wikipedia.org/wiki/Endliche_Gruppe), wenn $G$ eine endliche Menge ist, also eine endliche Anzahl von Elementen hat.\n",
        "\n",
        "* Die Annahme der Endlichkeit ermöglicht ein vereinfachtes Axiomensystem\n",
        "\n",
        "Ein Paar $(G, *)$ mit einer endlichen Menge $G$ und einer inneren zweistelligen Verknüpfung $*: G \\times G \\rightarrow G$ heißt Gruppe, wenn folgende Axiome erfüllt sind:\n",
        "\n",
        "* Assoziativität: Für alle Gruppenelemente $a, b, c$ gilt $(a * b) * c=a *(b * c)$,\n",
        "\n",
        "* [Kürzungsregel](https://de.m.wikipedia.org/wiki/Kürzbarkeit): Aus $a * x=a * x^{\\prime}$ oder $x * a=x^{\\prime} * a$ folgt $x=x^{\\prime}$\n",
        "\n",
        "Aus der Kürzungsregel folgt, dass die Links- und Rechtsmultiplikationen $x \\mapsto a * x$ und $x \\mapsto x * a$\n",
        "injektiv sind, woraus wegen der Endlichkeit auch die Surjektivität folgt. Daher gibt es ein $x$ mit\n",
        "$a * x=a,$ was zur Existenz des neutralen Elementes $e$ führt, und dann ein $x$ mit $a * x=e$, was\n",
        "die Existenz der inversen Elemente zeigt.\n",
        "\n",
        "* The [List of small groups](https://en.wikipedia.org/wiki/List_of_small_groups) contains finite groups of small [order](https://en.wikipedia.org/wiki/Order_(group_theory)) [up to](https://en.wikipedia.org/wiki/Up_to) [group isomorphism](https://en.wikipedia.org/wiki/Group_isomorphism).\n",
        "\n",
        "* Die folgende Liste enthält eine Auswahl [endlicher Gruppen kleiner Ordnung](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen).\n",
        "\n",
        "  * Diese Liste kann benutzt werden, um herauszufinden, zu welchen bekannten endlichen Gruppen eine Gruppe G isomorph ist.\n",
        "\n",
        "  * Als erstes bestimmt man die Ordnung von G und vergleicht sie mit den unten aufgelisteten Gruppen gleicher Ordnung.\n",
        "\n",
        "  * Ist bekannt, ob G abelsch (kommutativ) ist, so kann man einige Gruppen ausschließen. Anschließend vergleicht man die Ordnung einzelner Elemente von G mit den Elementen der aufgelisteten Gruppen, wodurch man G bis auf Isomorphie eindeutig bestimmen kann.\n",
        "\n",
        "In der nachfolgenden Liste werden folgende Bezeichnungen verwendet:\n",
        "\n",
        "- $\\mathbb{Z}_{n}$ ist die zyklische Gruppe der Ordnung $n$ (die auch als $C_{n}$ oder $\\mathbb{Z} / n \\mathbb{Z}$ geschrieben wird).\n",
        "\n",
        "- $D_{n}$ ist die Diedergruppe der Ordnung $2 n$.\n",
        "\n",
        "- $S_{n}$ ist die symmetrische Gruppe vom Grad $n$, mit $n !$ Permutationen von $n$ Elementen.\n",
        "\n",
        "- $A_{n}$ ist die alternierende Gruppe vom Grad $n$, mit $n ! / 2$ Permutationen von $n$ Elementen für $n \\geq 2$.\n",
        "\n",
        "- Dic $_{n}$ ist die dizyklische Gruppe der Ordnung $4 n$.\n",
        "\n",
        "- $V_{4}$ ist die [Klein'sche Vierergruppe](https://de.m.wikipedia.org/wiki/Kleinsche_Vierergruppe) der Ordnung $4 .$\n",
        "\n",
        "- $Q_{4 n}$ ist die Quaternionengruppe der Ordnung $4 n$ fur $n \\geq 2$.\n",
        "\n",
        "[Liste aller Gruppen bis Ordnung 20](https://de.m.wikipedia.org/wiki/Liste_kleiner_Gruppen#Liste_aller_Gruppen_bis_Ordnung_20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Finite Simple Groups*"
      ],
      "metadata": {
        "id": "Fpc3Bxykh5KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.spektrum.de/kolumne/endliche-einfache-gruppen-das-monster-und-der-laengste-beweis/2137146"
      ],
      "metadata": {
        "id": "5Y80lw5Zh3XE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN_XWVc0s5p"
      },
      "source": [
        "**Finite Simple Groups**\n",
        "\n",
        "* [Endliche einfache Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe) gelten in der Gruppentheorie als die Bausteine der [endlichen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_Gruppe).\n",
        "\n",
        "* Die endlichen einfachen Gruppen spielen für die endlichen Gruppen eine ähnliche Rolle wie die Primzahlen für die natürlichen Zahlen: Jede endliche Gruppe lässt sich in ihre einfachen Gruppen „zerteilen“ (für die Art der Eindeutigkeit siehe den Satz von Jordan-Hölder).\n",
        "\n",
        "* Die Rekonstruktion einer endlichen Gruppe aus diesen ihren „Faktoren“ ist aber nicht eindeutig.\n",
        "\n",
        "* Es gibt jedoch keine „noch einfacheren Gruppen“, aus denen sich die endlichen einfachen Gruppen konstruieren lassen.\n",
        "\n",
        "Obwohl die endlichen einfachen Gruppen seit 1982 als vollständig klassifiziert galten, schlossen Mathematiker um Aschbacher die Klassifikation erst im Jahre 2002 mit einem 1200 Seiten langen Beweis ab:\n",
        "\n",
        "* Fast alle dieser Gruppen lassen sich einer von 18 Familien endlicher einfacher Gruppen zuordnen.\n",
        "\n",
        "* Es existieren 26 Ausnahmen. Diese Gruppen werden als **sporadische Gruppen** bezeichnet (Zu den sporadischen Gruppen zählen die Conway-Gruppe, das Babymonster und die [**Monstergruppe**](\n",
        "https://de.m.wikipedia.org/wiki/Monstergruppe) (mit fast 1054 Elementen die größte sporadische Gruppe).\n",
        "\n",
        "* Die [sporadischen Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe) sind 26 spezielle Gruppen in der Gruppentheorie. Es handelt sich um die [endlichen einfachen Gruppen](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe), die sich nicht in eine der [(18) systematischen Familien mit unendlich vielen Mitgliedern](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Familien_endlicher_einfacher_Gruppen) (von endlichen einfachen Gruppen) einordnen lassen.\n",
        "\n",
        "> https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/\n",
        "\n",
        "\n",
        "*Klassifikation der endlichen einfachen Gruppe*\n",
        "\n",
        "Die endlichen einfachen Gruppen [lassen sich einteilen in](https://de.m.wikipedia.org/wiki/Endliche_einfache_Gruppe#Klassifikation) bzw [Classification of finite simple groups](https://en.m.wikipedia.org/wiki/Classification_of_finite_simple_groups), Every finite simple group is isomorphic to one of the following groups:\n",
        "\n",
        "* a member of one of three infinite classes of such, namely:\n",
        "\n",
        "  * (1) [zyklische Gruppen](https://de.m.wikipedia.org/wiki/Zyklische_Gruppe) von Primzahlordnung,\n",
        "\n",
        "  * (1) [alternierende Gruppen](https://de.m.wikipedia.org/wiki/Alternierende_Gruppe) $A_{n}$ mit $n>4$,\n",
        "\n",
        "  * (16) [Gruppen vom Lie-Typ](https://de.m.wikipedia.org/wiki/Gruppe_vom_Lie-Typ) über einem [endlichen Körper](https://de.m.wikipedia.org/wiki/Endlicher_Körper) (16 jeweils unendliche Familien),\n",
        "\n",
        "* (26) one of 26 groups called the \"sporadic groups\" / [26 sporadische Gruppen](https://de.m.wikipedia.org/wiki/Sporadische_Gruppe).\n",
        "\n",
        "* (1) the [Tits group](https://en.m.wikipedia.org/wiki/Tits_group) (which is sometimes considered a 27th sporadic group)\n",
        "\n",
        "\n",
        "[2004: Classification of Quasithin group](https://en.m.wikipedia.org/wiki/Quasithin_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Other Algebraic Structures*"
      ],
      "metadata": {
        "id": "rJzNsbqaiC3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Structures (Moduln, Ring, Field)*"
      ],
      "metadata": {
        "id": "KHUTVWQOmCm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/%C3%9Cbersicht_K%C3%B6rper.svg/775px-%C3%9Cbersicht_K%C3%B6rper.svg.png)"
      ],
      "metadata": {
        "id": "MdAk53GrsPJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Modul](\n",
        "https://de.m.wikipedia.org/wiki/Modul_(Mathematik)) ist ein n-dimensionaler Ring.**\n",
        "\n",
        "* Ein Modul ist eine algebraische Struktur, die eine Verallgemeinerung eines Vektorraums darstellt.\n",
        "\n",
        "* **A module is similar to a vector space, except that the scalars are only required to be elements of a ring. (Gilt NICHT multiplikative Inverse und multiplikative Kommuntativität)**\n",
        "\n",
        "* For example, the set Zn of n-dimensional vectors with integer entries forms a module, where “scalar multiplication” refers to multiplication by integer scalars.\n",
        "\n",
        "Folgende Zahlenbereiche sind additive Gruppen und damit $\\mathbb {Z}$ -Moduln:\n",
        "\n",
        "* die ganzen Zahlen $\\mathbb {Z}$ selbst\n",
        "\n",
        "* die rationalen Zahlen $\\mathbb {Q}$\n",
        "\n",
        "* die reellen Zahlen $\\mathbb {R}$\n",
        "\n",
        "* die algebraischen Zahlen $\\mathbb A$ bzw. $\\mathbb A$ $\\cap$ $\\mathbb R$\n",
        "\n",
        "* die komplexen Zahlen $\\mathbb {C}$\n"
      ],
      "metadata": {
        "id": "xPGuj4OPrOPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Ring](https://en.m.wikipedia.org/wiki/Ring_theory) ist eine Menge R mit <u>zwei inneren binären Verknüpfungen</u> „+“ und „∙“, sodass gilt:**\n",
        "\n",
        "1. **Addition: (R, +) ist eine abelsche Gruppe**\n",
        "\n",
        "* Addition is associative and commutative;\n",
        "\n",
        "* There is an additive identity, zero;\n",
        "\n",
        "* Every element has an additive inverse;\n",
        "\n",
        "2. **Multiplikation: (R, ∙) ist eine Halbgruppe**, das bedeutet:\n",
        "\n",
        "* Halbgruppe in der Multiplikation im Ring: **nur nur die Assoziativität, aber keine Inverse, neutrales element oder kommutativität**)\n",
        "\n",
        "> **Das bedeutet: -> Sowohl Ringe als auch Körper verlangen, dass bzgl. der Addition eine kommutative Gruppe vorliegt (abelsch!). Bei der Multiplikation erfolgt der Übergang vom Ring zum Körper durch die Verschärfung der Forderungen**\n",
        "\n",
        "* Unlike a field, a ring is not required to have multiplicative inverses, and the multiplication is not required to be commutative.\n",
        "\n",
        "> **A good example of a ring is the set of all n×n matrices under the operations of matrix addition and matrix multiplication.** [Matrix-Multiplication is non-commutative!](https://en.m.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity)\n",
        "\n",
        "$\\mathbf{A B} \\neq \\mathbf{B A}$\n",
        "For example\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "but\n",
        "\n",
        "$\n",
        "\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "> **The integers Z also form a ring under the operations of addition and multiplication.**\n",
        "\n",
        "3. **Die Distributivgesetze a*(b+c)=a*b+a*c und (a+b)*c = a*c+b*c sind für alle a,b,c ε $R$ erfüllt.**\n",
        "\n",
        "4. **Das neutrale Element 0 von (R, +) heißt Nullelement von R.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Ring_(Algebra)\n",
        "\n",
        "**Ein Ring heißt kommutativ**, falls er bezüglich der Multiplikation kommutativ ist (Ein Ring heißt kommutativ, falls er bezüglich der Multiplikation kommutativ ist, ansonsten spricht man von einem nicht-kommutativen Ring.)\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* 2×2 Real matrices.\n",
        "\n",
        "* Das wichtigste Beispiel eines Ringes sind die Integers / ist die Menge (􏰁$\\mathbb Z$,+,∙) der ganzen Zahlen mit der üblichen Addition und Multiplikation. Es handelt sich dabei um einen nullteilerfreien kommutativen Ring mit Einselement, also einen Integritätsring.\n",
        "\n",
        "* the Integers modulo some Natural number greater than one;\n",
        "\n",
        "* Ebenso bildet ($\\mathbb Q$,+,∙) der rationalen Zahlen mit der üblichen Addition und Multiplikation einen Ring. Da in diesem Fall nicht nur ($\\mathbb Q$,+), sondern auch ($\\mathbb Q$ \\ {0},∙) eine abelsche Gruppe bildet, liegt sogar ein Körper vor; es handelt sich dabei um den Quotientenkörper des Integritätsringes (􏰁$\\mathbb Z$,+,∙).\n",
        "\n",
        "* Kein Ring ist die Menge ($\\mathbb N$􏰀,+,∙) der natürlichen Zahlen mit der üblichen Addition und Multiplikation, da die Addition über den natürlichen Zahlen nicht invertierbar ist.\n",
        "\n",
        "https://www.quora.com/What-are-the-differences-between-rings-and-fields"
      ],
      "metadata": {
        "id": "usbC2QwNr7Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein [Körper (Field)](https://de.m.wikipedia.org/wiki/K%C3%B6rper_(Algebra)) ist eine spezielle Form von Ring**:\n",
        "\n",
        "> Man nennt die Elemente im Körper nicht Vektoren, sondern Skalare. Über dem Skalarkörper betrachtet man einen Vektorraum und dessen Elemente heißen Vektoren.\n",
        "\n",
        "* **A Field is a Ring whose non-zero elements form a commutative Group under multiplication (In short a field is a commutative ring with unity with all its non zero elements having multiplicative inverse.)**\n",
        "\n",
        "* Ein kommutativer unitärer Ring, der nicht der Nullring ist, heißt ein Körper, wenn in ihm jedes von Null verschiedene Element multiplikativ invertierbar ist.\n",
        "Anders formuliert, ist ein Körper ein kommutativer unitärer Ring K, in dem die Einheitengruppe K* gleich K \\ {0}, also maximal groß, ist.\n",
        "\n",
        "* Ein kommutativer unitärer Ring, der nicht der Nullring ist, ist ein Körper, wenn in ihm jedes von Null verschiedene Element ein Inverses bezüglich der Multiplikation besitzt. Anders formuliert, ist ein Körper ein kommutativer unitärer Ring $K$, in dem die Einheitengruppe $K^{*}$ gleich $K \\backslash\\{0\\}$ ist.\n",
        "\n",
        "\n",
        "Ein Tripel (K,+,•), bestehend aus einer Menge K und zwei binären Verknüpfungen „+“ und „•“ (die üblicherweise Addition und Multiplikation genannt werden), ist genau dann ein Körper, wenn folgende Eigenschaften erfüllt sind:\n",
        "\n",
        "* $(K,+)$ ist eine abelsche Gruppe (mit Neutralelement 0)\n",
        "\n",
        "* $(K \\backslash\\{0\\}, •)$ ist eine abelsche Gruppe (mit Neutralelement 1)\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ und $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ (Distributivgesetz)\n",
        "\n",
        "Additive Eigenschaften:\n",
        "\n",
        "* $a+(b+c)=(a+b)+c$ (Assoziativgesetz)\n",
        "\n",
        "* $a+b=b+a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $0 \\in K$ mit $0+a=a$ (neutrales Element)\n",
        "\n",
        "* Zu jedem $a \\in K$ existiert das additive Inverse $(-a)$ mit $(-a)+a=0$\n",
        "\n",
        "Multiplikative Eigenschaften:\n",
        "\n",
        "* $\\cdot a \\cdot(b \\cdot c)=(a \\cdot b) \\cdot c$ (Assoziativgesetz)\n",
        "\n",
        "* $a \\cdot b=b \\cdot a$ (Kommutativgesetz)\n",
        "\n",
        "* Es gibt ein Element $1 \\in K$ mit $1 \\cdot a=a$ (neutrales Element), und es ist $1 \\neq 0$.\n",
        "\n",
        "* Zu jedem $a \\in K \\backslash\\{0\\}$ existiert das multiplikative Inverse $a^{-1}$ mit $a^{-1} \\cdot a=1$\n",
        "\n",
        "Zusammenspiel von additiver und multiplikativer Struktur:\n",
        "\n",
        "* $a \\cdot(b+c)=a \\cdot b+a \\cdot c$ (Links-Distributivgesetz)\n",
        "\n",
        "* Das Rechts-Distributivgesetz $(a+b) \\cdot c=a \\cdot c+b \\cdot c$ folgt dann aus den übrigen Eigenschaften:\n",
        "$(a+b) \\cdot c=c \\cdot(a+b)=c \\cdot a+c \\cdot b=a \\cdot c+b \\cdot c$\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* The most familiar form of algebra is the elementary algebra that you learned in high school, namely the algebra of the real numbers. From an abstract point of view, this is the algebra of fields.\n",
        "\n",
        "* Note that the axioms for a field are precisely the axioms for algebra on the real numbers. As a result, the real numbers R form a field under the usual operations of addition and multiplication. However, the real numbers are not the only possible field. Indeed, you are already familiar with a few other examples:\n",
        "\n",
        "* set of rational numbers under addition and multiplication. The rational numbers Q form a field under the usual operations of addition and multiplication. In particular, we can add or multiply two elements of Q to obtain another element of Q, and these operations obey all of the axioms listed above.\n",
        "\n",
        "* The complex numbers C form a field under the commonly defined operations of addition and multiplication. Complex numbers do obey all of the listed axioms for a field, which is why elementary algebra works as usual for complex numbers.\n",
        "\n",
        "* The Integers modulo a Prime number.\n",
        "\n",
        "*An example of a set of numbers that is **not a field** is the set of integers. It is an \"integral domain.\" It is not a field because it lacks multiplicative inverses. Without multiplicative inverses, division may be impossible.*\n",
        "\n",
        "* Both are algebraic objects with a notion of addition and multiplication, **but the multiplication in a field is more specialized**: it is necessarily commutative and every nonzero element has a multiplicative inverse.\n"
      ],
      "metadata": {
        "id": "j07gGLGUtEU6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxw9DSyjiah"
      },
      "source": [
        "**Siehe auch**\n",
        "\n",
        "* [Algebraische Struktur](https://de.m.wikipedia.org/wiki/Algebraische_Struktur)\n",
        "\n",
        "* [Outline_of_algebraic_structures](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures)\n",
        "\n",
        "* [Mathematische Strukturen](https://de.m.wikipedia.org/wiki/Mathematische_Struktur) (neben topologischen Strukturen, geometrischen Strukturen und Zahlbereichen)\n",
        "\n",
        "* [Geometrische Gruppentheorie](https://de.wikipedia.org/wiki/Geometrische_Gruppentheorie) (Gruppenoperationen auf Graphen und metrischen Räumen, letztlich werden die Gruppen selbst zu solchen geometrischen Objekten)\n",
        "\n",
        "* [Verknüpfungen](https://de.m.wikipedia.org/wiki/Verknüpfung_(Mathematik))\n",
        "\n",
        "* [Gruppenoperation](https://de.wikipedia.org/wiki/Gruppenoperation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Icosahedral Symmetry](https://youtu.be/B-WI8JZR140)"
      ],
      "metadata": {
        "id": "ibeXZKTToqNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Homomorphism*"
      ],
      "metadata": {
        "id": "iqHduf9WDZMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Homomorphiesatz (Isomorphie on quotient group)\n",
        "\n",
        "Video: [First Isomorphism Theorem for Groups](https://www.youtube.com/watch?v=JiS43Twomsk&list=WL&index=10)"
      ],
      "metadata": {
        "id": "M8aSZ3c541kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Persistent Homology](https://youtu.be/ktKCzMmDXDk)\n",
        "\n",
        "Video: [Chapter 6: Homomorphism and (first) isomorphism theorem | Essence of Group Theory](https://youtu.be/2kmIHyD8zTk)\n",
        "\n",
        "Kern is a normal subgroup (normalteiler) von H, weil Rest is 0. alles andere sind cosets (mit jeweils rest 1,2,3, etc nachdem was der normalteiler ist)"
      ],
      "metadata": {
        "id": "Z8eBrD-9qS_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Monad_(linear_algebra)"
      ],
      "metadata": {
        "id": "8QS1Q_s8JBFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cantorsparadise.com/an-intro-to-topology-9e0478313b63"
      ],
      "metadata": {
        "id": "6hBdWGAqr4lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence (simplicial homology). Ist das die normal subgroup (normal subgroup)??\n",
        "\n",
        "Der Kern von f ist stets ein Normalteiler von G und das Bild von f ist eine Untergruppe von H. Nach dem Homomorphiesatz ist die Faktorgruppe G/Kern(f) isomorph zu Bild (f)."
      ],
      "metadata": {
        "id": "R4CVKRmNV-E5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35B6lF4hR7zq"
      },
      "source": [
        "**Homomorphiesatz (Fundamental Theorem on Homomorphism)**\n",
        "\n",
        "* [Homomorphiesatz](https://de.m.wikipedia.org/wiki/Homomorphiesatz)\n",
        "\n",
        "* https://youtu.be/QA9rrDMlaHc (Homomorphiesatz mit Hasen und Jaegern)\n",
        "\n",
        "* https://youtu.be/390eRzVSC2k (Homomorphie mit Modulo und kommutativen Diagramm)\n",
        "\n",
        "**Homomorphiesatz (allgemein)**:\n",
        "\n",
        "* Aus einer Abbildung $f$ zwischen zwei Gruppen $G$ und $H$, die weder injektiv noch surjektiv ist, wollen wir eine bijektive Abbildung herleiten.\n",
        "\n",
        "* Schritt 1: Das macht man, indem man zuerst auf der rechten Seite alle Elemente ausschliesst, die nicht Teil der Zielmenge sind, und sich nur auf die 'getroffenen' Elemente fokussiert (The image of $f$ is hierbei a subgroup of $H$.)\n",
        "\n",
        "* Schritt 2: Jetzt hat man nur noch das Problem auf der linken Seite, dass mehrere Startelemente in $G$ auf ein und dasselbe Zielelement in $H$ verweisen. Man betrachtet die Startelemente dann einfach als identisch. Das macht man in dem man $G$ umwandelt in $G$ / Kern ($f$) (man bildet die Faktorgruppe, und spricht aus: modulo Kern von f. Der Quotientenvektorraum von $G$ nach kern von $f$.\n",
        "  * Das geht, weil $f$ ein Homomorphismus ist\n",
        "  * Das bedeutet, wenn zwei Elemente a und b im Start auf ein Element im Ziel verweisen, dann unterscheiden sie sich um ein Kernelement. Heisst, a minus b ist ein Element, das auf 0 geschickt wird, und damit ein Kernelement.\n",
        "  * Und wenn ich jetzt modulo des kerns rechne, dann tue ich so, als ob es die Differenz nicht gibt. Weil es bedeutet a =b. Der Quotientenraum ist ein kunstlich geschaffener Raum, wo a und b identisch gemacht wurden (kongruent).\n",
        "  * Remember aus Modulorechnung: Zwei Zahlen sind kongruent (modulo des Moduls m), wenn ihre Differenz durch m teilbar ist. Hier ist das Modulo der Kern. Also Rest muss 0 sein.\n",
        "  * Modulo n (Reste berechnen, hierbei 0): die Differenz zweier Elemente ist teilbar durch n. Die beiden Elemente sind dann kongruent (identisch).\n",
        "  * $G$ modulo Kern $f$ ist isomorph (identisch =bijektiv und homomorph)) zu Bild $f$, das in $H$ liegt.\n",
        "\n",
        "* **Der Kern von $f$ ist stets ein Normalteiler von $G$ und das Bild von $f$ ist eine Untergruppe von $H$. Nach dem Homomorphiesatz ist die Faktorgruppe $G / \\operatorname{Kern}(f)$ [isomorph (bijektiv)](https://de.m.wikipedia.org/wiki/Isomorphismus) zu Bild $(f)$.**\n",
        "\n",
        "**Bedingungen:**\n",
        "\n",
        "* Let $G$ and $H$ be two groups.\n",
        "* and let $f$ : $G \\rightarrow H$ be a [group homomorphism](https://de.wikipedia.org/wiki/Gruppenhomomorphismus).\n",
        "* and let $K$ be a normal subgroup (Normalteiler) in $G$ and $\\varphi$ the natural surjective homomorphism $G \\rightarrow G / K$ (where $G / K$ is a quotient group). Diese Faktorgruppen sind homomorphe Bilder von G und **jedes homomorphe Bild von G ist zu einer solchen Faktorgruppe G/K isomorph**.\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/homomorphy.jpg)\n",
        "\n",
        "**Then:**\n",
        "\n",
        "1. **Dann ist der Kern von $f$ ein Normalteiler von $G$.**\n",
        "  * Normalteiler sind die [Kerne](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) von Gruppenhomomorphismen, weshalb dann klar ist, dass umgekehrt der Kern von $f$ ein Normalteiler von $G$ ist.\n",
        "\n",
        "  * If $K$ is a **subset** of ker $(f)$ then there exists a unique homomorphism $h: G / K \\rightarrow H$ such that $f=h$ $\\varphi$. In other words, the natural projection $\\varphi$ is universal among homomorphisms on $G$ that map $K$ to the identity element.\n",
        "\n",
        "2. **und daher kann die Faktorgruppe $G /$ ker $f$ gebildet werden.**\n",
        "\n",
        "3. **Nach dem [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) ist diese Faktorgruppe $G /$ ker $f$ isomorph zum Bild von $f$, das eine Untergruppe von $H$ ist.**\n",
        "  * The image of $f$ is isomorphic to the quotient group $G /$ ker ($f$). And in particular, if $f$ is surjective then $H$ is isomorphic to $G$ / ker $(f)$. [Source](https://en.m.wikipedia.org/wiki/Isomorphism_theorems#First_Isomorphism_Theorem_4)\n",
        "  * The image of $f$ is hierbei a subgroup of $H$.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Group_homomorphism_ver.2.svg/500px-Group_homomorphism_ver.2.svg.png)\n",
        "\n",
        "*Image of a group homomorphism (h) from G (left) to H (right).*\n",
        "\n",
        "*The smaller oval inside H is the image of h. N is the kernel of h and aN is a coset of N.* [Source](https://en.m.wikipedia.org/wiki/Group_homomorphism)\n",
        "\n",
        "See also: https://mathepedia.de/Kern_und_Bild_Homomorphismus.html\n",
        "\n",
        "*Diagram of the fundamental theorem on homomorphisms where f is a homomorphism, N is a normal subgroup of G and e is the identity element of G.*\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Diagram_of_the_fundamental_theorem_on_homomorphisms.svg/440px-Diagram_of_the_fundamental_theorem_on_homomorphisms.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Galois Group*"
      ],
      "metadata": {
        "id": "4VoU9BRnAoee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/Ct2fyigNgPY"
      ],
      "metadata": {
        "id": "MHp4y1T7Ar8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Lie Group*"
      ],
      "metadata": {
        "id": "Q-zfzmbO4ZOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video: [Lie Groups](https://youtu.be/ZRca3Ggpy_g)"
      ],
      "metadata": {
        "id": "Hf2JcTiT-9aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lie Groups Based Machine Learning\n",
        "- Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems\n",
        "https://lnkd.in/e-r-ChbX\n",
        "- Structure preserving deep learning\n",
        "https://lnkd.in/gPghU6x\n",
        "- Lie Group Cohomology and (Multi)Symplectic Integrators: New Geometric Tools for Lie Group Machine Learning Based on Souriau Geometric Statistical Mechanics\n",
        "https://lnkd.in/dCPaWBU\n",
        "\n",
        "more information at SEE GSI'23:\n",
        "https://gsi2023.org/\n"
      ],
      "metadata": {
        "id": "WQFsTGz04fla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From: [QHack 2022: Marco Cerezo —Barren plateaus and overparametrization in quantum neural networks](https://www.youtube.com/watch?v=rErONNdHbjg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1401.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1400.png)"
      ],
      "metadata": {
        "id": "imfDlpfHFb7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Theory**\n",
        "\n",
        "> “The essential phenomenon of Lie theory is that one may associate in a natural way to a Lie group $\\mathcal{G}$ its Lie algebra $\\mathfrak{g}$. The Lie algebra $\\mathfrak{g}$ is first of all a vector space and secondly is endowed with a bilinear nonassociative product called the Lie bracket [...]. **Amazingly, the group $\\mathcal{G}$ is almost completely determined by $\\mathfrak{g}$ and its Lie bracket**. Thus for many purposes <font color=\"blue\">**one can replace $\\mathcal{G}$ with $\\mathfrak{g}$. Since $\\mathcal{G}$ is a complicated nonlinear object and $\\mathfrak{g}$ is just a vector space, it is usually vastly simpler to work with $\\mathfrak{g}$**</font>. [...] This is one source of the power of Lie theory.\" Stillwell: “the miracle of Lie theory”. (*https://arxiv.org/pdf/1812.01537.pdf*)\n",
        "\n",
        "* Article about [Lie-Gruppe](https://de.m.wikipedia.org/wiki/Lie-Gruppe)\n",
        "\n",
        "* See also [Lie algebra representation](https://en.m.wikipedia.org/wiki/Lie_algebra_representation) and [representation of a Lie group](https://en.m.wikipedia.org/wiki/Representation_of_a_Lie_group) (a linear action of a Lie group on a vector space). Representations play an important role in the study of continuous symmetry.\n",
        "\n",
        "* See this very good paper: [A micro Lie theory for state estimation in robotics](https://arxiv.org/abs/1812.01537)\n",
        "\n",
        "* Lie theory for the roboticist: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "* https://github.com/artivis/manif/blob/devel/paper/Lie_theory_cheat_sheet.pdf"
      ],
      "metadata": {
        "id": "NoSEZYMD4ijj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Group**\n",
        "\n",
        "* A [Lie group](https://en.wikipedia.org/wiki/Lie_group) is a group that is also a **differentiable manifold**. A manifold is a space that locally resembles Euclidean space, whereas groups define the abstract, generic concept of **multiplication and the taking of inverses (division)**. Combining these two ideas, one obtains a continuous group where points can be multiplied together, and their inverse can be taken.\n",
        "\n",
        "* Lie groups provide a natural model for the concept of **continuous symmetry**, a celebrated example of which is the rotational symmetry in three dimensions (given by the special orthogonal group ${\\text{SO}}(3)$).\n",
        "\n",
        "* Lie groups were first found by studying matrix subgroups\n",
        "$G$ contained in ${\\text{GL}}_{n}(\\mathbb {R} )$ or ${\\text{GL}}_{n}(\\mathbb {C} )$, the groups of $n\\times n$ invertible matrices over $\\mathbb {R}$  or $\\mathbb {C}$ (now called the classical groups).\n",
        "\n",
        "* Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations."
      ],
      "metadata": {
        "id": "1a43uSa04kxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of Lie groups**\n",
        "\n",
        "> [Table of some common Lie groups and their associated Lie algebras](https://en.m.wikipedia.org/wiki/Table_of_Lie_groups)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_01.png)\n",
        "\n",
        "* **Der Einheitskreis in der komplexen Zahlenebene**, d. h. die Menge $S^{1}=\\{z \\in \\mathbb{C}:|z|=1\\}$ der komplexen Zahlen vom Betrag 1, ist eine Untergruppe von $\\left(\\mathbb{C}^{*}, \\cdot\\right)$, die sogenannte **Kreisgruppe**: Das Produkt zweier Zahlen vom Betrag 1 hat wieder Betrag 1, ebenso das Inverse. Auch hier hat man eine $_{n}$ mit der Differentialrechnung verträgliche Gruppenstruktur\", d. h. eine Lie-Gruppe.\n",
        "\n",
        "* **Die Menge $\\mathbb{C}^{*}=\\mathbb{C} \\backslash\\{0\\}$ der komplexen Zahlen ungleich 0 bildet mit der gewöhnlichen Multiplikation eine Gruppe $\\left(\\mathbb{C}^{*}, \\cdot\\right)$**. Die Multiplikation ist eine differenzierbare Abbildung $m: \\mathbb{C}^{*} \\times \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ definiert durch $m(x, y)=x y_{i}$ auch die durch $i(z)=z^{-1}=\\frac{1}{z}$ definierte Inversion $i: \\mathbb{C}^{*} \\rightarrow \\mathbb{C}^{*}$ ist differenzierbar. Die Gruppenstruktur der komplexen Ebene (bzgl. Multiplikation) ist also mit der Differentialrechnung verträglich.\n",
        "\n",
        "* [Beispiele fur Lie-Gruppen](https://de.wikipedia.org/wiki/Lie-Gruppe#Beispiele) sind:  allgemeine lineare Gruppe,  Orthogonale Gruppe,  Unitäre Gruppe & Spezielle Unitäre Gruppe,  Affine Gruppe, [Poincaré-Gruppe](https://en.m.wikipedia.org/wiki/Poincaré_group), Galilei-Gruppe\n",
        "\n",
        "* **Simple Lie Groups**: a simple Lie group is a connected non-abelian Lie group G which does not have nontrivial connected normal subgroups. The list of simple Lie groups can be used to read off the list of simple Lie algebras and [Riemannian symmetric spaces](https://en.wikipedia.org/wiki/Symmetric_space). See also [Classification of semisimple Lie algebras](https://en.wikipedia.org/wiki/Dynkin_diagram#Classification_of_semisimple_Lie_algebras)\n"
      ],
      "metadata": {
        "id": "V9P7SfTo4m6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie Algebra**\n",
        "\n",
        "* a [Lie algebra](https://en.m.wikipedia.org/wiki/Lie_algebra) **is a vector space $g$ together with an operation called the Lie bracket**, an alternating bilinear map $\\mathfrak{g} \\times \\mathfrak{g} \\rightarrow \\mathfrak{g},(x, y) \\mapsto[x, y]$, that satisfies the Jacobi identity.\n",
        "\n",
        "* The vector space $\\mathfrak{g}$ together with this operation is a non-associative algebra, meaning that the Lie bracket is not necessarily associative.\n",
        "\n",
        "> <font color=\"blue\">**Any Lie group gives rise to a Lie algebra, which is its tangent space at the identity.**\n",
        "\n",
        "* In physics, Lie groups appear as symmetry groups of physical systems, and their Lie algebras (tangent vectors near the identity) may be thought of as infinitesimal symmetry motions. Thus Lie algebras and their representations are used extensively in physics, notably in quantum mechanics and particle physics.\n",
        "\n",
        "* [Lie Algebra](https://de.m.wikipedia.org/wiki/Lie-Algebra) ist eine algebraische Struktur, die mit einer Lie-Klammer versehen ist, d. h. es existiert eine antisymmetrische Verknüpfung, die die Jacobi-Identität erfüllt.\n",
        "\n",
        "* Lie-Algebren werden hauptsächlich zum Studium geometrischer Objekte wie Lie-Gruppen und differenzierbarer Mannigfaltigkeiten eingesetzt.\n",
        "\n",
        "* **Simple Lie Algebra**: a [simple Lie algebra](https://en.wikipedia.org/wiki/Simple_Lie_algebra) is a Lie algebra that is nonabelian and contains no nonzero proper ideals. The classification of real simple Lie algebras is one of major achievements of Wilhelm Killing and Élie Cartan. A direct sum of simple Lie algebras is called a semisimple Lie algebra. A simple Lie group is a connected Lie group whose Lie algebra is simple.\n",
        "\n",
        "* **Semisimple Lie algebra**: a [Lie algebra is semisimple](https://en.wikipedia.org/wiki/Semisimple_Lie_algebra) if it is a direct sum of simple Lie algebras (non-abelian Lie algebras without any non-zero proper ideals)."
      ],
      "metadata": {
        "id": "xcVYeFvX4o4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie-Algebra der Lie-Gruppe**\n",
        "\n",
        "* Die Vektorfelder auf einer glatten Mannigfaltigkeit $M$ bilden mit der Lie-Klammer eine unendlich-dimensionale Lie-Algebra. Die zu einer Lie-Gruppe $G$ gehörende Lie-Algebra $\\mathfrak{g}$ besteht aus dem Unterraum der [links-invarianten](https://de.m.wikipedia.org/wiki/Translationsinvarianz) Vektorfelder auf $G$.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum $T_{e} G$ am neutralen Element $e$ von $G$. Insbesondere gilt also $\\operatorname{dim} G=\\operatorname{dim} \\mathfrak{g}$. Bezüglich der LieKlammer $[\\cdot, \\cdot]$ ist der Vektorraum $\\mathfrak{g}$ abgeschlossen.\n",
        "\n",
        "* **Somit ist der Tangentialraum einer Lie-Gruppe $G$ am neutralen Element eine Lie-Algebra. Diese Lie-Algebra nennt man die Lie-Algebra der Lie-Gruppe $G$.**\n",
        "\n",
        "* Zu jeder Lie-Gruppe $G$ mit Lie-Algebra $\\mathfrak{g}$ gibt es eine **Exponentialabbildung exp (exponential map)**: $\\mathfrak{g} \\rightarrow G$. Diese Exponentialabbildung kann man definieren durch $\\exp (A)=\\Phi_{1}(e)$, wobei $\\Phi_{t}$ der Fluss des links-invarianten Vektorfelds $A$ und $e \\in G$ das neutrale Element ist. Falls $G$ eine abgeschlossene Untergruppe der $\\mathrm{GL}(n, \\mathbb{R})$ oder $\\mathrm{GL}(n, \\mathbb{C})$ ist, so ist die so definierte Exponentialabbildung identisch mit der Matrixexponentialfunktion.\n",
        "\n",
        "\n",
        "* Jedes Skalarprodukt auf $T_{e} G=\\mathfrak{g}$ definiert eine $G$ -links-invariante Riemannsche Metrik auf $G$. Im Spezialfall, dass diese Metrik zusätzlich auch\n",
        "rechtsinvariant ist, stimmt die Exponentialabbildung der Riemannschen\n",
        "Mannigfaltigkeit $G$ am Punkt $e$ mit der Lie-Gruppen-Exponentialabbildung\n",
        "überein.\n",
        "\n",
        "* Mit der Lie-Gruppe $\\mathrm{SO}(n)$ ist eine Lie-Algebra $\\mathfrak{s o}(n)$ verknüpft, ein Vektorraum mit einem bilinearen alternierenden Produkt (Lie-Klammer), wobei der Vektorraum bezüglich der Lie- Klammer abgeschlossen ist.\n",
        "\n",
        "* Dieser Vektorraum ist isomorph zum Tangentialraum am neutralen Element der $\\mathrm{SO}(n)$ (neutrales Element ist die Einheitsmatrix), sodass insbesondere $\\operatorname{dim}_\\mathfrak{s o}(n)=\\operatorname{dim} \\mathrm{SO}(n)$ gilt.\n",
        "\n",
        "* Die Lie-Algebra besteht aus allen schiefsymmetrischen $n \\times n$\n",
        "-Matrizen und ihre Basis sind die sog. Erzeugenden Generators).\n",
        "\n",
        "* Die Exponentialabbildung verknüpft die Lie-Algebra mit der Lie-Gruppe:\n",
        "\n",
        "> $\\exp : \\mathfrak{s o}(n) \\rightarrow \\mathrm{SO}(n), J \\mapsto \\sum_{k=0}^{\\infty} \\frac{1}{k !} J^{k}$\n",
        "\n",
        "Exponentiation in quantum: $\\mathbb{G} = e^\\mathfrak{g}$"
      ],
      "metadata": {
        "id": "YiJEo1Y54q9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why using Lie Algebra?**\n",
        "\n",
        "* For example $\\mathcal{G}$ is all the operations on the surface of a ball (nonlinear): Lie Groups are Continuous Transformation Groups, like a 3D rotation vector on a curved surface. Very complicated to work with. (Lie Algebra SO(3)): $\n",
        "\\boldsymbol{W}^{\\wedge}=[\\boldsymbol{\\omega}]_{\\times}=\\left[\\begin{array}{ccc}\n",
        "0 & -\\omega_{z} & \\omega_{y} \\\\\n",
        "\\omega_{z} & 0 & -\\omega_{x} \\\\\n",
        "-\\omega_{y} & \\omega_{x} & 0\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        "* Meanwhile Lie Algebra $\\mathfrak{g}$ is at the origin of the tangent plane which is a linear vector space: Cartesian R3: $\n",
        "\\omega=\\left(\\omega_{x}, \\omega_{y}, \\omega_{z}\\right)\n",
        "$\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algebra\"** $\\rightarrow$ Exponential map translates between both\n",
        "\n",
        "**Lie Groups** were know as \"Continuous Transformation Groups\". **= a group that is also a smooth (differential) manifold**\n",
        "\n",
        "  * **a smooth manifold whose elements satisfy the group axioms**\n",
        "\n",
        "  * (so no singularities or breaks where differentiation or integration wouldn't work anymore)\n",
        "\n",
        "  * each point on a manifold represents one element of the Lie Group (i.e. 3d rotation matrix on the manifold)\n",
        "\n",
        "  * corresponding to it in the cartesian tangent space you can find a 3d rotation vector\n",
        "\n",
        "**Lie Algebra** is the in origin point on manifold (https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s)\n",
        "\n",
        "**Exponential Map** from (cartesian) tangent space to manifold (i.e 3 D surface):\n",
        "\n",
        "  * from tangent space a to manifold ('**exponential of a**'), like the exponential of a rotation vector (on tangent space) is the 3d rotation matrix (on manifold)\n",
        "\n",
        "  * and back: point on manifold x and its '**logarithm of x**' back on the tangent space\n",
        "\n",
        "  * it's an exact operation, and no approximation\n",
        "\n",
        "  * explanation and proof here: https://www.youtube.com/watch?v=nHOcoIyJj2o&t=3147s\n",
        "\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_07.png)"
      ],
      "metadata": {
        "id": "9tPBGJIU4s4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lie algebra is the origin point (= identity) on the tangent space!**\n",
        "\n",
        "> **Tangent space at the origin is called the \"Lie Algreba\"** **each time you take a derivative on the manifold, you are out of the manifold**, but you can stay on the tangent space for doing this operation (because there they are well defined)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_08.png)\n",
        "\n",
        "*https://arxiv.org/pdf/1812.01537.pdf*"
      ],
      "metadata": {
        "id": "YmuDaXN94zEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **antipodal point** to the origin: from origin point go half a turn around a 3d ball, you and up on the antipodal point, and there are many ways to go (any vector with length pie π)\n",
        "\n",
        "* The tangent space will cover the manifold multiple times !!\n",
        "\n",
        "  * \"**First cover of the manifold by the tangent space**\": all points on the tangent space will end up in the antipodal point when applying any vector with length pie π)\n",
        "\n",
        "  * there you can see relationship between Lie group and tangent space on manifold\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_02.png)"
      ],
      "metadata": {
        "id": "wiDgFHe2449f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the following image: $R^{m}$ and $T_{E} M$ are the same vector space but with different representations\n",
        "\n",
        "> **Lie Algebra $T_{E} M \\sim R_{m} \\text { (Cartesian Tangent Space) and } w \\sim w^{\\wedge}$**\n",
        "\n",
        "* This means that $T_{E} M$ is isomorph to $R_{m}$\n",
        "\n",
        "* Since you can always go to an $R_{m}$ space, any Lie Group will have an cartesian $R_{m}$ tangent space, which is a vector\n",
        "\n",
        "* you can alweays go from one to the other give the isomorphism\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_03.png)"
      ],
      "metadata": {
        "id": "cwmJvdVO47i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SO(3): Lie group of rotation matrices in 3D**\n",
        "\n",
        "* you can write [w]<sub>x</sub> as a linear combination of 3 base matrices $E$<sub>x, y, z</sub>, which facilitates the calculation / it's easier than working directly with [w]<sub>x</sub>\n",
        "\n",
        "* and since $T_{E} M \\sim R_{m}$ sowie $w \\sim w^{\\wedge}$ (Isomorphism), it's also an allowed (exact) operation\n",
        "\n",
        "* Let's write a tangent vector as a regular cartesian vector with 3 coordinates\n",
        "\n",
        "> $[\\omega]_{\\times}=\\omega_{x} \\mathbf{E}_{x}+\\omega_{y} \\mathbf{E}_{y}+\\omega_{z} \\mathbf{E}_{z}$\n",
        "\n",
        "* The matrix product $[\\omega]_{\\times}$ is equivalent to cross product of vectors $\\omega =\\omega_{x} \\mathbf +\\omega_{y} \\mathbf +\\omega_{z}$, isomorph, two ways of representing the same elemtn of the tangent space\n",
        "\n",
        "* looking at the previous slide: cartesian R<sup>m</sup> is easier to work with than Lie Algebra T<sub>E</sub>M\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_04.png)\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/liegroup_05.png)\n",
        "\n",
        "**An elementary example is the space of three dimensional vectors $\\mathfrak{g}=\\mathbb{R}^{3}$** with the bracket operation defined by the cross product $[x, y]=x \\times y$. This is skew-symmetric since $x \\times y=-y \\times x$, and instead of associativity it satisfies the Jacobi identity:\n",
        "\n",
        "> $\n",
        "x \\times(y \\times z)=(x \\times y) \\times z+y \\times(x \\times z)\n",
        "$\n",
        "\n",
        "* **This is the Lie algebra of the Lie group of rotations of space**, and each vector $v \\in \\mathbb{R}^{3}$ may be pictured as an infinitesimal rotation around the axis $v$, with velocity equal to the magnitude of $v$.\n",
        "\n",
        "* The Lie bracket is a measure of the noncommutativity between two rotations: since a rotation commutes with itself, we have the alternating property $[x, x]=x \\times x=0$."
      ],
      "metadata": {
        "id": "ds8k2eji4-9d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDNzWru2eKpK"
      },
      "source": [
        "##### <font color=\"blue\">*Topology*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topology is a branch of mathematics that studies the properties of geometric shapes and spaces preserved under continuous deformations like stretching and bending.\n",
        "It focuses on understanding fundamental characteristics of shapes, irrespective of specific measurements like distances or angles. Key concepts include topological spaces, homeomorphism, fundamental group, compactness, and connectedness.\n"
      ],
      "metadata": {
        "id": "hpASdEGT17Tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Topology*"
      ],
      "metadata": {
        "id": "uOwaDiWZDTxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Homological_algebra"
      ],
      "metadata": {
        "id": "4-rErV2smYu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://quantum-journal.org/views/qv-2023-01-26-70/"
      ],
      "metadata": {
        "id": "pRRqjqbIK3yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicial Homology (TDA)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1416.png)\n"
      ],
      "metadata": {
        "id": "0l5UtGJTKpJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1417.png)\n"
      ],
      "metadata": {
        "id": "U13kdlWrKrds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1418.png)\n"
      ],
      "metadata": {
        "id": "wVu6RaMlKsPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1419.png)\n"
      ],
      "metadata": {
        "id": "ud88megeKtzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1420.png)\n"
      ],
      "metadata": {
        "id": "_hlWriNLKu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1421.png)\n"
      ],
      "metadata": {
        "id": "2b-HFVA1KwWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1422.png)\n"
      ],
      "metadata": {
        "id": "9m7jXEzgKxIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1423.png)\n"
      ],
      "metadata": {
        "id": "_mR6uRUtKx_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Free abelian groups: https://youtu.be/xPeeFp_Hd3A\n",
        "\n",
        "simplicial complexes: https://youtu.be/Uq4dTjHfLpI\n",
        "\n",
        "computing homology groups: https://youtu.be/YNBi4Ix3cY0\n",
        "\n",
        "computing more homology groups: https://youtu.be/l7QWg0UzBRA\n",
        "\n",
        "betti numbers: https://youtu.be/NgrIPPqYKjQ\n",
        "\n",
        "persistent homology: https://youtu.be/ktKCzMmDXDk\n",
        "\n",
        "first isomorphism theorem: https://youtu.be/2kmIHyD8zTk\n",
        "\n",
        "isomorphic graphs: https://youtu.be/EwV4Puk2coU\n",
        "\n",
        "cohomoly and forms: https://youtu.be/2ptFnIj71SM\n",
        "\n",
        "differential forms: https://youtu.be/CYz_s82JnY8\n",
        "\n",
        "covectors and one-forms: https://youtu.be/ziD8ewQjaf4"
      ],
      "metadata": {
        "id": "gNk02DYZ_HXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Homology explained via Graph and Group Theory**\n",
        "\n",
        "* Find the nullspace of the matrix, then we can find all the cycles (min 29)\n",
        "\n",
        "* The number of vectors in the spanning set of the null space of the reduced matrix representation would be the number of generating cycles\n",
        "\n",
        "* A [free abelian group](https://en.m.wikipedia.org/wiki/Free_abelian_group) is an abelian group (commutative) with a basis.\n",
        "\n",
        "Source: [An introduction to homology | Algebraic Topology | NJ Wildberger](https://www.youtube.com/watch?v=ShWdSNJeuOg)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1391.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1392.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1394.png)\n",
        "\n",
        "Now adding a higher dimensional disc:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1395.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1396.png)\n",
        "\n",
        "Continue: https://www.youtube.com/watch?v=2wn10l9qbJI&list=PL6763F57A61FE6FE8&index=36&t=1362s\n",
        "\n",
        "Playlist: https://www.youtube.com/playlist?list=PL6763F57A61FE6FE8"
      ],
      "metadata": {
        "id": "QdlLIADIS-QI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeX5czO1VjRi"
      },
      "source": [
        "**Simplicial Homology (Application of Quotient Groups)**\n",
        "\n",
        "> <font color=\"blue\">**Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence**\n",
        "\n",
        "[Simplicial Homology: On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://m0nads.wordpress.com/tag/persistent-homology/)\n",
        "\n",
        "*Since Bn is a subgroup of Zn, we may form the quotient group Hn = Zn/Bn -> so Modulo (=Restwerte) ist dann die Dimension der Löcher (=invarianten)*\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_07.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_10.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_08.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_09.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_06.png)\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.intmath.com/differential-equations/3-integrable-combinations.php"
      ],
      "metadata": {
        "id": "xWw52dxv_93x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple examples of homology:\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Graph_homology\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology"
      ],
      "metadata": {
        "id": "iP7S89gV1zb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In biology, [homology](https://en.m.wikipedia.org/wiki/Homology_(biology)) is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles **are all derived from the same ancestral tetrapod structure**.\n",
        "\n",
        "https://de.wiktionary.org/wiki/homolog\n",
        "\n",
        "Biologie: **von einer gemeinsamen Urform ableitbar**.\n",
        "\n",
        "Worttrennung: ho·mo·log, keine Steigerung\n",
        "\n",
        "Beispiel: Der Flügelbug des Vogels ist unserem Handgelenk homolog.\n",
        "\n",
        "https://de.wiktionary.org/wiki/Homologie\n",
        "\n",
        "Biologie: Übereinstimmung von biologischen Strukturen (Organe, Makromoleküle et cetera) oder Verhaltensanteilen hinsichtlich ihrer Entwicklungsgeschichte und nicht in Hinsicht auf ihre Funktion"
      ],
      "metadata": {
        "id": "XvGbLiHsv5na"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FhQOJufFMx0"
      },
      "source": [
        "**Homology**\n",
        "\n",
        "* [Homology](https://en.m.wikipedia.org/wiki/Homology_(mathematics)) is a general way of **associating a sequence of algebraic objects, such as abelian groups (or modules), to other mathematical objects such as topological spaces.**\n",
        "\n",
        "* **Homology Groups**: Homology groups were originally defined in algebraic topology. Similar constructions are available in a wide variety of other contexts, such as abstract algebra, groups, Lie algebras, Galois theory, and algebraic geometry. See [Construction of homology groups](https://en.m.wikipedia.org/wiki/Homology_(mathematics)#Construction_of_homology_groups)\n",
        "\n",
        "* In homology we want to Linearize the equivalence relation! Homology and cohomology are linear theories (easier to compute, and methods of linear algebra applicable) (though in the process you loose a bit information\n",
        "\n",
        "\n",
        "* Homology counts components, holds, voids etc.\n",
        "\n",
        "* In topology we define something called homology for simplicial complexes.\n",
        "\n",
        "* **Homology of a simplicial complex is computable via linear algebra**.\n",
        "\n",
        "* Die Homologie ist ein mathematischer Ansatz, die Existenz von Löchern zu formalisieren.\n",
        "\n",
        "* Gewisse „sehr feine“ Löcher sind für die Homologie unsichtbar; hier kann u. U. auf die schwerer zu bestimmenden Homotopiegruppen zurückgegriffen werden.\n",
        "\n",
        "1. define set of vertices V0 (=vector space V0 with basis given by the set of vertices) and set of edges V1\n",
        "\n",
        "2. linearize equivalence relation: instead of looking at „tail of edge is equivalent to head of edge“, we consider the difference between these two\n",
        "\n",
        "  * **e $\\mapsto$ h(e) - t(e)** is the linear combination of two vertices, and hence an element of e zero! e wird abgebildet auf dem Element (h(e) - t(e))\n",
        "  * **h(e) $\\equiv$ t(e) mod (im d)** das heisst: h(e) ist identisch zu t(e) modulo dem Bild von d\n",
        "\n",
        "* The linear map d : V1 -> V0 sends an edge e to (h(e) - t(e))\n",
        "* e is an edge which is a basis element inside V1\n",
        "* So the image of d (difference) is just the supspace of V0 generated by these elements\n",
        "* Generate [equivalence relation](https://en.m.wikipedia.org/wiki/Equivalence_relation) (Homomorphiesatz?): impose reflexivity, symmetry and transivity.\n",
        "* So: to get the image of d you look at the subspace generated by e |—> h(e) - t(e)\n",
        "    * Closure under addition more or less corresponds to transitivity,\n",
        "    * closure under negation corresponds to symmetry, and\n",
        "    * closure under zero corresponds to reflexivity.\n",
        "    * So this precisely linearizes the equivalence relation from before.\n",
        "* Now we have one linear map. But we need two linear maps which composes 0 to get homology or cohomology. There are two ways you can get a composite to get 0 very easily.\n",
        "    * Either start from zero and map to V1 and then go to V0, or\n",
        "    * start at V1 and use d to get to V0 and then go to 0 with the zero map there.\n",
        "* Modular the image of this map which is zero\n",
        "\n",
        "* https://ncatlab.org/nlab/show/homology\n",
        "\n",
        "**Homological Algebra**\n",
        "\n",
        "* [Homological algebra](https://en.m.wikipedia.org/wiki/Homological_algebra) is the study of homological functors and the intricate algebraic structures that they entail; its development was closely intertwined with the emergence of category theory. A central concept is that of chain complexes, which can be studied through both their homology and cohomology.\n",
        "\n",
        "*A diagram used in the snake lemma, a basic result in homological algebra:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Snake_lemma_origin.svg/375px-Snake_lemma_origin.svg.png)\n",
        "\n",
        "\n",
        "* Why using Homological Algebra: **translate a problem of interest into a sequence of „higher“ (=homological algebra) linear algebra problems**\n",
        "\n",
        "* homological algebra is the **study of homological functors** and the intricate algebraic structures that they entail.\n",
        "\n",
        "* One quite useful and ubiquitous concept in mathematics is that of **chain complexes**, which can be studied through both their homology and cohomology.\n",
        "\n",
        "* Homological algebra affords the means to **extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces**, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.\n",
        "\n",
        "**Simplicial Homology**\n",
        "\n",
        "* Die [Simpliziale Homologie](https://de.m.wikipedia.org/wiki/Simpliziale_Homologie) ist in der Algebraischen Topologie, einem Teilgebiet der Mathematik, eine Methode, die einem beliebigen Simplizialkomplex **eine Folge [abelscher Gruppen](https://de.m.wikipedia.org/wiki/Abelsche_Gruppe) zuordnet**.\n",
        "\n",
        "* Anschaulich gesprochen zählt sie die Löcher unterschiedlicher Dimension des zugrunde liegenden Raumes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph homology**\n",
        "\n",
        "In algebraic topology and graph theory, [graph homology](https://en.m.wikipedia.org/wiki/Graph_homology) describes the homology groups of a graph, where the graph is considered as a topological space. It formalizes the idea of the number of \"holes\" in the graph. It is a special case of a simplicial homology, as a graph is a special case of a simplicial complex. Since a finite graph is a 1-complex (i.e., its 'faces' are the vertices - which are 0-dimensional, and the edges - which are 1-dimensional), the only non-trivial homology groups are the 0-th group and the 1-th group.\n",
        "\n",
        "The 1st homology group\n",
        "\n",
        "Similarly, $\\partial _{1}$ maps any cycle in C1 to the zero element of C0. In other words, the set of cycles in C1 **generates the null space (the kernel)** of $\\partial _{1}$.\n",
        "\n",
        "https://www.youtube.com/watch?v=ShWdSNJeuOg&t=240s"
      ],
      "metadata": {
        "id": "nKnPM0x-6BG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Graph Theory**\n",
        "\n",
        "In mathematics, [topological graph theory](https://en.m.wikipedia.org/wiki/Topological_graph_theory) is a branch of graph theory. It studies the embedding of graphs in surfaces, spatial embeddings of graphs, and graphs as topological spaces.[1] It also studies immersions of graphs.\n",
        "\n",
        "To an undirected graph we may associate an abstract simplicial complex C with a single-element set per vertex and a two-element set per edge.[2] The geometric realization |C| of the complex consists of a copy of the unit interval [0,1] per edge, with the endpoints of these intervals glued together at vertices. In this view, embeddings of graphs into a surface or as subdivisions of other graphs are both instances of topological embedding, homeomorphism of graphs is just the specialization of topological homeomorphism, the notion of a connected graph coincides with topological connectedness, and a connected graph is a tree if and only if its fundamental group is trivial.\n",
        "\n",
        "Other simplicial complexes associated with graphs include the **Whitney complex or clique complex**, with a set per clique of the graph, and the matching complex, with a set per matching of the graph (equivalently, the clique complex of the complement of the line graph). The matching complex of a complete bipartite graph is called a chessboard complex, as it can be also described as the complex of sets of nonattacking rooks on a chessboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvWc8p5p4Qlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph (topology)**\n",
        "\n",
        "In topology, a branch of mathematics, a [graph is a topological space](https://en.m.wikipedia.org/wiki/Graph_(topology)) which arises from a usual graph\n",
        "G\n",
        "=\n",
        "(\n",
        "E\n",
        ",\n",
        "V\n",
        ")\n",
        "{\\displaystyle G=(E,V)} by replacing vertices by points and each edge\n",
        "e\n",
        "=\n",
        "x\n",
        "y\n",
        "∈\n",
        "E\n",
        "{\\displaystyle e=xy\\in E} by a copy of the unit interval\n",
        "I\n",
        "=\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "{\\displaystyle I=[0,1]}, where\n",
        "0\n",
        "{\\displaystyle 0} is identified with the point associated to\n",
        "x\n",
        "x and\n",
        "1\n",
        "1 with the point associated to\n",
        "y\n",
        "y. That is, as topological spaces, graphs are exactly the simplicial 1-complexes and also exactly the one-dimensional **CW complexes**."
      ],
      "metadata": {
        "id": "40IB_N2Q6Mk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **path** is a walk with no repeated vertices\n",
        "\n",
        "A closed path is a **cycle**."
      ],
      "metadata": {
        "id": "kSH33jQVs3bM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLoLSdtySVs"
      },
      "source": [
        "**Cycles (ker), boundaries (img) and chains**\n",
        "\n",
        "* Cycles (and its Kern): A cycle is a closed submanifold,\n",
        "\n",
        "* boundary is a cycle which is also the boundary of a submanifold.\n",
        "\n",
        "* Boundary operator on [chains](https://en.m.wikipedia.org/wiki/Chain_(algebraic_topology)) (and its image): The boundary of a chain is the linear combination of boundaries of the simplices in the chain. The boundary of a k-chain is a (k−1)-chain. Note that the boundary of a simplex is not a simplex, but a chain with coefficients 1 or −1 – thus chains are the closure of simplices under the boundary operator.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Chainline.svg/320px-Chainline.svg.png)\n",
        "\n",
        "* *The boundary of a polygonal curve is a linear combination of its nodes; in this case, some linear combination of A1 through A6. Assuming the segments all are oriented left-to-right (in increasing order from Ak to Ak+1), the boundary is A6 − A1.*\n",
        "\n",
        "* If the 1 -chain $c=t_{1}+t_{2}+t_{3}$ is a path from point $v_{1}$ to point $v_{4},$ where $t_{1}=\\left[v_{1}, v_{2}\\right], t_{2}=\\left[v_{2}, v_{3}\\right]$ and $t_{3}=\\left[v_{3}, v_{4}\\right]$ are its\n",
        "constituent 1 -simplices, then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\partial_{1} c &=\\partial_{1}\\left(t_{1}+t_{2}+t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(t_{1}\\right)+\\partial_{1}\\left(t_{2}\\right)+\\partial_{1}\\left(t_{3}\\right) \\\\\n",
        "&=\\partial_{1}\\left(\\left[v_{1}, v_{2}\\right]\\right)+\\partial_{1}\\left(\\left[v_{2}, v_{3}\\right]\\right)+\\partial_{1}\\left(\\left[v_{3}, v_{4}\\right]\\right) \\\\\n",
        "&=\\left(\\left[v_{2}\\right]-\\left[v_{1}\\right]\\right)+\\left(\\left[v_{3}\\right]-\\left[v_{2}\\right]\\right)+\\left(\\left[v_{4}\\right]-\\left[v_{3}\\right]\\right) \\\\\n",
        "&=\\left[v_{4}\\right]-\\left[v_{1}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Closed_polygonal_line.svg/320px-Closed_polygonal_line.svg.png)\n",
        "\n",
        "* *A closed polygonal curve, assuming consistent orientation, has null boundary. (deswegen führt der boundary operator δ1 alle Werte immer in Null, zumindest bei geschlossenen objekten)*\n",
        "\n",
        "* Example 2: The boundary of the triangle is a formal sum of its edges with signs arranged to make the traversal of the boundary counterclockwise.\n",
        "\n",
        "   * Cycle: **A chain is called a cycle when its boundary is zero**.\n",
        "\n",
        "   * Boundary: A chain that is the boundary of another chain is called a boundary.\n",
        "\n",
        "   * **Boundaries are cycles**, so chains form a chain complex, whose homology groups (cycles modulo boundaries) are called simplicial homology groups.\n",
        "\n",
        "* Example 3: A 0-cycle is a linear combination of points such that the sum of all the coefficients is 0. Thus, the 0-homology group measures the number of path connected components of the space.\n",
        "\n",
        "* Example 4: The plane punctured at the origin has nontrivial 1-homology group **since the unit circle is a cycle, but not a boundary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazhAwJAFmL-"
      },
      "source": [
        "**Chain Complex**\n",
        "\n",
        "A chain complex\n",
        "(\n",
        "A\n",
        "∙\n",
        ",\n",
        "d\n",
        "∙\n",
        ")\n",
        "(A_{\\bullet },d_{\\bullet }) is a sequence of abelian groups or modules ..., A0, A1, A2, A3, A4, ... connected by homomorphisms (called boundary operators or differentials)\n",
        "\n",
        "> **[Chain Complex](https://en.m.wikipedia.org/wiki/Chain_complex) is a sequence of homomorphism of abelian groups**\n",
        "\n",
        "A chain complex $V$. is a sequence $\\left\\{V_{n}\\right\\}_{n \\in \\mathbb{Z}}$ of abelian groups or modules (for instance yector spaces) or similar equipped with linear maps $\\left\\{d_{n}: V_{n+1} \\rightarrow V_{n}\\right\\}$ such that $d^{2}=0,$ i.e. the composite of two consecutive such maps is the zero morphism $d_{n} \\circ d_{n+1}=0$\n",
        "\n",
        "* https://ncatlab.org/nlab/show/chain+complex\n",
        "\n",
        "* https://ncatlab.org/nlab/show/zero+morphism\n",
        "\n",
        "* https://m0nads.wordpress.com/tag/persistent-homology/\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Simplicial_homology_-_exactness_of_boundary_maps.svg/384px-Simplicial_homology_-_exactness_of_boundary_maps.svg.png)\n",
        "\n",
        "* The boundary of a boundary of a 2-simplex (left) and the boundary of a 1-chain (right) are taken.\n",
        "\n",
        "* Both are 0, being sums in which both the positive and negative of a 0-simplex occur once. The boundary of a boundary is always 0.\n",
        "\n",
        "* A nontrivial cycle is something that closes up like the boundary of a simplex, in that its boundary sums to 0, but which isn't actually the boundary of a simplex or chain.\n",
        "\n",
        "* Because trivial 1-cycles are equivalent to 0 in H1, the 1-cycle at right-middle is homologous to its sum with the boundary of the 2-simplex at left.\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_04.png)\n",
        "\n",
        "\n",
        "[Vergleich mit paper von Krishna 2017]\n",
        "\n",
        "5 kongruent 11 und 17 etc. modulo 3, weil (3 * 1) + 2 = 5, und (3 * 3) + 2 = 11. Reste müssen identisch sein.\n",
        "* **Zp (Kern)**: 5, 11, 17 etc. (alle Objekte aus einer Filtration zB in Cp, die kongruent zueinander sind, **weil deren Differenz ein ganzzahliges Vielfaches von Bp (modulo) ist)**.\n",
        "* **Hp (Hom)**: 2 (= das Loch)\n",
        "* **Bp (Img)**: 3 (modulo)\n",
        "- drei Basiselemente: 0, 1 und 2 (?)\n",
        "\n",
        "Before:\n",
        "\n",
        "5 kongruent 11 modulo 3, weil 3 * 1 + 2 = 5, und 3 * 3 + 2 + 11, Reste müssen identisch sein, sowie 17 etc.\n",
        "\n",
        "- drei Basiselemente: 0, 1 und 2\n",
        "- 3 ist wie Bp, also Image (element des vektorraums, und ist linear, weil zB multiplizier mit Skala bleibt im vektorraum)\n",
        "- Rest ware Hp, also 2 (das Loch)\n",
        "- 5 und 11 und 17 sind Zp (Kern)\n",
        "- Berate alle Objekte aus der Filtration\n",
        "- Ein element aus Zp was plus ein Element aus Bp\n",
        "\n",
        "![bb](https://raw.githubusercontent.com/deltorobarba/repo/master/simplicial_homology_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDP2ORM8EGQ"
      },
      "source": [
        "**Exact Sequences**\n",
        "\n",
        "An [exact sequence](https://en.m.wikipedia.org/wiki/Exact_sequence) is a sequence of morphisms between objects (for example, groups, rings, modules, and, more generally, objects of an abelian category) such that the image of one morphism equals the kernel of the next.\n",
        "\n",
        "In the context of group theory, a sequence\n",
        "\n",
        "> $G_{0} \\stackrel{f_{1}}{\\longrightarrow} G_{1} \\stackrel{f_{2}}{\\longrightarrow} G_{2} \\stackrel{f_{3}}{\\longrightarrow} \\cdots \\stackrel{f_{n}}{\\longrightarrow} G_{n}$\n",
        "\n",
        "of groups and group homomorphisms is called exact if the image of each homomorphism is equal to the kernel of the next:\n",
        "\n",
        "> $\\operatorname{im}\\left(f_{k}\\right)=\\operatorname{ker}\\left(f_{k+1}\\right)$\n",
        "\n",
        "The sequence of groups and homomorphisms may be either finite or infinite.\n",
        "\n",
        "> **Every exact sequence is a [chain complex](https://en.m.wikipedia.org/wiki/Chain_complex)**\n",
        "\n",
        "**Vergleich mit exact sequence: im⁡( f k ) = ker( f k + 1 ) bzw. Zp = Bp mit Hp = 0**\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Illustration_of_an_Exact_Sequence_of_Groups.svg/640px-Illustration_of_an_Exact_Sequence_of_Groups.svg.png)\n",
        "\n",
        "![ccc](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/KerIm_2015Joz_L2.png/640px-KerIm_2015Joz_L2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti Numbers (Betti Sequence)**\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Topologische_Invariante\n",
        "\n",
        "* The formal definition of homology uses the language of group theory. (The equivalence class of loops surrounding a hole have a group structure.) Persistent homology examines these homological features from a multiscale perspective.\n",
        "\n",
        "* Persistent homology is a powerful tool to compute, study and encode efficiently multiscale topological features of nested families of simplicial complexes and topological spaces.\n",
        "\n",
        "* It does not only provide efficient algorithms to compute the Betti numbers of each complex in the considered families, as required for homology inference in the previous section, but also encodes the evolution of the homology groups of the nested complexes across the scales.\n",
        "Im Bereich der algebraischen Topologie sind die Homologien beziehungsweise die **Homologiegruppen Invarianten eines topologischen Raums**.\n",
        "\n",
        "* **Simplicial homology groups and Betti numbers are topological invariants**: if K,K′ are two simplicial complexes whose geometric realizations are homotopy equivalent, then their homology groups are isomorphic and their Betti numbers are the same.\n",
        "\n",
        "* Persistent Homology, a recent breakthrough idea, extends Homology theory to work across a range of parameterized Simplicial Complexes, like the one arising from a point cloud, instead of just a single, isolated complex.\n",
        "It looks for topological invariants across various scales of a topological manifold.\n",
        "\n",
        "* „But there is a problem. Betti numbers are computationally demanding to calculate, “quickly overwhelming even the most powerful classical computers, even for not-so-large data sets,”\n",
        "\n",
        "* https://www.technologyreview.com/s/610138/a-small-scale-demonstration-shows-how-quantum-computing-could-revolutionize-data-analysis/\n",
        "\n",
        "* In algebraic topology, the Betti numbers are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* **The nth Betti number represents the rank of the nth homology group, denoted Hn, which tells us the maximum amount of cuts that must be made before separating a surface into two pieces or 0-cycles, 1-cycles, etc.[1] These numbers are used today in fields such as simplicial homology, computer science, digital images, etc**.\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Betti_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Homology_(mathematics)\n",
        "\n",
        "* Betti numbers is a compact method to present this Homology Groups by investigating the properties of the topological spaces.\n",
        "\n",
        "* It distinguishes topological spaces according to the connectivity of n-dimensional simplicial complexes. The nth Betti number represents the rank of the nth homology group, denoted as Hn.\n",
        "\n",
        "* Informally, the nth Betti number refers to the number of n-dimensional holes on a topological surface.\n",
        "\n",
        "* Figure 9 shows that the first three Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2- dimensional simplicial complexes: β0 is the number of connected components β1 is the number of holes(one-dimensional) and β2 is the number of two-dimensional \"voids\".\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_437.png)\n",
        "\n",
        "Die Bettizahlen geben an, wie viele k-dimensionale nicht zusammenhängende Flächen der entsprechende topologische Raum hat. Die ersten drei Bettizahlen besagen anschaulich also:\n",
        "\n",
        "* b0 ist die Anzahl der Wegzusammenhangskomponenten* (connected components)\n",
        "\n",
        "* b1 ist die Anzahl der „zweidimensionalen Löcher“.\n",
        "\n",
        "* b2 ist die Anzahl der dreidimensionalen Hohlräume.\n",
        "\n",
        "Der unten abgebildete Torus (gemeint ist Oberfläche) besteht aus einer Zusammenhangskomponente, hat zwei „zweidimensionale Löcher“, zum einen das in der Mitte, zum andern das im Inneren des Torus, und hat einen dreidimensionalen Hohlraum. Die Bettizahlen des Torus sind daher 1, 2, 1, die weiteren Bettizahlen sind 0.\n",
        "\n",
        "Ist der zu betrachtende topologische Raum jedoch keine orientierbare kompakte Mannigfaltigkeit, so versagt diese Anschauung allerdings schon.\n"
      ],
      "metadata": {
        "id": "8v0mv3YqBQGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Betti number & Rank of a group**\n",
        "\n",
        "* In algebraic topology, the [Betti numbers](https://en.m.wikipedia.org/wiki/Betti_number) are used to distinguish topological spaces based on the connectivity of n-dimensional simplicial complexes. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n",
        "\n",
        "* The nth Betti number represents the [rank (of a group)](https://en.m.wikipedia.org/wiki/Rank_of_a_group) of the nth [homology group](https://en.m.wikipedia.org/wiki/Homology_(mathematics)), denoted Hn, **which tells us the maximum number of cuts that can be made before separating a surface into two pieces or 0-cycles, 1-cycles**, etc.\n",
        "\n",
        "* The first few Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2-dimensional simplicial complexes:\n",
        "\n",
        "  * b0 is the number of (connected) components;\n",
        "  * b1 is the number of one-dimensional or \"circular\" holes;\n",
        "  * b2 is the number of two-dimensional \"voids\" or \"cavities\".\n",
        "\n",
        "* for example, a torus has one connected surface component so b0 = 1, two \"circular\" holes (one equatorial and one meridional) so b1 = 2, and a single cavity enclosed within the surface so b2 = 1.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/5/54/Torus_cycles.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvR1-z919rUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oP2QkFIRXLX"
      },
      "source": [
        "**Homology Group & Betti Number**\n",
        "\n",
        "Let $\\sigma=\\left(v_{0}, \\ldots, v_{k}\\right)$ be an oriented $k$ -simplex, viewed as a basis element of $C_{k}$. The boundary operator\n",
        "\n",
        "$\n",
        "\\partial_{k}: C_{k} \\rightarrow C_{k-1}\n",
        "$\n",
        "\n",
        "is the homomorphism defined by:\n",
        "\n",
        "$\n",
        "\\partial_{k}(\\sigma)=\\sum_{i=0}^{k}(-1)^{i}\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "where the oriented simplex\n",
        "\n",
        "$\n",
        "\\left(v_{0}, \\ldots, \\widehat{v_{i}}, \\ldots, v_{k}\\right)\n",
        "$\n",
        "\n",
        "is the $I^{\\text {th }}$ face of $\\sigma,$ obtained by deleting its $i^{\\text {th }}$ vertex.\n",
        "In $C_{k},$ elements of the subgroup\n",
        "\n",
        "$\n",
        "Z_{k}:=\\operatorname{ker} \\partial_{k}\n",
        "$\n",
        "\n",
        "are referred to as cycles, and the subgroup\n",
        "\n",
        "$\n",
        "B_{k}:=\\operatorname{im} \\partial_{k+1}\n",
        "$\n",
        "\n",
        "is said to consist of boundaries.\n",
        "\n",
        "The $k^{\\text {th }}$ homology group $H_{k}$ of $S$ is defined to be the [quotient abelian group](https://en.m.wikipedia.org/wiki/Quotient_group)\n",
        "\n",
        "$\n",
        "H_{k}(S)=Z_{k} / B_{k}\n",
        "$\n",
        "\n",
        "* It follows that the **homology group $H_{k}(S)$ is nonzero exactly when there are $k-$\n",
        "cycles on $S$ which are not boundaries**. In a sense, this means that there are $k$ -\n",
        "dimensional holes in the complex.\n",
        "\n",
        "* For example, consider the complex $S$ obtained by gluing two triangles (with no interior) along one edge, shown in the image. The edges of each triangle can be oriented so as to form a cycle. These two cycles are by construction not boundaries (since every 2 -chain is zero).\n",
        "\n",
        "* One can compute that the homology group $\\mathrm{H}_{1}(\\mathrm{S})$ is isomorphic to $\\mathrm{Z}^{2}$, with a basis given by the two cycles mentioned. This makes precise the informal idea that $S$ has two \"1-\n",
        "dimensional holes\".\n",
        "\n",
        "* Holes can be of different dimensions. The rank of the $k$ th homology group, the\n",
        "number\n",
        "\n",
        "$\n",
        "\\beta_{k}=\\operatorname{rank}\\left(H_{k}(S)\\right)\n",
        "$\n",
        "\n",
        "**is called the $k$ th Betti number of $S$. It gives a measure of the number of $k$ - dimensional holes in $S$.**\n",
        "\n",
        "**A homology class is thus represented by a cycle which is not the boundary of any submanifold: the cycle represents a hole, namely a hypothetical manifold whose boundary would be that cycle, but which is \"not there\".**\n",
        "\n",
        "* An homology class Hn (which represents a hole) is an [equivalence class](https://en.m.wikipedia.org/wiki/Equivalence_class) of\n",
        "    * [cycles](https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles) Ker(δ) modulo boundaries Im(δ) bzw.\n",
        "    * h(e) $\\equiv$ t(e) mod (im d)\n",
        "\n",
        "* Ker(δ) kann beschrieben werden: h(e) $\\equiv$ t(e) bzw. e $\\mapsto$ h(e) - t(e) or: ∂n−1 ◦ ∂n = 0\n",
        "\n",
        "* This is the linear combination of two vertices, and hence an element of e zero\n",
        "\n",
        "* Get the image of d bzw. Im(δ) when you look at the subspace generated by e $\\mapsto$ h(e) - t(e)\n",
        "\n",
        "*  Im(δ): boundaries?\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Simplicial_homology#Boundaries_and_cycles\n",
        "\n",
        "https://ncatlab.org/nlab/show/boundary+of+a+simplex\n",
        "\n",
        "https://ncatlab.org/nlab/show/chain+map\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Spectral_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental group**\n",
        "\n",
        "In the mathematical field of algebraic topology, the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) of a topological space is the group of the equivalence classes under homotopy of the loops contained in the space. It records information about the basic shape, or holes, of the topological space.\n",
        "\n",
        "Siehe auch: [Graph (topology)](https://en.m.wikipedia.org/wiki/Graph_(topology))\n",
        "\n",
        "If $X$ is a graph and ${\\displaystyle T\\subseteq X}$ a maximal tree, then the fundamental group $\\pi _{1}(X)$ equals the free group generated by elements ${\\displaystyle (f_{\\alpha })_{\\alpha \\in A}}$, where the ${\\displaystyle \\{f_{\\alpha }\\}}$ correspond bijectively to the edges of ${\\displaystyle X\\setminus T}$; in fact, $X$ is homotopy equivalent to a wedge sum of circles."
      ],
      "metadata": {
        "id": "7WtsJdDE9oVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Topological Data Analysis*\n",
        "\n",
        "https://www.spektrum.de/news/topologische-datenanalyse-big-data-fuer-quantencomputer/2128650\n",
        "\n",
        "* Der Physiker hat auch eine Vermutung, weshalb Quantencomputer diese Art von Aufgaben besser meistern. Es könnte eine bisher unerwartete Verbindung zwischen der Quantenmechanik und der TDA geben: die Supersymmetrie.\n",
        "\n",
        "* Damit der Quantenalgorithmus exponentiell schneller läuft als klassische Verfahren (was der üblichen Messlatte für einen Quantenvorteil entspricht), **muss die Anzahl hochdimensionaler Löcher in den Datensätzen unvorstellbar groß sein – in der Größenordnung von Billionen**.\n",
        "\n",
        "* »Solche Bedingungen sind in der realen Welt nur schwer zu finden«, sagt Cade. **Es sei unklar, ob derartige Daten überhaupt existieren**, so Ryan Babbush, einer der Hauptautoren der Google-Studie.\n",
        "\n",
        "* Ewin Tang, die jetzt an der University of Washington promoviert, hält die TDA nicht für jene praktische Quantenanwendung, nach der die Informatiker suchen. Sie geht davon aus, dass **Quantencomputer am ehesten nützlich sein werden, um etwas über Quantensysteme zu lernen – und nicht, um klassische Daten zu analysieren**.\n",
        "\n",
        "* Ein neuer kreativer Ansatz könnte jederzeit das schaffen, was Tang und ihren Kollegen bisher nicht gelang: ein effizientes TDA-Verfahren zu entwerfen, das auf gewöhnlichen Rechnern läuft. »Ich würde weder mein Haus noch mein Auto oder meine Katze darauf verwetten, dass das nicht passieren wird«, sagt Dunjko.\n",
        "\n",
        "https://www.nature.com/articles/ncomms10138"
      ],
      "metadata": {
        "id": "EBdBGIsgZodW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAh4XJMHLDp9"
      },
      "source": [
        "**Persistent Barcode, Diagram & Landscape**\n",
        "\n",
        "* What is the ideal size of d? - Consider all distances d between d1 min (to connect two balls) and d2 max size (all are connected). Each hole appears at a particular value of d and disappears at anotjer value of d. We can represent the persistence of this hole as a pair (d1, d2).\n",
        "* Out of this distance we get a bar. Several holes result in a barcode. Short bars represent noise. Long bars are features.\n",
        "* Persistent barcodes are stable with respect to pertubations if data (Edelsbrunner 2007).\n",
        "* Barcode is computable via linear algebra. Runtime is O (n3), it‘s cubic, where n is the number of simplices (Carlsson 2005).\n",
        "* A barcode is a visualization of an algebraic structure.\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_06.jpg)\n",
        "\n",
        "> **Homology of a simplicial complex is computable via linear algebra**\n",
        "\n",
        "\n",
        "![vvv](https://raw.githubusercontent.com/deltorobarba/repo/master/homology_07.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pjdD_O2saB"
      },
      "source": [
        "**Topological Data Analysis + Machine Learning**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/tda_01.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/in-topology-when-are-two-shapes-the-same-20210928/"
      ],
      "metadata": {
        "id": "rGSzNfpu55WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Invariants** (Distanz- und Ähnlichkeitsmaß in topologischen Räumen)\n",
        "* Similarity metrics for topology: A topological invariant is a property or characteristic of a space that remains unchanged under homeomorphisms, which are continuous transformations of the space that can be continuously undone.\n",
        "* Examples of topological invariants: Connectedness, Compactness, Euler characteristic, Betti number and homology groups, Fundamental Group\n",
        "* Beispiele: Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erklärt, dass wir eine Menge  Ω  C  R  offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen lässt.\n",
        "* [Separation Axioms](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms): Topologische Räume können klassifiziert werden nach Kolmogorov\n",
        "  * Frechet Räume: sind Vektorräume von glatten Funktionen (unendlich oft differenzierbar, stetig)\n",
        "Diese Räume lassen sich zwar mit verschiedenen Normen ausstatten, sind aber bezüglich keiner Norm vollständig, also keine Banachräume. Man kann auf ihnen aber eine Topologie definieren, sodass viele Sätze, die in Banachräumen gelten, ihre Gültigkeit behalten.\n",
        "  * Uniforme Räume: erlauben es zwar nicht Abstände einzuführen, aber Begriffe wie gleichmäßige Stetigkeit, Cauchy-Folgen, Vollständigkeit und Vervollständigung zu definieren."
      ],
      "metadata": {
        "id": "-WNPQL7ao82z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQo1DroORpML"
      },
      "source": [
        "**Basis: Homomorphiesatz (Fundamental Theorem on Homomorphism)**\n",
        "\n",
        "> Homomorphiesatz (Isomorphie on quotient group)\n",
        "\n",
        "Video: [First Isomorphism Theorem for Groups](https://www.youtube.com/watch?v=JiS43Twomsk&list=WL&index=10)\n",
        "\n",
        "Falls quotient group Ker/Img = 0, dann handelt es sich um eine Exact Sequence (simplicial homology). Ist das die normal subgroup (normal subgroup)??\n",
        "\n",
        "Der Kern von f ist stets ein Normalteiler von G und das Bild von f ist eine Untergruppe von H. Nach dem Homomorphiesatz ist die Faktorgruppe G/Kern(f) isomorph zu Bild (f).\n",
        "\n",
        "Video: [Persistent Homology](https://youtu.be/ktKCzMmDXDk)\n",
        "\n",
        "Video: [Chapter 6: Homomorphism and (first) isomorphism theorem | Essence of Group Theory](https://youtu.be/2kmIHyD8zTk)\n",
        "\n",
        "Kern is a normal subgroup (normalteiler) von H, weil Rest is 0. alles andere sind cosets (mit jeweils rest 1,2,3, etc nachdem was der normalteiler ist)\n",
        "\n",
        "......................\n",
        "\n",
        "* [Homomorphiesatz](https://de.m.wikipedia.org/wiki/Homomorphiesatz)\n",
        "\n",
        "* https://youtu.be/QA9rrDMlaHc (Homomorphiesatz mit Hasen und Jaegern)\n",
        "\n",
        "* https://youtu.be/390eRzVSC2k (Homomorphie mit Modulo und kommutativen Diagramm)\n",
        "\n",
        "**Homomorphiesatz (allgemein)**:\n",
        "\n",
        "* Aus einer Abbildung $f$ zwischen zwei Gruppen $G$ und $H$, die weder injektiv noch surjektiv ist, wollen wir eine bijektive Abbildung herleiten.\n",
        "\n",
        "* Schritt 1: Das macht man, indem man zuerst auf der rechten Seite alle Elemente ausschliesst, die nicht Teil der Zielmenge sind, und sich nur auf die 'getroffenen' Elemente fokussiert (The image of $f$ is hierbei a subgroup of $H$.)\n",
        "\n",
        "* Schritt 2: Jetzt hat man nur noch das Problem auf der linken Seite, dass mehrere Startelemente in $G$ auf ein und dasselbe Zielelement in $H$ verweisen. Man betrachtet die Startelemente dann einfach als identisch. Das macht man in dem man $G$ umwandelt in $G$ / Kern ($f$) (man bildet die Faktorgruppe, und spricht aus: modulo Kern von f. Der Quotientenvektorraum von $G$ nach kern von $f$.\n",
        "  * Das geht, weil $f$ ein Homomorphismus ist\n",
        "  * Das bedeutet, wenn zwei Elemente a und b im Start auf ein Element im Ziel verweisen, dann unterscheiden sie sich um ein Kernelement. Heisst, a minus b ist ein Element, das auf 0 geschickt wird, und damit ein Kernelement.\n",
        "  * Und wenn ich jetzt modulo des kerns rechne, dann tue ich so, als ob es die Differenz nicht gibt. Weil es bedeutet a =b. Der Quotientenraum ist ein kunstlich geschaffener Raum, wo a und b identisch gemacht wurden (kongruent).\n",
        "  * Remember aus Modulorechnung: Zwei Zahlen sind kongruent (modulo des Moduls m), wenn ihre Differenz durch m teilbar ist. Hier ist das Modulo der Kern. Also Rest muss 0 sein.\n",
        "  * Modulo n (Reste berechnen, hierbei 0): die Differenz zweier Elemente ist teilbar durch n. Die beiden Elemente sind dann kongruent (identisch).\n",
        "  * $G$ modulo Kern $f$ ist isomorph (identisch =bijektiv und homomorph)) zu Bild $f$, das in $H$ liegt.\n",
        "\n",
        "* **Der Kern von $f$ ist stets ein Normalteiler von $G$ und das Bild von $f$ ist eine Untergruppe von $H$. Nach dem Homomorphiesatz ist die Faktorgruppe $G / \\operatorname{Kern}(f)$ [isomorph (bijektiv)](https://de.m.wikipedia.org/wiki/Isomorphismus) zu Bild $(f)$.**\n",
        "\n",
        "**Bedingungen:**\n",
        "\n",
        "* Let $G$ and $H$ be two groups.\n",
        "* and let $f$ : $G \\rightarrow H$ be a [group homomorphism](https://de.wikipedia.org/wiki/Gruppenhomomorphismus).\n",
        "* and let $K$ be a normal subgroup (Normalteiler) in $G$ and $\\varphi$ the natural surjective homomorphism $G \\rightarrow G / K$ (where $G / K$ is a quotient group). Diese Faktorgruppen sind homomorphe Bilder von G und **jedes homomorphe Bild von G ist zu einer solchen Faktorgruppe G/K isomorph**.\n",
        "\n",
        "![cc](https://raw.githubusercontent.com/deltorobarba/repo/master/homomorphy.jpg)\n",
        "\n",
        "**Then:**\n",
        "\n",
        "1. **Dann ist der Kern von $f$ ein Normalteiler von $G$.**\n",
        "  * Normalteiler sind die [Kerne](https://de.m.wikipedia.org/wiki/Kern_(Algebra)) von Gruppenhomomorphismen, weshalb dann klar ist, dass umgekehrt der Kern von $f$ ein Normalteiler von $G$ ist.\n",
        "\n",
        "  * If $K$ is a **subset** of ker $(f)$ then there exists a unique homomorphism $h: G / K \\rightarrow H$ such that $f=h$ $\\varphi$. In other words, the natural projection $\\varphi$ is universal among homomorphisms on $G$ that map $K$ to the identity element.\n",
        "\n",
        "2. **und daher kann die Faktorgruppe $G /$ ker $f$ gebildet werden.**\n",
        "\n",
        "3. **Nach dem [Homomorphiesatz](https://de.wikipedia.org/wiki/Homomorphiesatz) ist diese Faktorgruppe $G /$ ker $f$ isomorph zum Bild von $f$, das eine Untergruppe von $H$ ist.**\n",
        "  * The image of $f$ is isomorphic to the quotient group $G /$ ker ($f$). And in particular, if $f$ is surjective then $H$ is isomorphic to $G$ / ker $(f)$. [Source](https://en.m.wikipedia.org/wiki/Isomorphism_theorems#First_Isomorphism_Theorem_4)\n",
        "  * The image of $f$ is hierbei a subgroup of $H$.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Group_homomorphism_ver.2.svg/500px-Group_homomorphism_ver.2.svg.png)\n",
        "\n",
        "*Image of a group homomorphism (h) from G (left) to H (right).*\n",
        "\n",
        "*The smaller oval inside H is the image of h. N is the kernel of h and aN is a coset of N.* [Source](https://en.m.wikipedia.org/wiki/Group_homomorphism)\n",
        "\n",
        "See also: https://mathepedia.de/Kern_und_Bild_Homomorphismus.html\n",
        "\n",
        "*Diagram of the fundamental theorem on homomorphisms where f is a homomorphism, N is a normal subgroup of G and e is the identity element of G.*\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Diagram_of_the_fundamental_theorem_on_homomorphisms.svg/440px-Diagram_of_the_fundamental_theorem_on_homomorphisms.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Homological_algebra\n",
        "\n",
        "https://www.cantorsparadise.com/an-intro-to-topology-9e0478313b63\n",
        "\n",
        "https://quantum-journal.org/views/qv-2023-01-26-70/"
      ],
      "metadata": {
        "id": "dfkGQgqsRpML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hopf Fibration*"
      ],
      "metadata": {
        "id": "GssEWmSFaZdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map of 4D sphere into 3D sphere\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Hopf-Faserung\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Principal_bundle\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Group_action#Orbit-stabilizer_theorem\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Villarceau-Kreise\n",
        "\n",
        "https://youtu.be/-lZTnEgbVvI\n",
        "\n",
        "https://youtu.be/nsHcKO7HvFY"
      ],
      "metadata": {
        "id": "I5Di4JSmaeeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tropical Geometry*"
      ],
      "metadata": {
        "id": "lh3x3_1K_e1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Sturmfels - Tropical Geometry](https://youtu.be/TNwCzl02uck)"
      ],
      "metadata": {
        "id": "1gdFgCSJfTg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algebraic geometry and differential equations\n",
        "\n",
        "https://youtu.be/8wUM38Ldd1E"
      ],
      "metadata": {
        "id": "hEPh6Gd22h2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebraic Geometry**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_geometry\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebraic_variety\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/System_of_polynomial_equations\n",
        "\n",
        "Particle physics and cosmology are linked to combinatorics and algebraic geometry in an unexpected way. Novel geometric objects hint at new mathematical structures that challenge our current understanding of the laws of nature. The workshop „Positive Geometry\" on January 23 at the Campus of the Technische Universität München will discuss the latest developments. The event is organized by Bernd Sturmfels, Director at the Max-Planck-Institut für Mathematik in den Naturwissenschaften, Jürgen Richter-Gebert, Chair for Geometry and Visualization at TUM, Johannes Henn, Director at MPP in collaboration with #SFB Transregio 109, Discretization in Geometry and Dynamics.\n",
        "More information: https://Inkd.in/eQVnYtyD\n",
        "particlephysics geometry cosmology"
      ],
      "metadata": {
        "id": "mFfIs1nSCsxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tropical Geometry**\n",
        "\n",
        "http://www.mittag-leffler.se/langa-program/tropical-geometry-amoebas-and-polytopes-0\n",
        "\n",
        "Video: [What is tropical mathematics?](https://youtu.be/DV-8kEn8udY)"
      ],
      "metadata": {
        "id": "hjvAO915_hkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algebraic Geometry in Astrophysics*"
      ],
      "metadata": {
        "id": "jZhLaeTPPc7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tropical geometry: https://scitalks.ca/PIRSA/22110028\n",
        "\n",
        "https://arxiv.org/abs/2008.12310\n",
        "\n",
        "https://arxiv.org/abs/2204.06414\n",
        "\n",
        "https://www.pnas.org/doi/10.1073/pnas.0406010101"
      ],
      "metadata": {
        "id": "Gp8_JfPRQ8_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.ias.edu/ideas/2007/algebraic-geometry-interface"
      ],
      "metadata": {
        "id": "f7v-yPRLQW_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quora.com/Which-branch-of-astrophysics-deals-with-geometry\n",
        "\n",
        "Some physicists tend to claim that differential geometry is not important anywhere except for general relativity where it is inevitable. Partially it’s true because you can do a lot of physics, even with underlying geometrical structure, without mentioning the geometry explicitly. On the other modern theoretical physics is using geometrical language in more and more areas. Sometimes it gives you really a new insight which is not seen without geometry, more often it is just an elegant way how to describe the things.\n",
        "\n",
        "Leaving the string theory aside (I am not expert on that), these are some usual applications of differential geometry.\n",
        "\n",
        "Lagrangian mechanics\n",
        "Lagrange’s formalism is essentially a geometrical thing, although it is not usually discussed on the abstract level in introductory courses. The picture is that physical particles live in Euclidean space and each is described by three Cartesian coordinates. This already gives you larger configuration space which is 3N-dimensional, where N is the number of particles. However, if your particles are subject to constraints (e.g., particle can move only along a circle), you configuration space is reduced (e.g. to a circle, instead of Euclidean plane) and it is actually a submanifold of 3N-dimensional space. What you do in Lagrange’s formalism is that you write the equations of motion solely in terms of the coordinates on this submanifold.\n",
        "\n",
        "On this submanifold some interesting structures arise. The velocity of particle is tangent to the submanifold so it is useful to introduce the tangent bundle (i.e. set of all tangent spaces at each points of the submanifold) and Lagrangian mechanics can be geometrically formulated on this bundle. It even makes sense to introduce a metric: it is simply a metric induced by embedding of submanifold into Euclidean space. Physically, the norm of any vector of the tangent bundle is related to the kinetic energy.\n",
        "\n",
        "2. Hamilton’s mechanics\n",
        "\n",
        "This is even more interesting. It turns out to be useful to interpret the momenta (canonically conjugate to coordinates) as the co-vectors, i.e. as the elements of cotangent bundle. The structure of Hamilton’s equations can be reformulated in terms of the so-called symplectic form. Transformations preserving the form of Hamilton’s equations are called symplectomorphisms. The symplectic form plays the role of the metric, although it is antisymetric rather than symmetric and symplectic geometry (as opposed to Riemannian geometry) is a very interesting area. It gives you deeper insight into the Legendre transformation (which relates Lagrangian and Hamiltonian picture), for example.\n",
        "\n",
        "4. Lie groups\n",
        "\n",
        "Lie groups are continuous transformations under which something is symmetric. In quantum mechanics and quantum field theory, it is important to formulate the field equations which are compatible with equations of special relativity. In order to understand this in detail, you need a differential geometry and group theory. In this language, physical fields form a representation space for the Lorentz (Poincare) group. Representations of Lorentz group are labelled by spin (roughly), so for example spin 0 corresponds to particular representation of Lorentz group which gives you Klein-Gordon equation, similarly for spin 1 (Maxwell or Proca equations), and for spin 1/2, 3/2 (Dirac, Rarita-Schwinger). This is related to the next point.\n",
        "\n",
        "3. Spinors\n",
        "\n",
        "In quantum mechanics we need to describe particles with spin. As I said, field for spin 1/2 particle is a particular representation of the Lorentz group. This representation has a funny property that rotation by  360∘\n",
        "360\n",
        "∘\n",
        "  does not act like an identity, by like minus identity. Objects which have this property are called spinors. Dirac discovered them by intuition and guessing, but the real understanding of spinors comes from differential geometry. You start with the definition of the so-called Clifford algebra which is kind of generalization of exterior algebra of differential forms. Clifford algebra is (in certain sense) adapted to the metric and hence is suitable for study of orthogonal transformations. What is known in physics as Dirac’s  γ\n",
        "γ\n",
        " -matrices arises again from some representations of Clifford algebra (or Clifford groups).\n",
        "\n",
        "4. Gauge theories\n",
        "\n",
        "It turns out that physical fields have (beside the Lorentz invariance) also another symmetry known as the gauge symmetry. When you have a free field described by a Lagrangian and you want to let it interact with another field, requiring the gauge invariance you can derive more-less uniquely correct Lagrangian for interacting fields. This was discovered by physicists but later they realized that these gauge symmetries can be conveniently described in geometrical language. We understand the gauge symmetry as some kind of internal symmetry. To each point of the spacetime we attach a copy of the space (called fibre) representing the internal states at that point. Hence, we obtain a structure called principal G-bundle where G is the group acting on the fibres. The interaction between the two fields then arises as the covariant derivative on principal G-bundle. So, differential geometry is very efficient tool for studying these things.\n",
        "\n",
        "There are many more applications, for example in thermodynamics (in TD you discuss all the time which infinitesimals are exact differentials and which are not; this can be conveniently expressed as the integrability of distributions which can be decided by Frobeniu’s theorem), in condensed matter physics (for example, graphene is quite popular today; there is a connection between the properties of graphene crystals and Riemannian geometry), etc. I have enumerated just 4 examples which came to my mind at the moment and which belong to fairly standard physics."
      ],
      "metadata": {
        "id": "RzgJ1aLePgqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">**Geometry 500**"
      ],
      "metadata": {
        "id": "u6Cz4-k-vvpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Multilinear Algebra*"
      ],
      "metadata": {
        "id": "Nj_F9wX47fTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Algebra*"
      ],
      "metadata": {
        "id": "ZiAK4jblJhPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear algebra**\n",
        "\n",
        "Multilinear Algebra, as the name suggests, deals with the properties and applications of multilinear maps, which are functions of several vector variables that are linear in each argument. In other words, it generalizes linear algebra, which deals with linear transformations between vector spaces. The key concepts in multilinear algebra include tensors, tensor products, and various types of multilinear forms, including bilinear, trilinear forms, and so on.\n",
        "\n",
        "* [Multilinear algebra](https://en.m.wikipedia.org/wiki/Multilinear_algebra) extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, **multilinear algebra builds on the concepts of**\n",
        "\n",
        "  * [Multivectors (p-vectors)](https://en.m.wikipedia.org/wiki/Multivector)\n",
        "\n",
        "  * [Exterior algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra) bzw. [Grassmann-Algebra](https://de.m.wikipedia.org/wiki/Graßmann-Algebra).\n",
        "\n",
        "* Fundamental objects of study in multilinear algebra are\n",
        "\n",
        "  * [Multilinear maps](https://en.m.wikipedia.org/wiki/Multilinear_map) (Multilineare_Abbildung: Abbildung, die für jedes ihrer Argumente linear ist)\n",
        "\n",
        "  * [Multilinear forms](https://en.m.wikipedia.org/wiki/Multilinear_form)\n",
        "\n",
        "* Abbildung von Modul in einen Ring (also Verallgemeinerung der K-Algebra von Vektorraum in den Korper, zB bei Integration oder Differential)\n",
        "\n",
        "  * Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "  * Jede lineare Abbildung ist eine 1-lineare Abbildung.\n",
        "\n",
        "  * Jede bilineare Abbildung ist eine 2-lineare Abbildung. (Sämtliche gemeinhin übliche Produkte sind bilineare Abbildungen: die Multiplikation in einem Körper (reelle, komplexe, rationale Zahlen) oder einem Ring (ganze Zahlen, Matrizen), aber auch das Vektor- oder Kreuzprodukt, und das Skalarprodukt auf einem reellen Vektorraum.\n",
        "\n",
        "    * Ein Spezialfall der bilinearen Abbildungen sind die Bilinearformen. Bei diesen ist der Wertebereich G mit dem Skalarkörper K der Vektorräume E und F identisch.)\n",
        "\n",
        "  * Sparprodukt ist eine 3-lineare Abbildung\n",
        "\n",
        "*  Multilinearform: wie linear- oder bilinearform, nur mehr argumente.\n"
      ],
      "metadata": {
        "id": "lKfW4ry1JlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graßmann-Algebra (Exterior Algebra)**\n",
        "\n",
        "Grassmann Algebra, on the other hand, is a branch of mathematics developed by Hermann Grassmann in the mid-19th century. It is an exterior (or \"wedge\") algebra that extends the concepts of vectors to higher dimensions through the creation of new mathematical entities called \"multivectors\". These multivectors can be interpreted geometrically as oriented areas, volumes, and higher-dimensional analogues. The wedge product operation in Grassmann Algebra is antisymmetric and multilinear, making it a critical component of the study of differential forms and integral calculus on manifolds.\n",
        "\n",
        "* There is a connection between Graßmann-Algebra and Multilinear Algebra in the sense that the exterior algebra (Grassmann Algebra) of a vector space can be built from the tensor algebra, a concept arising in multilinear algebra. The exterior algebra is essentially a quotient of the tensor algebra by a certain ideal, which gives rise to the antisymmetry property of the wedge product.\n",
        "\n",
        "* It's important to mention that Grassmann Algebra is a particular instance of multilinear algebra, as the operations it defines (such as the wedge product) are multilinear. However, not all multilinear algebraic structures fall within Grassmann Algebra, because they might not possess the same antisymmetry properties. This distinction is what differentiates general multilinear algebra from the more specific Grassmann Algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Multivector\n",
        "\n",
        "Die Graßmann-Algebra $\\Lambda V$ eines reellen Vektorraumes $V$ ist die Clifford-Algebra $Cl(V,0)$ mit der trivialen quadratischen Form $Q=0$.\n",
        "\n",
        "Diese Beziehung ist unter anderem für die Quantisierung supersymmetrischer Feldtheorien wichtig.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Clifford-Algebra#Graßmann-Algebra\n",
        "\n",
        "* [Graßmann-Algebra](https://de.m.wikipedia.org/wiki/Graßmann-Algebra) bzw. [Exterior Algebra](https://en.m.wikipedia.org/wiki/Exterior_algebra): Algebra der Differentialformen.\n",
        "\n",
        "* Exterior Algebra eines Vektorraums V ist eine assoziative, schiefsymmetrisch-graduierte Algebra mit Einselement.\n",
        "\n",
        "* The exterior algebra provides an algebraic setting in which to answer geometric questions.\n",
        "\n",
        "* **Exterior algebra, geometric algebra, and clifford algebra are linear algebras** [Quora](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* Sie ist – je nach Definition – **Unteralgebra oder eine Faktoralgebra einer antisymmetrisierten** [**Tensoralgebra**](https://de.m.wikipedia.org/wiki/Tensoralgebra) von V und wird durch $\\Lambda V$ dargestellt.\n",
        "\n",
        "* Die Multiplikation wird als **äußeres Produkt, Keilprodukt, Dachprodukt oder Wedgeprodukt** bezeichnet. Ein Spezialfall dieses Produkts ist mit dem Kreuzprodukt verwandt.\n",
        "\n",
        "* Anwendung: linearen Algebra (Theorie der Determinanten), Differentialgeometrie (Algebra der Differentialformen)"
      ],
      "metadata": {
        "id": "PaJipKZ_JnRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Vectors ($k$-blades)*"
      ],
      "metadata": {
        "id": "WqR3v9EaJrvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exterior Product (Wedge product u $\\wedge$ v) in Exterior Algebra (Graßmann-Algebra)**\n",
        "\n",
        "* **The exterior product (also called the Wedge product u $\\wedge$ v) used to construct bivectors and multivectors (is multilinear)**\n",
        "\n",
        "* the [exterior product or wedge product of vectors](https://en.m.wikipedia.org/wiki/Exterior_algebra) is an algebraic construction used in geometry to study areas, volumes, and their higher-dimensional analogues.\n",
        "\n",
        "> **The exterior product (also called the wedge product) used to construct bivectors and multivectors (is multilinear)**: The wedge product of two vectors is another object commonly called a bivector, and these lie in their own vector space. [Quora](https://www.quora.com/Is-the-wedge-product-the-same-thing-as-the-general-outer-product-If-not-why-is-the-wedge-symbol-used-for-both)\n",
        "\n",
        "* The exterior product of two vectors $u$ and $v$, denoted by $u\\wedge v$, is called a [bivector](https://en.m.wikipedia.org/wiki/Bivector) and lives in a space called the exterior square, a vector space that is distinct from the original space of vectors.\n",
        "\n",
        "* The exterior product, commonly called the wedge product, acts on tangent vectors and is an important operation in differential geometry that generalizes the cross product of 3-vectors.\n",
        "\n",
        "* See also: https://towardsdatascience.com/exterior-product-ecd5836c28ab\n",
        "\n",
        "* Add-on Differentiation:\n",
        "\n",
        "  * The **exterior product** is related to the tensor product in that the exterior product of two forms (a form is a skew-symmetric tensor of type (0,𝑝)) is just the antisymmetrization of the tensor product.\n",
        "\n",
        "  * The **cross product** is a speciality of the three-dimensional space; here the space of 2-forms has the same dimension as the space of 1-forms; indeed, given a metric, the hodge star maps between them. Since the metric also allows to associate vectors and 1-forms, you can define the cross product of v and 𝑤 by the following procedure: Determine the 1-forms corresponding to 𝑣 and 𝑤, calculate their exterior product (which is a 2-form), apply the Hodge star to the result (which, given that we are in three dimensions, again results in a 1-form), and finally determine the vector corresponding to that 1-form.\n",
        "\n",
        "  * [Cross product as an external product](https://en.m.wikipedia.org/wiki/Cross_product#Cross_product_as_an_external_product)\n",
        "\n",
        "  * https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "9yW5RW7IJtlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$k$-blade ($k$-vector)**\n",
        "\n",
        "* a [$k$-blade](https://en.m.wikipedia.org/wiki/Blade_(geometry)) or a simple $k$-vector is a generalization of the concept of scalars and vectors to include simple bivectors, trivectors, etc.\n",
        "\n",
        "* Specifically, a $k$ blade is a $k$-vector that can be expressed as the exterior product (informally wedge product) of 1-vectors, and is of grade $k$. In detail:\n",
        "\n",
        "  * A 0-blade is a scalar.\n",
        "\n",
        "  * A 1-blade is a vector. Every vector is simple.\n",
        "\n",
        "  * A 2-blade is a simple bivector. Sums of 2-blades are also bivectors, but not always simple. A 2-blade may be expressed as the wedge product of two vectors $a$ and $b$ : $a \\wedge b$.\n",
        "\n",
        "  * A 3-blade is a simple trivector, that is, it may be expressed as the wedge product of three vectors $a, b$, and $c$ : $a \\wedge b \\wedge c \\text {. }$\n",
        "\n",
        "  * In a vector space of dimension $n$, a blade of grade $n-1$ is called a [pseudovector](https://en.m.wikipedia.org/wiki/Pseudovector) or an [antivector](https://en.m.wikipedia.org/wiki/Antivector)"
      ],
      "metadata": {
        "id": "ffxYeT3eJvhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Antisymmetry**\n",
        "\n",
        "> $\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2}=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}$\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\mathbf{e}_{1} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} &=-\\mathbf{e}_{2} \\wedge \\mathbf{e}_{1} \\wedge \\mathbf{e}_{3} \\\\\n",
        "&=\\mathbf{e}_{2} \\wedge \\mathbf{e}_{3} \\wedge \\mathbf{e}_{1} \\\\\n",
        "&=-\\mathbf{e}_{3} \\wedge \\mathbf{e}_{2} \\wedge \\mathbf{e}_{1}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra"
      ],
      "metadata": {
        "id": "2ouXP-bDJxWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivector (Clifford number)**\n",
        "\n",
        "* In multilinear algebra, a [multivector](https://en.m.wikipedia.org/wiki/Multivector), sometimes called **Clifford number**,  is an element of the exterior algebra $\\Lambda(V)$ of a vector space $V$.\n",
        "\n",
        "* The exterior product $\\wedge$ (Wedge product) used to construct [bivectors](https://en.m.wikipedia.org/wiki/Bivector) and [multivectors](https://en.m.wikipedia.org/wiki/Multivector) (is multilinear)\n",
        "\n",
        "* This algebra is graded, associative and alternating, and consists of linear combinations of simple $k$ -vectors, also known as decomposable $k$ -vectors  or [$k$-blades](https://en.m.wikipedia.org/wiki/Blade_(geometry)), of the form\n",
        "\n",
        "> $v_{1} \\wedge \\cdots \\wedge v_{k}$\n",
        "\n",
        "where $v_{1}, \\ldots, v_{k}$ are in $V$.\n",
        "\n",
        "* A $k$ -vector is such a linear combination that is homogeneous of degree $k$ (all terms are\n",
        "$k$ -blades for the same $k$ ). Depending on the authors, a \"multivector\" may be either a $k-$\n",
        "vector or any element of the exterior algebra (any linear combination of $k$ -blades with potentially differing values of $k$ ).\n",
        "\n",
        "* In differential geometry, a $k$ -vector is a vector in the exterior algebra of the tangent\n",
        "vector space; that is, it is an antisymmetric tensor obtained by taking linear\n",
        "combinations of the exterior product of $k$ tangent vectors, for some integer $k \\geq 0 .$\n",
        "\n",
        "* A differential $k$ -form is a $k$ -vector in the exterior algebra of the dual of the tangent space,\n",
        "which is also the dual of the exterior algebra of the tangent space.\n",
        "\n",
        "> **For $k=0,1,2$ and $3, k$ -vectors are often called respectively scalars, vectors, bivectors and trivectors; they are respectively dual to 0 -forms, 1 -forms, 2 -forms and 3 forms.**"
      ],
      "metadata": {
        "id": "JbnIY2DPJzO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation for the exterior product of n 1-forms (ε, η, ω) to obtain an n-form (\"mesh\" of coordinate surfaces, here planes), for n = 1, 2, 3. The \"circulations\" show orientation*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0938.png)"
      ],
      "metadata": {
        "id": "yyX6pjvrJ1J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Geometric interpretation of grade n elements in a real exterior algebra for n = 0 (signed point), 1 (directed line segment, or vector), 2 (oriented plane element), 3 (oriented volume). The exterior product of n vectors can be visualized as any n-dimensional shape (e.g. n-parallelotope, n-ellipsoid); with magnitude (hypervolume), and orientation defined by that of its (n − 1)-dimensional boundary and on which side the interior is.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0939.png)"
      ],
      "metadata": {
        "id": "vcYQLc-WJ3H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Parallel plane segments with the same orientation and area corresponding to the same bivector a ∧ b:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Wedge_product.JPG/471px-Wedge_product.JPG)"
      ],
      "metadata": {
        "id": "7-GwR_RyJ40H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Maps (Operators)*"
      ],
      "metadata": {
        "id": "JYooOdCtJ76C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilinear Map**\n",
        "\n",
        "> [*A bilinear map*](https://en.m.wikipedia.org/wiki/Bilinear_map) *is a function combining elements of two vector spaces to yield an element of a third vector space, and is linear in each of its arguments. Matrix multiplication is an example as visible in $B : V × W → X$. In contrast: linear map war eine matrix als map von vector zu vector, das hier ist eine matrix als map zw matrix und matrix!* **In mathematics, a bilinear operator is a generalized \"multiplication\" which satisfies the distributive law.**\n",
        "\n",
        "> **[Multilinear Map](https://en.m.wikipedia.org/wiki/Multilinear_map): A function that is linear when all inputs except one are held constant** (they are linear in each input variable)\n",
        "\n",
        "* when we scale the input variable by $n$ (and all other are held constant) that's the same as scaling the ouput of the function by $n$\n",
        "\n",
        "* when we hold all input constant except one, and we do a sum in the input slot, that's the same thing as doing the sum of the outputs (in image below)\n",
        "\n",
        "* Riemann Curvature Tensor is a multilinear map. Wie findet man heraus, ob eine Surface flach oder gekruemmt ist?\n",
        "\n",
        "* **If the codomain of a multilinear map is the field of scalars, it is called a multilinear form**\n",
        "\n",
        "* For multilinear maps used in cryptography, see [Cryptographic multilinear map](https://en.m.wikipedia.org/wiki/Cryptographic_multilinear_map).\n",
        "\n",
        "* https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Matrix_multiplication\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 15: Tensor Product Spaces](https://www.youtube.com/watch?v=M-OLmxuLdbU&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=18)\n",
        "\n"
      ],
      "metadata": {
        "id": "LhNgU-EEMp8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_71.png)\n",
        "\n",
        "And all tensors are **multilinear maps** which means they are functions that take some number of inputs and they are linear in each input variable while all other input variables are held constant (kind of ceteris paribus in economics!).\n",
        "\n",
        "* If you make all inputs constant except one, we can scale the input before or scale the output after and we get the same result\n",
        "\n",
        "* And also if we replace these input vector components with a sum of two sets of vector components I can just distribute these out and get this sum here (red line under equation in image), so basically I can add the inputs or I can add the outputs.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_73.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OjmCCbxNnzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Special: Differential Operator*\n",
        "\n",
        "[Differential operator](https://en.m.wikipedia.org/wiki/Differential_operator) is an operator defined as a function of the differentiation operator. Consider differentiation as an abstract operation that **accepts a function and returns another function**.\n",
        "\n",
        "*Differential- & Nabla-Operator*\n",
        "\n",
        "Der [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, **um kontextabhängig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren**. Es ist zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei Differentialoperatoren [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) (Anwendung im [Gradientenverfahren](https://de.wikipedia.org/wiki/Gradientenverfahren) in der Numerik), [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes) oder [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes)\n",
        "\n",
        "https://www.youtube.com/watch?v=YW-bUVIOpB0&t=51s\n",
        "\n",
        "* **Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enthält.)\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enthält.)\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und Stärke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzufließen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "  * [Rotation](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "* **Diese drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei räumlicher Drehung des ursprünglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden. Diese Eigenschaft folgt aus den **koordinatenunabhängigen** Definitionen.\n",
        "\n",
        "https://www.youtube.com/watch?v=rB83DpBJQsE\n",
        "\n",
        "*Integraloperator*\n",
        "\n",
        "* **Integral**: Der [**Volterraoperator**](https://de.wikipedia.org/wiki/Integraloperator#Volterraoperator) $\\int_{0}^{t}$ zur Bildung des [bestimmten Integrals](https://de.wikipedia.org/wiki/Integralrechnung) (ist ein Beispiel fur einen [Integraloperator](https://de.wikipedia.org/wiki/Integraloperator). Operatoren wie diese, die einer Funktion eine Zahl zuordnen, nennt man [Funktional](https://de.wikipedia.org/wiki/Funktional).\n",
        "\n",
        "  * Die Fourier-Transformation ${\\mathcal {F}}$ ist z.B. ein **linearer Operator** (siehe unten).\n",
        "\n",
        "* Ein [linearer Integraloperator](https://de.wikipedia.org/wiki/Integraloperator) ist ein mathematisches Objekt aus der Funktionalanalysis. Dieses Objekt ist ein linearer Operator, der mit einer bestimmten Integralschreibweise mit einem Integralkern dargestellt werden kann."
      ],
      "metadata": {
        "id": "Y0ofnEZLKsn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Multilinear Forms (Functionals, Covectors, Differential Forms)*"
      ],
      "metadata": {
        "id": "MMxCx0fZKDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**0-form = Function**:\n",
        "\n",
        "* is a special case and is simply a smooth function. **A 0-forms eats a point and returns a number**\n",
        "\n",
        "* [Glatte Funktionen](https://de.m.wikipedia.org/wiki/Glatte_Funktion) sind 0-Formen. Is a skalar from a function.\n",
        "\n",
        "From 0-form to 1-form:\n",
        "\n",
        "> **Siehe auch [Level Sets](https://en.wikipedia.org/wiki/Level_set) bzw. Niveaumenge** (= die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist.)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1585.png)\n",
        "\n",
        "Visualizing One-Forms:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1586.png)\n"
      ],
      "metadata": {
        "id": "dr3LzbeZhgpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**1-form = Linear Form** (Pfaffsche Formen, Covector, Functional). **Eats a vector and returns a number (scalar):. Is a Covector.**\n",
        "\n",
        "> $\\mathcal{B}: V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* *Linear Form = 1-Form = Functional (incl Distribution) = Covector = Dualvector = Differentialformen*\n",
        "\n",
        "* This scalar can be a integrand (Wegintegral) or Differential, Temperature, Speed for 1-Form\n",
        "\n",
        "* [Pfaffsche Formen](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) sind 1-Formen (= zB Wegintegral). [1-Formen](https://en.m.wikipedia.org/wiki/One-form) bilden die Grundlage für die Einführung von Differentialformen.\n",
        "Pfaffsche Formen sind die natürlichen Integranden für Wegintegrale. Kovektorfeld oder kurz 1-Form ein Objekt, das in gewisser Weise dual zu einem Vektorfeld ist.\n",
        "\n",
        "* Exkurs **Dualraum**: Die Menge aller Linearformen (= stetigen, linearen Abbildungen) über einem gegebenen Vektorraum $V$ bildet dessen [Dualraum](https://de.wikipedia.org/wiki/Dualraum) $V^{*}$ und damit selbst wieder in natürlicher Weise einen $K$ -Vektorraum. Die Menge aller Funktionale ist wiederum in natürlicher Form ein Vektorraum uber dem gleichen Körper $\\mathbb{K}$, indem man für zwei Funktionale $f$ und $g$ über $V$ die Addition und Skalarmultiplikation punktweise definiert. Zu einem Vektorraum $V$ über einem Körper $K$ bezeichnet $V^{*}$ den zu $V$ gehörigen [Dualraum](https://de.m.wikipedia.org/wiki/Dualraum), das heißt die Menge aller linearen Abbildungen von $V$ nach $K$. All covectors can be written as the linear combination of the dual basis vectors!\n",
        "\n",
        "\n",
        "* Es ist eine Differentialform vom Grad 1. A one-form on a differentiable manifold is a smooth section of the cotangent bundle (= differential form).\n",
        "\n",
        "* A [linear form](https://de.m.wikipedia.org/wiki/Linearform) is a function that takes one or more vectors as input and outputs a number (= lineare Abbildung von einem Vektorraum in den zugrundeliegenden Körper)\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* One Form and Line Integral: the expression $f(x) d x$ from one-variable calculus is an example of a [$l-$ form](https://en.m.wikipedia.org/wiki/One-form) (Pfaff'sche Form), and can be integrated over an oriented interval $[a, b]$ in the domain of $f: \\int_{a}^{b} f(x) d x$. See Video: [Line Integrals in Differential Forms](https://youtu.be/dhZgGIYzPUU).\n",
        "\n",
        "* [Video: How to visualise a one-form](https://www.youtube.com/watch?v=dxz9JZPewu8): family of surfaces that it pierces. Integrating a one-form, we are just counting the number of surfaces that the line actually passes through\n",
        "\n",
        "* Each term contains one differential: $\\begin{array}{a}\n",
        "\\omega=2 x d x+3 y d y-d z, \\\\\n",
        "\\omega=x d y, \\\\\n",
        "\\omega=d x .\n",
        "\\end{array}$\n",
        "\n",
        "* Example from quantum mechanics:\n",
        "\n",
        "> **A column vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "x \\\\\n",
        "y\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>vector</u>.** i.e. Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state'\n",
        "\n",
        "> **A row vector $\\begin{equation}\n",
        "\\left[\\begin{array}{ll}\n",
        "2 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ represents the components of a <u>covector (linear form, functional)</u>.** i.e. Bra $[b_{0} \\quad b_{1}]$\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti–isomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra–ket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ⟨0∣1⟩ und ⟨1∣0⟩ ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ⟨0∣0⟩ und ⟨1∣1⟩ = 1.\n",
        "\n",
        "* In Funktionalanalyse ist der untersuchte Vektorraum $V$ zumeist ein Funktionenraum, wo diesen durch Funktionale Skalare zugeordnet werden. Beispiel: [Lebesgue-Integral](https://de.m.wikipedia.org/wiki/Lebesgue-Integral).\n",
        "\n",
        "* Siehe auch [Funktionaldeterminante](https://de.wikipedia.org/wiki/Funktionaldeterminante) oder Jacobi-Determinante fur Koordinatentransformationen zB von kartesisches zu Polarkoordinaten in der mehrdimensionalen Integralrechnung, also der **Berechnung von Oberflächen- und Volumenintegralen**  an.\n",
        "\n",
        "* Siehe auch [Functional integration](https://en.m.wikipedia.org/wiki/Functional_integration): Richard Feynman used functional integrals as the central idea in his sum over the histories formulation of quantum mechanics. This usage implies an integral taken over some function space.\n",
        "\n",
        "* [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 4: What are Covectors?](https://www.youtube.com/watch?v=LNoQ_Q5JQMY)"
      ],
      "metadata": {
        "id": "EbzZ6TUChpNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**2-form = Bilinear Form (Surface Integral)**: eats two vectors and returns a number  (scalar): $k=2$,  $f:V\\times V\\to K$\n",
        "\n",
        "> $\\mathcal{B}: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* Algebraic rules - linearity properties:\n",
        "\n",
        "  * $a \\mathcal{B}(\\vec{v}, \\vec{w})=\\mathcal{B}(a \\vec{v}, \\vec{w})=\\mathcal{B}(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}+\\vec{u}, \\vec{w})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $\\mathcal{B}(\\vec{v}, \\vec{w}+\\vec{t})=\\mathcal{B}(\\vec{v}, \\vec{w})+\\mathcal{B}(\\vec{v}, \\vec{t})$\n",
        "\n",
        "> $\\omega=8 d y \\wedge d z$\n",
        "\n",
        "* Similarly, the expression $f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z$ is a 2 -form that has a [surface integral](https://en.m.wikipedia.org/wiki/Surface_integral) over an oriented surface $S$ : $\\int_{S}(f(x, y, z) d x \\wedge d y+g(x, y, z) d z \\wedge d x+h(x, y, z) d y \\wedge d z) .$\n",
        "\n",
        "* The symbol $\\wedge$ denotes the exterior product, sometimes called the wedge product, of two differential forms.\n",
        "\n",
        "* [Bilinear Forms](https://en.m.wikipedia.org/wiki/Bilinear_form) sind ein Spezialfall der bilinearen Abbildungen (Wertebereich G ist mit dem Skalarkörper K der Vektorräume E und F identisch). **Winkel sind wichtiger Anwendungsfall dafur: man kann Winkel nicht mit linearen Abbildungen beschreiben, weil es dafur 2 Vektoren braucht.**\n",
        "\n",
        "  * A familiar and important example of a (symmetric) bilinear form is the standard [inner product (dot product)](https://en.m.wikipedia.org/wiki/Dot_product) of vectors. Jedes Skalarprodukt ist wiederum eine spezielle Bilinearform (es gelten noch weitere Eigenschaften: symmetrisch <v,w> = <w,v>, und positiv definit). Genauso Integral.\n",
        "\n",
        "  * [Bilinearform](https://de.wikipedia.org/wiki/Bilinearform), Bilinearform: cross product of two vectors, normal and tangent, see [Frenet–Serret_formulas](https://en.m.wikipedia.org/wiki/Frenet–Serret_formulas).\n",
        "\n",
        "* Die [Sesquilinearform](https://en.m.wikipedia.org/wiki/Sesquilinear_form) ist eine Generalizations der Bilinear Form auf den Koerper der komplexen Zahlen\n",
        "\n",
        "* Der [Metric Tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) ist ein Spezialfall einer Bilinear Form. **A metric tensor is two-form because it takes 2 vectors to output one scalar, which is length or angle**. The metric tensor has 2 additional properties that other bilinear forms might not have: components are symmetric, output must be positive:\n",
        "\n",
        "  * Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  * Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Just like the metric tensor <font color=\"red\">bilinear forms are (0,2) tensors</font> (so they transform using 2 covariant rules when we change coordinate systems):\n",
        "\n",
        "  * $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "  * $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 10: Bilinear Forms](https://www.youtube.com/watch?v=jLiBCaBEB3o&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14) und das Video [Bilinearform einfach erklärt :) | Math Intuition](https://www.youtube.com/watch?v=TjAFH6hWg1I)"
      ],
      "metadata": {
        "id": "4dmD8N0tSkWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**3 -form = Trilinear Form (Volume Integral)**: eats three vectors and returns a number:\n",
        "\n",
        "* $\\omega=-7 z d x \\wedge d y \\wedge d z$\n",
        "\n",
        "* Likewise, a $3-$ form $f(x, y, z) d x \\wedge d y \\wedge d z$ represents a [volume element](https://en.m.wikipedia.org/wiki/Volume_element) that can be integrated over an oriented region of space."
      ],
      "metadata": {
        "id": "mtRicmZJUH1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**$k$-form = Multilinear Form**: eats $k$-vectors and returns a number: $\\omega= .. $\n",
        "\n",
        "> $V \\times V \\times \\cdots \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* In general, a $k$-form is an object that may be integrated over a $k$-dimensional oriented manifold, and is homogeneous of degree $k$ in the coordinate differentials.\n",
        "\n",
        "* On an $n$-dimensional manifold, the top-dimensional form $(n$-form $)$ is called a [volume form](https://en.m.wikipedia.org/wiki/Volume_form)\n",
        "\n",
        "* A [multilinear form](https://en.m.wikipedia.org/wiki/Multilinear_form) on a vector space $V$ over a field $K$ is a map\n",
        "$f: V^{k} \\rightarrow K$\n",
        "that is separately $K$ -linear in each of its $k$ arguments.\n",
        "\n",
        "* A multilinear $k$ -form on $V$ over $\\mathbf{R}$ is called a (**covariant**) **$k$ -tensor**, and the vector space is usually denoted $\\mathcal{T}^{k}(V)$ or $\\mathcal{L}^{k}(V) \\cdot$\n",
        "\n",
        "* <font color=\"red\">Multilinear forms are (0,k) tensors</font> (so they transform using k covariant rules when we change coordinate systems) (??)\n",
        "\n",
        "* Kovariante Tensoren (Covectors) sind Multilinearformen [Source](https://de.m.wikipedia.org/wiki/Multilinearform)\n",
        "\n",
        "* Die Determinante in einem n-dimensionalen Vektorraum ist eine n-lineare Multilinearform.\n",
        "\n",
        "* https://unapologetic.wordpress.com/2009/10/22/multilinear-functionals/\n",
        "\n",
        "* Understanding the definition of tensors as multilinear maps: https://math.stackexchange.com/questions/2138459/understanding-the-definition-of-tensors-as-multilinear-maps\n",
        "\n",
        "* [Eigenchris: Tensor Calculus 6: Differential Forms are Covectors](https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8)\n",
        "\n",
        "* **Exkurs: Exterior Derivative (Äußere Ableitung)**\n",
        "\n",
        "  * Die [äußere Ableitung](https://de.m.wikipedia.org/wiki/Äußere_Ableitung) ist ein Operator, der einer k-Differentialform eine $(k+1)$-Differentialform zuordnet.\n",
        "\n",
        "  * Betrachtet man sie auf der Menge der $0$-Differentialformen, also auf der Menge der glatten Funktionen, so entspricht die äußere Ableitung der üblichen Ableitung für Funktionen.\n",
        "\n",
        "  * Die äußere Ableitung $\\mathrm{d} \\omega$ einer $k$ -Form $\\omega$ wird induktiv mithilfe der [Lie-Ableitung](https://de.m.wikipedia.org/wiki/Lie-Ableitung) (=die Ableitung eines Vektorfeldes oder allgemeiner eines Tensorfeldes entlang eines Vektorfeldes. ) und der Cartan-Formel\n",
        "\n",
        "  > $\n",
        "\\mathcal{L}_{X}=i_{X} \\circ \\mathrm{d}+\\mathrm{d} \\circ i_{X}\n",
        "$\n",
        "\n",
        "  * definiert; dabei ist $X$ ein Vektorfeld, $\\mathcal{L}_{X}$ die Lie-Ableitung und $i_{X}$ die Einsetzung von $X$.\n",
        "\n",
        "* **Exkurs: Go from Differential Operators (grad, div, curl) to Differential Forms with Exterior Derivative:**\n",
        "\n",
        "  * By taking the [exterior derivative](https://de.m.wikipedia.org/wiki/Äußere_Ableitung) of 0-forms, 1-forms and 2-forms we can express the three important operators of vector calculus - grad, curl and div - in the language of differential forms. The gradient of a sclara field f(x,y,z) is given by:\n",
        "\n",
        "  * $\\operatorname{grad} f=\\nabla f=\\frac{\\partial f}{\\partial x} \\hat{e}_{x}+\\frac{\\partial f}{\\partial y} \\hat{e}_{y}+\\frac{\\partial f}{\\partial z} \\hat{\\mathrm{e}}_{z}$\n",
        "\n",
        "  * Using the correspondance:\n",
        "\n",
        "  * $d x \\Leftrightarrow \\hat{\\mathbf{e}}_{x}, d y \\Leftrightarrow \\hat{\\mathbf{e}}_{y}, d z \\Leftrightarrow \\hat{\\mathbf{e}}_{z}$\n",
        "\n",
        "  * this gradient vector field can be associated with the exterior derivative of the 0-form f(x,y,z), which is a 1-form\n",
        "\n",
        "  * $d f=\\frac{\\partial f}{\\partial x} d x+\\frac{\\partial f}{\\partial y} d y+\\frac{\\partial f}{\\partial z} d z$\n",
        "\n",
        "  * Grad operator: $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$"
      ],
      "metadata": {
        "id": "gtnhb-2MLClh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "> **Types of Differential Forms where the $\\wedge$ symbol denotes a type of multiplication called the wedge product (exterior product)**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors (for Line Integral)\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation (for Surface Integral)\n",
        "* Trivector = 3D - oriented volume (for Volume Integral)\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "uDFxcwyecPgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential Form (Differential Geometry / Tangent Spaces)**:\n",
        "\n",
        "* **[Differential forms](https://en.m.wikipedia.org/wiki/Differential_form) are linear or, more generally, multilinear alternating functions of tangent vectors**. Eine Differentialform ordnet einem Punkt einer Mannigfaltigkeit eine **alternierende Multilinearform** auf dem zugehörigen Tangentialraum zu. Differentialformen sind dabei das bekannteste Beispiel von Schnitten.\n",
        "\n",
        "* The differential forms form an [alternating algebra](https://en.m.wikipedia.org/wiki/Alternating_algebra). This implies that:\n",
        "\n",
        "  > $dy\\wedge dx=-dx\\wedge dy$\n",
        "\n",
        "  > $dx\\wedge dx=0.$\n",
        "\n",
        "* This alternating property reflects the orientation of the domain of integration.\n",
        "\n",
        "* Differential forms provide a unified approach to define integrands over curves, surfaces, solids, and higher-dimensional manifolds. ([Differentialformen](https://de.m.wikipedia.org/wiki/Differentialform) erlauben eine koordinatenunabhängige Integration auf allgemeinen orientierten differenzierbaren Mannigfaltigkeiten.)\n",
        "\n",
        "* **Differential forms are a special class of tensors where you can find derivatives even in the absence of connections or metrics**. You can think of differential forms as a generalization of single variable calculus that works on manifolds, as well as curves, solids, and surfaces.\n",
        "\n",
        "* [Graßmann-Algebra](https://de.m.wikipedia.org/wiki/Graßmann-Algebra) ist die Algebra der Differentialformen\n",
        "\n",
        "* Simple Example of a Differential Form: in the expression $\\int_{0}^{1}$ <font color=\"blue\">$ x^{2} d x$</font> the term $x^{2}$ is the **integrand** and <font color=\"blue\">$ x^{2} d x$</font> is the **differential form** [(Source)](https://www.calculushowto.com/differential-operator/). So for identification, differential forms can be generally recognised as things containing differentials such as $d x, d y$ and $d z$.\n",
        "\n",
        "  * [Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes) (Vektoranalysis): sehr grundlegenden Satz über die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "  * [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) (sowie Koordinatentransformationen und Funktionaldeterminante in der Vektoranalysis): Aus mathematischer Sicht ist eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird Riemann'sche Volumenform genannt (**Hodge-Stern-Operator in der Differentialgeometrie**).\n",
        "\n",
        "* Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind. Covectors (functional): Differential Forms = Covector Fields. Video Eigenchris: [Differential Forms and Covectors](https://youtu.be/XGL-vpk-8dU)\n",
        "\n",
        "Definition der Differentialform: Es sei $U$\n",
        "- eine offene Teilmenge des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Untermannigfaltigkeit des $\\mathbb{R}^{n}$\n",
        "- oder eine differenzierbare Mannigfaltigkeit.\n",
        "\n",
        "In jedem dieser Fälle gibt es:\n",
        "- den Begriff der differenzierbaren Funktion auf $U$; der Raum der beliebig oft differenzierbaren Funktionen auf $U$ werde mit $C^{\\infty}(U)$ bezeichnet;\n",
        "- den Begriff des Tangentialraums $\\mathrm{T}_{p} U$ an $U$ in einem Punkt $p \\in U$;\n",
        "- den Begriff der Richtungsableitung $\\frac{\\partial f}{\\partial X}$ für einen Tangentialvektor $X \\in \\mathrm{T}_{p} U$ und eine differenzierbare Funktion $f$;\n",
        "- den Begriff des differenzierbaren Vektorfeldes auf $U$; der Raum der Vektorfelder auf $U$ sei mit $\\Gamma(\\mathrm{T} U)$ bezeichnet.\n",
        "- Der Dualraum des Tangentialraums $\\mathrm{T}_{p} U$ wird als Kotangentialraum $\\mathrm{T}_{p}^{*} U$\n",
        "bezeichnet.\n",
        "\n",
        "**Definition: Eine Differentialform vom Grad $k$ auf $U$ oder kurz $k$ -Form $\\omega$ ist ein glatter Schnitt in der $k$ -ten äußeren Potenz des Kotangentialbündels von $U$**.\n",
        "\n",
        "* In symbolischer Schreibweise bedeutet dies $\\omega \\in \\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$, wobei $T^{*} U$ das Kotangentialbündel von $U, \\Lambda^{k}\\left(T^{*} U\\right)$ die $k$ -te äußere Potenz von $T^{*} U$ und $\\Gamma\\left(\\Lambda^{k}\\left(T^{*} U\\right)\\right)$ somit die Menge der glatten Schnitte von $\\Lambda^{k}\\left(T^{*} U\\right)$ bezeichnet.\n",
        "\n",
        "* Dies bedeutet, **dass jedem Punkt $p \\in U$ eine alternierende Multilinearform $\\omega_{p}$ auf dem Tangentialraum $T_{p} U$ zugeordnet wird**; und zwar so, dass für $k$ glatte Vektorfelder $X_{1}, \\ldots, X_{k}$ die folgende Funktion glatt, also beliebig oft differenzierbar, ist:\n",
        "\n",
        "> $p \\mapsto \\omega_{p}\\left(\\left(X_{1}\\right)_{p}, \\ldots,\\left(X_{k}\\right)_{p}\\right) \\in \\mathbb{R}$\n",
        "\n",
        "* **Alternativ dazu kann man eine $k$ -Form $\\omega$ als eine alternierende, glatte multilineare Abbildung $\\omega:(\\Gamma T U)^{k} \\rightarrow C^{\\infty}(U)$ auffassen**.\n",
        "\n",
        "* Das bedeutet: $\\omega$ ordnet $k$ Vektorfeldern $X_{1}, \\ldots, X_{k}$ eine Funktion $\\omega\\left(X_{1}, \\ldots, X_{k}\\right)$ zu, sodass\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}+X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)=\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime}, \\ldots, X_{k}\\right)+\\omega\\left(X_{1}, \\ldots, X_{i}^{\\prime \\prime}, \\ldots, X_{k}\\right)$\n",
        "\n",
        "* $\\omega\\left(X_{1}, \\ldots, f \\cdot X_{i}, \\ldots, X_{k}\\right)=f \\cdot \\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$ für $f \\in C^{\\infty}(U), 1 \\leq i \\leq k$\n",
        "\n",
        "* und\n",
        "\n",
        "> $\\omega\\left(X_{1}, \\ldots, X_{i}, \\ldots, X_{j}, \\ldots, X_{k}\\right)=-\\omega\\left(X_{1}, \\ldots, X_{j}, \\ldots, X_{i}, \\ldots, X_{k}\\right)$\n",
        "gilt.\n",
        "\n",
        "* Alternative unter Rückgriff auf Tensorfelder: **Eine $k$ -Form ist ein alternierendes, kovariantes Tensorfeld der Stufe $k$.**"
      ],
      "metadata": {
        "id": "dl0fYX1-K_B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Tensor Algebra*"
      ],
      "metadata": {
        "id": "fVQl5cbp7iqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Analysis*"
      ],
      "metadata": {
        "id": "PsodM6QW8wsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Motivation:*** *In our subject of differential geometry, where you talk about manifolds, **one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning**. They are allowed to **undergo transformation**. And in order to handle this kind of situation, an important tool is the so-called **tensor analysis, or Ricci calculus**, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.*\n",
        "\n",
        "* Die [Tensoranalysis](https://de.wikipedia.org/wiki/Tensoranalysis) ist ein Teilgebiet der Differentialgeometrie beziehungsweise der Differentialtopologie.\n",
        "\n",
        "* In der Tensoranalysis wird **das Verhalten von geometrischen Differentialoperatoren auf Tensorfeldern untersucht**.\n",
        "\n",
        "> <font color=\"blue\">**Tensor Analysis ist eine Verallgemeinerung der Vektoranalysis**</font>\n",
        "\n",
        "* Zum Beispiel kann der Differentialoperator Rotation in diesem Kontext auf n Dimensionen verallgemeinert werden.\n",
        "\n",
        "* Zentrale Objekte der Tensoranalysis sind Tensorfelder. Es wird untersucht, wie Differentialoperatoren auf diesen Feldern wirken.\n",
        "\n",
        "* Vektoren und Matrizen, sofern sie geometrische oder physikalische Groessen reprasentieren, koennen unter dem begriff eines tensors subsumiert werden\n",
        "\n",
        "* **Tensorrechnung in 2 teilen:**\n",
        "\n",
        "  * Anschauungsraum fur ingenieure (**kartesische tensoren**).\n",
        "\n",
        "  * Fur **schiefwinklige (also affine) oder krummlinige Koordinaten** sind begriffe wie kovariant und kontravariant wichtig (zur arbeit mit metriken, deren skalarproduktauf einer nicht positiv definiten bilinearform beruht)."
      ],
      "metadata": {
        "id": "rx-E5d4r8zxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoralgebra**\n",
        "\n",
        "> **The [tensor algebra](https://en.m.wikipedia.org/wiki/Tensor_algebra) is important because many other algebras arise as quotient algebras of T(V)**. These include the [exterior algebra (Grassmann algebra of Differential Forms)](https://en.m.wikipedia.org/wiki/Exterior_algebra), the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra), the [Clifford algebras](https://en.m.wikipedia.org/wiki/Clifford_algebra), the [Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra) and [universal enveloping algebras](https://en.m.wikipedia.org/wiki/Universal_enveloping_algebra).\n",
        "\n",
        "* Die [Tensoralgebra](https://de.m.wikipedia.org/wiki/Tensoralgebra) ist ein mathematischer Begriff, der in vielen Bereichen der Mathematik wie der linearen Algebra, der Algebra, der Differentialgeometrie sowie in der Physik verwendet wird.\n",
        "\n",
        "* Sie fasst \"alle Tensoren\" über einem Vektorraum in der Struktur einer graduierten Algebra zusammen.\n",
        "\n",
        "* Es sei $V$ ein Vektorraum über einem Körper $K$ oder allgemeiner ein Modul über einem kommutativen Ring mit Einselement. Dann ist die Tensoralgebra (als Vektorraum) definiert durch die direkte Summe aller Tensorprodukte des Raums mit sich selbst.\n",
        "\n",
        ">$\n",
        "\\mathrm{T}(V):=\\bigoplus_{n \\geq 0} V^{\\otimes n}=K \\oplus V \\oplus(V \\otimes V) \\oplus(V \\otimes V \\otimes V) \\oplus \\ldots\n",
        "$\n",
        "\n",
        "* Mit der Multiplikation, die auf den homogenen Bestandteilen durch das Tensorprodukt gegeben ist, wird $\\mathrm{T}(V)$ zu einer $\\mathbb{N}_{0}$ -graduierten, unitären, assoziativen Algebra.\n",
        "\n",
        "* Quotientenräume der Tensoralgebra: Durch Herausteilen eines bestimmten Ideals kann man aus der Tensoralgebra beispielsweise die symmetrische Algebra, die äußere Algebra oder die **Clifford-Algebra** gewinnen. Diese Algebren sind in der Differentialgeometrie von Bedeutung.\n",
        "\n"
      ],
      "metadata": {
        "id": "MERE084982Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Example for Tensors: Einstein Field Equations in General Relativity:**\n",
        "\n",
        "> $\n",
        "R_{\\mu \\nu}-\\frac{1}{2} R g_{\\mu \\nu}+\\Lambda {g_{\\mu v}}=\\frac{8 \\pi G}{c^{4}} T_{\\mu \\nu}\n",
        "$\n",
        "\n",
        "4x4 rank 2 metric tensor: ${g_{\\mu v}}$ (measure lengths and angles in the curved geometry of spacetime)\n",
        "\n",
        "* Gravitation als geometrische Eigenschaft der gekrümmten vierdimensionalen Raumzeit.\n",
        "\n",
        "* Zur Beschreibung der **Raumzeit und ihrer Krümmung bedient man sich der Differentialgeometrie**, die die Euklidische Geometrie des uns vertrauten „flachen“ dreidimensionalen Raumes der klassischen Mechanik umfasst und erweitert.\n",
        "\n",
        "* Die Differentialgeometrie verwendet zur Beschreibung gekrümmter Räume, wie der Raumzeit der ART, sogenannte **Mannigfaltigkeiten**. Wichtige **Eigenschaften werden mit sogenannten Tensoren beschrieben**, die Abbildungen auf der Mannigfaltigkeit darstellen.\n",
        "\n",
        "* Die gekrümmte Raumzeit wird als [Lorentz-Mannigfaltigkeit](https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit) (Pseudo-riemannsche Mannigfaltigkeit) beschrieben.\n",
        "\n",
        "* Eine besondere Bedeutung kommt dem [**metrischen Tensor**](https://de.wikipedia.org/wiki/Metrischer_Tensor) zu. Wenn man in den metrischen Tensor zwei Vektorfelder einsetzt, erhält man für jeden Punkt der Raumzeit eine reelle Zahl.\n",
        "\n",
        "  * In dieser Hinsicht kann man den metrischen Tensor als ein verallgemeinertes, punktabhängiges Skalarprodukt für Vektoren der Raumzeit verstehen.\n",
        "\n",
        "  * Mit seiner Hilfe werden Abstand und Winkel definiert und er wird daher kurz als Metrik bezeichnet.\n",
        "\n",
        "* Ebenso bedeutend ist der [riemannsche Krümmungstensor](https://de.wikipedia.org/wiki/Riemannscher_Krümmungstensor) zur Beschreibung der Krümmung der Mannigfaltigkeit,\n",
        "\n",
        "  * der eine Kombination von ersten und zweiten Ableitungen des metrischen Tensors darstellt.\n",
        "\n",
        "  * Wenn ein beliebiger Tensor in irgendeinem Koordinatensystem in einem Punkt nicht null ist, kann man überhaupt kein Koordinatensystem finden, sodass er in diesem Punkt null wird. Dies gilt dementsprechend auch für den Krümmungstensor.\n",
        "\n",
        "  * Umgekehrt ist der Krümmungstensor in allen Koordinatensystemen null, wenn er in einem Koordinatensystem null ist. Man wird also in jedem Koordinatensystem bezüglich der Frage, ob eine Mannigfaltigkeit an einem bestimmten Punkt gekrümmt ist oder nicht, zum gleichen Ergebnis gelangen.\n",
        "\n",
        "* Die maßgebliche Größe zur Beschreibung von Energie und Impuls der Materie ist der [Energie-Impuls-Tensor](https://de.wikipedia.org/wiki/Energie-Impuls-Tensor). Dieser Tensor bestimmt die Krümmungseigenschaften der Raumzeit. Siehe auch [Vierertensor](https://de.wikipedia.org/wiki/Vierertensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "1xA1Fe1B83_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Example for Tensors: Quantum Mechanics & Quantum Computing**\n",
        "\n",
        "Quantum Superposition:\n",
        "\n",
        "> $\\vec{\\psi}=a \\vec{v}+b \\vec{w}$\n",
        "\n",
        "* linear combination, physical quantum states are just vectors, using linear combinations to give more complicated states\n",
        "\n",
        "\n",
        "Quantum Entanglement\n",
        "\n",
        "> $\\vec{\\psi} \\otimes \\vec{\\phi}$\n",
        "\n",
        "* two states are 'entangled' together means state these state vectors have been combined together using the 'tensor product' (circle X)\n",
        "\n",
        "* Takes the geometrical space where the first system lives and the second system and combines them together to create a more complicated geometrical space, and that's where the entangled system lives"
      ],
      "metadata": {
        "id": "thzMwgWj86A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu können, ist eine Basis nötig. Im n-dimensionalen Raum besteht diese aus n linear unabhängigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur für lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) möglich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabhängigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): Für echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabhängigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekrümmt sind, sind die Koordinatenachsen nicht räumlich fest, wie es für kartesische Koordinaten gilt. Dies führt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abhängt – im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenflächen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgewählten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "oZN7V4P688A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Tangent Space & Kotangentialraum**\n",
        "\n",
        "* **Tangential and Normal Components (Koordinaten)**: Given a vector at a point on a curve (just a vector, not a tangential vector!), that vector can be decomposed uniquely as a sum of two vectors,\n",
        "\n",
        "  * $\\mathbf{v}_{\\|}$ : one tangent to the curve, called the **tangential component** of the vector. $\\mathbf{v}_{\\perp}$  : another one perpendicular to the curve, called the **normal component** of the vector. Zusatzlich gibt es noch: The binormal unit vector B is defined as the cross product of T and N ([Source](https://en.m.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas))\n",
        "\n",
        "  * More formally, let $S$ be a surface, and $x$ be a point on the surface. Let $\\mathbf{v}$\n",
        "be a vector at $x$. Then one can write uniquely $\\mathbf{v}$ as a sum: $\\mathbf{v}=\\mathbf{v}_{\\|}+\\mathbf{v}_{\\perp}$. Similarly a vector at a point on a surface can be broken down the same way.\n",
        "\n",
        "  * More generally, given a submanifold $N$ of a manifold $M,$ and\n",
        "a vector in the tangent space to $M$ at a point of $N$, it can be\n",
        "decomposed into the component tangent to $N$ and the\n",
        "component normal to $N$.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Surface_normal_tangent.svg/248px-Surface_normal_tangent.svg.png)\n",
        "\n",
        "* **Calculate the components**: To calculate the tangential and normal components, consider a [unit normal](https://en.wikipedia.org/wiki/Normal_(geometry)) to the surface, that is, a [unit vector](https://en.wikipedia.org/wiki/Unit_vector) $\\hat{n}$ perpendicular to $S$ at $x$.\n",
        "\n",
        "  * Then, normal component: $\\mathbf{v}_{\\perp}=(\\mathbf{v} \\cdot \\hat{n}) \\hat{n}$ and thus tangential component: $\\mathbf{v}_{\\|}=\\mathbf{V}-\\mathbf{V}_{\\perp}$ where \".\" denotes the [dot product](https://en.wikipedia.org/wiki/Dot_product).\n",
        "\n",
        "  * Another formula for the tangential component is $\n",
        "\\mathbf{v}_{\\|}=-\\hat{n} \\times(\\hat{n} \\times \\mathbf{v})$ where \" $\\times$ \" denotes the [cross product](https://en.wikipedia.org/wiki/Cross_product).\n",
        "\n",
        "  * Note that these formulas do not depend on the particular unit normal $\\hat{n}$ used (there exist two unit normals to any surface at a given point, pointing in opposite directions, so one of the unit normals is the negative of the other one).\n",
        "\n",
        "* Siehe Artikel: [Tangential and normal components](https://en.wikipedia.org/wiki/Tangential_and_normal_components)\n",
        "\n",
        "* **Tangentialvektor**: Sei $\\gamma:(-\\varepsilon, \\varepsilon) \\rightarrow M$ eine differenzierbare Kurve mit $\\gamma(0)=x$ und dem **Kurvenparameter $t$** (siehe 'Parameterdarstellungen' oben), dann ist: $v=\\frac{d \\gamma}{d t}(0) \\in T_{x} M$ ein [Tangentialvektor](https://en.wikipedia.org/wiki/Tangent_vector). Die Tangentialvektoren in einem Punkt $x \\in M$ spannen einen Vektorraum auf, den Tangentialraum $T_{x} M$. Siehe auch [Tangentialbündel](https://de.wikipedia.org/wiki/Tangentialbündel).\n",
        "\n",
        "* **Tangent Space (Tangentialraum)** Ein [Tangentialraum](https://de.wikipedia.org/wiki/Tangentialraum) ist $T_{x} M$ ein Vektorraum, der eine differenzierbare Mannigfaltigkeit $M$ am Punkt $x$ linear approximiert.\n",
        "\n",
        "* **Normal (Vector) Space**: The normal vector space or normal space of a manifold at point P is the **set of vectors which are orthogonal to the tangent space at P**. Normal vectors are of special interest in the case of smooth curves and smooth surfaces.\n",
        "\n",
        "* **Kotangentialraum**: Der [Kotangentialraum](https://de.wikipedia.org/wiki/Kotangentialraum) ist der Dualraum des entsprechenden Tangentialraums. Er ist ein Vektorraum, der einem Punkt einer differenzierbaren Mannigfaltigkeit $M$ zugeordnet wird.\n",
        "\n",
        "  * Sei $M$ eine differenzierbare Mannigfaltigkeit und $T_{p} M$ ihr Tangentialraum am Punkt $p \\in M$. Dann ist der Kotangentialraum definiert als der Dualraum von $T_{p} M$.\n",
        "\n",
        "  * **Das heißt, der Kotangentialraum besteht aus allen Linearformen auf dem Tangentialraum $T_{p} M$**.\n",
        "\n",
        "  * In differential geometry, **one can attach to every point $x$ of a smooth (or differentiable) manifold, $\\mathcal{M},$ a vector space called the cotangent space at $x .$** Typically, the cotangent space, $T_{x}^{*} \\mathcal{M}$ is defined as the dual space of the tangent space at $x, T_{x} \\mathcal{M},$ although there are more direct definitions. The elements of the cotangent space are **called cotangent vectors or tangent covectors**.\n",
        "\n",
        "  * Let $\\mathcal{M}$ be a smooth manifold and let $x$ be a point in $\\mathcal{M}$. Let $T_{x} \\mathcal{M}$ be\n",
        "the tangent space at $x$. Then the cotangent space at $x$ is defined as the dual space of $T_{x} \\mathcal{M}$ : $\n",
        "T_{x}^{*} \\mathcal{M}=\\left(T_{x} \\mathcal{M}\\right)^{*}\n",
        "$\n",
        "\n",
        "  * Concretely, **elements of the cotangent space are linear functionals on $T_{x} \\mathcal{M}$**. That is, **every element $\\alpha \\in T_{x}^{*} \\mathcal{M}$ is a linear map** $\n",
        "\\alpha: T_{x} \\mathcal{M} \\rightarrow F\n",
        "$\n",
        "\n",
        "  * where $F$ is the underlying field of the vector space being considered, for example, the field of real numbers. **The elements of $T_{x}^{*} \\mathcal{M}$ are called cotangent vectors.**\n",
        "\n",
        "*Die [Hauptkrümmungen](https://de.wikipedia.org/wiki/Hauptkr%C3%BCmmung) sind [Eigenwerte](https://de.wikipedia.org/wiki/Eigenwertproblem) der [Weingartenabbildung](https://de.wikipedia.org/wiki/Weingartenabbildung)*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/e/eb/Minimal_surface_curvature_planes-en.svg)"
      ],
      "metadata": {
        "id": "E84nx70p9Db6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Koordinatentransformation (Geometric transformation) & Basiswechsel**\n",
        "\n",
        "* [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation), wenn sich ein Problem in einem anderen Koordinatensystem leichter lösen lässt, z. B. bei der Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten.\n",
        "\n",
        "* [Liste von Transformationen in der Mathematik](https://de.m.wikipedia.org/wiki/Liste_von_Transformationen_in_der_Mathematik) und [Intro: Koordinatentransformation](http://walter.bislins.ch/physik/index.asp?page=Koordinatentransformation). Beispiel: [Drehung](https://de.wikipedia.org/wiki/Drehung) (Rotation), [Skalierung](https://de.wikipedia.org/wiki/Skalar_(Mathematik)), [Scherung](https://de.wikipedia.org/wiki/Scherung_(Geometrie)) & [Verschiebung](https://de.wikipedia.org/wiki/Parallelverschiebung) (Translation). [Affine Transformationen](https://de.wikipedia.org/wiki/Affine_Abbildung) aus linearen Transformation und Translation. Translation ist ein Spezialfall einer affinen Transformation, bei der A die Einheitsmatrix ist.\n",
        "\n",
        "* In der Regel verwendet man spezielle Transformationen, bei denen diese Funktionen gewissen Einschränkungen – z. B. Differenzierbarkeit, Linearität oder Formtreue – unterliegen.\n",
        "\n",
        "* Bei **lineare Transformationen** ([Lineare Abbildung](https://de.wikipedia.org/wiki/Lineare_Abbildung)) sind die neuen Koordinaten lineare Funktionen der ursprünglichen. [Matrixmultiplikation](https://de.wikipedia.org/wiki/Matrizenmultiplikation) des alten Koordinatenvektors $\\vec{x} = (x_1, \\dots, x_n)$ mit der Matrix $A \\rightarrow {\\vec {x}}'=A{\\vec {x}}$.\n",
        "\n",
        "    * $x_{1}^{\\prime}=a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n}$\n",
        "    * $x_{2}^{\\prime}=a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n}$\n",
        "    * $\\cdots$\n",
        "    * $x_{n}^{\\prime}=a_{n 1} x_{1}+a_{n 2} x_{2}+\\cdots+a_{n n} x_{n} .$\n",
        "\n",
        "* **Ein Basiswechsel ist ein Spezialfall einer Koordinatentransformation**: [**Basiswechsel im Vektorraum (Transformationsmatrix)**](https://de.wikipedia.org/wiki/Basiswechsel_(Vektorraum)) (lineare Algebra) ist der Übergang zwischen zwei verschiedenen Basen eines endlichdimensionalen Vektorraums über einem Körper $K$. Beispiele:\n",
        "\n",
        "  * [Koordinatentransformation zwischen Kartesischen Koordinaten und Polarkoordinaten](https://de.m.wikipedia.org/wiki/Koordinatentransformation#Beispiele)\n",
        "\n",
        "  * [Transformation von Differential-Operatoren](https://de.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Differentialen) (Jacobi-Matrix)\n",
        "\n",
        "  * [Transformation von Vektorfeldern](https://de.m.wikipedia.org/wiki/Kugelkoordinaten#Transformation_von_Vektorfeldern_und_-Operatoren)\n",
        "\n",
        "  * [Lorentz-Transformationen](https://de.wikipedia.org/wiki/Lorentz-Transformation) zur Beschreibungen von Phänomenen in verschiedenen Bezugssystemen (Spezielle Relativitätstheorie). Verbinden in vierdimensionaler Raumzeit die Zeit- und Ortskoordinaten, mit denen verschiedene Beobachter angeben, wann und wo Ereignisse stattfinden. Lorentz-Transformationen erhalten Abstände in nichteuklidischer Raumzeit ([Minkowskiraum](https://de.wikipedia.org/wiki/Minkowski-Raum)), Winkel aber nicht, da der Minkowskiraum kein normierter Raum ist.\n",
        "\n",
        "  * [Galilei-Transformation](https://de.wikipedia.org/wiki/Galilei-Transformation): Äquivalent zu Lorentz-Transformationen im dreidimensionalen euklidischen Raum. Anwendbar, wenn sich Bezugssysteme durch geradlinig-gleichförmige Bewegung, Drehung und/oder eine Verschiebung in Raum oder Zeit unterscheiden. Alle Beobachtungen von Strecken, Winkeln und Zeitdifferenzen stimmen in beiden Bezugssystemen überein; alle beobachteten Geschwindigkeiten unterscheiden sich um die konstante Relativgeschwindigkeit der beiden Bezugssysteme.\n",
        "\n",
        "  * [Eichtransformation](https://de.wikipedia.org/wiki/Eichtransformation) verändert die Eichfelder einer physikalischen Theorie (z. B. die elektromagnetischen Potentiale oder die potentielle Energie) dergestalt, dass die physikalisch wirksamen Felder (z. B. das elektromagnetische Feld oder ein Kraftfeld) und damit alle beobachtbaren Abläufe dabei die gleichen bleiben, z. B. die Verschiebung des Nullpunkts der potentiellen Energie, die Wahl des Referenzpotentials bei der Messung elektrischer Spannungen, ein konstanter Phasenfaktor an der komplexen Wellenfunktion der Quantenmechanik.\n",
        "\n",
        "* [Youtube Video 1](https://www.youtube.com/watch?v=CR7e7Zc0QLg) und [Youtube Video 2](https://www.youtube.com/watch?v=FFVauAY_FMI)\n",
        "\n",
        "**Eigenchris Video series:**\n",
        "\n",
        "* [Eigenchris: Tensors For Beginners (-1): Motivation](https://www.youtube.com/watch?v=8ptMTLzV4-I&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1: Forward and Backward Transformations](https://www.youtube.com/watch?v=sdCmW5N1LW4&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=3)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 1.5: Correction on Forward + Backward Transforms](https://www.youtube.com/watch?v=ipRrCPvftTk&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=4)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 2: Vector definition](https://www.youtube.com/watch?v=uPbBDToXjBw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=5)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 3: Vector Transformation Rules](https://www.youtube.com/watch?v=A1h_eucHFW4&t=177s)\n",
        "\n"
      ],
      "metadata": {
        "id": "k7tUmz4d9FUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ as Kronecker Product, of Tensors and of Tensor Spaces*"
      ],
      "metadata": {
        "id": "zTjh6Dkf9Imc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Product** (generalization of the outer product)\n",
        "\n",
        "* the [tensor product V ⊗ W](https://en.m.wikipedia.org/wiki/Tensor_product) of two vector spaces V and W (over the same field) is a vector space which can be thought of as the space of all tensors that can be built from vectors from its constituent spaces using an additional operation which can be **considered as a generalization and abstraction of the outer product**.\n",
        "\n",
        "**Tensor Product: The 2 different tensor product use cases**\n",
        "\n",
        "* the \"little\" tensor product which combines individual tensors\n",
        "\n",
        "* the \"big\" tensor product which combines entire tensor vector spaces\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_62.png)\n",
        "\n",
        "* the Kronecker product, sometimes denoted by ⊗,[1] is an operation on two matrices of arbitrary size resulting in a block matrix.\n",
        "* It is a generalization of the outer product (which is denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis.\n",
        "* The Kronecker product is to be distinguished from the usual matrix multiplication, which is an entirely different operation. The Kronecker product is also sometimes called matrix direct product.\n",
        "* The Kronecker product is a special case of the tensor product, so it is bilinear and associative"
      ],
      "metadata": {
        "id": "JRvzAr6v9RWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Tensor Product $\\otimes$ vs Kronecker Product $\\otimes$**\n",
        "\n",
        "* They are technically different things, but highly related to each other.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 13: Tensor Product vs Kronecker Product](https://www.youtube.com/watch?v=qp_zg_TD0qE&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=16)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_40.png)\n",
        "\n",
        "* **Here are examples on how the tensor products works:**\n",
        "\n",
        "* Basis for Vector Space $V$\n",
        "\n",
        "> $\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}} \\in V$\n",
        "\n",
        "Basis for Dual Space $V*$\n",
        "\n",
        "> $\\epsilon^{1}, \\epsilon^{2} \\in V^{*}$\n",
        "\n",
        "* And the covectors are linear functions that are defined by these rules here, where the covector $\\epsilon^{i}$ is acting on the vector $\\overrightarrow{e_{j}}$ gives the Kronecker delta $\\delta_{j}^{i}$ as the result (=we get 1 if i and j are the same and 0 if not).\n",
        "\n",
        "> $\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{j}^{i}=\\left\\{\\begin{array}{l}1, i=j \\\\ 0, i \\neq j\\end{array}\\right.$\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)\n",
        "\n",
        "**To demonstrate that this is a linear map, we can pass in an input vector $\\overrightarrow{v}$**\n",
        "\n",
        "To get the output we can just we pass $v$ to the covector $\\epsilon$ first\n",
        "\n",
        "> $\\left(\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}\\right)(\\vec{v})$\n",
        "\n",
        "Pass v to the covector:\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}(\\vec{v})\\right)$\n",
        "\n",
        "Then we expand v as a linear combination of the basis vectors\n",
        "\n",
        "> $=\\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "The we bring the components outside since covectors are linear functions (we can scale before or after):\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\otimes\\left(\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)\\right)$\n",
        "\n",
        "And the covector acting on a vector becomes a Kronecker delta:\n",
        "\n",
        "> $=v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "Then Kronecker index cancellation rule we can remove the $k$ indexes and get j:\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "So this is a function that takes a vector input and produces a vector output\n",
        "\n",
        "\n",
        "**Here are examples on how the Kronecker products works:**\n",
        "\n",
        "* Here you get a row of columns when you take the first array on the left and distribute every element to the second array on the right:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_46.png)\n",
        "\n",
        ".. and multiplying this with another column vector we get the same thing (here you get a column of rows of columns)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_47.png)\n",
        "\n",
        "\n",
        "**Summary**\n",
        "\n",
        "> $=v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "* these coefficient are the entries of a matrix (on the tensor product side)\n",
        "\n",
        "* on the Kronecker delta side you can see this matrix !\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_48.png)\n",
        "\n",
        "* Tensor product and Kronecker product are doing basically the same kind of thing\n",
        "\n",
        "* It’s just the tensor product is combining the abstract vector and the abstract covector in the land of algebraic symbols\n",
        "\n",
        "* And the Kronecker product is combining the the vector array and the covector array  in the land of arrays\n",
        "\n",
        "* But the components that we get from the tensor are just the components of the matrix that we get from Kronecker product\n",
        "\n",
        "* So they are sort of the same operation they just do the work in different contexts\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_49.png)"
      ],
      "metadata": {
        "id": "kz-n4Xvc9TFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 1*\n",
        "\n",
        "We can see a lot of similarities between:\n",
        "* the Kronecker product for matrices and\n",
        "* the tensor product for abstract Hilbert spaces.\n",
        "\n",
        "* If a vector can be expressed as the tensor product of two vectors, we call it a product vector, i.e., $|\\Psi\\rangle$ **is a product vector** if $|\\Psi\\rangle=|\\phi \\otimes \\chi\\rangle$ for some $|\\phi\\rangle$ and $|\\chi\\rangle$. Any nonproduct vector is called entangled. The entangled states of a composite system occupy a distinguished position in the interpretation of quantum mechanics.\n",
        "\n",
        "* We would like to define a <font color=\"blue\">**tensor product for abstract Hilbert spaces**</font>.\n",
        "\n",
        "* The main need for this comes from the necessity of <font color=\"blue\">**describing composite quantum systems**</font>.\n",
        "\n",
        "* If we have two different systems $\\mathrm{A}$ and $\\mathrm{B}$, then **we have separate Hilbert spaces**, say $\\mathcal{H}_{A}$ and $\\mathcal{H}_{B}$, for describing the quantum states of these systems.\n",
        "\n",
        "* Basically a vector $|\\alpha\\rangle_{A}$ in $\\mathcal{H}_{A}$ gives a state of $\\mathrm{A}$ and a vector $|\\beta\\rangle_{B}$ in $\\mathcal{H}_{B}$ gives a state of $\\mathrm{B}$.\n",
        "\n",
        "* In a similar way, we should <font color=\"blue\">**have an entirely different Hilbert space** $\\mathcal{H}_{A B}$ for describing the states of the composite system AB.</font>\n",
        "\n",
        "* In particular, we would like to have a vector in $\\mathcal{H}_{A B}$ that describes the state of $\\mathrm{AB}$ such that $\\mathrm{A}$ is in state $\\mid \\alpha \\rangle_{A}$ and $\\mathrm{B}$ is in state $|\\beta\\rangle_{B}$. We will denote this state as $|\\alpha\\rangle_{A} \\otimes|\\beta\\rangle_{B}$.\n",
        "\n",
        "* We will also want to have this product $\\otimes$ be such that superpositions of states in $\\mathrm{A}$ or in B could also be equivalently described as superpositions of states in $\\mathrm{AB} ;$ hence **distributivity**.\n",
        "\n",
        "http://www.physics.metu.edu.tr/~sturgut/p455/qm-math4.pdf"
      ],
      "metadata": {
        "id": "L_0hUsHx9Wtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 2*\n",
        "\n",
        "* **The two notions represent operations on different objects: Kronecker product on matrices; tensor product on linear maps between vector spaces**.\n",
        "\n",
        "* But there is a connection: Given two matrices, we can think of them as representing linear maps between vector spaces equipped with a chosen basis.\n",
        "\n",
        "* The Kronecker product of the two matrices then represents the tensor product of the two linear maps.\n",
        "\n",
        "* (This claim makes sense because the tensor product of two vector spaces with distinguished bases comes with a distinguish basis.)\n",
        "\n",
        "https://math.stackexchange.com/questions/203947/tensor-product-and-kronecker-product"
      ],
      "metadata": {
        "id": "o08kUOZC9Ym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 3*\n",
        "\n",
        "Sometimes the Kronecker product is also called direct product\n",
        "or tensor product.\n",
        "\n",
        "https://www.worldscientific.com/doi/pdf/10.1142/9789811202520_0002"
      ],
      "metadata": {
        "id": "SdLDqnuT9aid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 4*\n",
        "\n",
        "* |u> <v| is a way of writing the <font color=\"blue\">**tensor product of a vector and a dual vector**</font> (ie, an element of the Hilbert space and an element of its dual, which is usually casually identified with the Hilbert space using the inner product).\n",
        "\n",
        "* This is a linear operator on the Hilbert space, sending |w > to <v|w>|u>.\n",
        "\n",
        "* In general, the tensor product of a vector space and its dual is the space of (finite rank) linear operators on the vector space.\n",
        "\n",
        "* On the other hand, |u>|v> is an element of the tensor product of the vector space with itself, usually used in physics for describing a composite of two identical systems. Again, since there is an isomorphism between the vector space and its dual, there is one between the space of composite states and the space of linear operators. This is interesting, but I've never seen this put to good use.\n",
        "\n",
        "* Finally, **the Kronecker product is just a particular representation of the tensor product, convenient for dealing with tensor products of linear operators**.\n",
        "\n",
        "Source https://www.physicsforums.com/threads/difference-between-the-outer-product.236244/"
      ],
      "metadata": {
        "id": "Xim2BtEb9cXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation 5*\n",
        "\n",
        "* the Kronecker product and tensor product essentially have the same mathematical \"actions\" (an expanded matrix with dimensions equal to the product of the two matrices), but the tensor product explicitly applies only to matrices representing linear maps.\n",
        "\n",
        "* Every matrix represents a linear map.\n",
        "\n",
        "* Linear maps are in some sense more general than matrices. They are also different (types of) objects, even though matrices can be used to represent _some_ linear maps. The matrix representation of a particular linear map depends on the choice of basis of the domain and target space, for example, whereas the linear map itself is invariant under such choices. On the other hand a matrix can also represent a bilinear form, not just a linear map. Consider reading Axler's _Linear Algebra Done Right_ for a good explanation of some of such subtleties.\n",
        "\n",
        "https://math.stackexchange.com//questions/203947"
      ],
      "metadata": {
        "id": "iGV-6fNH9eGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covariance (Covector) and Contravariance (Vector): Tensor Notation for Vectors & Covectors in Coordinate Systems*"
      ],
      "metadata": {
        "id": "icJDI9EQ9gc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of a Tensor**\n",
        "\n",
        "> **Tensor = an object that is invariant under a change of coordinates, and... has components that change in a special, predictable way under a change of coordinates**\n",
        "\n",
        "* the direction and the lengths of an object are invariant (=vector)\n",
        "\n",
        "> **The vector components are NOT invariant. They change  depending on the coordinate system they use.**\n",
        "\n",
        "* if you know how to switch between coordinate system, you should be able to know how to switch between vector components (forward and backward)\n",
        "\n",
        "> **Tensor: A collection of vectors and convectors combined together using the tensor product.** (abstract definition)\n",
        "\n",
        "> **Tensors as partial derivatives and gradients that transform with the Jacobian matrix.**\n",
        "\n",
        "* Tensoren sind Grössen, mit deren Hilfe man Skalare, Vektoren und weitere Grössen analoger Struktur in ein einheitliches Schema zur Beschreibung mathematischer und physikalischer Zusammenhänge einordnen kann.\n",
        "\n",
        "* Sie sind definiert durch ihre Transformationseigenschaften gegenüber orthogonalen Transformationen wie z.B. Drehungen. Es geht darum, was ändert sich, was ändert sich nicht, wenn man das Bezugssystem ändert?\n",
        "\n",
        "* Die Besonderheit von Tensorgleichungen ist, dass sie transformations-invariant sind. Wenn es gelingt, einen physikalischen Sachverhalt in Tensorschreibweise zu formulieren, so kann man wegen der speziellen Art, wie Tensoren transformieren, sicher sein, dass die Gleichungen in jedem beliebigen Koordinatensystem gelten.\n",
        "\n",
        "* **Rang (oder Stufe) eines Tensors und Indizes**: Tensoren haben **Indizes**. Die Anzahl der Indizes gibt den Rang oder die Stufe des Tensors an. Die Indizes laufen entsprechend der **Dimension** $D$ des Raumes über $1 . . D$. Bei der 4-dimensionalen Raumzeit beginnt die Nummerierung bei $0,$ wobei der Index 0 die zeitliche Komponente betrifft. Beim Arbeiten mit den Tensoren muss die **Reihenfolge der Indices** immer klar sein. Das Element $t_{12}$ eines Tensors ist in der Regel vom Element $t_{21}$ verschieden.\n",
        "\n",
        "  * Der einfachste Tensor ist ein Tensor mit Rang 0 . Dabei handelt es sich einfach um einen **Skalar**. Ein Skalar hat eigentlich keine Komponenten, sondern ist nur ein einzelner Wert und benötigt somit keinen Index; daher der Rang $0 .$\n",
        "\n",
        "  * Ein Tensor mit nur einem Index nennt man auch **Vektor**. Man sagt, der Vektor ist ein Tensor mit dem Rang 1. Der Index hat so viele Werte, wie die Dimension des Vektors. Bei einem 3 -dimensionalen Vektor hat der Index also 3 Werte (z.B: 1,2,3$),$ weil der Vektor 3 Komponenten hat: $\\vec{V}=\\left(V^{1}, V^{2}, V^{3}\\right)$.\n",
        "\n",
        "  * Ein Tensor vom Rang 2 hat 2 Indizes und stellt eine quadratische **Matrix** dar usw.\n",
        "\n",
        "  * Jede Tensor-Komponente kann eine Funktion oder eine Zahl sein. In der AR (allgemeinen Relativitatstheorie) sind die Komponenten in der Regel Funktionen der Raumzeit.\n",
        "\n",
        "* **Dimension eines Tensors**: Ein Vektor oder Tensor ist ein Objekt, welches Komponenten hat. Die Anzahl der Komponenten eines Vektors enstpricht der Dimension D des Raumes. Ein 3-dimensionaler Vektor besteht somit aus 3 Komponenten. Ein 3-dimensionaler Tensor der Stufe 2 besteht aus 3x3 Komponenten usw.\n",
        "\n",
        "> **Typ eines Tensors**: Wenn **Rang, Dimension und Komponenten-Arten** (Anzahl Indizes oben und unten) von Tensoren übereinstimmen, dann sagt man, die Tensoren sind vom selben Typ. Dies muss bei der Tensor-Arithmetik beachtet werden.\n",
        "\n",
        "* **Tensor-Arithmetik**: Weil Skalare und Vektoren Subklassen von Tensoren sind ist zu erwarten, dass Tensoren denselben bekannten Rechenregeln für Addition, Subtraktion, Multiplikation und Division folgen.  Dies stimmt meistens, jedoch mit einigen Änderungen und Einschränkungen. Tensoren zeigen zudem neue Eigenschaften, die es bei Skalaren und Vektoren nicht gibt. [Source](http://walter.bislins.ch/physik/index.asp?page=Tensor%2DArithmetik)\n",
        "\n",
        "* **Tensor vs. Matrix**: A tensor of rank n is a mathematical object that has n indices and m$^n$ components. **Matrix is a tensor rank 2, because you need 2 basis vectors. Tensors rank 3 are the boxes with several stacked matrices**.\n",
        "\n",
        "  * Matrix is just and array of numbers, meanwhile a tensor (like stress tensor) obeys specific transformation rules and has a rules physical meaning. We can use a matrix to represent a tensor, but a tensor has a deeper physical significance. Matrix: I can just write down the matrix and its elements, but don’t have to add anything else\n",
        "\n",
        "  * Tensor: I need to specify the coordinate system, the components, and the basis vectors that each of those components correspond to. A tensor requires more detailed specification than a matrix. And it has these transformation properties where it’s invariant under a change of coordinate system, it has physical significance.\n",
        "\n",
        "* See video: [Introduction to Tensors](https://www.youtube.com/watch?v=uaQeXi4E7gA&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=1&t=187s)"
      ],
      "metadata": {
        "id": "icw4h44B9uTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensoren und Koordinatensysteme**\n",
        "\n",
        "* In conventional math syntax we make **use of covariant indexes when dealing with Cartesian coordinate systems** $(x_{1},x_{2},x_{3})$ frequently without realizing this is a limited use of tensor syntax as covariant indexed components.\n",
        "\n",
        "* [Curvilinear coordinate systems](https://en.wikipedia.org/wiki/Curvilinear_coordinates), such as **cylindrical or spherical coordinates**, are often used in physical and geometric problems. Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and **covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another**.\n",
        "\n",
        "* In der Anwendung steht das Tensorsymbol immer für eine bestimmte Bedeutung. Zum Beispiel den Ort eines Teilchens. Der Ort kann in verschiedenen Koordinatensystemen gemessen werden. Entsprechend haben die Komponenten des Tensors in jedem Koordinatensystem andere Werte. Doch der Tensor bleibt derselbe (zB Länge eines Vektors), egal in welchem Koordinatensystem er gemessen wird.\n",
        "\n",
        "* **Gradient Vector**: Tensor calculus provides a generalization to the gradient vector formula from standard calculus **that works in all coordinate systems**: $\\nabla F=\\nabla_{i} F \\vec{Z}^{i}$ where: $\\nabla_{i} F=\\frac{\\partial F}{\\partial Z^{i}}$. In contrast, for standard calculus, the gradient vector formula is dependent on the coordinate system in use (example: Cartesian gradient vector formula vs. the polar gradient vector formula vs. the spherical gradient vector formula, etc.). In standard calculus, each coordinate system has its own specific formula, unlike **tensor calculus that has only one gradient formula that is equivalent for all coordinate systems**. This is made possible by an understanding of the metric tensor that tensor calculus makes use of."
      ],
      "metadata": {
        "id": "XavAxkWY9wMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covariance and Contravariance**\n",
        "\n",
        "> In multilinear algebra and tensor analysis, [covariance and contravariance](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors) describe how the quantitative description of certain geometric or physical entities changes with a [change of basis](https://en.wikipedia.org/wiki/Change_of_basis).\n",
        "\n",
        "* **The reason of having covariant or contravariant tensors is because you want to represent the same thing in a different coordinate system. Such a new representation is achieved by a transformation using a set of partial derivatives. In tensor analysis, a good transformation is one that leaves invariant the quantity you are interested in.**\n",
        "\n",
        "* Man unterscheidet in der [Tensor notation](https://en.wikipedia.org/wiki/Tensor_calculus#Syntax) bei Tensoren zwei Arten von Komponenten (*Jeder Tensor kann sowohl in kontravarianten, als auch in kovarianten Komponenten dargestellt werden*):\n",
        "\n",
        "  * **contravariant** upper component index: $v^{i}$\n",
        "\n",
        "  * **covariant** lower component index: $v_{i}$\n",
        "\n",
        "* [**Mixed Tensors**](https://en.wikipedia.org/wiki/Mixed_tensor) with both covariant and contravariant (for Tensors rang 2 or higher): $T^{m}{ }_{n}$. Consider related tensors: $T_{\\alpha \\beta \\gamma}, T_{\\alpha \\beta}^{\\gamma}, T_{\\alpha}{ }^{\\beta}{ }_{\\gamma}, T_{\\alpha}{ }^{\\beta \\gamma}, T_{\\beta \\gamma}^{\\alpha}, T^{\\alpha}{ }_{\\beta}^{\\gamma}, T^{\\alpha \\beta}{ }_{\\gamma}, T^{\\alpha \\beta \\gamma}$. The first one is covariant, the last one contravariant, and the remaining ones mixed.\n",
        "\n",
        "  * A mixed tensor of type or valence $\\left(\\begin{array}{l}M \\\\ N\\end{array}\\right),$ also written \"type $(M, N)$\" which has $M$ contravariant indices and $N$ covariant indices can be defined as a linear function which maps an $(M+N)$ -tuple of $M$ one-forms and $N$ vectors to a scalar.\n",
        "\n",
        "  * a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the [tangent bundle](https://en.wikipedia.org/wiki/Tangent_bundle) as well as the [cotangent bundle](https://en.wikipedia.org/wiki/Cotangent_bundle)\n",
        "\n",
        "* **Pushforwards (covariant) & Pullbacks (contravariant)**:\n",
        "\n",
        "  * Vectors with upper-index objects have [pushforwards](https://de.m.wikipedia.org/wiki/Pushforward), which are covariant.\n",
        "\n",
        "  * Covectors with lower-index objects have [pullbacks](https://de.m.wikipedia.org/wiki/Rücktransport), which are contravariant.\n",
        "\n",
        "  * In category theory covariance and contravariance are properties of [functors](https://en.m.wikipedia.org/wiki/Functor). Sometimes, contravariant functors are called [\"cofunctors\"](https://en.m.wikipedia.org/wiki/Functor).\n",
        "\n",
        "* Der Wechsel von einem Tensor in kontravarianter Darstellung zu einem Tensor in kovarianter Darstellung, wird [**\"Index ziehen\" oder Index Manipulation**](http://walter.bislins.ch/physik/index.asp?page=Index%2DManipulation+per+Metrik%2DTensor) genannt. Die Umrechnung geht durch Multiplikation mit dem sog. **Metrischen Tensor**.\n",
        "\n",
        "  * A given **contravariant index of a tensor can be lowered using the [metric tensor](https://en.wikipedia.org/wiki/Metric_tensor)** $g_{\\mu v}$ and a given covariant index can be raised using the inverse metric tensor $g^{\\mu v}$. Thus, $g_{\\mu v}$ could be called the index lowering operator and $g^{\\mu v}$ the index raising operator.\n",
        "\n",
        "* On a manifold, a tensor field will typically have multiple, upper and lower indices, where [Einstein notation](https://en.m.wikipedia.org/wiki/Einstein_notation) is widely used. * Tensors notation allows a vector $(\\vec{V})$ to be decomposed into an [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation) representing the [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) of a [basis vector](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) $\\left(\\vec{Z}_{i}\\right.$ or $\\left.\\vec{Z}^{i}\\right)$ with a component vector $\\left(V_{i}\\right.$ or $\\left.V^{i}\\right)$: $\\vec{V}=V^{i} \\vec{Z}_{i}=V_{i} \\vec{Z}^{i}$.\n",
        "\n",
        "* *Exkurs: In einem engeren Wortsinn bezeichnet kovariant in der mathematischen Physik Größen, **die wie Differentialformen transformieren**. Diese kovarianten Größen $P$ bilden einen Vektorraum $\\mathcal{V}$, auf dem eine Gruppe von linearen Transformationen wirkt. Die Menge der linearen Abbildungen der kovarianten Größen in die reellen Zahlen $Q: P \\mapsto Q(P) \\in \\mathbb{R}, \\quad Q(a P+b \\tilde{P})=a Q(P)+b Q(\\tilde{P})$ bildet den zu $\\mathcal{V}$ dualen Vektorraum $\\mathcal{V}^{*}$. Schreiben wir die transformierten, kovarianten Größen $P^{\\prime}$ mit einer Matrix $\\Lambda$ als $P^{\\prime}=\\Lambda P$ dann definiert $Q^{\\prime}\\left(P^{\\prime}\\right)=Q(P)$ das kontravariante oder kontragrediente Transformationsgesetz des Dualraumes $Q^{\\prime}=\\Lambda^{-1 \\mathrm{~T}} Q$. Wegen $\\left(\\Lambda_{2}\\right)^{-1 \\mathrm{~T}}\\left(\\Lambda_{1}\\right)^{-1 \\mathrm{~T}}=\\left(\\Lambda_{2} \\Lambda_{1}\\right)^{-1 \\mathrm{~T}}$ genügt die kontravariante Transformation derselben Gruppenverknüpfung wie die kovariante Transformation.*\n",
        "\n",
        "* Quellen: [Ko- und kontravariante Darstellung](https://www.math.tugraz.at/~ganster/lv_vektoranalysis_ss_10/20_ko-_und_kontravariante_darstellung.pdf), [Kovarianz und Kontravarianz von Vektoren](http://walter.bislins.ch/physik/index.asp?page=Kovarianz+und+Kontravarianz+von+Vektoren), [Kovariante und Kontravariante Komponenten](http://walter.bislins.ch/physik/index.asp?page=Kovariante+und+Kontravariante+Komponenten). Siehe auch Video: [Tensorrechnung, 3.1 : Was bedeutet Kovariant und kontravariant?](https://www.youtube.com/watch?v=PNRoBOzije8) und [Kovarianz, Kontravarianz, Tensoren](https://av.tib.eu/media/19916)\n"
      ],
      "metadata": {
        "id": "g1CsjpEU90rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ein beliebiger Vektor kann als Linearkombination entweder der kovarianten Basisvektoren oder der kontravarianten Basisvektoren dargestellt werden** (die Transformationen invers zueinander).\n",
        "\n",
        "* **Kovariante Basisvektoren** $\\vec{b}_{u}$ sind kombiniert mit **kontravariante Koordinaten** $a_{u_{i}}$. Heisst auch kontravarianter Vektor (besser: kontravarianter Koordinatenvektor) und sind Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "* **Kontravariante Basisvektoren** $\\vec{b}_{u_{i}}^{*}$ kombiniert mit **kovarianten Koordinaten** $a_{u_{i}}^{*}$. Heisst auch kovarianter Vektor bzw. Covector (Kovektoren als Linearformen von Normalenvektoren) und sind normal zu den Koordinatenflächen.\n",
        "\n",
        "* **Vektordarstellungen**: $\\vec{a}=\\sum_{i=1}^{n} a_{u_{i}} \\vec{b}_{u_{i}}=\\sum_{i=1}^{n} a_{u_{i}}^{*} \\vec{b}_{u_{i}}^{*}$\n",
        "\n",
        "* Wenn $\\vec{g}_{i} \\cdot \\vec{g}^{j}=\\delta_{i}^{j}$ (**Kronecker Delta / Skalarprodukt ist Null**) dann heissen $\\left\\{\\overrightarrow{g_{i}}\\right\\}$ und $\\left\\{\\overrightarrow{g^{j}}\\right\\}$ **reziprok** zueinander. Die beiden Klassen von Basisvektoren sind dual bzw. reziprok zueinander (siehe [duale Basis](https://de.wikipedia.org/wiki/Duale_Basis)). Diese beiden Basen bezeichnet man als **holonome Basen**.\n",
        "\n",
        "* Diese kreuzweise Paarung (kontra-ko bzw. ko-kontra) sorgt dafür, dass der Vektor $\\vec{a}$ unter Koordinatentransformation invariant ist (zB Geschwindigkeit eines Teilchens), da die Transformationen von Koordinaten und Basisvektoren invers zueinander sind und sich gegenseitig aufheben."
      ],
      "metadata": {
        "id": "51tAOvER93PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Vectors): Basiswechsel und Vector-(Coordinate)-Transformation (Contravariant)*"
      ],
      "metadata": {
        "id": "VkuVX0zf959A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vektorkoordinaten verhalten sich kontravariant**\n",
        "\n",
        "* **Problem**: For example, we consider the transformation from one coordinate system $x^{1}, \\ldots, x^{n}$ to another $x^{\\prime}, \\ldots, x^{\\prime n}$\n",
        "$x^{i}=f^{i}\\left(x^{\\prime}, x^{\\prime 2}, \\ldots, x^{\\prime n}\\right)$ where $f^{i}$ are certain functions.\n",
        "Take a look at a couple of specific quantities. How do we transform coordinates? The answer is \"**coordinate differentials**\":\n",
        "\n",
        "> $d x^{i}=\\frac{\\partial x^{i}}{\\partial x^{\\prime k}} d x^{\\prime k}$\n",
        "\n",
        "* **Darstellung**: $c^{1}$ (upper index, hochgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Vektorkoordinaten verhalten sich kontravariant. (wahrscheinlich: kovariante Basisvektoren (= \"kontravarianten Vektor\", weil Koordinaten kontravariant): Tangential an die Koordinatenlinien, d. h. kollinear zu den Koordinatenachsen (da, wie oben beschrieben, die Koordinatenachsen als Tangenten an die Koordinatenlinien definiert sind).\n",
        "\n",
        "  * Every quantity which under a transformation of (vector) coordinates, transforms like the coordinate differentials is called a contravariant tensor.\n",
        "\n",
        "  * **Vectors exhibit a behavior of changing scale inversely to changes in scale to the reference axes are called contravariant** (see second example below). As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).\n",
        "\n",
        "* **Examples**: (of contravariant vectors / vectors with contravariant components)\n",
        "\n",
        "  * position of an object relative to an observer, or any derivative of position with respect to time: velocity, acceleration, jerk, displacement, momentum, force.\n",
        "\n",
        "  * For instance, by changing scale from meters to centimeters (that is, **dividing the scale of the reference axes by 100**), the components of a measured velocity vector are **multiplied by 100**.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * In physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem.\n",
        "\n",
        "  * A contravariant vector or tangent vector (often abbreviated simply as vector, such as a direction vector or velocity vector) has components that contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be contravariant.\n",
        "\n",
        "  * If the **reference axes** were rotated in one direction, the **component representation** of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way. In Einstein notation, contravariant components are denoted with upper indices as in $\\mathbf{v}=v^{i} \\mathbf{e}_{i}$ (note: implicit summation over index \"i\")\n",
        "\n",
        "https://math.stackexchange.com/questions/8170/intuitive-way-to-understand-covariance-and-contravariance-in-tensor-algebra\n",
        "\n",
        "*Kontravariantes Transformationsverhalten*\n",
        "\n",
        "* Grundfrage: Wie werden Normalenvektoren in den alten Basisvektoren zu Normalenvektoren in neuen Basivektoren?\n",
        "\n",
        "* Die normalen Vektoren gehen gegen die Basis, d.h. kontravariant\n",
        "\n",
        "* Gegeben einen Normalenvektor a aus dem Vektorraum V, und diesen zerlegen in die Basisvektoren in alter Basis und dann transformieren in neue Basis:\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_03.png)\n",
        "\n",
        "Nun Kovektor angewandt auf Summe von Zahlen mal Vektoren (Vektoren sind kontravariant):\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_04.png)\n",
        "\n",
        "> **Man nennt a einen \"kontravarianten\" Vektor, aber eigentlich muss man sagen, dass seine Komponenten kontravariant transformieren. Die eigentlichen Vektoren aus dem Originalvektorraum sind kontravariant.**\n",
        "\n",
        "\n",
        "* and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates\n",
        "\n",
        "* The inverse of a covariant transformation is a **contravariant transformation**.\n",
        "\n",
        "  * Whenever a vector should be invariant under a change of basis, that is to say **it should represent the same geometrical or physical object <u>having the same magnitude and direction as before</u>, its components must transform according to the contravariant rule**.\n",
        "\n",
        "  * Conventionally, indices identifying the components of a vector are placed as upper indices and so are all indices of entities that transform in the same way.\n",
        "\n",
        "* The sum over pairwise matching indices of a product with the same lower and upper indices are invariant under a transformation.\n",
        "\n",
        "* A vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector $\\mathbf{v}$ is given, say, in components $V^{\\prime}$ on a chosen basis $\\mathbf{e}_{i}$. On another basis, say $\\mathbf{e}^{\\prime}{ }_{j}$, the same vector $\\mathbf{v}$ has different components $v^{\\prime j}$ and\n",
        "\n",
        "> $\n",
        "\\mathbf{v}=\\sum_{i} v^{i} \\mathbf{e}_{i}=\\sum_{j} v^{\\prime j} \\mathbf{e}_{j}^{\\prime}\n",
        "$\n",
        "\n",
        "* As a vector, $\\mathbf{v}$ should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its \"real world\" direction and magnitude should appear the same regardless of the basis vectors.\n",
        "\n",
        "* If we perform a change of basis by transforming the vectors $\\mathbf{e}_{i}$ into the basis vectors $\\mathbf{e}_{j}$, we must also ensure that the components $v^{i}$ transform into the new components $v$ to compensate.\n",
        "\n",
        "* The needed transformation of $\\mathbf{v}$ is called the contravariant\n",
        "transformation rule."
      ],
      "metadata": {
        "id": "4bkcyHaG-NV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation: Forward and Backward Transform between Basis Vectors in 2 dimensions**\n",
        "\n",
        "*Forward: build the new basis vectors (not any vector!) from the old basis vectors*:\n",
        "\n",
        "$\n",
        "\\begin{array}{l}\n",
        "\\text { Old Basis: }\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\\\\n",
        "\\text { New Basis: }\\left\\{\\widetilde{e_{1}}, \\widetilde{e_{2}}\\right\\}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_08.png)\n",
        "\n",
        "* Forward transformation (create new basis vectors):\n",
        "\n",
        "$\n",
        "\\begin{array}{ll}\n",
        "\\widetilde{e_{1}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}= & \\overrightarrow{e_{1}}+\\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Which gives us for the image above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}} \\\\\n",
        "\\widetilde{e_{2}}=-1 / 2 \\overrightarrow{e_{1}}+1 / 4 \\overrightarrow{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Result is in the forward matrix (Notice how the coefficient from the first equation end up in the first column (2 and 1), and how the coefficients from the second equation end up in the second column)\n",
        "\n",
        "$\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\overrightarrow{e_{1}}=1 / 4 \\widetilde{e_{1}}+(-1) \\widetilde{\\overrightarrow{e_{2}}} \\\\\n",
        "\\overrightarrow{e_{2}}=1 / 2 \\widetilde{e_{1}}+2 \\widetilde{e_{2}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Which gives as the backward matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in 2 dimensions?**\n",
        "\n",
        "* How do forward and backward matrices relate to each other?\n",
        "\n",
        "* They are inverses, and matrix multiplication (dot product) leads to the identity matrix:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "F B &=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "(2*1/4)+(-1/2*-1) & (2*1/2)+(-1/2*2) \\\\\n",
        "(1*1/4)+(1/4*-1) & (1*1/2)+(1/4*2)\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "> This means: $\n",
        "B=F^{-1}\n",
        "$\n",
        "\n",
        "*Reminder: Matrix Multiplication Rules:*\n",
        "\n",
        "$\\begin{aligned}\n",
        "&=\\left[\\begin{array}{cc}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "e & f \\\\\n",
        "g & h\n",
        "\\end{array}\\right]\n",
        "\\\\\n",
        "&=\\left[\\begin{array}{ll}\n",
        "ae+bg & af+bh \\\\\n",
        "ce+dg & cf+dh\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Summary of two transformation matrices between old basis and new basis:**\n",
        "\n",
        "> Forward Matrix: $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & -1 / 2 \\\\\n",
        "1 & 1 / 4\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> Backward Matrix: $\\begin{equation}\n",
        "B=\\left[\\begin{array}{cc}\n",
        "1 / 4 & 1 / 2 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_09.png)"
      ],
      "metadata": {
        "id": "3Xgqkmin-PcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and Backward Transform to n dimensions (Generalization)**\n",
        "\n",
        "First construct the new basis vectors from the old ones with right coefficients:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "\\widetilde{e_{1}}=F_{11} \\overrightarrow{e_{1}}+F_{21} \\overrightarrow{e_{2}}+\\cdots+F_{n 1} \\overrightarrow{e_{n}} \\\\\n",
        "\\widetilde{e_{2}}={F_{12} \\overrightarrow{e_{1}}+F_{22} \\overrightarrow{e_{2}}+\\cdots+F_{n 2} \\overrightarrow{e_{n}}}\\\\\n",
        "{\\ldots} \\\\\n",
        "\\widetilde{e_{n}}=F_{1 n} \\overrightarrow{e_{1}}+F_{2 n} \\overrightarrow{e_{2}}+\\cdots+F_{n n} \\overrightarrow{e_{n}}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $F$ (notice again **how it's transposed to the coefficients above**!)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "F_{11} & F_{12} & \\ldots & F_{1 n} \\\\\n",
        "F_{21} & F_{22} & \\ldots & F_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "F_{n 1} & F_{n 2} & \\ldots & F_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "**How to facilitate it without writing all these equations above?**\n",
        "\n",
        "Let’s take an example: in the following equation taken from the table above (not the coefficient matrix, because there the values are transposed!):\n",
        "\n",
        "$F_{12}$ tells us how much of $\\overrightarrow{e_{1}}$ is in $\\widetilde{e_{2}}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{e_{2}}=F_{12} \\overrightarrow{e_{1}}\n",
        "\\end{equation}$\n",
        "\n",
        "**So what I want is $\\sum F_{k j}$ that tells us how much of $\\overrightarrow{e_{k}}$ makes up $\\widetilde{{e}_{j}}$**:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Exkurs: later we will use the Einstein notation and drop the summation sign to make it easier:**\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        " Now we do the same for the backward transformation:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=B_{11} \\widetilde{e_{1}}+B_{21} \\widetilde{e_{2}}+\\cdots+B_{n 1} \\widetilde{e_{n}} \\\\\n",
        "\\overrightarrow{e_{2}} &=B_{1 2} \\widetilde{e_{1}}+B_{22} \\widetilde{e_{2}}+\\cdots+B_{n 2} \\widetilde{e_{n}} \\\\\n",
        "{\\ldots} \\\\\n",
        "\\overrightarrow{e_{n}} & =B_{1 n} \\widetilde{e_{1}}+B_{2 n} \\widetilde{e_{2}}+\\cdots+B_{n n} \\widetilde{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "Write it as a n x n coefficient matrix $B$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left[\\begin{array}{cccc}\n",
        "B_{11} & B_{12} & \\ldots & B_{1 n} \\\\\n",
        "B_{21} & B_{22} & \\ldots & B_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "B_{n 1} & B_{n 2} & \\ldots & B_{n n}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "Summarize the backward transformation as well:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Now both together forward and backward transformation:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "* The index that we do the summation over (here: k = 1 and j = 1 under the summation sign) is the first letter of the forward or backward transform and **corresponds to the index of the basis vector that we’re transforming**.\n",
        "\n",
        "* The second index of the transform (j and i at F, B and e) **corresponds to the output basis vector**\n",
        "\n",
        "**How are Forward and Backward Transformation related to each other in n dimensions?**\n",
        "\n",
        "How are they related to each other? (once again proving)\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "Replacing with following at the last term above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\stackrel{\\sim}{e_{j}}=\\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "And you get:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{j=1}^{n} B_{j i} \\sum_{k=1}^{n} F_{k j} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging summation signs:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k}\\left(\\sum_{j} F_{k j} B_{j i}\\right) \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "So now we build the old basis vectors $\\overrightarrow{e_{i}}$ using the summation of the old basis vectors $\\overrightarrow{e_{k}}$\n",
        "\n",
        "Now the following middle part should be an identity matrix:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ = $I$\n",
        "\n",
        "so that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\overrightarrow{e_{i}}=\\sum_{k} \\overrightarrow{e_{k}}\n",
        "\\end{equation}$\n",
        "\n",
        "Which means then that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\overrightarrow{e_{1}} &=\\overrightarrow{e_{1}} \\\\\n",
        "\\overrightarrow{e_{2}} &=\\overrightarrow{e_{2}} \\\\\n",
        "& \\vdots \\\\\n",
        "\\overrightarrow{e_{n}} &=\\overrightarrow{e_{n}}\n",
        "\\end{aligned}\n",
        "\\end{equation}$\n",
        "\n",
        "In order to achieve that we want for this part the following conditions:\n",
        "\n",
        "for this $\\begin{equation}\n",
        "\\left(\\sum_{j}  F_{k j} B_{j i }\\right)\n",
        "\\end{equation}$ we want:\n",
        "\n",
        "* it is 1 when i = k\n",
        "\n",
        "* it is 0 when i ≠ k\n",
        "\n",
        "because that will give us the identity matrix (because 1 is where row is equal to the columns, so i = k, and all else 0):\n",
        "\n",
        "$\\begin{equation}\n",
        "I_{n}=\\left[\\begin{array}{ccccc}\n",
        "1 & 0 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 0 & 1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\cdots & 1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "So for this operation we have the Kronecker Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\sum_{j} F_{k j} B_{j i}=\\delta_{i k}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=k \\\\\n",
        "0 \\text { if } i \\neq k\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n"
      ],
      "metadata": {
        "id": "7HbfIgCM-RXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Transformation Rules (Contravariant)**\n",
        "\n",
        "* **How can you transform from one basis to the other using the vector components (and not the basis vectors)?**\n",
        "\n",
        "* Now one could apply the forward matrix to the vector components of the old basis $\\overrightarrow{e_{i}}$ and should get the vector components of the new basis $\\widetilde{e_{i}}$ with\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{\\overrightarrow{e_{i}} (old vector components)} \\quad\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]_{\\widetilde{e_{i}} (new  vector components)}\n",
        "\\end{equation}$\n",
        "\n",
        "* We do Forward Transform in Basis Vectors and Backward Transform in Vector Components to get from old to new vector basis!\n",
        "\n",
        "* Achtung: In this case (=for vector components) you need to apply the backward transformation to get from the old components to the new components!.\n",
        "**The reason is: for basis vectors, forward brings us from old to new, and backward from new to old.** But with vector components it’s the opposite.\n",
        "This will be important for understanding covariance & contravariance!\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "  * So, we take the vector components of the old basis: $\\begin{equation}\n",
        "B\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]_{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "  * Then we multiply it with our backward transformation matrix: $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "0.25 & 0.5 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1.5\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "  * Making a few transformations: $\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "{\\left[\\begin{array}{l}\n",
        "0.25(1)+0.5(1.5) \\\\\n",
        "(-1)(1)+2(1.5)\n",
        "\\end{array}\\right]} \\\\\n",
        "{\\left[\\begin{array}{c}\n",
        "0.25+0.75 \\\\\n",
        "-1+3\n",
        "\\end{array}\\right]}\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "  * And the result are the correct vector components of the new basis: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "> **Be aware of difference between changes with basis vectors (forward from old to new, backward from new to old) and vector components (forward from new to old, backward from old to new):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_10.png)\n",
        "\n",
        "**Why does this make perfect sense?**\n",
        "\n",
        "* Look at this example: the vector components on the left (original) are [1, 1],\n",
        "\n",
        "* and the size of the new basis vectors $\\widetilde{e_{1}}$ is just double of the old one: $\\begin{equation}\n",
        "\\widetilde{e_{1}}=2 \\overrightarrow{e_{1}}\n",
        "\\end{equation}$ as well as $\\begin{equation}\n",
        "\\widetilde{e_{2}}=2 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$.\n",
        "\n",
        "* This corresponds to a forward transformation matrix for the basis vector $\\begin{equation}\n",
        "F=\\left[\\begin{array}{cc}\n",
        "2 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$ to get the new basis vector\n",
        "\n",
        "* the vector components of the vector $\\vec{v}$ are then in the new coordinate system half: [0.5, 0.5], which makes sense.\n",
        "\n",
        "* **This is because (as said in the beginning) the <u>length and the direction of a vector should never change</u> (it's invariant!), but the vector component can change and in this case they change opposite to the change of the basis vectors to keep the vectors stable as they were in both bcoordinate systems!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_11.png)\n",
        "\n",
        "> **Learning: Vector components behave the opposite way that basis vectors do !**\n",
        "\n",
        "* Similar, **if you rotate the basis vector clockwise, the vector components of the vector $\\vec{v}$ rotate anti-clockwise**.\n",
        "\n",
        "* But remember: $\\vec{v}$ has not moved, just its components changed opposite to the basis vector components!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_12.png)"
      ],
      "metadata": {
        "id": "hPW8cblf-TWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So we have two ways of writing a vector $\\vec{v}$:**\n",
        "\n",
        "  * **a linear combination of the old basis vectors with the coefficient being the old components,**\n",
        "\n",
        "  * **or we can write it as a linear combination of the new basis vectors with the coefficient being the new components**.\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=v_{1} \\overrightarrow{e_{1}}+v_{2} \\overrightarrow{e_{2}}+\\cdots+v_{n} \\overrightarrow{e_{n}}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "as well as:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\widetilde{v_{1}} \\widetilde{{e_{1}}}+\\widetilde{v_{2}} \\widetilde{e_{2}}+\\cdots+\\widetilde{v_{n}} \\widetilde{e_{n}}=\\sum_{j=1}^{n} \\widetilde{v_{j}} \\widetilde{e_{j}}\n",
        "\\end{equation}$\n",
        "\n",
        "So both if these summations are equal to $\\vec{v}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "Let’s bring out the forward and backward transformations at this point.\n",
        "\n",
        "* Forward: $\\begin{equation}\n",
        "\\widetilde{e_{j}}=\\sum_{i=1}^{n} F_{i j} \\overrightarrow{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* Backward: $\\begin{equation}\n",
        "\\overrightarrow{e_{j}}=\\sum_{i=1}^{n} B_{i j} \\stackrel{\\sim}{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "If we take the old basis and the old components, what we can do is we can use the backward transformation B to replace the old basis vectors\n",
        "\n",
        "> $\\overrightarrow{e_{j}}$ with the new basis vectors from B: $\\begin{equation}\n",
        "=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{j=1}^{n} v_{j}\\left(\\sum_{i=1}^{n} B_{i j} \\widetilde{{e}_{i}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "Rearranging the summation we get this:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right) \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "**Now we have $\\vec{v}$ written as a summation of the new basis vectors**.\n",
        "\n",
        "Nut now, let’s have a look at the middle part of the equation above:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\left(\\sum_{j=1}^{n} B_{i j} v_{j}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "If we compare that to our summation in the step before when we wrote $\\vec{v}$ as a summation of new basis vectors:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{j=1}^{n} v_{j} \\overrightarrow{e_{j}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{e_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "the coefficients have to be the new components $\\widetilde{v}_{i}$. And indeed $\\widetilde{v}_{i}$ equals:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\widetilde{v}_{i}=\\sum_{j=1}^{n} B_{i j} v_{j}\n",
        "\\end{equation}$\n",
        "\n",
        "we actually used the backward transformation! So, **this is the proof that to move from the old components to the new components we actually used the backwards transformation!**\n",
        "\n",
        "**Summary**: We know how basis vectors transform and how vector components transform (opposite direction).\n",
        "\n",
        "* Now because vector components behave contrary to the basis vectors we say that vector components are contra variant!\n",
        "\n",
        "* And we see later that vectors are contra variant tensors!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_13.png)\n",
        "\n",
        "**This means also we make a small but important change:**\n",
        "\n",
        "Here you can see we wrote the vector $\\vec{v}$ as a linear combination of the old and the new basis:\n",
        "\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v_{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}_{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "But since the vector components $v_{i}$ and $\\widetilde{v}_{i}$ behave contra variant we put the index up $v^{i}$ and $\\widetilde{v}^{i}$:\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\vec{v}=\\sum_{i=1}^{n} v^{i} \\overrightarrow{e_{i}}=\\sum_{i=1}^{n} \\widetilde{v}^{i} \\widetilde{{e}_{i}}\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "t8PMvuxn-Vdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Forward and Backward Transform (Linear Forms): Basiswechsel und Covector-(Coordinate)-Transformation (Covariant)*"
      ],
      "metadata": {
        "id": "Eg2UlbUr-YCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Rules:**\n",
        "\n",
        "* Vector Space $V$ to get from old to new: **Forward Transform in Basis Vectors (covariant) and Backward Transform in Vector Components (contravariant)**\n",
        "\n",
        "* Vector Space $V*$ to get from old to new: **Backward Transform in Basis Vectors (contravariant) and Forward Transform in Vector Components (covariant)**\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "\\overrightarrow{e_{i}} \\text{Old Basis V with Vectors} \\quad \\vec{v} &  \\widetilde{e_{i}} \\text{New Basis V with Vectors} \\quad \\widetilde{v}\\\\\n",
        "\\overrightarrow{\\epsilon_{i}} \\text{Old Dual Basis V* with Covectors} \\quad \\vec{\\alpha} & \\widetilde{\\epsilon_{i}} \\text{New Dual Basis V* with Covectors} \\quad \\widetilde{\\alpha}\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$"
      ],
      "metadata": {
        "id": "s9ikXvt0-iN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* covectors / linear forms, like scalars (for example differentials), change with basis (=covariant)\n",
        "\n",
        "* change of coordinate system does not change scalar (like temperature), hence linear forms =scalars) bechave covariant with basis (meanwhile direction in vectors change contravariant to keep the original position of a vector)\n",
        "\n",
        "* **All covectors can be written as the linear combination of the dual basis vectors!**\n",
        "\n",
        "* **Covector components can be obtained by counting how many covector lines that the basis vector pierces**\n",
        "\n",
        "* **Covector components transform in the opposite way that vector components do**\n",
        "\n",
        "* Videos:\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 5: Covector Components ](https://www.youtube.com/watch?v=rG2q77qunSw&t=10s)\n",
        "\n",
        "  * [Eigenchris: Tensors for Beginners 6: Covector Transformation Rules](https://www.youtube.com/watch?v=d5da-mcVJ20&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=9)\n",
        "\n",
        "\n",
        "**Fun facts**\n",
        "\n",
        "* Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!** We need Epsilon $\\epsilon^{n}$ as dual basis.\n",
        "\n",
        "* **Basis Vector in $V$ - Upper-left**: we started with the transformation rules for basis vectors = covariant: from old to new with the forward transform.\n",
        "\n",
        "* **Vector Components in $V$ - Bottom-left**: The transformation rules for vector components are the opposite compared to the basis vectors (from old to new with the backward transform). So they are contra variant.\n",
        "\n",
        "* **Basis Covector in $V*$ - Upper-right (Dual Space)**: Transformation rules for basis covectors are also opposite compared to basis vectors. So basis covectors also transform by the contra variant rule.\n",
        "\n",
        "\n",
        "* **Covector Components in $V*$ - Bottom-right (Dual Space)**: Covector components transform in the same way that basis vectors do = This means that covectors transform covariantly. (and contravariant to vectors components)\n",
        "\n",
        "> **Wie man sieht: man konstruiert (neue) basisvektoren aus (alten) basisvektoren und (neue) vektorkomponenten aus (alten) vektorkomponenten**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_18.png)"
      ],
      "metadata": {
        "id": "WATiQxEx-kZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does it mean when we say the covector has components?**\n",
        "\n",
        "* Look at this example: $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}} \\text { = } 2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}\n",
        "\\end{equation}$\n",
        "\n",
        "* So when we write a **column** vector $\\begin{equation}\n",
        "\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "1\n",
        "\\end{array}\\right]_{\\vec{e}_{i}}\n",
        "\\end{equation}$\n",
        "\n",
        "* What we mean is: This vector is given by the **linear combination of** $2 \\overrightarrow{e_{1}}+1 \\overrightarrow{e_{2}}$\n",
        "\n",
        "> **It tells you how much of each basis vector is needed to make the vector**\n",
        "\n",
        "**So what exactly are we measuring with when we write [2 1] ?** - **2 of what and 1 of what?**\n",
        "\n",
        "* Remember: Covectors are functions $\\begin{equation}\n",
        "\\alpha: V \\rightarrow \\mathbb{R}\n",
        "\\end{equation}$ (Linearformen, Funktionale etc)\n",
        "\n",
        "* **Covectors don't live in vector space $V$. They take vectors in $V$ as inputs.** (and then spit out scalar number etc, like integral or differential)\n",
        "\n",
        "* **We use can't basis vectors in $V$ like $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$ to measure covectors !!**\n",
        "\n",
        "\n",
        "**Epsilon $\\epsilon^{n}$ as dual basis**\n",
        "\n",
        "Take the basis $\\begin{equation}\n",
        "\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\} \\text { for } V \\text { . }\n",
        "\\end{equation}$\n",
        "\n",
        "We introduce two special covectors: $\\epsilon^{1},\\epsilon^{2}: V \\rightarrow \\mathbb{R}$, which are both functions from vectors to numbers (notice how the labels 1 and 2 are now above instead of below).\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0 \\\\\n",
        "\\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)=0 & \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=1\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "(Hint: kovariant basis from $V$ with kontravariant basis from $V*$ equals the identity)\n",
        "\n",
        "this means we can use the Kronecker-Delta:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{i}\\left(\\overrightarrow{e_{j}}\\right)=\\delta_{i j}=\\left\\{\\begin{array}{l}\n",
        "1 \\text { if } i=j \\\\\n",
        "0 \\text { if } i \\neq j\n",
        "\\end{array}\\right.\n",
        "\\end{equation}$\n",
        "\n",
        "**So what does this epsilon $\\epsilon$ mean?**\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{1}(\\vec{v})=\\epsilon^{1}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "We can rewrite this like this (this addition and scaling can be brought outside the function since it's a linear combination):\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} \\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)\n",
        "\\end{equation}$\n",
        "\n",
        "From the previous part we know that:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "\\epsilon^{1}\\left(\\overrightarrow{e_{1}}\\right)=1 & \\epsilon^{1}\\left(\\overrightarrow{e_{2}}\\right)=0\n",
        "\\end{array}\n",
        "\\end{equation}$\n",
        "\n",
        "So we can rewrite the equation above as:\n",
        "\n",
        "$\\begin{equation}\n",
        "=v^{1} * 1 +v^{2} * 0\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{1} = \\epsilon^{1}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "And the same goes for $\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$:\n",
        "\n",
        "$\\begin{equation}\n",
        "\\epsilon^{2}(\\vec{v})=\\epsilon^{2}\\left(v^{1} \\overrightarrow{e_{1}}+v^{2} \\overrightarrow{e_{2}}\\right)=v^{1} \\epsilon^{2}\\left(\\overrightarrow{e_{1}}\\right)+v^{2} \\epsilon^{2}\\left(\\overrightarrow{e_{2}}\\right)=v^{2}\n",
        "\\end{equation}$\n",
        "\n",
        "> $\\begin{equation}\n",
        "v^{2} = \\epsilon^{2}(\\vec{v})\n",
        "\\end{equation}$\n",
        "\n",
        "> The epsilons are projecting out vector components\n",
        "\n",
        "*Directions of epsilon basis vectors*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_15.png)\n",
        "\n",
        "See image below: Write a general covector alpha $\\alpha$ (which can be any covector of our choice) as a linear combination of the epsilon covectors.\n",
        "\n",
        "**This means that epsilon covectors form a basis for the set of all covectors!**\n",
        "\n",
        "> **And for that reason we call these epsilons the dual basis, because they are basis for the dual space $V*$.**\n",
        "\n",
        "* **We can't just flip column vectors on their side to get the row vectors! This works only in an orthonormal basis**.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_16.png)"
      ],
      "metadata": {
        "id": "1Fkz_0Ra-mfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Die Koeffizienten der Linearformen verhalten sich kovariant**\n",
        "\n",
        "* **Problem**: How do we transform some scalar $\\Phi$ ? The answer is \"**derivatives of a scalar**\":\n",
        "\n",
        "> $\\frac{\\partial \\Phi}{\\partial x^{i}}=\\frac{\\partial \\Phi}{\\partial x^{\\prime k}} \\frac{\\partial x^{\\prime k}}{\\partial x^{i}}$\n",
        "\n",
        "* **Darstellung**: $x_{1}$ (lower index, tiefgestellte Indizes)\n",
        "\n",
        "* **Definition**:\n",
        "\n",
        "  * Koeffizienten der Linearformen (=zB Skalare) verhalten sich kovariant. (wahrscheinlich: kontravariante Basisvektoren (= \"kovarianten Vektor\", weil Koordinaten kovariant): Normal zu den Koordinatenflächen)\n",
        "\n",
        "  * [Skalar](https://de.m.wikipedia.org/wiki/Skalar_(Mathematik)): In der Physik werden Skalare verwendet zur Beschreibung physikalischer Größen, die **richtungsunabhängig** sind. Beispiele für skalare physikalische Größen sind die **Masse eines Körpers, seine Temperatur, seine Energie und auch seine Entfernung von einem anderen Körper** (als Betrag der Differenz der Ortsvektoren). Anders gesagt: Eine skalare physikalische Größe ändert sich bei Änderungen der Lage oder Orientierung nicht. Wird hingegen für die vollständige Beschreibung der Größe eine Richtung benötigt, wie bei der Kraft oder der Geschwindigkeit, so wird ein Vektor verwendet, bei Abhängigkeit von mehreren Richtungen ein Tensor (genauer: Tensor 2. oder noch höherer Stufe).\n",
        "\n",
        "  * Every quantity which under a coordinate transformation, transforms like the derivatives of a scalar is called a covariant tensor.\n",
        "\n",
        "  * Covectors (also called **dual vectors**) typically have units of the inverse of distance or the inverse of distance with other units. The components of covectors **change in the same way as changes to scale of the reference axes and consequently are called covariant**.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * Koeffizienten der [Linearformen](https://de.wikipedia.org/wiki/Linearform) (=zB Skalare)\n",
        "\n",
        "  * An example of a covector is the gradient, which has units of a spatial derivative, or distance<sup>−1</sup>.\n",
        "\n",
        "  * Examples of covariant vectors generally appear when taking a gradient of a function.\n",
        "\n",
        "* **Beispiele fur Kovarianz [in der Physik](https://de.wikipedia.org/wiki/Kovarianz_(Physik))**\n",
        "\n",
        "  * Unter Galilei-Transformationen transformieren sich die Beschleunigung und die Kraft in den newtonschen Bewegungsgleichungen im gleichen Sinne wie die Ortsvektoren. Daher sind die Newtonschen Bewegungsgleichungen und damit die klassische Mechanik kovariant bzgl. der Gruppe der Galilei-Transformationen.\n",
        "\n",
        "  * Im gleichen Sinne sind die Einstein-Gleichungen der Gravitation in der allgemeinen Relativitätstheorie kovariant unter beliebigen (nichtlinearen glatten) Koordinatentransformationen.\n",
        "\n",
        "  * Ebenso ist die Dirac-Gleichung der Quantenelektrodynamik kovariant unter der Gruppe der linearen Lorentz-Transformationen.\n",
        "\n",
        "  * Die linke Seite der Klein-Gordon-Gleichung für ein Skalarfeld ändert sich unter Lorentz-Transformationen nicht, sie ist spezieller invariant oder skalar.\n",
        "\n",
        "* **Extension**:\n",
        "\n",
        "  * A covariant vector or cotangent vector (often abbreviated as covector) has components that co-vary with a change of basis.\n",
        "\n",
        "  * That is, the **components must be transformed by the same matrix as the change of basis matrix**. The components of covectors (as opposed to those of vectors) are said to be covariant.  In Einstein notation, covariant components are denoted with lower indices as in $\\mathbf{e}_{i}(\\mathbf{v})=v_{i}$\n",
        "\n",
        "  * A covariant vector has **components (=coordinates) that change oppositely to the coordinates or, equivalently,\n",
        "transform like the reference axes**.\n",
        "\n",
        "  * For instance, the components of the gradient vector of a function $\\nabla f=\\frac{\\partial f}{\\partial x^{1}} \\widehat{x}^{1}+\\frac{\\partial f}{\\partial x^{2}} \\widehat{x}^{2}+\\frac{\\partial f}{\\partial x^{3}} \\widehat{x}^{3}$\n",
        "transform like the reference axes themselves.\n",
        "\n",
        "  * See also [Covariant Transformation](https://en.wikipedia.org/wiki/Covariant_transformation)\n",
        "\n",
        "\n",
        "Die Komponenten eines Kovektors, eines Vektors aus dem Dualraum, transformieren kovariant.\n",
        "\n",
        "![iiu](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_05.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "amwdLqv5-oX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **The derivative of a function transforms covariantly!** (and a tangent vector of a smooth curve will transform as a contravariant tensor of order one under a change of coordinates)\n",
        "\n",
        "* In physics, a [covariant transformation](https://en.wikipedia.org/wiki/Covariant_transformation) is a rule that specifies how certain entities, such as vectors or tensors, change under a change of basis.\n",
        "\n",
        "* The transformation that describes the new basis vectors as a linear combination of the old basis vectors is defined as a covariant transformation.\n",
        "\n",
        "* Conventionally, indices identifying the basis vectors are placed as lower indices and so are all entities that transform in the same way.\n",
        "\n",
        "*Covariant Derivative*\n",
        "\n",
        "* the [covariant derivative](https://en.wikipedia.org/wiki/Covariant_derivative) is a way of specifying **a derivative along tangent vectors of a manifold**.\n",
        "\n",
        "* The name is motivated by the importance of changes of coordinate in physics: the covariant derivative transforms covariantly under a general coordinate transformation, that is, linearly via the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the transformation.\n",
        "\n",
        "* The covariant derivative is a generalization of the [directional derivative](https://en.wikipedia.org/wiki/Directional_derivative) / [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) from vector calculus (ps: Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale Räume ist das [Gâteaux-Differential](https://de.wikipedia.org/wiki/Gâteaux-Differential).)\n",
        "\n",
        "* The covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule,\n",
        "\n",
        "  * $\\nabla_{\\mathbf{u}} \\mathbf{v},$ which takes as its inputs:\n",
        "\n",
        "  * (1) a vector, $\\mathbf{u},$ defined at a point $P$, and\n",
        "\n",
        "  * (2) a vector field, $\\mathbf{v}$, defined in a neighborhood of $P$\n",
        "\n",
        "  * The output is the vector $\\nabla_{\\mathbf{u}} \\mathbf{v}(P)$, also at the point $P$.\n",
        "\n",
        "*Covariant derivative and covariant transformation*\n",
        "\n",
        "* The primary difference from the usual directional derivative is that $\\nabla_{\\mathrm{u}} \\mathrm{v}$ must, in a certain precise sense, **be independent of the manner in which it is expressed in a coordinate system**.\n",
        "\n",
        "* A vector may be described as a list of numbers in terms of a basis, **but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis**.\n",
        "\n",
        "* This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation.\n",
        "\n",
        "> **The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: <u>the covariant derivative must change by a covariant transformation</u>.**\n",
        "\n",
        "*In the Euclidean Space*\n",
        "\n",
        "* In the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points. In such a system one translates one of the vectors to the origin of the other, keeping it parallel.\n",
        "\n",
        "* **With a Cartesian (fixed orthonormal) coordinate system \"keeping it parallel\" amounts to keeping the components constant**.\n",
        "\n",
        "* Euclidean space provides the simplest example: a covariant derivative which is obtained by taking the ordinary directional derivative of the components in the direction of the displacement vector between the two nearby points.\n",
        "\n",
        "*In other (more general) Spaces*\n",
        "\n",
        "* In the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in **polar coordinates** in a two dimensional Euclidean plane, **then it contains extra terms that describe how the coordinate grid itself \"rotates\"**.\n",
        "\n",
        "* **In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves**, etc.\n",
        "\n",
        "* **In this case \"keeping it parallel\" does NOT amount to keeping components constant under translation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "u2amkyDK-qT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/covariant_transformation.png)\n",
        "\n",
        "* In the shown example, a vector $\\mathbf{v}=\\sum_{i \\in\\{x, y\\}} v^{i} \\mathbf{e}_{i}=\\sum_{j \\in\\{r, \\phi\\}} v^{\\prime j} \\mathbf{e}_{j}^{\\prime} .$ is described by two different coordinate\n",
        "systems:\n",
        "  * a rectangular coordinate\n",
        "system (the black grid),\n",
        "  * and a radial\n",
        "coordinate system (the red grid).\n",
        "\n",
        "* Basis vectors have been chosen for both\n",
        "coordinate systems: $\\mathbf{e}_{x}$ and $\\mathbf{e}_{\\mathbf{y}}$ for the rectangular coordinate system, and $\\mathbf{e}_{\\mathrm{r}}$\n",
        "and $\\mathbf{e}_{\\phi}$ for the radial coordinate system.\n",
        "\n",
        "* The radial basis vectors $\\mathbf{e}_{\\mathrm{r}}$ and $\\mathbf{e}_{\\phi}$ appear\n",
        "rotated anticlockwise with respect to the\n",
        "rectangular basis vectors $\\mathbf{e}_{\\mathrm{x}}$ and $\\mathbf{e}_{\\mathrm{y}}$.\n",
        "\n",
        "> **The covariant transformation, performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors**.\n",
        "\n",
        "* The coordinates of $v$ must be transformed into the new coordinate system, **but the vector $v$ itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates**.\n",
        "\n",
        "* The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view $v$ from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors $\\mathbf{e}_{\\mathbf{r}}$ and $\\mathbf{e}_{\\Phi}$. compared to how it appeared relative to the rectangular basis vectors $\\mathbf{e}_{x}$ and $\\mathbf{e}_{y}$.\n",
        "\n",
        "* **Thus, the needed contravariant transformation to $v$ in this example is a clockwise rotation.**\n",
        "\n",
        "Example\n",
        "\n",
        "* Consider the example of moving along a curve $\\gamma(t)$ in the Euclidean plane. In polar coordinates, $\\gamma$ may be written in terms of its radial and angular coordinates by $\\gamma(t)=(r(t), \\theta(t))$.\n",
        "\n",
        "* A vector at a particular time $t$ (for instance, the acceleration of the curve) is expressed in terms of $\\left(\\mathbf{e}_{r}, \\mathbf{e}_{\\theta}\\right),$ where $\\mathbf{e}_{r}$ and $\\mathbf{e}_{\\theta}$ are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and [tangential components](https://en.wikipedia.org/wiki/Tangential_and_normal_components).\n",
        "\n",
        "* At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the [Christoffel symbols](https://en.wikipedia.org/wiki/Christoffel_symbols)) serve to express this change.\n",
        "\n",
        "*Another Example*\n",
        "\n",
        "* In a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.\n",
        "\n",
        "* A vector e on a globe on the equator at point Q is directed to the north. Suppose we parallel transport the vector first along the equator until at point P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q.\n",
        "\n",
        "* **Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector**; instead, it has another orientation. **This would not happen in Euclidean space and is caused by the curvature of the surface of the globe**. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature."
      ],
      "metadata": {
        "id": "mpNqLFw7-sbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Bilinear Forms) $\\mathcal{B}_{i i} \\epsilon^{i} \\epsilon^{j} \\rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$*"
      ],
      "metadata": {
        "id": "GCJMH1Oj-vBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor product $\\otimes$ for bilinear maps: Bilinear forms are linear combinations of covector-covector-pairs** (including the metric tensor)\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$\n",
        "\n",
        "* Generalization: Bilinear Forms as Covector-Covector-Pairs\n",
        "\n",
        "* Why not vector-vector-pairs or so? - Bilinear forms take two vector inputs and since covectors take one vector each, a pair of covectors would take two vector inputs.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 12: Bilinear Forms are Covector-Covector pairs](https://www.youtube.com/watch?v=uDRzJIaN2qw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=15)\n",
        "\n",
        "**Advantages of the tensor product for bilinear maps:**\n",
        "\n",
        "* We can write bilinear forms as linear combinations of covector covector pairs and this:\n",
        "\n",
        "  * immediately gives us the transformation rules $\\begin{aligned} \\widetilde{\\epsilon}^{i} &=B_{j}^{i} \\epsilon^{j} & \\widetilde{\\mathcal{B}_{i j}} &=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l} \\\\ \\epsilon^{i} &=F_{j}^{i} \\widetilde{\\epsilon}^{j} & \\mathcal{B}_{i j} &=B_{i}^{k} B_{j}^{l} \\widetilde{B_{k l}} \\end{aligned}$\n",
        "\n",
        "  * the component multiplication formula: $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\Rightarrow \\mathcal{B}_{i j} v^{i} w^{i}$\n",
        "\n",
        "  * and the correct array shape\n",
        "\n",
        "* **Combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors (and not a row and a column vwector like for the Kronecker delta) associated with the covectors**\n",
        "\n",
        "* Meanwhile the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_43.png)\n"
      ],
      "metadata": {
        "id": "svrOb-ni_IT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "ZdLtfLao_KKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 1: write out any bilinear form as a linear combination of covector-covector pairs**\n",
        "\n",
        "*Look at our classic transformation rules*:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{i j}=B_{i}^{k} B_{j}^{l} \\widetilde{\\mathcal{B}_{k l}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{e_{j}}=F_{j}^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "> $\\overrightarrow{e_{j}}=B_{j}^{i} \\widetilde{e_{i}}$\n",
        "\n",
        "and\n",
        "\n",
        "> $\\widetilde{\\epsilon}^{i}=B_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\epsilon^{i}=F_{j}^{i} \\epsilon^{j}$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "If we can assume we can write out any bilinear form as a linear combination of covector-covector pairs **to get the transformation rule for these components** we just transform the basis covectors individually:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l} \\epsilon^{k} \\epsilon^{l}$\n",
        "\n",
        "Basis covectors are contra variant, so to build old from the new we use the forward transform $F$\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{k l}\\left(F_{i}^{k} \\widetilde{\\epsilon}^{i}\\right)\\left(F_{j}^{l} \\widetilde{\\epsilon}^{j}\\right)$\n",
        "\n",
        "and putting these in front gives us this:\n",
        "\n",
        "> $\\mathcal{B}=\\left(F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}\\right) \\widetilde{\\epsilon}^{i}\\widetilde{\\epsilon}^{j}$\n",
        "\n",
        "which as we can see is the correct one:\n",
        "\n",
        "> $\\widetilde{B_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$"
      ],
      "metadata": {
        "id": "-1G-vCul_L_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 2: We can also get the correct component multiplication formula when a bilinear form acts on two vector inputs:**\n",
        "\n",
        "Given:\n",
        "\n",
        "> $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "> $\\vec{w}=w^{l} \\overrightarrow{e_{l}}$\n",
        "\n",
        "**Proof:** replace components with equations above:\n",
        "\n",
        "> $s=\\mathcal{B}(\\vec{v}, \\vec{w})$\n",
        "\n",
        "Replace the bilinear form and the vectors with their linear combination expansions in some basis\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}, w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Now we pass each of these vector inputs to their corresponding covectors\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} \\epsilon^{i}\\left(v^{k} \\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(w^{l} \\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "\n",
        "$v$ and $w$ come out in front:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\epsilon^{i}\\left(\\overrightarrow{e_{k}}\\right) \\epsilon^{j}\\left(\\overrightarrow{e_{l}}\\right)$\n",
        "\n",
        "Replace ($\\overrightarrow{e_{k}}$) and ($\\overrightarrow{e_{l}}$) with Kronecker deltas\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{k} w^{l} \\delta_{k}^{i} \\delta_{l}^{j}$\n",
        "\n",
        "Finally using the index cancellation rules for $k$ and $l$ we get the correct component multiplication formula ends up giving us a single number as a result:\n",
        "\n",
        "> $s=\\mathcal{B}_{i j} v^{i} w^{j}$\n"
      ],
      "metadata": {
        "id": "GaERJjMl_Nua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit 3**\n",
        "\n",
        "* Remember with linear maps, the new way of writing with the tensor product makes a lot more tensors (with a row of rows): Even though we had two vector inputs we needed to write one as a column and one as a row  flipped on its side to make the multiplication work correctly. Which is awkward, because vectors should always be written as columns and not as rows!\n",
        "\n",
        "* When we write the bilinear forms as a row of rows the matrix multiplication formula makes a lot more sense. We can write out both vectors as columns\n",
        "\n",
        "* **Above is the old way (row vector & column vector both to describe vectors, and the transition matrix in the middle) and on the bottom is the new better way with tensor products $\\otimes$**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_42.png)"
      ],
      "metadata": {
        "id": "94JzCC8X_QsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Covector-Covector Pairs (Metric Tensor) $g(\\vec{\\color{blue}v}, \\vec{\\color{blue}w}) \\mapsto \\color{blue}{v^{i}} \\color{blue}{w^{j}} g_{i j}$ for Index Manipulation (contravariant $Z^{ij}$ and covariant $Z_{ij}$)*"
      ],
      "metadata": {
        "id": "YgcZdTDc_TCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor**\n",
        "\n",
        "* In differential geometry, one definition of a [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) is a type of function which takes as input a pair of tangent vectors $v$ and $w$ at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar $g(v, w)$ in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space.\n",
        "\n",
        "* Define length of and angle between tangent vectors when basis changes (same way like a dot product). Use Cases: How to measure the length of a vector when we change the vector (with a linear map) as well as when the basis changes?\n",
        "\n",
        "* Through integration, the metric tensor allows one to define and **compute the length of curves on the manifold** [Source](http://walter.bislins.ch/physik/index.asp?page=Metrik%2DTensor)\n",
        "\n",
        "**Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$. In a given basis we compute the output of a metric tensor using this formula**:\n",
        "\n",
        "> $g(\\vec{\\color{red}v}, \\vec{\\color{blue}w}) \\mapsto \\color{red}{v^{i}} \\color{blue}{w^{j}} g_{i j}$\n",
        "\n",
        "And this is the formula for computing the output of a metric tensor when it acts on two vectors\n",
        "\n",
        "> $\\left[\\begin{array}{ll}\\color{red}{v^{1}} & \\color{red}{v^{2}}\\end{array}\\right]\\left[\\begin{array}{ll}g_{11} & g_{12} \\\\ g_{21} & g_{22}\\end{array}\\right]\\left[\\begin{array}{l}\\color{blue}{w^{1}} \\\\ \\color{blue}{w^{2}}\\end{array}\\right]$\n",
        "\n",
        "**The algebraic properties of metric tensors are**:\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 9: The Metric Tensor](https://www.youtube.com/watch?v=C76lWSOTqnc&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=12)"
      ],
      "metadata": {
        "id": "_mR6eUfX_WoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)"
      ],
      "metadata": {
        "id": "L2D_3hgX_oLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). **Metric tensors are (0,2) tensors, because it transforms using tow covariant rules**:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "KPlaTJsa_qJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensors for measuring length or angles**\n",
        "\n",
        "* Is a tensor whose components in a given vector basis are given by the **dot products of the basis vectors**:\n",
        "\n",
        "  * <font color=\"blue\">$g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}$</font>\n",
        "\n",
        "* Since the dot products don't care about the order of the input $g_{i j}=g_{j i}$, which means the **tensor is symmetric** along the diagonal line (spur).\n",
        "\n",
        "  * $g_{i j}=\\overrightarrow{e_{i}} \\cdot \\overrightarrow{e_{j}}=\\overrightarrow{e_{j}} \\cdot \\overrightarrow{e_{i}}=g_{j i}$\n",
        "\n",
        "* when we want to get an **angle**, we put both vectors in:\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{v}=\\|\\vec{v}\\|^{2}=$ <font color=\"blue\">$v^{i} v^{j} g_{i j}$</font>\n",
        "\n",
        "  * $\\vec{w} \\cdot \\vec{w}=\\|\\vec{w}\\|^{2}=$ <font color=\"blue\">$w^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* when we want to get a **lengths** we out the same vector twice in\n",
        "\n",
        "  * $\\vec{v} \\cdot \\vec{w}=\\|\\vec{v}\\|\\|\\vec{w}\\| \\cos \\theta=$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "* **In a given basis the Metric tensor is a function $g: V \\times V \\rightarrow \\mathbb{R}$**:\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}) \\mapsto$ <font color=\"blue\">$v^{i} w^{j} g_{i j}$</font>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5NQ7BCXo_r5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric tensor $g$ and inverse metric tensor $\\mathfrak{g}$ for lowering & raising indexes (Contraction)**\n",
        "\n",
        "* In mathematics and mathematical physics, [raising and lowering indices](https://en.m.wikipedia.org/wiki/Raising_and_lowering_indices) are operations on tensors which change their type. Raising and lowering indices are a form of [index manipulation (Ricci calculus)](https://en.m.wikipedia.org/wiki/Ricci_calculus) in tensor expressions.\n",
        "\n",
        "* The metric tensor represents a matrix with scalar elements $\\left(g_{i j}\\right.$ or $\\mathfrak{g}^{i j}$ ) and is a tensor object which is used to raise or lower the index on another tensor object by an operation called [Tensor Contraction](https://en.m.wikipedia.org/wiki/Tensor_contraction), thus **allowing a covariant tensor to be converted to a contravariant tensor, and vice versa**.\n",
        "\n",
        "  * In multilinear algebra, a [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) is an operation on a tensor that arises from the natural pairing of a finite-dimensional vector space and its dual.\n",
        "\n",
        "  * In components, it is expressed as a sum of products of scalar components of the tensor(s) caused by applying the summation convention to a pair of dummy indices that are bound to each other in an expression.\n",
        "\n",
        "  * The contraction of a single mixed tensor occurs when a pair of literal indices (one a subscript, the other a superscript) of the tensor are set equal to each other and summed over. In the Einstein notation this summation is built into the notation. **The result is another tensor with order reduced by 2.**\n",
        "\n",
        "  * Es ist eine Verallgemeinerung der Spur einer linearen Abbildung auf Tensoren, die mindestens einfach kovariant und einfach kontravariant sind.\n",
        "\n",
        "* **For an orthonormal [Cartesian coordinate system](https://en.m.wikipedia.org/wiki/Cartesian_coordinate_system), the metric tensor is just the [kronecker delta](https://en.m.wikipedia.org/wiki/Kronecker_delta) $\\delta_{i j}$ or $\\delta^{i j},$ which is just a tensor equivalent of the identity matrix, and $\\delta_{i j}=\\delta^{i j}=\\delta_{j}^{i}$**. For measuring length of a vector: Pythagoras theorem is only valid in orthonormal bases!\n",
        "\n",
        "* Normally a metric tensor as a function from a pair of vectors to scalars $g: V x V \\rightarrow \\mathbb{R}$. But it can also be a function from a vector in $V$ to a covector in $V^{*}$ $g: V  \\rightarrow V^{*}$ using the metric tensor. Reverse direction: from a covector to its vector partner.\n",
        "\n",
        "* From the metric tensor we know its components have 2 lower indices, so it’s a member of $V^{*}$ tensor $V^{*}$ bzw. $g \\in V^{*} \\otimes V^{*}$. Now we introduce what's called the inverse metric tensor $\\mathfrak{g} \\in V \\otimes V$. The combination between both in a summation gives you the Kronecker delta: $\\mathfrak{g}^{k i} g_{i j}=\\delta_{j}^{k}$. This is how we define the inverse metric tensor.\n",
        "\n",
        "* The **ordinary metric tensor $g_{i j}$ is covariant** with $g_{i j}=\\vec{g}_{i} \\cdot \\vec{g}_{j}$. It **lowers indexes** and its components are covariant: $T_{i}=g_{i j} T^{j}$\n",
        "\n",
        "* The **inverse metric tensor $\\mathfrak{g}^{k i}$ is contravariant** with $\\mathfrak{g}^{i j}=\\vec{\\mathfrak{g}}^{i} \\cdot \\vec{\\mathfrak{g}}^{j}$. It **raises the indexes** and its components are contravariant (go in the other direction): $T^{i}=\\mathfrak{g}^{i j} T_{j}$.\n",
        "\n",
        "\n",
        "* Both metric tensors are related by the identity: $g_{i k} \\, \\mathfrak{g}^{j k}=\\delta_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_82.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "87h0yKnx_try"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sharp Operator $\\sharp$ and Flat Operator $\\flat$ to generalize raising and lowering indexes on the component of any tensor**\n",
        "\n",
        "* The ordinary metric tensor lowers indexes meanwhile the inverse metric tensor raises indexes.\n",
        "\n",
        "* But these raising and lowering operations don’t just apply to vector and covector components. We can also raise and lower the indexes on the components of other tensors\n",
        "\n",
        "* $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$ wird zu $\\rightarrow$ $\\flat \\vec{v}=v_{i} \\epsilon^{i}$. The flat operator (on the left side) lowers the indexes (on the right side) from $v^{i}$ to $v_{i}$. **Another way to think of it: the $\\flat$ operator is transforming a vector arrow into a covector stack** (like flattening the pointy arrow into a flat stack)\n",
        "\n",
        "* The covector alpha has components with downstairs indexes: $\\alpha=\\alpha_{i} \\epsilon^{i}$ and the vector alpha sharp has components with upstairs indexes $\\sharp \\alpha=\\alpha^{i} \\overrightarrow{e_{i}}$. The sharp operator is basically raising the index. Also the sharp operator is basically turning a flat covector stack into a sharp pointy arrow vector.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 16: Raising/Lowering Indexes (with motivation, sharp + flat operators)](https://www.youtube.com/watch?v=_z9R7OMpxhY)\n",
        "\n",
        "* See article [Sharp & Flat Operator - Isomorphismus](https://de.wikipedia.org/wiki/Äußere_Ableitung#Be-_und_Kreuz-_(Flat-_und_Sharp-)_Isomorphismus)\n",
        "\n",
        "* Take tensor $Q$ which is member of vector space $Q \\in V \\otimes V^{*} \\otimes V^{*}$ and has components $Q_{j k}^{I}$:  $Q=Q_{j k}^{i} \\vec{e}_{i} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "* If we multiply it but the inverse metric tensor and sum over $j$ like this $Q^{i}{ }_{j k} \\mathfrak{g}^{j x}$ we can raise the index upward: $=Q^{i x}{ }_{k}$ and we get this new tensor $Q^{\\prime}=Q_{k}^{i x} \\overrightarrow{e_{i} {e}_{x}} \\epsilon^{k}$\n",
        "\n",
        "* This is a member of the new vector space $Q^{\\prime} \\in V \\otimes \\color{red}{V} \\otimes V^{*}$ - we raised the middle index $Q \\in V \\otimes \\color{red}{V^{*}} \\otimes V^{*}$\n",
        "\n",
        "* All these vector spaces can be traveled between using the ordinary metric tensor to lower indexes (blue arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_84.png)\n",
        "\n",
        "* Or we can use the inverse metric tensor to raise indexes (red arrow):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_85.png)"
      ],
      "metadata": {
        "id": "spzpC64M_vis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Tensor as Special Bilinear Form**: Metric Tensor = \"First Fundamental Form\". The metric tensor is a special bilinear form. Generalisation via tensor product: Bilinear Forms as Covector-Covector-Pairs.\n",
        "\n",
        "* So what is the difference between metric tensor and bilinear form? The metric tensor is a bilinear form, but it's a very specific example of a bilinear form. The metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* **Videos**: https://youtu.be/Hf-BxbtCg_A and https://youtu.be/Dn0ZZRVuJcU"
      ],
      "metadata": {
        "id": "oyUSwoxm_xXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between Metric Tensor (Form) and Bilinear Forms**\n",
        "\n",
        "( The properties of a bilinear form look very similar to the metric tensor properties:\n",
        "\n",
        "* it takes two vectors as input to output a number (scalar like angle or length):\n",
        "\n",
        "> $g: V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "* and it follows the linearity properties:\n",
        "\n",
        "  * $a g(\\vec{v}, \\vec{w})=g(a \\vec{v}, \\vec{w})=g(\\vec{v}, a \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}+\\vec{u}, \\vec{w})=g(\\vec{v}, \\vec{w})+g(\\vec{u}, \\vec{w})$\n",
        "\n",
        "  * $g(\\vec{v}, \\vec{w}+\\vec{t})=g(\\vec{v}, \\vec{w})+g(\\vec{v}, \\vec{t})$\n",
        "\n",
        "To compute the output of a function in a given basis where ${\\mathcal{B}_{i j}}$ are the components of a matrix:\n",
        "\n",
        "> $\\mathcal{B}(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} \\mathcal{B}_{i j}$\n",
        "\n",
        "The same goes for the metric tensor compute the output of a metric tensor in a given basis:\n",
        "\n",
        "> $g(\\vec{v}, \\vec{w}) \\mapsto v^{i} w^{j} g_{i j}$\n",
        "\n",
        "**And just like the metric tensor bilinear forms are (0,2) tensors (so they transform using 2 covariant rules when we change coordinate systems)**:\n",
        "\n",
        "> $\\widetilde{\\mathcal{B}_{i j}}=F_{i}^{k} F_{j}^{l} \\mathcal{B}_{k l}$\n",
        "\n",
        "> $\\mathcal{B}_{k l}=B_{k}^{i} B_{l}^{j} \\widetilde{\\mathcal{B}_{i j}}$\n",
        "\n",
        "**So what is the difference between metric tensor and bilinear form?**\n",
        "\n",
        "* the metric tensor is a bilinear form, but it's a very specific example of a bilinear form\n",
        "\n",
        "* the metric tensor has 2 additional properties that other bilinear forms might not have:\n",
        "\n",
        "  1. Metric tensor components are symmetric so we can swap i and j (commutative), hwich means that the order of the input vectors in the metric tensor doesn't matter: $\\color{red}{g_{i j}} = \\color{red}{g_{j i}}$ in here: $g(\\vec{v}, \\vec{w})=v^{i} w^{j} \\color{red}{g_{i j}}=v^{i} w^{j} \\color{red}{g_{j i}}=g(\\vec{w}, \\vec{v})$\n",
        "\n",
        "  2. Metric tensor output must be positive (because it measures length): $g(\\vec{v}, \\vec{v})=\\|\\vec{v}\\|^{2} \\geq 0$\n",
        "\n",
        "* Examples of valid metric tensors (they have symmetric matrices and when we put the vector input twice in, we'll always get answers that are non-negativ):\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}5 & -3 / 4 \\\\ -3 / 4 & 5 / 16\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=5\\left(v^{1}\\right)^{2}+(-6 / 4) v^{1} v^{2}+5 / 16\\left(v^{2}\\right)^{2}$\n",
        "\n",
        "* Examples of Non-metric bilinear forms:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right]$ because it's not symmetric\n",
        "\n",
        "> $\\left[\\begin{array}{cc}1 & -5 \\\\ -5 & 1\\end{array}\\right]$ = $\\|\\vec{v}\\|^{2}=\\left(v^{1}\\right)^{2}+(-10) v^{1} v^{2}+\\left(v^{2}\\right)^{2}$, this is symmetric, but i.e. $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]$ would give a result of -8"
      ],
      "metadata": {
        "id": "g4uy9SPO_zMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector-Pairs (Linear Maps) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$ to map vector to vector within one basis*"
      ],
      "metadata": {
        "id": "V44oHA2a_1hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear maps**\n",
        "\n",
        "1. maps vectors to vectors $L: V \\mapsto W$, or $L: V \\mapsto V$\n",
        "\n",
        "2. Linearity (addition & scaling)\n",
        "\n",
        "* Both covectors and linear maps are functions. The only difference is that covectors output is a scalar, and linear maps output vectors.\n",
        "\n",
        "> **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* Linear maps are linear combinations of vector-covector-pairs $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ with the tensor product $\\overrightarrow{e_{i}} \\epsilon^{j}$ bzw. $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* **Vector changes, basis not: When we transform vectors using a linear map, the basis isn’t changing, we aren’t moving the basis**.\n",
        "\n",
        "  * While the output vector might be different than the input vector, we are still measuring the output vector using the same basis\n",
        "\n",
        "  * With forward and backward transform we modify the vector components when we change from one basis to another, or from one dual basis to another.\n",
        "\n",
        "  * With linear maps now we modify the vector components when we move a vector around within a given basis!\n",
        "\n",
        "* **Coordinate representations of linear maps end up being matrices!**. Column vectors are coordinate representation of vectors. Row vectors are coordinate representation of covectors.\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 7: Linear Maps](https://www.youtube.com/watch?v=dtvM-CzNe50&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=10)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 11: Linear maps are Vector-Covector Pairs](https://www.youtube.com/watch?v=YK2zVcWpROA&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=14)\n",
        "\n",
        "*Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components). Metric tensors are (0,2) tensors, because it transforms using tow covariant rules:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "8erR4QCL_9Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_19.png)"
      ],
      "metadata": {
        "id": "kjVQbCBy__67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Case / Fun fact about how matrices transform vectors:**\n",
        "\n",
        "When you use the column vector (1,0) as an input vector, you get the first column of the matrix as the output (3, -1):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "-1\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "When you use the column vector (0,1) as an input vector, you get the second column of the matrix as the output (-4, 2):\n",
        "\n",
        "> $\\begin{equation}\n",
        "\\left[\\begin{array}{cc}\n",
        "3 & -4 \\\\\n",
        "-1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "-4 \\\\\n",
        "2\n",
        "\\end{array}\\right]\n",
        "\\end{equation}$\n",
        "\n",
        "The column vectors 1, 0 and 0,1 are like copies of the basis vectors e1 and e2. Because: **Linear maps transform input vectors. But linear maps don't transform the basis!**\n",
        "\n",
        "**So when we transform vectors using a linear map, the basis isn’t changing, we aren’t moving the basis**. While the output vector might be different than the input vector, we are still measuring the output vector using the same basis."
      ],
      "metadata": {
        "id": "xEM1uOD7ACQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_20.png)"
      ],
      "metadata": {
        "id": "IFqMZilJAEFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maps - Case 1: Transformations of vectors within one basis (Linear Maps as Tensor Product $\\otimes$ (Vector-Covector-Pairs)**"
      ],
      "metadata": {
        "id": "pg7lwrp9AJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pure & Impure Matrices & how Vector-Covectors-Pairs help**\n",
        "\n",
        "* When we multiply a row vector and a column vector in this order we get a scalar: $\\begin{aligned} &\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right] \\end{aligned}$ $= (2)(3)+(1)(-4) = 6-4 =2$\n",
        "\n",
        "* But if we reverse the order we get a matrix (which is basically a linear map):  $\\left[\\begin{array}{c}3 \\\\ -4\\end{array}\\right]\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]$ = $\\left[\\begin{array}{cc}6 & 3 \\\\ -8 & -4\\end{array}\\right]$\n",
        "\n",
        "* **Can we do this the other way around (get the row and column vector from the matrix)?** (Answer: not always so easy)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_36.png)\n",
        "\n",
        "* Here is the proof why it doesn't work for this matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_37.png)\n",
        "\n",
        "* There are some matrices that can be broken up into column vectors and row vectors, and others not. We call them pure and impure matrices!\n",
        "\n",
        "* Pure matrices are boring when they are used as linear maps, because all the output vectors exist along the same direction.\n",
        "\n",
        "* The reason behind is that the columns of the matrices are all scalable multiples of each other (as shown below). Output vectors point all to the same direction.\n",
        "\n",
        "* The set of transformations a pure matrix can do as linear map is really limited\n",
        "\n",
        "* Impure matrices as linear maps are more interesting because they can send basis vectors into different directions.\n",
        "\n",
        "* **Question: Since impure matrices are more interesting, how can we construct impure matrices using column vector, row vector and products**?\n",
        "\n",
        "* **Solution: We define four special vector-covector-pairs using the old e basis $\\left\\{\\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}\\right\\}$  and the old epsilon dual basis $\\left\\{\\epsilon^{1}, \\epsilon^{2}\\right\\}$** (Hint: this will be the transition from 'classic' linear maps to vector-covector-tensor products with another way of writing them!)\n",
        "\n",
        "* For example the 1 on top left can be written using the column and row vector via $\\overrightarrow{e_{1}} \\epsilon_{1}$:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{1}$\n",
        "\n",
        "And we can do the same for all other 3 matrix components:\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{1}} \\epsilon_{2}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{1}$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}0 \\\\ 1\\end{array}\\right]\\left[\\begin{array}{ll}0 & 1\\end{array}\\right]=\\overrightarrow{e_{2}} \\epsilon_{2}$\n",
        "\n",
        "And you’ll notice that these 4 matrices when taking in linear combination, so when we scale each matrix by a different amount and then add them together, we get any general 2x2 matrix $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$ that we like, just by picking the right scaling numbers a, b, c and d:\n",
        "\n",
        "> $a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "So this set of 4 products forms a basis for all matrices that are linear maps from the vector space $V \\mapsto V$ to itself\n",
        "\n",
        "> $\\left\\{\\overrightarrow{e_{1}} \\epsilon^{1}, \\overrightarrow{e_{1}} \\epsilon^{2}, \\overrightarrow{e_{2}} \\epsilon^{1}, \\overrightarrow{e_{2}} \\epsilon^{2}\\right\\}$ is a basis for $V \\rightarrow V$\n",
        "\n",
        "So any general linear map $L$ can be written as this linear combination if we pick the coefficients right:\n",
        "\n",
        "> $L=a \\overrightarrow{e_{1}} \\epsilon^{1}+b \\overrightarrow{e_{1}} \\epsilon^{2}+c \\overrightarrow{e_{2}} \\epsilon^{1}+d \\overrightarrow{e_{2}} \\epsilon^{2}$\n",
        "\n",
        "And we can summarize this using the Einstein notation, any linear map $L$ can be written using these components\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "> **This is a vector and a covector written together - which is a linear map**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_38.png)\n"
      ],
      "metadata": {
        "id": "adsf_EarAL1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Formula for matrix multiplication**\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]\\left[\\begin{array}{l} \\color{red}x \\\\ \\color{blue}y\\end{array}\\right]=\\left[\\begin{array}{l}a \\color{red}x+b \\color{blue}y \\\\ c \\color{red}x+d \\color{blue}y\\end{array}\\right]$\n",
        "\n",
        "This originates in the following abstract algebraic definition:\n",
        "\n",
        "$L: V \\rightarrow W$\n",
        "\n",
        "$L(\\vec{v}+\\vec{w})=L(\\vec{v})+L(\\vec{w})$\n",
        "\n",
        "$L(n \\vec{v})=n L(\\vec{v})$\n",
        "\n",
        "**How to turn the $\\vec{v}$ coefficients into the $\\vec{w}$ coefficients?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_21.png)\n",
        "\n",
        "\n",
        "**Proof: Derive the matrix multiplication formulas just from the abstract linearity properties of linear maps**\n",
        "\n",
        "If we have a linear map $L$ that transforms a vector v into another vector w (where w can be written as a linear combination of basis vectors)\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=w^{1} \\overrightarrow{e_{1}}+w^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "and we know how $L$ transforms basis vectors (via the L coefficients)\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{1}}\\right)=L_{1}^{1} \\overrightarrow{e_{1}}+L_{1}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{2}}\\right)=L_{2}^{1} \\overrightarrow{e_{1}}+L_{2}^{2} \\overrightarrow{e_{2}}$\n",
        "\n",
        "this means we can transform the v components into  the w components using the formulas below\n",
        "\n",
        "$w^{1}=L_{1}^{1} v^{1}+L_{2}^{1} v^{2}$\n",
        "\n",
        "$w^{2}=L_{1}^{2} v^{1}+L_{2}^{2} v^{2}$.\n",
        "\n",
        "And if we repeat this argument for any number of dimensions, so if we have a linear map $L$ in n-dimensions\n",
        "\n",
        "$\\vec{w}=L(\\vec{v})=\\sum_{i=1}^{n} w^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "We would get all the $L$ coefficients from this formula below\n",
        "\n",
        "$L\\left(\\overrightarrow{e_{i}}\\right)=\\sum_{j=1}^{n} L_{i}^{j} \\overrightarrow{e_{j}}$\n",
        "\n",
        "we can transform the v components into the w components\n",
        "\n",
        "$w^{i}=\\sum_{j=1}^{n} L_{j}^{i} v^{j}$\n",
        "\n",
        "This is the standard matrix multiplication formula for multiplying matrices and vectors together.\n"
      ],
      "metadata": {
        "id": "whZ3s5JiAh4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefit of seeing linear maps as vector-covector-pairs - which are tensor products $\\otimes$**\n",
        "\n",
        "*When we have a linear map acting on a vector, we can get the correct matrix vector component multiplication formula*\n",
        "\n",
        "If you think of some linear map $L$ as a linear combination of these basis linear maps\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "and we have also a vector $\\vec{v}$ which is a linear combination of these basis vectors\n",
        "\n",
        "> $\\vec{v}=v^{k} \\overrightarrow{e_{k}}$\n",
        "\n",
        "What would we get in $\\vec{w}$ with $L$ acts on the input $\\vec{v}$?\n",
        "\n",
        "> $\\vec{w}=L(\\vec{v})$\n",
        "\n",
        "To find out we substitute the components $L$ and $\\vec{v}$ accordingly:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(v^{k} \\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "The $\\epsilon^{j}$ dual basis vector is now acting on the input vector! (and that's what covectors do, they act on vectors).\n",
        "\n",
        "So we use the linearity of $\\epsilon^{j}$ to take out the scaling coefficient $v^{k}$ and put it out in front:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$\n",
        "\n",
        "This leaves us with $\\epsilon^{j}$ acting on $\\left(\\overrightarrow{e_{k}}\\right)$. And by definition this is just a Kronecker delta:\n",
        "\n",
        ">  $\\epsilon^{j}\\left(\\overrightarrow{e_{k}}\\right)$ = $\\delta_{k}^{j}$\n",
        "\n",
        "So we can replace this in the equation:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{k} \\overrightarrow{e_{i}} \\delta_{k}^{j}$\n",
        "\n",
        "And by the kronecker delta cancellation rule, we can cancel out the $k$'s and replace it at $v$ with $j$:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}} \\delta^{j}$\n",
        "\n",
        "And this is our output vector written as a linear combination of the $e$ basis vectors:\n",
        "\n",
        "> $\\vec{w}= L_{j}^{i} v^{j} \\overrightarrow{e_{i}}$\n",
        "\n",
        "And we get these coefficients:\n",
        "\n",
        "> $L_{j}^{i} v^{j}$\n",
        "\n",
        "from the standard matrix multiplication rule:\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\vec{w}=L(\\vec{v}) \\\\\n",
        "w^{i}=L_{j}^{i} v^{j}\n",
        "\\end{array}$\n",
        "\n",
        "So from our equation above:\n",
        "\n",
        "> $L=L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "**we can see that this product pair is really a linear map**:\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "It took an input vector, transformed it, and gave us an output vector.\n",
        "\n",
        "> **Proof: vector-covector-pairs are linear maps - which are tensor products $\\otimes$  !**\n",
        "\n",
        "* These are pure matrices, so you get the boring linear maps! To get the more interesting linear maps (the impure matrices!), we need to combine a bunch of pure linear maps together in linear combination, which helps us to get more interesting impure linear maps like $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right]$\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] = a\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]+b\\left[\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right]+c\\left[\\begin{array}{ll}0 & 0 \\\\ 1 & 0\\end{array}\\right]+d\\left[\\begin{array}{ll}0 & 0 \\\\ 0 & 1\\end{array}\\right]$\n",
        "\n",
        "* **And finally this vector-covector-pair is actually a tensor product**\n",
        "\n",
        "> $ \\overrightarrow{e_{i}} \\epsilon^{j}$ normally written like this: $\\overrightarrow{e_{i}} \\otimes \\epsilon^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_39.png)"
      ],
      "metadata": {
        "id": "bv6NmiXzAOBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two ways of multiplying a row vector with a column vector: classic way (left) and tensor product (right - much better)**\n",
        "\n",
        "\n",
        "* The $\\otimes$ operator tells us to take the array on the left and distribute it to each of the components inside the array on the right.\n",
        "\n",
        "* This means take the column vector on the left and distribute a copy to each element inside the second array. And you get a row of columns (bottom right), which is basically like a matrix.\n",
        "\n",
        "* **This means that linear maps are rows of columns!**\n",
        "\n",
        "* Picture below: On the left is the old way (row vector & column vector both to describe vectors) and on the right side is the new better way with tensor products $\\otimes$\n",
        "\n",
        "* the coefficients of the linear map are just the entries of an array given by the Kronecker delta of the column vector representing the vector and the row vector representing the covector\n",
        "\n",
        "* (meanwhile combining two covectors using the tensor product can gives us a bilinear form whose coefficients are just the entries of the array given by the Kronecker product of the two row vectors associated with the covectors)\n",
        "\n",
        "* **The tensor product for linear maps gave a great change of perspective**. This idea of distributing arrays into each other will turn out to be very useful later on!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_41.png)"
      ],
      "metadata": {
        "id": "bsHsugHiAQoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector-Covector Pairs (Linear Maps) + Forward and Backward Transforms (Vectors) $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$ to map vector to vector across two bases*"
      ],
      "metadata": {
        "id": "VcEin6DXApdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **We need transformation rules for linear maps when we are moving a vector AND going from one basis to another**, because linear maps are only working within one basis (move vectors around within one basis), we need to find the coefficients of the linear map in the new basis when we change basis.\n",
        "\n",
        "* Task: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 8: Linear Map Transformation Rules](https://www.youtube.com/watch?v=SSSGA6ohkfw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=11)\n",
        "\n",
        "**How do we do this?**\n",
        "\n",
        "If you take vector components $\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ from the vector $\\vec{v}$ in the original basis $\\overrightarrow{e_{i}}$\n",
        "\n",
        "1. **apply the linear map** $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$, you get the new vector components $L(\\vec{v})$ $=\\left[\\begin{array}{c}1 / 2 \\\\ 2\\end{array}\\right]_{\\overrightarrow{e_{i}}}$ that is still **in the same original basis** $\\overrightarrow{e_{i}}$.\n",
        "\n",
        "2. then **apply the backward transform** with the matrix $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$, you get the new vector components for $\\vec{v}$ $\\left[\\begin{array}{l}3/4 \\\\ 1\\end{array}\\right]_{\\widetilde{e_{i}}}$ **in the new basis** ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "**We use the backward transform from old to new, since vector components behave contravariant !!**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_22.png)"
      ],
      "metadata": {
        "id": "nrdE_XbCA5ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you get the new vector components $L(\\vec{v})$ in the new basis ${\\widetilde{e_{i}}}$?**\n",
        "\n",
        "* Answer: We need to find the coefficients of the linear map ${\\widetilde{L_{j}^{q}}}$ in the new basis ${\\widetilde{e_{i}}}$.\n",
        "\n",
        "* Again: how do you get the new vector components $L(\\vec{v})$ (the transformed vector with the linear map $L$) in the new basis ${\\widetilde{e_{i}}}$, and not only the components of $\\vec{v}$ (the original vector) in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* Said differently: what are the components of the output vector in the new basis ${\\widetilde{e_{i}}}$?\n",
        "\n",
        "* we cannot apply the linear map $L$ $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}$ since it's only valid in the original basis\n",
        "\n",
        "* **We need to find a new matrix ${\\widetilde{L}}$ in the new basis ${\\widetilde{e_{i}}}$ that tells us how to build output vectors using the ${\\widetilde{e_{1}}}$ and ${\\widetilde{e_{2}}}$ basis vectors.**\n",
        "\n",
        "* This means we need to find the ${\\widetilde{L_{j}^{q}}}$ coefficients:\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{1}}}\\right)=\\widetilde{L_{1}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{1}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "  * $L\\left(\\color{red}{\\widetilde{e_{2}}}\\right)=\\widetilde{L_{2}^{1}} \\widetilde{e_{1}}+\\widetilde{L_{2}^{2}} \\widetilde{e_{2}}$\n",
        "\n",
        "**How to get the components of the output vector in the new basis ${\\widetilde{e_{i}}}$:**\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\color{red}{\\widetilde{e_{i}}}\\right)$\n",
        "\n",
        "Let's first use the forward transform $\\widetilde{e_{i}}=\\sum_{j=1}^{n} F_{i}^{j} \\overrightarrow{e_{j}}$ to rewrite the new basis vectors in terms of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=L\\left(\\sum_{j=1}^{n} F_{i}^{j} \\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use the linearity of $L$ to take the scale and sum coefficients outside the function:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$\n",
        "\n",
        "Now we use this definition $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)=\\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$ to write the output $L\\left(\\color{blue}{\\overrightarrow{e_{j}}}\\right)$ as linear combination of the old basis vectors:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} F_{i}^{j} \\sum_{k=1}^{n} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Then we re-arrange the sums a bit:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\color{blue}{\\overrightarrow{e_{k}}}$\n",
        "\n",
        "Now we write the old basis in terms of the new basis vectors with $\\color{blue}{\\overrightarrow{e_{k}}}=\\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{\\overrightarrow{e_{l}}}}$ using the bckward transform:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} \\sum_{l=1}^{n} B_{k}^{l} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "We re-arrange the sums again:\n",
        "\n",
        "> $\\sum_{q=1}^{n} \\widetilde{L_{i}^{q}} \\color{red}{\\widetilde{e_{q}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "* So on the left we have a linear combination of ${\\widetilde{e}}$ basis vectors with the summation index $q$.\n",
        "\n",
        "* So on the right we have a linear combination of ${\\widetilde{e}}$ basis vectors again but with the summation index $l$.\n",
        "\n",
        "* So we have a linear combination of ${\\widetilde{e}}$ basis vectors on both sides.\n",
        "\n",
        "But with different summation indexes. But the choice doesn't really matter. So we change all the $q$ with $l$:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\widetilde{L_{i}^{l}} \\color{red}{\\widetilde{e_{l}}}=\\sum_{l=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j} \\color{red}{\\widetilde{e_{l}}}$\n",
        "\n",
        "Now we see that the $\\widetilde{L_{i}^{l}}$ coefficients on the left side are equal to this part $\\sum_{j=1}^{n} \\sum_{k=1}^{n} F_{i}^{j} L_{j}^{k} B_{k}^{l}$ in the middle of the right equation:\n",
        "\n",
        "> $\\sum_{l=1}^{n} \\color{pink}{\\widetilde{L_{i}^{l}}} \\widetilde{e_{l}}=\\sum_{l=1}^{n} \\color{pink}{\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}} \\widetilde{e_{l}}$\n",
        "\n",
        "**This is saying that to transform the matrix coordinates from the old basis to the new basis we multiply the old matrix $L_{j}^{k}$ by the backward transform $B$ on the left and by the forward transform $F$ on the right:**\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "**This is what we just did (explanation below):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)"
      ],
      "metadata": {
        "id": "l4kEzo39A7c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUMMARY: How to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis)**\n",
        "\n",
        "* So, matrices, or linear maps, transform with both the forward transform and the backward transform. Here is why:\n",
        "\n",
        "* You have two ways to get the new basis vector component: you can use $\\widetilde{L}$ matrix n the bottom to go from left to right directly $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$\n",
        "\n",
        "* But it's the same thing as going the other way around:\n",
        "\n",
        "  1. To transform the new vector components into the old vector components you use the forward transform\n",
        "\n",
        "  2. And here to transform the components of the input vector into components of the output vector in the old basis, we just use the matrix $L$\n",
        "\n",
        "  3. And finally to get from the old vector components to the new vector components for the output vector we use the backward transform\n",
        "\n",
        "  4. Now we have the components of the new basis vector $\\left[\\begin{array}{l}? \\\\ ?\\end{array}\\right]_{\\widetilde{e_{j}}}$ with which you can describe any vector in the new vector space.\n",
        "\n",
        "* So the idea of transforming matrix components using both the forward and backward transformations makes sense.\n",
        "\n",
        "**Let's check this on our example from above:**\n",
        "\n",
        "This is the equation again to get from the new vector components in the old basis to the new vector components in the new basis (via old vector components in old basis, then old vector components in new basis):\n",
        "\n",
        "> $\\widetilde{L}_{i}^{l}=\\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "* Backward Transform $B$: $\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]$\n",
        "\n",
        "* Linear Map $L$ in old basis ${\\overrightarrow{e_{j}}}$: $\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\overrightarrow{e_{j}}}$\n",
        "\n",
        "* Forward Transform $F$: $\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "Now let's place them all in the equation to get the linear map in the new basis:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 4 & 1 / 2 \\\\ -1 & 2\\end{array}\\right]\\left[\\begin{array}{cc}1 / 2 & 0 \\\\ 0 & 2\\end{array}\\right]_{\\vec{e}_{j}}\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}1 / 8 & 1 \\\\ -1 / 2 & 4\\end{array}\\right]\\left[\\begin{array}{cc}2 & -1 / 2 \\\\ 1 & 1 / 4\\end{array}\\right]$\n",
        "\n",
        "We get this matrix the:\n",
        "\n",
        "> $L_{\\widetilde{e}_{j}}=\\left[\\begin{array}{cc}5 / 4 & 3 / 16 \\\\ 3 & 5 / 4\\end{array}\\right]$\n",
        "\n",
        "And this matrix above tells us how to write the outputs of the linear map as linear combination of the new basis:\n",
        "\n",
        "> $L\\left(\\widetilde{e_{1}}\\right)=5 / 4 \\widetilde{e_{1}}+3 \\widetilde{e_{2}}$\n",
        "\n",
        "> $L\\left(\\widetilde{e_{2}}\\right)=3 / 16 \\widetilde{e_{1}}+5 / 4 \\widetilde{e_{2}}$\n",
        "\n",
        "**Results are correct, the new matrix / linear map in the new basis is converting the vector properly from its original position to the new position (measuring both at the new basis vectors):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_24.png)"
      ],
      "metadata": {
        "id": "qozTbjNIA9Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward & Backward Transform of Linear Maps between different basis**\n",
        "\n",
        "* A tensor is an object that is invariant under a change of coordinates, and **has components that change in a special, predictable way under a change of coordinates**. The way we transform them is by a applying a series of forward and backward transforms!\n",
        "\n",
        "**Forward Transform (going from old $T$ to new $\\widetilde{T}$)**\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **backward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **forward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Backward Transformation (going from new $\\widetilde{T}$ to old $T$)**\n",
        "\n",
        "> $T_{r s t \\ldots}^{i j k \\ldots}=\\left(F_{\\color{red}a}^{i} F_{\\color{red}b}^{j} F_{\\color{red}c}^{k} \\cdots\\right) \\widetilde{T_{\\color{blue}{x y z} \\ldots}^{\\color{red}{a b c} \\ldots}}\\left(B_{r}^{\\color{blue}x} B_{s}^{\\color{blue}y} B_{t}^{\\color{blue}z} \\cdots\\right)$\n",
        "\n",
        "* all the upstairs indices $\\color{red}{i, j, k}$ will transform using the **forward transformation** in B on the bottom, because upstairs are the **contravariant components**.\n",
        "\n",
        "* the downstairs indices $\\color{blue}{r, s, t}$ will transform using the **backward transformation**, because downstairs are the **covariant components**.\n",
        "\n",
        "**Illustration from the linear maps part that helps to understand the $FTB$ = $\\widetilde{T}$ (oben) bzw. $FLB$ = $\\widetilde{L}$ (unten) transform:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_23.png)\n",
        "\n",
        "> $\\widetilde{T_{x y z \\ldots}^{a b c \\ldots}}=\\left(B_{\\color{red}i}^{a} B_{\\color{red}j}^{b} B_{\\color{red}k}^{c} \\cdots\\right) T_{\\color{blue}{r s t} \\ldots}^{\\color{red}{i j k} \\ldots}\\left(F_{x}^{\\color{blue}r} F_{y}^{\\color{blue}s} F_{z}^{\\color{blue}t} \\cdots\\right)$ (forward transform)\n",
        "\n",
        "**How many contravariant and covariant rules we need to follow during a transformation?** - Core Question when it gets more complicated with more basis & dual basis !!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_32.png)"
      ],
      "metadata": {
        "id": "5cOVS7o9BBX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "_AUtbuZDBDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Tensors (Any Vector-Covector combination) $L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$*"
      ],
      "metadata": {
        "id": "457rg_0-BFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Maps vs Bilinear Forms**\n",
        "\n",
        "* **Bilinear Forms: Linear combinations of covector-covector-pairs** $\\mathcal{B}=\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}=\\mathcal{B}_{i j}\\left(\\epsilon^{i} \\otimes \\epsilon^{j}\\right)$ (including metric tensor)\n",
        "\n",
        "* **Linear Maps: Linear combinations of vector-covector-pairs** $L=L_{j}^{i} (\\overrightarrow{e_{i}} \\otimes \\epsilon^{j})$\n",
        "\n",
        "* [Eigenchris: Tensors for Beginners 14: Tensors are general vector/covector combinations](https://www.youtube.com/watch?v=9R4vhqvE_jw&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=17)\n",
        "\n",
        "* **A tensor product takes two tensors and produces a new tensor:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_44.png)"
      ],
      "metadata": {
        "id": "m-CmxPR5BUhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take two new tensor**\n",
        "\n",
        "This is a (2,0) tensor\n",
        "\n",
        "> $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "This is a (1,2) tensor\n",
        "\n",
        "> $Q=Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\epsilon^{k}$\n",
        "\n",
        "**Now we can ask the same questions:**\n",
        "\n",
        "1. What are the coordinate transform rules?\n",
        "2. What is the multiplication formula for $Q$ ($D$)?\n",
        "3. What are the array shapes?\n",
        "\n",
        "**1. How to $D$ and $Q$ under a change of basis?**\n",
        "\n",
        "* Transforming tensor components is not hard, long as you have the transformation rules for basis vectors and covectors (plug backward transform in here for example twice on the left side)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_50.png)\n",
        "\n",
        "**2. What is the formula for $Q$ acting on the input $D$ = $Q(D)$**\n",
        "\n",
        "* This is tricky because there is no one way to do that.\n",
        "\n",
        "* **The challenge is now that: As we make these tensors bigger and bigger with more and more covariant and contravariant parts, we end up with more and more ways to do the summations, and more and more ways to compute functions.**\n",
        "\n",
        "* Writing $D$ = $Q(D)$ is ambuguous as it doesn't tell us exactly what to do\n",
        "\n",
        "* so we need to write it out in the Einstein notation like on the left side, for example $Q_{j k}^{i} D^{j k}$\n",
        "\n",
        "* Some examples are:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_51.png)\n",
        "\n",
        "**3. What is the array shape?**\n",
        "\n",
        "* It's like the Kronecker product between two column vectors: like this part $\\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$  in this here: $D=D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$\n",
        "\n",
        "* We can do this for the first example from above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_52.png)\n",
        "\n",
        "* For the other example it would look like this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_53.png)\n",
        "\n",
        "* One could use the 3d visualisation, but it's better in the matrix array way. Because looking at this you can see it’s a (1,2) tensor with one column aspect and two row aspects\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_54.png)"
      ],
      "metadata": {
        "id": "rXmd5aWCBWWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Array Multiplication**\n",
        "\n",
        "* Easy for smaller tensors\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_55.png)\n",
        "\n",
        "* That's not so easier for larger tensors that have high type numbers, because there are several possible multiplication rules (as discussed earlier):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_56.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_57.png)\n",
        "\n",
        "* For much more complex tensors, it's the easiest to just stick with the Einstein notation and also not do the array representation.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_58.png)\n",
        "\n",
        "* So with high type tensors the abstract notation and the array notation have their limitations. When trying to express tensor multiplication formulas it’s usually easier just to stick with the Einstein component notation. For this reason a lot of sources just write tensors like this and leave out basis vector and covectors completely:\n",
        "\n",
        "> $Q_{j k}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$ $\\mapsto$ $Q_{j k}^{i}$\n",
        "\n",
        "> $D^{a b} \\overrightarrow{e_{a}} \\overrightarrow{e_{b}}$ $\\mapsto$ $D=D^{a b}$\n",
        "\n",
        "* **But it's important to remember that tensor components always come from a choice of basis, and the same tensor can have different components if we choose to represent it in a different basis.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_59.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_60.png)"
      ],
      "metadata": {
        "id": "mGPcQ4POBYNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Product $\\otimes$ of Vector Spaces (Hilbert Spaces)*"
      ],
      "metadata": {
        "id": "vq-kSppvBbU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "These vectors are element of the vector space ${V}$\n",
        "\n",
        "> $\\vec{v}, \\vec{w}, \\overrightarrow{e_{1}}, \\overrightarrow{e_{2}}$ $\\in {V}$\n",
        "\n",
        "These covectors are element of the dual vector space ${V^{*}}$\n",
        "\n",
        "> $\\alpha, \\beta, \\epsilon^{1}, \\epsilon^{2}$ $\\in {V^{*}}$\n",
        "\n",
        "We can make following vector-covector-pairs:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "And we can add them and scale with them with the tensor product rules which forms a vector space:\n",
        "\n",
        "> $n(\\vec{v} \\otimes \\alpha)=(n \\vec{v}) \\otimes \\alpha=\\vec{v} \\otimes(n \\alpha)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{v} \\otimes \\beta=\\vec{v} \\otimes(\\alpha+\\beta)$\n",
        "\n",
        "> $\\vec{v} \\otimes \\alpha+\\vec{u} \\otimes \\alpha=(\\vec{v}+\\vec{u}) \\otimes \\alpha$\n",
        "\n",
        "So we know that these must be vectors in a vectors space:\n",
        "\n",
        "> $\\vec{v} \\alpha, \\vec{v} \\beta, \\vec{w} \\alpha, \\vec{w} \\beta, \\overrightarrow{e_{1}} \\epsilon^{2}, L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j}$\n",
        "\n",
        "But in which vector space do they live in? In this one:\n",
        "\n",
        "> $\\in V \\otimes V^{*}$\n",
        "\n",
        "**Now this is a new use case of $\\otimes$, because so far we used it for combining vectors or tensors. Here we use it to combine entire vector spaces.** (more about it below in a short separat chapter)"
      ],
      "metadata": {
        "id": "gr-jRSA3I5Ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V \\otimes V^{*}$?** (1,1)-tensors\n",
        "\n",
        "\n",
        "> **(1,1)-Tensors**: $L_{j}^{i} \\overrightarrow{e_{i}} \\epsilon^{j} \\in V \\otimes V^{*}$\n",
        "\n",
        "Remember: vectors have an upstairs index (because they transform contravariant):\n",
        "\n",
        "> $\\vec{v}=v^{i} \\overrightarrow{e_{i}}$\n",
        "\n",
        "and covectors habe a downstairs index (because they transform covariant):\n",
        "\n",
        "> $\\alpha=\\alpha_{i} \\epsilon^{i}$\n",
        "\n",
        "\n",
        "\n",
        "So if we do a summation with vector components like this over $j$, we end up with vector components as the output (because we have one upstairs $i$ index that's left):\n",
        "\n",
        "> $L_{j}^{i} v^{j}=w^{i}$\n",
        "\n",
        "In this case $L$ ist acting as a map from $V$ to $V$, or essentially a linear map.\n",
        "\n",
        "> $V \\mapsto V$\n",
        "\n",
        "> **This is a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a summation with covector components like this with sum over $i$, and our outout would have $j$ as the dowstairs index:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i}=\\beta_{j}$\n",
        "\n",
        "We would end up with covector components as the output. So covector in, covector out:\n",
        "\n",
        "> $V^{*} \\mapsto V^{*}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "Also we can provide $L$ with both vector components and covector components and we do two summations over $i$ and $j$, and that would give us a scalar as the output since there are no indices left to sum over.\n",
        "\n",
        "> $L_{j}^{i} v^{j} \\alpha_{i}=s$\n",
        "\n",
        "So in this case $L$ can be viewed as a function from a pair of vectors and covectors to scalars.\n",
        "\n",
        "> $V \\times V^{*} \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**\n",
        "\n",
        "And finally we can do the same thing but reverse the order of the inputs:\n",
        "\n",
        "> $L_{j}^{i} \\alpha_{i} v^{j}=s$\n",
        "\n",
        "And in this case $L$ is a function from a covector-vector-pair to scalar:\n",
        "\n",
        "> $V^{*} \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is also a (1,1)-Tensor and a member of the vector space $V \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "4bn8DJW0I9YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So what are the elements of $V^{*} \\otimes V^{*}$?** (0,2)-tensors\n",
        "\n",
        "**Now what if we use the tensor product to combine two covectors together?**\n",
        "\n",
        "Those covectors life in $V^{*}$:\n",
        "\n",
        "> $\\alpha, \\beta, \\gamma, \\delta, \\epsilon^{1}, \\epsilon^{2}$ $\\in V^{*}$\n",
        "\n",
        "So when we have covector-covector-pairs like following it turns out that they all life in the vector space **$V^{*} \\otimes V^{*}$**\n",
        "\n",
        "> $\\alpha \\beta, \\alpha \\gamma, \\delta \\beta, \\epsilon^{1} \\epsilon^{2}, \\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j}$ **$\\in V^{*} \\otimes V^{*}$**\n",
        "\n",
        "**So elements of the $V^{*} \\otimes V^{*}$are (0,2)-tensors**\n",
        "\n",
        "> $\\mathcal{B}_{i j} \\epsilon^{i} \\epsilon^{j} \\in V^{*} \\otimes V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "(Covector-covector-pairs and their linear combinations)\n",
        "\n",
        "If we take these $B$ components and do two summations with two sets of vector components (via $i$ and $j$), then we end up with a scalar:\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i} w^{j}=s$\n",
        "\n",
        "And this of course is a bilinear form (which takes a pair of vectors and outputs a scalar):\n",
        "\n",
        "> $V \\times V \\rightarrow \\mathbb{R}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "But we can also do a single summation over $i$ with a set of vector components and we’d be left with the index $j$ downstairs. So the output would be a set of covector components\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{i}=\\alpha_{j}$\n",
        "\n",
        "So in this case $B$ is a map from vectors to covectors:\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**\n",
        "\n",
        "Also we could choose to do summation with vector components over the $j$ index and then end up with covector components $i$\n",
        "\n",
        "> $\\mathcal{B}_{i j} v^{j}=\\beta_{i}$\n",
        "\n",
        "This would be another map from $V$ to $V^{*}$, but it would be a different map than the previous one, because we’re doing the summation differently.\n",
        "\n",
        "> $V \\rightarrow V^{*}$\n",
        "\n",
        "> **This is a (0,2)-Tensor and a member of the vector space $V^{*} \\otimes V^{*}$**"
      ],
      "metadata": {
        "id": "AZ-VNTnoI_JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create even larger vector spaces from basic building blocks $V$ and $V^{*}$**\n",
        "\n",
        "**So we can create larger and larger vector spaces out of the two basic building blocks $V$ and $V^{*}$**:\n",
        "\n",
        "* starting with two vector spaces $V$ and $V^{*}$ (basic building blocks)\n",
        "\n",
        "  * they contain tensors $v_{i}$ and $\\alpha_{j}$\n",
        "\n",
        "  * with vector components with upstairs index and covector components with downstairs index\n",
        "\n",
        "* we can combine these 2 vector spaces into new vector spaces using the tensor product $V \\otimes V$, $V \\otimes V^{*}$, $V^{*} \\otimes V$ and $V^{*} \\otimes V^{*}$\n",
        "\n",
        "  * and these vectors spaces have following vector components: $\\mathcal{A}^{i j}$, $L_{j}^{I}$, $L_{j}{ }^{i}$ and $\\mathcal{B}_{i j}$\n",
        "\n",
        "  * Indexes from $V$ go upstairs and index from $V^{*}$ go downstairs\n",
        "\n",
        "* And we can continue to make larger and larger vector spaces using the tensor product like $V \\otimes V \\otimes V$, $V^{*} \\otimes V \\otimes V$ etc\n",
        "\n",
        "  * And all these vector spaces contain tensors like $T^{i j k}$, $T_{i}^{j k}$, $T_{j}^{i k}$ etc.\n",
        "\n",
        "  * with components that have different combinations of upstairs and downstairs indexes depending on whether they are constructed using $V$ or $V^{*}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_65.png)"
      ],
      "metadata": {
        "id": "npgLhzSYJA3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So if we have some new tensor $T$ from a vector space we’ve never seen before, we can easily get the correct component indexes just by looking at the vector spaces.**\n",
        "\n",
        "> $T \\in V^{*} \\otimes V \\otimes V^{*} \\otimes V^{*}$\n",
        "\n",
        "Looking at these vector spaces we see that the basis would be made up of a covector, a vector, a covector and another covector in combination\n",
        "\n",
        "> $T={\\epsilon}^{i}  \\overrightarrow{e_{j}} \\epsilon^{k} {\\epsilon}^{l}$\n",
        "\n",
        "And we get the components just by **placing the indexes in the opposite position that we see in the basis**, so that all the summations work out properly.\n",
        "\n",
        "**And now we can ask: How can this tensor $T$ act on other tensors?**\n",
        "\n",
        "> $T_{i}^{j}{ }_{k l}$\n",
        "\n",
        "examples of other tensors to be acted on:\n",
        "\n",
        "> $u^{c} \\quad \\begin{array}{cc}D^{f g} & \\beta_{s} \\\\ Q_{u v}^{t} & L_{y}^{x} & w^{b} & U^{m n o}\\end{array}$\n",
        "\n",
        "**Well, we can basically do any summations we like as long as the upstairs indexes are matched with downstairs indexes and downstairs indexes are matched with upstairs indexes.**\n",
        "\n",
        "We could do something like this with four summations and you can see that all the indexes are positioned properly:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensors_69.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_68.png)"
      ],
      "metadata": {
        "id": "HEU05oWyJCrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is how the result for each would look like** [see explanation](https://youtu.be/M-OLmxuLdbU?list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&t=683)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_70.png)\n",
        "\n",
        "The tensors we get from these tensor product form new vector spaces:\n",
        "\n",
        "> $V \\otimes V$\n",
        "\n",
        "> $V \\otimes V^{*}$\n",
        "\n",
        "> $V^{*} \\otimes V$\n",
        "\n",
        "> $V^{*} \\otimes V^{*}$\n"
      ],
      "metadata": {
        "id": "LU6hlHpSJEt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Tensor Products $\\otimes$ in Quantum Mechanics*#"
      ],
      "metadata": {
        "id": "F4gfVMumBdd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two systems being described as a joint system:**\n",
        "\n",
        "1. the structure of the two systems is pre- served:\n",
        "\n",
        "2. a measurement on one of the systems does not disturb the other one;\n",
        "\n",
        "3. maximal information obtained on both systems separately gives maximal information on the joint system.\n",
        "\n",
        "With these conditions we show, within the framework of the propositional system formalism, that\n",
        "\n",
        "* if the systems are classical the joint system is described by the cartesian product of the corresponding phase spaces, and\n",
        "\n",
        "* if the systems are quantal the joint system is described by the tensor product of the corresponding Hilbert spaces.\n",
        "\n",
        "[Source: Paper Aerts](https://raw.githubusercontent.com/deltorobarba/papers/master/aerts.pdf)\n",
        "\n",
        "**One should deal with at least two operator products:**\n",
        "\n",
        "* one is given by the composition of two operators defined on the same vector space = product yields an operator de!ned on the common vector space of the factor operators\n",
        "\n",
        "* the other is the direct or tensor product of operators = leads to an operator de!ned on a different vector space: the direct or tensor product of the vector spaces of the factor operators\n",
        "\n",
        "The Kronecker product and some of its physical applications (Francisco M Fernández)"
      ],
      "metadata": {
        "id": "fZWP-A5kPUTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider two distinguishable particles:\n",
        "\n",
        "* Particle 1: its quantum mechanics is described by a complex vector space V. It has associated operators T1, T2, ..\n",
        "\n",
        "* Particle 2: its quantum mechanics is described by a complex vector space W. It has associated operators S1, S2, ..\n",
        "\n",
        "*This list of operators for each particle may include some or many of the operators you are already familiar with: position, momentum, spin, Hamiltonians, projectors, etc.*\n",
        "\n",
        "**Once we have two particles, the two of them together form our system.**\n",
        "\n",
        "* We are after the description of quantum states of this two-particle system.\n",
        "\n",
        "* On first thought, we may think that any state of this system should be described by giving the state v ∈ V of the first particle and the state w ∈ W of the second particle.\n",
        "\n",
        "* This information could be represented by the ordered list (v, w) where the first item is the state of the first particle and the second item the state of the second particle. This is a state of the two-particle system, but it is far from being the general state of the two-particle system. It misses remarkable new possibilities, as we shall soon see.\n",
        "\n",
        "We thus introduce a new notation. Instead of representing the state of the two-particle system with particle one in $v$ and particle two in $w$ as $(v, w)$, **we will represent it as $v \\otimes w$**.\n",
        "\n",
        "* <font color=\"blue\">This element $v \\otimes w$ will be viewed as a vector in a new vector space $V \\otimes W$ that will carry the description of the quantum states of the system of two particles.</font>\n",
        "\n",
        "* <font color=\"blue\">This $\\otimes$ operation is called the \"tensor product.\" In this case we have two vector spaces over $\\mathbb{C}$ **and the tensor product $V \\otimes W$ is a new complex vector space**:</font>\n",
        "\n",
        "> $v \\otimes w \\in V \\otimes W \\quad$ when $\\quad v \\in V, w \\in W$\n",
        "\n",
        "$\\operatorname{In} v \\otimes w$ there is no multiplication to be carried out, we are just placing one vector to the left of $\\otimes$ and another to the right of $\\otimes$.\n",
        "\n",
        "We have only described some elements of $V \\otimes W$, not quite given its definition yet. We now explain two physically motivated rules that define the tensor product completely.\n"
      ],
      "metadata": {
        "id": "3aRUE2W-PW9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Rule 1</u>: If the vector representing the state of the first particle is scaled by a complex number this is equivalent to scaling the state of the two particles. The same for the second particle. So we declare:**</font>\n",
        "\n",
        "> <font color=\"blue\">$(a v) \\otimes w=v \\otimes(a w)=a(v \\otimes w), \\quad a \\in \\mathbb{C}$\n",
        "\n",
        "* like in tensor algebra the linear map bzw. bilinear map, where a factor is attached only to one of two, not both\n",
        "\n",
        "<font color=\"blue\">**<u>Rule 2</u>: If the state of the first particle is a superposition of two states, the state of the two-particle system is also a superposition. We thus demand distributive properties for the tensor product:**</font>\n",
        "\n",
        "> <font color=\"blue\">$\\left(v_{1}+v_{2}\\right) \\otimes w=v_{1} \\otimes w+v_{2} \\otimes w$\n",
        "\n",
        "> <font color=\"blue\">$v \\otimes\\left(w_{1}+w_{2}\\right)=v \\otimes w_{1}+v \\otimes w_{2}$\n",
        "\n",
        "Example in Quantum Phase Estimation: We first perform a Hadamard gate on the first qubit to get the state and then distribute the superposition (omitted the normalization factor of 1/√2 for clarity):\n",
        "\n",
        "  * Original state of both qubits: $|0\\rangle \\otimes|\\psi\\rangle$\n",
        "\n",
        "  * Hadamard on first qubit: $|+\\rangle \\otimes|\\psi\\rangle$ =\n",
        "\n",
        "  * <font color=\"red\">Distribute superposition: $|0\\rangle|\\psi\\rangle+|1\\rangle|\\psi\\rangle$</font>\n",
        "\n",
        "> The tensor product $V \\otimes W$ is thus defined to be the vector space whose elements are **(complex) linear combinations** of elements of the form $v \\otimes w$, with $v \\in V, w \\in W$, with the above rules for manipulation. The tensor product $V \\otimes W$ is the complex vector space of states of the two-particle system!"
      ],
      "metadata": {
        "id": "2hHr5XVJPZLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment: Let $v_{1}, v_{2} \\in V$ and $w_{1}, w_{2} \\in W$. A vector in $V \\otimes W$ constructed by superposition is**\n",
        "\n",
        "> $\n",
        "\\alpha_{1}\\left(v_{1} \\otimes w_{1}\\right)+\\alpha_{2}\\left(v_{2} \\otimes w_{2}\\right) \\in V \\otimes W\n",
        "$\n",
        "\n",
        "<font color=\"blue\">This shows clearly that a general state of the two-particle system cannot be described by stating the state of the first particle and the state of the second particle</font>. The above superpositions give rise to entangled states. An entangled state of the two particles is one that, roughly, **cannot be disentangled into separate states of each of the particles**.\n",
        "\n",
        "*Explanation 1*\n",
        "\n",
        "* **The \"Kronecker product\", better known as the tensor product**, is the natural notion of a product for spaces of states, when these are considered properly:\n",
        "\n",
        "* **A space of states is not a Hilbert space $\\mathcal{H}$, but the projective Hilbert space $\\mathbb{P} \\mathcal{H}$ associated to it**. This is the statement that quantum states are rays in a Hilbert space.\n",
        "\n",
        "* <font color=\"blue\">Now, **why does the physical notion of combining the spaces of states of individual systems into a space of states of the combined system correspond to taking the tensor product?**</font>\n",
        "\n",
        "  * The reason is that <font color=\"blue\">**we want every action of an operator (which are linear maps) on the individual states to define an action on the combined state**</font>\n",
        "\n",
        "  * and the tensor product is exactly that, since, <font color=\"red\">for every pair of linear maps $T_{i}: \\mathcal{H}_{i} \\rightarrow \\mathcal{H}$ (which is a bilinear map $\\left(T_{1}, T_{2}\\right): \\mathcal{H}_{1} \\times \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$ ) there is a unique linear map $T_{1} \\otimes T_{2}: \\mathcal{H}_{1} \\otimes \\mathcal{H}_{2} \\rightarrow \\mathcal{H}$</font>\n",
        "\n",
        "* Alternatively, <font color=\"blue\">**concentrating more on the projective nature of the spaces of states, we observe that $|\\psi\\rangle$ and $a|\\psi\\rangle$ are the same state for any $a \\in \\mathbb{C}$.**</font> Therefore, denoting the sought-for physical product by $\\otimes$ (i.e. not assuming it is the tensor product), we must demand that\n",
        "\n",
        "><font color=\"red\">$\n",
        "|\\psi\\rangle \\otimes|\\phi\\rangle=(a|\\psi\\rangle) \\otimes|\\phi\\rangle=a(|\\psi\\rangle \\otimes|\\phi\\rangle)\n",
        "$</font>\n",
        "\n",
        "* since the states produced by $|\\psi\\rangle$ and $a|\\psi\\rangle$ must yield the same state, i.e. map onto the same projective state. **This obviously fails for the cartesian product, since the pair $(a|\\psi\\rangle,|\\phi\\rangle)$ <u>is not a multiple</u> of the $\\operatorname{pair}(|\\psi\\rangle,|\\phi\\rangle)$, but it is true for the tensor product**.\n",
        "\n",
        "\n",
        "https://physics.stackexchange.com/questions/148378/importance-of-kronecker-product-in-quantum-computation\n",
        "\n",
        "*Explanation 2*\n",
        "\n",
        "* In der Mathematik können mit Hilfe des Tensorprodukts (Kronecker-Produkts) von PauliMatrizen (mit Einheitsmatrix) die Darstellungen der höheren Clifford-Algebren über den reellen Zahlen aufgebaut werden.\n",
        "\n",
        "* Pauli-Matrizen können zur Darstellung von Hamilton-Operatoren und zur Näherung der Exponentialfunktion solcher Operatoren verwendet werden. Sind $\\sigma_{0}, \\sigma_{1}, \\sigma_{2}, \\sigma_{3}$ die vier Pauli-Matrizen, so kann man mit Hilfe des Kronecker-Produkt höherdimensionale Matrizen erzeugen.\n",
        "\n",
        "> $\n",
        "p:=\\sigma_{\\mu_{1}} \\otimes \\sigma_{\\mu_{2}} \\otimes \\ldots \\otimes \\sigma_{\\mu_{n}} \\quad ; \\quad \\mu_{1}, \\mu_{2}, \\ldots, \\mu_{n} \\in\\{0,1,2,3\\} \\quad ; \\quad n \\in \\mathbb{N}\n",
        "$\n",
        "\n",
        "* **Das Kronecker-Produkt von Pauli-Matrizen tritt bei der Beschreibung von Spin-1/2-Systemen auf, die aus mehreren Teilsystemen aufgebaut sind**.\n",
        "\n",
        "* Der Zusammenhang ist dadurch gegeben, dass **das Tensorprodukt zweier Operatoren in der zugehörigen Matrixdarstellung durch das Kronecker-Produkt der Matrizen gegeben ist** (siehe [Kronecker-Produkt#Zusammenhang mit Tensorprodukten](https://de.wikipedia.org/wiki/Kronecker-Produkt#Zusammenhang_mit_Tensorprodukten)).\n",
        "\n",
        "\n",
        "https://de.wikipedia.org/wiki/Pauli-Matrizen#Kronecker-Produkt_von_Pauli-Matrizen\n",
        "\n",
        "*Explanation 3*\n",
        "\n",
        "* The kronecker product in group theory is widely used, especially with [Wigner D-function](https://de.wikipedia.org/wiki/Wignersche_D-Matrix)\n",
        "\n",
        "* The main purpose of its use in physics is to get the higher dimensional vector space. For example, in atomic physics, when we want to calculate the eigenvalues and eigenvectors of a system of spins 1/2 or spin Hamiltonian.\n",
        "\n",
        "* We analytically or with the help of computer diagonalize spin Hamiltonian and find eigenvectors and eigenvalues with the kronecker product:\n",
        "\n",
        "  * By appling the kronecker product between differnt spins matrices e.g., two matrices (dimensions 2 × 2) of spins 1/2, we will get the matrix of dimensions (4 × 4).\n",
        "\n",
        "  * This method is very compact, which means we can use the computer to get the eigenvectors and eigenvalues of matrix after applying kronecker product for higher number of spins e.g., for the system of spins 1/2.\n",
        "\n",
        "https://arxiv.org/abs/quant-ph/0104019\n",
        "\n",
        "*Explanation 4*\n",
        "\n",
        "In quantum theory the analog of a Cartesian product of classical phase spaces is a tensor product of Hilbert spaces.\n",
        "\n",
        "https://quantum.phys.cmu.edu/CQT/chaps/cqt06.pdf\n",
        "\n",
        "*Explanation 5*\n",
        "\n",
        "How do you describe the combined state of two qubits? **Remember that each qubit is a vector space, so they can't just be multiplied**. Instead, you use a tensor product, which is a related operation that creates a new vector space from individual vector spaces, and is represented by the $\\otimes$ symbol.\n",
        "\n",
        "For example, the tensor product of two qubit states $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ and $\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]$ is calculated\n",
        "\n",
        "> $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right] \\otimes\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]=\\left[\\begin{array}{l}a\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right] \\\\ b\\left[\\begin{array}{l}c \\\\ d\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$\n",
        "\n",
        "The result is a four-dimensional matrix, with each element representing a probability.\n",
        "\n",
        "For example, $a c$ is the probability of the two qubits collapsing to 0 and $0, a d$ is the probability of 0 and 1, and so on.\n",
        "\n",
        "Just as a single qubit state $\\left[\\begin{array}{l}a \\\\ b\\end{array}\\right]$ must meet the requirement that $|a|^{2}+|b|^{2}=1$ in order to represent a quantum state,\n",
        "\n",
        "a two-qubit state $\\left[\\begin{array}{c}a c \\\\ a d \\\\ b c \\\\ b d\\end{array}\\right]$ must meet the requirement that $|a c|^{2}+|a d|^{2}+|b c|^{2}+|b d|^{2}=1$\n",
        "\n",
        "https://docs.microsoft.com/en-us/azure/quantum/overview-algebra-for-quantum-computing\n",
        "\n",
        "*Explanation 6*\n",
        "\n",
        "If we have a set, denoted by 'a', of possible outcomes from one event and a set of outcomes for another event, denoted by 'b', then the possible outcomes for the union event is always the tensor product a⊗b.\n",
        "\n",
        "https://www.quora.com/What-is-a-tensor-product-in-quantum-mechanics\n",
        "\n",
        "*Explanation 7*\n",
        "\n",
        "At some point in the history of quantum mechanics, it was accepted that a single particle is described by a wavefunction which is a function of the position of the particle r, denoted:\n",
        "\n",
        "> ψ\n",
        "(\n",
        "r\n",
        ")\n",
        ".\n",
        "\n",
        "At some (possibly later) point it was also accepted that two particles are described by a wavefunction which is a function of the positions of each one of the particles, r1 and r2, denoted:\n",
        "\n",
        "> ψ\n",
        "(\n",
        "r\n",
        "1\n",
        ",\n",
        "r\n",
        "2\n",
        ")\n",
        ".\n",
        "\n",
        "In other words, the Hilbert space describing the two-particle system is the tensor product of the Hilbert spaces describing the system of each particle.\n",
        "\n",
        "https://physics.stackexchange.com/questions/53039/when-and-how-did-the-idea-of-the-tensor-product-originate-in-the-history-quantum\n",
        "\n",
        "*Explanation 8*\n",
        "\n",
        "Tensor Products are used to describe systems consisting of multiple subsystems. Each subsystem is described by a vector in a vector space (Hilbert space). For example, let us have two systems I and $/ /$ with their corresponding Hilbert spaces $H_{1}$ and $H_{11}$. Thus, using the bra-ket notation, the vectors $\\left|\\Psi_{1}\\right\\rangle$ and $\\mid \\Psi_{I I}$ ) describe the states of system I and $\\|$ with the state of the total system given by the tensor product $\\left|\\psi_{i}\\right\\rangle \\otimes\\left|\\psi_{1 I}\\right\\rangle$.\n",
        "\n",
        "https://www.quantiki.org/wiki/tensor-product\n",
        "\n",
        "*Explanation 9*\n",
        "\n",
        "The Hilbert space of a composite system is the Hilbert space tensor product of the state spaces associated with the component systems\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "Others:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tensor_product_of_Hilbert_spaces\n",
        "\n",
        "https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics\n",
        "\n",
        "https://en.wikipedia.org/wiki/Matrix_mechanics\n",
        "\n",
        "http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html\n",
        "\n",
        "https://docs.microsoft.com/de-de/azure/quantum/overview-algebra-for-quantum-computing"
      ],
      "metadata": {
        "id": "QWKNU3sBPbDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary: Tensor Transformations & Einstein Notation*"
      ],
      "metadata": {
        "id": "pBNDxf7EBkj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "* Linear map is a (1,1) tensor, because they transform using one contravariant rule (= vector components) and one covariant rules (= basis components)\n",
        "\n",
        "* Metric tensors are (0,2) tensors, because it transforms using tow covariant rules\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_31.png)"
      ],
      "metadata": {
        "id": "JIDiucDuBmiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Tensor Transformation**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_overview.png)"
      ],
      "metadata": {
        "id": "yZth_WAwBoY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)"
      ],
      "metadata": {
        "id": "g_PC1EsqBqK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_35.png)"
      ],
      "metadata": {
        "id": "UxjiWAuDBsC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vectors and covectors (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)"
      ],
      "metadata": {
        "id": "u4TZq3cvBtyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of all transformations for vector fields and covector fields / differential forms (not single vectors!) (for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)"
      ],
      "metadata": {
        "id": "0umDhwJfBvmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einstein Notation**\n",
        "\n",
        "* $\\sum_{i=1}^{3} a_{i} x_{i}=a_{1} x_{1}+a_{2} x_{2}+a_{3} x_{3}$  is in this case the same as saying: $a_{i} x_{i}$\n",
        "\n",
        "* See video: [4 rules of Einstein notation](https://www.youtube.com/watch?v=CLrTj7D2fLM&list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_&index=3)\n",
        "\n",
        "* This derivation to transform matrix components is pretty complex and can be written in a much easier way:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_25.png)\n",
        "\n",
        "If we have the same index on and top and on the bottom we end up summing over that index letter and can drop the summation sign, in this case: $\\color{red}{\\sum_{k=1}^{n}}$:\n",
        "\n",
        "> $L\\left(\\overrightarrow{e_{j}}\\right)= \\color{red}{\\sum_{k=1}^{n}} L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "and rewrite it to:\n",
        "\n",
        " > $L\\left(\\overrightarrow{e_{j}}\\right)= L_{j}^{\\color{red}{k}} \\overrightarrow{e_{\\color{red}{k}}}$\n",
        "\n",
        "**Because when we see an index repeated on the top and bottom we know that there is a summation that's going to happen.**\n",
        "\n",
        "Another examples:\n",
        "\n",
        "> $\\color{red}{\\sum_{q=1}^{n}} \\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "can be rewritten by dropping the summation sign because ${\\color{red}{q}}$ (to which the sum sign refers to) is on the top and bottom:\n",
        "\n",
        "> $\\widetilde{L_{i}^{\\color{red}{q}}} \\widetilde{e_{\\color{red}{q}}}$\n",
        "\n",
        "And as for the total formula:\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}= \\sum_{j=1}^{n} \\sum_{k=1}^{n} B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "> $\\widetilde{L_{i}^{l}}=  B_{k}^{l} L_{j}^{k} F_{i}^{j}$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_26.png)\n",
        "\n",
        "* According to the [Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation), when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set \\{1,2,3\\} ,\n",
        "\n",
        "> $\n",
        "y=\\sum_{i=1}^{3} c_{i} x^{i}=c_{1} x^{1}+c_{2} x^{2}+c_{3} x^{3}\n",
        "$\n",
        "\n",
        "is simplified by the convention to:\n",
        "\n",
        ">$\n",
        "y=c_{i} x^{i}\n",
        "$\n",
        "\n",
        "* The upper indices are not exponents but are indices of coordinates, coefficients or basis vectors.\n",
        "\n",
        "In [general relativity](https://en.wikipedia.org/wiki/General_relativity), a common convention is that\n",
        "\n",
        "* the Greek alphabet is used for space and time components, where indices take on values 0,1,2 ,\n",
        "or 3 (frequently used letters are $\\mu, v, \\ldots),$\n",
        "\n",
        "* the Latin alphabet is used for spatial components only, where indices take on values $1,2,$ or 3 (frequently used letters are $i, j, \\ldots)$\n",
        "\n",
        "*Die [Penrosesche graphische Notation](https://de.m.wikipedia.org/wiki/Penrosesche_graphische_Notation) ist eine alternative Schreibweise für die Darstellung von Tensoren!*"
      ],
      "metadata": {
        "id": "OCv-TJ6LBxzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Tensor Field*"
      ],
      "metadata": {
        "id": "pAHNvHgpB0r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorfeld**\n",
        "\n",
        "* Tensorfelder sind Funktionen, **die jedem Punkt einen Tensor zuordnen** (Tensor meint in diesem Fall ein rein algebraisches Objekt)\n",
        "\n",
        "* Tensorfelder werden auf ihre analytischen Eigenschaften untersucht (zB differenziert). Man erhält durch Differenzieren eines Tensorfeldes wieder ein Tensorfeld. Tensorfelder sind besondere glatte Abbildungen, die in Tensorbündel hinein abbilden (siehe unten).\n",
        "\n",
        "* Sei $M$ eine differenzierbare Mannigfaltigkeit. Ein [Tensorfeld](https://de.wikipedia.org/wiki/Tensorfeld) vom Typ (r,s) ist ein glatter [Schnitt](https://de.m.wikipedia.org/wiki/Schnitt_(Faserbündel)) im Tensorbündel $T_{s}^{r}(M)$.\n",
        "\n",
        "  * Ein Tensorfeld ist also ein glattes Feld $M \\rightarrow T_{s}^{r}(M),$ welches jedem Punkt der Mannigfaltigkeit einen (r,s)-Tensor zuordnet.\n",
        "\n",
        "  * Die Menge der Tensorfelder wird oft mit $\\Gamma^{\\infty}\\left(T_{s}^{r}(M)\\right)$ bezeichnet."
      ],
      "metadata": {
        "id": "TP8jXL78B271"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensordichte**\n",
        "\n",
        "* [Tensordichte](https://de.wikipedia.org/wiki/Tensordichte) ist die Quantitätsgröße eines Tensorfeldes (Generalisierung)\n",
        "\n",
        "* die Tensordichte ist eine **Verallgemeinerung der Tensorfelder** in der Tensoranalysis\n",
        "\n",
        "* wurde eingeführt, um den „Unterschied zwischen Quantität und Intensität, soweit er physikalische Bedeutung hat“, zu erfassen: „die Tensoren sind die Intensitäts-, die Tensordichten die Quantitätsgrößen“.\n",
        "\n",
        "* eine **Tensordichte** ordnet einem Koordinatensystem ein Tensorfeld derart zu, dass es bei einem Koordinatenwechsel mit dem Absolutbetrag der Funktionaldeterminante multipliziert wird. Eine Tensordichte der Stufe null ist demnach eine skalare Dichte, deren Integral gemäß dem Transformationssatz eine Invariante liefert."
      ],
      "metadata": {
        "id": "WCB2oE9SB4xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field*"
      ],
      "metadata": {
        "id": "r-rllUswCV7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis of Vector Fields**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Tensorfeld\n",
        "\n",
        "> Basis Vectors = Partial Derivatives (Jacobian)\n",
        "\n",
        "> **From Single Vectors to Vector Fields (Transformations)**\n",
        "\n",
        "Energy Momentum Tensor: https://youtu.be/ii7rffG0EwU\n",
        "\n",
        "A (basis) vector can be re-interpreted as a partial derivative (see image below):\n",
        "\n",
        "> $\\overrightarrow{e_{x}} \\equiv \\frac{\\partial \\vec{R}}{\\partial x}$\n",
        "\n",
        "https://www.youtube.com/watch?v=rr5qEb_kT6c&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=3\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_112.png)\n",
        "\n",
        "The forward matrix for example is constructed from the scaling coefficients (=basis vector coiefficients) in this case from the new basis * the old basis coefficients:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_89.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=OMCguyCnTQk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=4\n",
        "\n",
        "* Now if you want to get the coefficients when your old basis in Cartesian and your new basis is Polar coordinates, then you have another forward map at every point in the polar coordinate system\n",
        "\n",
        "* if you now think of the basis vectors as partial derivatives, it makes things much easier. Use mutlivariable chain rules:\n",
        "\n",
        "  * the first old basis vector $\\overrightarrow{e_{x}}$ can be thought of as the partrial derivative in the x direction: $\\frac{\\partial \\vec{R}}{\\partial x}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * the second old basis vector $\\overrightarrow{e_{y}}$ can be thought of as the partrial derivative in the y direction: $\\frac{\\partial \\vec{R}}{\\partial y}=\\overrightarrow{e_{x}}$\n",
        "\n",
        "  * same goes for the new basis vector (polar coordinates) as partial derivaties into r and $\\theta$ directions\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_90.png)\n",
        "\n",
        "The underlined part are the coefficients to move from one basis (cartesian) to another (polar) (Achtung: not normalised):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_91.png)\n",
        "\n",
        "The forward matrix = the Jacobian matrix:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_92.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_95.png)\n",
        "\n",
        "And you can do the same the other way around to get the Backward transform, which is the inverse Jacobian:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_93.png)\n",
        "\n",
        "Example of that it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_94.png)\n",
        "\n",
        "It total it looks likes this:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_96.png)\n",
        "\n",
        "And we can store the (forward & backward) coefficients in matrix\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_97.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tYEF_rwU4Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Field Components**\n",
        "\n",
        "> Vectors Components = Derivatives in Vector Fields (vector fields of tangent vectors along curves)\n",
        "\n",
        "> Task: Figure out vector components in a new basis, **but instead of one vector, we consider vector fields**.\n",
        "\n",
        "We consider vectors along a curve (=tangent vectors)\n",
        "\n",
        "https://www.youtube.com/watch?v=9yOb9gHnLUk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=5\n",
        "\n",
        "\n",
        "The cases are both the same:\n",
        "\n",
        "* on we have a single vector in a new basis constructed from the basis vectors in the old basis and its components\n",
        "\n",
        "* on the bottom we have a whole vector field constructed from the old basis (using the chain rule!) and the vectir components ar derivates!\n",
        "\n",
        "* Framed in red is the vector we want to expand, in blue framed the basis vectors and in green framed the vector components\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_98.png)\n",
        "\n",
        "Example: in the cartesian coordinate system the basis vectors are every where the same. Just components are everywhere different. But it’s easy using the multivariable chain rule when you take the vector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_99.png)\n",
        "\n",
        "Other example iof tangents on a circle in a cartesian coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_100.png)\n",
        "\n",
        "\n",
        "Do these component make sense? Let's check with an example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_101.png)\n",
        "\n",
        "Also works in the polar coordinate system:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_103.png)\n",
        "\n",
        "Checking if it's true, and it works:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMCdUWicVfKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contravariance of Vector Field Components**\n",
        "\n",
        "https://www.youtube.com/watch?v=zKuyaQ4JRs8&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=6&t=154s\n",
        "\n",
        "* Vector components are contravariant\n",
        "\n",
        "* We can follow the same reasoning for vector fields of tangent vectors along curves\n",
        "\n",
        "In this example, the polar components have two be equal to the Cartesian components multiplied by the backward transform. So we get this transformation formula for the components of the tangent vectors (the box on the bottom right):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_104.png)\n",
        "\n",
        "Partial derivative basis vectors transform one way and the vector component derivatives transform the other way, the vector components are contravariant. (Dervative coefficients are opposites) But don't memorize them! Simply use multivariable chain rules for the four formulas on the bottom and you get the transformation (chain rules over cartesian frames in purple, chain rule sover polar coordinates framed in green)::\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_105.png)\n",
        "\n",
        "Remember when we used for forward and backward transforms with single vectors the following formula:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_107.png)\n",
        "\n",
        "**This is similar for Vector Fields: basis vectors transform one way, and the vector components transform the other wa, using the Jacobian $J$ and the Jacobian inverse $J^-1$ as the forward and backward transforms:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_106.png)\n",
        "\n",
        "..and this is for the specific example we had for the circular curve with radius 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_108.png)\n",
        "\n",
        "**One important last thing: we remove the position vector $R$ and leave the <u>derivative operators = (basis) vector</u>, and they will be considered basis vectors from now on:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_109.png)\n",
        "\n",
        "And this makes sense: The partial derivative with respect to x points in the x direction (and so on):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_110.png)\n",
        "\n",
        "This is useful becasue position vectors $R$ rely on an origin, and as we will see later, on manifolds on curved surfaces we cannot rely on the existnce of an origin point:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_111.png)"
      ],
      "metadata": {
        "id": "J152JTPfWWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Covector Field (1-form-Differential Form)*"
      ],
      "metadata": {
        "id": "8h7LKwE9CYwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covector Field (= 1-form-Differential Form)**\n",
        "\n",
        "> <font color=\"blue\">**Das Differential eines Skalarfeldes (linear form) ist ein Covector field, weil Differentiale von Skalaren Covectoren sind**\n",
        "\n",
        "> **Covectors: Differential Forms = Covector Fields**\n",
        "\n",
        "Das Differential eines Skalarfeldes (differentialform) ist ein Covector field, weil differentiale von skalaren covectoren sind??\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_119.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=XGL-vpk-8dU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=8&t=201s\n",
        "\n",
        "Re-interprete $d$ from $df$ as being an operator that takes a scalar field and outputs a covector field:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_120.png)\n",
        "\n",
        "Example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_126.png)\n",
        "\n",
        "The way to get the covector fields is by tracing out the level sets of the scalar function: **Skalarfeld links mit Temperaturen, Kovektorfeld rechts mit den Konturen, wo überall die gleichen Konturen existieren (Äquivalenzen)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_125.png)\n",
        "\n",
        "Another example:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_124.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_122.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_127.png)\n",
        "\n",
        "Example: If we think of x as a scalar field, it would look like this: it’s a scalar field where each point is given the x value at that point. And the covector field $dx$ would like like the other picture on top right: Covector fields $dx$ with level set curves being vertical lines and orientation to the right (because all x values are the same along this line, whcih aligns with the definition of a [Level Set (Niveaumenge)](https://de.wikipedia.org/wiki/Niveaumenge): **die Menge aller Punkte des Definitionsbereichs einer Funktion, denen ein gleicher Funktionswert zugeordnet ist**\n",
        "\n",
        "Covector fields $dy$ with level set curves being horizontal lines and orientation upwards:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_128.png)\n",
        "\n",
        "Another examples: Circles with constant radius are along the same lines:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_129.png)\n",
        "\n",
        "**How to calculate now? - How do covector fields like $df$ act on $\\vec{v}$ to give us output values?**\n",
        "\n",
        "> Count the number of tangent lines to the curve at p (where the vector originates at point p)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_130.png)\n"
      ],
      "metadata": {
        "id": "mjpXmcIJSASJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But what's the geometrical meaning of $df$ ($\\vec{v}$)?**\n",
        "\n",
        "On a map we can draw out curves of constant elevation (which is the same thing as level sets). We can think of this level set drawing of a mountain as a covector field associated with the mountain.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_131.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_132.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_133.png)\n",
        "\n",
        "> $d f(\\vec{v})$ tells us the rate of change of $f$ when moving at velocity $\\vec{v}$. **$d f(\\vec{v})$ is the directional derivative of $f$ in direction $\\vec{v}$**.\n",
        "\n",
        "**Covector Field Components: The following are the basis covectors**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_134.png)\n",
        "\n",
        "So just as we can expand individual covectors into linear combinations of dual basis vectors (and of course we get different components depending on which basis we use (all on top), we can also expand differential forms - also callee covector fields - into linear combinations of other covector fields where we get different components depending o which basis we use (on the bottom).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_135.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=r_20yXBdhJk&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=9\n",
        "\n"
      ],
      "metadata": {
        "id": "05_SpyG3SEPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation Rules of Differential Forms (Covector Fields)**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_136.png)\n",
        "\n",
        "Same logic applies for covector fields: if we for example want to build the Cartesian basis covector fields our of the polar basis covector fields (from new to old), we use the following coefficients, which are the entries of the Jacobian matrix (=forward transform, because covector basis components act contravariant).\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_137.png)\n",
        "\n",
        "**Summary of all transformations (between 2 basis of covector fields, for example between Cartesian and polar coordinate system):**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_138.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=4doR1XCXzKU&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=10"
      ],
      "metadata": {
        "id": "FffzqkWvSGIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration with Differential Forms**\n",
        "\n",
        "* With the following interpretation of differential forms we can create the integration and it doesn't depend on coordinate systms at all\n",
        "\n",
        "* Also we **just need start point and end point** and count the number of pierced covector components instead of computing the integral at each point on the line\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_147.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_149.png)\n",
        "\n",
        "Since the covector fields and the paths are the same in both following cases, the result of the integral which is negative 4, is also the same in both cases:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_148.png)\n",
        "\n",
        "> **Covector fields are invariant of the choice of coordinates. Covector field components depend on the choice if coordinate.*And that's why when we change the variable in an integral we still get the same answer.**\n",
        "\n",
        "https://www.youtube.com/watch?v=kyzSofggsqg&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=11\n",
        "\n",
        "https://www.youtube.com/watch?v=PzrGGbX-_54&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=12"
      ],
      "metadata": {
        "id": "rAtqpzXZSH87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Vector Field vs Covector Field*"
      ],
      "metadata": {
        "id": "ihDto5NgCay8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gradient ∇ vs 𝑑 operator (exterior derivative/differential)\n",
        "\n",
        "https://www.youtube.com/watch?v=nJpONHO_X5o&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=15\n",
        "\n",
        "https://www.youtube.com/watch?v=Do5vzLJRWRE&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=16\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_139.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_141.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_zLHtahCwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Extrinsic (Exterior) Geometry vs Intrinsic Geometry*"
      ],
      "metadata": {
        "id": "BJaouQGICdBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=VHkL5HpL0HY&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=7\n",
        "\n",
        "* Let's take a 2D surface like earth, and someone drives along it\n",
        "\n",
        "* we want to get the velocity of the car. We need position vectors, where we need an origin point (center of earth), then we get 2 vectors, take their limit to compute velocity\n",
        "\n",
        "* There are 2 issues with this:\n",
        "\n",
        "  1. Firstly the origin point that we've chosen doesn't live o the earth's 2D surface. We picked an origin point that was exterior to the surface.\n",
        "\n",
        "  2. Secondly the target velocity actually leaves the surface that we are studying and goes off into the outside space - **Remember here Lie algreba and Lie group**: the tangent is outside the 2D surface and the tangent point is the Lie algebra. All on the 2D surface however is part of the Lie group.\n",
        "\n",
        "* So here, both the origin point and the velocity vector are both defined using the 3 dimensional space, even though we are only studying 2 dimensional spherical surface\n",
        "\n",
        "* **This is why it's all called exterior geometrty and exterior product** - studying something outside\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_113.png)"
      ],
      "metadata": {
        "id": "zH_Dj6_LEh_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we try to find the distance between 2 points on a map of earth, it's called \"intrinsiv geometry\" when we are not allowed to leave the surface (and hence cannot draw a straight line, but need to find a geodesic on the flat map).\n",
        "\n",
        "And that gets even more complicated in General Relativity:\n",
        "\n",
        "1. 4D spacetime is curved space, so we cannot draw a straight line int he curved space. So that means we cant draw poisiton vectors.\n",
        "\n",
        "2. And we cannot pick an origin outside the 4D spacetime, because that would mean picking a point outside of the universe\n",
        "\n",
        "That's why its called intrinsic geometry: you need to stay inside! **But how do we study velocity if we are not allowed to use position vectors?**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_114.png)"
      ],
      "metadata": {
        "id": "Y8Mr4AgTEjud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution: we cannot draw straight lines, but we can draw curved paths on a 2D on a map for exmaple**.\n",
        "\n",
        "> **So we can't use normal position vectors to talk about directions on a surface, but we can use derivative operators to talk about different directions:**\n",
        "\n",
        "* If have have some path that's traveling around in our curved space $(x(\\lambda), y(\\lambda))$, we can still consider the direction that the path is pointing in using the derivative with respect to the curve parameter $\\frac{d}{d \\lambda}$\n",
        "\n",
        "* And we can break this direction up into its x and y components using the multivariable chain rules that we have for derivative operators $\\frac{d}{d \\lambda}=\\frac{d x}{d \\lambda} \\frac{\\partial}{\\partial x}+\\frac{d y}{d \\lambda} \\frac{\\partial}{\\partial y}$\n",
        "\n",
        "* But Achtung: you shouldn't think of this direction vector as actually connecting the two points on the earth (origin and destination of yellow arrow). The derivative just gives the general direction that the curve is traveling in at a given point.\n",
        "\n",
        "* So derivatives with us a way to talk about directions on a curved surface in a way that is complete intrinsic to the surface and doesn't require an outside space in any way.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_115.png)"
      ],
      "metadata": {
        "id": "cabaMULeElaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same approach in 4 D space: we can't draw straight lines, but we can still draw curved paths. And that means we can take derivatives with respect to the paths parameters to get a sense of different directions.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_116.png)"
      ],
      "metadata": {
        "id": "PFhrRHMWEnRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> So the old notation uses actual vectors from the **vector spaces R2 and R3** to define directions. But the new notation use the **vector space of derivative operators**.\n",
        "\n",
        "> **The vector space of derivative operators is formally known the \"Tangent Vector Space\" and is denoted: $T_{p}M$**, which is the vector space of derivatives at some point p on a surface $M$. And keep in mind that the vectors on the left and the vectors on the right are in fact from different vector spaces!!\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_117.png)\n",
        "\n",
        "And they do form a vector space because we can scale and add them linearly! So these partial derivatives are vectors in the tangent space $T_{p}M$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_118.png)"
      ],
      "metadata": {
        "id": "dvn3ZKheEpKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Metric Tensor Field*"
      ],
      "metadata": {
        "id": "QHq7RwnYCe8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Metric Tensor to compute arc length of a curve in Flat & Curved Space**\n",
        "\n",
        "**In Flat Space**\n",
        "\n",
        "So in summary the equation for calculating the arc length of a curve is an integral that depends on the magnitude of the curves tangent vectors $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|$ in this:\n",
        "\n",
        "> $\\operatorname{arclength}=\\int\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\| d \\lambda$\n",
        "\n",
        "And to calculate the squared magnitude of the tangent vectors we need to use the dot product\n",
        "\n",
        "> $\\left\\|\\frac{d \\vec{R}}{d \\lambda}\\right\\|^{2}=\\frac{d \\vec{R}}{d \\lambda} \\cdot \\frac{d \\vec{R}}{d \\lambda}$\n",
        "\n",
        "And we end up with this equation in Cartesian coordinates:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$\n",
        "\n",
        "And this equation in polar coordinates:\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda}\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p^{j}}\\right)$\n",
        "\n",
        "And the basis vector dot products $\\left(\\frac{\\partial \\vec{R}}{\\partial c^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial c^{j}}\\right)$ and $\\left(\\frac{\\partial \\vec{R}}{\\partial p^{i}} \\cdot \\frac{\\partial \\vec{R}}{\\partial p j}\\right)$ give us the components of the metric tensor:\n",
        "\n",
        "> $=\\frac{d c^{i}}{d \\lambda} \\frac{d c^{j}}{d \\lambda} g_{i j}$\n",
        "\n",
        "> $=\\frac{d p^{i}}{d \\lambda} \\frac{d p^{j}}{d \\lambda} \\widetilde{g_{i j}}$\n",
        "\n",
        "\n",
        "**So the key to getting the arc length of a curve is the tangent vector magnitude. And the key to getting the tangent vector magnitude is the metric tensor components $g_{i j}$ and $\\widetilde{g_{i j}}$**.\n",
        "\n",
        "And remember: the metric tensor is a (0,2) tensor because its components obey 2 covariant transformation laws:\n",
        "\n",
        "> $\\begin{aligned} \\widetilde{g_{i j}} &=\\frac{\\partial c^{k}}{\\partial p^{i}} \\frac{\\partial c^{l}}{\\partial p^{j}} g_{k l} \\\\ g_{k l} &=\\frac{\\partial p^{i}}{\\partial c^{k}} \\frac{\\partial p^{j}}{\\partial c^{l}} \\widetilde{g_{i j}} \\end{aligned}$\n",
        "\n",
        "In flat space there is only 1 metric tensor with which you can calculate the arc length of any curve as long as we can do this integral.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_150.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=BbQmTmSzUCI&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=13\n"
      ],
      "metadata": {
        "id": "L3N2cBc6FoEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Curved Space**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_142.png)\n",
        "\n",
        "In curved space: Vector field: vector changes from point to point. Covector fields have covectors that change from point to point. And now a metric tensor field involves a different metric tensor being placed everywhere in space that change from point to point.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_151.png)\n",
        "\n",
        "Every curved space has its own metric tensor field that gives you the rules for measuring distance on it:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_152.png)\n",
        "\n",
        "Metric tensors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_143.png)\n",
        "\n",
        "Simplify them:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_144.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_145.png)\n",
        "\n",
        "For the intrinsic view (left side only) we can remove $R$ vectors completely and treat derivative operators themselves as the vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_146.png)\n",
        "\n",
        "https://www.youtube.com/watch?v=SmjbpIgVKFs&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=14"
      ],
      "metadata": {
        "id": "yS56QI6iGRym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOpIknui8ULm"
      },
      "source": [
        "###### ***Differential Geometry of Smooth Surfaces***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential geometry of smooth surfaces**\n",
        "\n",
        "* Wikipedia: [First and second fundamental forms, the shape operator, and the curvature](https://en.m.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature)\n",
        "\n",
        "* Artikel: [A-quick-and-dirty-introduction-to-the-curvature-of-surfaces](http://wordpress.discretization.de/geometryprocessingandapplicationsws19/a-quick-and-dirty-introduction-to-the-curvature-of-surfaces/)\n",
        "\n",
        "* [Principal curvatures](https://en.m.wikipedia.org/wiki/Principal_curvature)\n",
        "\n",
        "* [Gauss map](https://en.wikipedia.org/wiki/Gauss_map) provides a mapping from every point on a curve or a surface to a corresponding point on a unit sphere\n",
        "\n",
        "* [Shape operator](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature) or Weingarten map\n",
        "\n",
        "* [Normal plane](https://en.wikipedia.org/wiki/Normal_plane_(geometry)): a normal vector that is at right angles to the surface\n",
        "\n",
        "* [Normal section](https://en.wikipedia.org/wiki/Normal_plane_(geometry)#Normal_section): Intersection of normal plane and  surface (*For most points on most surfaces, different normal sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures, call these κ1, κ2*)\n",
        "\n",
        "* **Extrinsic Properties**: Rely on an embedding of a surface in Euclidean space. Illustrated by the non-linear [Euler–Lagrange equations](https://en.m.wikipedia.org/wiki/Euler–Lagrange_equation) in [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations). Lagrange: [minimal surfaces](https://en.m.wikipedia.org/wiki/Minimal_surface) (can only be defined in terms of an embedding).\n",
        "\n",
        "* **Intrinsic Properties / [Gaussian curvature](https://en.m.wikipedia.org/wiki/Gaussian_curvature)**: Geometric properties determined by geodesic distances (not embeddings).\n",
        "\n",
        "  * Gaussian curvature of a surface = how curves on the surface change directions in three dimensional space\n",
        "\n",
        "  * An important role play Lie groups, namely the [symmetry groups](https://en.m.wikipedia.org/wiki/Symmetry_group) of the Euclidean plane, the sphere and the hyperbolic plane.  **These Lie groups can be used to describe surfaces of constant Gaussian curvature**; they also provide an essential ingredient in the modern approach to intrinsic differential geometry through [connections](https://en.m.wikipedia.org/wiki/Connection_(mathematics)).\n",
        "\n",
        "  * [Gaussian curvature](https://en.wikipedia.org/wiki/Gaussian_curvature) is an intrinsic invariant, i.e. invariant under local [isometries](https://en.m.wikipedia.org/wiki/Isometry). This point of view was extended to higher-dimensional spaces by Riemann and led to what is known today as [Riemannian geometry](https://en.m.wikipedia.org/wiki/Riemannian_geometry).\n",
        "\n",
        "* [First fundamental form](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#First_and_second_fundamental_forms,_the_shape_operator,_and_the_curvature): Gaussian curvature can be calculated from the first fundamental form => curvature and metric properties of a surface such as length and area, called [metric tensor](https://en.m.wikipedia.org/wiki/Metric_tensor) of surface.\n",
        "\n",
        "> $\\mathrm{I}(x, y)=\\langle x, y\\rangle .$\n",
        "\n",
        "* [Second fundamental form](https://en.wikipedia.org/wiki/Second_fundamental_form) encodes how lengths and angles of curves on the surface are distorted when the curves are pushed off of the surface. Together with First fundamental form, it serves to define extrinsic invariants of the surface, **its principal curvatures**.\n",
        "\n",
        ">$L d x^{2}+2 M d x d y+N d y^{2}$ (quadratic from ins 3D)\n",
        "\n",
        "* [Riemann curvature tensor](https://en.m.wikipedia.org/wiki/Riemann_curvature_tensor): Der Krümmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VJjblo9jZJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1443.png)\n"
      ],
      "metadata": {
        "id": "dcgQer6SUB0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1444.png)\n"
      ],
      "metadata": {
        "id": "TZkbzLiaUAzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![jj](https://raw.githubusercontent.com/deltorobarba/repo/master/gaussiancurvature.png)\n"
      ],
      "metadata": {
        "id": "WHI-KoMUT_dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Definition of second fundamental form*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/5/5e/Second_fundamental_form.svg)\n",
        "\n",
        "[Source](https://www.researchgate.net/figure/Measures-of-surface-curvature-a-The-principal-curvatures-are-calculated-from-the_fig6_321165318)\n"
      ],
      "metadata": {
        "id": "xCMvunOBT9v1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTS59KF68ULu"
      },
      "source": [
        "**Levi-Civita-Zusammenhang (Connection)**\n",
        "\n",
        "* Der [Levi-Civita-Zusammenhang](https://de.wikipedia.org/wiki/Levi-Civita-Zusammenhang#Hauptsatz_der_riemannschen_Geometrie) ist ein wesentliches Hilfsmittel zum Aufbau der riemannschen Krümmungstheorie. Denn der Krümmungstensor wird mit Hilfe eines Zusammenhangs definiert, daher bietet es sich an, in der riemannschen Geometrie den eindeutig ausgezeichneten Levi-Civita-Zusammenhang für die Definition des riemannschen Krümmungstensors zu verwenden.\n",
        "\n",
        "* Der [Zusammenhang](https://de.wikipedia.org/wiki/Zusammenhang_(Differentialgeometrie)) (Connection) ist in der Differentialgeometrie ein Hilfsmittel, um Richtungsänderungen im Laufe einer Bewegung zu quantifizieren und Richtungen in verschiedenen Punkten miteinander in Beziehung zu setzen.\n",
        "\n",
        "* Dieser Artikel behandelt im Wesentlichen den Zusammenhang auf einer differenzierbaren Mannigfaltigkeit beziehungsweise auf einem [Vektorbündel](https://de.wikipedia.org/wiki/Vektorbündel). Ein ausgezeichneter Zusammenhang auf einem Tensorbündel, einem besonderen Vektorbündel, heißt kovariante Ableitung.\n",
        "\n",
        "* Allgemeiner existieren auch [Zusammenhänge auf Prinzipalbündeln](https://de.wikipedia.org/wiki/Zusammenhang_(Prinzipalbündel)) mit analogen definierenden Eigenschaften.\n",
        "\n",
        "* In der Differentialgeometrie interessiert man sich für die Krümmung von Kurven, insbesondere von Geodäten. In euklidischen Räumen ist die Krümmung einfach durch die zweite Ableitung gegeben.\n",
        "\n",
        "* **Auf differenzierbaren Mannigfaltigkeiten ist die zweite Ableitung nicht direkt zu bilden. Ist $\\gamma$ eine Kurve, so muss man für die zweite Ableitung dieser Kurve den Differenzenquotienten mit den Vektoren $\\gamma^{\\prime}(t)$ und $\\gamma^{\\prime}\\left(t_{0}\\right)$ bilden. Diese Vektoren befinden sich jedoch in unterschiedlichen Vektorräumen, daher kann man nicht einfach die Differenz der beiden bilden**.\n",
        "\n",
        "* **Um das Problem zu lösen, hat man eine Abbildung definiert, welche man Zusammenhang nennt. Diese Abbildung soll einen Zusammenhang zwischen den beteiligten Vektorräumen bereitstellen und trägt daher auch diesen Namen**.\n",
        "\n",
        "In diesem Abschnitt bezeichnet $M$ eine glatte Mannigfaltigkeit, $T M$ das Tangentialbündel und $\\pi: E \\rightarrow M$ ein Vektorbündel. Mit $\\Gamma(E)$ wird die Menge der glatten Schnitte im Vektorbündel $E$ notiert.\n",
        "\n",
        "* **Indem man sagt, was die Richtungsableitung eines Vektorfeldes in Richtung eines Tangentialvektors ist, erhält man einen Zusammenhang auf einer differenzierbaren Mannigfaltigkeit $M$**. Demgemäß definiert man einen Zusammenhang auf einem Vektorbündel als eine Abbildung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "\\nabla: \\Gamma(T M) \\times \\Gamma(E) & \\rightarrow \\Gamma(E) \\\\\n",
        "(X, s) & \\mapsto \\nabla_{X} s\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "* die einem Vektorfeld $X$ auf $M$ und einem Schnitt $s$ im Vektorbündel $E$ wieder einen Schnitt in $E$ zuordnet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-V3Q8YM8ULv"
      },
      "source": [
        "**Vektorbündel & Tensorbündel**\n",
        "\n",
        "* [Vektorbündel](https://de.wikipedia.org/wiki/Vektorbündel) oder manchmal auch Vektorraumbündel sind Familien von Vektorräumen, die **durch die Punkte eines topologischen Raumes parametrisiert sind**.\n",
        "\n",
        "* Vektorbündel gehören damit auch zu den [Faserbündeln](https://de.m.wikipedia.org/wiki/Faserbündel). Remind: Faser ist ein Urbild von einem Element (\"Faser der Abbildung über einem Element\") - surjektiv! kann also mehrere Elemente im Urbild haben. Daher Faser $\\mathbb{R}$<sup>2</sup> zu Punkt auf $\\mathbb{R}$ (siehe [hier](https://de.m.wikipedia.org/wiki/Vektorbündel) die Illustration:\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Vectorbundle.svg/320px-Vectorbundle.svg.png)\n",
        "\n",
        "*Illustration des Vektorbündels $(E, B, \\pi)$. Hier ist der **Totalraum** $E=\\mathbb{R}^{2}$ und der **Basisraum** $B=\\mathbb{R} .$ Die Abbildung $\\pi: E \\rightarrow B$ projiziert jede Gerade $E_{x}$ auf den Punkt $x$. Der Raum $E_{x}=\\{p \\in E \\mid \\pi(p)=x\\}$ wird **Faser über $x$** genannt. Außerdem ist der Totalraum $E$ die Vereinigung aller Fasern.* (Comment: also Totalraum E ist Urbild mit Faser und Basisraum B ist Zielbild mit Element das von der Faser stammt)\n",
        "\n",
        "* [Tangentialbündel](https://de.wikipedia.org/wiki/Tangentialbündel)\n",
        "\n",
        "* [Tensorbündel](https://de.m.wikipedia.org/wiki/Tensoranalysis#Tensorbündel) ist ein bestimmtes Vektorbündel. Tensorfelder sind dann besondere glatte Abbildungen, die in dieses Vektorbündel hinein abbilden.\n",
        "\n",
        "* **Schnitt (Faserbündel)**\n",
        "\n",
        "  * [Schnitte](https://de.m.wikipedia.org/wiki/Schnitt_(Faserbündel)) sind Abbildungen, welche in der algebraischen Topologie, insbesondere in der Homotopietheorie, untersucht werden. Insbesondere interessiert man sich dafür, unter welchen Bedingungen solche Abbildungen existieren.\n",
        "\n",
        "  * Das bekannteste Beispiel von Schnitten sind die [**Differentialformen**](https://de.m.wikipedia.org/wiki/Differentialform).\n",
        "\n",
        "  * Ein Schnitt kann als **Verallgemeinerung des Graphen einer Funktion** aufgefasst werden.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bundle_section.svg/187px-Bundle_section.svg.png)\n",
        "\n",
        "*Die Abbildung s ist ein Schnitt in einem Faserbündel $p: E \\rightarrow B$. Dieser Schnitt s erlaubt es, den Basisraum $B$ mit dem Teilraum $s(B)$ von $E$ zu identifizieren.*\n",
        "\n",
        "* Die [Schnittkrümmung](https://de.wikipedia.org/wiki/Schnittkrümmung) ist eine Größe der riemannschen Geometrie, eines Teilgebiets der Mathematik. Mit ihrer Hilfe kann man die Krümmung einer n-dimensionalen riemannschen Mannigfaltigkeit beschreiben.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUvl0oGH8ULh"
      },
      "source": [
        "###### *Geodesics in curved space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEwWL-tS8ULh"
      },
      "source": [
        "Video: [Tensor Calculus 15: Geodesics and Christoffel Symbols (extrinsic geometry)](https://www.youtube.com/watch?v=1CuTNveXJRc)\n",
        "\n",
        "Video: [Tensor Calculus 16: Geodesic Examples on Plane and Sphere](https://www.youtube.com/watch?v=8sVDceI70HM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1576.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1577.png)"
      ],
      "metadata": {
        "id": "OC5VDhg_jVPS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjFey3T8ULx"
      },
      "source": [
        "###### *Covariant Derivative*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVN1fZ2C8ULx"
      },
      "source": [
        "**Covariant Derivative in Flat Space**\n",
        "\n",
        "> **Covariant Derivative = understanding the rate of change of vector (tensor) fields that takes changing basis vectors into account**\n",
        "\n",
        "* Video: [Tensor Calculus 17: The Covariant Derivative (flat space)](https://www.youtube.com/watch?v=U5iMpOn5IHw&t=4s)\n",
        "\n",
        "Challenge: different sources define covariant derivative in different ways.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1571.png)\n",
        "\n",
        "**Consider following two examples:**\n",
        "\n",
        "* the in the first picture the vector field is constant everywhere. hence the deriative of this vector field is zero in the x and y direction.\n",
        "\n",
        "* in the second image below the vector field is moving. So the vector field is NOT zero in the $r$ and $\\theta$ direction. The components 2 and 1 are constant, but the basis vectors $e_r$ and $e_\\theta$ are changing from point to point that causes the vectors in the vector field to change length and direction.\n",
        "\n",
        "> **Constant Components ≠ Constant Vector Field**\n",
        "\n",
        "*The covariant derivative components with the semicolon form a (1,1) tensor: one contravatiant transformation rule and one covariant transformation rule (where the inverse Jacobian transforms the contravariant index, and the Jacobian transforms the covariant index.)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVok3GM8ULy"
      },
      "source": [
        "**Covariant Derivative in Curved Space Definition (extrinsic)**\n",
        "\n",
        "Video: [Tensor Calculus 18: Covariant Derivative (extrinsic) and Parallel Transport](https://youtu.be/Af9JUiQtV1k)\n",
        "\n",
        "* Parallel Transport plays an important role\n",
        "\n",
        "* Objective: Dealing with rates of change of vector fields on curved surfaces\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1572.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1573.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1574.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g21QIodZ8UL3"
      },
      "source": [
        "**Covariant Derivative in Curved Space Definition (Intrinsic)**\n",
        "\n",
        "Video: [Tensor Calculus 19: Covariant Derivative (Intrinsic) and Geodesics](https://www.youtube.com/watch?v=EFKBp52LtDM)\n",
        "\n",
        "* Geodesics play an important role!\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1575.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qS8lKVO8UL5"
      },
      "source": [
        "**Covariant Derivative: Abstract Definition with Levi-Civita Connection (Fundamental Theorem of Riemannian Geometry)**\n",
        "\n",
        "Video: [Tensor Calculus 20: The Abstract Covariant Derivative (Levi-Civita Connection)](https://www.youtube.com/watch?v=cEEahoUUGyc)\n",
        "\n",
        "*What parallel transport is doing it is connecting the tangent vector space at red point p to tangent vector space at blue point q. So parallel transport gives us a way to map vectors from TpS to TqS. And since parallel transport is defined using the covariant derivative, it's really the covariant derivative that's providing the connection between the tangent spaces in this curved space*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1578.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1579.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1580.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPdLE5H8UL-"
      },
      "source": [
        "###### *Lie Bracket (Commutator in Flow Curves)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoq7AWyk8UL-"
      },
      "source": [
        "**Flow Curve (Integral Curve)**\n",
        "\n",
        "https://www.youtube.com/watch?v=SfOiOPuS2_U&list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx&index=24&t=69s\n",
        "\n",
        "In the following image:\n",
        "\n",
        "* The partial derivatives of the coordinate variables are basically like basis vectors (framed in blue)\n",
        "\n",
        "* the derivatives with respect to lambda (framed in red) are the components\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1582.png)\n",
        "\n",
        "**Lie Bracket (the commutator of 2 vector fields $\\vec{u}$ & $\\vec{v}$)**\n",
        "\n",
        "* Lie Bracket takes 2 vector fields and tells us if the rectangle of flow curves closes properly\n",
        "\n",
        "* Flow curves do not close properly if Lie bracket is non zero\n",
        "\n",
        "Task of calculating Lie bracket:\n",
        "\n",
        "1. Find change of $\\vec{u}$ in the direction of $\\vec{v}$\n",
        "\n",
        "2. Find change of $\\vec{v}$ in the direction of $\\vec{u}$\n",
        "\n",
        "> **Lie Bracket (Commutator) = measures how much vector field flow curves fail to close.**\n",
        "\n",
        "> $[\\vec{u}, \\vec{v}]=\\vec{u}(\\vec{v})-\\vec{v}(\\vec{u})$\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1583.png)\n",
        "\n",
        "**Coordinate lines are just flow curves along basis vectors**\n",
        "\n",
        "Because coordinate curves always close without a curve, so Lie bracket always has to be zero for them\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1584.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu1qRXiA8UL-"
      },
      "source": [
        "###### *Torsion Tensor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRJTRAij8UL_"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Torsion_tensor\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_259.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZZEEgj8UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_262.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iQ321N18UL_"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIJL6tVt8UMA"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_275.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiU5eKGE8UMH"
      },
      "source": [
        "###### *Detect Curvatur: Holonomy + Geodesic Deviation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4B8QnY8UMI"
      },
      "source": [
        "So we need a new tool, that works everyhwere:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_282.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuQX8qW8UMI"
      },
      "source": [
        "* With a correct parallel transport we can see if a surface is curved (when vectors at starting and at end point point in different directions)\n",
        "\n",
        "* the is holonomy! It's the twisting of a vector when transport it around in a loop\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_279.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ErFwUxZ8UMI"
      },
      "source": [
        "> $R(\\vec{u}, \\vec{v}) \\vec{w}=\\nabla_{\\vec{u}} \\nabla_{\\vec{v}} \\vec{w}-\\nabla_{\\vec{v}} \\nabla_{\\vec{u}} \\vec{w}-\\nabla_{[\\vec{u}, \\vec{v}]} \\vec{w}$\n",
        "\n",
        "* $R(\\vec{u}, \\vec{v})$ is an operator which acts on vector $\\vec{w}$ and produces change vector - This is the change vector after is is parallel transported around a small parallelogram (the blue handwritten arrow is this result !!!!)\n",
        "\n",
        "* this means the Riemann curvature tensor takes 3 vector inputs: 2 vectors defining the parallelogram and one starting input vector, and it outputs the change vector\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_276.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnkSoEwu8UMN"
      },
      "source": [
        "###### *Riemann Curvature Tensor, Sectional Curvature, Ricci-Tensor & Ricci-Scalar*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethu1_YS8UMK"
      },
      "source": [
        "Video: [Tensor Calculus 23: Riemann Curvature Tensor Components and Symmetries](https://www.youtube.com/watch?v=optrC-0HhMI)\n",
        "\n",
        "**Der Krümmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar.**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1581.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Riemann curvature tensor (Krümmungstensor), Ricci-Tensor, Ricci-Scalar**\n",
        "\n",
        "* Krümmung wird beschrieben, indem man Vektoren auf geschlossenen Wegen durch die Raumzeit schiebt und feststellt, ob sie verändert zurückkommen. Nehmen Sie vielleicht einen Globus und einen Bleistift in die Hand. Der Bleistift stellt einen Vektor dar. Legen Sie den Bleistift am Nordpol tangential an den Globus an. Verschieben Sie ihn nun längs irgendeines Längenkreises (Meridian) zum Äquator. Danach verschieben Sie den Bleistift längs des Äquators ein Stück nach Osten oder Westen und beachten dabei, dass der Bleistift tangential zum Globus bleiben muss. Dann verschieben Sie den Bleistift wieder längs eines Meridians zum Nordpol zurück. Wenn er dort ankommt, wird er in eine andere Richtung als zu Beginn seines Weges zeigen.\n",
        "\n",
        "* Der Unterschied zwischen den Richtungen zu Beginn und am Ende des Weges ist ein Maß für die Krümmung des Globus. Dass der Bleistift während des gesamten Weges tangential zum Globus verschoben werden muss, drückt aus, dass er die Oberfläche des Globus während der Verschiebung nicht verlassen darf.\n",
        "\n",
        "* Genau diese Operation, die kennzeichnet, wie sich ein Vektor verändert, wenn man ihn längs eines geschlossenen Weges durch die Raumzeit verschiebt, wird mathematisch durch den so genannten Krümmungstensor ausgedrückt.\n",
        "\n",
        "* **Der Krümmungstensor ist ein kompliziertes Gebilde. Man kann ihn in einem ersten Schritt zum Ricci-Tensor vereinfachen, in einem zweiten zum Ricci-Skalar**.\n",
        "\n",
        "* Der Ricci-Skalar ordnet jedem Punkt der Raumzeit einen einzelnen Zahlenwert zu, der die lokale Krümmung der Raumzeit an diesem Punkt kennzeichnet.\n",
        "\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "8XJQVKeDBC-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmHevWQ8UMO"
      },
      "source": [
        "**Our goal: detect curvature in space, but spreading of geodesics can happen also in flat space and there is no curvature. That's why we use the second devriative**.\n",
        "\n",
        "> We choose the second derivative because it will tell us about the spreading due to the curvature of space and not the spreading that could happen in flat space where the geodesics are angled in specific directions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_316.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_317.png)\n",
        "\n",
        "**Ricci Tensor: Sectional Curvature**\n",
        "\n",
        "The terming the numerator (on top) gives us the sign we need to determine how the geodesics travel in direction v are either converging or diverging. And the noralization in the denominator (bottom) keeps the result the same regardless of the length of s and v.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_322.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Tensor Calculus 25 - Geometric Meaning Ricci Tensor/Scalar (Volume Form)](https://www.youtube.com/watch?v=oQZTYt_Pxcc&t=570s)\n",
        "\n",
        "**Ricci Tensor**: Track \"volume change\" along geodesics\n",
        "\n",
        "1. Sectional Curvature: Orthonormal basis\n",
        "\n",
        "2. Volume element derivative: any basis\n",
        "\n",
        "> Ricci tensor tells us how volumes change as we move around in space along geodesics\n",
        "\n",
        "Video: [Tensor Calculus 24: Ricci Tensor Geometric Meaning (Sectional Curvature)](https://www.youtube.com/watch?v=ZhDNijOEw0Y&t=332s)\n",
        "\n",
        "**Ricci Tensor: Track volume change along geodesics**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_310.png)\n",
        "\n",
        "**Ricci Tensor**\n",
        "\n",
        "$R_{\\mu \\nu}$ = Einstein Field equation\n",
        "\n",
        "* The change of the size of the circle, given by the Ricci tensor, represents how quickly these two people get drawn together\n",
        "\n",
        "* The more spacetime is curved, the more quickly bodies will get drawn together\n",
        "\n",
        "* In general relativity gravitational attraction is just the natural result of curved spacetime (it doesn't require any forces like we see with Newtonian)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_309.png)\n",
        "\n",
        "**The first term measures changing from curved space, meanwhile the second term measures changes coming from flat space:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_369.png)\n",
        "\n",
        "> **Ricci scalar**: Keeps track of how the size of a ball deviates\n",
        "from standard flat-space size.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/tensor_372.png)"
      ],
      "metadata": {
        "id": "Vwm86DvIdeD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Geometric Algebra*"
      ],
      "metadata": {
        "id": "q09fihf25FPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Noncommutative Geometry*"
      ],
      "metadata": {
        "id": "oEn5oZesJVdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ocw.mit.edu/courses/18-238-geometry-and-quantum-field-theory-fall-2002/"
      ],
      "metadata": {
        "id": "82SPYo_nXk8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.mpim-bonn.mpg.de/node/111"
      ],
      "metadata": {
        "id": "c9gNGxhhXfN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Noncommutative_geometry\n",
        "\n",
        "Noncommutative Algebra: Weyl algebra, Clifford algebra, Superalgebra\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Noncommutative_ring"
      ],
      "metadata": {
        "id": "iUaQzHb9JY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences between commutative and noncommutative algebra**\n",
        "\n",
        "Because noncommutative rings of scientific interest are more complicated than commutative rings, their structure, properties and behavior are less well understood. A great deal of work has been done successfully generalizing some results from commutative rings to noncommutative rings. A major difference between rings which are and are not commutative is the necessity to separately consider [right ideals and left ideals](https://en.m.wikipedia.org/wiki/Ideal_(ring_theory)#Definitions_and_motivation). It is common for noncommutative ring theorists to enforce a condition on one of these types of ideals while not requiring it to hold for the opposite side. For commutative rings, the left–right distinction does not exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "pwIYqOy9JnTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Clifford Algebra*"
      ],
      "metadata": {
        "id": "HrtIDW9V0p96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Hierarchy of Clifford Algebra $Cℓ_{p,q}$](https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras)**\n",
        "\n",
        "*Clifford algebra over the reals is also called Geometric algebra*\n",
        "\n",
        "Algebra $\\rightarrow$ [Signature](https://en.m.wikipedia.org/wiki/Metric_signature) $\\rightarrow$ Equation\n",
        "\n",
        "${Cl}_{4,2} (\\mathbb {R})$ - [Conformal geometric algebra (CGA)](https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra) $\\rightarrow$ +,+,+,+,-,-, $\\rightarrow$ [Twistor](https://en.m.wikipedia.org/wiki/Twistor_space) $\\rightarrow$ [twistor theory](https://en.m.wikipedia.org/wiki/Twistor_theory)\n",
        "\n",
        "\n",
        "\n",
        "${Cl}_{1,3} (\\mathbb {R})$ - [Spacetime algebra\n",
        "](https://en.m.wikipedia.org/wiki/Spacetime_algebra) and ${Cl}_{1,3} (\\mathbb {C})$ - [Dirac algebra\n",
        "](https://en.m.wikipedia.org/wiki/Dirac_algebra) $\\rightarrow$ +,-,-,-, $\\rightarrow$ [Dirac equation](https://en.m.wikipedia.org/wiki/Dirac_equation) $\\rightarrow$ [relativistic spin-1/2](https://en.m.wikipedia.org/wiki/Relativistic_wave_equations#Spin_1.2F2) $\\rightarrow$ [Gamma matrices](https://en.m.wikipedia.org/wiki/Gamma_matrices)\n",
        "\n",
        "${Cl}_{3,0} (\\mathbb {R})$ - [Algebra of physical space (Pauli algebra)\n",
        "](https://en.m.wikipedia.org/wiki/Algebra_of_physical_space) $\\rightarrow$ +,+,+ $\\rightarrow$ [Pauli equation](https://en.m.wikipedia.org/wiki/Pauli_equation) $\\rightarrow$ [spin-1/2](https://en.m.wikipedia.org/wiki/Spin-1/2) $\\rightarrow$ [Pauli matrices](https://en.m.wikipedia.org/wiki/Pauli_matrices)\n",
        "\n",
        "${Cl}_{0,3} (\\mathbb {R})$ - [i.e. Quaternions](https://en.m.wikipedia.org/wiki/Clifford_algebra#Quaternions)\n",
        "\n",
        "\n",
        "${Cl}_{0,1} (\\mathbb {R})$ - [Clifford_algebra#Real_numbers](https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers) $\\rightarrow$ - $\\rightarrow$ [Schrödinger equation](https://en.m.wikipedia.org/wiki/Schr%C3%B6dinger_equation) $\\rightarrow$ spin-0\n",
        "\n",
        "> *See also: For a complete classification of these algebras see [Classification of Clifford algebras](https://en.m.wikipedia.org/wiki/Classification_of_Clifford_algebras).*\n"
      ],
      "metadata": {
        "id": "77his5x9yBCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Clifford Algebra $Cl_{p,q} (\\mathbb{C})$ (Incl. Weyl Algebra)*\n",
        "\n",
        "Paper: [On Clifford groups in quantum computing](https://arxiv.org/abs/1810.10259)\n",
        "\n",
        "> In quantum computing and quantum information theory, the [Clifford gates](https://en.m.wikipedia.org/wiki/Clifford_gates) are the elements of the Clifford group (siehe [Clifford Algebra](https://de.m.wikipedia.org/wiki/Clifford-Algebra)), a set of mathematical transformations which effect permutations of the [Pauli operators (Pauli group)](https://en.m.wikipedia.org/wiki/Pauli_group).\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "https://clifford.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_gates\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/representations-and-spinors/\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Majorana_equation\n",
        "\n",
        "https://www.mathphysicsbook.com/mathematics/clifford-groups/classification-of-clifford-algebras/pauli-and-dirac-matrices/\n"
      ],
      "metadata": {
        "id": "ginSvag_5QfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Properties**\n",
        "\n",
        "* In Geometric Algebra the **basis vectors for the space are typically real valued vectors**. Complex valued vectors have uses in GA (i.e. frequency domain representation of vectors in electrodynamics), but the underlying basis for the vector space is still real valued (i.e. span{𝐞1,𝐞2,𝐞3}.\n",
        "\n",
        "* Clifford algebras provide a further generalization, **allowing those basis vectors to reside in a complex vector space**, with suitable modifications of the vector product rules.\n",
        "\n",
        "https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra\n",
        "\n",
        "* Clifford Algebra focuses on abstract mathematical and algebraic properties\n",
        "\n",
        "* Geometric Algebra focuses on geometric and physical applications\n",
        "\n",
        "> A [Clifford algebra](https://en.m.wikipedia.org/wiki/Clifford_algebra) is an algebra generated by a vector space with a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form), and is a unital [associative algebra](https://en.m.wikipedia.org/wiki/Associative_algebra) (not commutative, but anti-commutative!).\n",
        "\n",
        "* The best known example of a quadratic form is the square of the amount of a vector: $|\\vec{v}|^{2}=x^{2}+y^{2}+z^{2}+\\ldots$ (like used in quantum mechanics on Bloch sphere)\n",
        "\n",
        "* As K-algebras, they generalize the real numbers, complex numbers, quaternions and several other hypercomplex number systems.\n",
        "\n",
        "> **The theory of Clifford algebras is intimately connected with the theory of [quadratic forms](https://en.m.wikipedia.org/wiki/Quadratic_form) and [orthogonal transformations](https://en.m.wikipedia.org/wiki/Orthogonal_group) (Orthogonal group, like SO(2), SO(3) and SO(4)).**\n",
        "\n",
        "* Clifford algebras have important applications in a variety of fields including geometry, theoretical physics and digital image processing.\n",
        "\n",
        "A Clifford algebra is a unital associative algebra that contains and is generated by a vector space $V$ over a field $K_{1}$ where $V$ is equipped with a quadratic form $Q: V \\rightarrow K$. The Clifford algebra $\\mathrm{Cl}(V, Q)$ is the \"freest\" algebra generated by $V$ subject to the condition\n",
        "\n",
        "> $v^{2}=Q(v) 1$ for all $v \\in V$\n",
        "\n",
        "where the product on the left is that of the algebra, and the 1 is its [multiplicative identity](https://en.m.wikipedia.org/wiki/Identity_element#Definitions) (neutral element).\n",
        "\n",
        "See also: https://en.m.wikipedia.org/wiki/Clifford_algebra#Universal_property_and_construction"
      ],
      "metadata": {
        "id": "f7RjlHat54zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extensions & Generalizations**\n",
        "\n",
        "More precisely, **Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra**, in the same way that the Weyl algebra is a quantization of the symmetric algebra.\n",
        "\n",
        "Weyl algebras and Clifford algebras admit a further structure of a [*-algebra](https://en.m.wikipedia.org/wiki/*-algebra), and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "Source: https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "[Weyl algebra](https://en.m.wikipedia.org/wiki/Weyl_algebra), a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the [symmetric algebra](https://en.m.wikipedia.org/wiki/Symmetric_algebra) by a [symplectic form](https://en.m.wikipedia.org/wiki/Symplectic_vector_space)\n",
        "\n",
        "Clifford algebra, a [quantum deformation](https://en.m.wikipedia.org/wiki/Quantum_group) of the exterior algebra by a [quadratic form](https://en.m.wikipedia.org/wiki/Quadratic_form).\n",
        "\n",
        "The Weyl algebra is also referred to as the symplectic Clifford algebra. Weyl algebras represent the same structure for symplectic bilinear forms that Clifford algebras represent for non-degenerate symmetric bilinear forms.\n",
        "\n",
        "*Clifford Algebra & Free Algebra*\n",
        "\n",
        "\n",
        "* for Clifford Algebra: The idea of being the \"freest\" or \"most general\" algebra subject to this identity can be formally expressed through the notion of a [universal property](https://en.m.wikipedia.org/wiki/Universal_property) (from category theory).\n",
        "\n",
        "* in the area of abstract algebra known as ring theory, a [free algebra](https://en.m.wikipedia.org/wiki/Free_algebra) is the noncommutative analogue of a polynomial ring since its elements may be described as \"polynomials\" with non-commuting variables. Likewise, the polynomial ring may be regarded as a free commutative algebra.\n",
        "\n",
        "*Clifford Algebra & Exterior Algebra*\n",
        "\n",
        "* Clifford Algebra as a quantization of the exterior algebra\n",
        "\n",
        "* Clifford algebras are closely related to exterior algebras. Indeed, if Q = 0 then the Clifford algebra Cl(V, Q) is just the exterior algebra ⋀(V). For nonzero Q there exists a canonical linear isomorphism between ⋀(V) and Cl(V, Q) whenever the ground field K does not have characteristic two.\n",
        "\n",
        "* **Clifford multiplication together with the distinguished subspace is strictly richer than the exterior product since it makes use of the extra information provided by Q.**\n",
        "\n",
        "* The Clifford algebra is a [filtered algebra](https://en.m.wikipedia.org/wiki/Filtered_algebra), the [associated graded algebra](https://en.m.wikipedia.org/wiki/Associated_graded_ring) is the exterior algebra.\n",
        "\n",
        "* More precisely, Clifford algebras may be thought of as quantizations (cf. quantum group) of the exterior algebra, **in the same way that the Weyl algebra is a quantization of the symmetric algebra.**\n",
        "\n",
        "* Weyl algebras and Clifford algebras admit a further structure of a *-algebra, and can be unified as even and odd terms of a [superalgebra](https://en.m.wikipedia.org/wiki/Superalgebra), as discussed in [CCR and CAR algebras](https://en.m.wikipedia.org/wiki/CCR_and_CAR_algebras).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Clifford_algebra#Relation_to_the_exterior_algebra\n",
        "\n",
        "*Clifford Algebra & Geometric Algebra*\n",
        "\n",
        "* Geometric Algebra is a special form of the more general Clifford algebra\n",
        "\n",
        "* Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.\n",
        "\n",
        "*Other notes*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra\n",
        "\n",
        "...the geometric algebra for this quadratic space is the Clifford algebra... [Source](https://en.m.wikipedia.org/wiki/Geometric_algebra)\n",
        "\n",
        "Clifford algebras were born of a synthesis of inner product spaces and Grassmann's exterior algebras, both of which have geometric applications.\n",
        "\n",
        "A Clifford algebra is constructed from an inner product space (𝑉,𝑄) by generating an associative algebra (whose product is a descendant of the tensor product in the tensor algebra for 𝑉). These are compatible in a sense made clear in the Wiki.\n",
        "\n",
        "https://math.stackexchange.com/questions/182024/relation-between-interior-product-inner-product-exterior-product-outer-produc"
      ],
      "metadata": {
        "id": "P3d4_VAH6tFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*Applications*\n",
        "\n",
        "* One of the principal applications of the exterior algebra is in differential geometry where it is used to define the bundle of differential forms on a smooth manifold. In the case of a (pseudo-)Riemannian manifold, the tangent spaces come equipped with a natural quadratic form induced by the metric. Thus, one can define a Clifford bundle in analogy with the exterior bundle. This has a number of important applications in Riemannian geometry. Perhaps more importantly is the link to a spin manifold, its associated spinor bundle and spinc manifolds.\n",
        "\n",
        "* More: https://en.m.wikipedia.org/wiki/Clifford_algebra#Applications\n",
        "\n",
        "*Examples of Clifford Algebras $Cℓ_{p,q}$*\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras\n",
        "\n",
        "Different focusea created based on use cases / application\n",
        "\n",
        "Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "See also hierrchy of clifford algebra: https://en.m.wikipedia.org/wiki/Basil_Hiley#\n",
        "\n"
      ],
      "metadata": {
        "id": "FHJ0usg67LAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Properties of Geometric Algebra*"
      ],
      "metadata": {
        "id": "YWqVgPOwPCIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=60z_hpEAtD8&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=19)\n",
        "\n",
        "Video: [Addendum to A Swift Introduction to Geometric Algebra](https://www.youtube.com/watch?v=0bOiy0HVMqA&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)\n",
        "\n",
        "Video: [From Zero to Geo Introduction (Geometric Algebra Series)](https://www.youtube.com/watch?v=2hBWCCAiCzQ&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq)"
      ],
      "metadata": {
        "id": "P5GPkVzEPEgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometric Algebra $Cl_{p,q} (\\mathbb{R})$ or $G(M^n)$ (Clifford algebra over reals)\n",
        "\n",
        "> In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does [Source](https://en.m.wikipedia.org/wiki/Vector_calculus)\n",
        "\n",
        "> Geometric Algebra is the Clifford Algebra over the field of real numbers.\n",
        "\n",
        "[**Geometric Product**](https://en.m.wikipedia.org/wiki/Geometric_algebra) *Sum of inner product (dot) and outer (not exterior??) product (wedge). Is the geometric product of any two vectors a and b as the sum of a symmetric product and an antisymmetric product:*\n",
        "\n",
        "> $a b=\\frac{1}{2}(a b+b a)+\\frac{1}{2}(a b-b a) =a \\cdot b+a \\wedge b$\n",
        "\n",
        "> $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n"
      ],
      "metadata": {
        "id": "cHOe2QmAPN5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Geometric Product: Sum of inner product (dot) and outer product (wedge)**: $\\vec{u} \\vec{v}=\\vec{u} \\cdot \\vec{v}+\\vec{u} \\wedge \\vec{v}$\n",
        "\n",
        "* Magnitude of u $\\wedge$ v (=bivector) is the area of a parallelogram - similar to cross product, but not restricted to R3 like [cross product](https://en.m.wikipedia.org/wiki/Cross_product) (including magnitude and orientation)\n",
        "\n",
        "* Geometric algebra (GA) is an extension or completion of vector algebra (VA)\n",
        "\n",
        "* **Geometric Algebra: special form of the more general Clifford algebra**\n",
        "\n",
        "* the [geometric algebra](https://en.m.wikipedia.org/wiki/Geometric_algebra) (GA) of a vector space with a quadratic form (usually the Euclidean metric or the Lorentz metric) is an algebra over a field, the Clifford algebra of a vector space with a quadratic form with its multiplication operation called the geometric product.\n",
        "\n",
        "> **The algebra elements are called multivectors, which contains both the scalars $F$ and the vector space $V$.**\n",
        "\n",
        "* Examples of geometric algebras applied in physics include the spacetime algebra (and the less common algebra of physical space) and the conformal geometric algebra.\n",
        "\n",
        "* Geometric calculus, an extension of GA that incorporates differentiation and integration, can be used to formulate other theories such as complex analysis and differential geometry, **e.g. by using the Clifford algebra instead of differential forms**.\n",
        "\n",
        "* Geometric algebra has been advocated as the **preferred mathematical framework for physics**. Proponents claim that it provides compact and intuitive descriptions in many areas including classical and quantum mechanics, electromagnetic theory and relativity. GA has also found use as a computational tool in computer graphics and robotics.\n"
      ],
      "metadata": {
        "id": "tE1L0QYwPPsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differential forms are included in geometric algebra**\n",
        "\n",
        "* Scalar = 0D objects\n",
        "* Vector = 1D object - oriented line, its magnitiude is its length, many vectors with same magnitude and same orientation are same vectors\n",
        "* Bivector = 2D object - oriented area, its magnitude is its area, many areas with same magnitude and same orientation\n",
        "* Trivector = 3D - oriented volume\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "AuG47_giPSx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Differences to other algebras**\n",
        "\n",
        "* **Geometric Algebra vs Clifford algebra**: [Source](https://www.quora.com/How-are-geometric-algebra-and-Clifford-algebra-different)\n",
        "\n",
        "  * **Geometric algebra is distinguished from Clifford algebra in general by its restriction to real numbers and its emphasis on its geometric interpretation and physical applications.**\n",
        "\n",
        "  * A Clifford algebra is a unital associative algebra that contains and is generated by a vector space V over a field K, where V is equipped with a quadratic form Q.\n",
        "\n",
        "  * A Geometric algebra is a Clifford algebra of a vector space over the field of real numbers endowed with a quadratic form.\n",
        "\n",
        "  * **So a Geometric Algebra is a special form of the more general Clifford algebra**. In particular it is a CA over the reals. Additionally rather than being just an abstract object it has specific geometric meaning. For instance the 3+1 dimensional spacetime algebra is a Geometric algebra.\n",
        "\n",
        "* **Geometric Algebra vs Tensor Algebra**: Every element of a geometric algebra can be identified with a tensor, but not every tensor can be identified with an element of a geometric algebra [Source](\n",
        "https://math.stackexchange.com/questions/725350/is-geometric-algebra-isomorphic-to-tensor-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Exterior Algebra**: Exterior algebra defines an antisymmetric wedge product. In an exterior algebra, one can add k-forms to other k-forms, but would not add forms of different rank. This restriction is relaxed in geometric algebra (GA). [Source](https://math.stackexchange.com/questions/1991814/whats-the-difference-between-geometric-exterior-and-multilinear-algebra)\n",
        "\n",
        "* **Geometric Algebra vs Vector Algebra**: [Source](\n",
        "https://en.m.wikipedia.org/wiki/Comparison_of_vector_algebra_and_geometric_algebra):\n",
        "\n",
        "  * Geometric algebra is an extension of vector algebra, providing additional algebraic structures on vector spaces, with geometric interpretations. Vector algebra uses all dimensions and signatures, as does geometric algebra, notably 3+1 spacetime as well as 2 dimensions.\n",
        "\n",
        "  * For example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. **The torque or curl is then a normal vector field in this 3rd dimension**.\n",
        "\n",
        "  * By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product."
      ],
      "metadata": {
        "id": "HkxCUeosPWmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example: The cross product in relation to the exterior product. In red are the orthogonal unit vector, and the \"parallel\" unit bivector.*\n",
        "\n",
        "* $\\mathbf u \\times \\mathbf v$ is perpendicular to the plane containing $\\mathbf {u}$ and $\\mathbf {v}$\n",
        "\n",
        "* $\\mathbf u \\wedge \\mathbf v$ is an oriented representation of the same plane.\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Exterior_calc_cross_product.svg/260px-Exterior_calc_cross_product.svg.png)"
      ],
      "metadata": {
        "id": "GLlRNs2BPZpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/N_vector_positive.svg/417px-N_vector_positive.svg.png)"
      ],
      "metadata": {
        "id": "9tCNN2HDPcAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "example: **Conformal geometric algebra $Cℓ_{4,2}$ - Twistor equation**"
      ],
      "metadata": {
        "id": "nt89GGCOPeSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Paravector"
      ],
      "metadata": {
        "id": "85mklZLQPgRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versor"
      ],
      "metadata": {
        "id": "GHAYUzVUPiWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$C_{0,1} (\\mathbb{R})$ - Schrödinger Equation*"
      ],
      "metadata": {
        "id": "4Q18zgBcxYJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Basil_Hiley#Hierarchy_of_Clifford_algebras"
      ],
      "metadata": {
        "id": "hqHXXeogxwel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$Cℓ_{3,0}(\\mathbb{R})$ - Pauli Algebra and Algebra of Physical Space*"
      ],
      "metadata": {
        "id": "ur1A1K3aOl3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algebra of physical space $Cℓ_{3,0}$ - Pauli equation**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Algebra_of_physical_space\n",
        "\n",
        "used in special relativity, classical electrodynamics and relativistic quantum mechanics (Dirac equation in quantum field theory)\n",
        "\n",
        "Whereas the mathematicians do not give special attention to the case  𝑛=2, the physicists, dealing with four-dimensional space-time, have every reason to do so, and it turns out to be most rewarding to develop procedures and proofs for the special case rather than refer to the general mathematical theorems. The technique for such a program has been developed some years ago [Source](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)"
      ],
      "metadata": {
        "id": "C_DZg7k4OpnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pauli Algebra $A_2$**\n",
        "\n",
        "See article [The Pauli Algebra - Mathematics LibreTexts](https://math.libretexts.org/Bookshelves/Abstract_and_Geometric_Algebra/Applied_Geometric_Algebra_(Tisza)/02%3A_The_Lorentz_Group_and_the_Pauli_Algebra/2.04%3A_The_Pauli_Algebra)\n",
        "\n",
        "$\\mathcal{A}_{2}$ is called the Pauli algebra. The basis matrices are\n",
        "\n",
        "> $\n",
        "\\begin{array}{c}\n",
        "\\sigma_{0}=I=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{1}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{2}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right) \\\\\n",
        "\\sigma_{3}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "\n",
        "The [Pauli matrices](https://de.m.wikipedia.org/wiki/Pauli-Matrizen) are the following four 2 × 2 matrices:\n",
        "\n",
        "> $\\sigma_{0}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right), \\sigma_{1}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right), \\sigma_{2}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right), \\sigma_{3}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)$\n",
        "\n",
        "\n",
        "* The three Pauli matrices satisfy the well known multiplication rules\n",
        "\n",
        "* All of the basis matrices are Hermitian, or self-adjoint\n",
        "\n",
        "**Pauli Matrices (Pauli Operators)**\n",
        "\n",
        "The Pauli matrices are [involutory](https://en.m.wikipedia.org/wiki/Involutory_matrix) (a square matrix that is its own inverse), meaning that the square of a Pauli matrix is the identity matrix.\n",
        "\n",
        ">$\n",
        "I^{2}=X^{2}=Y^{2}=Z^{2}=-i X Y Z=I\n",
        "$\n",
        "\n",
        "The Pauli matrices also [anti-commute](https://en.m.wikipedia.org/wiki/Anticommutative_property), for example $Z X=i Y=-X Z$.\n",
        "\n",
        "*Anticommutativity is a specific property of some non-commutative operations. In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments. **Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments**. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation, such as addition.*\n",
        "\n",
        "Single spin one half particle, focus on spin degrees of freedom:\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad 1 \\downarrow=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$\n",
        "\n",
        "**X, Y and Z axis on Bloch sphere:**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_077.PNG)\n",
        "\n",
        "Source: https://www.researchgate.net/figure/The-Bloch-sphere-representation-of-a-qubit-The-basis-states-are-located-at-the-north_fig2_284259345\n",
        "\n",
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Bloch_Sphere.svg/423px-Bloch_Sphere.svg.png)\n",
        "\n",
        "*Bloch sphere*"
      ],
      "metadata": {
        "id": "Vi6h1_4V3hsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$Cℓ_{1,3} (\\mathbb{C})$ - Dirac Equation*"
      ],
      "metadata": {
        "id": "QBuL0NbxO6cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dirac equation for spin-½ particles with a matrix representation of the **gamma matrices, which represent the generators of the algebra**\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Dirac_algebra"
      ],
      "metadata": {
        "id": "js5NF8RU528q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Gamma_matrices"
      ],
      "metadata": {
        "id": "KvGxpffVe4fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$Cℓ_{4,2}(\\mathbb{R})$ - Twistor (Conformal geometric algebra)*"
      ],
      "metadata": {
        "id": "Ym5Ax5nCOtuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Conformal_geometric_algebra"
      ],
      "metadata": {
        "id": "g1zDxtzDOvrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$Cℓ_{1,3} (\\mathbb{R})$ - Spacetime Algebra*"
      ],
      "metadata": {
        "id": "M5NxmpI85zgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A Swift Introduction to Spacetime Algebra](https://www.youtube.com/watch?v=e7aIVSVc8cI&list=PLBn8lN0DcvpkjulnKfyCmgO5SWzRxrRFq&index=21)\n",
        "\n",
        "> Spacetime Algebra: Geometric Algebra + Special Relativity\n",
        "\n",
        "**Spacetime Algebra $Cℓ_{1,3}$ - Dirac equation**\n",
        "\n",
        "> Spacetime algebra concerns the Clifford algebra Cl1,3(R) of the four-dimensional [Minkowski spacetime](https://en.m.wikipedia.org/wiki/Minkowski_space)\n",
        "\n",
        "* In mathematical physics, [spacetime algebra (STA)](https://en.m.wikipedia.org/wiki/Spacetime_algebra) is a name for the Clifford algebra Cl1,3(R), or equivalently the geometric algebra G(M4).\n",
        "\n",
        "* According to David Hestenes, spacetime algebra can be particularly closely associated with the geometry of special relativity and relativistic spacetime.\n",
        "\n",
        "* It is a vector space that allows not only vectors, but also bivectors (directed quantities associated with particular planes, such as areas, or rotations) or blades (quantities associated with particular hyper-volumes) to be combined, as well as rotated, reflected, or [Lorentz boosted](https://en.m.wikipedia.org/wiki/Lorentz_transformation#boost).\n",
        "\n",
        "* It is also the natural parent algebra of spinors in special relativity."
      ],
      "metadata": {
        "id": "T8yu7WqUO26x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$Cℓ_{0,0}, Cℓ_{0,1}, Cℓ_{0,2}(\\mathbb{R})$*"
      ],
      "metadata": {
        "id": "SN4YJmrE6HF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few low-dimensional cases are:\n",
        "\n",
        "* Cl0,0(R) is naturally **isomorphic to R** since there are no nonzero vectors.\n",
        "* Cl0,1(R) is a two-dimensional algebra generated by e1 that squares to −1, and is algebra-isomorphic to C, the field of **complex numbers**.\n",
        "* Cl0,2(R) is a four-dimensional algebra spanned by {1, e1, e2, e1e2}. The latter three elements all square to −1 and anticommute, and so the algebra is isomorphic to the **quaternions** H.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Clifford_algebra#Real_numbers\n"
      ],
      "metadata": {
        "id": "nywgP0rp6VF5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdr6F1zuU4MG"
      },
      "source": [
        "##### <font color=\"blue\">*Variationsanalyse*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AR9PIXysIa4"
      },
      "source": [
        "###### *Variational Principle*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Mathematisch gesehen ist die Wirkung ein Funktional. Während Funktionen bestimmten Zahlen andere Zahlen zuordnen, ordnen Funktionale bestimmten Funktionen Zahlen zu.\n",
        "\n",
        "https://www.spektrum.de/news/jenseits-von-einsteins-gravitationstheorie/1997152"
      ],
      "metadata": {
        "id": "7qhLvSkEBgaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXIblPI6_-oP"
      },
      "source": [
        "**Variationsrechnung**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Variationsrechnung\n",
        "\n",
        "* calculus og variation, khan academy https://youtube.com/playlist?list=PLdgVBOaXkb9CD8igcUr9Fmn5WXLpE8ZE_\n",
        "\n",
        "* **Find [stationary points](https://internal.ncl.ac.uk/ask/numeracy-maths-statistics/core-mathematics/calculus/stationary-points.html) (=derivative is zero, local minima or maxima) of a functional, like an integral I[f] (=here for example the path lenghts, or time spent travelling) is minimal between two points a and b.**\n",
        "\n",
        "  * A stationary point of a function $f(x)$ is a point where the derivative of $f(x)$ is equal to 0 .\n",
        "  * These points are called \"stationary\" because at these points the function is neither increasing nor decreasing.\n",
        "  * Graphically, this corresponds to points on the graph of $f(x)$ where the tangent to the curve is a horizontal line.\n",
        " * The stationary points of a function $y=f(x)$ are the solutions to $\n",
        "\\frac{d y}{d x}=0 $. This repeats in mathematical notation the definition given above: \"points where the gradient of the function is zero\".\n",
        "\n",
        "* **The integral is a functional (=function of functions), its stationary point is a fix point / minima of a functional (not function). Solve (usually differential) equations for stationary function f(x) (via calculus of variations)**\n",
        "\n",
        "* from regular calculus to calculus of variations: find stationary functions, not only stationary points, a function becomes a functional.\n",
        "\n",
        "* **Typical problem in variational calculus: find minimal path between points A and B, not necessarily a linear one (in physics for examples check Brachistochrone !)**.\n",
        "\n",
        "* Also consider that velocity depending on position changes the minimum paths or time to travel (later in vector analysis relevant for Kurvenintegral)\n",
        "\n",
        "In general, Calculus of variations seeks to find y = f(x) such that this integral:\n",
        "\n",
        "> $I[f]=\\int_{x_{1}}^{x_{2}} F\\left(x, y, \\frac{d y}{d x}\\right) d x$\n",
        "\n",
        "is stationary (ps: $\\frac{d y}{d x}$ = $y'$)\n",
        "\n",
        "1. Die [Variationsrechnung](https://de.wikipedia.org/wiki/Variationsrechnung) ist eine **Erweiterung der Funktionalanalysis und beschaeftigt sich mit <u>nichtlinearen Funktionalen</u>** (in der Funktionalanalysis sind es linear Funktionale)\n",
        "\n",
        "2. The [calculus of variations](https://en.m.wikipedia.org/wiki/Calculus_of_variations) is a field that **uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals**: mappings from a set of functions to the real numbers. Functionals are often expressed as definite integrals involving functions and their derivatives. <u>**Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations.**</u>\n",
        "\n",
        "* In calculus of variations we are **NOT concerned with finding fix points of functions (like local maxima in a function), but rather fix points of functionals.**\n",
        "\n",
        "\n",
        "* dann führt eine Variation der Wirkung: https://de.m.wikipedia.org/wiki/Feldtheorie_(Physik)#Formalismus\n",
        "\n",
        "* Beispiel: https://de.wikipedia.org/wiki/Fluiddynamik\n",
        "\n",
        "* Martin: formulier problem in variationelle formulierung (dann bist du in sobolove räume), und dann Eigenschaften von Testfunktionen ausnutzen\n",
        "\n",
        "* **Variation der Elemente**: die [Variation der Elemente](https://de.wikipedia.org/wiki/Variation_der_Elemente) ist eine im 19. Jahrhundert entwickelte Methode zur genauen Bahnbestimmung von Himmelskörpern. Sie dient bis heute zur Modellierung von [Bahnstörungen](https://de.wikipedia.org/wiki/Bahnstörung).\n",
        "\n",
        "* **History of variational principles in physics**:\n",
        "https://en.m.wikipedia.org/wiki/History_of_variational_principles_in_physics\n",
        "\n",
        "* [Gâteaux-Differential](https://de.wikipedia.org/wiki/Gâteaux-Differential) ist eine **Verallgemeinerung des gewöhnlichen Differentiationsbegriffes** dar, indem es die Richtungsableitung auch in unendlichdimensionalen Räumen definiert.\n",
        "\n",
        "* Variational method in quantum mechanics: In quantum mechanics, the [variational method](https://en.m.wikipedia.org/wiki/Variational_method_(quantum_mechanics)) is one way of finding approximations to the lowest energy eigenstate or ground state, and some excited states. This allows calculating approximate wavefunctions such as molecular orbitals. The basis for this method is the variational principle.\n",
        "\n",
        "Die Variationsrechnung beschäftigt sich mit der Minimierung bzw. Maximierung von Funktionalen, die als Integral dargestellt werden können. Man könnte sie daher als „natürliche“ Methode zur Lösung physikalischer Probleme bezeichnen, da die Physik ja bekanntlich von Extremalprinzipen regiert wird (kürzeste Bahn, kleinste Wirkung, Gesamtenergie, Hamilton-Funktion). Die „Variation“ dieser Integralbeziehung bezüglich einer abhängigen Größe (in der Physik z.B. der Bahnkurve im Zustandsraum) führt auf eine Differentialgleichung, deren Lösung diesen Integralausdruck minimiert respektive maximiert.\n",
        "\n",
        "https://link.springer.com/chapter/10.1007/978-3-642-83621-3_7\n",
        "\n",
        "Der Name Variationsrechnung bezieht sich dabei auf die Technik der Variation der Argumente. Wesentliches Ziel der Variationsrechnung ist das Finden von Extrema (haufig unter Nebenbedingungen) für ein gegebenes Funktional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqXlKua2Mj8"
      },
      "source": [
        "**Variational Principle**\n",
        "\n",
        "* a [variational principle](https://en.wikipedia.org/wiki/Variational_principle) is one that enables a problem to be solved using calculus of variations, which concerns finding such functions which optimize the values of quantities that depend upon those functions.\n",
        "\n",
        "* For example, the problem of determining the shape of a hanging chain suspended at both ends—a catenary—can be solved using variational calculus, and in this case, the variational principle is the following: The solution is a function that minimizes the gravitational potential energy of the chain.\n",
        "\n",
        "* Any physical law which can be expressed as a variational principle describes a **self-adjoint operator.** These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/1/10/Total_variation.gif)\n",
        "\n",
        "*As the green ball travels on the graph of the given function, the length of the path travelled by that ball's projection on the y-axis, shown as a red ball, is the total variation of the function.*\n",
        "\n",
        "* the [total variation](https://en.wikipedia.org/wiki/Total_variation) identifies several slightly different concepts, related to the (local or global) structure of the codomain of a function or a measure. For a real-valued continuous function f, defined on an interval [a, b] ⊂ ℝ, its total variation on the interval of definition is a measure of the one-dimensional [arclength](https://en.wikipedia.org/wiki/Arc_length) of the curve with parametric equation x ↦ f(x), for x ∈ [a, b].\n",
        "\n",
        "* In der Variationsrechnung und der Theorie der stochastischen Prozesse ist die [Variation](https://de.wikipedia.org/wiki/Variation_(Mathematik)) (auch totale Variation genannt) einer Funktion **ein Maß für das lokale Schwingungsverhalten der Funktion**.\n",
        "\n",
        "* Bei den stochastischen Prozessen ist die Variation von besonderer Bedeutung, da sie die Klasse der zeitstetigen Prozesse in zwei fundamental verschiedene Unterklassen unterteilt: jene mit endlicher und solche mit unendlicher Variation.\n",
        "\n",
        "Die [erste Variation](https://de.wikipedia.org/wiki/Erste_Variation) ist eine verallgemeinerte Richtungsableitung eines Funktionals. Ihre Eigenschaften sind in der angewandten Mathematik und der theoretischen Physik relevant. Die erste Variation spielt eine zentrale Rolle in der Variationsrechnung und wird in der analytischen Mechanik genutzt. Ein verwandtes Konzept ist die Funktionalableitung.\n",
        "\n",
        "In der Analysis ist eine Funktion von [beschränkter Variation](https://de.wikipedia.org/wiki/Beschränkte_Variation) (beschränkter Schwankung), wenn ihre totale Variation (totale Schwankung) endlich ist, sie also in gewisser Weise nicht beliebig stark oszilliert. Diese Begriffe hängen eng mit der Stetigkeit und der Integrierbarkeit von Funktionen zusammen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anwendungsgebiete**\n",
        "\n",
        "* Die Variationsrechnung ist die mathematische Grundlage aller physikalischen Extremalprinzipien und deshalb besonders in der theoretischen Physik wichtig, so etwa\n",
        "\n",
        "  * im Lagrange-Formalismus der klassischen Mechanik\n",
        "\n",
        "  * bzw. der Bahnbestimmung, in der Quantenmechanik in Anwendung des Prinzips der kleinsten Wirkung\n",
        "\n",
        "  * und in der statistischen Physik im Rahmen der Dichtefunktionaltheorie.\n",
        "\n",
        "  * In der Mathematik wurde die Variationsrechnung beispielsweise bei der riemannschen Behandlung des Dirichlet-Prinzips für harmonische Funktionen verwendet.\n",
        "\n",
        "  * Auch in der Steuerungs- und Regelungstheorie findet die Variationsrechnung Anwendung, wenn es um die Bestimmung von Optimalreglern geht.\n",
        "\n",
        "* Ein typisches Anwendungsbeispiel ist das Brachistochronenproblem: Auf welcher Kurve in einem Schwerefeld von einem Punkt A zu einem Punkt B, der unterhalb, aber nicht direkt unter A liegt, benötigt ein Objekt die geringste Zeit zum Durchlaufen der Kurve? Von allen Kurven zwischen A und B minimiert eine den Ausdruck, der die Zeit des Durchlaufens der Kurve beschreibt. Dieser Ausdruck ist ein Integral, das die unbekannte, gesuchte Funktion, die die Kurve von A nach B beschreibt, und deren Ableitungen enthält."
      ],
      "metadata": {
        "id": "A4hJCXFKFMa9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkRUt1ihCd_"
      },
      "source": [
        "**Fundamentallemma der Variationsrechnung**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Fundamentallemma_der_Variationsrechnung\n",
        "\n",
        "**Fundamentalsatz der Variationsrechnung**\n",
        "\n",
        "* Fundamental Theorem of the Calculus of Variations - [Fundamentalsatz der Variationsrechnung](https://de.wikipedia.org/wiki/Fundamentalsatz_der_Variationsrechnung)\n",
        "\n",
        "* eng verwandt mit dem [weierstraßschen Satz vom Minimum](https://de.wikipedia.org/wiki/Satz_vom_Minimum_und_Maximum)\n",
        "\n",
        "* Er behandelt die in der Variationsrechnung zentrale Frage, unter welchen Bedingungen reellwertige Funktionale ein Minimum annehmen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eloiR80j3LKg"
      },
      "source": [
        "###### *Brachistochrone & Tautochronie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNCf_416SQR"
      },
      "source": [
        "[**Brachistochrone Curve**](https://en.wikipedia.org/wiki/Brachistochrone_curve) (in rot): Der Körper gleitet auf einer solchen Bahn schneller zum Ziel als auf jeder anderen Bahn, beispielsweise auf einer geradlinigen, obwohl diese kürzer ist.\n",
        "\n",
        "\n",
        "![vv](https://upload.wikimedia.org/wikipedia/commons/6/63/Brachistochrone.gif)\n",
        "\n",
        "* Brachistochrone: Path between 2 points $A$ and $B$ which minimizes the time taken by a particle falling from $A$ to $B$ under the influence of gravity.\n",
        "\n",
        "* Time = distance / speed. Goal: Mix of minimize distance and maximize speed\n",
        "\n",
        "* Johann I Bernoulli hat sich mit dem **Problem des schnellsten Falles** beschäftigt. Im Jahre 1696 fand er schließlich die Lösung in der **Brachistochrone**. Heute sieht man dies oft als die **Geburtsstunde der Variationsrechnung**.\n",
        "\n",
        "* Video: [The Brachistochrone Problem and Solution | Calculus of Variations](https://www.youtube.com/watch?v=zYOAUG8PxyM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvO3MMS6JFg"
      },
      "source": [
        "**Tautochronie** der Brachistochrone – von jedem Startpunkt auf der Kurve erreichen die Kugeln das „Ziel“ gleichzeitig.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/b/bd/Tautochrone_curve.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Bewegungsgleichungen (Newton)*"
      ],
      "metadata": {
        "id": "sprbeM9PikBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hintergrund: Bewegungsgleichungen**\n",
        "\n",
        "* Unter einer [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die räumliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung äußerer Einflüsse vollständig beschreibt.\n",
        "\n",
        "* In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung (=Beschleunigung / Acceleration)\n",
        "\n",
        "* Diese Differentialgleichungen sind für viele Systeme nicht analytisch lösbar, sodass man bei der Lösung geeignete Näherungsverfahren anwenden muss.\n",
        "\n",
        "> **Es gibt drei Ansätze für Bewegungsgleichungen: Newtonian Mechanics, Lagrangian and Hamiltonian. Für letztere beiden gilt the Principle of Least Action.**"
      ],
      "metadata": {
        "id": "hXh4vP1EJIiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newtonsche Gesetze**\n",
        "\n",
        "Die [Newtonschen Gesetze](https://de.m.wikipedia.org/wiki/Newtonsche_Gesetze) (Fundamentalkonzept der klassischen Mechanik, Extremalprinzip des Wirkungsfunktionals) gelten als die Grundlage der klassischen Mechanik, auf der alle weiteren Modelle basieren. Zentrales Konzept dieser Formulierung ist die Einführung von Kräften, die eine Beschleunigung $\\ddot{\\vec{x}}$ einer Masse $m$ hervorrufen. Die Bewegungsgleichung dieser Masse wird bestimmt durch die Überlagerung der Kräfte $\\vec{F}_{i}$, die auf die Masse wirken\n",
        "\n",
        "> $\n",
        "m \\ddot{\\vec{x}}=\\sum_{i=1}^{N} \\vec{F}_{i}\n",
        "$\n",
        "\n",
        "1. Ein kräftefreier Körper bleibt in Ruhe oder bewegt sich geradlinig mit konstanter Geschwindigkeit (siehe [Trägheit](https://de.m.wikipedia.org/wiki/Trägheit#Bedeutung_für_wichtige_Prinzipien_der_Mechanik))\n",
        "\n",
        "2. Kraft gleich Masse mal Beschleunigung. $(\\vec{F}=m \\cdot \\vec{a})$ $\\rightarrow$ Equation of Motion (2. Newtonsches Gesetz)\n",
        "\n",
        "3. Kraft gleich Gegenkraft: Eine Kraft von Körper A auf Körper B geht immer mit einer gleich großen, aber entgegen gerichteten Kraft von Körper B auf Körper A einher.\n",
        "\n",
        ">$\n",
        "\\vec{F}_{A \\rightarrow B}=-\\vec{F}_{B \\rightarrow A}\n",
        "$\n",
        "\n",
        "**Bewegungsgleichungen**\n",
        "\n",
        "* eine [Bewegungsgleichung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) ist eine Gleichung, die die Entwicklung eines mechanischen Systems bei äußeren Einflüssen beschreibt\n",
        "\n",
        "* Unter einer Bewegungsgleichung versteht man eine mathematische Gleichung (oder auch ein Gleichungssystem), welche die räumliche und zeitliche Entwicklung eines mechanischen Systems unter Einwirkung äußerer Einflüsse vollständig beschreibt. In der Regel handelt es sich um Systeme von Differentialgleichungen zweiter Ordnung.\n",
        "\n",
        "* Diese Differentialgleichungen sind für viele Systeme nicht analytisch lösbar, sodass man bei der Lösung geeignete Näherungsverfahren anwenden muss.\n",
        "\n",
        "* Lösung: Die Lösung der Bewegungsgleichung ist die [Trajektorie](https://de.m.wikipedia.org/wiki/Trajektorie_(Physik)), auf der sich das System bewegt. Sie ist, abgesehen von einigen einfachen Fällen (siehe Beispiele unten), meist nicht in analytisch geschlossener Form darstellbar und muss über [numerische Methoden](https://de.m.wikipedia.org/wiki/Numerische_Mathematik) gewonnen werden. Dies ist z. B. zur Ermittlung der Trajektorien dreier Himmelskörper, die sich gegenseitig gravitativ anziehen, erforderlich (siehe [Dreikörperproblem](https://de.m.wikipedia.org/wiki/Dreik%C3%B6rperproblem)). Zur Lösung eines N-Teilchensystems lässt sich die [discrete element method](https://de.m.wikipedia.org/wiki/Discrete_element_method) anwenden. In einfachen Fällen wird die geschlossene Lösung als „Bahngleichung“ bezeichnet.\n",
        "\n",
        "**Newtonsche Axiome**\n",
        "\n",
        "Prinzipen: Zum Aufstellen von Bewegungsgleichungen in der klassischen Physik wird verwendet:\n",
        "\n",
        "* das 2. Newtonsche Gesetz,\n",
        "* der Lagrange-Formalismus oder\n",
        "* der Hamilton-Formalismus\n",
        "\n",
        "Darauf basierend ergibt sich die Bewegungsgleichung der Quantenmechanik, die Schrödingergleichung.\n",
        "\n",
        "**In der Technischen Mechanik werden verwendet**:\n",
        "\n",
        "* das Prinzip der virtuellen Arbeit (D’Alembertsches Prinzip)\n",
        "* das Prinzip der virtuellen Leistung (Prinzip von Jourdain)\n",
        "* das Prinzip des kleinsten Zwanges.\n",
        "\n"
      ],
      "metadata": {
        "id": "mi3ntZBhiiWf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYzlWwuA8hXl"
      },
      "source": [
        "###### *Principle of Least / Stationary Action (Lagrange-Formalismus)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange-Formalismus**\n",
        "\n",
        "Der [Lagrange-Formalismus](https://de.m.wikipedia.org/wiki/Lagrange-Formalismus) beschreibt die Gesetze der klassischen Mechanik durch die Lagrange-Funktion $L$, die für Systeme mit einem generalisierten Potential und holonomen [Zwangsbedingungen](https://de.m.wikipedia.org/wiki/Zwangsbedingung) als Differenz aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben ist:\n",
        "\n",
        ">$\n",
        "L=T-V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der Euler-Lagrange-Gleichungen, die die Ableitungen nach der Zeit $t$, den Geschwindigkeiten $\\dot{q}_{i}$ und den [generalisierten Koordinaten](https://de.m.wikipedia.org/wiki/Generalisierte_Koordinate) $q_{i}$ miteinander in Verbindung setzt:\n",
        "\n",
        ">$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\frac{\\partial L}{\\partial \\dot{q}_{i}}=\\frac{\\partial L}{\\partial q_{i}}\n",
        "$\n",
        "\n",
        "> Action $S$ = Kinetic Energy - Potential Energy = $\\int (T - V) dt$ = $\\int (\\frac{1}{2} mv^2 - mgh) dt$\n",
        "\n",
        "$L$ is the Lagrangian of the particle: $L=E_{u}-U$.\n",
        "\n",
        "*Need to solve Lagrange equations to make $S$ stationary:*\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{1}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{1}}\\right)$\n",
        "\n",
        ">$\\frac{\\partial L}{\\partial q_{2}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{2}}\\right)$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial q_{3}}=\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{3}}\\right) \\quad \\begin{array}{c}\\\\ \\end{array}$\n",
        "\n",
        "We can use a general coordinate system:\n",
        "$\\\\ {\\left[q_{1}(t), q_{2}(t), q_{3}(t)\\right]}$\n",
        "\n",
        "*Lagrange-Gleichungen erster Art*\n",
        "\n",
        "* Mit den Lagrange-Gleichungen erster Art lassen sich die Zwangskräfte berechnen.\n",
        "\n",
        "* Wenn man annimmt, dass sich die äußeren Kräfte aus einem Potential ableiten lassen, kann man die Bewegungsgleichung schreiben (Lagrange-Gleichung 1. Art):\n",
        "\n",
        "*Lagrange-Gleichungen zweiter Art*\n",
        "\n",
        "* Die Lagrange-Gleichungen zweiter Art ergeben sich als sogenannte Euler-Lagrange-Gleichungen eines Variationsproblems und liefern die Bewegungsgleichungen, wenn die Lagrange-Funktion gegeben ist.\n",
        "\n",
        "* Sie folgen aus der Variation des mit der Lagrange-Funktion gebildeten Wirkungsintegrals im Hamiltonschen Prinzip.\n",
        "\n",
        "Eingeführte Formulierung der klassischen Mechanik, in der die Dynamik eines Systems durch **eine einzige skalare Funktion, die Lagrange-Funktion**, beschrieben wird. Der Formalismus ist (im Gegensatz zu der newtonschen Mechanik, die a priori nur in Inertialsystemen gilt) auch in beschleunigten Bezugssystemen gültig. Der Lagrange-Formalismus ist invariant gegen Koordinatentransformationen.\n",
        "\n",
        "Der [Langrange Formalismus](https://de.wikipedia.org/wiki/Lagrange-Formalismus) ist eine mögliche (von vielen!) Formulierung der klassischen Mechanik. Hier wird die Dynamik eines Systems durch die Langrange-Funktion L(${\\boldsymbol{q}}$, $\\dot{\\boldsymbol{q}}$, $t$) beschrieben:\n",
        "\n",
        "* ${\\boldsymbol{q}}$ = (q1, q2, ..., qw) - allgemeine Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ = (d / dt) q ... ($\\dot{\\boldsymbol{q}}$1, $\\dot{\\boldsymbol{q}}$2.. $\\dot{\\boldsymbol{q}}$n) - Vektor der Wirkung (allgemeine Geschwindigkeiten)\n",
        "* $t$ - Zeit (Die explizite Zeitabhängige berücksichtigt externe, zeitabhängige Faktoren (z.B. Magnet-Felder etc). Wird auf Null gesetzt, denn bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "> $L_{x}(q, \\dot{q}, t)$ wird zu: $L_{x}(q, \\dot{q})$ = $T$<sub>kin</sub> - $V$<sub>pot</sub>"
      ],
      "metadata": {
        "id": "BR8DkOZUtAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Principle of Least Action or Stationary Action*\n",
        "\n",
        "> Action  𝑆  = Kinetic Energy - Potential Energy\n",
        "\n",
        "* Principle of Least Action = better: [Stationary Action](https://en.m.wikipedia.org/wiki/Stationary-action_principle) = Wirkung\n",
        "\n",
        "https://youtu.be/dPxhTiiq-1A\n",
        "\n",
        "[The Principle of Stationary Action](https://www.youtube.com/watch?v=M05ixbSOY80): If a particle/system $P$ travels from one point to another in the time interval $\\left[t_{1}, t_{2}\\right]$, the path the particle traverses is such that this function:\n",
        "\n",
        "> $\n",
        "S=\\int_{t_{1}}^{t_{2}} \\mathcal{L} \\text { dt}$\n",
        "\n",
        "is stationary.\n",
        "\n",
        "* Total energy = kinetic energy + potential energy.\n",
        "\n",
        "> **Kinetic - potential energy = Lagrangian $L$**\n",
        "\n",
        "> has no physical meaning !! It's a ver useful mathematical tool\n",
        "\n",
        "* Kinetic energy depends on velocity of a particle (detonated with a dot over x,y,z), potential energy depends on position of a particle x,y,z. Hence the Lagrangian of a particle depends on all of the positions and all of their time derivatives.\n",
        "\n",
        "* **Use and solve the three Lagrangian equations in order to determine the equation of motion of a particle (and Lagrange equations are equivalent to Newton's second law)**\n",
        "\n",
        "* And they can easily applied to other coordinate systems than cartesian (i.e cylindric, or spherical)\n",
        "\n",
        "* Also: **Lagrange equations are very similar to Euler-Lagrange Equations!**\n",
        "\n",
        "* Because the three Lagrange equations very strongly resemble the Euler-Lagrange-equation, there must be some functional that's being made stationary by Lagrange equations\n",
        "\n",
        "* **$S$ is the action integral (= a functional!)**\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_01.png)\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/deltorobarba/repo/master/lagrange_02.png)"
      ],
      "metadata": {
        "id": "rfHQAhk-EqQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Euler-Lagrange Equation*"
      ],
      "metadata": {
        "id": "puX57AODxaUG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_F2rBw5bk4"
      },
      "source": [
        "**Euler-Lagrange Equation**\n",
        "\n",
        "*Why using Euler-Lagrange Equation?*\n",
        "\n",
        "1) lagrangian mechanies gives equation of motion without considering forces at all, only energy\n",
        "\n",
        "2) This is more convenient for complicated systems with multiple forces to be considered\n",
        "\n",
        "3) Great for dealing with multiple coordinate\n",
        "\n",
        "*How to get the Euler-Lagrange Equation?*\n",
        "\n",
        "* **Step 1: Lagrangian equation**: L = T (kinetic energy) - V (potential energy)\n",
        "\n",
        "  * T = $\\frac{1}{2}m \\dot x ^2 $\n",
        "\n",
        "    * with $\\dot x$ short for: $\\frac{dx}{dt}$\n",
        "\n",
        "  * V = $\\frac{1}{2}k x ^2 $\n",
        "\n",
        "  * so: L = $\\frac{1}{2}m \\dot x ^2 $ - $\\frac{1}{2}k x ^2 $ -> This is our Lagrangian for a specific system !\n",
        "\n",
        "* **Step 2: Now take Euler–Lagrange Equation**: $\\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot q}\\right)=\\frac{\\partial L}{\\partial q}$\n",
        "\n",
        "  * we see it contains the Lagrangian\n",
        "\n",
        "  * it's consistent with Newtonian classical mechanics F=ma etc\n",
        "\n",
        "  * we can plug in our Lagrangian for a specific system (replace L with formula above). we get the equation of motion!\n",
        "\n",
        "* **Step 3: Equation of Motion**: $m \\ddot x = -kx$\n",
        "\n",
        "  * mass m of an object multipliplied by acceleration $\\ddot x$ (which is the second derivative of x, the whole left side is same as newton's: f=m*a)\n",
        "\n",
        "  * we are stating something about the forces acting on the system\n",
        "\n",
        "*More about Euler–Lagrange equation*\n",
        "\n",
        "* Langrangian = Kinetic Energy - Potential Engergy\n",
        "\n",
        "* then insert it into the **Euler-Langrange Equation**:\n",
        "\n",
        "> $\\frac{d}{d t} \\frac{\\partial L}{\\partial \\dot{\\theta}}=\\frac{\\partial L}{\\partial \\theta}$\n",
        "\n",
        "* the Euler-Langrange Equation is the condition of the action $S$ to be minimized, where action is integral of Lagrangian\n",
        "\n",
        "* Means: of all the possible paths a particle could follow, the actual path it chooses is the one that minimizes (or actually \"extremizes\") the action = **principle of least action**\n",
        "\n",
        "* with Lagrangian we don't need any vectors anymore like in Newtonian, we can sue whatever coordinates. Also it makes it easier to deal with constraints and understands symmetries\n",
        "\n",
        "* F = ma in the Euler Lagrange Equation gives us a single second-order differential equation\n",
        "\n",
        "The Euler–Lagrange equation is an equation satisfied by a function q of a real argument t, which is a stationary point of the functional:\n",
        "\n",
        "> $S(\\boldsymbol{q})=\\int_{a}^{b} L(t, \\boldsymbol{q}(t), \\dot{\\boldsymbol{q}}(t)) \\mathrm{d} t$\n",
        "\n",
        "* ${\\boldsymbol{q}}$ - Koordinaten im Raum\n",
        "* $\\dot{\\boldsymbol{q}}$ - Vektor der Wirkung\n",
        "* t - Zeit (wird auf Null gesetzt, den bei einem Wechsel in die mikroskopische Theorie verschwindet die Zeit)\n",
        "\n",
        "The Euler–Lagrange equation, then, is given by\n",
        "\n",
        "> $L_{x}(t, q(t), \\dot{q}(t))-\\frac{\\mathrm{d}}{\\mathrm{d} t} L_{v}(t, q(t), \\dot{q}(t))=0$\n",
        "\n",
        "* partial derivative of one dimension, then second dimension and then time\n",
        "\n",
        "* the [Euler equation](https://en.m.wikipedia.org/wiki/Euler–Lagrange_equation) is a **second-order partial differential** equation whose **solutions are the functions for which a given functional is stationary**.\n",
        "\n",
        "* Because **a differentiable functional is stationary at its local extrema**, the Euler–Lagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing or maximizing it.\n",
        "\n",
        "* This is analogous to [Fermat's theorem](https://en.m.wikipedia.org/wiki/Fermat%27s_theorem_(stationary_points)) in calculus, stating that at any point where a differentiable function attains a local extremum its derivative is zero.\n",
        "\n",
        "* In Lagrangian mechanics, according to [Hamilton's principle](https://en.m.wikipedia.org/wiki/Hamilton%27s_principle) of stationary action, the evolution of a physical system is described by the solutions to the Euler equation for the action of the system. In this context Euler equations are usually called Lagrange equations. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations.\n",
        "\n",
        "  * Hamilton's principle is William Rowan Hamilton's formulation of the [principle of stationary action](https://en.m.wikipedia.org/wiki/Principle_of_least_action) (also called: 'Principle of least action'). It states that the **dynamics of a physical system are determined by a variational problem for a functional based on a single function**, the Lagrangian, which may contain all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Hamiltonian Function*"
      ],
      "metadata": {
        "id": "SqYBnfUAD1yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamilton'sches Prinzip (Wirkungsfunktional)*\n",
        "\n",
        "> Das [Hamilton'sche Prinzip](https://de.wikipedia.org/wiki/Hamiltonsches_Prinzip) zeichnet tatsachlich durchlaufene Bahnen dadurch aus, **dass bei ihnen die Wirkung (=Funktional) S[q] (verglichen mit anderen Bahnen) ein Minimum annimmt (minimal variation ??)**.\n",
        "\n",
        "* Das Hamiltonsche Prinzip der Theoretischen Mechanik ist ein **Extremalprinzip**. Physikalische Felder und Teilchen nehmen danach für eine bestimmte Größe einen extremalen (d. h. größten oder kleinsten) Wert an. Diese Bewertung nennt man Wirkung (=Action), mathematisch ist die Wirkung ein Funktional, daher auch die Bezeichnung **Wirkungsfunktional**.\n",
        "\n",
        "* Die Wirkung erweist sich in vielen Fällen nicht als minimal, sondern nur als **„stationär“** (d. h. extremal). Deshalb wird das Prinzip von manchen Lehrbuchautoren auch das Prinzip der **stationären Wirkung** genannt. Manche Autoren nennen das Hamiltonsche Prinzip auch **'Prinzip der kleinsten Wirkung'**, was jedoch – wie oben ausgeführt – nicht präzise ist.\n",
        "\n",
        "* Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation:\n",
        "\n",
        "> $\\frac{\\delta \\mathcal{S}}{\\delta \\mathbf{q}(t)}=0$\n",
        "\n",
        "* That is, the system takes a path in configuration space for which the action is stationary, with fixed boundary conditions at the beginning and the end of the path.\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Least_action_principle.svg/500px-Least_action_principle.svg.png)\n",
        "\n",
        "*As the system evolves, q traces a path through configuration space (only some are shown). The path taken by the system (red) has a stationary action (δS = 0) under small changes in the configuration of the system (δq).*"
      ],
      "metadata": {
        "id": "aH2WDPHcw04D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hamiltonsche Mechanik & Hamilton-Funktion*\n",
        "\n",
        "> Die [**Hamilton-Funktion**](https://de.m.wikipedia.org/wiki/Hamilton-Funktion) eines Systems von Teilchen ist (,wenn keine rheonomen (d. h. zeitabhängigen) Zwangsbedingungen vorliegen), **die Gesamtenergie als Funktion der Orte und Impulse der Teilchen** und gegebenenfalls der Zeit.\n",
        "\n",
        "* Total Energy = Kinetic Energy + Potential Energy\n",
        "\n",
        "* rewrite everything in terms of momentum, and we get the **Hamiltonian**:\n",
        "\n",
        "> $H=\\frac{p^{2}}{2 m l^{2}}-m g l \\cos \\theta$\n",
        "\n",
        "* the Hamiltonian gives us a pair of first-order differential equations for theta and pi\n",
        "\n",
        "* the generalized version is Momentum * Velocity - L (Lagrangian):\n",
        "\n",
        "> $H=P \\dot{\\theta}-L$\n",
        "\n",
        "* we get a new geometric perspective by connecting it with something known as \"flow phase space\": P (momentum) and Theta create a new vector space (pairs of them) called the \"phase space\", where I can see what the particle will do in the future. Energy is constant = particle travels along a line of constant energy\n",
        "\n",
        "Die [Hamiltonsche Mechanik](https://de.m.wikipedia.org/wiki/Hamiltonsche_Mechanik) ist die am stärksten verallgemeinerte Formulierung der klassischen Mechanik und Ausgangspunkt der Entwicklung neuerer Theorien und Modelle, wie der Quantenmechanik. Zentrale Gleichung dieser Formulierung ist die Hamilton-Funktion H. Sie ist folgendermaßen definiert:\n",
        "\n",
        "> $\n",
        "H=\\sum_{i} \\dot{q}_{i} p_{i}-L(\\vec{q}, \\dot{\\vec{q}}, t)\n",
        "$\n",
        "\n",
        "Dabei sind $\\dot{q}_{i}$ die generalisierten Geschwindigkeiten und $p_{i}$ die generalisierten Impulse.\n",
        "\n",
        "> Die hamiltonschen Bewegungsgleichungen folgen aus dem hamiltonschen Prinzip der stationären Wirkung.\n",
        "\n",
        "Ist die potentielle Energie unabhängig von der Geschwindigkeit und hängen die TransformationsGleichungen, die die generalisierten Koordinaten definieren, nicht von der Zeit ab, ist die Hamilton-Funktion in der klassischen Mechanik durch die Summe aus kinetischer Energie $T$ und potentieller Energie $V$ gegeben:\n",
        "\n",
        "> $\n",
        "H=T+V\n",
        "$\n",
        "\n",
        "Die Bewegungsgleichungen ergeben sich durch Anwenden der kanonischen Gleichungen:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "\\dot{q}_{i} &=\\frac{\\partial H}{\\partial p_{i}} \\\\\n",
        "\\dot{p}_{i} &=-\\frac{\\partial H}{\\partial q_{i}}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Mit dem Hamilton-Jacobi-Formalismus existiert eine modifizierte Form dieser Beschreibung, die die Hamilton-Funktion mit der Wirkung verknüpft."
      ],
      "metadata": {
        "id": "ntdvyhHFuhn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operators in Classical Mechanics**\n",
        "\n",
        "[Operators in classical mechanics](https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics): In classical mechanics, the movement of a particle (or system of particles) is completely determined by the **Lagrangian** $L(q, \\dot{q}, t)$ or equivalently the **Hamiltonian** $H(q, p, t)$, a function of the generalized coordinates $q$, generalized velocities $\\dot{q}=\\mathrm{d} q / \\mathrm{d} t$ and its conjugate momenta:\n",
        "\n",
        ">$\n",
        "p=\\frac{\\partial L}{\\partial \\dot{q}}\n",
        "$\n",
        "\n",
        "If either $L$ or $H$ is independent of a generalized coordinate $q$, meaning the $L$ and $H$ do not change when $q$ is changed, which in turn means the dynamics of the particle are still the same even when q changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate $q$ is a symmetry). **Operators in classical mechanics are related to these symmetries.**\n",
        "\n",
        "More technically, when $H$ is invariant under the action of a certain group of transformations $G$ :\n",
        "\n",
        ">$\n",
        "S \\in G, H(S(q, p))=H(q, p)\n",
        "$\n",
        "\n",
        "the elements of $G$ are physical operators, which map physical states among themselves.\n",
        "\n",
        "**Connections to Quantum Mechanics**\n",
        "\n",
        "* Functions on phase phase in classical mechanics turn into operators in the quantum space of states in quantum mechanics\n",
        "\n",
        "* And if you know the state of a quantum system at time t0, the Schrödinger equation says they at a later time t will be $|\\psi \\rangle$ $\\rightarrow$ $e^{-\\frac{i}{\\hbar} H t}|\\psi\\rangle$ acting on the state,\n",
        "\n",
        "* where $H$ is the operator version of the classical Hamiltonian function\n",
        "\n",
        "**Feynman Path Integral Formulation**\n",
        "\n",
        "> *Principle of Least Action in Quantum Mechanics (Path Integral Formulation)*\n",
        "\n",
        "The [path integral formulation](https://en.m.wikipedia.org/wiki/Path_integral_formulation) is a description in quantum mechanics that **generalizes the action principle of classical mechanics**. It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or functional integral, over an infinity of quantum-mechanically possible trajectories to compute a quantum amplitude.\n",
        "\n",
        "* Richard Feynman zeigte in den 1940ern, dass sich das Hamiltonsche Prinzip in der Quantenfeldtheorie gerade dadurch ergibt, dass alle möglichen Pfade (auch die nicht zielgerichteten) zulässig sind und aufintegriert werden. Dabei überlagern sich Pfade mit extremaler Wirkung konstruktiv und davon abweichende destruktiv, so dass die Natur schließlich zielgerichtet erscheint.\n",
        "\n",
        "* Principle of least action is equivalent to newtonian mechanics (one can derive the on from the other). And both is for large scale objects, **meanwhile the principle of least action is the large scale approximation of the feynman path integral on quantum objects!** source at Min 7:53 here: https://www.youtube.com/watch?v=dPxhTiiq-1A\n",
        "\n",
        "**Bewegungsgleichung der Allgemeinen Relativitätstheorie**\n",
        "\n",
        "Die [Bewegung](https://de.m.wikipedia.org/wiki/Bewegungsgleichung) eines Körpers wird durch die Geodätengleichung der gekrümmten Raumzeit beschrieben, sofern nur gravitative Kräfte auf ihn einwirken. Dann bewegt sich der Körper entlang einer Geodäten der Raumzeit. Die Geodätengleichung lautet\n",
        "\n",
        "> $\n",
        "\\ddot{x}^{\\mu}+\\Gamma_{\\lambda \\nu}^{\\mu} \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=\\ddot{x}^{\\mu}+\\frac{g^{\\mu \\rho}}{2}\\left(\\partial_{\\lambda} g_{\\nu \\rho}+\\partial_{\\nu} g_{\\lambda \\rho}-\\partial_{\\rho} g_{\\lambda \\nu}\\right) \\dot{x}^{\\lambda} \\dot{x}^{\\nu}=0\n",
        "$\n",
        "\n",
        "wobei $\\Gamma_{\\lambda \\nu}^{\\mu}$ ein Christoffelsymbol 2. Art ist, welches die Abhängigkeit des metrischen Tensors vom Raumzeitpunkt (Ereignis), d. h. der Krümmung der Raumzeit, charakterisiert."
      ],
      "metadata": {
        "id": "anQlQ5k0TzaJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zER_lMlN46"
      },
      "source": [
        "###### *Special: Minimal Surfaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**minimal surface**\n",
        "\n",
        "A [minimal surface](https://en.m.wikipedia.org/wiki/Minimal_surface) is a surface that locally minimizes its area.\n",
        "\n",
        "* Intro Video: https://youtu.be/_t-3lCZXlPM\n",
        "\n",
        "* Siehe auch: https://www.chemie-schule.de/KnowHow/Oberflächenspannung\n",
        "\n",
        "* This is equivalent to having **zero mean curvature = second derivative is always zero at any point**\n",
        "\n",
        "* **Minimal surfaces represent the lowest energy state!** A flat plane is minimalist surface area\n",
        "\n",
        "* Erwin Schrödinger used Minimal Surfaces equations in 1926 to describe the quantum state of real phsysical systems\n",
        "\n",
        "* the apparent horizon of a back whole can be are always minimal surfaces\n",
        "\n",
        "* non trivial minimal surface: [Katenoid](https://en.m.wikipedia.org/wiki/Catenoid) and a [Helicoid (Wendelfläche)](https://de.m.wikipedia.org/wiki/Wendelfläche)\n",
        "\n",
        "* For a given constraint there may also exist several minimal surfaces with different areas (for example, see [minimal surface of revolution](https://en.m.wikipedia.org/wiki/Minimal_surface_of_revolution)): the standard definitions only relate to a local optimum, not a global optimum.\n",
        "\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Catenoid.svg/600px-Catenoid.svg.png)\n",
        "\n",
        "\n",
        "Minimal surface theory originates with Lagrange who in 1762 considered the variational problem of finding the surface z = z(x, y) of least area stretched across a given closed contour. He derived the Euler–Lagrange equation for the solution\n",
        "\n",
        "> $\\frac{d}{d x}\\left(\\frac{z_{x}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)+\\frac{d}{d y}\\left(\\frac{z_{y}}{\\sqrt{1+z_{x}^{2}+z_{y}^{2}}}\\right)=0$\n",
        "\n",
        "He did not succeed in finding any solution beyond the plane. In 1776 Jean Baptiste Marie Meusnier discovered that the helicoid and catenoid satisfy the equation and that the differential expression corresponds to twice the mean curvature of the surface, concluding that surfaces with zero mean curvature are area-minimizing.\n",
        "\n",
        "By expanding Lagrange's equation to\n",
        "\n",
        "> $\\left(1+z_{x}^{2}\\right) z_{y y}-2 z_{x} z_{y} z_{x y}+\\left(1+z_{y}^{2}\\right) z_{x x}=0$\n",
        "\n",
        "Gaspard Monge and Legendre in 1795 derived representation formulas for the solution surfaces. While these were successfully used by Heinrich Scherk in 1830 to derive his surfaces, they were generally regarded as practically unusable. Catalan proved in 1842/43 that the helicoid is the only ruled minimal surface.\n",
        "\n",
        "Eine Minimalfläche ist eine Fläche im Raum, die lokal minimalen Flächeninhalt hat. Derartige Formen nehmen beispielsweise Seifenhäute an, wenn sie über einen entsprechenden Rahmen (wie etwa einen Blasring) gespannt sind. In mathematischer Sprache sind Minimalflächen die kritischen Punkte des Flächeninhaltsfunktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\int \\sqrt{g(u)} \\mathrm{d}^{n} u$\n",
        "\n",
        "\n",
        "Hierbei sind die Größen $g(u):=\\operatorname{det}\\left(g_{i j}(u)\\right)_{i, j=1, \\ldots, n}$ und $g_{i j}(u)=\\left(\\frac{\\partial \\mathbf{x}}{\\partial u_{i}}\\right)^{T} \\frac{\\partial \\mathbf{x}}{\\partial u_{j}}$ für $i, j=1, \\ldots, n$  erklärt (vgl. Hesse-Matrix).\n",
        "\n",
        "**Man beachte, dass eine Minimalfläche nicht notwendig minimalen Flächeninhalt hat, sondern lediglich ein stationärer Punkt des Flächeninhaltsfunktionals ist.** Man kann zeigen, dass das Verschwinden der ersten Variation des Flächeninhaltsfunktionals in zwei Raumdimensionen äquivalent zum Verschwinden der mittleren Krümmung H ist, falls die betrachtete Mannigfaltigkeit hinreichend regulär ist.\n",
        "\n",
        "\n",
        "**Formulierung als Variationsproblem**\n",
        "\n",
        "Eine Fläche ist genau dann eine [Minimalfläche](https://de.m.wikipedia.org/wiki/Minimalfläche), wenn sie an jedem Punkt die mittlere Krümmung null hat. Damit stellt sich eine Minimalfläche als Spezialfall einer Fläche vorgeschriebener mittlerer Krümmung dar. Diese entziehen sich ebenfalls nicht der Variationsrechnung, sie sind Minima des Hildebrandtschen Funktionals\n",
        "\n",
        "> $A(\\mathbf{x})=\\iint\\left(\\left|\\mathbf{x}_{u} \\times \\mathbf{x}_{v}\\right|+2\\left(Q(\\mathbf{x}), \\mathbf{x}_{u}, \\mathbf{x}_{v}\\right)\\right) \\mathrm{d} u \\mathrm{~d} v$\n",
        "\n",
        "Die Eulerschen Gleichungen als notwendige Minimalitätsbedingungen dieses Funktionals sind das nach Franz Rellich benannte H-Flächen-System\n",
        "\n",
        "Siehe auch: [Schwarz minimal surface](https://en.m.wikipedia.org/wiki/Schwarz_minimal_surface)\n",
        "\n",
        "$\n",
        "\\Delta \\mathbf{x}=2 H \\mathbf{x}_{u} \\times \\mathbf{x}_{v}, \\quad \\mathbf{x}_{u}^{2}-\\mathbf{x}_{v}^{2}=0=\\mathbf{x}_{u} \\mathbf{x}_{v}\n",
        "$\n",
        "\n",
        "Hierbei ist $H=\\operatorname{div} Q$ die mittlere Krümmung."
      ],
      "metadata": {
        "id": "D5Q9F5n4hpl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Vektor Calculus*"
      ],
      "metadata": {
        "id": "DfJ1ATjNLmjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Summary*"
      ],
      "metadata": {
        "id": "xcWGR3zwT-Fo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib8wN8Qsy42W"
      },
      "source": [
        "* Die [Vektoranalysis](https://de.m.wikipedia.org/wiki/Vektoranalysis) ist ein **Teilgebiet der Tensoranalysis**, beschäftigt sich hauptsächlich mit **Vektorfeldern in zwei oder mehr Dimensionen**\n",
        "\n",
        "* Die Vektoranalysis **verallgemeinert die Differential- und der Integralrechnung** (z.B. werden Verzerrungen auf Oberflachen bei der Integration berucksichtigt, oder Stroemungen bei Wegen)\n",
        "\n",
        "* Betrachtet werden **Vektorfelder**, die jedem Punkt des Raumes einen Vektor zuordnen, und **Skalarfelder**, die jedem Punkt des Raumes einen Skalar zuordnen.\n",
        "\n",
        "*  Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat.\n",
        "\n",
        "* Zusammenfassung: https://www.maths2mind.com/geometrie/vektorrechnung-ebene-im-raum/vektoranalysis\n",
        "\n",
        "*Grundbegriffe*\n",
        "\n",
        "* **Kurven** in $\\mathbb{R}^{2}$ oder $\\mathbb{R}^{3}$: Wege bzw. **parametrisierte** Kurven in $\\mathbb{R}^{n}$ sind **stetige** Abbildungen $\\gamma$ [a, b] -> $\\mathbb{R}^{n}$\n",
        "\n",
        "* **Reguläre Wege** bzw. Kurven: **stetig + differenzierbar** und Norm der Ableitung ist die Summe der Komponenten: $\\|\\dot{\\gamma}(t)\\|^{2}=\\left|\\dot{\\gamma}_{1}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{2}(t)\\right|^{2}+\\left|\\dot{\\gamma}_{3}(t)\\right|^{2} \\neq 0$ fur alle t $\\in$ [a,b]. Bedeutet auch: es gibt uberall einen Tangentialvektor.\n",
        "\n",
        "* **Tangentialvektor**: Ableitung / Geschwindigkeitsvektor an einem Punkt, der in eine Richtung zeigt, der tangential zur Kurve zeigt. Den Vektor normiert man (dividiert durch Norm): $T_{\\gamma}(t):=\\frac{\\dot{\\gamma}(t)}{\\|\\dot{\\gamma}(t)\\|}$\n",
        "\n",
        "* **Normalenvektor**: nur definiert in einer Ebene, also in R2. Sollte senkrecht auf der Kurve / senkrecht auf dem Tangentialvektor stehen (man muss also diesen Punkt $\\dot{\\gamma}(t)=\\left(\\begin{array}{l}\\dot{\\gamma}_{1}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$ um 90 Grad drehen, damit er senkrech steht): $N_{\\gamma}(t):=\\frac{1}{\\|\\dot{\\gamma}(t)\\|}\\left(\\begin{array}{c}-\\dot{\\gamma}_{2}(t) \\\\ \\dot{\\gamma}_{2}(t)\\end{array}\\right)$.\n",
        "\n",
        "* **Die Norm im Normalenvektor (Jacobi-Determinante) gibt zB die Kruemmung / Verzerrung einer Flaeche an, die bei der Integration beruecksichtigt werden muss.**\n",
        "\n",
        "* Eine [**vektorielle Größe**](https://de.wikipedia.org/wiki/Vektorielle_Größe) oder gerichtete Größe ist eine physikalische Größe, die – im Gegensatz zu den skalaren Größen – einen Richtungscharakter hat.\n",
        "  * Typische vektorielle Größen sind die kinematischen Größen Geschwindigkeit und Beschleunigung, die dynamischen Größen Impuls und Kraft bzw. Drehimpuls und Drehmoment sowie die Feldstärken der elektrischen und magnetischen Felder der Elektrodynamik.\n",
        "  * Vektorielle Größen werden sowohl zeichnerisch als auch rechnerisch wie geometrische Vektoren behandelt, wobei einige Besonderheiten zu beachten sind."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background: Vector Analysis**\n",
        "\n",
        "* Um einen Vektor mittels Koordinaten darstellen zu können, ist eine Basis nötig. Im n-dimensionalen Raum besteht diese aus n linear unabhängigen Vektoren, den Basisvektoren.\n",
        "\n",
        "* **Basis Vectors and Vector Components**: Jeder beliebige Vektor kann als Linearkombination der Basisvektoren dargestellt werden, wobei die Koeffizienten der Linearkombination die <u>Komponenten des Vektors</u> genannt werden.\n",
        "\n",
        "* [Orthogonal Coordinates](https://en.m.wikipedia.org/wiki/Orthogonal_coordinates) und [Cartesian tensor](https://en.m.wikipedia.org/wiki/Cartesian_tensor)\n",
        "\n",
        "* **Geradlinige Koordinaten mit Globaler Basis**: **Globale Basen** zeichnen sich dadurch aus, dass die Basisvektoren in jedem Punkt identisch sind, was nur für lineare bzw. affine Koordinaten (die Koordinatenlinien sind geradlinig, aber im Allgemeinen schiefwinklig) möglich ist. Folge: **Bei geradlinigen Koordinatensystemen steckt die Ortsabhängigkeit eines Vektorfeldes allein in den Koordinaten (und nicht in den Basen)**.\n",
        "\n",
        "* **Curvilinear Coordinate mit local basis**: [Curvilinear Coordinates](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten): Für echt krummlinige (also nicht-geradlinige) Koordinaten variieren Basisvektoren und Komponenten von Punkt zu Punkt, weshalb die Basis als lokale Basis bezeichnet wird. Die Ortsabhängigkeit eines Vektorfeldes verteilt sich auf die Koordinaten sowie auf die Basisvektoren. [Verschiedene Basen bei krummlinigen Koordinaten](https://de.m.wikipedia.org/wiki/Krummlinige_Koordinaten#Verschiedene_Basen). **Die Koordinatenachsen sind als Tangenten an die Koordinatenlinien definiert**. Da die Koordinatenlinien im Allgemeinen gekrümmt sind, sind die Koordinatenachsen nicht räumlich fest, wie es für kartesische Koordinaten gilt. Dies führt auf das Konzept der **lokalen Basisvektoren**, deren Richtung vom betrachteten Raumpunkt abhängt – im Gegensatz zu globalen Basisvektoren der kartesischen oder affinen Koordinaten. Siehe auch [Tensors in curvilinear coordinates](https://en.m.wikipedia.org/wiki/Tensors_in_curvilinear_coordinates)\n",
        "\n",
        "*Koordinatenflächen, Koordinatenlinien und Koordinatenachsen (entlang der Basisvektoren eines ausgewählten Ortes):*\n",
        "\n",
        "![fff](https://upload.wikimedia.org/wikipedia/commons/5/57/General_curvilinear_coordinates_1.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "AobQvOfI9Az_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Unifying Principle: Integrating a differential operator acting on a field over a domain is the same as adding the field components along the boundary (local to global transition!)**\n",
        "\n",
        "Source: [A unified view of Vector Calculus (Stoke's Theorem, Divergence Theorem & Green's Theorem)](https://m.youtube.com/watch?v=PIoqMNL7tV0&feature=youtu.be)\n",
        "\n",
        "Fundamental Theorem of Calculus: If $f(x)$ differentiable on $[a, b]$\n",
        "\n",
        ">$\n",
        "\\int_a^b f^{\\prime}(x) d x=f(b)-f(a)\n",
        "$"
      ],
      "metadata": {
        "id": "7wCvJGyaUTuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_097.jpg)"
      ],
      "metadata": {
        "id": "Ddm461kwVgkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_095.jpg)"
      ],
      "metadata": {
        "id": "691EMDoJUEQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_096.jpg)"
      ],
      "metadata": {
        "id": "_l0OHdTfVfGz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujcsbmyfEJDt"
      },
      "source": [
        "**Koordinatentransformation**\n",
        "\n",
        "https://de.wikipedia.org/wiki/Differentialgeometrie#Koordinatentransformationen\n",
        "\n",
        "* Fur die Berechnung des Flächen- oder Volumenintegrals eine geeignete Substitutionsfunktion zu finden ist nicht trivial. Sie transformiert das Volumenintegral oft von einem Koordinatensystem in ein anderes, um die Berechnung zu vereinfachen oder überhaupt zu ermöglichen.\n",
        "\n",
        "* Bei der Integration über geometrische Objekte ist es sogar oft unpraktisch, über kartesische Koordinaten zu integrieren. So lässt sich in der Physik das Integral über ein radialsymmetrisches Potentialfeld, dessen Wert nur von einem Radius r abhängt, wesentlich leichter in Kugelkoordinaten berechnen. Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$  an.\n",
        "\n",
        "* Um dies zu tun, wendet man eine Koordinatentransformation $\\Phi$ an. Nach dem [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) gilt dann in diesem Beispiel:\n",
        "\n",
        "> $\n",
        "\\int_{\\Omega} U(\\vec{r}) d V=\\int_{\\Phi^{-1}(\\Omega)} U(\\Phi(r, \\theta, \\varphi)) \\cdot|\\operatorname{det} D \\Phi(r, \\theta, \\varphi)| \\mathrm{d} r \\mathrm{~d} \\theta \\mathrm{d} \\varphi\n",
        "$\n",
        "\n",
        "* Der vektorielle Faktor ist das [Spatprodukt](https://de.wikipedia.org/wiki/Spatprodukt) aller partiellen Ableitungen von $\\vec{\\xi}(u, v, w)$\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}\n",
        "$\n",
        "\n",
        "Generell lassen sich Spatprodukte auch als Determinanten schreiben, so gilt hier:\n",
        "\n",
        "> $\n",
        "\\vec{N}=\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\times \\frac{\\partial \\vec{\\xi}}{\\partial v}\\right) \\cdot \\frac{\\partial \\vec{\\xi}}{\\partial w}=\\operatorname{det}\\left(\\frac{\\partial \\vec{\\xi}}{\\partial u} \\frac{\\partial \\vec{\\xi}}{\\partial v} \\frac{\\partial \\vec{\\xi}}{\\partial w}\\right)=\\operatorname{det}\\left(J_{\\vec{\\xi}}\\right)\n",
        "$\n",
        "\n",
        "Die aneinandergereihten partiellen Gradienten $(\\vec{\\xi}$ ist eine vektorwertige Funktion)\n",
        "formen gerade die Elemente der $3 \\times 3$ Jacobi-Matrix. Die zugehörige Jacobi-Determinante, auch als [**Funktionaldeterminante**](https://de.wikipedia.org/wiki/Funktionaldeterminante) bezeichnet, berechnet genau den\n",
        "zusätzlichen Faktor für eine Koordinatentransformation.\n",
        "\n",
        "Ist das Volumenelement skalar, reduziert sich der Faktor auf dessen euklidische Norm $\\|\\vec{N}\\| .$ Nachdem das Volumenintegral parametrisiert ist, kann mit Hilfe des [Satzes von Fubini](https://de.wikipedia.org/wiki/Satz_von_Fubini) das Integral Schritt für Schritt berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrJh93jH1D-"
      },
      "source": [
        "**Transformationssatz**\n",
        "\n",
        "* Der [Transformationssatz](https://de.wikipedia.org/wiki/Transformationssatz) (auch Transformationsformel) beschreibt in der Analysis das Verhalten von Integralen unter Koordinatentransformationen. Er ist somit die Verallgemeinerung der Integration durch Substitution auf Funktionen höherer Dimensionen.\n",
        "\n",
        "* Der Transformationssatz wird als Hilfsmittel bei der Berechnung von Integralen verwendet, wenn sich das Integral nach Überführung in ein anderes Koordinatensystem leichter berechnen lässt.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{R}^{d}$ eine offene Menge und $\\Phi: \\Omega \\rightarrow \\Phi(\\Omega) \\subseteq \\mathbb{R}^{d}$ ein [Diffeomorphismus](https://de.wikipedia.org/wiki/Diffeomorphismus) (=eine bijektive, stetig differenzierbare Abbildung, deren Umkehrabbildung auch stetig differenzierbar ist). Dann ist die Funktion $f$ auf $\\Phi(\\Omega)$ genau dann integrierbar, wenn die Funktion $x \\mapsto f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))|$ auf $\\Omega$ integrierbar ist. In diesem Fall gilt:\n",
        "\n",
        "> $\n",
        "\\int_{\\Phi(\\Omega)} f(y) \\mathrm{d} y=\\int_{\\Omega} f(\\Phi(x)) \\cdot|\\operatorname{det}(D \\Phi(x))| \\mathrm{d} x\n",
        "$\n",
        "\n",
        "* Dabei ist $D \\Phi(x)$ die Jacobi-Matrix und $\\operatorname{det}(D \\Phi(x))$ die Funktionaldeterminante von $\\Phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUDAAXziE4Pj"
      },
      "source": [
        "**Funktionaldeterminante**\n",
        "\n",
        "* Die [Funktionaldeterminante oder Jacobi-Determinante](https://de.wikipedia.org/wiki/Funktionaldeterminante) ist eine mathematische Größe, die in der mehrdimensionalen Integralrechnung, also der Berechnung von **Oberflächen- und Volumenintegralen**, eine Rolle spielt. Insbesondere findet sie in der [Flächenformel](https://de.wikipedia.org/wiki/Flächenformel) und dem aus dieser hervorgehenden Transformationssatz Verwendung.\n",
        "\n",
        "* [Exkurs Determinante](https://de.wikipedia.org/wiki/Determinante): *die Determinante eine Zahl (ein Skalar), die einer quadratischen Matrix zugeordnet wird und aus ihren Einträgen berechnet werden kann. Sie gibt an, wie sich das **Volumen (oder der Flächeninhalt)** bei der durch die Matrix beschriebenen linearen Abbildung ändert*\n",
        "\n",
        "* **Lokales Verhalten einer Funktion**: Die Funktionaldeterminante gibt zu einem gegebenen Punkt wichtige Informationen über das Verhalten der Funktion $f$ in der Nähe dieses Punktes.\n",
        "\n",
        "  * Wenn beispielsweise die Funktionaldeterminante einer stetig differenzierbaren Funktion in einem Punkt $p$ ungleich null ist, so ist die Funktion in einer Umgebung von $p$ invertierbar.\n",
        "\n",
        "  * Weiterhin gilt, dass bei positiver Determinante in $p$ die Funktion ihre Orientierung beibehält und bei negativer Funktionaldeterminante die Orientierung umkehrt.\n",
        "\n",
        "  * Der absolute Wert der Determinante im Punkt $p$ gibt den Wert an, mit dem die Funktion in der Nähe von $p$ expandiert oder schrumpft.\n",
        "\n",
        "* Für eine differenzierbare Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ ist die Funktionaldeterminante definiert als die Determinante der Jacobi-Matrix von $f,$ also als\n",
        "det $D f(x)$\n",
        "mit\n",
        "\n",
        "> $\n",
        "D f(x)=\\left(\\frac{\\partial f_{i}}{\\partial x_{j}}(x)\\right)_{i, j=1, \\ldots, n}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Aix1sXHVnV"
      },
      "source": [
        "**Beispiel: Polarkoordinaten**\n",
        "\n",
        "**1. Koordinatentransformation**: Die Umrechnungsformeln von Polarkoordinaten in kartesische Koordinaten lauten:\n",
        "\n",
        "> $\n",
        "\\begin{array}{l}\n",
        "x=r \\cos \\varphi \\\\\n",
        "y=r \\sin \\varphi\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**2. Funktionaldeterminante** lautet also:\n",
        "\n",
        "> $\n",
        "\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}=\\operatorname{det}\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial \\varphi} \\\\\n",
        "\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial \\varphi}\n",
        "\\end{array}\\right)=\\operatorname{det}\\left(\\begin{array}{cc}\n",
        "\\cos \\varphi & -r \\sin \\varphi \\\\\n",
        "\\sin \\varphi & r \\cos \\varphi\n",
        "\\end{array}\\right)=r \\cdot(\\cos \\varphi)^{2}+r \\cdot(\\sin \\varphi)^{2}=r\n",
        "$\n",
        "\n",
        "**3. Flächen- oder Volumenintegral**: Folglich ergibt sich für das Flächenelement $\\mathrm{d} A$ (alternativ kann man bei dreidimensionalen Kugelkoordinaten an dieser Stelle auch das Volumenelement $\\mathrm {d} V$ mit der Funktionaldeterminante berechnen):\n",
        "\n",
        "> $\n",
        "\\mathrm{d} A=\\left|\\operatorname{det} \\frac{\\partial(x, y)}{\\partial(r, \\varphi)}\\right| \\mathrm{d} r \\mathrm{~d} \\varphi=r \\mathrm{~d} r \\mathrm{~d} \\varphi\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Felder*"
      ],
      "metadata": {
        "id": "aGvecdZZd-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbKijNH7RjtR"
      },
      "source": [
        "*Skalarfeld*\n",
        "\n",
        "* In der mehrdimensionalen Analysis, der Vektorrechnung und der Differentialgeometrie ist ein [skalares Feld (kurz Skalarfeld)](https://de.wikipedia.org/wiki/Skalarfeld) $\\varphi$ **eine Funktion, die jedem Punkt eines Raumes eine reelle Zahl (Skalar) zuordnet**, z. B. eine Temperatur, Luftdruck.\n",
        "\n",
        "* Wichtige Operationen im Zusammenhang mit Skalarfeldern sind:\n",
        "\n",
        "  * **[Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)) eines Skalarfeldes, der ein Vektorfeld ist**.\n",
        "\n",
        "  * [Richtungsableitung](https://de.wikipedia.org/wiki/Richtungsableitung) eines Skalarfeldes: die Richtungsableitung einer von mehreren Variablen abhängigen Funktion ist die **momentane Änderungsrate dieser Funktion in einer durch einen Vektor vorgegebenen Richtung**. (Eine Verallgemeinerung der Richtungsableitung auf unendlichdimensionale Räume ist das [Gâteaux-Differential](https://de.m.wikipedia.org/wiki/G%C3%A2teaux-Differential).)\n",
        "  * Ein Skalarfeld ist das einfachste [Tensorfeld](https://de.m.wikipedia.org/wiki/Tensorfeld).\n",
        "\n",
        "* Skalarfelder sind von großer Bedeutung in der Feldbeschreibung der Physik und in der mehrdimensionalen Vektoranalysis.\n",
        "\n",
        "* Man unterscheidet dabei zwischen **reellwertigen** Skalarfeldern $\\varphi\\colon M\\to \\mathbb {R}$ und **komplexwertigen** Skalarfeldern $\\displaystyle \\varphi \\colon M\\to \\mathbb {C} $.\n",
        "\n",
        "* Man spricht von einem **stationären Skalarfeld**, wenn die Funktionswerte nur vom Ort abhängen. Hängen sie auch von der Zeit ab, handelt es sich um ein **instationäres Skalarfeld**.\n",
        "\n",
        "* Beispiele für Skalarfelder in der Physik sind der Luftdruck, die Temperatur, Dichte oder allgemein **Potentiale (= Skalarpotentiale)**.\n",
        "\n",
        "* Im Gegensatz zum Skalarfeld ordnet ein Vektorfeld jedem Punkt einen Vektor zu. Ein Skalarfeld ist das einfachste Tensorfeld\n",
        "\n",
        "* Ein **Niveaufeld** ist ein Feld, wo überall die gleiche skalare Grösse vorliegt (zB die gleiche Temperatur ohne Änderungen (Steigungen, Senken)) = english: Level Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vektorfeld*\n",
        "\n",
        "If you give a **magnitude and direction** at any given point, then you get a [vector field](https://en.m.wikipedia.org/wiki/Vector_field)\n",
        "\n",
        "Vector Fields in 2D: https://www.geogebra.org/m/cXgNb58T\n",
        "\n",
        "Vector field 3D: https://www.geogebra.org/m/u3xregNW\n",
        "\n",
        "**Der Gradient eines Skalarfeldes ist ein Vektorfeld (zB: Skalar ist die Temperatur in einem Pool, und Temperaturveranderung uber Zeit ist die erste Ableitung dessen und dann ein Vektorfeld), und die zweite Ableitung ist die Beschleunigung der Temperaturveranderung.**\n",
        "\n",
        "* Beispiele von Vektorfeldern:\n",
        "  * **Gradientenfeld** (das von einer Punktquelle nach allen Seiten gleichmäßig fließende Feld einer Strömung und das elektrische Feld um eine Punktladung - Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde**, bzw. – kürzer formuliert – der Gradient des Skalarfelds (= **ein Skalarfeld, das nach dem Ort abgeleitet wird, ist ein Gradientenfeld**, mit Skalarpotenzial inkl. angegeben).\n",
        "  * **Zentralfelder**: ein Intervall, welches die Null enthält(Gravitationsfeld),\n",
        "  * **\"Wirbelfelder\"** (das in Kreislinien um den Ausfluss einer , Badewanne\" herumwirbelnde Strömungsfeld, oder das Magnetfeld um einen stromdurchflossenen Draht.)\n",
        "\n",
        "* In der mehrdimensionalen Analysis und der Differentialgeometrie ist ein [Vektorfeld](https://de.m.wikipedia.org/wiki/Vektorfeld) eine Funktion, die jedem Punkt eines Raumes einen Vektor zuordnet.\n",
        "\n",
        "* Eine Abbildung $V: D \\rightarrow \\mathbb{R}^{n}, D \\subseteq \\mathbb{R}^{n}$ mit n = 1,2,3..\n",
        "\n",
        "* Die Temperatur eines Swimmingpools ist ein Skalarfeld: Jedem Punkt wird der Skalarwert seiner Temperatur zugeordnet. Die Wasserbewegung entspricht dagegen einem Vektorfeld, da jedem Punkt ein Geschwindigkeitsvektor zugeordnet wird, der Betrag und Richtung hat.\n",
        "\n",
        "* Meist sind Vektorfelder stetig differenzierbar = Komponenten sind stetig differenzierbar, zB Vektor $v1$ mit (x,y,z) und ihren drei Ableitungen als Komponente des Vektorfeldes $V$\n",
        "\n",
        "* Das duale Konzept zu einem Vektorfeld ist eine Funktion, die jedem Punkt eine [Linearform](https://de.m.wikipedia.org/wiki/Linearform), [zB [stetige lineare Funktionale](https://de.m.wikipedia.org/wiki/Funktional#Stetige_lineare_Funktionale)] zuordnet, eine solche Abbildung wird [pfaffsche Form](https://de.m.wikipedia.org/wiki/Pfaffsche_Form) (One-Form) genannt. (Pfaffsche Formen sind die natürlichen Integranden für Wegintegrale.)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_029.jpg)\n",
        "\n",
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des „konservativen Kraftfelds“ oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* siehe mehr unten unter \"Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus\"\n",
        "\n",
        "**Vektorfelder, die Gradienten eines Skalarfelds sind**, werden in Anlehnung an den Begriff des „konservativen Kraftfelds“ oft auch als **konservative Vektorfelder** bezeichnet - ihnen allen gemeinsam sind dabei die folgenden drei einander äquivalenten Eigenschaften:\n",
        "\n",
        "1. **Wegunabhängigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve $S$ innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abhängig, nicht dagegen von ihrer Länge.\n",
        "\n",
        "2. **Verschwinden des Ringintegrals für beliebige Randkurven** $S$ :\n",
        "$\n",
        "\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0\n",
        "$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit** des Feldes:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$\n"
      ],
      "metadata": {
        "id": "VpsrL-yaTDrr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9TPhq5fEux"
      },
      "source": [
        "*Fundamentalzerlegung (Fundamentalsatz der Vektoranalysis)*\n",
        "\n",
        "* der [Helmholtzscher Zerlegungssatz](https://de.m.wikipedia.org/wiki/Helmholtz-Theorem) ist der Fundamentalsatz der Vektoranalysis. Beschreibt den allgemeinen Fall.\n",
        "\n",
        "* Jedes Vektorfeld $\\vec{F}$ lässt sich als eine Überlagerung eines Quellenanteils $\\vec{F}_{Q}$ und eines Wirbelanteils $\\vec{F}_{W}$ beschreiben."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Potentiale*"
      ],
      "metadata": {
        "id": "rc8jFaF5d0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Skalarpotential*\n",
        "\n",
        "> [Skalarpotenzial](https://de.wikipedia.org/wiki/Skalarpotential) ist ein Maß für die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein Körper entgegen der wirkenden Kraft bewegt, dann erhöht sich seine potenzielle Energie.\n",
        "\n",
        "Das [Skalarpotential](https://de.wikipedia.org/wiki/Skalarpotential), oft einfach auch nur Potential genannt, ist in der Mathematik ein - im Unterschied zum Vektorpotential - skalares Feld $\\Phi(\\vec{r})$, dessen Gradient gemäß folgender Formel\n",
        "\n",
        "> $\n",
        "\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "ein als \"Gradientenfeld\" genanntes Vektorfeld $\\vec{F}(\\vec{r})$ liefert.\n",
        "\n",
        "Kurz: **Die erste Ableitung des Skalarpotentials (= ein Skalarfeld) ergibt das Gradientenfeld (= ein spezielles Vektorfeld).**\n",
        "\n",
        "* Ist $\\vec{F}(\\vec{r})$ ein **konservatives** Kraftfeld, in dem die Kraft $\\vec{F}$ dem Prinzip des kleinsten Zwanges folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Definition\n",
        "$\n",
        "\\vec{F}(\\vec{r})=-\\operatorname{grad} \\Phi(\\vec{r})=-\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Skalarpotentiale bilden u. a. die mathematische Grundlage der Untersuchung konservativer Kraftfelder wie des elektrischen und des Gravitationsfelds, aber auch von wirbelfreien sogenannten Potentialströmungen.\n",
        "\n",
        "**Ein Skalarfeld $\\Phi: \\vec{r} \\mapsto \\Phi(\\vec{r})$ ist genau dann ein Skalarpotential**, wenn es in einem einfach zusammenhängenden Gebiet\n",
        "\n",
        "1. zweimal stetig differenzierbar ist, das heißt keine , Sprünge\", Stufen oder andere\n",
        "Unstetigkeitsstellen enthält;\n",
        "\n",
        "2. zu ihm ein Vektorfeld $\\vec{F}: \\vec{r} \\mapsto \\vec{F}(\\vec{r})$ existiert, so dass gilt:\n",
        "$\\vec{F}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})$\n",
        "\n",
        "$\\vec{F}$ wird daher oft auch das zugehörige Gradientenfeld genannt, das als Gradient des Skalarpotentials $\\Phi$ seinerseits stets folgende Bedingungen erfüllt:\n",
        "\n",
        "1. **Wegunabhängigkeit des Kurvenintegrals**: Der Wert des Kurvenintegrals entlang einer beliebigen Kurve S innerhalb des Feldes ist nur von ihrem Anfangs- und Endpunkt abhängig, nicht dagegen von ihrer Länge.\n",
        "\n",
        "2. **Verschwinden des geschlossenen Kurvenintegrals für beliebige Randkurven S**:\n",
        "$\\oint_{S} \\operatorname{grad} \\Phi(\\vec{r}) \\mathrm{d} \\vec{r}=\\oint_{S} \\vec{F}(\\vec{r}) \\mathrm{d} \\vec{r}=0$\n",
        "\n",
        "3. **Generelle Rotationsfreiheit bzw. Wirbelfreiheit des Feldes**:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\operatorname{rot} \\vec{F}(\\vec{r})=\\vec{\\nabla} \\times \\vec{F}(\\vec{r})=\\overrightarrow{0}$\n",
        "\n",
        "![cc](https://upload.wikimedia.org/wikipedia/commons/d/d9/GravityPotential.jpg)\n",
        "\n",
        "*Das Gravitationspotential einer homogenen Kugel*"
      ],
      "metadata": {
        "id": "87gT5L69aUhd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2pH152ScafZ"
      },
      "source": [
        "*Vektorpotential*\n",
        "\n",
        "**Ist ein Vektorfeld $v$ das Gradientenfeld einer Funktion $f,$ das heißt $v=\\nabla f$  (=Skalarfeld ist abgeleitet nach Ort), so bezeichnet man $f$ als Potential**. [Vektorpotential](https://de.wikipedia.org/wiki/Vektorpotential)\n",
        "\n",
        "* Wirbelfelder, die Rotationen eines anderen Vektorfelds sind, sind stets quellenfrei – quellenfreie Vektorfelder können daher umgekehrt immer auch als Rotation eines anderen Vektorfelds interpretiert werden, das man in diesem Fall als „Vektorpotential“ des betreffenden quellenfreien Vektorfelds bezeichnet\n",
        "\n",
        "* Mathematisch ist das Vektorpotential (im Unterschied zum Skalarpotential) ein Vektorfeld $\\mathbf{A}(\\mathbf{r}),$ dessen Rotation ein zweites Vektorfeld $\\mathbf{B}(\\mathbf{r})$ liefert gemäß folgender Formel:\n",
        "\n",
        "> $\n",
        "\\mathbf{B}(\\mathbf{r}) \\stackrel{\\text { def }}{=} \\operatorname{rot} \\mathbf{A}(\\mathbf{r})=\\nabla \\times \\mathbf{A}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "Vektorpotentiale lassen sich u. a. dazu verwenden, die zur Beschreibung des elektromagnetischen Felds verwendeten Maxwell-Gleichungen zu entkoppeln und dadurch leichter lösbar zu machen.\n",
        "\n",
        "Obwohl es zunächst nur als mathematisches Hilfsmittel eingeführt wurde, kommt ihm in der Quantenmechanik physikalische Realität zu, wie das [Aharonov-Bohm-Experiment](https://de.wikipedia.org/wiki/Aharonov-Bohm-Effekt) zeigte.\n",
        "\n",
        "**Beziehungen zwischen Vektor- und Skalarpotential**: Gemäß dem helmholtzschen Theorem kann (fast) jedes Vektorfeld $\\mathrm{K}(\\mathrm{r})$\n",
        "\n",
        "* als **Superposition zweier Komponenten $\\mathbf{F}(\\mathbf{r})$ und $\\mathbf{G}(\\mathbf{r})$ aufgefasst werden**,\n",
        "\n",
        "* deren erste der Gradient eines Skalarpotentials $\\Phi(\\mathbf{r})$ ist, die zweite dagegen die Rotation eines Vektorpotentials $\\mathbf{\\Gamma}(\\mathbf{r}):$\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "* Ist $\\mathbf{F}(\\mathbf{r})$ ein konservatives Kraftfeld, in dem die Kraft $\\mathbf{F}$ dem [Prinzip des kleinsten Zwanges](https://de.wikipedia.org/wiki/Prinzip_des_kleinsten_Zwanges) folgend stets der Richtung des maximalen Anstiegs des Potentials $\\Phi$ entgegengerichtet ist, gilt alternativ die Schreibweise\n",
        "\n",
        "> $\n",
        "\\mathbf{K}(\\mathbf{r})=\\mathbf{F}(\\mathbf{r})+\\mathbf{G}(\\mathbf{r})=-\\operatorname{grad} \\Phi(\\mathbf{r})+\\operatorname{rot} \\mathbf{\\Gamma}(\\mathbf{r})=-\\nabla \\Phi(\\mathbf{r})+\\nabla \\times \\mathbf{\\Gamma}(\\mathbf{r})\n",
        "$\n",
        "\n",
        "https://de.wikipedia.org/wiki/Skalarpotential#Beziehungen_zwischen_Skalar-_und_Vektorpotential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vCM9-DX-j2"
      },
      "source": [
        "###### *Differentialoperator*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpGcMJaKroS6"
      },
      "source": [
        "**Differentialoperator**\n",
        "\n",
        "Die drei kovarianten Differentialoperatoren (Covector-Maps!)\n",
        "\n",
        "> **Folgende drei Rechenoperationen sind in der Vektoranalysis von besonderer Bedeutung**, weil sie Felder produzieren, die sich bei räumlicher Drehung des ursprünglichen Feldes mitdrehen. Operativ formuliert: Bei Gradient, Rotation und Divergenz **spielt es keine Rolle, ob sie vor oder nach einer Drehung angewendet werden**. Diese Eigenschaft folgt aus den **koordinatenunabhängigen** Definitionen.\n",
        "\n",
        "**Differential**: Der [**Differentialoperator**](https://de.wikipedia.org/wiki/Differentialoperator) $\\frac{\\mathrm{d}}{\\mathrm{d} x}$ zur Bildung von [Differentialen](https://de.wikipedia.org/wiki/Differential_(Mathematik)) (ist eine Funktion, die einer Funktion eine Funktion zuordnet und die Ableitung nach einer oder mehreren Variablen enthält.).\n",
        "\n",
        "Als Differentialoperator kann er beispielsweise auf ein Skalarfeld angewandt werden und wird in diesem Fall ein Vektorfeld liefern, das Gradientenfeld genannt wird.\n",
        "\n",
        "  * [Gradient](https://de.wikipedia.org/wiki/Gradient_(Mathematik)): Gibt die Richtung und Stärke des steilsten Anstiegs eines Skalarfeldes an. Der Gradient eines Skalarfeldes ist ein Vektorfeld. Ein [Gradientenfeld](https://de.wikipedia.org/wiki/Gradientenfeld) **ist ein Vektorfeld**, das aus einem Skalarfeld durch Differentiation nach dem Ort abgeleitet wurde, bzw. – kürzer formuliert – der Gradient des Skalarfelds. $\\operatorname{grad} \\phi:=\\vec{\\nabla} \\phi=\\left(\\begin{array}{c}\\frac{\\partial \\phi}{\\partial x} \\\\ \\frac{\\partial \\phi}{\\partial y} \\\\ \\frac{\\partial \\phi}{\\partial z}\\end{array}\\right)$\n",
        "\n",
        "  * [Divergenz](https://de.wikipedia.org/wiki/Divergenz_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, von Punkten wegzufließen. $\\operatorname{div} \\vec{F}:=\\vec{\\nabla} \\cdot \\vec{F}=\\frac{\\partial F_{x}}{\\partial x}+\\frac{\\partial F_{y}}{\\partial y}+\\frac{\\partial F_{z}}{\\partial z}$\n",
        "\n",
        "  * [Rotation (curl)](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes): Gibt die Tendenz eines Vektorfeldes an, um Punkte zu rotieren. Curl = Circulation Density of a Vector Field (it's a velocity field). Siehe auch [Koordinatentransformation](https://de.wikipedia.org/wiki/Koordinatentransformation#Drehung_(Rotation)) sowie [Drehmatrix](https://de.wikipedia.org/wiki/Drehmatrix). $\\operatorname{rot} \\vec{F}:=\\vec{\\nabla} \\times \\vec{F}=\\left(\\begin{array}{c}\\frac{\\partial F_{z}}{\\partial y}-\\frac{\\partial F_{y}}{\\partial z} \\\\ \\frac{\\partial F_{x}}{\\partial z}-\\frac{\\partial F_{z}}{\\partial x} \\\\ \\frac{\\partial F_{y}}{\\partial x}-\\frac{\\partial F_{x}}{\\partial y}\\end{array}\\right)$\n",
        "\n",
        "*Depiction of a two-dimensional vector field with a uniform curl.*:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1555.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nabla-Operator (Del operator)*\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Del\n",
        "\n",
        "The differential operator del, also called nabla, is an important vector differential operator. It appears frequently in physics in places like the differential form of Maxwell's equations. In three-dimensional Cartesian coordinates, del is defined as\n",
        "\n",
        "> $\\nabla=\\hat{\\mathbf{x}} \\frac{\\partial}{\\partial x}+\\hat{\\mathbf{y}} \\frac{\\partial}{\\partial y}+\\hat{\\mathbf{z}} \\frac{\\partial}{\\partial z}$\n",
        "\n",
        "Del defines the gradient, and is used to calculate the curl, divergence, and Laplacian of various objects.\n",
        "\n",
        "Koordinatenunabhängige Definition mit dem Nabla-Operator\n",
        "\n",
        "* Der [Nabla-Operator](https://de.wikipedia.org/wiki/Nabla-Operator) ist ein Symbol, das in der Vektor- und Tensoranalysis benutzt wird, um kontextabhängig einen der drei Differentialoperatoren Gradient, Divergenz oder Rotation zu notieren.\n",
        "\n",
        "* [**Nabla Operator**](https://de.wikipedia.org/wiki/Nabla-Operator) $\\nabla$ zur Bestimmung des Gradienten einer mehrdimensionalen Funktion. Mit einem der drei **Differentialoperatoren**.\n",
        "\n",
        "* Der Nabla-Operator ist auch in anderen Koordinatensystemen definiert und so kann mit ihm zum Beispiel die Rotation [koordinatenunabhängig](https://de.wikipedia.org/wiki/Rotation_eines_Vektorfeldes#Koordinatenunabhängige_Definition_mit_dem_Nabla-Operator) durch\n",
        "\n",
        "> $\\operatorname{rot} \\vec{F}:=\\nabla \\times \\vec{F}$\n",
        "\n",
        "definiert werden. Mit dem Nabla-Operator können auch der Gradient- sowie die Divergenz eines Vektorfeldes dargestellt und Produktregeln hergeleitet werden.\n",
        "\n",
        "Formal ist der Nabla-Operator ein Vektor, dessen Komponenten die partiellen\n",
        "Ableitungsoperatoren $\\frac{\\partial}{\\partial x_{i}}$ sind:\n",
        "\n",
        "> $\n",
        "\\vec{\\nabla}=\\left(\\frac{\\partial}{\\partial x_{1}}, \\ldots, \\frac{\\partial}{\\partial x_{n}}\\right)\n",
        "$"
      ],
      "metadata": {
        "id": "ARBZopZZd73L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSvDHxVmgzEU"
      },
      "source": [
        "*Laplace Operator*\n",
        "\n",
        "Der [**Laplace-Operator**](https://de.wikipedia.org/wiki/Laplace-Operator) ist ein linearer Differentialoperator innerhalb der [mehrdimensionalen Analysis](https://de.wikipedia.org/wiki/Analysis#Mehrdimensionale_reelle_Analysis) ($\\Delta \\colon D(\\Delta )\\to L^{2}(\\mathbb{R} ^{n})$ und ein unbeschränkter Operator). Der Laplace-Operator kommt in vielen Differentialgleichungen vor, die das Verhalten physikalischer Felder beschreiben. Beispiele sind die Poisson-Gleichung der Elektrostatik, die **Navier-Stokes-Gleichungen** für Strömungen von Flüssigkeiten oder Gasen und die Diffusionsgleichung für die Wärmeleitung.\n",
        "\n",
        "Der Laplace-Operator ordnet einem zweimal differenzierbaren Skalarfeld $f$ **die Divergenz seines Gradienten zu**,\n",
        "\n",
        ">$\n",
        "\\Delta f=\\operatorname{div}(\\operatorname{grad} f)\n",
        "$\n",
        "\n",
        "oder mit dem Nabla-Operator notiert\n",
        "\n",
        ">$\n",
        "\\Delta f=\\nabla \\cdot(\\nabla f)=(\\nabla \\cdot \\nabla) f=\\nabla^{2} f\n",
        "$\n",
        "\n",
        "**Laplacian in the Heat Equation**\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Flow & Flux*"
      ],
      "metadata": {
        "id": "6Nb2j8QPdg3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\vec{F}=\\langle M, N\\rangle$ be a vector field with continuous components defined on a smooth curve $C$, parameterized by $\\vec{r}(t)=\\langle f(t), g(t)\\rangle$, let $\\vec{T}$ be the unit tangent vector of $\\vec{r}(t)$, and let $\\vec{n}$ be the clockwise $90^{\\circ}$ degree rotation of $\\vec{T}$.\n",
        "\n",
        "$\\vec{F}$ - Vectorfield\n",
        "\n",
        "$\\vec{T}$ - Curve or Path\n",
        "\n",
        "$ds$ - Wegänderung / Weglänge\n",
        "\n",
        "**The flow of $\\vec{F}$ along $C$ is**\n",
        "\n",
        "> $\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{T} \\mathrm{~d} s=\\int_{C} \\vec{F} \\cdot \\overrightarrow{d r} .\n",
        "$\n",
        "\n",
        "* Flow - the degree to which my path is aligned with the vectorfield = Tangential to curve. Flow (special case: Circulation).\n",
        "\n",
        "**The [flux](https://en.m.wikipedia.org/wiki/Flux) of $\\vec{F}$ across $C$ is**\n",
        "\n",
        ">$\n",
        "\\int_{C} \\vec{F} \\cdot \\vec{n} \\mathrm{~d} s=\\int_{C} M \\mathrm{~d} y-N \\mathrm{~d} x=\\int_{C}\\left(M g^{\\prime}(t)-N f^{\\prime}(t)\\right) d\n",
        "$\n",
        "\n",
        "* Flow - the degree to which my path is normal to the vectorfield = Normal to curve\n",
        "\n",
        "Vector Fields can represent different things, like force fields. Here we consider velcoity fields, like in some turbulent water or wind system. The vector tells you magnitude and direction water or wind is flowing at that particular point.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1556.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "zBJu_o0zwhge"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolnoE19IyYG"
      },
      "source": [
        "###### *Kurvenintegral (Line Integral)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [What is a LINE INTEGRAL? // Big Idea, Derivation & Formula](https://www.youtube.com/watch?v=WA5_a3C2iqY&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=3)\n",
        "\n",
        "**Now what is the area of this rectangle? How can we calculate that?**\n",
        "\n",
        "* the height is just the function value at whatever particular point you are at $f\\left(x_{k}, y_{k}\\right)$\n",
        "\n",
        "* what is the size of the base? It's $\\Delta s_k$, which is the Arc length\n",
        "\n",
        "Standard definition of the [line integral](https://en.m.wikipedia.org/wiki/Line_integral)\n",
        "\n",
        "> $\\Delta A_{k}=f\\left(x_{k}, y_{k}\\right) \\Delta s_{k}$\n",
        "\n",
        "wird zu:\n",
        "\n",
        "> $\\int_{C} f(x, y) d s$\n",
        "\n",
        "> $= \\lim _{n \\rightarrow \\infty} \\sum_{k=1}^{\\infty} f(x_{k}, y_{k}) \\Delta s_{k}$\n",
        "\n",
        "(standard Riemann integral definition)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1557.png)"
      ],
      "metadata": {
        "id": "remNbl5LQNZE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEoDCIYLOem"
      },
      "source": [
        "* z.B. zur Berechnung des Umfangs eines Objekts. welche Kraft ist noetig, um durch ein Vektorfeld auf einer Kurve entlang zu gehen?\n",
        "\n",
        "* Das [Kurven-, Linien-, Weg- oder Konturintegral](https://de.wikipedia.org/wiki/Kurvenintegral) erweitert den gewöhnlichen Integralbegriff für die Integration\n",
        "\n",
        "  * in der komplexen Ebene (Funktionentheorie) oder\n",
        "\n",
        "  * im mehrdimensionalen Raum (Vektoranalysis).\n",
        "\n",
        "* Den Weg, die Linie oder die Kurve, über die integriert wird, nennt man den Integrationsweg.\n",
        "\n",
        "* Wegintegrale über geschlossene Kurven werden auch als Ringintegral, Umlaufintegral oder Zirkulation bezeichnet und mit dem Symbol\n",
        "∮ bzw. $\\textstyle \\oint$  geschrieben.\n",
        "\n",
        "Auch Linienintegral. Berechne z.B. den kurzesten Weg zwischen zwei Punkten unter Berucksichtigung der Geschwindigkeit (eine gerade Linie ist nicht immer der kurzeste Weg oder ein moglicher Weg).\n",
        "\n",
        "*The line integral over a scalar field f can be thought of as the area under the curve C along a surface z = f(x,y), described by the field*\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1558.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_lY1plvJOZ-"
      },
      "source": [
        "**Kurvenintegral 1. Art (über Skalarfelder, nicht-orientiert)**\n",
        "\n",
        "* zB zur Berechnung der Masse eines Drahtes [entlang einer Helix](https://www.youtube.com/watch?v=8XcqTg1NPKg) mit einer gegebenen Dichte\n",
        "\n",
        "Wegintegral erster Art ist das **Wegintegral einer stetigen Funktion**, $\n",
        "f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\n",
        "$ entlang eines stückweise stetig differenzierbaren Weges $\n",
        "\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}\n",
        "$ ist definiert als\n",
        "\n",
        "> $\\int_{\\mathcal{C}} f \\mathrm{~d} s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\|_{2} \\mathrm{~d} t$\n",
        "\n",
        "Für eine stetige Funktion $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ und einem regulären Weg $\\gamma:[a, b] \\rightarrow \\mathbb{R}^{n}$ definiert man das Kurvenintegral von $f$ längs $\\gamma$ durch: $\\int_{\\gamma} f d s:=\\int_{a}^{b} f(\\gamma(t))\\|\\dot{\\gamma}(t)\\| d t$\n",
        "\n",
        "  * $ds$ sowie $ \\|\\dot{\\gamma}(t)\\| d t$ sind das '**Linienelement**'\n",
        "\n",
        "  * $ \\|\\dot{\\gamma}(t)\\|$ ist die Norm von der Ableitung, die man berucksichtigen muss beim Integrieren wie schnell man durch die Kurve lauft (Gewichtungsfaktor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvuqF-aJDBO"
      },
      "source": [
        "**Kurvenintegral 2. Art (über Vektorfelder, orientiert)**\n",
        "\n",
        "* nicht mehr skalare, sondern vektorielle Funktion integrieren (Vektorfeld)\n",
        "\n",
        "* ps: jedes Kurvenintegral zweiter Art ist auch ein Kurvenintegral erster Art\n",
        "\n",
        "* z.B. um eine Arbeit, Zirkulation oder elektrische Spannung [zu berechnen](https://www.youtube.com/watch?v=HmgkyI_Q0Oo)\n",
        "\n",
        "* nennt man daher auch \"Arbeitsintegral\", wenn man sich zB ein Kraftfeld v vorstellt. Oder Zirkulation mit Integral entlang des Weges berechnen, wenn v ein Geschwindigkeitsfeld ist. Oder elektrische Spannung, wenn es ein elektrisches Feld ist.\n",
        "\n",
        "* Zunächst fragen: hat v ein Skalarpotential? Ist die betrachtete Menge einfach zusammenhängend und ist zB die Rotation des Vektorfeldes gleich der Nullvektor? Berechnung des Skalarpotentials: mit der Ansatzmethode oder mit der Kurvenintegralmethode\n",
        "\n",
        "* Danach fragen, ob der Weg geschlossen oder offen ist? (geschlossen: Anfangspunkt = Endpunkt, wie bei Kreis oder Dreieck). Ist er geschlossen, ist der Wert des Kurvenintegrals gleich Null. (**Remember**: Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ für beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQdL1yhzPGzI"
      },
      "source": [
        "###### *Oberfläche (Surface Area)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Introduction (Explicit, Implicit and Parametric)*"
      ],
      "metadata": {
        "id": "Rs9afq_oyhKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Surface_integral"
      ],
      "metadata": {
        "id": "EsFuukjmxCb8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChIebVaPLd6"
      },
      "source": [
        "* z.B. zur Berechnung der Oberfläche eines Balls, Funktion der Temperatur an jedem Punkt an einer Oberfläche, um die durchschnittliche Temperatur auf der Oberfläche zu berechnen (https://youtu.be/AIxiYG-gZ00 at min 3:00)\n",
        "\n",
        "* Durch Parametrisierung wird z.B. die Kruemmung einer Ebene in $R^3$ beruecksichtigt. Man berechnet naemlich die Flaeche, in dem man von $R^3$ auf $R^2$ projiziert und dann das Integral berechnet (das geht, weil Flaeche in $R^3$ ist offen, stetig differenzierbar und bijektiv)\n",
        "\n",
        "* Das Oberflächenintegral oder Flächenintegral ist eine Verallgemeinerung des Integralbegriffes auf ebenen oder gekrümmten Flächen. Das Integrationsgebiet $\\mathcal{F}$ ist also nicht ein eindimensionales Intervall, sondern eine zweidimensionale Menge im dreidimensionalen Raum $\\mathbb{R}^{3}$. Für eine allgemeinere Darstellung im $\\mathbb{R}^{n}$ mit $n \\geq 2$ siehe: Integration auf Mannigfaltigkeiten.\n",
        "\n",
        "Es wird generell zwischen einem skalaren und einem vektoriellen Oberflächenintegral unterschieden, je nach Form des Integranden und\n",
        "des sogenannten Oberflächenelements. Sie lauten\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} f \\mathrm{~d} \\sigma$ mit skalarer Funktion $f$ und skalarem Oberflächenelement $\\mathrm{d} \\sigma$ sowie\n",
        "\n",
        "> $\\iint_{\\mathcal{F}} \\vec{v} \\cdot \\mathrm{d} \\vec{\\sigma}$ mit vektorwertiger Funktion $\\vec{v}$ und vektoriellem Oberflächenelement $\\mathrm{d} \\vec{\\sigma}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcvZH7qLBOzs"
      },
      "source": [
        "Allgemein lässt sich eine Fläche im $\\mathbb{R}^{3}$ mit zwei Parametern $u$ und $v$ in\n",
        "folgender Form darstellen:\n",
        "\n",
        "> $\n",
        "\\varphi: B \\rightarrow \\mathbb{R}^{3}, \\quad(u, v) \\mapsto \\vec{\\varphi}(u, v)=\\left(\\begin{array}{l}\n",
        "x(u, v) \\\\\n",
        "y(u, v) \\\\\n",
        "z(u, v)\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Auf der Fläche $\\vec{\\varphi}(u, v)$ bilden die Kurvenscharen $u=$ const bzw. $v=$ const die Koordinatenlinien. Diese überziehen die Fläche mit einem Koordinatennetz.\n",
        "wobei durch jeden Punkt zwei Koordinatenlinien verlaufen. Somit hat ieder Punkt auf der Fläche eindeutige Koordinaten $\\left(u_{0}, v_{0}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45jm0deB13y"
      },
      "source": [
        "Mit den Parametrisierungen und den Oberflächenelementen kann man nun die Oberflächenintegrale definieren. Diese mehrdimensionalen Integrale sind Lebesgue-Integrale, können aber in den meisten Anwendungsfällen als mehrfache Riemann-Integrale berechnet werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yO9AukUtSzo"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Oberflächenintegral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction Video: [Describing Surfaces Explicitly, Implicitly & Parametrically // Vector Calculus](https://youtu.be/jZRqCfi5_Uo)"
      ],
      "metadata": {
        "id": "8vVdeCClvrPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_047.PNG)"
      ],
      "metadata": {
        "id": "2l2A7LKvxN6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_048.PNG)"
      ],
      "metadata": {
        "id": "Vy8LqMptxP8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_049.PNG)"
      ],
      "metadata": {
        "id": "txf7BHR5xRyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_050.PNG)"
      ],
      "metadata": {
        "id": "NOOvynWUxTWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_051.PNG)"
      ],
      "metadata": {
        "id": "shyAhgDcxUy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this a transformation. The parametrization is a way to map the simple rectangle case to the complex cone case:"
      ],
      "metadata": {
        "id": "HbWGt8pbwJIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_052.PNG)"
      ],
      "metadata": {
        "id": "0nkMWGDIxXS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Calculate surface area*"
      ],
      "metadata": {
        "id": "9F9ROmFYzbtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Surface Area = $\\int_{c}^{d} \\int_{a}^{b}\\left|\\vec{r}_{u} \\times \\vec{r}_{v}\\right| d u \\, d v$\n",
        "\n",
        "* ich parametrize zwischen a und b (zB zwischen 0 und 2 $\\pi$ fur $\\theta$) bzw c und d (zB zwischen 0 und $\\pi$ fur $\\phi$)\n",
        "\n",
        "* ich bestimme das partielle Differential von ${r}$ with respect to $_{u}$ und dann nochmal das partielle Differential von ${r}$ with respect to $_{v}$\n",
        "\n",
        "* dann nehme ich beide partiellen Differentiale und berechne das Cross Product"
      ],
      "metadata": {
        "id": "Imn0bOPskCfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://youtu.be/hnjfY9hRIXk"
      ],
      "metadata": {
        "id": "jPhsipagzjxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I can describe a surface parametrically, that means there is somewhere there is a u/v coordinate system with a region. And there is a map / transformation to a more squiggely version in 3D for example:"
      ],
      "metadata": {
        "id": "ZRcm323Cz4qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_053.PNG)"
      ],
      "metadata": {
        "id": "zxlu5_9s2CZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r_u}$ is the partial derivative in the direction where $v$ is constant\n",
        "\n",
        "$\\vec{r_v}$ is the partial derivative in the direction where $u$ is constant\n",
        "\n",
        "And above pointing is the cross product of $\\vec{r_u}$  with $\\vec{r_v}$"
      ],
      "metadata": {
        "id": "DorAizAb1CDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now comes the most important: the length of the cross product is the same as the area of the parallelogram of the two vectors that form the cross product. $\\rightarrow$ With that I can approximate the surface area when I take the limit (and takes the definition of an integral)."
      ],
      "metadata": {
        "id": "Mp_2P1Y_2UAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_054.PNG)"
      ],
      "metadata": {
        "id": "skuzWvoB3IIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here you get the final formula:"
      ],
      "metadata": {
        "id": "2DXWCQfO3iWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_055.PNG)"
      ],
      "metadata": {
        "id": "g57LTd053g-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 1: Calculation for a Sphere*"
      ],
      "metadata": {
        "id": "sOKlhEom4-yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/iTkdDnC0seQ"
      ],
      "metadata": {
        "id": "IE91hyiV3vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see example calculation here for the surface of a sphere:"
      ],
      "metadata": {
        "id": "o_WQFs7q3ndU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_058.PNG)"
      ],
      "metadata": {
        "id": "IbRBNcSD5TG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First we take spherical coordinate, because it's more natural for a sphere"
      ],
      "metadata": {
        "id": "W4da3T_U34-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_057.PNG)"
      ],
      "metadata": {
        "id": "BEYNs3Ty5Q0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_056.PNG)"
      ],
      "metadata": {
        "id": "lOWlLVQL5Pvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_059.PNG)"
      ],
      "metadata": {
        "id": "lqQekDTy5U8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\vec{r}_{\\phi}$ und $\\vec{r}_{\\theta}$ sind die partial derivatives with respect ti phi and theta jeweils\n",
        "\n",
        "$\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}$ ist das cross product (das ist die Determinante)\n",
        "\n",
        "Dann nehme ich davon $\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right|$ (square root), um die Fläche der Oberfläche zu erhalten"
      ],
      "metadata": {
        "id": "9ueegH1T5ttN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_060.PNG)"
      ],
      "metadata": {
        "id": "Zm6soIMU7FIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_061.PNG)"
      ],
      "metadata": {
        "id": "CeifKd7M7HFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I plug it in into $A=\\int_{0}^{2 \\pi} \\int_{0}^{\\pi}\\left|\\vec{r}_{\\phi} \\times \\vec{r}_{\\theta}\\right| d \\phi \\, d \\theta$\n",
        "\n",
        "* mit den Limits dass phi is zwischen 0 und pi\n",
        "\n",
        "* und theta is zwischen 0 und 2*pi"
      ],
      "metadata": {
        "id": "shp6Hska7QI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_062.PNG)"
      ],
      "metadata": {
        "id": "Ui1roOrb7v0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 2: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "Pf3GY2W7nkkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 1](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "ZZkvuyz8nryU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find surface area of a plane z = -x that lies inside a cylinder $x^2 + y^2 = 4$\n",
        "\n",
        "* First find a parametrization for the plane: i use standard polar coordinate with sin and cosine, and for the z-component (the height of the surface area) I can use -x\n",
        "\n",
        "> $\\vec{r}(r, \\theta)=\\langle r \\cos \\theta, r \\sin \\theta,-r \\cos \\theta \\rangle$\n",
        "\n",
        "* second, i know what my constraints are: $2\\pi$, because the cylinder = 4 so radius = 2\n",
        "\n",
        "> $0 \\leq r \\leq 2 \\quad 0 \\leq \\theta \\leq 2 \\pi$"
      ],
      "metadata": {
        "id": "4W0gn2ven_XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_063.png)"
      ],
      "metadata": {
        "id": "1Mtbl0FwqNG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example 3: Surface area of the portion of a plane that lies within a cylinder*"
      ],
      "metadata": {
        "id": "WS0hoJa5wico"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Computing the Surface Area of a surface parametrically // Example 2](https://www.youtube.com/watch?v=e7-nwb_ncbk&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1&t=1s)"
      ],
      "metadata": {
        "id": "mNac0ltVrYme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I make my parameters $\\theta$ and $z$ (height) $\\rightarrow$ die richtige (iS von einfachste) Parametrisierung ist sehr wichtig"
      ],
      "metadata": {
        "id": "4Kxxwv2vwjxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_064.png)"
      ],
      "metadata": {
        "id": "Vo5gBKjswuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_065.png)"
      ],
      "metadata": {
        "id": "42E-WtVtxPNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_066.png)"
      ],
      "metadata": {
        "id": "XvNkFTfcxQae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Area for Implicit Surfaces*"
      ],
      "metadata": {
        "id": "xDFYb8uW7h9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [Surface Area for Implicit & Explicit Surfaces](https://www.youtube.com/watch?v=k13kwLzoTpo&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=1)"
      ],
      "metadata": {
        "id": "Q2FhPOJS7mFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cases where I don't have a parametrized version (like above), but an implicit version"
      ],
      "metadata": {
        "id": "elSoMeoZ8OBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine two surfaces: one flat on the bottom and one curved above it.\n",
        "\n",
        "* Take the normal vector of both and compare them.\n",
        "\n",
        "* If both surfaces are flat, the inner product = 1 and its integrand of the above surface will give you the surface area of the bottom (since they are equal).\n",
        "\n",
        "* If the inner product is not 1, then there is a difference and it will compute the surface area of the above surface."
      ],
      "metadata": {
        "id": "JGd3syYr7rqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_067.png)"
      ],
      "metadata": {
        "id": "EP-BKu5x8HLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implicit Surface F(x, y, z) = C\n",
        "\n",
        "Assume:\n",
        "\n",
        "* Smooth (i.e. F differentiable, $\\nabla F \\neq \\vec{0}$\n",
        "and continuous)\n",
        "\n",
        "* The surface is 'above' a region in the xy-plane with $\\nabla F \\bullet \\hat{k} \\neq 0$\n",
        "\n",
        "> Surface Area $=\\iint_{R} \\frac{|\\nabla F|}{|\\nabla F \\cdot \\hat{k}|} d A$"
      ],
      "metadata": {
        "id": "XxRR64jL8p5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_069.png)"
      ],
      "metadata": {
        "id": "qyMsoMtf-Okp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The more the plane above is twisted, the more the gradient $|\\nabla F \\cdot \\hat{k}|$ gets smaller, and if the denominator gets smaller, the total surface is bigger overall. We practically get a ratio between the original length of the gradient vector and then this portion of the gradient vector in the $\\hat{k}$ direction -  that is our scaling factor!"
      ],
      "metadata": {
        "id": "l-81RyUh_gBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I change this now with a generic plane, because choosing the $\\hat{k}$ direction was a bit arbitrary:"
      ],
      "metadata": {
        "id": "6GLNqJEd99f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_068.png)"
      ],
      "metadata": {
        "id": "AZrhMZvg95h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Surface Area for Explicit Surfaces*"
      ],
      "metadata": {
        "id": "-TPsQ4p0KZ2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's turn to explicit surfaces**"
      ],
      "metadata": {
        "id": "wcq8cfnW-TM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_070.png)"
      ],
      "metadata": {
        "id": "N4Jpyv30-lYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example video: [Computing the Surface Area of an Implicitly Defined Surface](https://www.youtube.com/watch?v=810yT1zQtxA&list=PLBn8lN0DcvpnIIfjhsT_n2DYZiqJAdfrr&index=2)"
      ],
      "metadata": {
        "id": "TAeCA5jt_97i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Oberflächenintegral (Surface Integral)*"
      ],
      "metadata": {
        "id": "CQu9zwVlu7sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Formulas & Examples*\n",
        "\n",
        "Moving on to surface integrals:\n",
        "\n",
        "Now $G(x, y, z)$ defined on a smooth surface\n",
        "\n",
        "> Surface Integral $=\\iint_S G(x, y, z) d \\sigma$\n",
        "\n",
        "* g (x,y,z) is a function that is defined on that surface\n",
        "\n",
        "> the surface integral is adding up the values of that function along each elements of the surface area\n",
        "\n",
        "> It's like some 'height' is added: you have the position x and y. And z can be the temperature at each point on a surface, or the thickness, or else. It's like an abstract 'height'.\n",
        "\n",
        "Changing our Surface Area (SA) definitions to Surface Integral (SI) definitions with cross product and function:\n",
        "\n",
        "*Potential Applications of Surface Integrals:*\n",
        "\n",
        "* Here: add up density * areas = surface integral to compute thickness\n",
        "\n",
        "Repetition of Surface Areas computed only with the cross product:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1559.png)"
      ],
      "metadata": {
        "id": "f02swzD0zHoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Orientable vs Non-Orientable Surfaces*\n",
        "\n",
        "Source: https://youtu.be/S48JsV-pCBo\n",
        "\n",
        "> <font color=\"blue\">**Definition: A smooth surface is orientable if there is a continuous, field of unit normal vectors**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1560.png)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1561.png)"
      ],
      "metadata": {
        "id": "imvAz-081zR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fundamental Theorem of Calculus (Konservatives bzw. wirbelfreies Vektorfeld)*"
      ],
      "metadata": {
        "id": "0USsGJE7Tfz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Konservatives (bzw. wirbelfreies) Vektorfeld & Fundamental Theory of Calculus*\n",
        "\n",
        "Ein konservatives Feld kann geschrieben werden als Derivative eines **Potential Function** (siehe Skalarpotenzial):\n",
        "\n",
        "> $\\vec{F} = \\Delta f$\n",
        "\n",
        "> Das Skalarpotenzial ist ein Maß für die potenzielle Energie. Das heisst, wenn sich in einem konservativen Kraftfeld ein Körper entgegen der wirkenden Kraft bewegt, dann erhöht sich seine potenzielle Energie.\n",
        "\n",
        "https://youtu.be/tb3KSeF0WrQ\n",
        "\n",
        "Source: [Conservative Vector Fields // Vector Calculus](https://www.youtube.com/watch?v=76nzOtupeRc&list=PLHXZ9OQGMqxfW0GMqeUE1bLKaYor6kbHa&index=13)\n",
        "\n",
        "Like electric (electromagnetic) fields or gravitational fields - the work done (i.e. by the force of gravity on a moving ball in the air) only depends on the endpoints, the differences in the height in this example (now matter which way the ball went).\n",
        "\n",
        "A field $\\vec{F}$ is conservative on an open domain if (the line integral along the curve c):\n",
        "\n",
        ">$\\int_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "is the same for ALL paths $C$ between points $A$ and $B$ in the domain.\n",
        "\n",
        "**The work done is in all 3 cases the same, even though the paths are different:**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_009.png)\n",
        "\n",
        "If you compute it you get the integral of the derivative at the end. We know from the **Fundamental Theory of Calculus** that if you integrate the derivative, then you get the endpoints! In the above case the work done is zero.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/vector_010.png)\n",
        "\n",
        "**When is a field conservative?**\n",
        "\n",
        "A continuous field $\\vec{F}$ is conservative * if and only if\n",
        "\n",
        ">$\n",
        "\\vec{F}=\\nabla f\n",
        "$\n",
        "\n",
        "For some differentiable 'potential function' $f$ (=scalar function).\n",
        "\n"
      ],
      "metadata": {
        "id": "xSQIpQcXex8o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56sZZ-KLKsX_"
      },
      "source": [
        "*Konservatives bzw. wirbelfreies Vektorfeld*\n",
        "\n",
        "* **In Physik sind conservative forces jene, wo es keine Friktion, Air resistance etc. gibt**\n",
        "\n",
        "* Vektorfelder, die Gradienten eines Skalarfelds sind, werden in Anlehnung an den Begriff des „konservativen Kraftfelds“ oft auch als konservative Vektorfelder bezeichnet (siehe Eigenschaften unten unter Gradientenfeld)\n",
        "\n",
        "* Als [wirbelfrei bzw. konservativ](https://de.wikipedia.org/wiki/Wirbelfreies_Vektorfeld) wird in der Physik und Potentialtheorie ein Vektorfeld $\\vec{X}(\\vec{r})$ bezeichnet, in dem das **Kurvenintegral** $\n",
        "\\oint_{S} \\vec{X}(\\vec{r}) \\cdot \\mathrm{d} \\vec{s}=0$ für beliebige in sich geschlossene Randkurven $S$ stets den Wert null liefert.\n",
        "\n",
        "* Deutet man $\\vec{X}(\\vec{r})$ als Kraftfeld, so ist das Kurvenintegral die gesamte längs der Randkurve $S$ gegen die Kraft $\\vec{X}(\\vec{r})$ verrichtete Arbeit.\n",
        "\n",
        "* Wirbelfrei sind z. B. das ruhende elektrische Feld in der Elektrostatik und das Gravitationsfeld, aber auch Felder wie das Geschwindigkeitsfeld einer Potentialströmung.\n",
        "\n",
        "> Ist $\\vec{X}(\\vec{r})$ wirbelfrei, dann gilt: $\\operatorname{rot} \\vec{X}(\\vec{r})=\\overrightarrow{0}$\n",
        "d. h. die Rotation des Vektorfeldes ist gleich null.\n",
        "\n",
        "* Ist der Definitionsbereich einfach **zusammenhängend, so gilt auch die Umkehrung**.\n",
        "\n",
        "> Wirbelfreie Vektorfelder lassen sich stets als **Gradient eines zugrundeliegenden skalaren Felds** $\\Phi(\\vec{r})$ formulieren (siehe Gradientenfeld): $\n",
        "\\vec{X}(\\vec{r})=\\operatorname{grad} \\Phi(\\vec{r})=\\vec{\\nabla} \\Phi(\\vec{r})\n",
        "$\n",
        "\n",
        "* Daraus folgt, dass für das skalare Feld $\\Phi$ gilt:\n",
        "$\\operatorname{rot}(\\operatorname{grad} \\Phi(\\vec{r}))=\\overrightarrow{0}$ (**siehe auch unten auch Gradientfeld & Skalarpotenzial**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLH2-gTPd43O"
      },
      "source": [
        "*Quellenfreie und wirbelfreie Vektorfelder; Zerlegungssatz*\n",
        "\n",
        "Ein mindestens **zweimal stetig differenzierbares Vektorfeld** $\\mathbf{v}(\\mathbf{r})$ im $\\mathbb{R}^{3}$ heißt quellenfrei (beziehungsweise wirbelfrel), wenn seine **Quellendichte (Divergenz) beziehungsweise Wirbeldichte (Rotation) dort überall Null ist**. Unter der weiteren\n",
        "Voraussetzung, dass die Komponenten von $\\mathbf{v}$ im Unendlichen hinreichend rasch verschwinden, gilt der sogenannte Zerlegungssatz: Jedes Vektorfeld $\\mathbf{v}(\\mathbf{r})$ ist\n",
        "eindeutig durch seine Quellen bzw. Wirbel bestimmt, und zwar gilt die folgende\n",
        "Zerlegung in einen wirbelfreien beziehungsweise quellenfreien Anteil:\n",
        "\n",
        "> $\n",
        "\\mathbf{v}(\\mathbf{r}) \\equiv-\\operatorname{grad}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{div}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}+\\operatorname{rot}_{\\mathbf{r}} \\int_{\\mathbb{R}^{3}} d^{3} \\mathbf{r}^{\\prime} \\frac{\\operatorname{rot}^{\\prime} \\mathbf{v}\\left(\\mathbf{r}^{\\prime}\\right)}{4 \\pi\\left|\\mathbf{r}-\\mathbf{r}^{\\prime}\\right|}\n",
        "$\n",
        "\n",
        "Dies entspricht der Zerlegung eines statischen elektromagnetischen Feldes in den elektrischen beziehungsweise magnetischen Anteil (siehe Elektrodynamik). Es sind also genau die Gradientenfelder (d. h. die , elektrischen Feldkomponenten\") wirbelfrei bzw. genau die Wirbelfelder (d. h. die ,magnetischen Feldkomponenten\") quellenfrei. Dabei sind grad $\\phi(\\mathbf{r}):=\\nabla \\phi,$ div $\\mathbf{v}:=\\nabla \\cdot \\mathbf{v}$ und rot $\\mathbf{v}:=\\nabla \\times \\mathbf{v}$ die bekannten, mit dem Nabla-Operator $(\\nabla)$ der Vektoranalysis\n",
        "gebildeten Operationen."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do I know if a vector field is conservative?**\n",
        "\n",
        ">$\n",
        "\\vec{F}=M(x, y, z) \\hat{\\imath}+N(x, y, z) \\hat{\\jmath}+P(x, y, z) \\hat{k}\n",
        "$\n",
        "\n",
        "is conservative* if and only if\n",
        "\n",
        ">$\n",
        "\\frac{\\partial M}{\\partial y}=\\frac{\\partial N}{\\partial x} \\quad \\frac{\\partial N}{\\partial z}=\\frac{\\partial P}{\\partial y} \\quad \\frac{\\partial M}{\\partial z}=\\frac{\\partial P}{\\partial x}\n",
        "$\n",
        "\n",
        "*for $M, N, P$ continuous first partials, on an open simply connected domain\n",
        "\n",
        "Source: https://youtu.be/ZGUvyGeNT44"
      ],
      "metadata": {
        "id": "Q8yG2xqwbngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**How do we find the (scalar) potential function so $\\vec{F}=\\nabla f ?$**\n",
        "\n",
        "* You take M, N and P and integrate them to get $\\vec{F}$, because\n",
        "\n",
        "> $\\vec{F} = \\nabla f=$ $\\langle \\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta z},\\rangle$ = $M, N, P$ from above\n",
        "\n",
        "* then you fill in a beginning and endpoint of the curve (parametrization), and according to the Fundamental theorem of Line Integrals (where only the beginning and endpoints count - no matter which path has been taken - you take the difference of the endpoint minus beginning\n",
        "\n",
        "Source: https://youtu.be/jlza4rEFXKM"
      ],
      "metadata": {
        "id": "-w6SBRW9cbqc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQKZZ-1M-GE"
      },
      "source": [
        "###### *Green's Theorem*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRhCrZ_fK_8D"
      },
      "source": [
        "*Green's Theorem: Curl or Circulation (Flow) Density (Circulation-Curl Form)*\n",
        "\n",
        "[Satz von Green](https://de.wikipedia.org/wiki/Satz_von_Green)\n",
        "\n",
        "* Der Satz von Green (auch Green-Riemannsche Formel oder Lemma von Green, gelegentlich auch Satz von Gauß-Green)\n",
        "\n",
        "* **erlaubt es, das Integral über eine ebene Fläche durch ein Kurvenintegral auszudrücken**.\n",
        "\n",
        "* Der Satz ist ein Spezialfall des Satzes von Stokes.\n",
        "\n",
        "Integrals ̈atze (Gauss, Stokes, Greenschen Formeln, Lo ̈sung der Poissongleichung, Fundamentalsatz der Vektoranalysis II)\n",
        "\n",
        "* We have the **local** Curl Density (seen under Differentialoperator: Rotation) at a specific point\n",
        "\n",
        "> Circulation Density: $\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)$\n",
        "\n",
        "* And we have the **global** flow (seen in Line Integral)\n",
        "\n",
        "> Circulation: $=\\oint_{C} \\vec{F} \\cdot d \\vec{r}$\n",
        "\n",
        "* with Green's Theorem we can now bring both together\n",
        "\n",
        "Source Video: https://youtu.be/JB99RbQAilI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1564.png)\n",
        "\n",
        "It's at first a bit strange how the points inside are relevant for the curl on the path at the boundary. Circulation only occurs at the boundary. It doesn't matter what happens on the inside. Circluation is only a measure along the boundary.\n",
        "\n",
        "But we are saying circulation is equal to a double integral. An area integral. An area integral with all our circulation densities where it really does matter what's going along in the middle, what these spinners are doing in the inside. They contribute at the end to the curl that happens at the boundary.\n",
        "\n",
        "It's like in the Fundamental Theorem of Calculus (and Line Integral): you take the integral of a derivative and end up with the boundaries. It represents what happens at the boundary.  of some interval. And somehow the difference of the boundary of some interval is related to integrating over the entire integral.\n",
        "\n",
        "And similarly here we are relating information over the entiore region, theb information of the circulation density and accumulating it all up, and that results in just a circulation along the boundary."
      ],
      "metadata": {
        "id": "IhjyNptYvpah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Green's Theorem: Divergence or Flux Density (Divergence-Flux Form or Normal Form)*\n",
        "\n",
        "Source: https://youtu.be/GsjJs71SBec\n",
        "\n",
        "> The concept of Flux along a boundary: The tendency of the vector field to be aligned with the outward normal.\n",
        "\n",
        "> The Flux Density = is also called the Divergence ! (how much the vector field is spreading away from some point at one specific point)\n",
        "\n",
        "* Flux is a global property\n",
        "\n",
        "* at the picture of can be gas leaving a source in the center\n",
        "\n",
        "* If I want to have some notion of the degree to which my vector field is leaving away from one specific point, around which I draw a rectangle\n",
        "\n",
        "* Let's draw this rectangle (like before, but we analysed whsat is the circlution around it) but this time compute flux:\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1565.png)"
      ],
      "metadata": {
        "id": "qQDFOBfOfApb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Green's Theorem: Exercise for 2D (Circulation & Flux)*\n",
        "\n",
        "Source: https://youtu.be/ig62HfJLCG0\n",
        "\n",
        "My field is: $\\vec{F} = x^2 \\hat{i} + xy \\hat{j}$\n",
        "\n",
        "* with M = $x^2$ and N = xy\n",
        "\n",
        "My path around an area is a square: x = $\\pm 1$, y = $\\pm 1$\n",
        "\n",
        "Remember when you compute a double integral: inside first for x and then outside after for y\n",
        "\n",
        "How to get to the terms inside the double integral?\n",
        "\n",
        "* Circulation-Curl Form: $\\oint_{C} \\vec{F} \\cdot d \\vec{r}=\\iint_{R}\\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right) d x d y$\n",
        "\n",
        "* Divergence Flux Form: $\\oint_{C} \\vec{F} \\cdot \\vec{n} d s=\\iint_{R}\\left(\\frac{\\partial M}{\\partial x}+\\frac{\\partial N}{\\partial y}\\right) d x d y$\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1566.png)"
      ],
      "metadata": {
        "id": "kmwWODfmpZ2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stoke's Theorem*"
      ],
      "metadata": {
        "id": "521_dSZMrpGC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y64AmnhcNBpb"
      },
      "source": [
        "> Stokes Theorem is a generalization of Green's Theorem on Curl or Circulation (Flow)\n",
        "\n",
        "[Satz von Stokes](https://de.m.wikipedia.org/wiki/Satz_von_Stokes)\n",
        "\n",
        "* sehr grundlegenden Satz über die Integration von Differentialformen, der den Hauptsatz der Differential- und Integralrechnung erweitert\n",
        "\n",
        "*  eine Verbindungslinie von der Differentialgeometrie zur Algebraischen Topologie eröffnet.\n",
        "\n",
        "* Dieser Zusammenhang wird durch den Satz von de Rham beschrieben, für den der Satz von Stokes grundlegend ist.\n",
        "\n",
        "* Im Folaenden ist $n=3$ und es wird die Schreibweise mit Mehrfachintegralen\n",
        "verwendet.\n",
        "\n",
        "Das qeschlossene Kurvenintegral einer vektoriellen Größe (rechte Seite) kann\n",
        "mittels der Rotation in ein Flächenintegral über eine von dem geschlossenen\n",
        "Integrationsweg $\\Gamma=\\partial A$ berandete, nicht notwendig ebene Fläche\n",
        "umgewandelt werden (linke Seite). Dabei werden - wie auch beim\n",
        "Gauß'schen Satz - die gewöhnlichen Orientierungseigenschaften\n",
        "vorausgesetzt. Es gilt:\n",
        "\n",
        "> $\n",
        "\\iint_{A} \\operatorname{rot} \\vec{F} \\cdot \\mathrm{d} \\vec{A}=\\oint_{\\Gamma=\\partial A} \\vec{F}(\\vec{r}) \\cdot \\mathrm{d} \\vec{r}\n",
        "$\n",
        "\n",
        "Der Vektor $\\mathrm{d} \\vec{A}$ ist gleich dem Betrag der zur betrachteten Fläche $A$ bzw. zu\n",
        "$\\partial V$ gehörenden infinitesimalen Flächenelemente multipliziert mit dem\n",
        "zugehörigen Normalenvektor. Auf der rechten Seite wird durch das\n",
        "Kreissymbol im Integralzeichen daran erinnert, dass über eine geschlossene\n",
        "Kurve integriert wird.\n",
        "\n",
        "Explanation Video: https://youtu.be/0UvNF_cfBJ4\n",
        "\n",
        "Example Calculation: https://youtu.be/ms4JjH0BANU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: What is the circulation around the boundary??**\n",
        "\n",
        "* it is influenced by the curling at each point along the boundary\n",
        "\n",
        "* the tendency to curl within the surface is given by (cross product of del with F vector field) and inner product of that with the normal vector at each point\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1562.png)\n",
        "\n",
        "Conditions on Stokes Theorem: For $S$ a piecewise smooth **oriented** surface with piecewise smooth boundary $C$ and $\\vec{F}$ a field with **continuous first partials** for each component on an open region containing $S$"
      ],
      "metadata": {
        "id": "G25WFhk52AjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Divergence Theorem (Volumenintegral / Gaußscher Integralsatz)*"
      ],
      "metadata": {
        "id": "LxkcVzsECAEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Verallgemeinerung des Green'schen Theorem on Divergence\n",
        "\n",
        "See also Volume Form: https://en.m.wikipedia.org/wiki/Volume_form\n",
        "\n",
        "Deutsch: [Gaußscher Integralsatz](https://de.m.wikipedia.org/wiki/Gaußscher_Integralsatz)\n",
        "\n",
        "* In vector calculus, the [divergence theorem](https://en.m.wikipedia.org/wiki/Divergence_theorem), also known as Gauss's theorem or Ostrogradsky's theorem, is a theorem which relates the flux of a vector field through a closed surface to the divergence of the field in the volume enclosed.\n",
        "\n",
        "> More precisely, **the divergence theorem states that the surface integral of a vector field over a closed surface, which is called the flux through the surface, is equal to the [volume integral](https://en.m.wikipedia.org/wiki/Volume_integral) of the divergence over the region inside the surface**.\n",
        "\n",
        "* Intuitively, it states that the sum of all sources of the field in a region (with sinks regarded as negative sources) gives the net flux out of the region.\n",
        "\n",
        "Beispiel: elektrostatische Verbindung (von Ionenbindung): https://de.m.wikipedia.org/wiki/Elektrostatik\n",
        "\n",
        "Stokes and Greens theorem on manifolds (derivative and exterior derivative): https://youtu.be/1lGM5DEdMaw\n",
        "\n",
        "[Integralsatz](https://de.wikipedia.org/wiki/Integralsatz) ist ein Namensbestandteil bestimmter mathematischer Sätze, in deren Aussage ein Integral vorkommt.\n",
        "\n",
        "Unter dem Begriff der klassischen Integralsätze werden der **Satz von Gauß, der Satz von Green, der Satz von Stokes** und einige ihrer Spezialfälle zusammengefasst. Diese Sätze der Vektoranalysis hängen eng miteinander zusammen: der Integralsatz von Stokes umfasst die anderen beiden Sätze als Spezialfälle.\n",
        "\n",
        "Außerdem gibt es neben den klassischen Integralsätzen noch weitere Sätze, die man kurz als Integralsätze bezeichnet. Zu diesen zählt beispielsweise der cauchysche Integralsatz, der ein zentrales Resultat aus der Funktionentheorie ist.\n",
        "\n",
        "[Integralsatz von Gauß](https://de.m.wikipedia.org/wiki/Gaußscher_Integralsatz)\n",
        "\n",
        "* Im Folgenden sei das „Integrationsvolumen“ V n-dimensional.\n",
        "\n",
        "* Das [Volumenintegral](https://de.m.wikipedia.org/wiki/Volumenintegral) über den Gradienten einer skalaren Größe $\\phi$, kann dann in ein [Oberflächenintegral](https://de.m.wikipedia.org/wiki/Oberflächenintegral) (bzw. Hyperflächenintegral) über den Rand dieses Volumens umgewandelt werden:\n",
        "\n",
        "> $\n",
        "\\int_{V} \\operatorname{grad} \\phi(\\vec{x}) \\mathrm{d} V=\\oint_{\\partial V} \\phi \\mathrm{d} \\vec{A}\n",
        "$\n",
        "\n",
        "Explanation Video: https://youtu.be/pY4t-ikhzhU\n",
        "\n",
        "Example Calculation: https://youtu.be/E7RUu1K8UDM\n",
        "\n",
        "Another Example calculation: [Divergence Theorem for regions bounded by two surfaces](https://youtu.be/RZE7iB71X7o) (Inside and outside surface, like inner sphere and outer sphere)\n",
        "\n",
        "Another Example calculation: [Deriving Gauss's Law for Electric Flux via the Divergence Theorem from Vector Calculus](https://youtu.be/8PmIarVZmc8) (see here some background: https://de.m.wikipedia.org/wiki/Gaußsches_Gesetz)"
      ],
      "metadata": {
        "id": "JdmqQyXMp5fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1563.png)"
      ],
      "metadata": {
        "id": "JmH0XpY1yU_5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwK1ZsvrPIY"
      },
      "source": [
        "<font color=\"black\">**Exkurs: Volumenintegrale**\n",
        "\n",
        "> [Volume integrals](https://en.m.wikipedia.org/wiki/Volume_integral) are especially important in physics for many applications, for example, to calculate flux densities.\n",
        "\n",
        "* z.B. zur Berechnung des Volumeninhaltes eines Objektes\n",
        "\n",
        "* Das [Volumenintegral](https://de.wikipedia.org/wiki/Volumenintegral) erweitert das Oberflächenintegral auf die Integration über ein beliebiges dreidimensionales Integrationsgebiet, wobei eine Funktion dreimal hintereinander integriert wird, jeweils über eine Richtung eines dreidimensionalen Raumes.\n",
        "\n",
        "* **Dabei muss es sich jedoch nicht notwendigerweise um ein Volumen eines geometrischen Körpers handeln**.\n",
        "\n",
        "* Zur vereinfachten Darstellung wird oft nur ein einziges Integralzeichen geschrieben und die Volumenintegration lediglich durch das Volumenelement $\\mathrm {d} V$ angedeutet:\n",
        "\n",
        "> $\\iiint_{V} f(r) d^{3} r=\\int_{V} f(\\vec{x}) \\mathrm{d} V$\n",
        "\n",
        "* wobei die zu integrierende Funktion zumindest von drei Variablen ${\\vec {x}}=(x,y,z)$ für eine (kartesische) Beschreibung im dreidimensionalen Raum $\\mathbb{R^3}$ abhängt, **es sind aber auch höherdimensionale Räume möglich**.\n",
        "\n",
        "* Es handelt sich um ein **skalares Volumenintegral**, wenn der Integrand $f$ und das Volumenelement $\\mathrm{d} V$ skalar sind. Bei einem **vektoriellen Integranden**, z. B. einem Vektorfeld $\\vec{f}$, ist auch das Volumenelement $\\mathrm{d} \\vec{V}$ ein Vektor, sodass sich ein vektorielles Volumenintegral ergibt.\n",
        "\n",
        "* Um ein Volumenintegral zu berechnen, ist meist eine Parametrisierung des Integrationsgebiets nötig.\n",
        "\n",
        "**Volumenform**\n",
        "\n",
        "* Eine [Volumenform](https://de.m.wikipedia.org/wiki/Volumenform) ist ein mathematisches Objekt, welches zur Integration über Raumbereiche benötigt wird, insbesondere bei der Verwendung spezieller Koordinatensysteme, also ein Spezialfall eines Volumens.\n",
        "\n",
        "* In der Physik und im Ingenieurwesen sind auch Bezeichnungen wie infinitesimales Volumenelement oder Maßfaktor gebräuchlich.\n",
        "\n",
        "* Beispiele in 3 Dimensionen:\n",
        "\n",
        "  * [Kartesische Koordinaten](https://de.m.wikipedia.org/wiki/Kartesisches_Koordinatensystem): $\\mathrm{d} V=\\mathrm{d} x \\cdot \\mathrm{d} y \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Zylinderkoordinaten](https://de.m.wikipedia.org/wiki/Polarkoordinaten#Zylinderkoordinaten) (ebene Polarkoordinaten um eine dritte Koordinate ergänzt): $\\mathrm{d} V=\\rho \\cdot \\mathrm{d} \\rho \\cdot \\mathrm{d} \\varphi \\cdot \\mathrm{d} z$\n",
        "\n",
        "  * [Kugelkoordinaten](https://de.m.wikipedia.org/wiki/Kugelkoordinaten): $\\mathrm{d} V=r^{2} \\cdot \\sin \\theta \\cdot \\mathrm{d} r \\cdot \\mathrm{d} \\theta \\cdot \\mathrm{d} \\varphi$\n",
        "\n",
        "* Das Volumenelement in drei Dimensionen lässt sich nach dem [Transformationssatz](https://de.m.wikipedia.org/wiki/Transformationssatz) mit Hilfe der [Funktionaldeterminante](https://de.m.wikipedia.org/wiki/Funktionaldeterminante) det $J$ berechnen. Die [Jacobi-Matrix](https://de.m.wikipedia.org/wiki/Jacobi-Matrix) für die Transformation von den Koordinaten $\\left\\{x_{1}, x_{2}, x_{3}\\right\\}$ zu $\\left\\{x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right\\}$ ist hierbei definiert durch\n",
        "\n",
        ">$\n",
        "J=\\frac{\\partial\\left(x_{1}, x_{2}, x_{3}\\right)}{\\partial\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, x_{3}^{\\prime}\\right)}\n",
        "$\n",
        "\n",
        "* Das Volumenelement ist dann gegeben durch\n",
        "\n",
        ">$\n",
        "\\mathrm{d} V^{\\prime}=|\\operatorname{det} J| \\mathrm{d} x_{1}^{\\prime} \\mathrm{d} x_{2}^{\\prime} \\mathrm{d} x_{3}^{\\prime}\n",
        "$\n",
        "\n",
        "* Aus mathematischer Sicht ist **eine Volumenform auf einer $n$-dimensionalen Mannigfaltigkeit eine nirgends verschwindende Differentialform vom Grad $n$**. Im Fall einer orientierten riemannschen Mannigfaltigkeit ergibt sich eine kanonische Volumenform aus der verwendeten Metrik, die den Wert 1 auf einer positiv orientierten Orthonormalbasis annimmt. Diese wird [Riemann'sche Volumenform](https://de.m.wikipedia.org/wiki/Hodge-Stern-Operator#Riemannsche_Volumenform) genannt (Hodge-Stern-Operator in der Differentialgeometrie)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">**Analysis 100**"
      ],
      "metadata": {
        "id": "b8EVGNJmEfDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Number Theory*"
      ],
      "metadata": {
        "id": "zXtTBgYGRpLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Types of Numbers*"
      ],
      "metadata": {
        "id": "PsVksRCQ8GJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperoperation - Notations for Large Numbers**\n",
        "\n",
        "* Siehe: [Hyperoperation](https://en.m.wikipedia.org/wiki/Hyperoperation) und [Knuth‘s Up Arrow Notation](https://en.m.wikipedia.org/wiki/Knuth%27s_up-arrow_notation)\n",
        "\n",
        "* [Liste besonderer Zahlen](https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen)\n",
        "\n",
        "* Video: [Numbers too big to imagine](https://youtu.be/u1x_FJZX6Vw?si=4bqa_0_nbdd4DquX)\n",
        "\n",
        "* Video: [The rare levels beyond exponents](https://youtu.be/eVRJLD0HJcE?si=Vnp9XQN6FfquUqF7)\n",
        "\n",
        "* Video: [Beyond Exponentiation: A tetration investigation](https://youtu.be/qdqPTEpq5Xw?si=TmTWE0lrK91MCXXS)\n",
        "\n",
        "* Video: [The incomprehensible scale of 52!](https://youtu.be/hoeIllSxpEU?si=dAosbKqFhITVro7D)\n",
        "\n",
        "* Video: [Climbing past complex numbers](https://youtu.be/q3Tbf-d9sE4?si=v6rMmh0GUCGFPezs)"
      ],
      "metadata": {
        "id": "OZMb3wu6YrQD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV73El80RpLz"
      },
      "source": [
        "**Number Theory (Zahlenarten)**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebraische_Zahlentheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Diophantische_Gleichung: Polynomfunktion mit ganzzahligen Koeffizienten ist und nur ganzzahlige Lösungen gesucht werden\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Zahldarstellung\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Analytic_number_theory\n",
        "\n",
        "* https://mathworld.wolfram.com/topics/NumberTheory.html\n",
        "\n",
        "> [Number theory Full Course [A to Z]](https://www.youtube.com/watch?v=19SW3P_PRHQ)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/number_001.jpg)\n",
        "\n",
        "\n",
        "Every rational number is algebraic, and some irrational numbers are too. https://lnkd.in/eG_NgkCs\n",
        "\n",
        "\n",
        "[Large_numbers](https://en.m.wikipedia.org/wiki/Large_numbers), [Names of large numbers](https://en.m.wikipedia.org/wiki/Names_of_large_numbers), [Liste besonderer Zahlen](https://de.m.wikipedia.org/wiki/Liste_besonderer_Zahlen)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1631.png)\n",
        "\n",
        "* [Constructible numbers](https://en.m.wikipedia.org/wiki/Constructible_number)\n",
        "* [Algebraic numbers](https://en.m.wikipedia.org/wiki/Algebraic_number)\n",
        "* [Transcendental numbers](https://en.m.wikipedia.org/wiki/Transcendental_number)\n",
        "* [Computable numbers](https://en.m.wikipedia.org/wiki/Computable_number)\n",
        "* **Non-Computable numbers**\n",
        "  * [Chaitin constant](https://en.m.wikipedia.org/wiki/Chaitin%27s_constant)\n",
        "  * [Die meisten reellen Zahlen kennen wir nicht](https://www.spektrum.de/kolumne/die-meisten-reellen-zahlen-sind-nicht-berechenbar/2133762) (gehorchen nicht einmal einer Rechenvorschrift)\n",
        "* Special cross-section: [Normal numbers](https://en.m.wikipedia.org/wiki/Normal_number)\n",
        "  * [Champernowne’s constant](https://en.m.wikipedia.org/wiki/Champernowne_constant) (whole numbers) - normal and transendental\n",
        "  * [Copeland-Erdös-number](https://de.m.wikipedia.org/wiki/Copeland-Erdős-Zahl) (primes)\n",
        "\n",
        "\n",
        "Source: [All the Numbers - Numberphile](https://www.youtube.com/watch?v=5TkIe60y2GI&t=458s)\n",
        "\n",
        "**Empty section: normal and Non-Computable numbers (we have no examples)**: We have no examples. But proofs have shown: this is the greatest amount of numbers: most numbers are normal and most numbers are uncomputable. So this section should be full, but we have no example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero**\n",
        "\n",
        "* [Problems with Zero - Numberphile](https://www.youtube.com/watch?v=BRRolKTlF6Q)\n",
        "\n",
        "* something divided by zero: from negative it approaches negative infinity, from positive it approaches positive infinity, hence error\n",
        "\n",
        "* zero to the power of 0: in the complex plane you get different answers, hence error\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Null"
      ],
      "metadata": {
        "id": "0AMAyO4IRpL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dedekind-Zahl**\n",
        "\n",
        "https://www.scinexx.de/news/technik/mathematik-neunte-dedekind-zahl-geknackt/\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Dedekind-Zahl"
      ],
      "metadata": {
        "id": "zVFzSZg-RpL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carmichael-Zahl**\n",
        "\n",
        "Eine [Carmichael-Zahl](https://de.m.wikipedia.org/wiki/Carmichael-Zahl) ist eine natürliche Zahl mit besonderer Primfaktorzerlegung\n",
        "\n",
        "Eine Carmichael-Zahl ist stets ungerade und enthält mindestens 3 verschiedene Primfaktoren. Die kleinsten Carmichael-Zahlen sind 561, 1105, 1729.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/carmichael-zahl/1414\n",
        "\n",
        "https://www.faz.net/aktuell/wissen/daniel-larsen-findet-einen-mathebeweis-zu-carmichael-zahlen-18583248.html"
      ],
      "metadata": {
        "id": "x8PquzzzRpL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Narcissistic Number**\n",
        "\n",
        "number theory, a narcissistic number is a number that can be expressed as the sum of its own digits raised to the power of the number of digits.\n",
        "\n",
        "> $153 = 1^3 + 5^3 + 3^3$\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Narcissistic_number"
      ],
      "metadata": {
        "id": "eH_-IPmeRpL0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtVSBRsTRpL1"
      },
      "source": [
        "**Hyperreelle Zahlen & Nichtstandardanalysis (Infinitesimalrechnung)**\n",
        "\n",
        "* There are also applications of nonstandard analysis to the theory of stochastic processes, particularly constructions of Brownian motion as random walks.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Nichtstandardanalysis\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Hyperreelle_Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Infinitesimalrechnung\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Surreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Hyperreal_number\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Infinitesimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnOUGB8GRpL1"
      },
      "source": [
        "**Transcendental numbers:**\n",
        "\n",
        "* Zahlen, die Lösung einer algebraischen Gleichung: (Wurzel aus 2) - 2 = 0\n",
        "\n",
        "* Transzendente Zahlen (in der Menge der reellen Zahlen): Zahlen, die nicht Lösung einer algebraischen Gleichung sind: e oder pi\n",
        "\n",
        "> from: https://www.youtube.com/watch?v=P24tmohytXs\n",
        "\n",
        "* pie π or Euler number\n",
        "\n",
        "* Never end after comma: 3.14159265358979323846....\n",
        "\n",
        "* Cannot be displayed as fraction\n",
        "\n",
        "* [Transzedente Zahl](https://de.m.wikipedia.org/wiki/Transzendente_Zahl) heisst eine reelle Zahl (oder allgemeiner eine komplexe Zahl), wenn sie nicht Nullstelle eines (vom Nullpolynom verschiedenen) Polynoms mit ganzzahligen Koeffizienten ist. Andernfalls handelt es sich um eine algebraische Zahl. Jede reelle transzendente Zahl ist überdies irrational.\n",
        "\n",
        "* omnem rationem transcendunt, lat.: Sie sind jenseits aller Vernunft\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplex Numbers**\n",
        "\n",
        "* --> real tessarines\n",
        "\n",
        "* also: [split complex number](https://en.m.wikipedia.org/wiki/Split-complex_number), hyperbolic number, double number)\n",
        "\n",
        "* In algebra, a split complex number (or hyperbolic number, also perplex number, double number) has two real number components x and y, and is written $z = x + y j$, where $j^2$ = 1. The conjugate of z is $z^{∗}$ = x − y j. Since j2 = 1, the product of a number z with its conjugate is $zz^{∗}$ = $x^2 − y^2$, an [isotropic quadratic form](https://en.m.wikipedia.org/wiki/Isotropic_quadratic_form), =N(z) = $x^2 − y^2$.\n",
        "\n",
        "* Very perplexing, right? They are defined as of form a+hb, where a and b are real numbers, h²=1.\n",
        "\n",
        "  * But wait, that sounds too easy! Until you realize that h isn't +1 or -1. It is \"something else\".\n",
        "\n",
        "* Perplex numbers have many applications outside of algebra and geometry, such as in Quantum Mechanics and the Theory of Relativity.\n",
        "\n",
        "* Special Relativity:https://aapt.scitation.org/doi/10.1119/1.14605\n",
        "\n",
        "* Quantum Mechanics: https://www.intlpress.com/site/pub/files/_fulltext/journals/cis/2014/0014/0003/CIS-2014-0014-0003-a001.pdf"
      ],
      "metadata": {
        "id": "4AtGvGkWRpL1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRwD3iCjRpL1"
      },
      "source": [
        "**p-adic numbers**\n",
        "\n",
        "* Size is different in p-adic numbers\n",
        "\n",
        "* they have nothing to do with the real number line: here two numbers are close, when their first several digits are the same\n",
        "\n",
        "* in p-adic absolute value, two numbers are close when their last digits are the same. so two numbers are close in this field, when they agree on\n",
        "\n",
        "https://youtu.be/3gyHKCDq1YA\n",
        "\n",
        "* infinite before comma: ....985356295141.3\n",
        "\n",
        "* [p-adische Zahl](https://de.m.wikipedia.org/wiki/P-adische_Zahl) ist eine Zahl, die sich in einer Potenzreihe zu einer Primzahl darstellen lässt\n",
        "\n",
        "* p-adic number systems emerge from modular arithmetic\n",
        "\n",
        "* https://www.quantamagazine.org/how-the-towering-p-adic-numbers-work-20201019/\n",
        "\n",
        "* https://www.quantamagazine.org/peter-scholze-and-the-future-of-arithmetic-geometry-20160628/\n",
        "\n",
        "* https://www.quantamagazine.org/with-a-new-shape-mathematicians-link-geometry-and-numbers-20210719/\n",
        "\n",
        "* \"Das Dualsystem ist das Stellenwertsystem mit der Basis 2, liefert also die dyadische (2-adische) Darstellung von Zahlen (Dyadik) (gr. δύο = zwei).\" [Source](https://de.m.wikipedia.org/wiki/Dualsystem#Grundrechenarten_im_Dualsystem)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypercomplex Numbers**\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Hyperkomplexe_Zahlen.svg/519px-Hyperkomplexe_Zahlen.svg.png)\n",
        "\n",
        "* [Hypercomplex numbers](https://en.m.wikipedia.org/wiki/Hypercomplex_number)\n",
        "\n",
        "  * [Hyperkomplexe Zahlen](https://de.m.wikipedia.org/wiki/Hyperkomplexe_Zahl) sind Verallgemeinerungen der komplexen Zahlen.\n",
        "\n",
        "  * https://www.quantamagazine.org/the-octonion-math-that-could-underpin-physics-20180720/\n",
        "\n",
        "  * Video: [Climbing past complex numbers](https://youtu.be/q3Tbf-d9sE4?si=v6rMmh0GUCGFPezs)\n",
        "\n",
        "  * [Cayley–Dickson construction](https://en.m.wikipedia.org/wiki/Cayley–Dickson_construction) bzw. [Verdoppelungsverfahren](https://de.m.wikipedia.org/wiki/Verdopplungsverfahren) (Cayley Dickson Algebra). It is possible to continue applying the Cayley–Dickson construction arbitrarily many times.\n",
        "\n",
        "  * https://en.m.wiktionary.org/wiki/nilpotent\n",
        "\n",
        "  * Github [hypercomplex](https://github.com/discretegames/hypercomplex)\n",
        "\n",
        "  * https://math.stackexchange.com/questions/2993448/omegath-iteration-of-cayley-dickson-construction\n",
        "\n",
        "* Dimension 1: Real numbers\n",
        "\n",
        "* Dimension 2: [Complex numbers](https://en.m.wikipedia.org/wiki/Complex_number)\n",
        "\n",
        "  * [Bicomplex number](https://en.m.wikipedia.org/wiki/Bicomplex_number) (tessarines)\n",
        "  \n",
        "  * https://physics.stackexchange.com/questions/155762/complex-numbers-in-quantum-mechanics-and-in-special-relativity\n",
        "\n",
        "  * you can use split-complex numbers in relativity, but ironically complex numbers have proved more popular for this).\n",
        "\n",
        "\n",
        "* Dimension 4: [Quaternions](https://en.m.wikipedia.org/wiki/Quaternion) - representation of rotations of 3-space\n",
        "\n",
        "  * [Split-quaternion](https://en.m.wikipedia.org/wiki/Split-quaternion) (Coquaternions) when the coefficients are complex numbers\n",
        "\n",
        "  * [Biquaternion](https://en.m.wikipedia.org/wiki/Biquaternion) when the coefficients are [split-complex numbers](https://en.m.wikipedia.org/wiki/Split-complex_number)\n",
        "\n",
        "  * [Dual quaternion](https://en.m.wikipedia.org/wiki/Dual_quaternion) when the coefficients are [dual numbers](https://en.m.wikipedia.org/wiki/Dual_number), Video: https://youtu.be/ceaNqdHdqtg\n",
        "\n",
        "  * [Hyperbolic quaternion](https://en.m.wikipedia.org/wiki/Hyperbolic_quaternion)\n",
        "\n",
        "* Dimension 8: [Octonions](https://en.m.wikipedia.org/wiki/Octonion) (Cayleyzahlen)\n",
        "\n",
        "* Dimension 16: [Sedenion (Hexadecanion)](https://en.m.wikipedia.org/wiki/Sedenion)\n",
        "\n",
        "* Dimension 32: [Pathion (Trigintaduonion)](https://en.m.wiktionary.org/wiki/trigintaduonion)\n",
        "  * In the trigintaduonion fields which are associated with the electromagnetic, gravitational, strong and weak interaction\n",
        "  * https://archive.org/details/arxiv-0704.0136\n",
        "  * https://nitinuchil.wordpress.com/2020/09/09/hypercomplex-math/\n",
        "\n",
        "* Dimension 64: Chingons (sexagintaquattuornions)\n",
        "\n",
        "* Dimension 128: Routons (centumduodetrigintanions)\n",
        "\n",
        "* Dimension 128: Voudons (ducentiquinquagintasexions)\n",
        "\n",
        "*Quaternions (4D), octonions (8D), sedenions (16D), pathions (32D), chingons (64D), routons (128D), and voudons (256D).  These names were coined by Robert P.C. de Marrais and Tony Smith.  It is an alternate naming system providing relief from the difficult Latin names, such as: trigintaduonions (32D), sexagintaquattuornions (64D), centumduodetrigintanions (128D), and ducentiquinquagintasexions (256D), [Source](https://nitinuchil.wordpress.com/2020/09/09/hypercomplex-math/)*\n",
        "\n",
        "*Addon: Bicomplex number*\n",
        "\n",
        "* --> \"Tessarine\" redirects here. For real tessarines, see Split-complex number.\n",
        "\n",
        "* In abstract algebra, a bicomplex number is a pair (w, z) of complex numbers constructed by the Cayley–Dickson process that defines the bicomplex conjugat ${\\displaystyle (w,z)^{*}=(w,-z)}$, and the product of two bicomplex numbers as\n",
        "\n",
        "> {\\displaystyle (u,v)(w,z)=(uw-vz,uz+vw).}\n",
        "\n",
        "* Then the bicomplex norm is given by\n",
        "\n",
        "> ${\\displaystyle (w,z)^{*}(w,z)=(w,-z)(w,z)=(w^{2}+z^{2},0),}$\n",
        "\n",
        "* a quadratic form in the first component.\n",
        "\n",
        "* https://hsm.stackexchange.com/questions/12866/why-are-quaternions-more-popular-than-tessarines-despite-being-non-commutative\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1200.png)\n",
        "\n",
        "Images [source](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.527.356&rep=rep1&type=pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "16DHxo5fwwFq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIPeyc9RpL3"
      },
      "source": [
        "**Primzahlen**\n",
        "\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Skewes-Zahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahl\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Primzahlsatz\n",
        "\n",
        "* **Belphegors Primzahl**\n",
        "\n",
        "  * 1000000000000066600000000000001\n",
        "  * It's 666 with 13 zero's on either side & 1's on both ends.\n",
        "  * https://de.wikipedia.org/wiki/Belphegors_Primzahl\n",
        "\n",
        "*Riemannsche Vermutung (Primzahlverteilung & Zeta-Funktion)*\n",
        "\n",
        "* Die Verteilung der Primzahlen ist sehr merkwürdig und damit interessant. So zeigt die Verteilung von Primzahlen in (relativ) kurzen Intervallen eine gewisse „Zufälligkeit“, während andererseits beliebig lange Intervalle existieren, die keine Primzahl enthalten.\n",
        "\n",
        "* Bernhard Riemann setzte sich in seiner Arbeit „Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse“ (1859) zum Ziel, die Verteilung der Primzahlen mit analytischen Methoden zu bestimmen, stieß dabei auf Riemannsche ζ-Funktion und formulierte die Riemannsche Vermutung. Basierend auf den Riemannschen Ideen gelang 1896 der Beweis des Primzahlsatzes, mit dem man für große Zahlen x mit immer größerer relativer Genauigkeit sagen kann, wieviele Primzahlen ≤ x es gibt.\n",
        "\n",
        "* Will man diese Anzahlen noch genauer wissen, so kommt man schnell in einen Bereich mathematischer Fragestellungen mit zahlreichen offenen Problemen, z. B. den Goldbach-Problemen oder Fragen über Primzahlzwillinge.\n",
        "\n",
        "https://www.spektrum.de/lexikon/mathematik/primzahlverteilung/8085\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Vermutung\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Riemannsche_Zetafunktion\n",
        "\n",
        "A team of computer scientists have published an innovative polynomial time algorithm for constructing with high probability a prime number of a given size. Their algorithm could be useful in public-key cryptography.\n",
        "\n",
        "How to Build a Big Prime Number, by Stephen Ornes, Quanta magazine, July 13, 2023, https://www.quantamagazine.org/how-to-build-a-big-prime-number-20230713/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Modular Form, Periods  L-Functions and Ellyptic Curves*"
      ],
      "metadata": {
        "id": "Gekbi4BMkqsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.linkedin.com/posts/quanta-magazine_functions-called-modular-forms-are-useful-activity-7160333259535511552-8Yga\n",
        "\n",
        "https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/"
      ],
      "metadata": {
        "id": "4zhAoQ0s7X3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behold Modular Forms, the ‘Fifth Fundamental Operation’ of Math\n",
        "\n",
        "Modular forms are one of the most beautiful and mysterious objects in mathematics. What are they?\n",
        "\n",
        "https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/"
      ],
      "metadata": {
        "id": "IoGRMfS_Hoy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/long-sought-math-proof-unlocks-more-mysterious-modular-forms-20230309/"
      ],
      "metadata": {
        "id": "50syWbcMKRT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.quantamagazine.org/echoes-of-electromagnetism-found-in-number-theory-20231012/"
      ],
      "metadata": {
        "id": "FWjvUdbGk02x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Periods** in mathematics are numbers that arise from the geometric or arithmetic properties of various objects, such as algebraic varieties, elliptic curves, and motives. They can be defined in a number of ways, but one common approach is to consider the integrals of differential forms over these objects. For example, the period of an elliptic curve is the integral of a differential form over the curve that is invariant under the action of the group of automorphisms of the curve.\n",
        "\n",
        "**L-functions** are a special type of function that is defined on a complex variable. They are associated with a variety of mathematical objects, including Dirichlet characters, elliptic curves, and automorphic forms. L-functions can be defined in a number of ways, but one common approach is to start with a Dirichlet series of the form\n",
        "\n",
        "$$\\sum_{n=1}^\\infty a_n n^{-s},$$\n",
        "\n",
        "where the coefficients $a_n$ are complex numbers. If the series converges for a sufficiently wide range of values of $s,$ then it can be extended to a meromorphic function on the complex plane. This function is called an L-function.\n",
        "\n",
        "L-functions play an important role in many areas of mathematics, including number theory, algebraic geometry, and analysis. They are used to study a variety of problems, such as the distribution of prime numbers, the behavior of elliptic curves, and the representation theory of groups.\n",
        "\n",
        "There is a deep connection between periods and L-functions. In many cases, the special values of an L-function at integer arguments can be expressed as periods. This connection is known as the period conjecture, and it is one of the most important open problems in mathematics.\n",
        "\n",
        "Here is an example of a period: the period of the unit circle. This period is equal to the integral of the differential form $dz$ over the unit circle. This integral can be evaluated using complex analysis, and the result is $2\\pi i.$\n",
        "\n",
        "Here is an example of an L-function: the Riemann zeta function. The Riemann zeta function is defined by the Dirichlet series\n",
        "\n",
        "$$\\zeta(s) = \\sum_{n=1}^\\infty n^{-s}.$$\n",
        "\n",
        "This series converges for all real numbers $s > 1,$ and it can be extended to a meromorphic function on the entire complex plane with a single pole at $s = 1.$ The Riemann zeta function is one of the most important L-functions, and it has been studied by mathematicians for centuries.\n",
        "\n",
        "The connection between periods and L-functions is a complex and fascinating topic. It is one of the most active areas of research in mathematics today."
      ],
      "metadata": {
        "id": "yEDOUymZku6M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH5j2eXJRpL5"
      },
      "source": [
        "**Modular Forms (Algebraic Number Theory)**\n",
        "\n",
        "* used in stringtheorie\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Modulform\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Ramanujan-Thetafunktion\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Thetafunktion\n",
        "\n",
        "* https://www.quantamagazine.org/moonshine-link-discovered-for-pariah-symmetries-20170922/\n",
        "\n",
        "* https://www.quantamagazine.org/universal-math-solutions-in-dimensions-8-and-24-20190513/\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Kaluza-Klein-Theorie\n",
        "\n",
        "[Why is there no quintic formula](https://youtu.be/1EWUsef0iFs)\n",
        "\n",
        "[Counting points on the E8 lattice with modular forms (theta function)](https://youtu.be/AxPwhJTHxSg)\n",
        "\n",
        "[The bridge between number theory and complex analysis](https://youtu.be/zLEyIT_BCgk)\n",
        "\n",
        "[Ellyptic curves and modular forms (the proof of Fermats last theorem](https://youtu.be/grzFM5XciAY)\n",
        "\n",
        "[The math behind Fermats last theorem (modular forms)](https://youtu.be/z7A_bSl8kIw)\n",
        "\n",
        "[Algebraic number theory - an illustraded guide (is 5 a prime?)](https://youtu.be/4m_EaWA08H0)\n",
        "\n",
        "[Why is Pi here? (half factorial without gamma function)](https://youtu.be/y-yb_EmVtg0)\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Eisenstein_series\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Modular_form\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Modular_group\n",
        "\n",
        "Video https://youtu.be/z7A_bSl8kIw\n",
        "\n",
        "Video https://youtu.be/grzFM5XciAY\n",
        "\n",
        "Video https://youtu.be/zLEyIT_BCgk\n",
        "\n",
        "**Can you explain what the modularity condition is in modular forms?**\n",
        "\n",
        "Sure. The modularity condition is a condition that a function must satisfy in order to be considered a modular form. It is a technical condition, but it can be roughly summarized as follows:\n",
        "\n",
        "* The function must be holomorphic (complex analytic) on the upper half-plane.\n",
        "* The function must be invariant under the action of the modular group SL(2,Z).\n",
        "* The function must have a Fourier series expansion that only has non-negative powers of q.\n",
        "\n",
        "The modularity condition is important because it ensures that modular forms have certain properties that make them useful in mathematics. For example, modular forms are known to be related to elliptic curves, which are important objects in number theory.\n",
        "\n",
        "The modularity condition is also interesting from a mathematical perspective. It is a difficult condition to satisfy, and there are only a finite number of modular forms of a given weight and level. This makes modular forms a rich and well-studied class of functions.\n",
        "\n",
        "Here is a more detailed explanation of the modularity condition:\n",
        "\n",
        "* **Holomorphic on the upper half-plane:** This means that the function must be smooth and have no singularities in the upper half-plane, which is the set of complex numbers z such that Im(z) > 0.\n",
        "* **Invariant under the action of SL(2,Z):** This means that the function must be the same under the action of any element of the modular group SL(2,Z). The modular group is the group of 2x2 matrices with integer entries and determinant 1.\n",
        "* **Fourier series expansion with only non-negative powers of q:** This means that the function can be written as a Fourier series with only non-negative powers of q, where q is a complex number such that |q| < 1.\n",
        "\n",
        "The modularity condition is a powerful tool in mathematics. It has been used to prove many important results in number theory, and it continues to be an active area of research.\n",
        "\n",
        "**Are modular forms used in quantum computing?**\n",
        "\n",
        "Yes, modular forms are used in quantum computing. They have been used in a variety of applications, including:\n",
        "\n",
        "* **Quantum error correction:** Modular forms can be used to construct quantum error-correcting codes. These codes are used to protect quantum information from errors that can occur during quantum computation.\n",
        "* **Quantum cryptography:** Modular forms can be used to construct quantum cryptographic protocols. These protocols are used to securely transmit information over a quantum channel.\n",
        "* **Quantum algorithms:** Modular forms have been used to develop quantum algorithms for problems such as integer factorization and the discrete logarithm problem.\n",
        "\n",
        "The use of modular forms in quantum computing is still a relatively new area of research. However, there is a growing body of work that suggests that modular forms have the potential to be a powerful tool for quantum computation.\n",
        "\n",
        "Here are some specific examples of how modular forms have been used in quantum computing:\n",
        "\n",
        "* In 2005, Vadim Lyubashevsky showed that modular forms could be used to construct quantum error-correcting codes that are more efficient than previous codes.\n",
        "* In 2006, Peter Shor showed that modular forms could be used to construct quantum algorithms for integer factorization that are significantly faster than classical algorithms.\n",
        "* In 2010, Craig Gidney showed that modular forms could be used to construct quantum algorithms for the discrete logarithm problem that are significantly faster than classical algorithms.\n",
        "\n",
        "These are just a few examples of how modular forms have been used in quantum computing. As research in this area continues, it is likely that we will see even more applications of modular forms in quantum computing in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discrete Logarithm Problem (DLP)** (and integer factorization)\n",
        "\n",
        "The discrete logarithm problem (DLP) is a computational problem in mathematics and computer science. It is the problem of finding the exponent k in the equation g^k = a (mod n), where g and n are known integers, and a is an unknown integer.\n",
        "\n",
        "The DLP is a difficult problem to solve in general. There is no known polynomial-time algorithm for solving the DLP, and it is believed that such an algorithm does not exist. This makes the DLP a useful problem in cryptography, as it can be used to construct secure cryptographic systems.\n",
        "\n",
        "For example, the Diffie-Hellman key exchange protocol uses the DLP to securely exchange keys over an insecure channel. The Elliptic Curve Digital Signature Algorithm (ECDSA) also uses the DLP to sign digital documents.\n",
        "\n",
        "The DLP is a very important problem in cryptography, and it is likely to remain so for many years to come.\n",
        "\n",
        "Here are some examples of how the DLP is used in cryptography:\n",
        "\n",
        "* **Diffie-Hellman key exchange:** The Diffie-Hellman key exchange protocol is a method for two parties to securely exchange a secret key over an insecure channel. The protocol works by using the DLP to compute the shared secret key.\n",
        "* **Elliptic Curve Digital Signature Algorithm (ECDSA):** The ECDSA is a digital signature algorithm that uses the DLP to sign digital documents. The ECDSA is a popular choice for digital signatures because it is relatively efficient and secure.\n",
        "\n",
        "The DLP is a difficult problem to solve, but it is not impossible. There are some algorithms that can be used to solve the DLP, but they are not very efficient. If a polynomial-time algorithm for solving the DLP were to be found, it would have a major impact on cryptography.\n",
        "\n",
        "*difference between the work of Peter Shor and Craig Gidney*\n",
        "\n",
        "Peter Shor's algorithm is a quantum algorithm for integer factorization. It was developed in 1994, and it is one of the most important results in quantum computing. Shor's algorithm shows that it is possible to factor large integers in polynomial time using a quantum computer. This has major implications for cryptography, as many of the most widely used cryptographic systems are based on the difficulty of factoring large integers.\n",
        "\n",
        "Craig Gidney's work on the discrete logarithm problem is complementary to Shor's algorithm. The discrete logarithm problem is closely related to integer factorization, and Gidney's work shows that it is also possible to solve the discrete logarithm problem in polynomial time using a quantum computer. This has similar implications for cryptography, as many of the most widely used cryptographic systems are also based on the difficulty of solving the discrete logarithm problem.\n",
        "\n",
        "The main difference between Shor's algorithm and Gidney's work is that Shor's algorithm is specifically designed for integer factorization, while Gidney's work is more general and can be used to solve the discrete logarithm problem as well. Additionally, Shor's algorithm is a probabilistic algorithm, while Gidney's work is a deterministic algorithm. This means that Shor's algorithm may not always be able to factor an integer, while Gidney's work is guaranteed to be able to solve the discrete logarithm problem if the correct input is given.\n",
        "\n",
        "In summary, the work of Peter Shor and Craig Gidney are both important results in quantum computing. Shor's algorithm shows that it is possible to factor large integers in polynomial time using a quantum computer, while Gidney's work shows that it is also possible to solve the discrete logarithm problem in polynomial time using a quantum computer. These results have major implications for cryptography, as many of the most widely used cryptographic systems are based on the difficulty of factoring large integers or solving the discrete logarithm problem."
      ],
      "metadata": {
        "id": "Vim2vUK9RpL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Continouity*"
      ],
      "metadata": {
        "id": "q8twCzNYmL4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://de.m.wikipedia.org/wiki/Stetige_Funktion\n",
        "\n",
        "[Hölderstetigkeit](https://de.m.wikipedia.org/wiki/H%C3%B6lderstetigkeit)\n",
        "\n",
        "[Lipschitzstetigkeit](https://de.m.wikipedia.org/wiki/Lipschitzstetigkeit)"
      ],
      "metadata": {
        "id": "b-91jO0xmQuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means: \"M1-Lipschitz continuous w.r.t. the supremum norm\"?**\n",
        "\n",
        "A function f is M1-Lipschitz continuous with respect to the supremum norm if there exists a constant M such that for all x and y in the domain of f, we have:\n",
        "\n",
        "```\n",
        "||f(x) - f(y)||_∞ ≤ M||x - y||_∞\n",
        "```\n",
        "\n",
        "where ||x||_∞ denotes the supremum norm of x, which is the largest absolute value of any of its components.\n",
        "\n",
        "In other words, the change in the value of f is bounded by a constant multiple of the change in the input. This means that f is \"smooth\" in the sense that small changes in the input do not lead to large changes in the output.\n",
        "\n",
        "M1-Lipschitz continuity is a stronger condition than Lipschitz continuity, which only requires that there exists a constant M such that for all x and y in the domain of f, we have:\n",
        "\n",
        "```\n",
        "||f(x) - f(y)|| ≤ M||x - y||\n",
        "```\n",
        "\n",
        "where ||x|| denotes any norm on the domain of f.\n",
        "\n",
        "M1-Lipschitz continuity is often used in the analysis of numerical algorithms, as it ensures that the errors in the algorithm do not grow too quickly.\n",
        "\n",
        "Here are some examples of M1-Lipschitz continuous functions:\n",
        "\n",
        "* The linear function f(x) = mx + b is M1-Lipschitz continuous with the constant M = |m|.\n",
        "* The absolute value function f(x) = |x| is M1-Lipschitz continuous with the constant M = 1.\n",
        "* The exponential function f(x) = ex is M1-Lipschitz continuous with the constant M = e.\n",
        "\n",
        "Here are some examples of functions that are not M1-Lipschitz continuous:\n",
        "\n",
        "* The square root function f(x) = √x is not M1-Lipschitz continuous, as it grows faster than any linear function.\n",
        "* The logarithmic function f(x) = log(x) is not M1-Lipschitz continuous, as it grows slower than any linear function.\n",
        "* The trigonometric functions sin(x) and cos(x) are not M1-Lipschitz continuous, as they can oscillate wildly."
      ],
      "metadata": {
        "id": "GdcT6GHsme5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What means: \"a loss function that is α-Hölder continuous with constant M2 in the first argument w.r.t. the total variation distance for some α ∈ (0,1]\"?**\n",
        "\n",
        "A loss function that is α-Hölder continuous with constant M2 in the first argument with respect to the total variation distance for some α ∈ (0,1] means that the loss function does not change too much when the first argument is changed slightly, and the amount of change is bounded by a constant multiple of the amount of change in the first argument.\n",
        "\n",
        "The total variation distance is a measure of the distance between two probability distributions. It is defined as the sum of the absolute values of the differences in the cumulative probabilities of all events.\n",
        "\n",
        "α-Hölder continuity means that the maximum change in the loss function is bounded by a constant multiple of the αth power of the change in the first argument. This means that the loss function is more sensitive to small changes in the first argument when α is closer to 1, and less sensitive when α is closer to 0.\n",
        "\n",
        "In other words, a loss function that is α-Hölder continuous with constant M2 in the first argument with respect to the total variation distance is a loss function that does not change too much when the first argument is changed slightly, and the amount of change is bounded by a constant multiple of the amount of change in the first argument, even if the two probability distributions are very different.\n",
        "\n",
        "Here is an example of such a loss function:\n",
        "\n",
        "```\n",
        "loss(p, q) = ||p - q||_1\n",
        "```\n",
        "\n",
        "where ||p - q||_1 is the total variation distance between the probability distributions p and q. This loss function is 1-Hölder continuous with constant M2 = 2, since the total variation distance is a norm.\n",
        "\n",
        "Another example is the Kullback-Leibler divergence:\n",
        "\n",
        "```\n",
        "loss(p, q) = KL(p || q)\n",
        "```\n",
        "\n",
        "where KL(p || q) is the Kullback-Leibler divergence between the probability distributions p and q. This loss function is also 1-Hölder continuous with constant M2 = 1.\n",
        "\n",
        "α-Hölder continuity is a useful property for loss functions to have, as it makes them more robust to noise and outliers in the data."
      ],
      "metadata": {
        "id": "7tQOwRR7mihN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KRiQv6BoMmA"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionalanalysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operator**\n",
        "\n",
        "* Operator: a function that has a function as input and the output is another functiom.\n",
        "* This is an operator, because the squaring function is mapped to the doubling function under differentiation.\n",
        "* When we see a function like this, it means that the function y(x) when fed into the operator L becomes f(x). By solving a differential equation we mean recovering y(x) from f(x).\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1333.jpg)\n"
      ],
      "metadata": {
        "id": "ppApZ0cS7_F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Green's Function**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Greensche_Funktion\n",
        "\n",
        "Video: [Green's functions: the genius way to solve DEs](https://youtu.be/ism2SfZgFJg)\n",
        "\n",
        "Video: [Green's Functions](https://youtu.be/-riPW1yt_fA)\n",
        "\n",
        "Video: [Introducing Green's Functions for Partial Differential Equations (PDEs)](https://youtu.be/xNqLZnM-PPY)\n",
        "\n",
        "Video: [Green's functions, Delta functions and distribution theory](https://youtu.be/AqfYSNsrnhI)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1334.jpg)\n"
      ],
      "metadata": {
        "id": "qvEaRxI18Q6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4I7IyMVeJte"
      },
      "source": [
        "**Schwache Ableitung & schwache Lösung**\n",
        "\n",
        "* Eine [schwache Ableitung](https://de.wikipedia.org/wiki/Schwache_Ableitung) bzw. [weak derivative](https://en.wikipedia.org/wiki/Weak_derivative) ist in der Funktionalanalysis, einem Teilgebiet der Mathematik, eine Erweiterung des Begriffs der gewöhnlichen (klassischen) Ableitung.\n",
        "\n",
        "* Er ermöglicht es, Funktionen eine Ableitung zuzuordnen, die nicht (stark bzw. im klassischen Sinne) differenzierbar sind.\n",
        "\n",
        "* Schwache Ableitungen spielen eine große Rolle in der Theorie der partiellen Differentialgleichungen. **Räume schwach differenzierbarer Funktionen sind die Sobolev-Räume**.\n",
        "\n",
        "* Ein noch allgemeinerer Begriff der Ableitung ist die Distributionenableitung.\n",
        "\n",
        "* In mathematics, a weak derivative is a generalization of the concept of the derivative of a function (strong derivative) **for functions not assumed differentiable, but only integrable**, i.e., to lie in the $L^{p}$ space $L^{1}([a, b])$. See distributions for a more general definition.\n",
        "\n",
        "**This concept gives rise to the definition of [weak solutions](https://en.wikipedia.org/wiki/Weak_solution) in Sobolev spaces, which are useful for problems of differential equations and in functional analysis.**\n",
        "\n",
        "The **absolute value function** u : [−1, 1] → [0, 1], u(t) = |t|, which is not differentiable at t = 0, has a weak derivative v known as the [**sign function**](https://de.wikipedia.org/wiki/Vorzeichenfunktion) given by\n",
        "\n",
        "![gg](https://mathepedia.de/img/Abs_x.png)\n",
        "\n",
        "*Signumfunktion als schwache Ableitung der Betragsfunktions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbR-3EnGQ33f"
      },
      "source": [
        "**Anfangswertprobleme (Cauchy-Problem)**\n",
        "\n",
        "* Beispielsweise werden alle schwingenden Pendel durch eine Differentialgleichung beschrieben (siehe: Pendelgleichung), und der generelle Bewegungsablauf folgt immer dem gleichen Prinzip. Der konkrete Bewegungsablauf ist jedoch durch die Rand- oder Anfangsbedingung(en) (wann wurde das Pendel angestoßen, und wie weit) bestimmt. Die Lösbarkeit von Anfangswertproblemen bei gewöhnlichen Differentialgleichungen 1. Ordnung wird durch den Satz von Picard-Lindelöf beschrieben. https://mathepedia.de/Gewoehnliche_Differentialgleichungen.html\n",
        "\n",
        "* Wer zu einer Differentialgleichung eine [**Anfangsbedingung**](https://de.wikipedia.org/wiki/Anfangsbedingung) hinzufügt, stellt damit ein **Anfangswertproblem**. Eine besonders spannende Frage lautet dabei, wie eine Anfangsbedingung zu einer gegebenen Differentialgleichung beschaffen sein muss, damit das entstehende Anfangswertproblem **genau eine eindeutig bestimmte Lösung zulässt**.\n",
        "\n",
        "Der freie Fall (etwa eines Apfels vom Baum) wird beschrieben durch die Bewegungsgleichung\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "y^{\\prime \\prime}(t) &=-g \\\\\n",
        "\\Rightarrow y^{\\prime}(t) &=-g \\cdot t+v_{0}\n",
        "\\end{aligned}$\n",
        "\n",
        "mit der Konstanten $g \\approx 9,81 \\mathrm{~m} / \\mathrm{s}^{2}$ (Erdbeschleunigung).\n",
        "Die Lösungsmenge dieser Differentialgleichung besteht zunächst aus allen Funktionen der Form\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=-\\frac{1}{2} g t^{2}+v_{0} \\cdot t+y_{0}\n",
        "$\n",
        "\n",
        "mit beliebigen Integrationskonstanten $y_{0}$ und $v_{0}$.\n",
        "Eine mögliche Anfangsbedingung sagt z. B. aus, dass der Apfel zu Beginn der Bewegung an einem\n",
        "Ast in drei Metern Höhe hängt:\n",
        "\n",
        ">$\n",
        "y(0)=y_{0}=3 \\mathrm{~m}\n",
        "$\n",
        "\n",
        "und sich in Ruhe befindet:\n",
        "\n",
        ">$\n",
        "y^{\\prime}(0)=v_{0}=0 \\mathrm{~m} / \\mathrm{s}\n",
        "$\n",
        "\n",
        "Diese Anfangsbedingung zeichnet nun in der Lösungsmenge der Differentialgleichung die eine Funktion\n",
        "\n",
        ">$\n",
        "\\Rightarrow y(t)=3 \\mathrm{~m}-\\frac{1}{2} g t^{2}\n",
        "$\n",
        "\n",
        "als die eindeutig bestimmte Lösung des Anfangswertproblems aus (Loesung dann zB uber das [Runge-Kutta-Verfahren](https://de.wikipedia.org/wiki/Runge-Kutta-Verfahren) - Einschrittverfahren zur näherungsweisen Lösung von Anfangswertproblemen in der numerischen Mathematik)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfUxHFgakA7f"
      },
      "source": [
        "**Randwertproblem (Boundary value)**\n",
        "\n",
        "* Bei partiellen Differentialgleichungen, wenn also die gesuchte Funktion nicht nur von einer, sondern von mehreren Variablen abhängt, werden oftmals [Randbedingungen](https://de.wikipedia.org/wiki/Randbedingung) an Stelle von Anfangsbedingungen verwendet. Manchmal wird dann der Spezialfall einer Randbedingung, deren Definitionsbereich eine Hyperebene im vollen Definitionsbereich der Differentialgleichung bildet, Anfangsbedingung genannt.\n",
        "\n",
        "* In der Betriebswirtschaftslehre und der Volkswirtschaftslehre entsprechen die Randbedingungen den kurzfristig oder gar nicht durch den Entscheidungsträger beeinflussbaren Datenparametern wie beispielsweise die Umweltzustände der Witterung oder der Gesetze.\n",
        "\n",
        "Sei die gegebene Differentialgleichung $y^{\\prime \\prime}(x)=-y(x)$. Die Lösungsmenge dieser Gleichung ist $a \\sin (x)+b \\cos (x)$.\n",
        "\n",
        "* Gesucht ist die Lösung mit $y(0)=1$ und $y(\\pi / 2)=0 \\Rightarrow$ Die Lösung ist $y=\\cos (x)$.\n",
        "\n",
        "* Periodische Randbedingung: Gesuchtłist die Lösung mit $y(0)=0$ und $y(\\pi)=0 \\Rightarrow$ Es gibt unendlich viele Lösungen der Form $a \\sin (x)$ mit beliebigem $a$.\n",
        "\n",
        "* Gesucht ist die Lösung mit $y(0)=0$ und $y(2 \\pi)=1 \\Rightarrow$ Es gibt keine Lösung.\n",
        "\n",
        "Arten von Randbedingungen (Es gibt unterschiedliche Möglichkeiten, auf dem Rand des betrachteten Gebietes Werte vorzuschreiben):\n",
        "\n",
        "* Werte der Lösung vorschreiben; im Fall einer auf dem Intervall $[a, b]$ definierten gewöhnlichen Differentialgleichung schreibt man also $y(a)$ und $y(b)$ vor und spricht\n",
        "dann von [**Dirichlet-Randbedingungen**](https://de.wikipedia.org/wiki/Dirichlet-Randbedingung).\n",
        "\n",
        "* Bedingungen an die Ableitungen stellen, also $y^{\\prime}(a)$ und $y^{\\prime}(b)$ vorgeben, dann spricht man [**von Neumann-Randbedingungen**](https://de.wikipedia.org/wiki/Neumann-Randbedingung) (bei gewöhnlichen Differentialgleichungen, wie oben ausgeführt, von Anfangsbedingungen).\n",
        "\n",
        "* Ein Spezialfall sind [**periodische Randbedingungen**](https://de.wikipedia.org/wiki/Periodische_Randbedingung), hier muss (im Beispiel einer auf dem Intervall $[a, b]$ betrachteten gewöhnlichen Differentialgleichung) gelten: $y(a)=y(b)$ bzw. $y^{\\prime}(a)=y^{\\prime}(b)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partielle Differentialgleichung**\n",
        "\n",
        "> https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419\n",
        "\n",
        "* **[Partielle Differentialgleichungen](\n",
        "https://de.wikipedia.org/wiki/Partielle_Differentialgleichung) werden in erster Linie durch Trennung der Variablen und spätere Integration gelöst.**\n",
        "\n",
        "* Sobolevräume sind ein grundlegendes Werkzeug bei der Behandlung von PDE's (rein und angewandt).\n",
        "\n",
        "* See also Variational Formulations - wichtig fur partielle Differentialgleichungen\n",
        "\n",
        "* [Euler-Gleichungen (Strömungsmechanik)](https://de.wikipedia.org/wiki/Euler-Gleichungen_(Strömungsmechanik)) bildet dann ein System von **nichtlinearen partiellen** Differentialgleichungen **erster Ordnung** Wird die Viskosität vernachlässigt ($\\eta =\\lambda =0$), so erhält man die Euler-Gleichungen (hier für den kompressiblen Fall)\n",
        "\n",
        "> $\\rho \\frac{\\partial \\vec{v}}{\\partial t}+\\rho(\\vec{v} \\cdot \\nabla) \\vec{v}=-\\nabla p+\\vec{f}$\n",
        "\n",
        "* [**Navier-Stokes-Gleichungen**](https://de.wikipedia.org/wiki/Navier-Stokes-Gleichungen) bilden dann ein System von **nichtlinearen partiellen** Differentialgleichungen **zweiter Ordnung**\n",
        "\n",
        "  * Die Gleichungen sind eine **Erweiterung der Euler-Gleichungen** der Strömungsmechanik **um Viskosität beschreibende Terme** (Die Navier-Stokes-Gleichungen beinhalten die Euler-Gleichungen als den Sonderfall, in dem die innere Reibung (Viskosität) und die Wärmeleitung des Fluids vernachlässigt werden.)\n",
        "\n",
        "  * Die Navier-Stokes-Gleichungen bilden das Verhalten von Wasser, Luft und Ölen ab und werden daher in diskretisierter Form bei der Entwicklung von Fahrzeugen wie Autos und Flugzeugen angewendet.\n",
        "\n",
        "  * Dies geschieht in Näherungsform, da keine exakten analytischen Lösungen für diese komplizierten Anwendungsfälle bekannt sind.\n",
        "\n",
        "  * Siehe auch https://de.wikipedia.org/wiki/Numerische_Strömungsmechanik\n",
        "\n",
        "> $\\rho \\overrightarrow{\\vec{v}}=\\rho\\left(\\frac{\\partial \\vec{v}}{\\partial t}+(\\vec{v} \\cdot \\nabla) \\vec{v}\\right)=-\\nabla p+\\mu \\Delta \\vec{v}+(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\vec{v})+\\vec{f}$\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Potentialströmung\n",
        "\n",
        "* https://de.wikipedia.org/wiki/Black-Scholes-Modell\n",
        "\n",
        "* Loesungsverfahren\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Finite-Elemente-Methode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Spektralmethode\n",
        "\n",
        "  * https://de.wikipedia.org/wiki/Liste_numerischer_Verfahren\n",
        "\n",
        "  * https://en.wikipedia.org/wiki/Method_of_characteristics\n",
        "\n",
        "* Die Methode der Charakteristiken ist eine Methode zur Lösung partieller\n",
        "Differentialgleichungen (PDGL/PDE), die typischerweise erster Ordnung und quasilinear sind\n",
        "\n",
        "* Die grundlegende Idee besteht darin, **die PDE durch eine geeignete Koordinatentransformation auf ein System gewöhnlicher Differentialgleichungen auf bestimmten Hyperflächen, sogenannten Charakteristiken, zurückzuführen.**\n",
        "\n",
        "* Die PDE kann dann als Anfangswertproblem in dem neuen System mit Anfangswerten auf den die Charakteristik schneidenden Hyperflächen gelöst werden. Störungen breiten sich längs der Charakteristiken aus.\n",
        "\n",
        "* Charakteristiken spielen eine Rolle in der qualitativen Diskussion der Lösung bestimmter PDE und in der Frage, wann Anfangswertprobleme für diese PDE korrekt gestellt sind.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdOgQKlZAssy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving the heat equation:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}=\\alpha \\nabla^{2} T$\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$\n",
        "\n",
        "* can be solved with Fourier series\n",
        "\n",
        "* [DE2: But what is a partial differential equation?](https://www.youtube.com/watch?v=ly4S0oi3Yz8&list=PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6&index=2)\n",
        "\n",
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$"
      ],
      "metadata": {
        "id": "aO7NYOE50lp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_001.png)"
      ],
      "metadata": {
        "id": "RRBeNGa92iKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small change in time:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_002.png)"
      ],
      "metadata": {
        "id": "EQLycVW52oK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small change in temperature after a small step in space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_003.png)"
      ],
      "metadata": {
        "id": "RKaZoj0J2pCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What that actually encodes is that we look at the limit of that ratio (temperature/space or time change) for smaller and smaller nudges to the input rather than a specific finitely small value\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_004.png)"
      ],
      "metadata": {
        "id": "YE3q91CB3c03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation $\\frac{\\partial T}{\\partial t}(x, t)=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$ tells us that\n",
        "\n",
        "* the way this function changes with respect to time $\\frac{\\partial T}{\\partial t}(x, t)$\n",
        "\n",
        "* depends on how it changes with respect to space $\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}(x, t)$.\n",
        "\n",
        "* <font color=\"blue\">More specifically it's proportional to the second partial derivative with respect to x.</font>\n",
        "\n",
        "* at a high level, the intuition is that at points where the temperature distribution curves, it tends to change more quickly in the direction of that curvature.\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_005.png)"
      ],
      "metadata": {
        "id": "3cBh_epu3xyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat equation (as partial differential equation) for three dimensions:\n",
        "\n",
        "> $\\frac{\\partial T}{\\partial t}(x, y, z, t)=\\alpha\\left(\\frac{\\partial^{2} T}{\\partial x^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial y^{2}}(x, y, z, t)+\\frac{\\partial^{2} T}{\\partial z^{2}}(x, y, z, t)\\right)$\n",
        "\n",
        "*It checks how different is a point from the average of its neigbours.* (you use second derivative for it)\n",
        "\n",
        "$\\frac{dT_2}{dt}$ = $\\Delta T_2$ - $\\Delta T_1$ (difference of differences) = $\\Delta \\Delta T_1$ (second difference = second derivative)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_007.png)"
      ],
      "metadata": {
        "id": "w9EjvvKl_LWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "* imagine discrete points. and then check the average of the neighbouring points.\n",
        "\n",
        "* if a point is much higher temperature than its neighbours, it will decrease. If it's lower, it will increase.\n",
        "\n",
        "* the equation reflects this relationship between the points and their differences\n",
        "\n",
        "* and the \"second difference\" is for continuous cases the \"second derivative\":\n",
        "\n",
        "> $\\frac{d T_{2}}{d t}=\\frac{\\alpha}{2} \\Delta \\Delta T_{1}$ $\\rightarrow$ $\\frac{\\partial T}{\\partial t}=\\alpha \\cdot \\frac{\\partial^{2} T}{\\partial x^{2}}$\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.** (you use second derivative for it)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/pde_006.png)\n",
        "\n",
        "For higher dimensions than 1 (like 3D) you use also the second derivative, but add the other dimensions as well, and the result is called nabla, the Laplacian:\n",
        "\n",
        "$\\frac{\\partial T}{\\partial t}=\\alpha(\\underbrace{\\frac{\\partial^{2} T}{\\partial x^{2}}+\\frac{\\partial^{2} T}{\\partial y^{2}}+\\frac{\\partial^{2} T}{\\partial z^{2}}}_{\\nabla^{2} T})$\n",
        "\n",
        "${\\nabla^{2}} T$ is called the \"Laplacian\" (the divergence of the gradient div(grad)f = $\\nabla\\nabla$f\n",
        "\n",
        "**It checks how different is a point from the average of its neigbours.**"
      ],
      "metadata": {
        "id": "qyKPD8oU7rEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Harmonic Analysis*"
      ],
      "metadata": {
        "id": "O8cFhBpbIFFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Signal_processing\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Window_function\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Kaiser_window"
      ],
      "metadata": {
        "id": "c0p4BOfYdU4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Fourier Analysis*"
      ],
      "metadata": {
        "id": "7bWObSh9Mh0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Discrete Fourier Transform](https://youtu.be/yYEMxqreA10)"
      ],
      "metadata": {
        "id": "CJhXQ68Ua1uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The maths behind fourier transform](https://youtu.be/FOOQrrOo-II)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1343.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1344.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1346.jpg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1345.jpg)"
      ],
      "metadata": {
        "id": "Clt-PAZNLm0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $f(x)$ is a periodic function with period $2 \\pi$. If we want to approximate this function with a trigonometric polynomial of degree $n$.\n",
        "\n",
        "**Step 1: Approximate periodic functions using infinite sums of sines and cosines:**\n",
        "\n",
        "> <font color=\"blue\">$F_n(x)=a_0+a_1 \\cos (x)+b_1 \\sin (x)+a_2 \\cos (2 x)+b_2 \\sin (2 x)+\\cdots+a_n \\cos (n x)+b_n \\sin (n x)$\n",
        "\n",
        "For $k \\geq 1$ the \"best\" coefficients to use are the following Fourier coefficients:\n",
        "\n",
        "> $\n",
        "\\begin{aligned}\n",
        "&a_0=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(x) d x \\\\\n",
        "&a_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\cos (k x) d x \\\\\n",
        "&b_k=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) \\sin (k x) d x\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Step 2: Replace sin and cosine with $e$ using Eulers Formula:**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$\n",
        "\n",
        "\n",
        "we get this euqation:\n",
        "\n",
        "> <font color=\"blue\">$X_k=x_0 e^{-b_0 j}+x_1 e^{-b_1 j}+\\ldots+x_n e^{-b_{N-1} j}$</font>\n",
        "\n",
        "**Step 3: Taking the sum where where ${\\frac{2 \\pi k n}{N}} = b_n$** for the Discrete Fourier Transform (oft hat man nur einige Messpunkte / Samples aus Experiment oder Simulation)\n",
        "\n",
        "> <font color=\"blue\">$X_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-\\frac{j 2 \\pi k n}{N}}$</font>\n",
        "\n",
        "\n",
        "$\\omega=\\frac{2 \\pi n}{T}$ ist die Kreisfrequenz der diskreten Fourier Transform für nicht-periodische Funktionen, wodurch man auch schreiben kann:\n",
        "\n",
        "> <font color=\"blue\">$F(\\omega)_{k}=\\sum_{n=0}^{N-1} x_{n} \\cdot e^{-i \\omega k}$</font> $\\quad$ ([Source](https://de.m.wikipedia.org/wiki/Fourierreihe#Zusammenhang_mit_der_Fourier-Transformation_für_nicht-periodische_Funktionen))\n",
        "\n",
        "$\\omega_{k}=\\frac{k \\pi}{T}$ is the basic frequency and you can expand f(x) as a sum of sines and cosines that are also periodic in 2 T, and **higher and higher harmonics of those basic sines and cosines (and each with a contribution factor)**.\n",
        "\n",
        "$k = \\frac{2 \\pi}{wavelength}$, e.g. wavelength = $4 \\pi$ (2 complete turns within one period of time), then $k = \\frac{1}{2}$.\n",
        "\n",
        "**Step 4: Understand orthogonal projection**: Dabei ist folgendes ein Skalarprodukt / inner product / projection:\n",
        "\n",
        "> $\\langle x_{n}, e^{-i \\omega k}\\rangle$\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$\n",
        "\n",
        "**Step 5: Get Continous Fourier Transform:** For very large N (the resolution with which we can resolve different frequencies becomes infinitesimally small) the sum $\\sum$ is becoming a Riemann integral (continous case) -  If a Fourier series can approximate a function well on an intervall, can it do it also on the whole real line between -$\\infty$ and $\\infty$?\n",
        "\n",
        "> <font color=\"blue\">$X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$</font>\n",
        "\n",
        "where we calculate the coefficients $a_k$ and $b_k$ at each particular frequency with function $x(t)$ and **analyzing function (sinusoids) $e^{-j 2 \\pi F t} d t$**.\n",
        "\n",
        "  * you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "  * wherever the function and the analyzing function are similar, they´ll multiply and sum to a large coefficient\n",
        "\n",
        "  * and wherever the function and the analyzing function are dissimilar, they´ll multiply and sum to a small coefficient\n",
        "\n",
        "*Appendix: Instead of getting one complex coefficient per frequency you can also work with two real coefficients per frequency and end up with two integrals:*\n",
        "\n",
        "* one to correlate with signal with the cosine function:\n",
        "\n",
        "> $x_{a}(F)=\\int_{-\\infty}^{\\infty} x(t) \\cos 2 \\pi \\, Ft \\,d t$\n",
        "\n",
        "* and one to correlate the signal with the sine function:\n",
        "\n",
        "> $X_{b}(F)=\\int_{-\\infty}^{\\infty} x(t) \\sin 2 \\pi \\, Ft \\, d t$\n"
      ],
      "metadata": {
        "id": "xto8Ha2DK0Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Whereas a Taylor Series attempts to approximate a function locally about the point where the expansion is taken, a Fourier series attempts to approximate a periodic function over its entire domain. That is, a Tavlor series approximates a function point wise and a Fourier series approximates a function globally.*\n",
        "\n",
        "* [Steve Brunton: The Fourier Transform](https://www.youtube.com/watch?v=jVYs-GTqm5U)\n",
        "\n",
        "* [Looking Glass: Fourier Transform](https://youtu.be/Xxut2PN-V8Q)\n",
        "\n",
        "* https://sites.oxy.edu/ron/math/120/03/labs/lab8.PDF\n",
        "\n",
        "* https://math.stackexchange.com/questions/47430/is-fourier-series-an-inverse-of-taylor-series\n",
        "\n",
        "* http://dev.ipol.im/~coco/website/taylorfourier.html\n",
        "\n",
        "* https://math.stackexchange.com/questions/7301/connection-between-fourier-transform-and-taylor-series"
      ],
      "metadata": {
        "id": "Z6mGDVo2QU7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Start with approximation of f(x) between -$\\pi$ and $\\pi$ with some orthogonal base vectors made by sin and cos and their coefficients A and B calculated via the inner product with f(x) with each frequency of sin and cos (all are orthogonal base vectors)**\n",
        "\n",
        "**Approximate <u>periodic functions</u>: [Fourier Series](https://de.m.wikipedia.org/wiki/Fourierreihe)**\n",
        "\n",
        "You expand f(x) as a sum of sines and cosines (Grundtöne) that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics (Obertöne) of those basic sines and cosines.\n",
        "\n",
        "> <font color=\"blue\">$f(t)=$$\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left[a_{n} \\cdot \\cos \\left(n \\omega_{0} t\\right)+b_{n} \\sin \\left(n \\omega_{0} t\\right)\\right]$\n",
        "\n",
        "mit folgenden Termen:\n",
        "\n",
        "* <font color=\"blue\">$a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$</font>\n",
        "\n",
        "* <font color=\"blue\">$\\omega_{0}=\\frac{2 \\pi}{T}$</font> $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "* $T$ *ist die Periode: wie viele Zeiteinheiten werden benötigt, um eine Periode der Funktion vollständig zu umlaufen?*\n",
        "\n",
        "* <font color=\"blue\">$a_n$ und $b_n$</font>: siehe weiter unten unter 'orthogonale Projektion'\n",
        "\n",
        "\n",
        "We can approximate f(x) with an expansion of sine and cosines of higher and higher frquency (and shorter wavelength):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos (k x)+B_{k} \\sin (k x)\\right)$\n",
        "\n",
        "How do we calculate the coefficients? You calculate the (Hilbert space) inner product between f(x) and cos(kx) (=the particular k-th frequency cosine wave) bzw. sin(kx). I project f(x) into the cosine k-direction (and I need to normalize by this function):\n",
        "\n",
        "> $A_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos (k x) d x$ = $\\frac{1}{||cos(kx)||^2}$ $\\langle f(x), cos(k x)\\rangle$\n",
        "\n",
        "> $B_{k}=\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin (k x) d x$ = $\\frac{1}{||sin(kx)||^2}$ $\\langle f(x), sin(k x)\\rangle$\n",
        "\n",
        "**Fourier transform is just another representation on another orthogonal basis vectors (sine and cosine with different frequencies).**\n"
      ],
      "metadata": {
        "id": "7NB7Ne-NuLnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Inner product / projection on orthogonal basis vectors to get coefficients**\n",
        "\n",
        "**<u>Orthogonal Projection (Inner Product)</u>: Calculate the coefficients $a_k$ and $b_k$ bzw. $e$ at each particular frequency**\n",
        "\n",
        "*Why do we use inner product / orthogonal projection? - If you`re familiar with calculating correlations, the Fourier transform is essentially the same:*\n",
        "\n",
        "* you`re multiplying a function, or in our case a signal, by an analyzing function (in our case: sinusoids)\n",
        "\n",
        "* <font color=\"blue\">wherever the function and the analyzing function are similar, they´ll multiply and sum to a large coefficient (=inner product large when vectors are not orthogonal)</font>\n",
        "\n",
        "* and wherever the function and the analyzing function are dissimilar, they´ll multiply and sum to a small coefficient\n",
        "\n",
        "> <font color=\"blue\">$a_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"blue\">$b_{n}=\\frac{2}{T} \\int_{(T)} $<font color=\"orange\">$f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right)$</font>$ d t$\n",
        "\n",
        "> <font color=\"orange\">$\\langle f(t) , \\cos (x) \\rangle$</font> = Inner Product / Projection of f(t) on cos(x) to get coefficient (how similar?)\n",
        "\n",
        "*Die Koeffizienten berechnen den Anteil den f(x) jeweils an der analysing function hat. Oben multipliziert man diesen Anteil dann mit der jeweiligen analysing function (sehr ähnlich zu probabilities of states in quantum mechanics). Das ist eine orthogonale Projektion (inner product f(x) mit analysing function) am unteren Beispiel:*\n",
        "\n",
        "> $a_n$ = $\\frac{2}{2} \\int_{0}^{1}(1-t) \\cdot \\cos (n \\pi t) d t$\n",
        "\n",
        "$\\rightarrow$ dieser Teil ist das inner product / Projektion von $f(x) = 1-t$ auf die analysing function $\\cos (n \\pi t) d t$\n",
        "\n",
        "\n",
        "*For when you use $e$ instead of sin & cos: We want to know the $c_k$ coefficients by projecting f(x) onto orthogonal basis vectors sin and cos*\n",
        "\n",
        "<font color=\"black\">The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$\n",
        "\n",
        "> <font color=\"blue\">$c_{k}$$=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$</font>$=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle$\n",
        "\n",
        "\n",
        "* we want to calculate the coefficients $a_k$ and $b_k$ at each particular frequency (and each frequency is orthogonal to each other, both cos vs sine as well as cos k vs cos j)\n",
        "\n",
        "> <font color=\"blue\">$c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$ (inner product)\n",
        "\n",
        "\n",
        "**Most important is the inner product / projection on orthogonal basis vectors to get coefficients!!**\n",
        "\n",
        "The Fourier series definition is exactly the same as how we write a vector F in an orthogonal basis in $R^2$, in a 2 dimensional vector space.\n",
        "\n",
        "* I pick some basis (picture on top right) for example x and y,\n",
        "\n",
        "  * and then I take the inner product of F in the x-direction and then the inner product of F in the y-direction,\n",
        "\n",
        "  * and I take those coefficients, let's call it $a_1$ and $a_2$, and I take them and multiply them by the X unit vector and the Y unit vector, and I add them up\n",
        "\n",
        "* and in Fourier: sine and cosine functions are orthogonal, just like x and y are orthogonal vectors.\n",
        "\n",
        "  * And then i take my function f and project it on the sine and cosine to see how much of f is in this cosine direction and how much of f is in the sine direction\n",
        "\n",
        "  * from that I get my $A_k$-th coefficient and $B_k$-th coefficient, and then I multiply that by my cosine function (and sine function) and I add all of those up\n",
        "\n",
        "\n",
        "> $a_{0}=\\frac{2}{T} \\int_{(T)} f(t) d t$\n",
        "\n",
        "> $a_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\cos \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $b_{n}=\\frac{2}{T} \\int_{(T)} f(t) \\cdot \\sin \\left(n \\omega_{0} t\\right) d t$\n",
        "\n",
        "> $\\omega_{0}=\\frac{2 \\pi}{T}$ $\\quad $= \"Kreisfrequenz\"\n",
        "\n",
        "*T ist die Periode: wie viele Zeiteinheiten werden benötigt, um eine Periode der Funktion vollständig zu umlaufen?*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pY8yHtyJwMjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Finetune with approximation of f(x) to get between 0 and $L$, which is periodic in L and repeats beyond 0 and L**\n",
        "\n",
        "* we go now in the domain 0 to L (before above it was -$\\pi$ to $\\pi$)\n",
        "\n",
        "* now we make the sines and cosines periodic between 0 and L (=space of Lebesgue integrable functions)\n",
        "\n",
        "> $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "We the Fourier series for L-periodic functions (and not just 2 $\\pi$ like before):\n",
        "\n",
        "> $f(x)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left(A_{k} \\cos \\left(\\frac{2 \\pi k x}{L}\\right)+B_{k} \\sin \\left(\\frac{2 \\pi k x}{L}\\right)\\right)$\n",
        "\n",
        "How do we get A and B?\n",
        "\n",
        "* We take the inner product of f(x) with $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$ (und entsprechend auch fur sin)\n",
        "\n",
        "* this inner product is the projection of f(x) onto the orthogonal basis vector (here each k - $\\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$)\n",
        "\n",
        "* and then normalized by the norm of the cosine function, which in thhis case the norm squared is $\\frac{2}{L}$:\n",
        "\n",
        "> $A_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\cos \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "> $B_{k}=\\frac{2}{L} \\int_{0}^{L} f(x) \\sin \\left(\\frac{2 \\pi k}{L} x\\right) d x$\n",
        "\n",
        "* these cosine and sine functions are an orthogonal basis for my function space (Hilbert space of functions f(x))\n",
        "\n",
        "* If I plugged in a cosine k and a sine j, these functions are orthogonal. Like if I plug in two cosines with different k's, their inner product is 0 (because they should be orthogonal). Umgekehrt: if I plug in cos kx and cos kx then I would get a non zero inner product.\n",
        "\n",
        "* btw: the approximation of f(x) between 0 and L will repeat infineily beyond 0 and l."
      ],
      "metadata": {
        "id": "52T4qM5-A2r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Introduce $e^{ikx}$ to combine sin and cos in one complex coefficient**\n",
        "\n",
        "**<u>Analysing Function</u>: Statt sin und cos kann man auch $e$ verwenden**\n",
        "\n",
        "> $e^{j x}=\\cos x+j \\sin x$ (Eulers formula)\n",
        "\n",
        "$e^{i k x}$ is the analyzing function made of sinusoids (eigentlich: $e^{-j 2 \\pi F t} d t$)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i x \\frac{k \\pi}{L}}$</font> (becoming a Riemann integral) with $\\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency)\n",
        "\n",
        "> <font color=\"blue\">$f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$</font> $= \\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "\n",
        "\n",
        "Anderes Beispiel mit einem Integral statt Summe geschrieben:\n",
        "\n",
        "> $X(F)=\\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi F t} d t$\n",
        "\n",
        "* basics: remember the Euler expansion:\n",
        "\n",
        "> $e^{i k x}=\\cos (k x)+i \\sin (k x)$\n",
        "\n",
        "* now we talk about the fourier series in terms of a complex basis:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k x}$\n",
        "\n",
        "(you could expand that in sin and cos: $f(x)=\\sum_{k=-\\infty}^{\\infty}\\left(\\alpha_{k}+i \\beta_{k}\\right)(\\cos (k x)+i \\sin (k x))$)\n",
        "\n",
        "See also [Inner Products in Hilbert Space](https://www.youtube.com/watch?v=g-eNeXlZKAQ&list=PLMrJAkhIeNNT_Xh3Oy0Y4LTj0Oxo8GqsC&index=4)\n",
        "\n",
        "* inner product of functions is consistent with definition of inner products of vectors\n",
        "\n",
        "* similar functions should have a large inner product\n",
        "\n",
        "* if I go to infinity with delta x (make it finer and finer), then the Riemann approximation $\\langle f,g \\rangle$ becomes the continuous integral formulation $\\int$\n",
        "\n",
        "You can expand f(x) as a sum of sines and cosines that are also periodic in 2 L (on the line from -L to L), and higher and higher harmonics of those basic sines and cosines.\n",
        "\n",
        "You can represent this as a complex Fourier series:\n",
        "\n",
        "> $f(x)=\\sum_{k=-\\infty}^{\\infty} c_{k} e^{i k \\pi \\frac{x}{L}} \\quad \\omega_{k}=\\frac{k \\pi}{L}$ (basic frequency / Kreisfrequenz)\n",
        "\n",
        "<font color=\"red\">**The $c_{k}$ are the Fourier coefficients that are obtained by projecting my function $f$ into each of these orthogonal function directions given by $e^{i k \\pi \\frac{x}{L}}$ = $\\Psi_k$**\n",
        "\n",
        "> $c_{k}=\\frac{1}{2 \\pi}\\left\\langle f(x), \\Psi_{k}\\right\\rangle=\\frac{1}{2 L} \\int_{-L}^{L} f(x) \\underbrace{e^{-i k \\pi \\frac{x}{L}}}_{\\Psi_{k}} d x$"
      ],
      "metadata": {
        "id": "dLbwYXpgvwx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Näherungsverfahren für periodische Funktionen: Fourier Series**\n",
        "\n",
        "* nutzt man weil zB manche periodische Funktionen sehr kompliziert sind, um sie mit einer Funktion f(x) zu beschreiben\n",
        "\n",
        "**Näherungsverfahren für nicht-periodische Funktionen: Fourier Transform**\n",
        "\n",
        "**Näherungsverfahren für Polynome: Taylor Polynom**\n",
        "\n",
        "https://medium.com/sho-jp/fourier-transform-101-part-1-b69ea3cb4837\n",
        "\n",
        "**Harmonic Analysis**\n",
        "\n",
        "Aus Sicht der [abstrakten harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) sind sowohl die Fourier-Reihen und die Fourier-Integrale als auch die Laplace-Transformation, die Mellin-Transformation oder auch die Hadamard-Transformation Spezialfälle einer **allgemeineren (Fourier-)Transformation**."
      ],
      "metadata": {
        "id": "MWiANZ7vSEOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "*We can clearly observe a peak value at 10 Hz with a magnitude of one while all other frequencies hover around zero. We can verify this from the original signal where there are 10 complete cycles in a second with an amplitude of one:*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_49.png)\n",
        "\n",
        "*When a similar principle is applied to a more complicated time series as shown in the green plot below, we can deduce from its Fourier transform that the data comprises of 3 different elementary components with 3 different frequencies (2, 5 and 10 Hz) at 3 different amplitudes (0.5, 1 and 2):*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_50.png)\n",
        "\n",
        "Source: https://medium.com/@khairulomar/deconstructing-time-series-using-fourier-transform-e52dd535a44e"
      ],
      "metadata": {
        "id": "1q0EqJnVRvQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetrien**\n",
        "\n",
        "Die Kosinusfunktion ist achsensymmetrisch:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_51.png)\n",
        "\n",
        "In Worten: Ein x-Wert und der negative x-Wert haben denselben Kosinuswert. Als Formel: cos(−𝑥)=cos𝑥\n",
        "\n",
        "Beispiel:\n",
        "\n",
        ">$\\cos \\left(\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        ">$\\cos \\left(-\\frac{3}{8} \\pi\\right)=0,38$\n",
        "\n",
        "Die Sinusfunktion ist punktsymmetrisch zum Koordinatenursprung. Stelle dir vor, wie du den rechten Arm des Graphen um (0|0) drehst.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_52.png)\n",
        "\n",
        "In Worten: sin(−𝑥)\n",
        "sin\n",
        "(\n",
        "-\n",
        "x\n",
        ")\n",
        " ist sin𝑥\n",
        "sin\n",
        "x\n",
        " mit umgedrehtem Vorzeichen.\n",
        "Als Formel: sin(−𝑥)=−sin𝑥\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "> $\\sin \\left(\\frac{\\pi}{4}\\right)=0,71$\n",
        "\n",
        "> $\\sin \\left(-\\frac{\\pi}{4}\\right)=-0,71$\n",
        "\n",
        "Source: https://www.kapiert.de/sinus-und-kosinusfunktionen-eigenschaften/\n",
        "\n"
      ],
      "metadata": {
        "id": "Ax6bbzUGSd6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/fourier_53.png)\n",
        "\n",
        "*Source: [Mathe mit Nina](https://youtu.be/u6Fqi8596qA)*\n"
      ],
      "metadata": {
        "id": "bdefArXifISa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeO_i6Z9-rLD"
      },
      "source": [
        "###### *Laplace Transform*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhmFbOnmaUS"
      },
      "source": [
        "**Fourier** $\\quad X(\\omega)=\\int_{-\\infty}^{\\infty} x(t) e^{-i \\omega t} d t$\n",
        "\n",
        "**Laplace** $\\quad X(s)=\\int_{0}^{\\infty} x(t) e^{-s t} d t$\n",
        "\n",
        "with s = ${\\alpha + i \\omega}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1193.png)"
      ],
      "metadata": {
        "id": "msM9RjFYPKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1194.png)"
      ],
      "metadata": {
        "id": "qke32YL1QCrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Taylor Series*"
      ],
      "metadata": {
        "id": "xDUD9L1G7HIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor Series Expansion to approximate $e^x$**\n",
        "\n",
        "> <font color=\"blue\">$e^{x} \\approx \\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !} \\approx 1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\ldots$\n",
        "\n",
        "*(Näherungsverfahren für Polynome: Taylor Polynom)*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1347.jpg)\n",
        "\n",
        "Funktionsvorschrift:\n",
        "\n",
        "$a_{i}=a_{1} \\cdot q^{i-1}$ bzw $a_{i}=a_{0} \\cdot q^{i}$ für Anfangsglied a1. Oder auch (andere Folge): $a_{i}=a_{0} \\cdot q^{i}$\n",
        "\n",
        "Rekursionsvorschrift (wie Fibonacci):\n",
        "\n",
        "$a_{i+1}=a_{i} \\cdot q$. Oder auch (andere Folge): $a_{i}=q \\cdot a_{i-1}$\n",
        "\n",
        "*The following code examples are taken from ['Python for Undergraduate Engineers'](https://pythonforundergradengineers.com/creating-taylor-series-functions-with-python.html)*"
      ],
      "metadata": {
        "id": "rkbWd2CM6u6w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3QOyKV6llLn"
      },
      "source": [
        "**Taylor Series Expansion to approximate $cos(x)$**\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x) \\approx \\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !} \\approx 1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\frac{x^{8}}{8 !}-\\frac{x^{10}}{10 !}+\\ldots$\n",
        "\n",
        "* We can code this formula into a function that contains a for loop. Note the variable x is the value we are trying to find the cosine of, the variable n is the number of terms in the Taylor Series, and the variable i is the loop index which is also the Taylor Series term number.\n",
        "* We are using a separate variable for the coefficient coef which is equal to (−1)<sup>i</sup>, the numerator num which is equal to x<sup>2i</sup> and the denominator denom which is equal to (2i!). Breaking the Taylor Series formula into three parts can cut down on coding errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Beziehung zwischen Sinus, Kosinus und Exponentialfunktion**\n",
        "\n",
        "Die [trigonometrischen Funktionen](https://de.m.wikipedia.org/wiki/Trigonometrische_Funktion) sind eng mit der [Exponentialfunktion](https://de.m.wikipedia.org/wiki/Exponentialfunktion) verbunden, wie die [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel) zeigt:\n",
        "\n",
        "Eigentlich hat $e$ nichts mit periodischen Funktionen wie $sin$ oder $cos$ zu tun. Aber fügt man $i$ in den exponent von $e$, dann wird daraus eine rotierende, periodische Funktion.*\n",
        "\n",
        "**Part I: Let`s get the Taylor expansions for three common functions**\n",
        "\n",
        "> $e^{x}=1+x+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{x^{5}}{5 !}+\\cdots$\n",
        "\n",
        "> <font color=\"blue\">$\\cos (x)=1-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !}-\\frac{x^{6}}{6 !}+\\cdots$\n",
        "\n",
        "> <font color=\"red\">$\\sin (x)=x-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !}-\\frac{x^{7}}{7 !}+\\cdots$\n",
        "\n",
        "* there is some relation between these three:\n",
        "\n",
        "  * all all built upon $\\frac{x^{n}}{n !}$\n",
        "\n",
        "  * cosinus has all even numbers, sinus all odd numbers, and exponential both\n",
        "\n",
        "  * Vorzeichen is alternating for cos and sin, but all posisitv for exponential\n",
        "\n",
        "**Part II: Add $i$ to relate all three functions**\n",
        "\n",
        "* You can't just add up cos and sin to get exp! You need to add the $i = \\sqrt{-1}$\n",
        "\n",
        "* We need to replace all $x$ with an $i x$ in the exp series\n",
        "\n",
        "> $e^{ix}=1+ix+\\frac{(ix)^{2}}{2 !}+\\frac{(ix)^{3}}{3 !}+\\frac{(ix)^{4}}{4 !}+\\frac{(ix)^{5}}{5 !}+\\cdots$\n",
        "\n",
        "* It introduces the following: $i=i \\quad i^{2}=-1 \\quad i^{3}=-i \\quad i^{4}=1$\n",
        "\n",
        "* this turns the exponential into the following:\n",
        "\n",
        "> $e^{i x}=1+i x-\\frac{x^{2}}{2 !}-\\frac{ix^{3}}{3 !}+\\frac{x^{4}}{4 !}+\\frac{i x^{5}}{5 !}+$\n",
        "\n",
        "* finally, let's separate the terms with $i$ (all odd) from those without $i$ (all even):\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$\\left[1-\\frac{x^{2}}{2!}+\\frac{x^{4}}{4!}-\\cdots \\right]$</font> + $i$ <font color=\"red\">$\\left[x-\\frac{x^{3}}{3!}+\\frac{x^{5}}{5!}-\\cdots\\right]$</font>\n",
        "\n",
        "> $e^{ix}$ = <font color=\"blue\">$cos(x)$</font> + $i$ <font color=\"red\">$ \\, sin(x)$</font>\n",
        "\n",
        "*$sin(x)$ ist der imaginäre Anteil, und $cos(x)$ ist der reale Anteil für $z=e^{i \\varphi}=x+i y$*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/400px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "\n",
        "**Part III: Let`s replace $x$ with $\\pi$**\n",
        "\n",
        "> $e^{i \\pi}=\\cos (\\pi)+i \\sin (\\pi)$\n",
        "\n",
        "* sin and cos repeat and one full period is $2 \\pi$ radians (=360°)\n",
        "\n",
        "* $cos(\\pi)$ is half a rotation = -1 and $sin(\\pi)$ = 0, which means:\n",
        "\n",
        "> $e^{i \\pi}= -1$ $\\,$ and $\\,$ $e^{2\\pi i} = 1$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIb2bU82PFrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Music Theory*"
      ],
      "metadata": {
        "id": "fq5mO57kWnjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [A beginner's guide to music theory](https://youtu.be/n2z02J4fJwg)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1372.jpg)"
      ],
      "metadata": {
        "id": "enDRiPrwWj_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [The mathematical problem with music, and how to solve it](https://youtu.be/nK2jYk37Rlg)\n",
        "\n",
        "* Frequency (in Hertz) = no. of vibrations per second. Typically between 50 to a few thousand Hertz. Higher frequency = higher pitch (i.e. 200. vs 1000)\n",
        "\n",
        "* all musical tones are multiples of pure tones (harmonics) = behave to the mathematical sine function. For example tone f consists of frequencies of several integer multiples of a pure tone f, 2f, 3f.. (first harmonic, second harmonic, third harmonic..).\n",
        "\n",
        "* Melodies = sequences of tones. Strictly speaking these two melodies have not even one frequency in common, but they sound similar. But both melodies have the **same ratios between the frequencies of their tones.** Frequencies are different, but ratios are the same.\n",
        "\n",
        "* Changing from melody 1 to melody 2 is called **transposition from one key to another**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1373.jpg)\n",
        "\n",
        "* Another important concept is the **Musical intervall**, which is the frequency ratio mentioned above:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1374.jpg)\n",
        "\n",
        "* One important interval is called the octave, and it corresponds to a frequency ratio of 2:1. The Octave is very pleasent to the ear. **Octave equivalence**: two tones that are an Octave apart sound highly similar: hence have same name and belong to the same pitch class.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1375.jpg)\n",
        "\n",
        "* Another important is called the **Fifth** with ration 3 over 2:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1376.jpg)\n"
      ],
      "metadata": {
        "id": "A5ewjWi3Xe1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGNbw_qosx7"
      },
      "source": [
        "##### <font color=\"blue\">*Funktionentheorie*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-vaCcWrF4t"
      },
      "source": [
        "Die [Funktionentheorie](https://de.wikipedia.org/wiki/Funktionentheorie) befasst sich mit der Theorie differenzierbarer komplexwertiger Funktionen mit komplexen Variablen. Da insbesondere die Funktionentheorie einer komplexen Variablen reichlich Gebrauch von Methoden aus der reellen Analysis macht, nennt man das Teilgebiet auch komplexe Analysis.\n",
        "\n",
        "* Für holomorphe Funktionen gilt, dass Real- und Imaginärteil [harmonische Funktionen](https://de.wikipedia.org/wiki/Harmonische_Funktion) sind, also die [Laplace-Gleichung](https://de.wikipedia.org/wiki/Laplace-Gleichung) erfüllen. Dies verknüpft die Funktionentheorie mit den partiellen Differentialgleichungen, beide Gebiete haben sich regelmäßig gegenseitig beeinflusst.\n",
        "\n",
        "* Das Wegintegral einer holomorphen Funktion ist vom Weg unabhängig. Dies war historisch das erste Beispiel einer Homotopieinvarianz. Aus diesem Aspekt der Funktionentheorie entstanden viele Ideen der algebraischen Topologie, beginnend mit Bernhard Riemann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmbwvHZAec-"
      },
      "source": [
        "**Komplexe Funktion**\n",
        "\n",
        "* Eine [komplexe Funktion](https://de.wikipedia.org/wiki/Funktionentheorie#Komplexe_Funktionen) ordnet einer komplexen Zahl eine weitere [komplexe Zahl](https://de.m.wikipedia.org/wiki/Komplexe_Zahl) zu\n",
        "\n",
        "* Da jede komplexe Zahl durch zwei reelle Zahlen in\n",
        "der Form $x+$ iy geschrieben werden kann, lässt sich eine allgemeine Form einer komplexen Funktion darstellen durch:\n",
        "\n",
        "> $\n",
        "x+i y \\mapsto f(x+i y)=u(x, y)+i v(x, y)\n",
        "$\n",
        "\n",
        "* Dabei sind $u(x, y)$ und $v(x, y)$ reelle Funktionen, die von zwei reellen Variablen $x$ und $y$ abhängen.\n",
        "\n",
        "* $u(x, y)$ heißt der Realteil und $v(x, y)$ der Imaginärteil der Funktion.\n",
        "\n",
        "* **Insofern ist eine komplexe Funktion nichts anderes als eine Abbildung von $\\mathbb{R}^{2}$ nach $\\mathbb{R}^{2}$ (also eine Abbildung, die zwei reellen Zahlen wieder zwei reelle Zahlen zuordnet).**\n",
        "\n",
        "* **Tatsächlich könnte man die Funktionentheorie auch mit Methoden der reellen Analysis aufbauen.**\n",
        "\n",
        "* Der Unterschied zur reellen Analysis wird erst deutlicher, wenn man **komplex-differenzierbare Funktionen** betrachtet und dabei die <u>**multiplikative Struktur des Körpers der komplexen Zahlen**</u> ins Spiel bringt, die dem Vektorraum $\\mathbb{R}^{2}$ fehlt.\n",
        "\n",
        "* Wie auch bei reellwertigen und reellen Funktionen ist die Verwendung des Begriffes einer komplexen Funktion in der Literatur aber nicht eindeutig. Teilweise wird er synonym mit einer komplexwertigen Funktion verwendet, teilweise wird er auch nur für komplexwertige Funktionen einer komplexen Variablen verwendet, also Funktionen\n",
        "\n",
        "> $\n",
        "f: D \\rightarrow \\mathbb{C}\n",
        "$\n",
        "\n",
        "bei denen $D \\subseteq \\mathbb{C}$ ist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDS2jqpHCZov"
      },
      "source": [
        "**Komplexwertige Funktion**\n",
        "\n",
        "* Eine [komplexwertige Funktion](https://de.wikipedia.org/wiki/Komplexwertige_Funktion) ist eine Funktion, deren Funktionswerte komplexe Zahlen sind\n",
        "\n",
        "* Eng damit verwandt ist der Begriff der **komplexen Funktion**, der in der Literatur aber nicht eindeutig verwendet wird\n",
        "\n",
        "* Komplexwertige Funktionen werden in der Analysis und in der Funktionentheorie untersucht und haben vielfältige Anwendungen wie zum Beispiel in der Physik und der Elektrotechnik, wo sie beispielsweise zur Beschreibung von Schwingungen dienen.\n",
        "\n",
        "* Eine komplexwertige Funktion ist eine Funktion\n",
        "$f: D \\rightarrow \\mathbb{C}$ bei der die Zielmenge die Menge der komplexen Zahlen ist.\n",
        "\n",
        "* **An die Definitionsmenge $D$ sind keine Anforderungen gestellt**.\n",
        "\n",
        "* **Aufgrund der Einbettung der reellen Zahlen in die komplexen Zahlen lassen sich alle reellwertigen Funktionen auch als komplexwertige Funktionen auffassen.**\n",
        "\n",
        "* Beispiel: Die Funktion $f: \\mathbb{R} \\rightarrow \\mathbb{C}$ definiert durch $f(x)= \\mathrm{e}^{\\mathrm{i} x}=\\cos (x)+\\mathrm{i} \\sin (x)$ ist eine komplexwertige Funktion einer reellen Variable, und zwar die [Eulersche Formel](https://de.wikipedia.org/wiki/Eulersche_Formel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpqu4CjYvL7"
      },
      "source": [
        "**Holomorphe Funktion**\n",
        "\n",
        "* Holomorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* [Holomorphe Funktionen](https://de.wikipedia.org/wiki/Holomorphe_Funktion) sind **an jedem Punkt komplex differenzierbar**.\n",
        "\n",
        "* Eine Funktion $f\\colon U\\to {\\mathbb  {C}}$ mit einer offenen Menge $ U\\subseteq \\mathbb {C}$ heißt holomorph, falls sie in jedem Punkt von $U$ komplex differenzierbar ist.\n",
        "\n",
        "* Holomorphie eine sehr starke Eigenschaft ist, die im Reellen kein Pendant besitzen (z.B. ist jede holomorphe Funktion beliebig oft (stetig) differenzierbar und lässt sich lokal in jedem Punkt in eine Potenzreihe entwickeln.)\n",
        "\n",
        "Es sei $U \\subseteq \\mathbb{C}$ eine offene Teilmenge der komplexen Ebene und $z_{0} \\in U$ ein Punkt dieser Teilmenge. Eine Funktion $f: U \\rightarrow \\mathbb{C}$ heißt komplex differenzierbar im Punkt $z_{0}$, falls der Grenzwert\n",
        "\n",
        ">$\n",
        "\\lim _{h \\rightarrow 0} \\frac{f\\left(z_{0}+h\\right)-f\\left(z_{0}\\right)}{h}\n",
        "$\n",
        "\n",
        "existiert. Man bezeichnet ihn dann als $f^{\\prime}\\left(z_{0}\\right)$.\n",
        "\n",
        "Die Funktion $f$ heißt holomorph im Punkt $z_{0}$, falls eine Umgebung von $z_{0}$ existiert, in der $f$ komplex differenzierbar ist. Ist $f$ auf ganz $U$ holomorph, so nennt man $f$ holomorph.\n",
        "\n",
        "**Ist weiter $U=\\mathbb{C},$ so nennt man $f$ eine <u>ganze Funktion</u>**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VeegSuMPXcu"
      },
      "source": [
        "**Biholomorphe Funktionen**\n",
        "\n",
        "* Eine Funktion, die holomorph, bijektiv und deren Umkehrfunktion holomorph ist, nennt man [biholomorph](https://de.wikipedia.org/wiki/Biholomorphe_Abbildung).\n",
        "\n",
        "* Im Fall einer komplexen Veränderlichen ist das äquivalent dazu, dass die Abbildung bijektiv und konform ist.\n",
        "* Aus Sicht der Kategorientheorie ist eine biholomorphe Abbildung ein Isomorphismus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StjTRqJOsq4"
      },
      "source": [
        "**Cauchy-Riemannsche Differentialgleichungen**\n",
        "\n",
        "* Siehe [Cauchy-Riemannsche Differentialgleichungen](https://de.wikipedia.org/wiki/Cauchy-Riemannsche_partielle_Differentialgleichungen) - Mit einer Einleitung [hier](https://de.wikipedia.org/wiki/Holomorphe_Funktion#Cauchy-Riemannsche_Differentialgleichungen).\n",
        "\n",
        "* Zerlegt man eine Funktion $f(x+i y)=u(x, y)+i v(x, y)$ in ihren Real-und Imaginärteil mit reellen Funktionen $u, v,$ so hat die totale Ableitung $L$ als Darstellungsmatrix die **Jacobi-Matrix**\n",
        "\n",
        ">$\n",
        "\\left(\\begin{array}{ll}\n",
        "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
        "\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "Folglich ist die Funktion $f$ genau dann komplex differenzierbar, wenn sie reell differenzierbar ist und für $u, v$ die Cauchy-Riemannschen Differentialgleichungen\n",
        "\n",
        ">$\n",
        "\\begin{array}{l}\n",
        "\\frac{\\partial u}{\\partial x}=\\frac{\\partial v}{\\partial y} \\\\\n",
        "\\frac{\\partial u}{\\partial y}=-\\frac{\\partial v}{\\partial x}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "erfüllt sind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNafAzsW_vP"
      },
      "source": [
        "**Analytische Funktion (=holomorph)**\n",
        "\n",
        "* Als [analytische Funktion](https://de.wikipedia.org/wiki/Analytische_Funktion) bezeichnet man eine Funktion, die lokal durch eine **konvergente Potenzreihe** gegeben ist.\n",
        "\n",
        "* Aufgrund der Unterschiede zwischen reeller und komplexer Analysis spricht man zur Verdeutlichung oft auch explizit von reell-analytischen oder komplex-analytischen Funktionen.\n",
        "\n",
        "* **Im Komplexen sind die Eigenschaften analytisch und holomorph äquivalent.**\n",
        "\n",
        "* **Ist eine Funktion in der gesamten komplexen Ebene definiert und analytisch, nennt man sie ganz.**\n",
        "\n",
        "\n",
        "Es sei $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{K}=\\mathbb{C} .$ Es sei $D \\subseteq \\mathbb{K}$ eine offene Teilmenge. Eine Funktion $f: D \\rightarrow \\mathbb{K}$ heißt analytisch im Punkt $x_{0} \\in D,$ wenn es eine **Potenzreihe**\n",
        "\n",
        ">$\n",
        "\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "gibt, die auf einer Umgebung von $x_{0}$ gegen $f(x)$ konvergiert. Ist $f$ in jedem Punkt von $D$ analytisch, so heißt $f$ analytisch.\n",
        "\n",
        "Viele gängige Funktionen der reellen Analysis wie beispielsweise Polynome, Exponential- und Logarithmusfunktionen, trigonometrische Funktionen und rationale Ausdrücke in diesen Funktionen sind analytisch.\n",
        "\n",
        "Unter einer [Potenzreihe](https://de.wikipedia.org/wiki/Potenzreihe) $P(x)$ versteht man in der Analysis eine unendliche Reihe der Form\n",
        "\n",
        ">$\n",
        "P(x)=\\sum_{n=0}^{\\infty} a_{n}\\left(x-x_{0}\\right)^{n}\n",
        "$\n",
        "\n",
        "mit einer beliebigen Folge $\\left(a_{n}\\right)_{n \\in \\mathbb{N}_{0}}$ reeller oder komplexer Zahlen\n",
        "dem Entwicklungspunkt $x_{0}$ der Potenzreihe.\n",
        "\n",
        "Potenzreihen spielen eine wichtige Rolle in der Funktionentheorie und **erlauben oft eine sinnvolle Fortsetzung reeller Funktionen in die komplexe Zahlenebene**. Insbesondere stellt sich die Frage, für welche reellen oder komplexen Zahlen eine Potenzreihe konvergiert. Diese Frage führt zum Begriff des [Konvergenzradius](https://de.wikipedia.org/wiki/Konvergenzradius).\n",
        "\n",
        "* Jede Polynomfunktion lässt sich als Potenzreihe auffassen, bei der fast alle Koeffizienten $a_{n}$ gleich 0 sind.\n",
        "\n",
        "* Wichtige andere Beispiele sind **Taylorreihe** und **Maclaurinsche Reihe**.\n",
        "\n",
        "* Funktionen, die sich durch eine Potenzreihe darstellen lassen, werden auch [analytische Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) genannt.\n",
        "\n",
        "**Beispielhaft die Potenzreihendarstellung einiger bekannter Funktionen**:\n",
        "\n",
        "* **Exponentialfunktion**: $e^{x}=\\exp (x)=\\sum_{n=0}^{\\infty} \\frac{x^{n}}{n !}=\\frac{x^{0}}{0 !}+\\frac{x^{1}}{1 !}+\\frac{x^{2}}{2 !}+\\frac{x^{3}}{3 !}+\\cdots$ für alle\n",
        "$x \\in \\mathbb{R},$ d. h., der Konvergenzradius ist unendlich.\n",
        "\n",
        "* **Sinus**: $\\sin (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n+1}}{(2 n+1) !}=\\frac{x}{1 !}-\\frac{x^{3}}{3 !}+\\frac{x^{5}}{5 !} \\mp \\cdots$\n",
        "\n",
        "* **Kosinus**: $\\cos (x)=\\sum_{n=0}^{\\infty}(-1)^{n} \\frac{x^{2 n}}{(2 n) !}=\\frac{x^{0}}{0 !}-\\frac{x^{2}}{2 !}+\\frac{x^{4}}{4 !} \\mp \\cdots$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GiFSM9LMFFp"
      },
      "source": [
        "**Ganze Funktion**\n",
        "\n",
        "* In der Funktionentheorie ist eine [ganze Funktion](https://de.wikipedia.org/wiki/Ganze_Funktion) eine Funktion, **die in der gesamten komplexen Zahleneben**e $\\mathbb {C}$  holomorph (also analytisch) ist.\n",
        "\n",
        "* an entire function, also called an integral function, is a complex-valued function that is holomorphic at all finite points over the whole complex plane.\n",
        "\n",
        "* **Every entire function f(z) can be represented as a power series**\n",
        "\n",
        "* Typische Beispiele ganzer Funktionen sind Polynome oder die **Exponentialfunktion** sowie Summen, Produkte und Verknüpfungen davon, etwa die **trigonometrischen Funktionen** und die **Hyperbelfunktionen**.\n",
        "\n",
        "* Jede ganze Funktion kann als eine überall konvergierende Potenzreihe um ein beliebiges Zentrum dargestellt werden. Weder der Logarithmus noch die Wurzelfunktion sind ganz.\n",
        "\n",
        "* Eine ganze Funktion kann eine **isolierte Singularität**, insbesondere sogar eine wesentliche Singularität im komplexen Punkt im Unendlichen (und nur da) besitzen.\n",
        "\n",
        "**Beispiele**\n",
        "\n",
        "* der Kehrwert der Gammafunktion $1 / \\Gamma(z)$\n",
        "\n",
        "* die Fehlerfunktion $\\operatorname{erf}(z)$\n",
        "\n",
        "* der Integralsinus $\\operatorname{Si}(z)$\n",
        "\n",
        "* die Airy-Funktionen $\\operatorname{Ai}(z)$ und $\\operatorname{Bi}(z)$\n",
        "\n",
        "* die Fresnelschen Integrale $S(z)$ und $C(z)$\n",
        "\n",
        "* die Riemannsche Xi-Funktion $\\xi(z)$\n",
        "\n",
        "* die Besselfunktionen erster Art $J_{n}(z)$ für ganzzahlige $n$\n",
        "\n",
        "* die Struve-Funktionen $H_{n}(z)$ für ganzzahlige $n>-2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Era7KDwoxIS"
      },
      "source": [
        "**Laurent-Reihe**\n",
        "\n",
        "**Exkurs**: Die Laurent-Reihe ist eine **unendliche Reihe ähnlich einer Potenzreihe**, aber zusätzlich **mit negativen Exponenten**. Allgemein hat eine Laurent-Reihe in $x$ mit Entwicklungspunkt $c$ diese Gestalt (Dabei sind die $a_{n}$ und $c$ meist komplexe Zahlen):\n",
        "\n",
        ">$\n",
        "f(x)=\\sum_{n=-\\infty}^{\\infty} a_{n}(x-c)^{n}\n",
        "$\n",
        "\n",
        "Es sei $D$ eine nichtleere offene Teilmenge der Menge $\\mathbb{C}$ der komplexen Zahlen und $P_{f}$ eine weitere Teilmenge von $\\mathbb{C}$, die nur aus isolierten Punkten besteht. Eine Funktion $f$ heißt meromorph, wenn sie für Werte aus $D \\backslash P_{f}$ definiert und holomorph ist und für Werte aus $P_{f}$ Pole hat. $P_{f}$ wird als Polstellenmenge von $f$ bezeichnet.\n",
        "\n",
        "* Zerlegung einer komplex differenzierbaren Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4Q4vb9RlIR"
      },
      "source": [
        "**Meromorphe Funktion**\n",
        "\n",
        "* Meromorphie ist eine Eigenschaft von bestimmten komplexwertigen Funktionen\n",
        "\n",
        "* Für viele Fragestellungen der Funktionentheorie ist der **Begriff der holomorphen Funktion zu speziell**.\n",
        "\n",
        "* Dies liegt daran, dass der Kehrwert $\\frac{1}{f}$ einer holomorphen Funktion $f$ an einer Nullstelle von $f$ eine Definitionslücke hat und somit $\\frac{1}{f}$ dort auch **nicht komplex differenzierbar ist**.\n",
        "\n",
        "* Man führt daher den allgemeineren Begriff der [meromorphen Funktion](https://de.wikipedia.org/wiki/Meromorphe_Funktion) ein, die auch **isolierte Polstellen** besitzen kann.\n",
        "\n",
        "* Meromorphe Funktionen lassen sich lokal als [Laurentreihen](https://de.wikipedia.org/wiki/Laurent-Reihe) mit abbrechendem Hauptteil darstellen. Ist $U$ ein Gebiet von $\\mathbb{C},$ so bildet die Menge der auf $U$ meromorphen Funktionen einen Körper.\n",
        "\n",
        "* Alle holomorphen Funktionen sind auch meromorph, da ihre Polstellenmenge leer ist.\n",
        "\n",
        "Zum Beispiel ist die Gamma-Funktion meromorph, weil sie holomorph ist auf $\\mathbb{C}$, abgesehen von abzahlbar vielen nicht-hebbaren Singularitaeten (hierbei in allen negativen ganzen Zahlen, da der Definitionsbereich einer Gammafunktion $\\mathbb{C}$  \\ -$\\mathbb{N}$ <sub>0</sub> ist).\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "* [Gamma Funktion](https://en.wikipedia.org/wiki/Gamma_function)\n",
        "\n",
        "* [Elliptische Funktion](https://de.wikipedia.org/wiki/Elliptische_Funktion)\n",
        "\n",
        "*Der Absolutwert der Gammafunktion geht nach Unendlich an den Polstellen (links). Rechts hat sie keine Polstellen und steigt nur schnell an.*\n",
        "\n",
        "*The [gamma function](https://en.wikipedia.org/wiki/Gamma_function) is meromorphic in the whole complex plane.*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/3/33/Gamma_abs_3D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjZAY8WRubkK"
      },
      "source": [
        "**Isolierte Singularität**\n",
        "\n",
        "* [Isolierte Singularitäten](https://de.wikipedia.org/wiki/Isolierte_Singularität) sind besondere [**isolierte Punkte**](https://de.wikipedia.org/wiki/Isolierter_Punkt) in der Quellmenge einer holomorphen Funktion.\n",
        "\n",
        "* Man unterscheidet bei isolierten Singularitäten zwischen **hebbaren Singularitäten**, **Polstellen** und **wesentlichen Singularitäten**.\n",
        "\n",
        "* Es sei $\\Omega \\subseteq \\mathbb{C}$ eine offene Teilmenge, $z_{0} \\in \\Omega$. Ferner sei $f: \\Omega \\backslash\\left\\{z_{0}\\right\\} \\rightarrow \\mathbb{C}$ eine holomorphe komplexwertige Funktion. Dann heißt $z_{0}$ isolierte Singularität von $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAgJBUpT0Oo"
      },
      "source": [
        "**Klasse 1: Hebbare Singularität (Definitionslücke)**\n",
        "\n",
        "* Der Punkt $z_{0}$ heißt [hebbare Singularität](https://de.wikipedia.org/wiki/Definitionslücke), wenn $f$ auf $\\Omega$ holomorph fortsetzbar ist.\n",
        "\n",
        "* Hat Grenzwert in Form von einer Zahl bzw. nach dem riemannschen Hebbarkeitssatz, wenn $f$ in einer Umgebung von $z_{0}$ beschränkt ist.\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z)=c$\n",
        "\n",
        "> $f(z)$ beschrankt in $U\\left(z_{0}\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAm2tWA_vART"
      },
      "source": [
        "**Klasse 2: Polstelle**\n",
        "\n",
        "* im Prinzip das gleiche wie bei hebbaren Singularitaten, aber mit dem Unterschied, dass die Funktion nah an der Singularitat unbeschraenkt ist = also ins unendliche geht $\\lim _{z \\rightarrow z_{0}}|f(z)|=\\infty$\n",
        "\n",
        "Alternative Definition:\n",
        "\n",
        "> $\\lim _{z \\rightarrow z_{0}} f(z) \\cdot\\left(z-z_{0}\\right)^{k}=c \\neq 0(k \\geqslant 1)$\n",
        "\n",
        "Man multipliziert an den unendlichen Funktionswert immer oefter eine fast Null in Form des Polynoms $\\left(z-z_{0}\\right)^{k}$, bis man eine endliche komplexe Zahl als Grenzwert hat, und nicht mehr unendlich (aber nicht Null). Die Anzahl der Polynome k multipliziert mit dem Funktionswert ist der Grad / Ordnung der Polstelle.\n",
        "\n",
        "* Der Punkt $z_{0}$ heißt [Polstelle](https://de.wikipedia.org/wiki/Polstelle) oder $P o l$, wenn\n",
        "\n",
        "  * $z_{0}$ **keine hebbare Singularität ist** und\n",
        "  * es eine natürliche $\\operatorname{Zahl} k$ gibt, sodass $\\left(z-z_{0}\\right)^{k} \\cdot f(z)$ **eine hebbare Singularität bei $z_{0}$ hat**.\n",
        "\n",
        "* Ist das $k$ minimal gewählt, dann sagt man $f$ habe in $z_{0}$ einen Pol $k$ -ter Ordnung.\n",
        "\n",
        "* Man bezeichnet eine einpunktige Definitionslücke einer Funktion als Polstelle oder auch kürzer als Pol, wenn die Funktionswerte in jeder Umgebung des Punktes (betragsmäßig) beliebig groß werden.\n",
        "\n",
        "* Damit gehören die Polstellen zu den isolierten Singularitäten.\n",
        "\n",
        "* Das Besondere an Polstellen ist, dass sich die Punkte in einer Umgebung **nicht chaotisch verhalten, sondern in einem gewissen Sinne gleichmäßig gegen unendlich streben**. Deshalb können dort Grenzwertbetrachtungen durchgeführt werden.\n",
        "\n",
        "* Generell spricht man nur bei [glatten](https://de.wikipedia.org/wiki/Glatte_Funktion) (stetig & differenzierbar) oder [analytischen Funktionen](https://de.wikipedia.org/wiki/Analytische_Funktion) von Polen.\n",
        "\n",
        "*Fur reelle Funktionen: f(x)=1/x hat einen Pol erster Ordnung an der Stelle x=0**\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/9/90/GraphKehrwertfunktion.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNpaYyHVD4o"
      },
      "source": [
        "**Klasse 3: Wesentliche Singularität**\n",
        "\n",
        "* nicht hebbar und keine Polstelle, dann heißt $z_{0}$ eine wesentliche Singularität von $f$\n",
        "\n",
        "* zum Beispiel fur $f(z)=e^{-\\frac{1}{z}}, z_{0}=0$, eine Funktion die von links ins unendliche strebt, und von rechts nach Null\n",
        "  * keine hebbare Singularitaet, weil zwei verschiedene Grenzwerte existieren und eine davon nicht endlich ist.\n",
        "  * keine Polstelle, weil ein Grenzwert gleich Null ist (muss aber unbeschraenkt sein bei Polstellen)\n",
        "\n",
        "\n",
        "*Plot der Funktion $\\exp(1/z)$. Sie hat im Nullpunkt eine wesentliche Singularität (Bildmitte). Der Farbton entspricht dem komplexen Argument des Funktionswertes, während die Helligkeit seinen Betrag darstellt. Hier sieht man, dass sich die wesentliche Singularität unterschiedlich verhält, je nachdem, wie man sich ihr nähert (im Gegensatz dazu wäre ein Pol gleichmäßig weiß).*\n",
        "\n",
        "![ff](https://upload.wikimedia.org/wikipedia/commons/0/0b/Essential_singularity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSHsrLNpJDv"
      },
      "source": [
        "**Cauchysche Integralformel**\n",
        "\n",
        "* Die [cauchysche Integralformel](https://de.wikipedia.org/wiki/Cauchysche_Integralformel) ist eine der fundamentalen Aussagen der Funktionentheorie, eines Teilgebietes der Mathematik.\n",
        "\n",
        "* Sie besagt in ihrer schwächsten Form, dass die Werte einer holomorphen Funktion $f$ im Inneren einer Kreisscheibe bereits durch ihre Werte auf dem Rand dieser Kreisscheibe bestimmt sind.\n",
        "\n",
        "* Eine starke Verallgemeinerung davon ist der Residuensatz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVLX7lrIqBtX"
      },
      "source": [
        "**Residuensatz**\n",
        "\n",
        "* Der [Residuensatz](https://de.wikipedia.org/wiki/Residuensatz) ist ein wichtiger Satz der Funktionentheorie. Er stellt eine **Verallgemeinerung des cauchyschen Integralsatzes und der cauchyschen Integralformel** dar. Seine Bedeutung liegt nicht nur in den weitreichenden Folgen innerhalb der Funktionentheorie, sondern auch in der Berechnung von Integralen über reelle Funktionen.\n",
        "\n",
        "* Er besagt, dass das Kurvenintegral längs einer geschlossenen Kurve über eine bis auf isolierte Singularitäten holomorphe Funktion lediglich vom Residuum in den Singularitäten im Innern der Kurve und der Umlaufzahl der Kurve um diese Singularitäten abhängt. Anstelle eines Kurvenintegrals muss man also nur Residuen und Umlaufzahlen berechnen, was in vielen Fällen einfacher ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivq0YtFjjOPz"
      },
      "source": [
        "**Cauchyscher Integralsatz**\n",
        "\n",
        "* Der [cauchysche Integralsatz](https://de.wikipedia.org/wiki/Cauchyscher_Integralsatz)  ist einer der wichtigsten Sätze der Funktionentheorie.\n",
        "\n",
        "* Er handelt von Kurvenintegralen für holomorphe (auf einer offenen Menge komplex-differenzierbare) Funktionen.\n",
        "\n",
        "* Im Kern besagt er, dass zwei dieselben Punkte verbindende Wege das gleiche Wegintegral besitzen, falls die Funktion überall zwischen den zwei Wegen holomorph ist.\n",
        "\n",
        "* Der Satz gewinnt seine Bedeutung unter anderem daraus, dass man ihn zum Beweis der cauchyschen Integralformel und des Residuensatzes benutzt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHwO7VW3o75X"
      },
      "source": [
        "##### <font color=\"blue\">*Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JijVDdYpeTw"
      },
      "source": [
        "###### *Chaos Theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJSt90Zne8M"
      },
      "source": [
        "**Chaos Theory**\n",
        "\n",
        "* Many phenomena in nature can be described by dynamical systems; [chaos theory](https://en.m.wikipedia.org/wiki/Chaos_theory) makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n",
        "\n",
        "* [Complexity theory](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory. Chaos theory is a method of qualitative and quantitative analysis to investigate the behavior of [dynamical systems](https://en.m.wikipedia.org/wiki/Dynamical_system) that cannot be explained and predicted by single data relationships, but must be explained and predicted by whole, continuous data relationships.\n",
        "\n",
        "* Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random.\n",
        "\n",
        "* The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things:\n",
        "\n",
        "  * how much uncertainty can be tolerated in the forecast (uncertainty in a forecast increases exponentially with elapsed time)\n",
        "  * how accurately its current state can be measured,\n",
        "  * and a time scale depending on the dynamics of the system, called the [Lyapunov time](https://en.m.wikipedia.org/wiki/Lyapunov_time) (chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years)\n",
        "\n",
        "* In practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Lyapunov exponent*"
      ],
      "metadata": {
        "id": "RTdYXBryoZqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lyapunov exponent**\n",
        "\n",
        "* the Lyapunov exponent or Lyapunov characteristic exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories.\n",
        "\n",
        "* Quantitatively, two trajectories in phase space with initial separation vector $\\delta \\mathbf {Z} _{0}$ diverge (provided that the divergence can be treated within the linearized approximation) at a rate given by\n",
        "\n",
        "> ${\\displaystyle |\\delta \\mathbf {Z} (t)|\\approx e^{\\lambda t}|\\delta \\mathbf {Z} _{0}|}$\n",
        "\n",
        "where $\\lambda$ is the Lyapunov exponent.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1197.png)"
      ],
      "metadata": {
        "id": "uoo8xNAEmiam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Complex Systems*"
      ],
      "metadata": {
        "id": "fhdD4JnqZHKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on4GTn_0efWt"
      },
      "source": [
        "**Complex Systems**\n",
        "\n",
        "* [Complex Systems](https://en.m.wikipedia.org/wiki/Complex_system) is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* [Chaos](https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory) is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "*Properties:*\n",
        "\n",
        "1. **Agentenbasiert**: Komplexe Systeme bestehen aus einzelnen Teilen, die miteinander in Wechselwirkung stehen (Moleküle, Individuen, Software-Agenten etc.).\n",
        "2. **Nichtlinearität**: Kleine Störungen des Systems oder minimale Unterschiede in den Anfangsbedingungen führen oft zu sehr unterschiedlichen Ergebnissen (Schmetterlingseffekt, Phasenübergänge). Die Wirkzusammenhänge der Systemkomponenten sind im Allgemeinen nichtlinear.\n",
        "3. [**Emergenz**](https://de.wikipedia.org/wiki/Emergenz): Im Gegensatz zu lediglich komplizierten Systemen zeigen komplexe Systeme Emergenz. Entgegen einer verbreiteten Vereinfachung bedeutet Emergenz nicht, dass die Eigenschaften der emergierenden Systemebenen von den darunter liegenden Ebenen vollständig unabhängig sind. Emergente Eigenschaften lassen sich jedoch auch nicht aus der isolierten Analyse des Verhaltens einzelner Systemkomponenten erklären und nur sehr begrenzt ableiten.\n",
        "4. **Wechselwirkung** (Interaktion): Die Wechselwirkungen zwischen den Teilen des Systems (Systemkomponenten) sind lokal, ihre Auswirkungen in der Regel global.\n",
        "5. **Offenes System**: Komplexe Systeme sind üblicherweise offene Systeme. Sie stehen also im Kontakt mit ihrer Umgebung und befinden sich fern vom thermodynamischen Gleichgewicht. Das bedeutet, dass sie von einem permanenten Durchfluss von Energie bzw. Materie abhängen.\n",
        "6. **Selbstorganisation**: Dies ermöglicht die Bildung insgesamt stabiler Strukturen (Selbststabilisierung oder Homöostase), die ihrerseits das thermodynamische Ungleichgewicht aufrechterhalten. Sie sind dabei in der Lage, Informationen zu verarbeiten bzw. zu lernen.\n",
        "7. **Selbstregulation**: Dadurch können sie die Fähigkeit zur inneren Harmonisierung entwickeln. Sie sind also in der Lage, aufgrund der Informationen und derer Verarbeitung das innere Gleichgewicht und Balance zu verstärken.\n",
        "8. **Pfade**: Komplexe Systeme zeigen Pfadabhängigkeit: Ihr zeitliches Verhalten ist nicht nur vom aktuellen Zustand, sondern auch von der Vorgeschichte des Systems abhängig.\n",
        "9. **Attraktoren**: Die meisten komplexen Systeme weisen so genannte Attraktoren auf, d. h., dass das System unabhängig von seinen Anfangsbedingungen bestimmte Zustände oder Zustandsabfolgen anstrebt, wobei diese Zustandsabfolgen auch chaotisch sein können; dies sind die „seltsamen Attraktoren“ der Chaosforschung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YzAyotrq51A"
      },
      "source": [
        "* Complexity theory is rooted in chaos theory bzw: In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence.\n",
        "\n",
        "* Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order.\n",
        "\n",
        "* The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the [\"edge of chaos\"](https://en.wikipedia.org/wiki/Edge_of_chaos).\n",
        "\n",
        "* **When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails.**\n",
        "\n",
        "https://en.wikipedia.org/wiki/Complex_system#Complexity_and_chaos_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Control Theory*"
      ],
      "metadata": {
        "id": "s9qXLJO4F-JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Control_theory\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Stochastic_control\n",
        "\n",
        "Stochastic Control Problems: These problems arise in many areas of finance, including asset allocation, option pricing, and risk management. They often involve optimizing an expected utility function subject to a stochastic differential equation, and these utility functions can be non-linear, leading to high-order or even non-polynomial optimization problems."
      ],
      "metadata": {
        "id": "joMvEqsrGCsK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIwXOz85zxW_"
      },
      "source": [
        "###### *Fractal Dimensions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzaMSBJmGj_F"
      },
      "source": [
        "**Fractal Geometry**\n",
        "\n",
        "* a [fractal](https://en.m.wikipedia.org/wiki/Fractal) is a subset of Euclidean space with a fractal dimension that strictly exceeds its topological dimension. Fractals appear the same at different scales, as illustrated in successive magnifications of the Mandelbrot set.\n",
        "\n",
        "* Fractals exhibit similar patterns at increasingly smaller scales, a property called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge,it is called affine self-similar.\n",
        "\n",
        "* idealization is that everything is smooth (rebellion against calculus, differentiable, where assumption is things look smooth if you zoom in enough). Mandelbrot: nature is fractal (capture roughness)\n",
        "\n",
        "* self-similar shapes give a basis for modeling the regularity in some forms of roughness. but that doesn't mean all is only perfectly self-similar either!! (Perfect self similar are: Von Koch snowflake, Sierpensky triangle)\n",
        "\n",
        "> <font color=\"blue\">Fractal dimension: Sierpensky triangle is 1,585 dimensional, Von Koch snowflake is 1,262 dimensional, Britain coast line 1,21 dimensional, Norway: 1,52 dimensional, calm sea 2,05 dimensional, waves 2,3 dimensional</font>\n",
        "\n",
        "> **Fractals are shapes with a non-integer dimension, captures idea of roughness, but dimension can vary depending on how much you zoom in**. But a shape is considered a fractal only when the measures dimension stays approximately constant across multiple different scales.\n",
        "\n",
        "> Useful in security: **Is something fractal? Yes - then probably from nature. No - then probably man-made**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deterministic fractals**:\n",
        "  * [Julia set](https://de.m.wikipedia.org/wiki/Julia-Menge) - siehe auch [Newtonfraktal](https://de.m.wikipedia.org/wiki/Newtonfraktal). The Julia set and the Fatou set are two complementary sets (Julia \"laces\" and Fatou \"dusts\").\n",
        "  * [Mandelbrot set](https://en.m.wikipedia.org/wiki/Mandelbrot_set) (is kind of like a map of Julia sets)\n",
        "  * [Logistic map](https://de.m.wikipedia.org/wiki/Logistische_Gleichung) (Feigenbaum Attractor) with [Feigenbaum-Konstante](https://de.m.wikipedia.org/wiki/Feigenbaum-Konstante),\n",
        "    * [Vergesst den Pi-Tag, feiert lieber den Feigenbaum-Tag](https://www.spektrum.de/kolumne/die-feigenbaum-konstante-ist-die-wichtigste-groesse-der-dynamik/2120670)\n",
        "    * [Bifurcation Diagram](https://en.m.wikipedia.org/wiki/Bifurcation_diagram)\n",
        "  * Peano curve, Pentaflake, 3D Hilbert curve etc.\n",
        "  * A [fractal curve](https://en.m.wikipedia.org/wiki/Fractal_curve) is, loosely, a mathematical curve whose shape retains the same general pattern of irregularity, regardless of how high it is magnified, that is, its graph takes the form of a fractal. They do NOT have finite length. A famous example is the boundary of the Mandelbrot set. [Space-filling curves](https://en.m.wikipedia.org/wiki/Space-filling_curve) are special cases of fractal curves. Its range contains the entire 2-dimensional unit square (or more generally an n-dimensional unit hypercube). Space-filling curves in the 2-dimensional plane are sometimes called [Peano curve](https://en.m.wikipedia.org/wiki/Peano_curve). Peano curves are constructed by [Hilbert curves](https://en.m.wikipedia.org/wiki/Hilbert_curve) to form a single continuous loop over the entire sphere.\n",
        "\n",
        "\n",
        "**Random and natural fractals**:\n",
        "  * Zeros of a Wiener process,\n",
        "  * Brownian motion,\n",
        "  * [Coastline of Ireland, Great Britain or Norway](https://en.m.wikipedia.org/wiki/Coastline_paradox)  (Coastline paradox)\n",
        "  * von [Koch curve](https://de.m.wikipedia.org/wiki/Koch-Kurve) with random orientation\n",
        "  * The surface of Broccoli or human brain,\n",
        "  * Distribution of [galaxy clusters](https://en.m.wikipedia.org/wiki/Galaxy_cluster)\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1196.png)\n",
        "\n",
        "*Zeros of a Wiener process:*\n",
        "\n",
        "![gg](https://upload.wikimedia.org/wikipedia/commons/8/83/Wiener_process_set_of_zeros.gif)"
      ],
      "metadata": {
        "id": "PeB3l9rViknz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fractal Dimensions**: die [fraktale Dimension](https://de.m.wikipedia.org/wiki/Fraktale_Dimension) einer Menge ist eine Verallgemeinerung des Dimensionsbegriffs von geometrischen Objekten wie Kurven (eindimensional) und Flächen (zweidimensional).\n",
        "\n",
        "  * [Hausdorff dimension](https://de.m.wikipedia.org/wiki/Hausdorff-Dimension):  Hausdorff dimension of a single point is zero, of a line segment is 1, of a square is 2, and of a cube is 3. That is, for sets of points that define a smooth shape or a shape that has a small number of corners—the shapes of traditional geometry and science—the Hausdorff dimension is an integer agreeing with the usual sense of dimension, also known as the [topological dimension (inductive dimension)](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "  * [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "  * [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension)\n",
        "  * [Box-counting dimension](https://en.m.wikipedia.org/wiki/Minkowski–Bouligand_dimension) (Minkowski–Bouligand dimension)\n",
        "  * [List of fractals by Hausdorff dimension](https://en.m.wikipedia.org/wiki/List_of_fractals_by_Hausdorff_dimension)\n",
        "\n",
        "*Example of Box Counting dimension:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Great_Britain_Box.svg/640px-Great_Britain_Box.svg.png)\n"
      ],
      "metadata": {
        "id": "b-iO6R8pfzwE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DcbvY2o-JyH"
      },
      "source": [
        "###### *Dynamical System*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-OG2Rw8EzzH"
      },
      "source": [
        "[**Dynamisches System**](https://de.m.wikipedia.org/wiki/Dynamisches_System)\n",
        "\n",
        "> **A dynamical system involves one or more variables that change over time according to autonomous differential equations.**\n",
        "\n",
        "> $\\dot{x}=$ rate of change of $x$ as time changes\n",
        "\n",
        "> $\\dot{y}=$ rate of change of $y$ as time changes\n",
        "\n",
        "They depend on t:\n",
        "\n",
        "$\\frac{d x}{d t}=$ rate of change of $x$ as time changes $\\frac{d y}{d t}=$ rate of change of $y$ as time changes\n",
        "\n",
        "but the differential equation that describe x dot and y dot don’t actually involve time t:\n",
        "\n",
        "> $\\dot{x}=-y-0.1 x$\n",
        "\n",
        "> $\\dot{y}=x-0.4 y$\n",
        "\n",
        "This makes them autonomous. Each combination of x and y to only corresponds to one combination of x dot and y dot. You can represent it in a dynamical system called the phase space. Each point in space is a unique state of the system. And has its own rate of change shown as a vector.\n",
        "\n",
        "> <font color=\"red\">Siehe auch: [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. STS is interesting because it bridges the two major parts of mathematical physics – the [dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory) and [topological (quantum) field theories](https://en.m.wikipedia.org/wiki/Topological_quantum_field_theory). Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.</font>\n",
        "\n",
        "* [Dynamical systems theory](https://en.m.wikipedia.org/wiki/Dynamical_systems_theory). Ein (deterministisches) [dynamisches System](https://de.m.wikipedia.org/wiki/Dynamisches_System) ist ein Modell eines zeitabhängigen Prozesses, der homogen bezüglich der Zeit ist (Verlauf hängt nur vom Anfangszustand, aber nicht von der Wahl des Anfangszeitpunkts)\n",
        "\n",
        "* Wichtige Fragestellungen: Langzeitverhalten (zum Beispiel [Stabilität](https://de.wikipedia.org/wiki/Stabilitätstheorie), [Periodizität](https://de.wikipedia.org/wiki/Periodische_Funktion), [Chaos](https://de.wikipedia.org/wiki/Chaosforschung) und [Ergodizität](https://de.wikipedia.org/wiki/Ergodizität)), die [Systemidentifikation](https://de.wikipedia.org/wiki/Systemidentifikation) und ihre [Regelung](https://de.wikipedia.org/wiki/Regelung_(Natur_und_Technik))\n",
        "\n",
        "* Formal betrachte man ein **dynamisches System** bestehend aus einem topologischen Raum $X$ und einer Transformation $f: \\mathcal{T} \\times X \\longrightarrow X$, wobei $\\mathcal{T}$ ein linear geordnetes Monoid ist wie $\\mathcal{T}=\\mathbb{N}, \\mathbb{Z},[0, \\infty[$ oder $\\mathbb{R}$ und $f$ normalerweise stetig oder mindestens messbar ist (oder mindestens wird verlangt, dass $f(t, \\cdot): X \\longrightarrow X$ stetig/messbar ist für jedes $t \\in \\mathcal{T}$ ) und erfüllt $f(t+s, x)=f(t, f(s, x))$ für alle »Zeiten « $t, s \\in \\mathcal{T}$ und Punkte $x \\in X$.\n",
        "\n",
        "\n",
        "Ein dynamisches System ist ein Tripel $(T, X, \\Phi)$, bestehend aus\n",
        "\n",
        "* **Zeitraum**: einer Menge $T=\\mathbb{N}_{0}, \\mathbb{Z}, \\mathbb{R}_{0}^{+}$ oder $\\mathbb{R}$,\n",
        "* **Zustandsraum** (dem Phasenraum): einer nichtleeren Menge $X$,\n",
        "* **Operation** $\\Phi: T \\times X \\rightarrow X$ von $T$ auf $X,$\n",
        "\n",
        "so dass für alle Zustände $x \\in X$ und alle Zeitpunkte $t, s \\in T$ gilt:\n",
        "\n",
        "1. **Identitätseigenschaft**: $\\Phi(0, x)=x$\n",
        "\n",
        "2. **Halbgruppeneigenschaft**: $\\Phi(s, \\Phi(t, x))=\\Phi(s+t, x)$\n",
        "\n",
        "* Wenn $T=\\mathbb{N}_{0}$ oder $T=\\mathbb{Z}$ ist, dann heißt $(T, X, \\Phi)$ **zeitdiskret** oder kurz diskret, und mit $T=\\mathbb{R}_{0}^{+}$ oder $T=\\mathbb{R}$ nennt man $(T, X, \\Phi)$ **zeitkontinuierlich** oder kontinuierlich.\n",
        "\n",
        "* Beispiele: [Exponentielles Wachstum und Federpendel](https://de.m.wikipedia.org/wiki/Dynamisches_System#Einführende_Beispiele)\n",
        "\n",
        "  * das Strömungsverhalten von Flüssigkeiten und Gasen\n",
        "  * Bewegungen von Himmelskörpern unter gegenseitiger Beeinflussung durch die Gravitation\n",
        "  * Populationsgrößen von Lebewesen unter Berücksichtigung der Räuber-Beute-Beziehung\n",
        "  * die Entwicklung wirtschaftlicher Kenngrößen unter Einfluss der Marktgesetze.\n",
        "\n",
        "* Das Langzeitverhalten eines dynamischen Systems lässt sich durch den globalen Attraktor beschreiben, da bei physikalischen oder technischen Systemen oft **Dissipation vorliegt, insbesondere Reibung**.\n",
        "\n",
        "* Die [Determiniertheit (als Systemeigenschaft)](https://de.wikipedia.org/wiki/Systemeigenschaften#Determiniertheit) ist der Grad der „Vorbestimmtheit“ des Systems: Ein System geht von einem Zustand Z1 in den Zustand Z2 über: Z1 → Z2. Bei deterministischen Systemen ist dieser Übergang bestimmt (zwingend), bei stochastischen wahrscheinlich. Deterministische Systeme erlauben prinzipiell die Ableitung ihres Verhaltens aus einem vorherigen Zustand, stochastische Systeme nicht. **Aus der Komplexität eines Systems lässt sich keine Aussage über die Vorhersagbarkeit treffen**: Es gibt einfache deterministische Systeme, die chaotisch sind (z. B. Doppelpendel) und komplexe deterministische Systeme (Chloroplasten bei der Photosynthese).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYGXfTQpV2tS"
      },
      "source": [
        "**Phasenraum und Trajektorie**\n",
        "\n",
        "* Der [Phasenraum](https://de.wikipedia.org/wiki/Phasenraum) beschreibt die Menge aller möglichen Zustände eines dynamischen Systems. Ein Zustand wird durch einen Punkt im Phasenraum eindeutig abgebildet.\n",
        "\n",
        "* **Jeder Zustand ist ein Punkt im Phasenraum und wird durch beliebig viele [Zustandsgrößen](https://de.m.wikipedia.org/wiki/Zustandsgröße) dargestellt, welche die Dimensionen des Phasenraums bilden.**\n",
        "\n",
        "* kontinuierliche Systeme werden durch Linien (Trajektorien) repräsentiert\n",
        "* diskrete Systeme werden durch Mengen isolierter Punkte repräsentiert.\n",
        "\n",
        "* In der Mechanik besteht er aus verallgemeinerten Koordinaten (Konfigurationsraum) und zugehörigen verallgemeinerten Geschwindigkeiten, siehe [Prinzip der virtuellen Leistung](https://de.m.wikipedia.org/wiki/Prinzip_der_virtuellen_Leistung)\n",
        "\n",
        "* Die zeitliche Entwicklung eines Punktes im Phasenraum wird durch **Differentialgleichungen** beschrieben und durch **Trajektorien** (Bahnkurven, Orbit) im Phasenraum dargestellt. Dies sind **Differentialgleichungen erster Ordnung in der Zeit** und durch einen Anfangspunkt eindeutig festgelegt (ist die Differentialgleichung zeitunabhängig, sind dies **autonome Differentialgleichungen**). Dementsprechend kreuzen sich zwei Trajektorien im Phasenraum auch nicht, da an einem Kreuzungspunkt der weitere Verlauf nicht eindeutig ist. Geschlossene Kurven beschreiben oszillierende (periodische) Systeme.\n",
        "\n",
        "* *Konstruktion eines Phasen(raum)porträts für ein [mathematisches Pendel](https://de.wikipedia.org/wiki/Mathematisches_Pendel)*\n",
        "\n",
        "![hh](https://upload.wikimedia.org/wikipedia/commons/c/cd/Pendulum_phase_portrait_illustration.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m066sar7fCUi"
      },
      "source": [
        "###### *Attractor*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxw1WlN9hhlY"
      },
      "source": [
        "**Attraktor**\n",
        "\n",
        "* [Attraktor](https://de.wikipedia.org/wiki/Attraktor) ist ein Begriff aus der Theorie dynamischer Systeme und beschreibt **eine Untermenge eines Phasenraums** (d. h. eine gewisse Anzahl von Zuständen), auf die sich ein dynamisches System im Laufe der Zeit zubewegt und die unter der Dynamik dieses Systems nicht mehr verlassen wird.\n",
        "\n",
        "* Das heißt, eine Menge von Variablen nähert sich im Laufe der Zeit (asymptotisch) einem bestimmten Wert, einer Kurve oder etwas Komplexerem (also einer Region im n-dimensionalen Raum) und bleibt dann im weiteren Zeitverlauf in der Nähe dieses Attraktors.\n",
        "\n",
        "* Bekannte Beispiele sind der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor), der [Rössler-Attraktor](https://de.wikipedia.org/wiki/R%C3%B6ssler-Attraktor) und die [Nullstellen](https://de.wikipedia.org/wiki/Nullstelle) einer differenzierbaren Funktion, welche Attraktoren des zugehörigen Newton-Verfahrens sind.\n",
        "\n",
        "> **Die Menge aller Punkte des Phasenraums, die unter der Dynamik demselben Attraktor zustreben, heißt Attraktions- oder Einzugsgebiet dieses Attraktors**.\n",
        "\n",
        "\n",
        "Unter einem Attraktor versteht man eine Teilmenge $A \\subseteq X$,\n",
        "die den folgenden Bedingungen genügt\n",
        "1. $A$ ist vorwärts invariant;\n",
        "2. Das Sammelbecken $B(A)$ ist eine Umgebung von $A$;\n",
        "3. $A$ ist eine minimale nicht leere Teilmenge von $X$ mit Bedingungen 1\n",
        "und 2 .\n",
        "\n",
        "Bedingung 1 erfordert eine gewisse Stabilität des Attraktors. Daraus folgt offensichtlich, dass $A \\subseteq B(A)$. Anhand Bedingung 2 wird weiterhin verlangt, dass $A \\subseteq B(A)^{\\circ}$ und bedeutet u. a., jeder Punkt in einer gewissen Nähe von $A$ nähere sich dem Attraktor beliebig. Manche Autoren lassen Bedingung 2 weg. Bedingung 3 erfordert, dass der Attraktor nicht in weitere Komponenten zerlegt werden kann (ansonsten\n",
        "wäre bspw. der ganze Raum trivialerweise ein Attraktor).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cai3DPiwy4"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine ganzzahlige Dimension besitzen**\n",
        "\n",
        "*Bei der Untersuchung dynamischer Systeme interessiert man sich – ausgehend von einem bestimmten [Anfangszustand (=Anfangsbedingung)](https://de.wikipedia.org/wiki/Anfangsbedingung) – vor allem für das Verhalten für $t\\to \\infty$ . Der Grenzwert in diesem Fall wird als Attraktor bezeichnet. Typische und häufige Beispiele von Attraktoren sind:*\n",
        "\n",
        "* **asymptotisch stabile Fixpunkte**: Das System nähert sich immer stärker einem bestimmten Endzustand an, in dem die Dynamik erliegt; ein statisches System entsteht. Typisches Beispiel ist ein gedämpftes Pendel, das sich dem Ruhezustand im tiefsten Punkt annähert.\n",
        "\n",
        "* **(asymptotisch) stabile Grenzzyklen**: Der Endzustand ist die Abfolge gleicher Zustände, die periodisch durchlaufen werden (periodische Orbits). Ein Beispiel dafür ist die Simulation der Räuber-Beute-Beziehung, die für bestimmte Parameter der Rückkoppelung auf ein periodisches Ansteigen und Sinken der Populationsgrößen hinausläuft.\n",
        "\n",
        "* Für ein hybrides dynamisches System mit chaotischer Dynamik konnte im $\\mathbb {R} ^{n}$ die Oberfläche eines [n-Simplex](https://de.wikipedia.org/wiki/Simplex_(Mathematik)) als Attraktor identifiziert werden: (asymptotisch stabile) Grenztori: Treten mehrere miteinander inkommensurable Frequenzen auf, so ist die Trajektorie nicht geschlossen, und der Attraktor ist ein Grenztorus, der von der Trajektorie asymptotisch vollständig ausgefüllt wird. Die zu diesem Attraktor korrespondierende Zeitreihe ist quasiperiodisch, d. h., es gibt keine echte Periode, aber das Frequenzspektrum besteht aus scharfen Linien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMS8uZlNlFTx"
      },
      "source": [
        "**Attraktoren, die im Phasenraum eine fraktale Dimension besitzen**\n",
        "\n",
        "*Die Existenz von Attraktoren mit komplizierterer Struktur war zwar schon länger bekannt, man betrachtete sie aber zunächst als instabile Sonderfälle, deren Auftreten nur bei bestimmter Wahl des Ausgangszustands und der Systemparameter beobachtet wird. Dies änderte sich mit der Definition eines neuen, speziellen Typs von Attraktor:*\n",
        "\n",
        "* [Seltsamer Attraktor](https://de.wikipedia.org/wiki/Seltsamer_Attraktor): In seinem Endzustand zeigt das System häufig ein chaotisches Verhalten (es gibt jedoch auch Ausnahmen, z. B. quasiperiodisch angetriebene nichtlineare Systeme).  Ein seltsamer Attraktor ist ein Attraktor, also ein Ort im Phasenraum, der den Endzustand eines dynamischen Prozesses darstellt, **dessen fraktale Dimension nicht ganzzahlig und dessen Kolmogorov-Entropie echt positiv ist**. Es handelt sich damit um ein Fraktal, das nicht in geschlossener Form geometrisch beschrieben werden kann. Gelegentlich wird auch der Begriff chaotischer Attraktor bevorzugt, da die „Seltsamkeit“ dieses Objekts sich mit den Mitteln der Chaostheorie erklären lässt. Der dynamische Prozess zeigt ein aperiodisches Verhalten.\n",
        "\n",
        "    * [Lorenz Attraktor](https://de.m.wikipedia.org/wiki/Lorenz-Attraktor)\n",
        "\n",
        "    * [Rössler attractor](https://en.m.wikipedia.org/wiki/Rössler_attractor)\n",
        "\n",
        "  * [Multiscroll attractor](https://en.m.wikipedia.org/wiki/Multiscroll_attractor)\n",
        "\n",
        "  * [Hénon map](https://en.m.wikipedia.org/wiki/Hénon_map)\n",
        "\n",
        "* Der seltsame Attraktor lässt sich **nicht in einer geschlossenen geometrischen Form** beschreiben und **besitzt keine ganzzahlige Dimension**. Attraktoren nichtlinearer dynamischer Systeme weisen dann eine **fraktale Struktur** auf.\n",
        "\n",
        "* Wichtiges Merkmal ist das chaotische Verhalten, d. h., jede noch so geringe Änderung des Anfangszustands führt im weiteren Verlauf zu signifikanten Zustandsänderungen. Prominentestes Beispiel ist der **Lorenz-Attraktor, der bei der Modellierung von Luftströmungen in der Atmosphäre entdeckt wurde**.\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Attractor#Fixed_point\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Limit_cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxAvwOlbmQn8"
      },
      "source": [
        "**Special: Lorenz-Attraktor**\n",
        "\n",
        "Der [Lorenz-Attraktor](https://de.wikipedia.org/wiki/Lorenz-Attraktor) oder englisch: [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) ist der seltsame Attraktor eines Systems von drei gekoppelten, nichtlinearen **gewöhnlichen Differentialgleichungen**:\n",
        "\n",
        "$\\dot{X}=a(Y-X)$\n",
        "\n",
        "$\\dot{Y}=X(b-Z)-Y$\n",
        "\n",
        "$\\dot{Z}=X Y-c Z$\n",
        "\n",
        "* Formuliert wurde das System um 1963 von dem Meteorologen Edward N. Lorenz, der es als Idealisierung eines [hydrodynamischen Systems (Fluiddynamik)](https://de.wikipedia.org/wiki/Fluiddynamik) entwickelte. Basierend auf einer Arbeit von Barry Saltzman (1931–2001) ging es Lorenz dabei um eine Modellierung der Zustände in der Erdatmosphäre zum Zweck einer Langzeitvorhersage.\n",
        "\n",
        "* Allerdings betonte Lorenz, dass das von ihm entwickelte System allenfalls für sehr begrenzte Parameterbereiche von $a,b,c$ realistische Resultate liefert.\n",
        "\n",
        "* Die mathematische Beschreibung des Modells durch die Navier-Stokes-Gleichungen führt über verschiedene Vereinfachungen, beispielsweise endlich abgebrochene Reihendarstellungen, zu dem oben angegebenen Gleichungssystem.\n",
        "\n",
        "* Die numerische Lösung des Systems zeigt bei bestimmten Parameterwerten deterministisch chaotisches Verhalten, die Trajektorien folgen einem seltsamen Attraktor. Damit spielt der Lorenzattraktor für die mathematische Chaostheorie eine Rolle, denn die Gleichungen stellen wohl eines der einfachsten Systeme mit chaotischem Verhalten dar.\n",
        "\n",
        "* Die typische Parametereinstellung mit chaotischer Lösung lautet: $a=10,b=28$ und $c=8/3$, wobei\n",
        "  * $a$ mit der [Prandtl-Zahl](https://de.wikipedia.org/wiki/Prandtl-Zahl) (=dimensionslose Kennzahl von Fluiden, das heißt von Gasen oder tropfbaren Flüssigkeiten. Sie ist definiert als Verhältnis zwischen kinematischer Viskosität und Temperaturleitfähigkeit. Die Prandtl-Zahl stellt die Verknüpfung des Geschwindigkeitfeldes mit dem Temperaturfeld eines Fluids dar.)\n",
        "  * $b$ mit der [Rayleigh-Zahl](https://de.wikipedia.org/wiki/Rayleigh-Zahl) (=eine dimensionslose Kennzahl, die den Charakter der Wärmeübertragung innerhalb eines Fluids beschreibt) identifiziert werden kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3xyzZzNOEhv"
      },
      "source": [
        "###### *Summary: Attractor, Trajectory in phase space, Lyapunov exponent (forecast error)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIujOlBtNqx-"
      },
      "source": [
        "For this specific system shown, the vector field looks like this, and let scatter a bunch of random points around to represent different possible states see how they evolve and move around: They all spiral towards the center.\n",
        "\n",
        "For this exampe the attraction is zero, and the basin is every point in space. Notice that at the origin x dot and y dot also equal zero.\n",
        "\n",
        "$\\dot{x}=-(0)-0.1(0)=0$\n",
        "\n",
        "$\\dot{y}=(0)-0.4(0)=0$\n",
        "\n",
        "The origin is in this case **fixed point (attractor)**, because every point in there will stay forever. It seem like an inevitability, hence determinism (as chaos is).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkfT8XP8QmJd"
      },
      "source": [
        "But there are also other types of attractors, like the **Van der Pol oscillator**:\n",
        "\n",
        "> $\\dot{x}=\\mu\\left(x-\\frac{1}{3} x^{3}-y\\right)$\n",
        "\n",
        "> $\\dot{y}=\\frac{1}{\\mu} x$\n",
        "\n",
        "It creates something called: \"**Limit Cycle Attractor**\":\n",
        "\n",
        "* it doesn't end up in a point\n",
        "\n",
        "* typically appear in physical systems with some sort of oscillation (electrical circuits, tectonic plates, etc).\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luo2ipo2RaUH"
      },
      "source": [
        "**Strange Attractor** (has a fractal dimension)\n",
        "\n",
        "Lorenz Attractor: Lorenz system describes convection cycles in the atmoshphere. Very sensitive to changes in initial conditions already after a short time:\n",
        "\n",
        "> $\\dot{x}=\\sigma(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(\\rho-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\beta z$\n",
        "\n",
        "The Lorenz equations have a few parameters that can be tweaked to alter the behavior of a system. This is what is known as ‘strange attractor’:\n",
        "\n",
        "> $\\dot{x}=10(y-x)$\n",
        "\n",
        "> $\\dot{y}=x(28-z)-y$\n",
        "\n",
        "> $\\dot{z}=x y-\\frac{8}{3} z$\n",
        "\n",
        "1. No point in the space is ever visited more than once by the same trajectory - it that happens, the trajectory would travel in a predictable loop.\n",
        "2. And no 2 trajectories will ever intersect. If that happened, they would merge into the same path, giving two different sets of initial conditions the same outcome.\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_03.png)\n",
        "\n",
        "A single trajectory will visit an infinite number of points in this limited space, and this limited space will have an infinite number of trajectories.\n",
        "\n",
        "* Trajectories are just curved, so they should be 1-dimensional..normally!\n",
        "* For the strange attractor: no matter how much you zoom in on this attractor, you can always find more and more trajectories everywhere.\n",
        "* That’s why this attractor is said to have a **non integer dimension** - it’s **made up of infinite long curves in a finite space, which are so detailed, that they start to partially fill up higher dimensions**. It’s not 1, 2 or 3 dimensional, its somewhere in between. (=detail at arbitrarily small scales)\n",
        "\n",
        "Conclusion: Lorenz attractor is a fractal space, and hence a strange attractor.\n",
        "\n",
        "Even is both points started at the same point, small initial differences can bring them to complete different paths:\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chros_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOAWTDsoV7-F"
      },
      "source": [
        "The difference in trajectories increases exponentially:\n",
        "\n",
        "**$\\lambda$ stands for Lyapunov exponent**\n",
        "\n",
        "$\\lambda$ > 0, trajectories will increase exponentially (= chaotic)\n",
        "\n",
        "$\\lambda$ = 0, distance will stay constant\n",
        "\n",
        "$\\lambda$ < 0, distance will converge to zero.\n",
        "\n",
        "*Unfortunately there is no way to find the Lyapunov exponent only by looking at the equations. It is measured by running the simulation, keeping track of mainy pairs of trajectories, and find the average rate of change in their distance. But it provides a simple metric to comunicate how chaotic a system is*\n",
        "\n",
        "![gg](https://raw.githubusercontent.com/deltorobarba/repo/master/chaos_06.png)\n",
        "\n",
        "**For the Lorenz attractor it is $\\lambda$ ≈ 0.9**\n",
        "\n",
        "Used to find out duration in which the predictions are valid: \"Predictability Horizon\". We get this by re-arranging our previous equation $d_{t}=d_{0} e^{\\lambda t}$:\n",
        "\n",
        "> Predictability horizon $=\\frac{1}{\\lambda} \\ln \\frac{a}{d_{0}}$\n",
        "\n",
        "$d_{0}=$ initial error\n",
        "\n",
        "$a=$ maximum allowed error\n",
        "\n",
        "**For the Lorenz attractor, after 10 time steps, any error would have multiplied by 8,000 already.**\n",
        "\n",
        "Example what that means with this exponential divergence:\n",
        "\n",
        "We have a simulation that predicts where ocean currents flow, and you want to keep the error less than 1,000 km.\n",
        "* If you ran it twice, once an initial error of 1m and once the initial error was a million times smaller than one micrometer (0,000001m), how much longer  the simulation with the smaller error would stay below the margin of error.\n",
        "* The simulation would be valid 9 days instead of 3 days, but for an initial error that is a million times smaller !!\n",
        "\n",
        "Let’s write the expressions for the two predictability horizons, and put them in a fraction:\n",
        "\n",
        "> $\\frac{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{0.000001}\\right)}{\\left(\\frac{1}{\\lambda} \\ln \\frac{1000}{1}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(\\frac{1}{\\lambda} \\ln 10^{9}\\right)}{\\left(\\frac{1}{\\lambda} \\ln 10^{3}\\right)}$\n",
        "\n",
        "> = $\\frac{\\left(9 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}{\\left(3 \\cdot \\frac{1}{\\lambda} \\ln 10\\right)}$\n",
        "\n",
        "> = $\\frac{9}{3}$.\n",
        "\n",
        "= It’s 3 times longer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DTHRJEKOJbkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">**Stochastic 250**"
      ],
      "metadata": {
        "id": "82Lb_gBqUzi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Stochastic*"
      ],
      "metadata": {
        "id": "KaRe47oJEybG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Probability Theory*"
      ],
      "metadata": {
        "id": "XgUdyonIRpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions that belong to the category \"distance-based\" are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.\n",
        "\n",
        "> Great overview: http://juliaml.github.io/LossFunctions.jl/stable/losses/distance/\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/regression_loss.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "bmOIiW0sRpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Linear) Least Squares**\n",
        "\n",
        "* Least Squares: Deren Parameter werden so bestimmt, dass die Summe der Abweichungsquadrate e der Beobachtungen y von den Werten der Funktion minimiert wird.\n",
        "\n",
        "* Da die Kleinste-Quadrate-Schätzung die Residuenquadratsumme minimiert, ist es dasjenige Schätzverfahren, welches das [Bestimmtheitsmaß](https://de.wikipedia.org/wiki/Bestimmtheitsmaß) maximiert.\n",
        "\n",
        "* Das Bestimmtheitsmaß der Regression, auch empirisches Bestimmtheitsmaß, ist eine dimensionslose Maßzahl die den Anteil der Variabilität in den Messwerten der abhängigen Variablen ausdrückt, der durch das lineare Modell „erklärt“ wird. Mithilfe dieser Definition können die Extremwerte für das Bestimmtheitsmaß aufgezeigt werden. Für das\n",
        "Bestimmtheitsmaß gilt, dass es umso năher am Wert 1 ist, je kleiner die Residuenquadratsumme ist. Es wird maximal gleich 1 wenn $\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=0$ ist, also alle Residuen null sind. In diesem Fall ist die Anpassung an die Daten perfekt, was bedeutet, dass für jede Beobachtung $y_{i}=\\hat{y}_{i}$ ist.\n",
        "\n",
        "* [Least Squares](https://en.wikipedia.org/wiki/Least_squares) / [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) & [Linear Least Squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
      ],
      "metadata": {
        "id": "ybpZTameRpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gauss–Markov theorem (BLUE)**\n",
        "\n",
        "*  states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, **if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero**.\n",
        "\n",
        "* stellt eine theoretische Rechtfertigung der Methode der kleinsten Quadrate dar\n",
        "\n",
        "* Der Satz besagt, dass in einem linearen Regressionsmodell, in dem die **Störgrößen (error term) einen Erwartungswert von null und eine konstante Varianz haben sowie unkorreliert sind** (Annahmen des klassischen linearen Regressionsmodells), der Kleinste-Quadrate-Schätzer – vorausgesetzt er existiert – ein bester linearer erwartungstreuer Schätzer ist (englisch Best Linear Unbiased Estimator, kurz: BLUE).\n",
        "\n",
        "* Hierbei bedeutet der „beste“, dass er – innerhalb der Klasse der linearen erwartungstreuen Schätzer – die „kleinste“ Kovarianzmatrix aufweist und somit minimalvariant ist. Die Störgrößen müssen nicht notwendigerweise normalverteilt sein. Sie müssen im Fall der verallgemeinerten Kleinste-Quadrate-Schätzung auch nicht unabhängig und identisch verteilt sein.\n",
        "\n",
        "The Gauss-Markov assumptions concern the set of error random variables, $\\varepsilon_{i}:$\n",
        "\n",
        "1. They have mean zero: $\\mathrm{E}\\left[\\varepsilon_{i}\\right]=0$\n",
        "\n",
        "2. They are homoscedastic, that is all have the same finite variance: $\\operatorname{Var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}<\\infty$ for all $i$,\n",
        "3. Distinct error terms are uncorrelated: $\\operatorname{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0, \\forall i \\neq j$.\n",
        "\n",
        "A linear estimator of $\\beta_{j}$ is a linear combination $\\widehat{\\beta}_{j}=c_{1 j} y_{1}+\\cdots+c_{n j} y_{n}$\n",
        "\n",
        "* The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n",
        "\n",
        "* The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the [James–Stein estimator](https://en.wikipedia.org/wiki/James–Stein_estimator) (which also drops linearity), [ridge regression(Tikhonov_regularization)](https://en.wikipedia.org/wiki/Tikhonov_regularization), or simply any [degenerate estimator](https://en.wikipedia.org/wiki/Degenerate_distribution).\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Gauss–Markov_theorem"
      ],
      "metadata": {
        "id": "l0xBPcNPRpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "* Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.\n",
        "\n",
        "* “Ordinary Least Squares” (OLS) method is used to find the best line intercept (b) and the slope (m). [in y = mx + b, m is the slope and b the intercept]\n",
        "\n",
        "\n",
        "> $m=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
        "\n",
        "> $b=\\bar{y}-m * \\bar{x}$\n",
        "\n",
        "* In other words → with OLS Linear Regression the goal is to find the line (or hyperplane) that minimizes the vertical offsets. We define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
        "\n",
        "* OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
        "\n",
        "* The OLS method minimizes the sum of squared residuals, and leads to a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression) for the estimated value of the unknown parameter vector β.\n",
        "\n",
        "* It is important to point out though that OLS method will work for a univariate dataset (ie., single independent variables and single dependent variables). Multivariate dataset contains a single independent variables set and multiple dependent variables sets, requiring a machine learning algorithm called “Gradient Descent”.\n",
        "\n",
        "* [Wiki](https://en.wikipedia.org/wiki/Ordinary_least_squares) & [Medium](https://medium.com/@jorgesleonel/linear-regression-307937441a8b)"
      ],
      "metadata": {
        "id": "Io8XPm78RpMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Least Squares (WLS)**\n",
        "\n",
        "* are used when heteroscedasticity is present in the error terms of the model.\n",
        "* https://en.wikipedia.org/wiki/Weighted_least_squares"
      ],
      "metadata": {
        "id": "BhP9nZcTRpMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized Least Squares (GLS)**\n",
        "\n",
        "* is an extension of the OLS method, that **allows efficient estimation of β when either heteroscedasticity, or correlations, or both are present among the error terms of the model**, as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
        "\n",
        "* To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ith case is inversely proportional to var(εi). This special case of GLS is called \"weighted least squares\".\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Generalized_least_squares"
      ],
      "metadata": {
        "id": "nNbrVPssRpMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE, SAE & SSE**\n",
        "\n",
        "**Sum of Errors (SE)** the difference in the predicted value and the actual value.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma(\\hat{Y}-Y)$\n",
        "\n",
        "Errors terms cancel each other out.\n",
        "\n",
        "**Sum of Absolute Errors (SAE)** takes the absolute values of the errors for all iterations.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma (|\\hat{Y}-Y|)$\n",
        "\n",
        "This loss function is not differentiable at 0.\n",
        "\n",
        "**Sum of Squared Errors (SSE)** is differentiable at all points and gives non-negative errors. But you could argue that why cannot we go for higher orders like 4th order or so. Then what if we consider to take 4th order loss function, which would look like:\n",
        "\n",
        "$\\mathbf{L}=\\left[\\Sigma(\\hat{Y}-Y)^{2}\\right]$\n",
        "\n",
        "The gradient of the loss function will vanish at minima & maxima. And the error will grow with the sample size.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/sumoferrors.png)\n",
        "\n",
        "* Minimizing Sum of Squared Errors / SSE ([wiki](https://de.m.wikipedia.org/wiki/Residuenquadratsumme) and [medium](https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511)).  We can think of the SSE loss as the (unscaled) variance of the model errors.\n",
        "* Therefore **minimizing the SEE loss is equivalent to minimizing the variance of the model residuals**. For this reason, the sum of squares loss is often referred to as the Residual Sum of Squares error (RSS) for linear models. We can think of minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.\n",
        "* Ideal when distribution of residuals in normal: the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the [best unbiased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) for the parameters .\n",
        "* Problem: Because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss\n"
      ],
      "metadata": {
        "id": "xG8zVgjpRpMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE (L2) & RMSE (Squared Euclidean Distance)**\n",
        "\n",
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis.\n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/3d-function-2.svg/566px-3d-function-2.svg.png)\n",
        "\n",
        "*A paraboloid, the graph of squared Euclidean distance from the origin*\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/3d-function-5.svg/566px-3d-function-5.svg.png)\n",
        "\n",
        "*A cone, the graph of Euclidean distance from the origin in the plane*"
      ],
      "metadata": {
        "id": "siZNV3XNRpMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "$\\mathrm{MSE}={\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Mean Squared Error (L2 or Quadratic Loss). Error decreases as we increase our sample data as the distribution of our data becomes more and more narrower (referring to normal distribution). The more data we have, the less is the error.\n",
        "* Can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better. It is always non – negative and values close to zero are better. The MSE is the second moment of the error (about the origin) and thus incorporates both the variance of the estimator and its bias.\n",
        "* Problem: Sensitive to outliers and the order of loss is more than that of the data. As my data is of order 1 and the loss function, MSE has an order of 2 (squared). So we cannot directly correlate data with the error.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate)\n",
        "\n",
        "**Mean Squared Logarithmic Error (MSLR)**\n",
        "\n",
        "* Mean Squared Logarithmic Error\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\n",
        "\n",
        "**RMSE** (Root-Mean-Square Error)\n",
        "\n",
        "$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Root-Mean-Square Error is the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
        "* The **RMSE is directly interpretable in terms of measurement units**, and so is a better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit. Metric can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable\n",
        "* https://www.sciencedirect.com/science/article/pii/S096014811831231X\n",
        "* The **RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian**.\n",
        "https://www.geosci-model-dev-discuss.net/7/C473/2014/gmdd-7-C473-2014-supplement.pdf\n",
        "* When both metrics are calculated, the MAE tends to be much smaller than the RMSE because the RMSE penalizes large errors while the MAE gives the same weight to all errors.\n",
        "* They summarized that the **RMSE tends to become increasingly larger than the MAE** (but not necessarily in a monotonic fashion) as the distribution of error magnitudes becomes more variable. The RMSE tends to 1 grow larger than the MAE with n2 since its lower limit is fixed at the MAE and its upper 11 limit (n2 · MAE) increases with n2 .\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Root-mean-square_deviation) & [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)"
      ],
      "metadata": {
        "id": "L_hXqjauRpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE (L1) & MAPE**\n",
        "\n",
        "$\\mathrm{MAE}=\\frac{1}{n} \\sum_{j=1}^{n}\\left|y_{j}-\\hat{y}_{j}\\right|$\n",
        "\n",
        "* If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. MBE can convey useful information, but should be interpreted cautiously because positive and negative errors will cancel out.\n",
        "\n",
        "* Mean Absolute Error (L1 Loss)\n",
        "* Computes the mean of absolute difference between labels and predictions\n",
        "* measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "* On some regression problems, the **distribution of the target variable may be mostly Gaussian, but may have outliers**, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "* Metric can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Extremwerte als Ausreißer mit geringerem Einfluss auf das Modell ansehen: MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously receive unrealistically huge negative/positive values in our training environment, but not our testing environment).\n"
      ],
      "metadata": {
        "id": "6WWtt8-3RpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE vs MSE**\n",
        "\n",
        "* One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values.\n",
        "\n",
        "* This isn’t good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate.\n",
        "\n",
        "* The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/mae_vs_mse.PNG)"
      ],
      "metadata": {
        "id": "MsW3oJwSRpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "$\\mathrm{M}=\\frac{1}{n} \\sum_{t=1}^{n}\\left|\\frac{A_{t}-F_{t}}{A_{t}}\\right|$\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is a statistical measure of **how accurate a forecast** system is.\n",
        "\n",
        "* It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values. Where At is the actual value and Ft is the forecast value.\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is the most common measure used to forecast error, and works best if there are no extremes to the data (and no zeros).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "-6ZmPJnMRpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
        "\n",
        "* There are 3 different definitions of sMAPE. Two of them are below:\n",
        "\n",
        "$\\operatorname{SMAPE}=\\frac{100 \\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(\\left|A_{t}\\right|+\\left|F_{t}\\right|\\right) / 2}$\n",
        "\n",
        "* Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
        "\n",
        "* At is the actual value and Ft is the forecast value\n",
        "\n",
        "* The absolute difference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n",
        "\n",
        "* Armstrong's original definition is as follows:\n",
        "\n",
        "$\\mathrm{SMAPE (old)}=\\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(A_{t}+F_{t}\\right) / 2}$\n",
        "\n",
        "* The problem is that it can be negative (if ${\\displaystyle A_{t}+F_{t}<0}$) or even undefined (if ${\\displaystyle A_{t}+F_{t}=0}$). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator.\n",
        "\n",
        "* In contrast to the mean absolute percentage error, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator)\n",
        "\n",
        "* One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the second SMAPE formula:\n",
        "\n",
        "  * Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76%\n",
        "\n",
        "  * Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) & [Wiki2](https://wiki2.org/en/Symmetric_mean_absolute_percentage_error) & [other](https://www.brightworkresearch.com/the-problem-with-using-smape-for-forecast-error-measurement/)"
      ],
      "metadata": {
        "id": "woNJbBtORpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean absolute scaled error (MASE)**\n",
        "\n",
        "* mean absolute scaled error (MASE) is a measure of the accuracy of forecasts.\n",
        "\n",
        "*  It is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast. It was proposed in 2005.\n",
        "\n",
        "* The mean absolute scaled error has the following desirable propertie: [Wiki](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error)"
      ],
      "metadata": {
        "id": "0BjFXbE5RpMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss (Smooth Mean Absolute Error)**\n",
        "\n",
        "* TLDR: will better find a minimum than L1, but less exposed to outliers than L2. However one has to tune the hyperparameter delta. The larger (3+), the more it is L2, the smaller (1), the more it is L1.\n",
        "\n",
        "* The Huber loss **combines the best properties of MSE and MAE** (Mean Absolute Error). It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter.\n",
        "\n",
        "* It's **less sensitive to outliers** in data than the squared error loss. It’s **also differentiable at 0**. It’s basically absolute error, which becomes quadratic when error is small.  How small that error has to be to make it quadratic depends on a hyperparameter 𝛿.\n",
        "\n",
        "* Once differentiable.\n",
        "\n",
        "$L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
        "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
        "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
        "\\end{array}\\right.$\n",
        "\n",
        "* **Huber loss approaches MSE when 𝛿 ~ 0 and MAE when 𝛿 ~ ∞**\n",
        "\n",
        "* The choice of delta is critical because it determines what you’re willing to consider as an outlier. Residuals larger than delta are minimized with L1 (which is less sensitive to large outliers), while residuals smaller than delta are minimized “appropriately” with L2.\n",
        "\n",
        "* One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n",
        "Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, **it combines good properties from both MSE and MAE**.\n",
        "\n",
        "* However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Huber_loss) * [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
        "\n",
        "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
        "\n",
        "* The biggest problem with using MAE to train neural networks is the constant large gradient, which may cause the minimum point to be missed when the gradient descent is about to end. For MSE, the gradient will decrease as the loss decreases, making the result more accurate.\n",
        "\n",
        "* In this case, Huber loss is very useful. It will fall near the minimum value due to the decreasing gradient. It is more robust to outliers than MSE. Therefore, Huber loss combines the advantages of MSE and MAE. However, the problem with Huber loss is that we may need to constantly adjust the hyperparameters\n",
        "\n",
        "* https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "* When you compare this statement with the benefits and disbenefits of both the MAE and the MSE, you’ll gain some insights about how to adapt this delta parameter:\n",
        "\n",
        "* **If your dataset contains large outliers**, it’s likely that your model will not be able to predict them correctly at once. In fact, it might take quite some time for it to recognize these, if it can do so at all. This results in large errors between predicted values and actual targets, because they’re outliers. Since MSE squares errors, large outliers will distort your loss value significantly. If outliers are present, you likely don’t want to use MSE. Huber loss will still be useful, but you’ll have to use small values for 𝛿.\n",
        "\n",
        "* If it does not contain many outliers, it’s likely that it will generate quite accurate predictions from the start – or at least, from some epochs after starting the training process. In this case, you may observe that the errors are very small overall. Then, one can argue, it may be worthwhile to let the largest small errors contribute more significantly to the error than the smaller ones. In this case, MSE is actually useful; hence, with Huber loss, you’ll likely want to use quite large values for 𝛿.\n",
        "\n",
        "* If you don’t know, you can always start somewhere in between – for example, in the plot above, 𝛿 = 1 represented MAE quite accurately, while 𝛿 = 3 tends to go towards MSE already. What if you used 𝛿 = 1.5 instead? You may benefit from both worlds.\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/\n",
        "\n",
        "* For target = 0, the loss increases when the error increases. However, the speed with which it increases depends on this 𝛿 value. In fact, Grover (2019) writes about this as follows: Huber loss approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞ (large numbers.)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/320px-Huber_loss.svg.png)\n",
        "\n",
        "*Huber loss (green,\n",
        "δ\n",
        "=\n",
        "1) and squared error loss (blue) as a function of\n",
        "y\n",
        "−\n",
        "f\n",
        "(\n",
        "x\n",
        ")*\n",
        "\n",
        "![huber](https://raw.githubusercontent.com/deltorobarba/repo/master/huberloss.jpg)"
      ],
      "metadata": {
        "id": "TgFnTKWvRpMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-Cosh Loss**\n",
        "\n",
        "* TLDR: Similar to MAE, will not be affected by outliers. Log-Cosh has all the points of Huber loss, and no need to set hyperparameters. Compared with Huber, Log-Cosh derivation is more complicated, requires more computation, and is not used much in deep learning.\n",
        "\n",
        "* * Log-cosh is another function used in regression tasks that’s smoother than L2 (is smoothed towards large errors (presumably caused by outliers) so that the final error score isn’t impacted thoroughly.)\n",
        "* Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. “Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.” (Grover, 2019). Oops, that’s not intuitive but nevertheless quite important – this is the maths behind Logcosh loss:\n",
        "\n",
        "> $\\log \\cosh (t)=\\sum_{p \\in P} \\log (\\cosh (p-t))$\n",
        "\n",
        "* Similar to Huber Loss, but twice differentiable everywhere\n",
        "* [Wiki Hyperbolic Functions](https://en.m.wikipedia.org/wiki/Hyperbolic_functions), [TF Class](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh), [Machinecurve](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "* However, Log-Cosh is second-order differentiable everywhere, which is still very useful in some machine learning models. For example, XGBoost uses Newton's method to find the best advantage. Newton's method requires solving the second derivative (Hessian). Therefore, for machine learning frameworks such as XGBoost, the second order of the loss function is differentiable. But the Log-cosh loss is not perfect, and there are still some problems. For example, if the error is large, the first step and Hessian will become fixed, which leads to the lack of split points in XGBoost.\n",
        "\n",
        "https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "![logcosh](https://raw.githubusercontent.com/deltorobarba/repo/master/logcosh.jpeg)"
      ],
      "metadata": {
        "id": "BS-jajamRpMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile Loss (Pinball Loss)**\n",
        "\n",
        "* TLDR: for Heteroskedastizität, i.e. for risk management when variance changes. Quantile Loss, you can set different quantiles to control the proportion of overestimation and underestimation in loss.\n",
        "\n",
        "* estimates conditional “quantile” of a response variable given certain values of predictor variables\n",
        "* is an extension of MAE (**when quantile is 50th percentile, it’s MAE**)\n",
        "* Im Gegensatz zur Kleinste-Quadrate-Schätzung, die den Erwartungswert der Zielgröße schätzt, ist die Quantilsregression dazu geeignet, ihre Quantile zu schätzen.\n",
        "* Fitting models for many percentiles, you can estimate the entire conditional distribution. Often, the answers to important questions are found by modeling percentiles in the tails of the distribution. For that reason **quantile regression provides critical insights in financial risk management & fraud detection**.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Quantilsregression), [TF Class](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/PinballLoss) & [TF Function](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/pinball_loss)\n",
        "\n",
        "* project where predictions were subject to high uncertainty. The client required for their decision to be driven by both the predicted machine learning output and a measure of the potential prediction error. The quantile regression loss function solves this and similar problems by replacing a single value prediction by prediction intervals.\n",
        "\n",
        "* The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
        "\n",
        "* For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.\n",
        "\n",
        "* The usual regression algorithm is to fit the expected or median training data, and the quantile loss function can be used to fit different quantiles of training data by giving different quantiles.\n",
        "\n",
        "![sdd](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss.jpg)\n",
        "\n",
        "* Set different quantiles to fit different straight lines: This function is a piecewise functio., γ is the quantile coefficient. y is the true value, f(x) is the predicted value. According to the size of the predicted value and the true value, there are two cases to consider.\n",
        "\n",
        "* y> f(x) For overestimation, the predicted value is greater than the true value;\n",
        "\n",
        "* y< f(x) to underestimate, the predicted value is smaller than the real value.\n",
        "\n",
        "* Use different pass coefficients to control the weight of overestimation and underestimation in the entire loss value.\n",
        "\n",
        "* Especially when γ=0.5 When the quantile loss degenerates into the mean absolute error MAE, **MAE can also be regarded as a special case of quantile loss-median loss**. The picture below is taken with different median points [0.25,0.5,0.7] Obtaining different quantile loss function curves can also be seen as MAE at 0.5.\n",
        "\n",
        "![fgfgf](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss2.jpg)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Pinball_Loss_Function.svg/320px-Pinball_Loss_Function.svg.png)\n",
        "\n",
        "*Pinball-Verlustfunktion mit\n",
        "τ\n",
        "=0,9. Für\n",
        "ε\n",
        "<\n",
        "0 beträgt der Fehler\n",
        "−\n",
        "0\n",
        ",\n",
        "1\n",
        "ε, für\n",
        "ε\n",
        "≥\n",
        "0 beträgt er\n",
        "0\n",
        ",\n",
        "9\n",
        "ε.*\n",
        "\n",
        "* https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/"
      ],
      "metadata": {
        "id": "v-8rQF9PRpMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson Loss**\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-103abfddc312\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459\n"
      ],
      "metadata": {
        "id": "WYKjQ9BdRpMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stochastic-deterministic Processes, Markov vs nom Markov, probabilistic vs stochastic*"
      ],
      "metadata": {
        "id": "9kh5t94z0n7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1605.png)\n",
        "\n",
        "https://www.researchgate.net/figure/A-flowchart-showing-different-types-of-stochastic-processes-Processes-lower-in-the-chart_fig2_330251417\n",
        "\n",
        "http://home.ubalt.edu/ntsbarsh/simulation/sim.htm"
      ],
      "metadata": {
        "id": "m_CrbI1s1u5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a deterministic stochastic operator?**\n",
        "\n",
        "Deterministic stochastic operator, also known as Markov operator, is a concept used in the context of Markov processes and dynamical systems theory. They are used to describe the evolution of probability distributions in a system over time.\n",
        "\n",
        "Technically, a deterministic stochastic operator is a function that operates on a vector of probabilities, preserving the total probability and maintaining non-negativity. It is deterministic because the operator is defined by deterministic rules, and it is stochastic because it operates on vectors of probabilities, which inherently involve randomness.\n",
        "\n",
        "In a discrete context, the deterministic stochastic operator can be represented as a stochastic matrix (also known as a probability matrix, transition matrix, substitution matrix, or Markov matrix). In a continuous context, they can be represented by integral operators known as Markov operators.\n",
        "\n",
        "The properties of a deterministic stochastic operator include:\n",
        "1. Non-negativity: All entries of the operator (or matrix) are non-negative. This reflects the fact that probabilities are never negative.\n",
        "2. Column stochastic: Each column of the operator (or matrix) sums to 1. This ensures that the total probability is conserved.\n",
        "\n",
        "Deterministic stochastic operators are widely used in different fields, including statistics, physics, economics, computer science, etc. They form the foundation of Markov chains, Markov processes, and many algorithms like PageRank, which Google's search algorithm is based on."
      ],
      "metadata": {
        "id": "3d0Awgbc6TIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is climate deterministic or stochastic?**\n",
        "\n",
        "https://mpimet.mpg.de/en/communication/news/is-climate-deterministic-or-stochastic"
      ],
      "metadata": {
        "id": "2TU5JZll6uzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**deterministic set of stochastic operators?**\n",
        "\n",
        "In the context of Markov processes, a deterministic set of stochastic operators could be a collection of transition matrices that define a Markov chain. Each transition matrix is a stochastic operator because it operates on probability vectors (the state of the Markov chain) and has the properties of non-negativity and column stochasticity.\n",
        "\n",
        "As an example, consider a simple weather model where the weather on any given day can either be \"Sunny\" or \"Rainy\". We can model the weather over several days as a Markov chain, where the probability of the weather being sunny or rainy on a particular day depends only on the weather of the previous day.\n",
        "\n",
        "Suppose that:\n",
        "\n",
        "- The probability that tomorrow will be sunny given that today is sunny is 0.9 (and hence, the probability that it will be rainy is 0.1).\n",
        "- The probability that tomorrow will be rainy given that today is rainy is 0.5 (and hence, the probability that it will be sunny is 0.5).\n",
        "\n",
        "We can represent this model as a transition matrix (a deterministic stochastic operator), where the state [1, 0] represents \"Sunny\" and the state [0, 1] represents \"Rainy\":\n",
        "\n",
        "```\n",
        "   | Sunny Rainy\n",
        "--------------\n",
        "Sunny |  0.9   0.1\n",
        "Rainy |  0.5   0.5\n",
        "```\n",
        "\n",
        "In this matrix, each column represents the current state, each row represents the next state, and each entry is the probability of transitioning from one state to another. This transition matrix defines the dynamics of our weather model and is deterministic in the sense that the transition probabilities do not change over time.\n",
        "\n",
        "This is a simple example. In more complex scenarios, you might have a different transition matrix for each step, especially if your system's dynamics change over time. But even then, each transition matrix would still be a deterministic stochastic operator, as it would still have fixed, predetermined values.\n"
      ],
      "metadata": {
        "id": "RTMSanBi2f-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Statistical hypothesis testing (Sigma)*"
      ],
      "metadata": {
        "id": "fQEB0NP4Fk57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1 Sigma: 95% confidence (2 Sigma?), 0.683 Wahrscheinlichkeit\n",
        "* 3 Sigma: 1 in 300 or 0,27% false positive, 0.997 Wahrscheinlichkeit\n",
        "* 5 Sigma: 1 in 3,5 Mio\n",
        "* https://en.m.wikipedia.org/wiki/Statistical_hypothesis_testing\n",
        "* https://reference.wolfram.com/applications/eda/ExperimentalErrorsAndErrorAnalysis.html\n",
        "* https://www.studysmarter.de/schule/mathe/stochastik/sigma-regeln/\n",
        "* https://www.zmescience.com/science/what-5-sigma-means-0423423/"
      ],
      "metadata": {
        "id": "9w9iTJlLFpdH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1flpzoQFTVa"
      },
      "source": [
        "###### *Stochastic Analysis*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Analysis**\n",
        "\n",
        "* [Stochastic calculus](https://en.m.wikipedia.org/wiki/Stochastic_calculus) bzw. [Stochastische Analysis](https://de.m.wikipedia.org/wiki/Stochastische_Analysis) is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. This field is created and started by the Japanese mathematician Kiyoshi Itô during World War 2.\n",
        "\n",
        "* [Stochastische_Integration](https://de.m.wikipedia.org/wiki/Stochastische_Integration): Es sind stochastische Prozesse mit unendlicher Variation, insbesondere der Wiener-Prozess, als Integratoren zugelassen.\n",
        "\n",
        "* [Stochastischer Prozess](https://de.m.wikipedia.org/wiki/Stochastischer_Prozess): (auch Zufallsprozess) zeitlich geordnete, zufällige Vorgänge. Theorie der stochastischen Prozesse ist Erweiterung der Wahrscheinlichkeitstheorie dar und bildet Grundlage für stochastische Analysis.\n",
        "\n",
        "* > Stochastic: Statistik + Probability Theory (incl stochastic / random processes)\n",
        "\n",
        "* Bei einem Sparguthaben entspräche dies dem **exponentiellen Wachstum** durch Zinseszins. Bei Aktien wird dieses Wachstumsgesetz hingegen **in der Realität offenbar durch eine komplizierte Zufallsbewegung überlagert**.\n",
        "\n",
        "* Bei zufälligen Störungen, die sich aus vielen kleinen Einzeländerungen zusammensetzen, **wird von einer Normalverteilung als einfachstem Modell ausgegangen**.\n",
        "\n",
        "* Außerdem zeigt sich, dass die Varianz der Störungen proportional zum betrachteten Zeitraum $\\Delta t$ ist. Der Wiener-Prozess $W_t$ besitzt alle diese gewünschten Eigenschaften, eignet sich also als ein Modell für die zeitliche Entwicklung der Zufallskomponente des Aktienkurses.\n"
      ],
      "metadata": {
        "id": "DC-KHFg7J17I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0iE2bRTLxY"
      },
      "source": [
        "**Stochastic Differential Equation ( →  Black Scholes, Spontaneous Symmetry Breaking)**\n",
        "\n",
        "* Eine [stochastischen Differentialgleichung](https://de.wikipedia.org/wiki/Stochastische_Differentialgleichung) ist eine Verallgemeinerung des Begriffs der gewöhnlichen Differentialgleichung auf stochastische Prozesse.\n",
        "\n",
        "> Stochastische Differentialgleichungen werden in zahlreichen Anwendungen eingesetzt, **um zeitabhängige Vorgänge zu modellieren**, die neben deterministischen Einflüssen zusätzlich **stochastischen Störfaktoren (Rauschen)** ausgesetzt sind.\n",
        "\n",
        "* Die formale Theorie der stochastischen Differentialgleichungen wurde erst in den 1940er Jahren durch den japanischen Mathematiker Itō Kiyoshi formuliert.\n",
        "\n",
        "* Gemeinsam mit der **stochastischen Integration** begründet die Theorie der **stochastischen Differentialgleichungen** die **stochastische Analysis**.\n",
        "\n",
        "* **Objective**: Genau wie bei deterministischen Funktionen möchte man auch bei stochastischen Prozessen den Zusammenhang zwischen dem Wert der Funktion und ihrer momentanen Änderung (ihrer Ableitung) in einer Gleichung formulieren.\n",
        "\n",
        "* **Challenge**: Was im einen Fall zu einer gewöhnlichen Differentialgleichung führt, ist im anderen Fall problematisch, da viele stochastische Prozesse, wie beispielsweise **der Wiener-Prozess, nirgends differenzierbar sind**.\n",
        "\n",
        "* Loesung ist wie bei gewohnlichen Differentialgleichungen plus einen stochastischen Term\n",
        "\n",
        "*Beim Typus der stochastischen Differentialgleichungen treten in der Gleichung stochastische Prozesse auf. Eigentlich sind stochastische Differentialgleichungen keine Differentialgleichungen [im obigen Sinne](https://de.m.wikipedia.org/wiki/Differentialgleichung#Weitere_Typen), sondern lediglich gewisse Differentialrelationen, welche als Differentialgleichung interpretiert werden können.*\n",
        "\n",
        "*Beispiele fur stochastische Differentialgleichungen*\n",
        "\n",
        "* Die SDGL für die geometrische brownsche Bewegung lautet $\\mathrm{d} S_{t}=r S_{t} \\mathrm{~d} t+\\sigma S_{t} \\mathrm{~d} W_{t} .$ Sie wird beispielsweise im Black-Scholes-Modell zur Beschreibung von Aktienkursen verwendet.\n",
        "\n",
        "* Die SDGL für einen Ornstein-Uhlenbeck-Prozess ist $\\mathrm{d} X_{t}=\\theta\\left(\\mu-X_{t}\\right) \\mathrm{d} t+\\sigma \\mathrm{d} W_{t} .$ Sie wird unter anderem im Vasicek-Modell zur finanzmathematischen Modellierung von Zinssätzen über den Momentanzins\n",
        "verwendet.\n",
        "\n",
        "* Die SDGL für den Wurzel-Diffusionsprozess nach William Feller lautet $\\mathrm{d} X_{t}=\\kappa\\left(\\theta-X_{t}\\right) \\mathrm{d} t+\\sigma \\sqrt{X_{t}} \\mathrm{~d} W_{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelei85Xp8rU"
      },
      "source": [
        "**Supersymmetric theory of stochastic dynamics & Spontaneous Symmetry breaking**\n",
        "\n",
        "* [Supersymmetric theory of stochastic dynamics](https://en.m.wikipedia.org/wiki/Supersymmetric_theory_of_stochastic_dynamics) or stochastics (STS) **is an exact theory of stochastic (partial) differential equations (SDEs)**, the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise.\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Chaos_theory#Chaos_as_a_spontaneous_breakdown_of_topological_supersymmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpmflh50O1X"
      },
      "source": [
        "###### *Markov Process & Monte Carlo Simulation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_psDmmPW0P"
      },
      "source": [
        "A Markov chain is a **discrete-time stochastic process** that progresses from one state to another with certain probabilities that can be **represented by a graph and state transition matrix P** as indicated below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/markov.PNG\" alt=\"markov\">\n",
        "\n",
        "* Markov chains may be modeled by **finite state machines, and random walks**\n",
        "\n",
        "* A Stochastic (State Transition) Matrix describes a Markov chain X<sub>t</sub> over a [**finite state space (Probability space) S**](https://en.m.wikipedia.org/wiki/Probability_space) with cardinality S.\n",
        "\n",
        "  * Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties.\n",
        "\n",
        "  * Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* Let P be the transition matrix of Markov chain {X0, X1, ...}.\n",
        "\n",
        "  * **Reducibility**: a Markov chain is said to be irreducible if it is possible to get to any state from any state. In other words, a Markov chain is irreducible if there exists a chain of steps between any two states that has positive probability.\n",
        "\n",
        "  * **Periodicity**: a state in a Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. Thus, starting in state 'i', the chain can return to 'i' only at multiples of the period 'k', and k is the largest such integer. State 'i' is aperiodic if k = 1 and periodic if k > 1.\n",
        "\n",
        "  * **Transience and Recurrence**: A state 'i' is said to be transient if, given that we start in state 'i', there is a non-zero probability that we will never return to 'i'. State i is recurrent (or persistent) if it is not transient. A recurrent state is known as positive recurrent if it is expected to return within a finite number of steps and null recurrent otherwise. Transience and recurrence issues are central to the study of Markov chains and help describe the Markov chain's overall structure. The presence of many transient states may suggest that the Markov chain is absorbing, and a strong form of recurrence is necessary in an ergodic Markov chain.\n",
        "\n",
        "  * **Ergodicity**: a state 'i' is said to be ergodic if it is aperiodic and positive recurrent. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic. Ergodic Markov chains are, in some senses, the processes with the \"nicest\" behavior.\n",
        "\n",
        "  * **Absorbing State**: a state i is called absorbing if it is impossible to leave this state. Therefore, the state 'i' is absorbing if pii = 1 and pij = 0 for i ≠ j. If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain. Absorbing states are crucial for the discussion of absorbing Markov chains. A common type of Markov chain with transient states is an absorbing one. An absorbing Markov chain is a Markov chain in which it is impossible to leave some states, and any state could (after some number of steps, with positive probability) reach such a state. It follows that all non-absorbing states in an absorbing Markov chain are transient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcP1_FkDDDOv"
      },
      "source": [
        "**Monte Carlo Simulation**\n",
        "\n",
        "* Monte Carlo can be thought of as carrying out many experiments, each time changing the variables in a model and observing the response.\n",
        "\n",
        "* Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on **repeated random sampling to obtain numerical results**. The underlying concept is to use randomness to solve problems that might be deterministic in principle.\n",
        "\n",
        "* Monte Carlo methods are useful for **simulating systems with many coupled degrees of freedom**, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean–Vlasov processes, kinetic models of gases). Other examples include **modeling phenomena with significant uncertainty in inputs** such as the calculation of risk in business\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6jdLMMCTSD"
      },
      "source": [
        "**Markov Chain Monte Carlo (MCMC)**\n",
        "\n",
        "* Markov Chain Monte Carlo refers to a **class of methods** for sampling from a probability distribution in order to **construct the most likely distribution**.\n",
        "\n",
        "> MCMC can be considered as a **random walk** that gradually converges to the true distribution.\n",
        "\n",
        "* We cannot directly calculate the (i.e. logistic) distribution, so instead we generate thousands of values — called samples — for the parameters of the function (alpha and beta) to **create an approximation of the distribution**.\n",
        "\n",
        "* The idea behind MCMC is that **as we generate more samples, our approximation gets closer and closer to the actual true distribution**.\n",
        "\n",
        "* Markov Chain and Monte Carlo, MCMC is a method that repeatedly draws random values for the parameters of a distribution based on the current values. **Each sample of values is random, but the choices for the values are limited by the current state and the assumed prior distribution of the parameters**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chCbs1o_Iud"
      },
      "source": [
        "###### *White Noise*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0ShhpknOvG"
      },
      "source": [
        "\n",
        "**A white noise process has following conditions**\n",
        "\n",
        "* Mean (level) is zero (does not change over time - stationary process)\n",
        "* Variance is constant (does not change over time - stationary process)\n",
        "* Zero autocorrelation (values do not correlate with lag values)\n",
        "\n",
        "**White Noise: Independent & Identically Distributed**\n",
        "\n",
        "* Hence, in a time series is white noise if the variables are independent and identically distributed (IID) with a mean of zero.\n",
        "\n",
        "* The term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies\n",
        "\n",
        "* **Necessary Condition**: Independence: variables are statistically uncorrelated = their covariance is zero. Therefore, the covariance matrix R of the components of a white noise vector w with n elements must be an n by n diagonal matrix, where each diagonal element Rᵢᵢ is the variance of component wᵢ; and the correlation matrix must be the n by n identity matrix.\n",
        "\n",
        "* **Sufficient Condition**: every variable in w has a normal distribution with zero mean and the same variance, w is said to be a Gaussian white noise vector. In that case, the joint distribution of w is a multivariate normal distribution; the independence between the variables then implies that the distribution has spherical symmetry in n-dimensional space. Therefore, any orthogonal transformation of the vector will result in a Gaussian white random vector. In particular, under most types of discrete Fourier transform, such as FFT and Hartley, the transform W of w will be a Gaussian white noise vector, too; that is, the n Fourier coefficients of w will be independent Gaussian variables with zero mean and the same variance.\n",
        "\n",
        "* A random vector (that is, a partially indeterminate process that produces vectors of real numbers) is said to be a white noise vector or white random vector if its components each have a probability distribution with zero mean and finite variance, and are statistically independent: that is, their joint probability distribution must be the product of the distributions of the individual components.\n",
        "\n",
        "* First moment has to be zero and second moment has to be finite though. (iid ) White noise is always an independent process but reverse may not be true.\n",
        "\n",
        "* The technical definition of white noise is that it has equal intensity at all frequencies. This corresponds to a delta function autocorrelation. This is only possible if there is no correlation between any sequential values. So yes, the independence is true both backwards and forwards. Note that the actual distribution is irrelevant.\n",
        "\n",
        "\n",
        "\n",
        "**Types of White Noise Processes**\n",
        "* If the variables in the series are drawn from a Gaussian distribution, the series is called Gaussian white noise\n",
        "* There are also white noise processes, like Levy etc.\n",
        "\n",
        "**Relationship to Stochastic Processes**\n",
        "* White noise is the generalized mean-square derivative of the Wiener process or Brownian motion (so Wiener is an integrated White Noise)\n",
        "\n",
        "**White noise is an important concept in time series analysis and forecasting**\n",
        "\n",
        "* **Predictability**: If your time series is white noise, then, by definition, it is random. You cannot reasonably model it and make predictions.\n",
        "* **Model Diagnostics**: The statistics and diagnostic plots can be uses on time series to check if it is white noise. The series of errors from a time series forecast model should ideally be white noise. If the series of forecast errors are not white noise, it suggests improvements could be made to the predictive model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8idRoUCpsoKR"
      },
      "source": [
        "###### *Random Walk*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdW8o7tCtbPh"
      },
      "source": [
        "**Random Walk**\n",
        "\n",
        "* Random walk is another time series model where the current observation is equal to the previous observation with a random step up or down. Known as a stochastic or random process.\n",
        "\n",
        "> y<sub>(t)</sub> = B<sub>0</sub> + B<sub>1</sub> * X<sub>(t-1)</sub> + e<sub>(t)</sub>\n",
        "\n",
        "* A random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence.\n",
        "\n",
        "* The process used to generate the series forces dependence from one-time step to the next. This dependence provides some consistency from step-to-step rather than the large jumps that a series of independent, random numbers provides. It is this dependency that gives the process its name as a “random walk” or a “drunkard’s walk”.\n",
        "\n",
        "> **A simple random walk is a martingale**\n",
        "\n",
        "* In higher dimensions, the set of randomly walked points has interesting geometric properties. In fact, one gets a discrete fractal, that is, a set which exhibits stochastic self-similarity on large scales.\n",
        "* Examples include the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler: all can be approximated by random walk models, even though they may not be truly random in reality\n",
        "* Random walks serve as a fundamental model for the recorded stochastic activity / stochastic processes.\n",
        "* As a more mathematical application, the value of π can be approximated by the use of random walk in an agent-based modeling environment.\n",
        "\n",
        "* There are many types of time-dependent processes referred to as random walks - most often refers to a special category of Markov chains or Markov processes. A random walk on the integers (and the gambler's ruin problem) are examples of **Markov processes in discrete time**.\n",
        "\n",
        "* Specific cases or limits of random walks include the Lévy flight and diffusion models such as Brownian motion. **A Wiener process (~ Brownian motion) is the integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments. A Wiener process is the scaling limit of random walk in dimension 1.\n",
        "\n",
        "* Random walks can take place on a variety of spaces: graphs, on the integers or real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie.\n",
        "\n",
        "* **Random Walk and Autocorrelation**\n",
        "\n",
        "  * We can calculate the correlation between each observation and the observations at previous time steps. Given the way that the random walk is constructed, we would expect a strong autocorrelation with the previous observation and a linear fall off from there with previous lag values.\n",
        "\n",
        "* **Stationarity**\n",
        "\n",
        "  * A stationary time series is one where the values are not a function of time. Given the way that the random walk is constructed and the results of reviewing the autocorrelation, we know that the observations in a random walk are dependent on time.\n",
        "  * The current observation is a random step from the previous observation. Therefore we can expect a random walk to be non-stationary. In fact, all random walk processes are non-stationary. Note that not all non-stationary time series are random walks.\n",
        "  * Additionally, a non-stationary time series does not have a consistent mean and/or variance over time. A review of the random walk line plot might suggest this to be the case. We can confirm this using a statistical significance test, specifically the Augmented Dickey-Fuller test.\n",
        "\n",
        "**IID**\n",
        "\n",
        "  * This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”).\n",
        "\n",
        "  * This is equivalent to saying that the first difference of the variable is a series to which the mean model should be applied. So, if you begin with a time series that wanders all over the map, but you find that its first difference looks like it is an i.i.d. sequence, then a random walk model is a potentially good candidate.\n",
        "\n",
        "* **Prediction**\n",
        "\n",
        "  * A random walk is unpredictable; it cannot reasonably be predicted. Given the way that the random walk is constructed, we can expect that the best prediction we could make would be to use the observation at the previous time step as what will happen in the next time step. Simply because we know that the next time step will be a function of the prior time step.\n",
        "  * This is often called the naive forecast, or a persistence model. We can implement this in Python by first splitting the dataset into train and test sets, then using the persistence model to predict the outcome using a rolling forecast method. Once all predictions are collected for the test set, the mean squared error is calculated.\n",
        "\n",
        "* **Drift**\n",
        "\n",
        "  * A random walk model is said to have “drift” or “no drift” according to whether the distribution of step sizes has a non-zero mean or a zero mean. At period n, the k-step-ahead forecast that the random walk model without drift gives for the variable Y is\n",
        "\n",
        "  > $\\hat{Y}_{n+k}=Y_{n}$\n",
        "\n",
        "  * In others words, it predicts that all future values will equal the last observed value. This doesn’t really mean you expect them to all be the same, but just that you think they are equally likely to be higher or lower, and you are staying on the fence as far as point predictions are concerned. If you extrapolate forecasts from the random walk model into the distant future, they will go off on a horizontal line, just like the forecasts of the mean model. So, qualitatively the long-term point forecasts of the random walk model look similar to those of the mean model, except that they are always “re-anchored” on the last observed value rather than the mean.of the historical data.\n",
        "\n",
        "  * For the random-walk-with-drift model, the k-step-ahead forecast from period n is:\n",
        "\n",
        "  > $\\hat{\\mathrm{Y}}_{\\mathrm{n}+\\mathrm{k}}=\\mathrm{Y}_{\\mathrm{n}}+\\mathrm{k} \\hat{\\mathrm{d}}$\n",
        "\n",
        "  * where dˆ is the estimated drift, i.e., the average increase from one period to the next. So, the long-term forecasts from the random-walk-with-drift model look like a trend line with slope dˆ , but it is always re-anchored on the last observed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk3roPmAuCDi"
      },
      "source": [
        "**Applications of Random Walks**\n",
        "\n",
        "* In computer networks, random walks can model the number of transmission packets buffered at a server.\n",
        "* In population genetics, random walk describes the statistical properties of genetic drift.\n",
        "* In image segmentation, random walks are used to determine the labels (i.e., “object” or “background”) to associate with each pixel.\n",
        "* In brain research, random walks and reinforced random walks are used to model cascades of neuron firing in the brain.\n",
        "Random walks have also been used to sample massive online graphs such as online social networks.\n",
        "\n",
        "* **Random Walks in Financial Time Series**\n",
        "\n",
        "  * **Is time series a random walk?**: Your time series may be a random walk. Some ways to check if your time series is a random walk are as follows:\n",
        "The time series shows a strong temporal dependence that decays linearly or in a similar pattern.\n",
        "  * The time series is non-stationary and making it stationary shows no obviously learnable structure in the data.\n",
        "The persistence model provides the best source of reliable predictions.\n",
        "  * This last point is key for time series forecasting. Baseline forecasts with the persistence model quickly flesh out whether you can do significantly better. If you can’t, you’re probably working with a random walk. Many time series are random walks, particularly those of security prices over time. The random walk hypothesis is a theory that stock market prices are a random walk and cannot be predicted.\n",
        "  * \"A random walk is one in which future steps or directions cannot be predicted on the basis of past history. When the term is applied to the stock market, it means that short-run changes in stock prices are unpredictable.\" - Page 26, A Random Walk down Wall Street: The Time-tested Strategy for Successful Investing. https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/\n",
        "\n",
        "* https://towardsdatascience.com/random-walks-with-python-8420981bc4bc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbYRNP_zXe4"
      },
      "source": [
        "###### *Wiener Process (Brownian Motion)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldqy2UtMpa8P"
      },
      "source": [
        "**Wiener Process (Brownian Motion)**\n",
        "\n",
        "* Video: Physics of Randomness: https://youtu.be/5jBVYvHeG2c\n",
        "\n",
        "* Brownian Motion: Flower particles move randomly because they are hit by particles that move. And particles move faster with more heat.\n",
        "\n",
        "* Same in astrophysics with stars under the gravitational influence of smaller stars, big stars move a bit randomly\n",
        "\n",
        "* Same with stock market where the price is influenced by many factors all the time (and hence the price moves randomly up and down)\n",
        "\n",
        "**The Wiener process is a real valued continuous-time (continuous state-space) stochastic process**\n",
        "\n",
        "* W<sub>0</sub> = 0 (P-almost certain)\n",
        "* The Wiener process has (stochastically) independent increments.\n",
        "* The increases are therefore stationary and normally distributed with the expected value zero and the variance t - s.\n",
        "* The individual paths are (P-) almost certainly continuous.\n",
        "\n",
        "**Applications**\n",
        "\n",
        "* In physics it is used to study Brownian motion, the diffusion of minute particles suspended in fluid, and other types of diffusion via the Fokker–Planck and Langevin equations.\n",
        "* It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman–Kac formula, a solution to the Schrödinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology.\n",
        "* It is also prominent in the mathematical theory of finance, in particular the Black–Scholes option pricing model.\n",
        "\n",
        "**Properties of a Wiener Process**\n",
        "\n",
        "1. The Wiener process belongs to the family of **Markov processes** and there specifically to the class of **Levy processes**. It also fulfills the strong markov property. It is one of the best known Lévy processes (**càdlàg** stochastic processes with stationary independent increments).\n",
        "2. The Wiener Process is a **special Gaussian process** with an expected value function E(W<sub>t</sub>)  = 0 and and the covariance function Cov (W<sub>s</sub>, W<sub>t</sub>) = min (s,t)\n",
        "3. The Wiener process is a (continuous time) **martingale** (Lévy characterisation: the Wiener process is an almost surely continuous martingale with W0 = 0 and quadratic variation [Wt, Wt] = t, which means that Wt2 − t is also a martingale).\n",
        "4. The Wiener process is a **Levy process** with steady paths and constant expectation 0.\n",
        "\n",
        "*Another characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent N(0, 1) random variables. This representation can be obtained using the Karhunen–Loève theorem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9PQvsR1bKfq"
      },
      "source": [
        "**Wiener process as a limit of random walk (& Differences between Wiener Process & Random Walk)**\n",
        "\n",
        "* https://www.quora.com/What-is-an-intuitive-explanation-of-a-Wiener-process?top_ans=3955819\n",
        "\n",
        "* A Wiener process (~ Brownian motion) is the **integral of a white noise generalized Gaussian process**. It is not stationary, but it has stationary increments.\n",
        "\n",
        "* Let X<sub>1</sub>, X<sub>2</sub>, X<sub>n</sub> be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1.  The central limit theorem asserts that W<sup>(n)</sup> (1) converges in distribution to a standard Gaussian random variable W(1) as n → ∞.\n",
        "\n",
        "* [Donsker's theorem](https://en.m.wikipedia.org/wiki/Donsker%27s_theorem) asserts that as n → ∞ , W<sub>n</sub> approaches a Wiener process, which explains the ubiquity of Brownian motion. **Donsker's invariance principle** states that: As random variables taking values in the Skorokhod space D [0,1], the random function W<sup>(n)</sup> converges in distribution to a standard Brownian motion W := (W(t))<sub>t ∈ [0,1]</sub> as n → ∞.\n",
        "\n",
        "![Donsker's Invariance Principle](https://upload.wikimedia.org/wikipedia/commons/8/8c/Donskers_invariance_principle.gif)\n",
        "\n",
        "*Donsker's invariance principle for simple random walk on Z*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deltorobarba/repo/master/wiener.jpg\" alt=\"wiener\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKFiwhd7X_5I"
      },
      "source": [
        "**Differences to other stochastic processes**\n",
        "\n",
        "**Random Walk**\n",
        "\n",
        "\n",
        "* **A Wiener process is the [scaling limit](https://en.m.wikipedia.org/wiki/Scaling_limit) of random walk in dimension 1**. The **convergence of a random walk toward the Wiener process is controlled by the central limit theorem**, and by **Donsker's theorem**. The **Green's function** of the diffusion equation that controls the Wiener process, suggests that, **after a large number of steps, the random walk converges toward a Wiener process**.\n",
        "\n",
        "* A random walk is a discrete fractal (a function with integer dimensions; 1, 2, ...), but a **Wiener process trajectory is a true fractal**, and there is a connection between the two (a Wiener process walk is a fractal of **Hausdorff dimension** 2).\n",
        "\n",
        "* Unlike the random walk, a **Wiener Process is scale invariant**. A Wiener process enjoys many symmetries random walk does not. For example, a **Wiener process walk is invariant to rotations, but the random walk is not**, since the underlying grid is not. This means that in many cases, problems on a random walk are easier to solve by translating them to a Wiener process, solving the problem there, and then translating back.\n",
        "\n",
        "**(Gaussian) White Noise**\n",
        "\n",
        "* The Wiener process is used to represent the integral (from time zero to time t) of a zero mean, unit variance, delta correlated **<u>Gaussian</u> white noise process**\n",
        "\n",
        "**Brownian Motion**\n",
        "\n",
        "* **\"Brownian motion\" is a phenomenon that can be modeled with a Wiener Process**, because a Wiener process is a stochastic process with similar behavior to Brownian motion, the physical phenomenon of a minute particle diffusing in a fluid.\n",
        "\n",
        "* The Brownian motion process (and the Poisson process in one dimension) are both examples of **Markov processes in continuous time**\n",
        "\n",
        "* Itō also paved the way for the Wiener process from physics to other sciences: the **stochastic differential equations** he set up made it possible to adapt the Brownian motion to more statistical problems.\n",
        "\n",
        "* The **geometric Brownian motion** derived from a stochastic differential equation solves the problem that the **Wiener process, regardless of its starting value, almost certainly reaches negative values over time, which is impossible for stocks**. Since the development of the famous **Black-Scholes model**, the geometric Brownian movement has been the standard.\n",
        "\n",
        "**Ornstein-Uhlenbeck-Process**\n",
        "\n",
        "* The problem raised by the **[non-rectifiable paths](https://en.m.wikipedia.org/wiki/Arc_length)** of the Wiener process in the modeling of Brownian paths leads to the Ornstein-Uhlenbeck process and also makes the need for a theory of stochastic integration and differentiation clear\n",
        "* here it is not the motion but the speed of the particle as one that is not rectifiable process derived from the Wiener process, from which one obtains rectifiable particle paths through integration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0K1gPOoznzE"
      },
      "source": [
        "###### *Gaussian Process*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoS5J618HxN"
      },
      "source": [
        "* A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. **every finite linear combination of them is normally distributed**.\n",
        "\n",
        "* The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
        "\n",
        "* Gaussian Processes are a class of stationary, zero-mean stochastic processes which are completely dependent on their autocovariance functions. This class of models can be used for both regression and classification tasks.\n",
        "\n",
        "* Gaussian Processes provide estimates about uncertainty, for example giving an estimate of how sure an algorithm is that an item belongs to a class or not.\n",
        "\n",
        "* In order to deal with situations which embed a certain degree of uncertainty is typically made use of probability distributions.\n",
        "\n",
        "* Gaussian processes can allow us to describe probability distributions of which we can later update the distribution using Bayes Rule once we gather new training data.\n",
        "\n",
        "* **Relation to other Stochastic Processes**\n",
        "\n",
        "  * A **Wiener process (~ Brownian motion)** is the integral of a white noise generalized Gaussian process. It is not stationary, but it has stationary increments.\n",
        "\n",
        "  * The **fractional Brownian motion** is a Gaussian process whose covariance function is a generalisation of that of the Wiener process.\n",
        "\n",
        "  * The **Ornstein–Uhlenbeck** process is a stationary Gaussian process.\n",
        "\n",
        "  * The **Brownian bridge** is (like the Ornstein–Uhlenbeck process) an example of a Gaussian process whose increments are not independent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOw3me6qbku"
      },
      "source": [
        "###### *Further Stochastic Processes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEr4Ub47xgN1"
      },
      "source": [
        "**Brownian Bridge**\n",
        "\n",
        "* A Brownian bridge is a **continuous Gaussian process** with X<sub>0</sub> = X<sub>1</sub> = 0, and with mean and covariance functions given in (c) and (d), respectively.\n",
        "\n",
        "* The **Brownian bridge is (like the Ornstein–Uhlenbeck process)** an example of a Gaussian process **whose increments are not independent**.\n",
        "\n",
        "* There are several ways of constructing a Brownian bridge from a standard Brownian motion [Source](https://www.randomservices.org/random/brown/Bridge.html)\n",
        "\n",
        "* **Applications: Path Simulation for Stock Shares**: The simple Monte Carlo method with Euler method supplemented by the Brownian bridge correction for the possibility of falling below or exceeding the barriers between discretization times.\n",
        "\n",
        "  * By merely discreetly viewing (simulating) the (log) share price, those paths can also lead to a positive final payment in which the share price between the selected times k delta t has exceeded the lower barrier or exceeded the upper barrier without this is noticed in the discretized model.\n",
        "\n",
        "  * To calculate the probability of such an unnoticed barrier violation, Brownian Bridge is used (with the help of the independence and stationarity of its growth).\n",
        "\n",
        "  * With the help of the statements about the Brown Bridge, one can formally. Specify the Monte Carlo algorithm that can be used to evaluate double barrier options without having to discretize the price path.\n",
        "\n",
        "* **Application: Bond Prices**\n",
        "\n",
        "  * Computation of bond prices in a structural default model with jumps with an unbiased Monte-Carlo simulation.\n",
        "\n",
        "  * The algorithm requires the evaluation of integrals with the density of the first-passage time of a Brownian bridge as the integrand. (Metwally and Atiya (2002) suggest an approximation of these integrals.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0CRhOchiHXb"
      },
      "source": [
        "**Ornstein-Uhlenbeck Process**\n",
        "\n",
        "> Simulating a stochastic differential equation\n",
        "\n",
        "* the Ornstein–Uhlenbeck process is a **stochastic process** with applications in financial mathematics and the physical sciences.\n",
        "\n",
        "* Its original application in physics was as a model for the **<u>velocity</u> of a massive Brownian particle under the influence of <u>friction</u>**. It is named after Leonard Ornstein and George Eugene Uhlenbeck.\n",
        "\n",
        "* The Ornstein–Uhlenbeck process is a **stationary Gauss–Markov process**, which means that it is a **Gaussian process, a Markov process, and is temporally homogeneous**. In fact, it is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables.\n",
        "\n",
        "* Over time, the process tends to **drift towards its mean function**: such a process is called **mean-reverting**.\n",
        "\n",
        "* The process can be considered to be a **modification of the random walk in continuous time, or Wiener process**, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the center.\n",
        "\n",
        "* The Ornstein–Uhlenbeck process can also be considered as the **continuous-time analogue of the discrete-time AR(1) process**.\n",
        "\n",
        "* The Ornstein–Uhlenbeck process can be interpreted as a **scaling limit of a discrete process**, in the same way that Brownian motion is a scaling limit of random walks.\n",
        "\n",
        "* Generalization: It is possible to extend Ornstein–Uhlenbeck processes to processes where the background driving process is a Lévy process (instead of a simple Brownian motion).\n",
        "\n",
        "* In addition, in finance, stochastic processes are used where the volatility increases for larger values of C.\n",
        "\n",
        "* The Ornstein–Uhlenbeck process (just like the Brownian Bridge) a is an example of a **Gaussian process whose increments are not independent**. Look for stock returns devoid of explanatory factors, and analyze the corresponding residuals as stochastic processes. (e.g. mean reverting?). Can residuals be fitted to (increments of) OU processes or other MR processes? If so, what is the typical correlation time-scale? Mean reversion days: how long does it take to converge (e.g. model distribution of days).\n",
        "\n",
        "**In Financial Mathematics**\n",
        "\n",
        "* The Ornstein–Uhlenbeck process is one of several approaches used to model (with modifications) interest rates, currency exchange rates, and commodity prices stochastically.\n",
        "\n",
        "* The parameter μ (mu) represents the equilibrium or mean value supported by fundamentals; σ (signa) the degree of volatility around it caused by shocks, and θ (theta) the rate by which these shocks dissipate and the variable reverts towards the mean.\n",
        "\n",
        "* One application of the process is a trading strategy known as pairs trade.\n",
        "\n",
        "* Stationary and mean-reverting around mean=10 (red dotted line)\n",
        "* **In financial engineering: how long does it take in average to converge? mean-reversion is an investment opportunity!**\n",
        "* Now, we are going to take a look at the time evolution of the distribution of the process. To do this, we will simulate many independent realizations of the same process in a vectorized way. We define a vector X that will contain all realizations of the process at a given time (that is, we do not keep all realizations at all times in memory). This vector will be overwritten at every time step. We will show the estimated distribution (histograms) at several points in time:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJqrw3_Nqev5"
      },
      "source": [
        "**Jump diffusion**\n",
        "\n",
        "* [Jump diffusion](https://en.m.wikipedia.org/wiki/Jump_diffusion) is a stochastic process that involves jumps and diffusion.\n",
        "\n",
        "* It has important applications in magnetic reconnection, coronal mass ejections, condensed matter physics, option pricing, and pattern theory and computational vision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQP6Jd68WsL"
      },
      "source": [
        "**Lévy Process**\n",
        "\n",
        "* A Lévy process is a stochastic process with **[independent, stationary increments](https://de.m.wikipedia.org/wiki/Prozess_mit_unabhängigen_Zuwächsen)** (= the course of the future of the process is independent of the past): it represents the motion of a point whose successive displacements are random and independent, and statistically identical over different time intervals of the same length.\n",
        "\n",
        "* A Lévy process may thus be viewed as the **continuous-time analog of a random walk**.\n",
        "\n",
        "* The most well known **examples of Lévy processes are Wiener process (~ Brownian motion), and Poisson process**. Aside from Brownian motion with drift, all other proper (that is, not deterministic) Lévy processes have discontinuous paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAvA4JovM_-"
      },
      "source": [
        "**Bernoulli Process**\n",
        "\n",
        "* The outcomes of a Bernoulli process will follow a [Binomial distribution](https://en.m.wikipedia.org/wiki/Binomial_distribution).\n",
        "\n",
        "* https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/\n",
        "\n",
        "* A Bernoulli process is a finite or infinite sequence of binary random variables, so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1.\n",
        "\n",
        "* The component Bernoulli variables X<sub>i</sub> are [identically distributed and independent](https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). Prosaically, a Bernoulli process is a repeated coin flipping, possibly with an unfair coin (but with consistent unfairness).\n",
        "\n",
        "* Every variable X<sub>i</sub> in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided dice); this generalization is known as the **Bernoulli scheme**.\n",
        "\n",
        "* The problem of determining the process, given only a limited sample of Bernoulli trials, may be called the problem of checking whether a coin is fair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPTaHcB54U"
      },
      "source": [
        "**Poisson (Point) Process**\n",
        "\n",
        "* Poisson point process is a type of random mathematical object that consists of points randomly located on a mathematical space\n",
        "\n",
        "* Its name derives from the fact that if a collection of random points in some space forms a Poisson process, then the number of points in a region of finite size is a random variable with a Poisson distribution.\n",
        "\n",
        "* The process was discovered independently and repeatedly in several settings, including experiments on radioactive decay, telephone call arrivals and insurance mathematics. The Poisson point process is often defined on the real line, where it can be considered as a stochastic process.\n",
        "\n",
        "* On the real line, the Poisson process is a type of **continuous-time Markov process** known as a birth-death process (with just births and zero deaths) and is called a pure or simple birth process.\n",
        "\n",
        "* In this setting, it is used, for example, in queueing theory to model random events, such as the arrival of customers at a store, phone calls at an exchange or occurrence of earthquakes, distributed in time. In the plane, the point process, also known as a spatial Poisson process, can represent the locations of scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest.\n",
        "\n",
        "* In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process.\n",
        "\n",
        "* Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena where there is sufficiently strong interaction between the points. This has inspired the proposal of other point processes, some of which are constructed with the Poisson point process, that seek to capture such interaction.\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-process-everything-you-need-to-know-322aa0ab9e9a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Measure & Probability Theory*"
      ],
      "metadata": {
        "id": "ECcHaTG2nKiM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4KDiI20GSS8"
      },
      "source": [
        "###### *Measure (Maß)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FMu2VsVHXMJ"
      },
      "source": [
        "* Ein Maß ist in der Mathematik **eine Funktion**, die geeigneten Teilmengen einer Grundmenge Zahlen zuordnet, die als „Maß“ für die Größe dieser Mengen interpretiert werden können.\n",
        "\n",
        "* Es sei $\\mathcal{A}$ eine o-Algebra über einer nicht-leeren Grundmenge $\\Omega .$ Eine Funktion $\\mu: \\mathcal{A} \\rightarrow[0, \\infty]$ heißt Maß auf $\\mathcal{A}$, wenn die beiden folgenden Bedingungen erfüllt sind:\n",
        "\n",
        "> $\\mu(\\emptyset)=0$\n",
        "\n",
        "> Additivität: Für jede Folge $\\left(A_{n}\\right)_{n \\in \\mathbb{N}}$ paarweise disjunkter Mengen aus $\\mathcal{A}$ gilt $\\mu\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right)$\n",
        "\n",
        "* Ist die o-Algebra aus dem Zusammenhang klar, so spricht man auch von einem Maß auf $\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tOKtjbyH7Ai"
      },
      "source": [
        "* Eine Teilmenge von $\\Omega$, die in $\\mathcal{A}$ liegt, wird messbar genannt. Für solch ein $A \\in \\mathcal{A}$ heißt $\\mu(A)$ das Maß der Menge $A$.\n",
        "\n",
        "* Das **Tripel $(\\Omega, \\mathcal{A}, \\mu)$ wird Maßraum** genannt.\n",
        "\n",
        "* Das Paar $(\\Omega, \\mathcal{A})$ bestehend aus der Grundmenge und der darauf definierten $\\sigma$-Algebra **heißt Messraum oder auch messbarer Raum**.\n",
        "\n",
        "* **Ein Maß $\\mu$ ist also eine auf einem Messraum definierte nicht-negative [$\\sigma$ -additive](https://de.m.wikipedia.org/wiki/Σ-Additivität) [Mengenfunktion](https://de.m.wikipedia.org/wiki/Mengenfunktion) mit $\\mu(\\emptyset)=0$.**\n",
        "\n",
        "* Das Maß $\\mu$ heißt Wahrscheinlichkeitsmaß (oder normiertes Maß), wenn zusätzlich $\\mu(\\Omega)=1$ gilt. Ein Maßraum $(\\Omega, \\mathcal{A}, \\mu)$ mit einem Wahrscheinlichkeitsmaß $\\mu$ ist ein Wahrscheinlichkeitsraum.\n",
        "\n",
        "* Ist allgemeiner $\\mu(\\Omega)<\\infty,$ so nennt man $\\mu$ ein [endliches Maß](https://de.m.wikipedia.org/wiki/Endliches_Maß). Existieren abzählbar viele Mengen, deren Maßendlich ist und deren Vereinigung ganz $\\Omega$ ergibt, dann wird $\\mu$ ein [$\\sigma$ -endliches](https://de.m.wikipedia.org/wiki/Σ-Endlichkeit) (oder $\\sigma$ -finites) Maß genannt. $^{[4]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGw0qfYwrCJ"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Messbare_Funktion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtT12NSGYu4"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Maß_(Mathematik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbegVOVJGqzz"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Σ-Additivität"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1v7eQtS4gqu"
      },
      "source": [
        "**Zählmaß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbVSW8Q6RdZ"
      },
      "source": [
        "$\\mu$ (A) :=\n",
        "* #A, wenn A endlich\n",
        "* ∞, sonst\n",
        "\n",
        "Rechenregeln in [0, ∞] :\n",
        "* x + ∞ := ∞ für alle x ∈ [0, ∞]\n",
        "* x * ∞ := ∞ für alle x ∈ [0, ∞]\n",
        "* 0 * ∞ := 0 (in der Maßtheorie sinnvoll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzLXsG6C4Lx"
      },
      "source": [
        "**Dirac Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpN2hkk7KTX"
      },
      "source": [
        "p ∈ X (Punktmaß, weil es einem Punkt ein Maß zuordnet, und sonst Null).\n",
        "\n",
        "Ist p innerhalb eine Menge A als Teilmenge von X, dann ist Diracmaß = 1, sonst 0.\n",
        "\n",
        "δ<sub>p</sub> (A) :=\n",
        "* 1, wenn p ∈ A\n",
        "* 0, wenn sonst.\n",
        "\n",
        "(δ = delta klein)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJL1d3ifDByw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Diracmaß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkR6CvL_C0uu"
      },
      "source": [
        "**Lebesgue Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAnnK16Dbua"
      },
      "source": [
        "* Das Lebesgue-Maß ist das **Maß im euklidischen Raum**, das geometrischen Objekten ihren Inhalt (Länge, Flächeninhalt, Volumen, …) zuordnet.\n",
        "\n",
        "* Das Lebesgue-Borel-Maß auf der Borel-$\\sigma$-Algebra $\\mathcal{B}\\left(\\mathbb{R}^{n}\\right)$ (auch als Borel-Lebesgue-Maß oder nur Bore/-Maß bezeichnet) ist das eindeutige Maß $\\lambda$ mit der Eigenschaft, dass es $n$ -dimensionalen\n",
        "Hyperrechtecken ihr $n$ -dimensionales Volumen zuordnet:\n",
        "\n",
        ">$\\lambda\\left(\\left[a_{1}, b_{1}\\right] \\times \\cdots \\times\\left[a_{n}, b_{n}\\right]\\right)=\\left(b_{1}-a_{1}\\right) \\cdots \\cdots\\left(b_{n}-a_{n}\\right)$\n",
        "\n",
        "* Das heißt, **es ist das Maß, das Intervallen inre Länge zuordnet (im Eindimensionalen), Rechtecken ihren Flächeninhalt zuordnet (im Zweidimensionalen), Quadern ihr Volumen zuordnet (im Dreidimensionalen) usw.**\n",
        "\n",
        "* Durch diese Bedingung wird der Inhalt $\\lambda(B)$ beliebiger Borel-Mengen eindeutig festgelegt. Die BorelMengen werden auch Borel-messbar oder $B$ -messbar genannt. **Das Borel-Maß ist bewegungsinvariant und normiert, aber nicht vollständig.**\n",
        "\n",
        "* Das Lebesgue-Maß ist das Haar-Maß **auf der lokalkompakten topologischen Gruppe** $\\mathbb {R} ^{n}$ mit der Addition, die Existenz folgt daher bereits aus der Existenz des Haarmaßes.\n",
        "\n",
        "* Insbesondere ist es translationsinvariant, das bedeutet, dass sich das Maß einer Menge unter Translation nicht ändert. Zudem ist es invariant unter Spiegelungen und Drehungen, also sogar bewegungsinvariant. Das Lebesgue-Maß ist σ-endlich und regulär."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XsGc9DFRb"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Lebesgue-Maß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymG9QHcCF1Hg"
      },
      "source": [
        "**Haarsches Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLkzlmPEF8cn"
      },
      "source": [
        "* Das Haarsche Maß wurde eingeführt, um Ergebnisse der **Maßtheorie in der Gruppentheorie anwendbar zu machen**.\n",
        "\n",
        "* Beispiel: Das Lebesgue-Maß $B$ auf $\\mathbb{R}^{n}$ und $\\mathbb{C}^{n}$ ist das Haarsche Maß auf den additiven Gruppen $\\left(\\mathbb{R}^{n},+\\right)$ bzw. $\\left(\\mathbb{C}^{n},+\\right)$.\n",
        "\n",
        "* Es ist eine Verallgemeinerung des Lebesgue-Maßes. Das Lebesgue-Maß ist ein Maß auf dem euklidischen Raum, das unter Translationen invariant ist.\n",
        "\n",
        "* Der euklidische Raum ist eine lokalkompakte topologische Gruppe bezüglich der Addition. Das Haarsche Maß ist für jede lokalkompakte (im Folgenden immer als hausdorffsch vorauszusetzende) topologische Gruppe definierbar, insbesondere also für jede Lie-Gruppe.\n",
        "\n",
        "* Lokalkompakte Gruppen mit ihren Haarschen Maßen werden in der [harmonischen Analyse](https://de.m.wikipedia.org/wiki/Harmonische_Analyse) untersucht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzktY0hJF4LR"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Haarsches_Maß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw99f_qvDl6i"
      },
      "source": [
        "**Haussdorf Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2DTGGZGDv5H"
      },
      "source": [
        "* Das Hausdorff-Maß ist eine Verallgemeinerung des Lebesgue-Maßes auf nicht notwendig ganzzahlige Dimensionen. Mit seiner Hilfe lässt sich die Hausdorff-Dimension definieren, ein Dimensionsbegriff, mit dem beispielsweise fraktale Mengen untersucht werden können."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9M76qlDrVX"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Hausdorff-Maß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V27J3X8qM9cN"
      },
      "source": [
        "**Jordan Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6tM4coFNIxa"
      },
      "source": [
        "* Mit dem Jordan-Maß kann man beschränkten Teilmengen des $\\mathbb {R} ^{n}$ einen Inhalt zuordnen und erhält einen Integralbegriff, der dem [riemannschen Integralbegriff](https://de.m.wikipedia.org/wiki/Riemannsches_Integral) analog ist.\n",
        "\n",
        "* Das Jordan-Maß ist kein Maß im Sinne der Maßtheorie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcW8Znq_NB8I"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Jordan-Maß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ef5-QUNE22"
      },
      "source": [
        "Siehe auch: https://de.m.wikipedia.org/wiki/Inhalt_(Maßtheorie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomYI_xzCurv"
      },
      "source": [
        "**Radon Maß**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGB7cfQ9VeD"
      },
      "source": [
        "Es handelt sich um ein spezielles Maß auf der Borelschen σ-Algebra eines Hausdorff-Raums mit bestimmten Regularitätseigenschaften. Der Begriff wird in der Fachliteratur jedoch nicht einheitlich verwendet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AnX69MC1wo"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Radonmaß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0PG2uLQIPM"
      },
      "source": [
        "**Measurable Space (Messbarer Raum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraYrSFNKL4d"
      },
      "source": [
        "Das Tupel $(\\Omega, \\mathcal{A}, \\mu)$ heißt Maßraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge (universe / Set) genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine $\\sigma$ -Algebra über der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "Zusammen mit einem Maß $\\mu$ wird aus einem messbaren Raum ein Maßraum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrI52A0M-lGw"
      },
      "source": [
        "* **A measurable space or Borel space is a basic object in measure theory**. It consists of a set and a σ-algebra, which defines the subsets that will be measured.\n",
        "\n",
        "* Note that in contrast to a measure space, **no measure is needed for a measurable space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCr-8c-d62V"
      },
      "source": [
        "**Messraum und Maßraum sind spezielle σ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZG-bj9KyHs"
      },
      "source": [
        "**Example**\n",
        "\n",
        "Look at the set\n",
        "\n",
        "X=\\{1,2,3\\}\n",
        "\n",
        "One possible $\\sigma$ -algebra would be\n",
        "\n",
        "$\\mathcal{A}_{1}=\\{X, \\emptyset\\}$\n",
        "\n",
        "Then $\\left(X, \\mathcal{A}_{1}\\right)$ is a measurable space.\n",
        "\n",
        "Another possible $\\sigma$ -algebra would be the power set on $X$ :\n",
        "\n",
        "$\\mathcal{A}_{2}=\\mathcal{P}(X)$\n",
        "\n",
        "With this, a second measurable space on the set $X$ is given by $\\left(X, \\mathcal{A}_{2}\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kIK_3HLLTT"
      },
      "source": [
        "**Types**\n",
        "\n",
        "* If $X$ is finite or countable infinite, **the $\\sigma$ -algebra is most of the times the power set on $X$**, so $\\mathcal{A}=\\mathcal{P}(X) .$ This leads to the measurable space $(X, \\mathcal{P}(X))$\n",
        "\n",
        "* **If $X$ is a topological space, the $\\sigma$ -algebra is most commonly the Borel $\\sigma$ -algebra $\\mathcal{B}$**, so $\\mathcal{A}=\\mathcal{B}(X)$. This leads to the measurable space $(X, \\mathcal{B}(X))$ that is common for all topological spaces such as\n",
        "the real numbers $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHhdyPQ-hgE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measurable_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0juq9WKIedRw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clc0uHou3vFE"
      },
      "source": [
        "**Riemann vs Lebesgue Integral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YpeX84I3z0Q"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Integralrechnung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8ajZ5T31sN"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Maßtheorie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3G8b13_MBD3"
      },
      "source": [
        "https://de.wikipedia.org/wiki/Satz_von_Fubini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGhitrlOQEHM"
      },
      "source": [
        "###### *Measure Space (Maßraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> One important example of a measure space is a probability space."
      ],
      "metadata": {
        "id": "pPIoNoM1u-dH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsroJGMFRuY"
      },
      "source": [
        "Ein Maßraum ist eine spezielle mathematische Struktur, die eine essentielle Rolle in der Maßtheorie und dem axiomatischen Aufbau der Stochastik spielt.\n",
        "\n",
        "Das Tripel $(\\Omega, \\mathcal{A}, \\mu)$ heißt Maßraum, wenn\n",
        "\n",
        "* $\\Omega$ eine beliebige, nichtleere Menge ist. $\\Omega$ wird dann auch Grundmenge genannt.\n",
        "\n",
        "* $\\mathcal{A}$ eine [$\\sigma$ -Algebra](https://de.m.wikipedia.org/wiki/Σ-Algebra) über der Grundmenge $\\Omega$ ist.\n",
        "\n",
        "* $\\mu$ ein [Maß](https://de.m.wikipedia.org/wiki/Maß_(Mathematik)) ist, das auf $\\mathcal{A}$ definiert ist.\n",
        "\n",
        "Alternativ kann man einen Maßraum auch als einen [Messraum](https://de.m.wikipedia.org/wiki/Messraum_(Mathematik)) $(\\Omega, \\mathcal{A})$ versehen mit einem Maß $\\mu$ definieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqmDCJDWy4sq"
      },
      "source": [
        "A measure space is a triple $(X, \\mathcal{A}, \\mu),$ where\n",
        "\n",
        "* $X$ is a set\n",
        "\n",
        "* $\\mathcal{A}$ is a $\\sigma$ -algebra on the set $X$\n",
        "\n",
        "* $\\boldsymbol{\\mu}$ is a measure on $(X, \\mathcal{A})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34X5FmJ4d--B"
      },
      "source": [
        "**Messraum und Maßraum sind spezielle σ-Algebren.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDp68lYUypzs"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Measure_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIqej8jGuOt"
      },
      "source": [
        "**Beispiele**\n",
        "\n",
        "* Ein einfaches Beispiel für einen Maßraum sind die natürlichen Zahlen als Grundmenge $\\Omega=\\mathbb{N}$, als $\\sigma$ Algebra wählt man die Potenzmenge $\\mathcal{A}=\\mathcal{P}(\\mathbb{N})$ und als Maß das Diracmaß auf der $1: \\mu=\\delta_{1}$\n",
        "\n",
        "* Ein bekannter Maßraum ist die Grundmenge $\\mathbb{R}$, versehen mit der borelschen $\\sigma$ -Algebra $\\mathcal{B}(\\mathbb{R})$ und dem Lebesgue-Maß. **Dies ist der kanonische Maßraum in der Integrationstheorie.**\n",
        "\n",
        "* Die in der Wahrscheinlichkeitstheorie verwendeten **[Wahrscheinlichkeitsräume](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum)** $(\\Omega, \\mathcal{A}, P)$ sind allesamt Maßräume. Sie bestehen aus der Ergebnismenge $\\Omega$, der Ereignisalgebra $\\mathcal{A}$ und dem Wahrscheinlichkeitsmaß $P$ (= synonym 'Wahrscheinlichkeitsverteilung' oder einfach 'Verteilung'). Mit den Eigenschaften: Die drei Forderungen Normiertheit, σ-Additivität und Werte im Intervall zwischen 0 und 1 werden auch die Kolmogorow-Axiome genannt. ([Source](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTp6RhYL8Je"
      },
      "source": [
        "**Klassen von Maßräumen**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Σ-Endlichkeit\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollständiges_Maß\n",
        "\n",
        "und mehr.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKgmO9kQZPo"
      },
      "source": [
        "* Verzichtet man auf Abstände und Winkel, behält jedoch das Volumen geometrischer Körper bei, gelangt man in das Gebiet der Maßtheorie. (für ein wahrscheinlichkeitsmaß volumen = 1)\n",
        "\n",
        "* Der Maßtheorie gelang es, den Begriff des Volumens (oder eines anderen Maßes) auf eine enorm große Klasse von Mengen auszudehnen, die sogenannten messbaren Mengen. In vielen Fällen ist es jedoch unmöglich, allen Mengen ein Maß zuzuordnen (siehe Maßproblem).\n",
        "Die messbaren Mengen bilden dabei eine σ-Algebra. Mit Hilfen von messbaren Mengen lassen sich messbare Funktionen zwischen Messräumen definieren.\n",
        "\n",
        "* Um einen topologischen Raum zu einem Messraum zu machen, muss man ihn mit einer σ-Algebra ausstatten. Die σ-Algebra der Borel-Mengen ist die verbreitetste, aber nicht die einzige Wahl.\n",
        "\n",
        "* Ein Maßraum ist ein Messraum, der mit einem Maß versehen ist. Ein euklidischer Raum mit dem [Lebesgue-Maß](https://de.m.wikipedia.org/wiki/Lebesgue-Maß) ist beispielsweise ein Maßraum. In der Integrationstheorie werden Integrierbarkeit und Integrale messbarer Funktionen auf Maßräumen definiert. Mengen vom Maß null werden Nullmengen genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0omcCEyljo"
      },
      "source": [
        "* A measure space is a basic object of measure theory, a branch of mathematics that studies generalized notions of volumes.\n",
        "\n",
        "* It contains an underlying set, the subsets of this set that are feasible for measuring (the σ-algebra) and the method that is used for measuring (the measure).\n",
        "\n",
        "* **One important example of a measure space is a probability space**.\n",
        "\n",
        "* A measurable space consists of the first two components without a specific measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZqzchtzaSz"
      },
      "source": [
        "A **complete measure** (or, more precisely, a complete measure space) is a measure space in which every subset of every null set is measurable (having measure zero)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtShSxHrzeGE"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Complete_measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9JLE04AcXav"
      },
      "source": [
        "**Probability Space (Wahrscheinlichkeitsraum)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2axZJitcXav"
      },
      "source": [
        "Gegeben sei\n",
        "\n",
        "* eine Menge $\\Omega$, der sogenannte **Ergebnisraum**,\n",
        "\n",
        "* eine o-Algebra $\\Sigma$ auf dieser Menge, das **Ereignissystem**.\n",
        "\n",
        "Dann heißt eine Abbildung (=Funktion)\n",
        "\n",
        ">$P: \\Sigma \\rightarrow[0,1]$\n",
        "\n",
        "mit den Eigenschaften\n",
        "\n",
        "*  Normiertheit: Es ist $P(\\Omega)=1$\n",
        "\n",
        "* $\\sigma$-Additivität: Für jede abzählbare Folge von paarweise disjunkten Mengen $A_{1}, A_{2}, A_{3}, \\ldots$ aus $\\Sigma$ gilt $P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right)$\n",
        "\n",
        "ein Wahrscheinlichkeitsmaß oder eine Wahrscheinlichkeitsverteilung.\n",
        "\n",
        "Die drei Forderungen Normiertheit, $\\sigma$-Additivität und Werte im Intervall zwischen O und 1 werden auch die [Kolmogorow-Axiome](https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitstheorie#Axiome_von_Kolmogorow) genannt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qf8jvmMcXaw"
      },
      "source": [
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsmaß"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3sQ4P3KcXaw"
      },
      "source": [
        "* **One important example of a measure space is a probability space.**\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Maßraum, bei dem das Maß des ganzen Raums gleich 1 ist.\n",
        "\n",
        "* In der Wahrscheinlichkeitstheorie werden für die verwendeten maßtheoretischen Begriffe meist eigene Bezeichnungen verwendet, die der Beschreibung von Zufallsexperimenten angepasst sind: Messbare Mengen werden Ereignisse und messbare Funktionen zwischen Wahrscheinlichkeitsräumen werden Zufallsvariable genannt; ihre Integrale sind Erwartungswerte.\n",
        "\n",
        "* Das Produkt einer endlichen oder unendlichen Familie von Wahrscheinlichkeitsräumen ist wieder ein Wahrscheinlichkeitsraum. Im Gegensatz dazu ist für allgemeine Maßräume nur das Produkt endlich vieler Räume definiert. Dementsprechend gibt es zahlreiche unendlichdimensionale Wahrscheinlichkeitsmaße, beispielsweise die Normalverteilung, aber kein unendlichdimensionales Lebesgue-Maß.\n",
        "Diese Räume sind weniger geometrisch. Insbesondere lässt sich die Idee der Dimension, wie sie in der einen oder anderen Form auf alle anderen Räume anwendbar ist, nicht auf Messräume, Maßräume und Wahrscheinlichkeitsräume anwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFqotKf4cXaw"
      },
      "source": [
        "* Es handelt sich um ein mathematisches Modell zur Beschreibung von Zufallsexperimenten. Hierbei werden die verschiedenen möglichen Ausgänge des Experiments zu einer Menge zusammengefasst. Teilmengen dieser Ergebnismenge können dann unter bestimmten Voraussetzungen Zahlen zwischen 0 und 1 zugeordnet werden, die als Wahrscheinlichkeiten interpretiert werden.\n",
        "\n",
        "* Ein Wahrscheinlichkeitsraum ist ein Maßraum (Ω, Σ, P) dessen Maß P ein Wahrscheinlichkeitsmaß ist. Im Einzelnen bedeutet das:\n",
        "\n",
        "* Ω ist eine beliebige nichtleere Menge, genannt die Ergebnismenge. Ihre Elemente heißen Ergebnisse.\n",
        "\n",
        "* Σ (Sigma) ist eine σ-Algebra über der Grundmenge Ω (Omega), also eine Menge bestehend aus Teilmengen von Ω, die Ω enthält und abgeschlossen gegenüber der Bildung von Komplementen und abzählbaren Vereinigungen ist. Die Elemente von Σ heißen Ereignisse. Die σ-Algebra Σ selbst wird auch Ereignissystem oder Ereignisalgebra genannt.\n",
        "\n",
        "* P : Σ –> [0,1] ist ein Wahrscheinlichkeitsmaß, das heißt eine Mengenfunktion, die den Ereignissen Zahlen zuordnet, derart dass P(∅) = 0 ist, P (A1 ∪ A2 ∪ … ) = P(A1) + P(A2) + … für paarweise disjunkte (d. h. sich gegenseitig ausschließende) Ereignisse A1, A2, … gilt (3. Kolmogorow-Axiom) und P(Ω) = 1 ist (2. Kolmogorow-Axiom).\n",
        "\n",
        "* Der Messraum (Ω, Σ) wird auch Ereignisraum genannt. Ein Wahrscheinlichkeitsraum ist also ein Ereignisraum, auf dem zusätzlich ein Wahrscheinlichkeitsmaß gegeben ist.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Wahrscheinlichkeitsraum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4A4TeH7SewU"
      },
      "source": [
        "###### *Measurable Space (Messraum)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbXZ_kJWGTey"
      },
      "source": [
        "* [Messraum](https://en.wikipedia.org/wiki/Measurable_space) und Maßraum sind spezielle σ-Algebren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErN__bggDh9m"
      },
      "source": [
        "###### *Field of Sets (Algebra oder Mengensystem)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIdFJLmUG_C"
      },
      "source": [
        "**Mengensystem** oder **Mengenalgebra** oder **Fields of Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3axOdZgGNHd"
      },
      "source": [
        "* **A field of sets is a pair $\\langle X, \\mathcal{F}\\rangle$ where $X$ is a set and $\\mathcal{F}$ is an algebra over $X$**\n",
        "\n",
        "* i.e., a subset of the power set of $X$, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying $X \\in \\mathcal{F}$.\n",
        "\n",
        "* In other words, $\\mathcal{F}$ forms\n",
        "a subalgebra of the power set Boolean algebra of $X$ (with the same identity element $X \\in \\mathcal{F}$ ).\n",
        "(Many authors refer to $\\mathcal{F}$ itself as a field of sets.) Elements of $X$ are called points and those of $\\mathcal{F}$\n",
        "are called complexes and are said to be the admissible sets of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct9dwFuXEuFS"
      },
      "source": [
        "Ω (Omega) sei eine beliebige (Grund-)Menge. Ein System $\\mathcal{A}$ (oder $\\mathcal{F}$) von Teilmengen von Ω heißt eine Mengenalgebra oder Algebra über Ω, wenn folgende Eigenschaften erfüllt sind:\n",
        "\n",
        "1. $\\mathcal{A} \\neq \\emptyset$ ( $\\mathcal{A}$ ist nicht leer)\n",
        "\n",
        "2. $A, B \\in \\mathcal{A} \\Rightarrow A \\cup B \\in \\mathcal{A}$ (Stabilität/Abgeschlossenheit bezüglich Vereinigung)\n",
        "\n",
        "3. $A \\in \\mathcal{A} \\Rightarrow A^{\\mathrm{c}} \\in \\mathcal{A}$ (Stabilität/Abgeschlossenheit bezüglich Komplementbildung $\\left.A^{c}=\\Omega \\backslash A\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ksvdizjJS-"
      },
      "source": [
        "* In der Mathematik ist (Mengen-)Algebra ein Grundbegriff der Maßtheorie. Er beschreibt ein nicht-leeres Mengensystem, das vereinigungs- und komplementstabil ist.\n",
        "\n",
        "* A field of sets is a **pair ⟨X,F⟩** where X is a set and F is an algebra over X i.e., a subset of the power set of X, closed under complements of individual sets and under the union (hence also under the intersection) of pairs of sets, and satisfying X ∈ F.\n",
        "\n",
        "* In other words, F forms a subalgebra of the power set Boolean algebra of X (with the same identity element X ∈ F). (Many authors refer to F itself as a field of sets.)\n",
        "\n",
        "* **Elements of X are called points and those of F are called complexes and are said to be the admissible sets of X.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRO-7ajId2V"
      },
      "source": [
        "* For arbitrary set $Y$, its power set (Potenzmenge) $2^{Y}$ (or, somewhat pedantically, the pair $\\left\\langle Y, 2^{Y}\\right\\rangle$ of this set and its power set) is a field of sets.\n",
        "\n",
        "* If $Y$ is finite (namely, $n$ -element), then $2^{Y}$ is finite (namely, $2^{n}$ element).\n",
        "\n",
        "* It appears that every finite field of sets (it means, $\\langle X, \\mathcal{F}\\rangle$ with $\\mathcal{F}$ finite, while $X$ may be infinite) admits a representation of the form $\\left\\langle Y, 2^{Y}\\right\\rangle$ with finite $Y ;$ it means a function $f: X \\rightarrow Y$ that establishes a one-to-one correspondence between $\\mathcal{F}$ and $2^{Y}$ via inverse image:\n",
        "$S=f^{-1}[B]=\\{x \\in X \\mid f(x) \\in B\\}$ where $S \\in \\mathcal{F}$ and $B \\in 2^{Y}$ (that is, $B \\subset Y$ ).\n",
        "\n",
        "* One notable consequence: the number of complexes, if finite, is always of the form $2^{n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkRhwrZHSGp"
      },
      "source": [
        "**Beispiele für Algebra**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFXRKkuHYNd"
      },
      "source": [
        "* Für jede beliebige Menge $\\Omega$ ist $\\{\\emptyset, \\Omega\\}$ die kleinste und die Potenzmenge $\\mathcal{P}(\\Omega)$ die größtmögliche Mengenalgebra.\n",
        "* Jede $\\sigma$ -Algebra ist eine Mengenalgebra.\n",
        "* Für jede Menge $\\Omega$ ist das Mengensystem $\\mathcal{A}=\\left\\{A \\subseteq \\Omega \\mid A \\text { endlich oder } A^{c} \\text { endlich }\\right\\}$ eine Mengenalgebra. Wenn $\\Omega$ unendich ist, dann ist $\\mathcal{A}$ keine $\\sigma$ -Algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded5mK7pcYoX"
      },
      "source": [
        "**Separative and compact fields of sets: towards Stone duality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7fxFxMgcbMU"
      },
      "source": [
        "* A field of sets is called **separative (or differentiated)** if and only if for every pair of distinct points there is a complex containing one and not the other.\n",
        "\n",
        "* A field of sets is called **compact** if and only if for every proper filter over X the intersection of all the complexes contained in the filter is non-empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL_pBvidGa5"
      },
      "source": [
        "Given a field of sets $\\mathbf{X}=\\langle X, \\mathcal{F}\\rangle$ the complexes form a base for a topology. We denote by $T(\\mathbf{X})$ the corresponding topological space, $\\langle X, \\mathcal{T}\\rangle$ where $\\mathcal{T}$ is the topology formed by taking arbitrary unions of complexes. Then\n",
        "\n",
        "1. $T(\\mathbf{X})$ is always a [zero-dimensional space](https://en.m.wikipedia.org/wiki/Zero-dimensional_space)\n",
        "\n",
        "2. $T(\\mathbf{X})$ is a [Hausdorff space](https://en.m.wikipedia.org/wiki/Hausdorff_space) if and only if $\\mathbf{X}$ is separative.\n",
        "\n",
        "3. $T(\\mathbf{X})$ is a compact space with compact open sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is compact.\n",
        "\n",
        "4. $T(\\mathbf{X})$ is a Boolean space with clopen sets $\\mathcal{F}$ if and only if $\\mathbf{X}$ is both separative and compact (in which case it is described as being descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I7YepgQlABC"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Field_of_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8T8hpoiWFR"
      },
      "source": [
        "###### *σ-algebra (Sigma Algebra)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpgaCc5SSQx"
      },
      "source": [
        "**σ-algebra (Sigma Algebra)**\n",
        "\n",
        "* https://mathepedia.de/Sigma-Algebren.html\n",
        "\n",
        "* Sei $\\Omega \\neq \\emptyset$ eine Menge, $\\mathfrak{P}(\\Omega)$ die Potenzmenge und $\\mathcal{F} \\subseteq \\mathfrak{P}(\\Omega)$ ein Mengensystem (=field of sets).\n",
        "\n",
        "Definition\n",
        "\n",
        "$\\mathcal{F}$ heißt Algebra, wenn folgende Eigenschaften gelten:\n",
        "\n",
        "(1) $\\quad \\emptyset \\in \\mathcal{F}$\n",
        "\n",
        "(2) $\\quad A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}$\n",
        "\n",
        "(3) $\\quad A, B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}$\n",
        "\n",
        "$\\mathcal{F}$ heißt $\\sigma$ -Algebra, wenn die Punkte (1) und (2) gelten\n",
        "und zusätzlich\n",
        "\n",
        "(4) $\\quad A_{1}, A_{2}, \\ldots \\in \\mathcal{F} \\Rightarrow \\bigcup A_{k} \\in \\mathcal{F}$\n",
        "\n",
        "gilt.\n",
        "\n",
        "Algebren sind bezüglich der endlichen Vereinigung abgeschlossene Mengensysteme und $\\sigma$ -Algebren sind bezüglich der abzählbaren Vereinigung abgeschlossene\n",
        "Mengensysteme. Wegen (1) und (2) gilt stets $\\Omega \\in \\mathcal{F}$.\n",
        "\n",
        "* If an algebra over a **set is closed under countable unions** (hence also under countable intersections), it is called a **sigma algebra** and **the corresponding field of sets (Mengensystem) is called a measurable space**. The complexes of a measurable space are called measurable sets.\n",
        "\n",
        "* * **A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it.** (Alternative: $\\langle$ Ω , $\\mathcal{F}$, $\\mu$ $\\rangle$)\n",
        "\n",
        "* If $\\mu$ is in fact a probability measure we speak of a probability space and call its underlying measurable space a sample space.\n",
        "\n",
        "* The points of a sample space are called samples and represent potential outcomes while the measurable sets (complexes) are called events and represent properties of outcomes for which we wish to assign probabilities. (Many use the term sample space simply for the underlying set of a probability space, particularly in the case where every subset is an event.)\n",
        "\n",
        "* Measure spaces and probability spaces play a foundational role in measure theory and probability theory respectively.\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Algebra_(Mengensystem)\n",
        "\n",
        "**Pain Point**\n",
        "\n",
        "* How to define a messbare Menge?\n",
        "\n",
        "* Wenn wir eine Menge aus den reellen Zahlen haben und mochten eine Teilbereich [a bis b] messen, dann brauchen wir einen allgemeinen Massbegriff unabhangig von der konkreten Menge. Hier kommt Masstheorie und Sigma-Algebra.\n",
        "\n",
        "**Definition**\n",
        "\n",
        "* A σ-algebra defines the **set of events that can be measured**, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".\n",
        "\n",
        "* Exkurs: Ergebnis vs Ereignis. Die möglichen Ausgänge eines Zufallsexperimentes nennt man **Ergebnisse** (zB auf einem Wüfel die Zahlen 1,2,3..). Wenn man alle möglichen Ergebnisse eines Zufallsexperimentes in einer Menge zusammenfasst, erhält man die **Ergebnismenge**. Sie wird üblicherweise mit dem Symbol Ω (sprich Omega) bezeichnet. Beim Würfeln ist Ω= {1; 2; 3; 4; 5; 6} die Ergebnismenge. Jede Zusammenfassung von einem oder mehreren Ergebnissen eines Zufallsexperimentes in einer Menge nennt man **Ereignis** (zB auf einem Würfel die Menge an geraden Zahlen {2,4,6} und ungeraden Zahlen {1,3,5}.\n",
        "\n",
        "* Eine Sigma-Algebra F ist ein System, um alle möglichen **Ereignisse** (nicht Ergebnisse!) eines Zufallsexperiment zu beschreiben. Ereignisse sind an sich selbst Mengen, die man wie jede Menge vereinigen oder schneiden bzw. auch das Komplement bilden kann um so das Gegenereignis zu erhalten. Fasst man hier alle möglichen Kombinationen an Ereignissen in einer Menge zusammen, bekommt man eine Menge, die wiederum Mengen als Elemente enthält - eine Menge von Mengen sozusagen. Oft sagt man dazu auch einfach Mengensystem. Welche Eigenschaften ein Mengensystem genau haben muss, damit es eine Sigma-Algebra ist steht weiter unten.\n",
        "\n",
        "* Beispiel: Glücksrad mit blau, rot und grün. Dann haben wir folglich drei Ergebnisse, die wir auch abkürzen können: Ω={B,R,G}. Generell kann man sich schon merken: Ω und ∅ sind immer Elemente einer Sigma-Algebra. Daher haben wir hier 8 mögliche Teilmengen von Ω, die wir als Ereignis betrachten können und demnach als Menge in der Sigma-Algebra zusammenfassen (Potenzmenge von Omega): F ={∅, {B}, {R}, {G}, {B,R}, {B,G}, {R,G}, {B,R,G}} ([Source](https://www.massmatics.de/merkzettel/#!876:Ereignisraum_&_Sigma-Algebra)).\n",
        "\n",
        "* Bei diskreten Ergebnismengen kann man für die Sigma-Algebra immer die Potenzmenge P(Ω)nehmen und hat demnach dann stets diesen **Ereignisraum: (Ω,P(Ω))**\n",
        "\n",
        "* Und für die reellen Zahlen gibt es die sogenannte **Borelsche Sigma-Algebra B**, die man dann auch in der Regel benutzt. Ist die Ergebnismenge Ω eine Teilmenge der reellen Zahlen (oder ℝ selbst), so nehmen wir die Borelsche-Sigma B und der Ereignisraum lautet (Ω,B).\n",
        "\n",
        "* Wenn wir eine Sigma Algebra A gegeben haben, dann heisst jede Teilmenge in diesem Mengensystem (jedes Element aus dieser Sigma Algebra A) eine messbare Teilmenge (=die Mengen die wir messen wollen).\n",
        "\n",
        "* **<u>Die Elemente der Sigma Algebra sind die messbaren Teilmengen von unserer Grundmenge X</u>** (Und messbar ist der wesentliche Begriff). Das ist zB die Menge an vergangenen Trading-Events am Finanzmarkt bis zum Zeitpunkt t.\n",
        "\n",
        "* Sigma Algebra ist ein **Mengensystem von einer Teilmenge einer gegebenen Grundmenge** = der Raum, **den wir beschreiben wollen** (mit drei Eigenschaften). Die Menge einer Sigma-Algebra nennt man „**messbare Teilmengen**“.\n",
        "\n",
        "**Eigenschaften**\n",
        "\n",
        "* **A $\\subseteq$ P(X) (=Potenzmenge) heisst Sigma Algebra, wenn gilt** (Die Mengen, die in dieser Sigma Algebra liegen, das sind jene, die folgende drei Eigenschaften erfüllen, und sind die, die wir messen wollen (=diesen Mengen wollen wir ein Mass zuordnen). Potenzmenge selbst soll eine Sigma Algebra sein. Sollten gewissen Eigenschaften der Potenzmenge fordern). **<u>A collection of subsets</u> A is called a σ-algebra on a set X if the following properties are met:**\n",
        "\n",
        "1. **A contains X (the set itself)**: $\\quad \\phi, X \\in A$ (Leere Menge (sollte Länge oder Volumen Null haben) und ganze Grundmenge selbst haben wir im Mengensystem / sollen messbar sein. Das ist was Sigma Algebra sagt). **Ω ∈ F (Ergebnismenge muss enthalten sein)**\n",
        "\n",
        "2. **If A contains a subset S, then A also contains the complement of S**: $A \\in A \\Rightarrow A^{c}:=X \\backslash A \\in A$ (Irgendein Element in der Algebra: dann sollte auch dessen Komplement im Mengensystem enthalten sein.) Hiermit ist auch Regel 1 eingeschlossen! Deswegen liegt auch die leere Menge (Gegenereignis von Ω) in F.\n",
        "\n",
        "3. **Consider a countable collection of subsets. If each subset is included in A, then A must also contain their reunion.**: $A_{i} \\in A$ fur i $\\in N \\Rightarrow \\bigcup_{i=1}^{\\infty} A_{i} \\in A$ ((Letzter Punkt macht das Sigma aus): Abzählbarkeit, abzählbare Summe (A i‘s aus unseren Mengensystem A): wir haben endlich viele bzw. abzählbar viele, dann können wir die Vereinigung bilden / abziehbare Vereinigung. Die abzählbare Vereinigung soll wieder in der Sigma Algebra liegen = Wenn wir Längen haben, dann sollten wir die auch addieren können, auch wenn sich die Addition bis unendlich streckt! (blick auf messbarkeit))\n",
        "\n",
        "Having defined such a σ-algebra A, we call **the elements of σ-algebra A measurable sets** and the couple (X, A) a measurable space. An arbitrary set X can be a member of a multitude of σ-algebras. We denote the set of all σ-algebras that contain X with M(X). The **intersection of all those σ-algebras is called the σ-algebra generated by X**.\n",
        "\n",
        "**A σ-algebra (also σ-field) on a set X is a collection Σ of subsets of X that includes X itself, is closed under complement, and is closed under countable unions**. The definition implies that it also includes the empty subset and that it is closed under countable intersections. The pair (X, Σ) is called a measurable space or Borel space. A σ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of finitely many subsets, which is a weaker condition.\n",
        "\n",
        "**Borel‘sche Sigma-Algebra**\n",
        "\n",
        "* T ist ein topologischer Raum (oder ein metrischer Raum im engeren Sinn.) und X eine Menge darin. „Offene Mengen“.\n",
        "\n",
        "* Die Borel’sche Sigma Algebra auf topologischen Raum X ist jene kleinste Sigma Algebra, die von den offenen Mengen erzeugt wird.\n",
        "\n",
        "* B(X) := (T)\n",
        "\n",
        "**Measurable function**\n",
        "\n",
        "* **A set is measurable when it’s included in a σ-algebra.**\n",
        "\n",
        "* We can also extend the “measurable” attribute to functions. Here’s how:\n",
        "\n",
        "* Let’s consider (X, A) and (Y, B) two measurable spaces. A function f from A to B is called measurable if every set from B comes from applying f to a set from A. Formally, we say that for any element S of B, the pre-image of S under the function f is in A.\n",
        "\n",
        "\n",
        "**Application**\n",
        "\n",
        "* The main use of σ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a σ-algebra.\n",
        "\n",
        "* This concept is important in mathematical analysis as the **foundation for Lebesgue integration**, and in probability theory, where it is **interpreted as the collection of events which can be assigned probabilities**.\n",
        "\n",
        "* Also, **in probability, σ-algebras are pivotal in the definition of conditional expectation**.\n",
        "\n",
        "* In statistics, (sub) σ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "1. **Minimum**: Sigma Algebra A enthält leere Menge und Grundmenge selbst (kleinste Sigma Algebra die möglich ist): A = {𝜙,X}\n",
        "2. **Maximum**: Sigma Algebra enthält die Potenzmenge (beinhaltet alle Teilmengen von X): A = P(X)\n",
        "\n",
        "* If {A1, A2, A3, …} is a countable partition of X then the **collection of all unions of sets in the partition** (including the empty set) is a σ-algebra.\n",
        "\n",
        "* A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the σ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. \"coarsest\") σ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZiS2oORjId"
      },
      "source": [
        "###### *Measure-theoretic probability theory*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-XQOB0Rk43"
      },
      "source": [
        "**Measure-theoretic probability theory**\n",
        "\n",
        "* The raison d'être of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.\n",
        "\n",
        "* Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density.\n",
        "\n",
        "* The modern approach to probability theory solves these problems using measure theory to define the probability space:\n",
        "\n",
        "Given any set $\\Omega$ (also called sample space) and a $\\sigma$ -algebra $\\mathcal{F}$ on it, a measure $P$ defined on $\\mathcal{F}$ is\n",
        "called a probability measure if $P(\\Omega)=1$\n",
        "\n",
        "If $\\mathcal{F}$ is the Borel $\\sigma$ -algebra on the set of real numbers, then there is a unique probability measure on\n",
        "$\\mathcal{F}$ for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf.\n",
        "\n",
        "This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.\n",
        "\n",
        "The probability of a set $E$ in the $\\sigma$ -algebra $\\mathcal{F}$ is defined as\n",
        "\n",
        "$P(E)=\\int_{\\omega \\in E} \\mu_{F}(d \\omega)$\n",
        "\n",
        "where the integration is with respect to the measure $\\mu_{F}$ induced by $F$\n",
        "\n",
        "Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside R<sup>n</sup>, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.\n",
        "\n",
        "When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure.\n",
        "\n",
        "* Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes.\n",
        "\n",
        "* Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure.\n",
        "\n",
        "* If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.\n",
        "\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Probability_theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S4z_HeLRbtc"
      },
      "source": [
        "###### *Filtrations*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdKxs43TtLO"
      },
      "source": [
        "**Filtrations**\n",
        "\n",
        "* https://de.m.wikipedia.org/wiki/Filtrierung_(Wahrscheinlichkeitstheorie)\n",
        "\n",
        "* In martingale theory and the theory of stochastic processes, a **filtration is an increasing sequence of σ-algebras on a measurable space**.\n",
        "\n",
        "* That is, given a measurable space $(\\Omega, \\mathcal{F}),$ a filtration is a sequence of $\\sigma$ -algebras $\\left\\{\\mathcal{F}_{t}\\right\\}_{t \\geq 0}$ with $\\mathcal{F}_{t} \\subseteq \\mathcal{F}$ where each $t$ is a non-negative real number and\n",
        "\n",
        "> $t_{1} \\leq t_{2} \\Longrightarrow \\mathcal{F}_{t_{1}} \\subseteq \\mathcal{F}_{t_{2}}$\n",
        "\n",
        "* The exact range of the \"times\" $t$ will usually depend on context: the set of values for $t$ might be discrete or continuous, bounded or unbounded. For example,\n",
        "\n",
        "> $t \\in\\{0,1, \\ldots, N\\}, \\mathbb{N}_{0},[0, T]$ or $[0,+\\infty)$\n",
        "\n",
        "* **A σ-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time t\".**\n",
        "\n",
        "* **Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information**.\n",
        "\n",
        "* A typical example is in mathematical finance, where a filtration represents the information available up to and including each time t, and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n",
        "\n",
        "A Filtration is a growing sequence of sigma algebras\n",
        "\n",
        "> $\\mathcal{F}_{1} \\subseteq \\mathcal{F}_{2} \\ldots \\subseteq \\mathcal{F}_{n}$\n",
        "\n",
        "When talking of martingales we need to talk of conditional expectations, and in particular conditional expectations w.r.t σ algebra's. So whenever we write\n",
        "\n",
        "> $E\\left[Y_{n} \\mid X_{1}, X_{2}, \\ldots, X_{n}\\right]$\n",
        "\n",
        "which can be written as\n",
        "\n",
        "> $E\\left[Y_{n+1} \\mid \\mathcal{F}_{n}\\right]$\n",
        "\n",
        "where F𝑛 is a sigma algebra that makes random variables\n",
        "\n",
        "> $X_{1}, \\ldots, X_{n}$\n",
        "\n",
        "measurable. Finally a flitration F1,…Fn is simply an increasing sequence of sigma algebras. That is **we are conditioning on growing amounts of information**.\n",
        "\n",
        "> **Der Begriff der Filtrierung ist unerlässlich, um, ausgehend vom Begriff des stochastischen Prozesses, wichtige Begriffe wie Martingale oder Stoppzeiten einzuführen.**\n",
        "\n",
        "* Als Menge $T$ wird wie bei stochastischen Prozessen meist $\\mathbb{R}_{+}$ oder $\\mathbb{N}_{0}$ gewählt und $t \\in T$ als Zeitpunkt interpretiert.\n",
        "\n",
        "* **$\\sigma$ -Algebren modellieren verfügbare Information**. Die Mengen der $\\sigma$ -Algebra $\\mathcal{F}_{t}$ geben zu jedem Zeitpunkt $t$ an, wie viele Informationen zur Zeit bekannt sind. Für jedes Ereignis $A \\subseteq \\Omega$ bedeutet $A \\in \\mathcal{F}_{t}$ übersetzt, dass zum Zeitpunkt $t$ die Frage $,$ ist $\\omega \\in A ?^{\\prime \\prime}$ eindeutig mit $,$ ja\" oder $,$ nein\" beantwortet werden kann.\n",
        "\n",
        "* Dass die Filtrierung stets aufsteigend geordnet ist, bedeutet demnach, **dass eine einmal erlangte Information nicht mehr verloren geht.**\n",
        "\n",
        "* Ist ein stochastischer Prozess $\\left(X_{t}\\right)_{t \\in T}$ an eine Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ adaptiert, bedeutet dies also, dass der Verlauf der Funktion $s \\mapsto X_{s}(\\omega)$ im Intervall $[0, t]$ zum Zeitpunkt $t$ (für beliebiges, aber unbekanntes $\\omega \\in \\Omega$ und in Hinsicht auf die durch Ereignisse $A \\in \\mathcal{F}_{s}, s \\in[0, t]$ formulierbaren Fragen bekannt ist.\n",
        "\n",
        "* Der Begriff wird aufgrund seiner Bedeutung in den meisten fortgeschrittenen Lehrbüchern über stochastische Prozesse definiert. In einigen Lehrbüchern, zum Beispiel im Buch Probability von Albert N. Schirjajew, wird der Begriff aus didaktischen Gründen zunächst umfassend für Prozesse mit diskreten\n",
        "Werten in diskreter Zeit eingeführt.\n",
        "\n",
        "**Filtration in Finance**\n",
        "\n",
        "* In a multiperiod market, information about the market scenario is revealed in stages.\n",
        "\n",
        "* Some events may be completely determined by the end of the first trading period, others by the end of the second, and others not until the termination of all trading.\n",
        "\n",
        "* This suggests the following classification of events: for each t ≤ T ,\n",
        "\n",
        "(1) Ft = {all events determined in the first t trading periods}.\n",
        "\n",
        "* The finite sequence (Ft)0≤t≤T is a filtration of the space Ω of market scenarios.\n",
        "\n",
        "* In general, a filtration of a set Ω (not necessarily finite) is defined to be a collection Ft, indexed by a time parameter t (time may be either discrete or continuous), such that\n",
        "\n",
        "(a) each Ft is a σ−algebra of subsets (events) of Ω; and\n",
        "\n",
        "(b) if s<t then Fs ⊆Ft.\n",
        "\n",
        "**Beispiel**\n",
        "\n",
        "* Betrachtet man als Beispiel einen Wahrscheinlichkeitsraum $(\\mathbb{Z}, \\mathcal{P}(\\mathbb{Z}), P)$ mit abzählbarer Grundmenge $\\mathbb{Z}$ die standardmäßig mit der Potenzmenge als $\\sigma$ -Algebra ausgestattet ist, so wäre eine mögliche Filtrierung beispielsweise\n",
        "\n",
        "> $\\mathcal{F}_{n}:=\\sigma(\\mathcal{P}(\\{-n, \\ldots, n\\}))$\n",
        "\n",
        "* Sie modelliert die Informationen, dass man bis zum n-ten Zeitschritt sich bis zu n Schritte vom Ursprung entfernt hat und wäre beispielsweise die passende Filtrierung für einen einfachen symmetrischen Random\n",
        "Walk.\n",
        "\n",
        "**Filtration and Stochastic Processes**\n",
        "\n",
        "*  https://almostsure.wordpress.com/2009/11/08/filtrations-and-adapted-processes/\n",
        "\n",
        "* In mathematics, a filtration $\\mathcal{F}$ is an indexed family $\\left(S_{i}\\right)_{i \\in I}$ of subobjects of a given algebraic structure $S,$ with the index $i$ running over some totally ordered index set $I$, subject to the condition\n",
        "that\n",
        "\n",
        "> if $i \\leq j$ in $I,$ then $S_{i} \\subset S_{j}$\n",
        "\n",
        "* If the index i is the time parameter of some stochastic process, then the filtration can be interpreted as **representing all historical but not future information available about the stochastic process**, with the algebraic structure S<sub>i</sub> gaining in complexity with time.\n",
        "\n",
        "* Hence, a process that is adapted to a filtration F, is also called **non-anticipating**, i.e. one that cannot see into the future.\n",
        "\n",
        "* Eine Filtrierung (auch Filtration, Filterung oder Filtern) ist in der Theorie der stochastischen Prozesse eine Familie von verschachtelten σ-Algebren. Sie modelliert die zu verschiedenen Zeitpunkten verfügbaren Informationen zum Verlauf eines Zufallsprozesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aoj2PpxvDQV"
      },
      "source": [
        "###### *Martingale*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFV1b0SVIDD"
      },
      "source": [
        "**Martingale**\n",
        "\n",
        "* Let (Ω,F,P) be a probability space and (Ft)0≤t≤T or (Ft)0≤t<∞ a filtration by sub- σ−algebras of F. An adapted sequence Xt of integrable random variables is defined to be a\n",
        "\n",
        "  * martingale if E(Xt+1|Ft) = Xt ∀t (=for all t).\n",
        "  * submartingale if E(Xt+1|Ft) ≥ Xt ∀t.\n",
        "  * supermartingale if E(Xt+1|Ft) ≤ Xt ∀t.\n",
        "\n",
        "> A measure space is a triple $\\langle X, \\mathcal{F}, \\mu\\rangle$ where $\\langle X, \\mathcal{F}\\rangle$ is a measurable space and $\\mu$ is a measure defined on it. If $\\mu$ is in fact a probability measure we speak of a probability space and call its\n",
        "underlying measurable space a sample space. The points of a sample space are called samples\n",
        "and represent potential outcomes while the measurable sets (complexes) are called events and\n",
        "represent properties of outcomes for which we wish to assign probabilities. (Many use the term\n",
        "sample space simply for the underlying set of a probability space, particularly in the case where\n",
        "every subset is an event.) Measure spaces and probability spaces play a foundational role in\n",
        "measure theory and probability theory respectively.\n",
        "\n",
        "* In probability theory, a martingale is a sequence of random variables (for example **a stochastic process**) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value.\n",
        "\n",
        "* A **martingale is characterized by the fact that it is fair on average**. Martingales arise naturally from the modeling of fair gambling.(In a fair game of chance, **the expected value of each win is zero**)\n",
        "\n",
        "* The closely related to the martingales are the super martingales, which are stochastic processes with an average loss and submartingales, which are stochastic processes with an average gain.\n",
        "\n",
        "* The property of being a (sub- / super-) martingale does not belong to stochastic processes alone, but always to a stochastic process **in combination with filtration**. Therefore, the filtration should always be specified.\n",
        "\n",
        "A **basic definition** of a discrete-time martingale is a discrete-time stochastic process (i.e., a sequence of random variables) X1, X2, X3, ... that satisfies for any time n,\n",
        "\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|X_{n}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(X_{n+1} \\mid X_{1}, \\ldots, X_{n}\\right)=X_{n}\n",
        "\\end{array}$\n",
        "\n",
        "That is, the conditional expected value of the next observation, given all the past observations, is equal to the most recent observation.\n",
        "\n",
        "A **continuous-time martingale** with respect to the stochastic process X<sub>t</sub> is a stochastic process Y<sub>t</sub> such that for all t\n",
        "\n",
        "> $\\begin{array}{l}\n",
        "\\mathbf{E}\\left(\\left|Y_{t}\\right|\\right)<\\infty \\\\\n",
        "\\mathbf{E}\\left(Y_{t} \\mid\\left\\{X_{\\tau}, \\tau \\leq s\\right\\}\\right)=Y_{s} \\quad \\forall s \\leq t\n",
        "\\end{array}$\n",
        "\n",
        "This expresses the property that the conditional expectation of an observation at time t, given all the observations up to time s, is equal to the observation at time s (of course, provided that s ≤ t). Note that the second property implies that Yn is measurable with respect to X1 … Xn.\n",
        "\n",
        "In **full generality**, a stochastic process Y : T × Ω → S taking value in a [Banach space](https://en.m.wikipedia.org/wiki/Banach_space) S is a martingale with respect to a filtration Σ∗ and probability measure P if\n",
        "\n",
        "**Examples of martingales**\n",
        "\n",
        "* An unbiased random walk (in any number of dimensions)\n",
        "\n",
        "* A Wiener process Wt is a martingale, and for a Wiener process the processes W<sub>t</sub><sup>2</sup> - t and the geometric Brownian movement without drift are martingales.\n",
        "\n",
        "* Stopped Brownian motion, which can be used to model the trajectory of such games\n",
        "\n",
        "* A gambler's fortune (capital) is a martingale if all the betting games which the gambler plays are fair. To be more specific: suppose Xn is a gambler's fortune after n tosses of a fair coin, where the gambler wins USD 1 if the coin comes up heads and loses USD 1 if it comes up tails. The gambler's conditional expected fortune after the next trial, given the history, is equal to their present fortune. This sequence is thus a martingale.\n",
        "\n",
        "* If { Nt : t ≥ 0 } is a Poisson process with intensity λ, then the compensated Poisson process { Nt − λt : t ≥ 0 } is a continuous-time martingale with [right-continuous/left-limit](https://en.m.wikipedia.org/wiki/Classification_of_discontinuities) sample paths.\n",
        "\n",
        "* ([Likelihood-ratio testing](https://en.m.wikipedia.org/wiki/Likelihood-ratio_test) in statistics) A random variable X is thought to be distributed according either to probability density f or to a different probability density g. A random sample X1, ..., Xn is taken. Let Yn be the \"likelihood ratio\":\n",
        "\n",
        "> $Y_{n}=\\prod_{i=1}^{n} \\frac{g\\left(X_{i}\\right)}{f\\left(X_{i}\\right)}$\n",
        "\n",
        "If X is actually distributed according to the density f rather than according to g, then { Yn : n = 1, 2, 3, ... } is a martingale with respect to { Xn : n = 1, 2, 3, ... }.\n",
        "\n",
        "* [**Stopped Brownian Motion**](https://en.m.wikipedia.org/wiki/Stopped_process#Brownian_motion): a stopped process is a stochastic process that is forced to assume the same value after a prescribed (possibly random) time.\n",
        "\n",
        "Martingale Property vs Markov Property\n",
        "\n",
        "* In order to formally define the concept of Brownian motion and utilise it as a basis for an asset price model, it is necessary to define the Markov and Martingale properties. These provide an intuition as to how an asset price will behave over time.\n",
        "\n",
        "* The **Markov property** states that a stochastic process essentially has \"no memory\". This means that the conditional probability distribution of the future states of the process are independent of any previous state, with the exception of the current state.\n",
        "\n",
        "* The **Martingale property** states that the future expectation of a stochastic process is equal to the current value, given all known information about the prior events.\n",
        "\n",
        "* https://www.quantstart.com/articles/The-Markov-and-Martingale-Properties/\n",
        "\n",
        "**Simulate Martingale Process**: Toss a coin. toss results (1=lose 0=win). The first step is to find the edges of the losing runs, (steps + edges). You then need to take the difference of the sizes of the steps and shove those values back into the original data. When you take a cumsum of toss2 it gives you the current length of your losing streak. Your bet is then 2 ** cumsum(toss2)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Adapted (Stochastic) Process*"
      ],
      "metadata": {
        "id": "wc37qhDWywkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPqovDyR3ll"
      },
      "source": [
        "[Adapted (Stochastic) Process](https://de.m.wikipedia.org/wiki/Adaptierter_stochastischer_Prozess)\n",
        "\n",
        "* for exmaple in Finance\n",
        "\n",
        "* The share prices of assets in a multiperiod market depend on market scenarios, but evolve in such a way that their values at any time t, being observable at time t, do not depend on the unobservable post-t futures of the scenarios.\n",
        "\n",
        "* Thus, the price process St of a traded asset is **adapted to the natural filtration** (Ft)0≤t≤T defined by (1).\n",
        "\n",
        "* In general, a sequence Xt of random variables is said to be **adapted to a filtration** (Ft)0≤t≤T if, for each t, the random variable Xt is **Ft−measurable**, that is, if all events of the form {ω : Xt(ω) ∈ B}, where **B is a Borel** subset of the real numbers R, are members of the σ−algebra Ft."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Stopping Time*"
      ],
      "metadata": {
        "id": "_j3LHUI5y6Co"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWnh7gmSp_P"
      },
      "source": [
        "**Stopping Time**\n",
        "\n",
        "* [Stoppzeit (Markov Moment)](\n",
        "https://de.m.wikipedia.org/wiki/Stoppzeit): In der Stochastik bezeichnet der Begriff der Stoppzeit eine spezielle Art von Zufallsvariablen, die auf filtrierten Wahrscheinlichkeitsräumen definiert werden.\n",
        "\n",
        "  * Stoppzeiten sind nicht nur von Bedeutung für die Theorie der stochastischen Prozesse (beispielsweise bei der Lokalisierung von Prozessklassen oder Untersuchungen von gestoppten Prozessen), sondern auch von praktischer Relevanz, etwa für das Problem des optimalen Ausübungszeitpunkts für amerikanische Optionen.\n",
        "\n",
        "  * Eine Stoppzeit kann man als die Wartezeit interpretieren, die vergeht, bis ein bestimmtes zufälliges Ereignis eintritt. Wenn wie üblich die Filtrierung die vorhandene Information zu verschiedenen Zeitpunkten angibt, bedeutet die obige Bedingung also, dass zu jeder Zeit bekannt sein soll, ob dieses Ereignis bereits eingetreten ist oder nicht.\n",
        "\n",
        "* Optional Stopping Theorem: Das [Optional Stopping Theorem](https://de.m.wikipedia.org/wiki/Optional_Stopping_Theorem) ist ein mathematischer Satz über Martingale, eine spezielle Klasse von stochastischen Prozessen, und damit der Wahrscheinlichkeitstheorie zuzuordnen.\n",
        "\n",
        "* [Optional Sampling Theorem](https://de.m.wikipedia.org/wiki/Optional_Sampling_Theorem): Eine populäre Version dieses Theorems besagt, dass es bei einem fairen, sich wiederholenden Spiel keine Abbruchstrategie gibt, mit der man seinen Gesamtgewinn verbessern kann.\n",
        "\n",
        "* [Starke Markoweigenschaft](https://de.m.wikipedia.org/wiki/Starke_Markoweigenschaft)\n",
        "\n",
        "* Filtrierung von Stoppzeiten:\n",
        "\n",
        "  * Eine Stoppzeit $\\tau: \\Omega \\rightarrow[0, \\infty]$ bezüglich einer beliebigen Filtrierung $\\left(\\mathcal{F}_{t}\\right)_{t \\in[0, \\infty)}$ erzeugt in Analogie zur natürlichen Filtrierung eine $\\sigma$ -Algebra, die sogenannte $\\sigma$ -Algebra der $\\tau$ -Vergangenheit\n",
        "\n",
        "  > $\\mathcal{F}_{\\tau}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t}\\right\\} \\text { mit } \\mathcal{F}_{\\infty}=\\sigma\\left(\\bigcup_{t \\in[0, \\infty)} \\mathcal{F}_{t}\\right)$\n",
        "\n",
        "  * Sei nun $\\left(\\tau_{j}\\right)_{j \\in J}$ eine geordnete Familie von Stoppzeiten mit $P\\left(\\tau_{i} \\leq \\tau_{j}\\right)=1$ für alle $i, j \\in J$ mit $i \\leq j$ dann ist die Familie $\\left(\\mathcal{F}_{\\tau_{j}}\\right)_{j \\in J}$ eine Filtrierung, diese ist beim Studium von Stoppzeiten stochastischer Prozesse von Bedeutung.\n",
        "\n",
        "  * In Analogie erzeugt man die rechtsstetige Version der Filtrierung $\\left(\\mathcal{F}_{\\tau_{j}+}\\right)_{j \\in J}$ wobei:\n",
        "\n",
        "  > $\\mathcal{F}_{r+}:=\\left\\{A \\in \\mathcal{F}_{\\infty} \\mid \\forall t \\in[0, \\infty): A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t+}\\right\\} \\text { und } \\mathcal{F}_{t+}=\\bigcap_{u \\in(t, \\infty)} \\mathcal{F}_{u}$\n",
        "\n",
        "  * Es gilt immer $\\mathcal{F}_{\\tau} \\subseteq \\mathcal{F}_{r+}$\n",
        "\n",
        "* [Vorhersagbarer Prozess](https://de.m.wikipedia.org/wiki/Vorhersagbarer_Prozess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *σ-Algebra der τ-Vergangenheit*"
      ],
      "metadata": {
        "id": "qCTiyeYsy0Y0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhsdp5wBSiAq"
      },
      "source": [
        "**σ-Algebra der τ-Vergangenheit**\n",
        "\n",
        "* Die [σ-Algebra der τ-Vergangenheit](https://de.m.wikipedia.org/wiki/Σ-Algebra_der_τ-Vergangenheit) ist ein **Mengensystem**, sowie ein von der Stoppzeit abgeleitetes Konzept\n",
        "\n",
        "* Die σ-Algebra der τ-Vergangenheit ist eine **spezielle σ-Algebra**, welche über die Filtrierung und die Stoppzeit definiert wird. Sie findet beispielsweise Anwendung bei der Definition der starken Markow-Eigenschaft und dem Optional Sampling Theorem.\n",
        "\n",
        "* Sie entsteht durch Kombination einer Filtrierung mit einer Stoppzeit und findet meist Anwendung bei Aussagen über gestoppte Prozesse, also stochastische Prozesse, die an einem zufälligen Zeitpunkt angehalten werden. Zu diesen Aussagen gehören beispielsweise das Optional Stopping Theorem, das Optional Sampling Theorem und die Definition der starken Markow-Eigenschaft.\n",
        "\n",
        "* Gegeben sei ein Wahrscheinlichkeitsraum $(\\Omega, \\mathcal{A}, P)$ sowie eine Filtrierung $\\mathbb{F}=\\left(\\mathcal{F}_{t}\\right)_{t \\in T}$ bezüglich der Ober- $\\sigma$ -Algebra $\\mathcal{A}$ und eine Stoppzeit $\\tau$ bezüglich $\\mathbb{F}$. Dann heißt\n",
        "\n",
        "$\\mathcal{F}_{\\tau}=\\left\\{A \\in \\mathcal{A} \\mid A \\cap\\{\\tau \\leq t\\} \\in \\mathcal{F}_{t} \\text { für alle } t \\in T\\right\\}$\n",
        "\n",
        "die $\\sigma$ -Algebra der $\\tau$ -Vergangenheit."
      ]
    }
  ]
}