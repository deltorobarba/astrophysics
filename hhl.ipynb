{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hhl.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DuqBj0b2Igpf",
        "PREcYcRRPEQ4",
        "jCP5D4cYWAKs",
        "dToaU4Glugq8",
        "xCygJWw5N7zQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/master/hhl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46nztkAR5VJE"
      },
      "source": [
        "# **Harrow-Hassidim-Lloyd Algorithm (HHL)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jObfS0OgRhq6"
      },
      "source": [
        "!pip install cirq --quiet\n",
        "import cirq\n",
        "import sympy\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt # %matplotlib inline\n",
        "print(cirq.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okjoSzWrnzak"
      },
      "source": [
        "https://www.quantamagazine.org/new-quantum-algorithms-finally-crack-nonlinear-equations-20210105/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuqBj0b2Igpf"
      },
      "source": [
        "###### **<font color=\"blue\">Promise, Disadvantages & Applications of HHL</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymoe7LgXrWL_"
      },
      "source": [
        "The [quantum algorithm for linear systems of equations](https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd.\n",
        "\n",
        "> is a quantum algorithm for solving linear systems. \n",
        "\n",
        "> The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.\n",
        "\n",
        "* The algorithm is one of the main fundamental algorithms expected to provide a speedup over their classical counterparts, along with Shor's factoring algorithm, Grover's search algorithm, the quantum fourier transform and quantum simulation. \n",
        "\n",
        "* Provided the linear system is sparse and has a low condition number $\\kappa_{1}$ and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of $O\\left(\\log (N) \\kappa^{2}\\right)$, where $N$ is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in $O(N \\kappa)$ (or $O(N \\sqrt{\\kappa})$ for positive semidefinite matrices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfhXMt-vq9J"
      },
      "source": [
        "**Promise**: \n",
        "\n",
        "* Solving 10,000 linear equation: a classical computer needs in best case 10,000 steps. HHL just 13.\n",
        "\n",
        "* Unlike the classical solutions to the Deutsch-Jozsa and search problems, most of our classical methods for matrix manipulation do work in polynomial time. However, as data analysis becomes more and more powerful (and more and more demanding on today‚Äôs computers), the size of these matrices can make even polynomial time too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nk2sy0ovs5n"
      },
      "source": [
        "**Disadvantages:**\n",
        "\n",
        "* solution vector is not yielded (rather it prepares a quantum state that is proportional to the solution): Actually reading out the solution vector would take O(N)time, so we can only maintain the logarithmic runtime by sampling the solution vector like ‚ü®x|M|x‚ü©, where M is a quantum-mechanical operator. Therefore, **HHL is useful mainly in applications where only samples from the solution vector are needed**. \n",
        "\n",
        "* Entries of matrix have to be sparse: Additionally, although HHL is exponentially faster than Conjugate Gradient in N, it is polynomially slower in s and ùúÖ, so HHL is restricted to only those matrices that are sparse and have low condition numbers.\n",
        "\n",
        "* Must satisfy robust invertibility (means that entries of matrix must all approx. of same size)\n",
        "\n",
        "* Preparation of input vector is complicated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkF6-ZITtr7v"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsCX48YvttGG"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Quantum_optimization_algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFxhAdMPyZSq"
      },
      "source": [
        "https://www.quantamagazine.org/a-new-approach-to-multiplication-opens-the-door-to-better-quantum-computers-20190424/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rZnrWRuye6T"
      },
      "source": [
        "https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDrgK9b_Pn1J"
      },
      "source": [
        "https://medium.com/mit-6-s089-intro-to-quantum-computing/hhl-solving-linear-systems-of-equations-with-quantum-computing-efb07eb32f74"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZVFhm-v0-BI"
      },
      "source": [
        "https://qiskit.org/textbook/ch-applications/hhl_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRu5vAkIIp5w"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTF-4382uo5n"
      },
      "source": [
        "<font color=\"black\">**Applications of HHL & Familiar methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlaaHIR08KF"
      },
      "source": [
        "* Systems of linear equations arise naturally in many real-life applications in a wide range of areas, such as in the solution of Partial Differential Equations, the calibration of financial models, fluid simulation or numerical field calculation. \n",
        "\n",
        "* Makes use of Quantum Phase Estimation\n",
        "\n",
        "* Quantum Matrix Inversion\n",
        "\n",
        "* **Used in many quantum machine learning algorithms as a building block**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YObfeUgOQfyO"
      },
      "source": [
        "**Applications of HHL**\n",
        "\n",
        "* The quantum algorithm for linear systems of equations has been applied to a support vector machine, which is an optimized linear or non-linear binary classifier (https://arxiv.org/abs/1307.0471v2)\n",
        "\n",
        "* for Least-squares fitting (https://arxiv.org/abs/1204.5242)\n",
        "\n",
        "* for finite-element-methods (https://arxiv.org/abs/1512.05903) (but only for higher problems which include solutions with higher-order derivatives and large spatial dimensions. For example, problems in many-body dynamics require the solution of equations containing derivatives on orders scaling with the number of bodies, and some problems in computational finance, such as Black-Scholes models, require large spatial dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8zUGVpJ3__u"
      },
      "source": [
        "**Familiar methods of solutions**\n",
        "\n",
        "* Substitution method\n",
        "* Graphical method\n",
        "* Matrix method\n",
        "* Cramer's rule\n",
        "* Gaussian elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">**Solving Systems of Linear Equations: Squared Matrix (for HHL)**"
      ],
      "metadata": {
        "id": "PREcYcRRPEQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**If A is squared is a matrix (and has [full rank](https://de.m.wikipedia.org/wiki/Rang_(Mathematik))) in a linear system of equations:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "then you can take the [inverse](https://de.m.wikipedia.org/wiki/Inverse_Matrix) if A to solve for x:\n",
        "\n",
        "> $x = A^{-1} b$\n",
        "\n",
        "<font color=\"red\">*This part is important for HHL:*\n",
        "\n",
        "* $\\hat{A} = \\hat{A}^{\\dagger}$ $\\quad$ - Hermitian operators are [Self-adjoint operators](https://en.m.wikipedia.org/wiki/Self-adjoint_operator)\n",
        "\n",
        "* Adjungierte Matrix = transponiert + complex konjugiert (Vorzeichen umgekehrt). Hermetian: selbstadjunktiert = symmetrisch\n",
        "\n",
        "* Following from this, in bra-ket notation: $\n",
        "\\left\\langle\\phi_{i}|\\hat{A}| \\phi_{j}\\right\\rangle=\\left\\langle\\phi_{j}|\\hat{A}| \\phi_{i}\\right\\rangle^{*}\n",
        "$\n",
        "\n",
        "\n",
        "<font color=\"red\">*Since $A$ is Hermitian, it has a spectral decomposition : $\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$*\n",
        "\n",
        "<font color=\"red\">*You need the Eigendecomposition (spectral decomposition) to get the inverse of a matrix (=here unitary and hence normal)*\n",
        "\n",
        "Getting the [Matrix inverse via eigendecomposition](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition): If a matrix $\\mathbf{A}$ can be eigendecomposed and if none of its eigenvalues are zero, then $\\mathbf{A}$ is invertible and its inverse is given by\n",
        "\n",
        ">$\n",
        "\\mathbf{A}^{-1}=\\mathbf{Q}^{-1} \\mathbf{\\Lambda}^{-1} \\mathbf{Q}\n",
        "$\n",
        "\n",
        "If $\\mathbf{A}$ is a symmetric matrix, since $\\mathbf{Q}$ is formed from the eigenvectors of $\\mathbf{A}, \\mathbf{Q}$ is guaranteed to be an orthogonal matrix, therefore $\\mathbf{Q}^{-1}=\\mathbf{Q}^{\\mathrm{T}}$. Furthermore, because $\\mathbf{\\Lambda}$ is a diagonal matrix, its inverse is easy to calculate:\n",
        "\n",
        ">$\n",
        "\\left[\\Lambda^{-1}\\right]_{i i}=\\frac{1}{\\lambda_{i}}\n",
        "$\n",
        "\n",
        "*Eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. **Only diagonalizable matrices can be factorized in this way**. When the matrix being factorized is a [normal](https://en.m.wikipedia.org/wiki/Normal_matrix) or [real symmetric matrix](https://en.m.wikipedia.org/wiki/Symmetric_matrix), the decomposition is called \"spectral decomposition\", derived from the [**spectral theorem**](https://en.m.wikipedia.org/wiki/Spectral_theorem).*\n",
        "\n",
        "  * *Among complex matrices, all unitary, Hermitian, and skew-Hermitian matrices are normal*\n",
        "\n",
        "  * *The spectral theorem states that a matrix is normal if and only if it is unitarily similar to a diagonal matrix, and therefore any matrix A satisfying the equation $A^*A = AA^*$ is diagonalizable.*\n",
        "\n",
        "  * *A complex square matrix A is normal if it commutes with its conjugate transpose $A^*$ : $A$ normal $\\Longleftrightarrow A^{*} A=A A^{*}$*"
      ],
      "metadata": {
        "id": "5z-Ez-u4RWhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also other ways to get the matrix inverse besides Eigendecomposition, for example Gaussian Elemination, Cramers rule, mewtons method, cholesky decomposition: https://en.m.wikipedia.org/wiki/Invertible_matrix"
      ],
      "metadata": {
        "id": "xEDbM5qbXhK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Matrix_inverse_via_eigendecomposition"
      ],
      "metadata": {
        "id": "AwM8vr_OXjVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">**Solving Systems of Linear Equations: Non-Squared Matrix (Linear Least Squares)**"
      ],
      "metadata": {
        "id": "jCP5D4cYWAKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m\\times n$ matrix. It is related to the polar decomposition.*\n",
        "\n",
        "* non squared matrix\n",
        "\n",
        "* to solve you need the inverse, but normal eigendecomposition doesn work\n",
        "\n",
        "> you need to compute mooore penrose pseudo-inverse, and here you need to apply singular value decomposition to get eigendecomposition\n",
        "\n",
        "* SVD is a type of [Matrix decomposition](https://en.m.wikipedia.org/wiki/Matrix_decomposition), specicially one based on eigenvalue concepts. and SVD is a full rank decomposition (besides broader [Rank factorization methods](https://en.m.wikipedia.org/wiki/Rank_factorization))\n",
        "\n",
        "* matrix decompositions in general are used to take a large matrix apart (i.e. factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U) so the new matrices require fewer additions and multiplications to solve, compared with the original system $A\\mathbf {x} =\\mathbf {b}$"
      ],
      "metadata": {
        "id": "g9VGZpsMZff2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**If a matrix A is not squared:**\n",
        "\n",
        "> $A x = b$ $\\quad$ ($A$ is not regular)\n",
        "\n",
        "You multiply both sides by the $A^{T}$, which is the transpose of A:\n",
        "\n",
        "> $A^{T} A x = A^{T} b$\n",
        "\n",
        "Then move $A^{T} A$ on right side (by taking their inverse). This is not the original x anymore, but the [least squares](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate#Lineare_Modellfunktion) $\\hat{x}$\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "And that term <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> is know as (Moore‚ÄìPenrose) pseudo-inverse <font color=\"blue\">$A^{+}$</font>:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$A^{+}$</font> $b$\n",
        "\n",
        "And a pseudo-inverse is nothing else than our least-squares solutions. See more details under [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)"
      ],
      "metadata": {
        "id": "8VExX1YtO5YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Example:**\n",
        "\n",
        "> $A x = b$\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right] x=\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "* We can immediately see that matrix A is of rank 2 because the first two rows are multiples of each other (2 and -2 and -2 and 2), just the last row numbers are not multiples of each other (5 and 3).\n",
        "\n",
        "* Now we can apply the pseudo-inverse to find the least-squares solution:\n",
        "\n",
        "> $\\hat{x} =$ <font color=\"blue\">$(A^{T} A)^{-1} A^{T}$</font> $b$\n",
        "\n",
        "> $\\hat{x}=$ <font color=\"blue\">$\\left(\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{ccc}2 & -2 & 5 \\\\ -2 & 2 & 3\\end{array}\\right]$</font>$\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$\n",
        "\n",
        "> $\\hat{x}=$$\\left[\\begin{array}{l}-4 \\\\ -2\\end{array}\\right]$\n",
        "\n",
        "\n",
        "*Inserting the least-squares $\\hat{x}$ into the original equation:*\n",
        "\n",
        "> $\\left[\\begin{array}{cc}2 & -2 \\\\ -2 & 2 \\\\ 5 & 3\\end{array}\\right]\\left[\\begin{array}{l}-4 \\\\ -3\\end{array}\\right]$ = $\\left[\\begin{array}{c}2 \\cdot(-4)+(-2) \\cdot(-3) \\\\ (-2)\\cdot (-4)+2 \\cdot (-3) \\\\ 5\\cdot (-4)+3 \\cdot (-3)\\end{array}\\right]$ = $\\left[\\begin{array}{c}-2 \\\\ 2 \\\\ -29\\end{array}\\right]$ $\\approx$ $\\left[\\begin{array}{c}-1 \\\\ 7 \\\\ -26\\end{array}\\right]$"
      ],
      "metadata": {
        "id": "HD69HxTxRfun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Mathematical Background about Linear least squares:**\n",
        "\n",
        "\n",
        "*The [Linear least squares (LLS)](https://en.wikipedia.org/wiki/Linear_least_squares) is the least squares approximation of linear functions to data. It is a set of formulations for solving statistical problems involved in linear regression. Numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods.*\n",
        "\n",
        "*If solutions to a linear system exist but A does not have full column rank, then we have an indeterminate system, all of whose infinitude of solutions are given by this last equation. ([Moore-Penrose Inverse](https://en.m.wikipedia.org/wiki/Moore‚ÄìPenrose_inverse#Obtaining_all_solutions_of_a_linear_system))*\n",
        "\n",
        "> Very important articles are [Numerical linear algebra](https://en.m.wikipedia.org/wiki/Numerical_linear_algebra), [Numerical methods for linear least squares](https://en.m.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares), [Eigendecomposition of a matrix](https://en.m.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), and [System of linear equations](https://en.m.wikipedia.org/wiki/System_of_linear_equations), incl. outlook to [Non-Linear Least-Squares](https://en.m.wikipedia.org/wiki/Non-linear_least_squares)"
      ],
      "metadata": {
        "id": "deVP7deKV1lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Mathematical Background about Pseudo-Inverse:**\n",
        "\n",
        "*The [pseudo-inverse](https://de.m.wikipedia.org/wiki/Pseudoinverse) ist eine Verallgemeinerung der inversen Matrix auf singul√§re und nichtquadratische Matrizen, weshalb sie h√§ufig auch als verallgemeinerte Inverse bezeichnet wird. Der h√§ufigste Anwendungsfall f√ºr Pseudoinversen ist die L√∂sung linearer Gleichungssysteme und [linearer Ausgleichsprobleme](https://de.wikipedia.org/wiki/Ausgleichungsrechnung) (curve fitting wie in Regression oder im Machine Learning).*\n",
        "\n",
        "*The pseudoinverse provides a least squares solution to a system of linear equations.' Siehe auch Linear Least Squares in bezug auf [Moore-Penrose inverse](https://en.m.wikipedia.org/wiki/Moore‚ÄìPenrose_inverse) (= pseudoinserve = generalization of the inverse matrix).*\n",
        "\n",
        "* Overdetermined case: A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a solution (see below under ¬ß Applications). \n",
        "\n",
        "* Underdetermined case: Another use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.\n",
        "\n",
        "* The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers. It can be computed using the [singular value decomposition](https://en.m.wikipedia.org/wiki/Singular_value_decomposition)."
      ],
      "metadata": {
        "id": "uSBKMGD-P_Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">*Underdetermined system*\n",
        "\n",
        "* *a system of linear equations or a system of polynomial equations is considered underdetermined if there are fewer equations than unknowns*\n",
        "\n",
        "* *An [underdetermined system of linear equations](https://en.m.wikipedia.org/wiki/Underdetermined_system) has an infinite number of solutions, if any. However, in optimization problems that are subject to linear equality constraints, only one of the solutions is relevant, namely the one giving the highest or lowest value of an objective function.*\n",
        "\n",
        "  * *In an underdetermined system of linear equations the use of the (Moore Pensore) pseudoinverse is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions.*\n",
        "\n",
        "  * *The pseudoinverse facilitates the statement and proof of results in linear algebra. The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers.* \n",
        "\n",
        "  * *It can be computed using the singular value decomposition.*\n",
        "\n",
        "* *The solution set for two equations in three variables is, in general, a line ([Source](https://en.m.wikipedia.org/wiki/System_of_linear_equations#General_behavior)):*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Intersecting_Planes_2.svg/240px-Intersecting_Planes_2.svg.png)"
      ],
      "metadata": {
        "id": "F5GdcTdgLRJ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToaU4Glugq8"
      },
      "source": [
        "###### <font color=\"blue\">**HHL Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKEqJ1OVvYPS"
      },
      "source": [
        "> **Given a matrix $A \\in \\mathbb{C}^{N \\times N}$ and a vector $\\vec{b} \\in \\mathbb{C}^{N}$, find $\\vec{x} \\in \\mathbb{C}^{N}$ satisfying $A \\vec{x}=\\vec{b}$**\n",
        "\n",
        "> The spectrum of $A$ is given by: $A\\left|v_{j}\\right\\rangle=\\lambda_{j}\\left|v_{j}\\right\\rangle, 1 \\geq\\left|\\lambda_{j}\\right| \\geq 1 / \\kappa$\n",
        "\n",
        "*One crucial remark to keep in mind is that the classical algorithm returns the full solution, while the HHL can only approximate functions of the solution vector.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Solving a system of linear equations with a quantum computer (HHL)**\n",
        "\n",
        "We want to solve a system of linear equations by finding $\\vec{x}$:\n",
        "\n",
        "> $A \\vec{x} = \\vec{b}$\n",
        "\n",
        "Classically you would take the inverse of $A$ (via spectral decomposition / eigendecomposition):\n",
        "\n",
        "> $\\vec{x} = A^{-1} \\vec{b}$\n",
        "\n",
        "The first step towards solving a system of linear equations with a quantum computer is to encode the problem in the quantum language. \n",
        "\n",
        "* By rescaling the system, we can assume $\\vec{b}$ and $\\vec{x}$ to be normalised and map them to the respective quantum states $|b\\rangle$ and $|x\\rangle$. \n",
        "\n",
        "* Usually the mapping used is such that $i^{\\text {th }}$ component of $\\vec{b}$ (resp. $\\vec{x}$ ) corresponds to the amplitude of the $i^{\\text {th }}$ basis state of the quantum state $|b\\rangle$ (resp. $|x\\rangle$ ). \n",
        "\n",
        "From now on, we will focus on the rescaled problem\n",
        "\n",
        "><font color=\"blue\">$A|x\\rangle=|b\\rangle\n",
        "$</font> $\\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        "><font color=\"blue\">$|x\\rangle=A^{-1}|b\\rangle$</font> $\\quad$ (the solution is: $|x\\rangle = \\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle$)\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        " \n",
        ">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        ">$\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$\n",
        "\n",
        "and the right hand side of the system can be written in the eigenbasis of $A$ as\n",
        "\n",
        ">$\n",
        "|b\\rangle=\\sum_{j=0}^{N-1} b_{j}\\left|u_{j}\\right\\rangle, \\quad b_{j} \\in \\mathbb{C}\n",
        "$\n",
        "\n",
        "It is useful to keep in mind that the goal of the HHL is to exit the algorithm with the readout register in the state\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle\n",
        "$\n",
        "\n",
        "Note that here we already have an implicit normalisation constant since we are talking about a quantum state."
      ],
      "metadata": {
        "id": "M6rk5CuVYtO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qiskit.org/textbook/ch-applications/hhl_tutorial.html"
      ],
      "metadata": {
        "id": "_fTx-FfaYLO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Remember: Hermetian Operator $A$:*\n",
        "\n",
        "* **Transponierte Matrix**: bijektive, selbstinverse Abbildung einer reellen Matrix\n",
        "\n",
        "* **Konjugierte Matrizen**: Vertauschen der Vorzeichen\n",
        "\n",
        "* **Adjungierte Matrix = transponiert + konjugiert** / komplexwertige Matrix, die transponiert + (komplex) conjugiert ist (Vorzeichen umgekehrt)\n",
        "\n",
        "* **Hermetisch: selbstadjunktiert = symmetrisch** / also wenn man die adjunktierte von A* bildet, kommt wieder A raus: A* = A / Spiegelung an der Diagonalen. Matrix muss quadratisch sein im reellen ist es die Symmetrie"
      ],
      "metadata": {
        "id": "GpbZtjbRZJrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"blue\">**HHL Algorithm**"
      ],
      "metadata": {
        "id": "xCygJWw5N7zQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPc-sWLFPAl8"
      },
      "source": [
        "**Main Subroutines in HHL:**\n",
        "\n",
        "* Hamiltonian simulation\n",
        "* Phase estimation (newer: linear combination of unitaries)\n",
        "* (Variable-time) amplitude amplification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij_dB1BOtFEY"
      },
      "source": [
        "**HHL-Approach**\n",
        "\n",
        "1. Prepare the initial state $|b\\rangle$. Note that $|b\\rangle=\\sum_{j} c_{j}\\left|v_{j}\\right\\rangle$.\n",
        "\n",
        "2. Use the so-called phase estimation algorithm to perform the map\n",
        "$|b\\rangle \\rightarrow \\sum_{j} c_{j}\\left|v_{j}\\right\\rangle\\left|\\tilde{\\lambda}_{j}\\right\\rangle$\n",
        "\n",
        "* $|\\tilde{\\lambda}_{j}\\rangle$ -> This register contains the eigenvalue estimates.\n",
        "\n",
        "3. Apply a one-qubit conditional rotation to perform the map\n",
        "$|0\\rangle \\rightarrow \\frac{1}{\\kappa \\tilde{\\lambda}_{j}}|0\\rangle+\\sqrt{1-\\frac{1}{\\kappa^{2} \\tilde{\\lambda}_{j}^{2}}}|1\\rangle$\n",
        "\n",
        "4. Undo step 2 - apply the inverse of phase estimation\n",
        "$\\sum_{j} \\frac{c_{j}}{\\kappa \\tilde{\\lambda}_{j}}\\left|v_{j}\\right\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle \\approx \\frac{1}{\\kappa A}|b\\rangle|0\\rangle+|\\mathrm{bad}\\rangle|1\\rangle$\n",
        "\n",
        "5. Use amplitude amplification to get rid of the ‚Äûbad‚Äú part of the state with |1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-28aw9k0W5g"
      },
      "source": [
        "https://en.m.wikipedia.org/wiki/System_of_linear_equations#Matrix_solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHP9P0QWxwLB"
      },
      "source": [
        "[Peter Witteg: Quantum Machine Learning - 37 - Overview of the HHL Algorithm](https://m.youtube.com/watch?v=hQpdPM-6wtU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBhbEu38xh-T"
      },
      "source": [
        "[Google Quantum: Quantum Algorithms for Systems of Linear Equations (Quantum Summer Symposium 2020)](https://m.youtube.com/watch?v=Xvp56xeNZo4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qiskit.org/textbook/ch-applications/hhl_tutorial.html"
      ],
      "metadata": {
        "id": "dvzqeyeZVrok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit\n",
        "\n",
        "import numpy as np\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.algorithms.linear_solvers.hhl import HHL\n",
        "from qiskit.algorithms.linear_solvers.matrices import TridiagonalToeplitz\n",
        "from qiskit.algorithms.linear_solvers.observables import MatrixFunctional"
      ],
      "metadata": {
        "id": "zOmBw8JaVGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = TridiagonalToeplitz(2, 1, 1 / 3, trotter_steps=2)\n",
        "right_hand_side = [1.0, -2.1, 3.2, -4.3]\n",
        "observable = MatrixFunctional(1, 1 / 2)\n",
        "rhs = right_hand_side / np.linalg.norm(right_hand_side)\n",
        "\n",
        "# Initial state circuit\n",
        "num_qubits = matrix.num_state_qubits\n",
        "qc = QuantumCircuit(num_qubits)\n",
        "qc.isometry(rhs, list(range(num_qubits)), None)\n",
        "\n",
        "hhl = HHL()\n",
        "solution = hhl.solve(matrix, qc, observable)\n",
        "approx_result = solution.observable"
      ],
      "metadata": {
        "id": "pNOGdaYqVhYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qiskit.org/documentation/stubs/qiskit.algorithms.HHL.html"
      ],
      "metadata": {
        "id": "zfaBZWM1U7Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/quantumlib/Cirq/blob/master/examples/hhl.py"
      ],
      "metadata": {
        "id": "X_MTJockUj_g"
      }
    }
  ]
}