{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7CL5LxdRiQLl",
        "gmN2IqZ4FDsw",
        "nrkLBUH1hb36",
        "qYW5u_0Oi7l5",
        "0JfDpLK4KBqT",
        "4SpYIJ602L38",
        "SZkKviQcpLi5",
        "F1l9_Erf5T5S",
        "6quxRLo3U9Qf",
        "32OR42wtUS3y",
        "_Syw4dL96u31",
        "Cmqnes9brFPk",
        "ijtvzpIDxEzV",
        "ThL-AmiaxI8J",
        "8SsJmnB0xNNN",
        "l-DiG5X3xTfp",
        "oKvbzZ8cxOS1",
        "eLjOkdyQxLeb",
        "V36uKfvAxRCm",
        "hiezBxLBxVLy",
        "IJCykPfwxdQn",
        "zXCXyX9DxfdE",
        "GL1iMe8igraR",
        "iptu2LVks6If",
        "jBh1Tr-vn_4f",
        "cqdowsM5oHEP",
        "1fAAZcgWoiJB",
        "cyrUknMLo8bl",
        "M0kD-s4t82ez",
        "9GXuS8c94Iay",
        "gXZXzSSsrrUL",
        "VHiwFI7ovP7J",
        "dcFMdt_xthGK",
        "cPAo7HRV06Ol",
        "3hazOulegQat",
        "EBxdSfTogMN6",
        "RscthBMzOLUM",
        "7sf5tauFtttx",
        "jdJ5yNTZibYy",
        "AZ-w7Fv4O91T",
        "xEVSiuftty-3"
      ],
      "authorship_tag": "ABX9TyNdSNbd4RWeVeQO4dP9SHwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/sciences/blob/master/machinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">**Machine Learning**"
      ],
      "metadata": {
        "id": "KTKJItkyUBMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1000.png)"
      ],
      "metadata": {
        "id": "cQZhym5giJlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">**Quantum Book**"
      ],
      "metadata": {
        "id": "7CL5LxdRiQLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Spaces & Distances*"
      ],
      "metadata": {
        "id": "gmN2IqZ4FDsw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrkLBUH1hb36"
      },
      "source": [
        "###### *Spaces*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqI_Wvvlu5y7"
      },
      "source": [
        "**What constitutes a 'space'?**\n",
        "\n",
        "* **A space is a [set](https://en.m.wikipedia.org/wiki/Set_(mathematics)) - (sometimes called a [universe - Grundmenge](https://en.m.wikipedia.org/wiki/Universe_(mathematics))) with some added structure.**\n",
        "\n",
        "* A space consists of selected **mathematical objects that are treated as points**, and selected **relationships between these points** (nature of the points can vary widely: for example, the **points can be elements of a set, functions on another space, or subspaces of another space.**)\n",
        "\n",
        "\n",
        "* **[Taxonomy of Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces)**: While each type of space has its own definition, the general idea of \"space\" evades formalization (modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, but it does not define the notion of \"space\" itself)\n",
        "\n",
        "Quelle: [Einordnung in die Hierarchie mathematischer Strukturen](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Einordnung_in_die_Hierarchie_mathematischer_Strukturen) sowie [Topologische Räume](https://de.m.wikipedia.org/wiki/Topologischer_Raum#Beispiele)\n",
        "\n",
        "Topologischer Raum |  | dazugehörige Struktur\n",
        "--- | --- | ---\n",
        "[Euklidischer Raum](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) | hat | Skalarprodukt\n",
        "[Normierter Raum](https://de.m.wikipedia.org/wiki/Normierter_Raum) | hat | Norm\n",
        "[Metrischer Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) | hat | Metrik\n",
        "[Uniformer Raum](https://de.m.wikipedia.org/wiki/Uniformer_Raum) | hat | Uniforme Struktur\n",
        "[Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum) | hat | Topologie\n",
        "\n",
        "![xxx](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Beziehungen_zwischen_mathematischen_Räumen.svg/220px-Beziehungen_zwischen_mathematischen_Räumen.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ostHcxEvNZ5r"
      },
      "source": [
        "![Normed Vector Space](https://upload.wikimedia.org/wikipedia/commons/7/74/Mathematical_Spaces.png)\n",
        "\n",
        "Quelle: [Mathematical Spaces](https://en.m.wikipedia.org/wiki/Space_(mathematics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYW5u_0Oi7l5"
      },
      "source": [
        "###### *Inner Product Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3TsP_aDsFR"
      },
      "source": [
        "**Inner Product**\n",
        "\n",
        "> * **Inner product $\\rightarrow$ measures distances, lengths, angles**\n",
        "\n",
        "* Das innere Produkt (Skalarprodukt / Produktpunkt called when applied to functions - the alternate name of inner product in linear algebra is 'dot product') stellt quasi eine Geometrie im Vektorraum her, wir können dadurch definieren, welche Vektoren orthogonal, und welche parallel zueinander sind.\n",
        "\n",
        "* **Das Skalarprodukt benötigt man**,\n",
        "  * um die Lange von Vektoren zu berechnen,\n",
        "  * den Winkel zwischen Vektoren zu berechnen (uber Cosinus von Alpha) und\n",
        "  * ob zwei Vektoren senkrecht zueinander stehen.\n",
        "\n",
        "* Das **Dot Product / Scalar Product / [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt)** (auch inneres Produkt oder Punktprodukt) ist eine [mathematische Verknüpfung](https://de.wikipedia.org/wiki/Verknüpfung_(Mathematik)), die zwei Vektoren eine Zahl (Skalar) zuordnet.\n",
        "\n",
        "* **Scalar vs Scalar Product**: A [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics)) is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector. The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically, the determinant can be viewed as the volume scaling factor of the linear transformation described by the matrix.\n",
        "\n",
        "* Geometrisch berechnet man das Skalarprodukt zweier Vektoren $\\vec{a}$ und $\\vec{b}$ nach der Formel:\n",
        "\n",
        "> $\n",
        "\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})\n",
        "$\n",
        "\n",
        "* Null, wenn sie senkrecht zueinander stehen, und maximal, wenn sie die gleiche Richtung haben.\n",
        "\n",
        "* **A common special case of the inner product is the scalar product or dot product, is written with a centered dot a ⋅ b.**\n",
        "\n",
        "* In [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) the inner product is the dot product, also known as the scalar product. (They generalize Euclidean spaces to vector spaces of any (possibly infinite) dimension.)\n",
        "\n",
        "* Ist das Skalarprodukt von zwei Vektoren $\n",
        "\\vec{a} \\cdot \\vec{b}= 0$, dann folgt daraus, dass diese orthogonal zueinander stehen.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * A simple example is the real numbers $\\mathbb{R}$ with the standard multiplication as the inner product $\\langle x, y\\rangle:=x y$\n",
        "\n",
        "  * **Inner Product of Functions** says how similar two functions are (how much they align with each other). **Inner product of function are used a lot in Fourier Transform**\n",
        "\n",
        "    * i.e. if they are orthogonal, then zero. if they are very similar, then they have a large inner product\n",
        "\n",
        "    > $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "\n",
        "    * You can also take samples from both functions and calculate the inner product between both. Up to infinity, you get at the integral like written above (Riemann approximation of the continuuos integral above):\n",
        "\n",
        "    > $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcOpkYQjFKx"
      },
      "source": [
        "**Inner Product Space**\n",
        "\n",
        "* Der **[Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space)** (Prähilbertraum bzw. Skalarprodukt) ist ein **Vektorraum**, auf dem ein **inneres Produkt definiert ist**.\n",
        "\n",
        "* An [inner product space](https://en.m.wikipedia.org/wiki/Inner_product_space) is a normed space, **where the norm of a vector is the square root of the inner product of the vector by itself**: $\\sqrt{\\vec{x} \\cdot \\vec{x}} = \\sqrt{{x}^{2}}$\n",
        "\n",
        "* [Inner product spaces](https://en.m.wikipedia.org/wiki/Inner_product_space) generalize **Euclidean spaces (in which the inner product is the dot product, also known as the scalar product**) to vector spaces of any (possibly infinite) dimension.\n",
        "\n",
        "An **inner product space** is a vector space $V$ over the field $\\mathbb{F}$ together with a map\n",
        "\n",
        "$\n",
        "\\langle\\cdot, \\cdot\\rangle: V \\times V \\rightarrow \\mathbb{F}\n",
        "$\n",
        "\n",
        "called an inner product that satisfies the following conditions $(1),(2),$ and $(3)$ for all vectors $x, y, z \\in V$ and all scalars $a \\in \\mathbb{F}:$ [see here](https://en.m.wikipedia.org/wiki/Inner_product_space)\n",
        "\n",
        "* In linear algebra, **an inner product space is a vector space with an additional structure called an inner product**. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Geometric interpretation of the angle between two vectors defined using an inner product.\n",
        "\n",
        "> **Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors**. They also provide the means of defining orthogonality between vectors (**zero inner product**).\n",
        "\n",
        "> An inner product **naturally induces an associated norm**, (|x| and |y| are the norms of x and y, in the picture) thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Normed Vector Space*"
      ],
      "metadata": {
        "id": "0JfDpLK4KBqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "\n",
        "classical probability theory: 1-Norm: ∑ pi = ∑ | p | = || p || 1 = 1\n",
        "\n",
        "quantum probabiliyt theory: 2-Norm: || | ψ > || 2 = 1\n",
        "\n",
        "**Reeller Vektor**\n",
        "\n",
        "Die 1-, 2-, 3- und $\\infty$-Normen des reellen Vektors $x=(3,-2,6)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3|^3+|-2|^3+|6|^3}=\\sqrt[3]{251} \\approx 6,308 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Komplexer Vektor**\n",
        "\n",
        "Die 1-, 2-, 3-und $\\infty$-Normen des komplexen Vektors $x=(3-4 i,-2 i)$ sind jeweils gegeben als\n",
        "\n",
        ">$\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_3=\\sqrt[3]{|3-4 i|^3+|-2 i|^3}=\\sqrt[3]{5^3+2^3}=\\sqrt[3]{133} \\approx 5,104 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}\n",
        "$"
      ],
      "metadata": {
        "id": "1Ui-wrEg177C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhAD7kvd3uuL"
      },
      "source": [
        "> **Eine Norm gibt die <u>Größe / Länge (Betrag) eines Elements</u> in einem Vektorraum an**\n",
        "\n",
        "* Eine Metrik gibt hingegen die Distanz zwischen <u>zwei Vektoren (Punkten)</u> an. A norm induces a (distance) metric by the formula d (x,y) = ‖ y-x ‖ (but Instead of distance between points, a norm gives us the length of a vector, as measured from the origin)\n",
        "\n",
        "* Eine **[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) ist eine Abbildung (Funktion)** $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$, welche einem Element von einem reellen oder komplexen Vektorraum eine **nicht-negative reelle Zahl** $\\mathbb{R}^{\\geq \\ 0 }$ zuordnet und folgende Eigenschaften besitzt (für alle $x, y$ aus dem $\\mathbb{K}$ Vektorraum und alle $\\lambda$ aus $\\mathbb{K}$):\n",
        "\n",
        "1. **[Definitheit](https://de.m.wikipedia.org/wiki/Definitheit)**:\n",
        "  * It is **nonnegative**, that is for every vector x, one has ‖x‖ ≥ 0.\n",
        "  * It is **positive on nonzero vectors**, that is, ‖x‖ = 0 ⟺ x = 0.\n",
        "\n",
        "2. **[Absolute Homogenität](https://de.m.wikipedia.org/wiki/Homogene_Funktion)**: For every vector x, and every **scalar α**, one has ‖ α x ‖ = | α | ‖ x ‖.\n",
        "\n",
        "3. **[Subadditivität, Dreiecksungleichung](https://de.m.wikipedia.org/wiki/Additive_Funktion#Sub-_und_Superadditivität)**: for every vectors x and y, one has ‖ x+y ‖ ≤ ‖ x ‖ + ‖ y ‖.\n",
        "\n",
        "* From Inner Products to Norms: Eine Norm kann (muss aber nicht) von einem [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (eine sogenannte ['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall is **the norm of a vector the square root of the inner product of the vector by itself**. A complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "\n",
        "* **Any normed vector space is a metric space** by defining d(x, y) = ‖ y - x ‖, see also metrics on vector spaces. If such a space is complete, we call it a [Banach space](https://de.m.wikipedia.org/wiki/Banachraum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygj95-xb7Eu"
      },
      "source": [
        "[**1. Normen auf endlichdimensionalen Vektorräumen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "\n",
        "* [**Zahlnorm**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): ein Beispiel ist die [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion).\n",
        "\n",
        "  * ist **induziert vom Standardskalarprodukt** (erfüllt die drei Normaxiome Definitheit, absolute Homogenität und Subadditivität) zweier reeller bzw. komplexen Zahlen. Die Betragsnorm ist. der Betrag einer reellen Zahl $z \\in \\mathbb{R}$:\n",
        "  > $\n",
        "\\|z\\|=|z|=\\sqrt{z^{2}}=\\left\\{\\begin{array}{cl}\n",
        "z & \\text { für } z \\geq 0 \\\\\n",
        "-z & \\text { für } z < 0\n",
        "\\end {array}\\right.$\n",
        "\n",
        "* [**Matrixnorm**](https://de.m.wikipedia.org/wiki/Matrixnorm): Siehe [Norm -> Matrixnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Matrixnormen) sowie [https://de.m.wikipedia.org/wiki/Matrixnorm](Matrixnorm)\n",
        "  * [Natürliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): Eine natürliche Matrixnorm entspricht anschaulich dem größtmöglichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor entsteht\n",
        "  * [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm):  Anschaulich entspricht die Spektralnorm damit dem größtmöglichen Streckungsfaktor, der durch die Anwendung der Matrix auf einen Vektor der Länge Eins entsteht = Die Spektralnorm einer Matrix entspricht ihrem maximalen Singulärwert, also der Wurzel des größten Eigenwerts des Produkts der adjungierten (transponierten) Matrix mit dieser Matrix. Spektralnorm die von der euklidischen Norm abgeleitete natürliche Matrixnorm. Von: [Singulärwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "\n",
        "* [**Vektornormen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen): ein Beispiel sind die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm)\n",
        "\n",
        "  * Die [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) sind eine Klasse von [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen), die für reelle Zahlen $p \\geq 1$ definiert sind.\n",
        "\n",
        "  * Die $p$ -Norm eines Vektors $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{K}^{n}$ mit $\\mathbb{K}=\\mathbb{R}$ oder $\\mathbb{C}$ ist für reelles\n",
        "$1 \\leq p<\\infty$ definiert durch:\n",
        "\n",
        "  > **$\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$**\n",
        "\n",
        "  * p-Normen **erfüllen die Minkowski-Ungleichung sowie die Hölder-Ungleichung**. For all p ≥ 1, the p-norms erfüllen die drei Normaxiome Definitheit, absolute Homogenität und Subadditivität.\n",
        "\n",
        "  * Die wichtigsten Vektornormen sind die [**Summennorm**](https://de.m.wikipedia.org/wiki/Summennorm) , Euklidische Norm und [**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**2. Normen auf unendlichdimensionalen Vektorräumen**](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorräumen)\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**[Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum)**\n",
        "\n",
        "* **The sequence space (Folgenraum) is a special case of the function space (Funktionenraum): $\\ell_{\\infty}=L_{\\infty}(\\mathbb{N})$ where the natural numbers are equipped with the counting measure.**\n",
        "\n",
        "* Die $\\ell^{p}$ -Normen sind die Verallgemeinerung der $p$ -Normen auf Folgenräume, wobei lediglich die endliche Summe durch eine unendliche ersetzt wird. Die $\\ell^{p}$ -Norm einer in $p$ -ter Potenz betragsweise summierbaren Folge ist für reelles $1 \\leq p<\\infty$ dann definiert als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Versehen mit diesen Normen werden die $\\ell$ - Räume jeweils zu vollständigen normierten Räumen. ${ }^{[6]}$ Für den Grenzwert $p \\rightarrow \\infty$ ergibt sich der Raum der beschränkten Folgen $\\ell^{\\infty}$ mit der Supremumsnorm.\n",
        "\n",
        "**[Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum**\n",
        "\n",
        "* Siehe auch L-p-Raum: https://de.m.wikipedia.org/wiki/Lp-Raum\n",
        "\n",
        "* Die $\\mathcal{L}^{p}$ -Normen einer in $p$ -ter Potenz **Lebesgue-integrierbaren Funktion** mit $1<p<\\infty$ sind in Analogie zu den $\\ell^{p}$ -Normen definiert als\n",
        "\n",
        "> $\n",
        "\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}\n",
        "$\n",
        "\n",
        "* **wobei die Summe durch ein Integral ersetzt wurde**. Ebenso wie bei der wesentlichen Supremumsnorm sind diese Narmen zunächst nur Halbnormen, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Maß Null II von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man wieder die Menge der Äauivalenzklassen unnn Funktionen $[f] \\in L^{p}(\\Omega)$, die fast überall gleich sind, und definiert auf diesen $L^{p}$ -Räumen die $L^{p}$ -Normen durch\n",
        "$\\|[f]\\|_{L P(\\Omega)}=\\|f\\|_{\\mathcal{L}^{p}(\\Omega)}$\n"
      ],
      "metadata": {
        "id": "EC3JVQRj2H01"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27wE3ZTM_154"
      },
      "source": [
        "**Beispiele fur Normed Vector Spaces**\n",
        "\n",
        "* **A [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) or normed space <u>is a vector space</u> over the real or complex numbers, on which a norm is defined**.\n",
        "\n",
        "  * Ist $V$ ein [Vektorraum](https://de.m.wikipedia.org/wiki/Vektorraum) über dem Körper $\\mathbb{K}$ der reellen oder der komplexen Zahlen und $\\|\\cdot\\|: V \\rightarrow \\mathbb{R}_{0}^{+}$ eine Norm auf $V,$ dann nennt man das Paar $(V,\\|\\cdot\\|)$ einen **normierten Vektorraum**.\n",
        "\n",
        "  * **Any normed vector space is a metric space** by defining d(x, y) = ‖ y - x ‖, see also metrics on vector spaces. (If such a space is complete, we call it a Banach space.)\n",
        "\n",
        "  * **Be careful**: A [vector space](https://en.m.wikipedia.org/wiki/Vector_space) is an [algebraic structure](https://en.m.wikipedia.org/wiki/Outline_of_algebraic_structures), meanwhile a [normed vector space](https://en.m.wikipedia.org/wiki/Normed_vector_space) is a type of [abstract (topological) space](https://en.m.wikipedia.org/wiki/Space_(mathematics)#Taxonomy_of_spaces) (a normed vector space is a vector space over the real or complex numbers, on which a norm is defined). (See also [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space)\n",
        "\n",
        "* **Banach Space**: **$\\mathbb{R}$<sup>n</sup> together with the p-norm is a [Banach space](https://de.wikipedia.org/wiki/Banachraum) = ein vollständiger normierter Vektorraum**. This Banach space is the Lp-space over Rn. Viele **Folgenräume $\\ell$** oder **Funktionenräume $L$** sind unendlichdimensionale Banachräume. Function Spaces $L$ are a type of infinite vector space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces**, whose vectors are functions.\n",
        "\n",
        "* **Hilbert Space**: Ein Banachraum, dessen Norm durch ein Skalarprodukt induziert ist, heißt **[Hilbertraum](https://de.wikipedia.org/wiki/Hilbertraum)**. (z.B. p2-Norm Euklidische Norm, aber nicht p1-Summennorm). Lässt man die Bedingung der Vollständigkeit fallen, spricht man von einem Prähilbertraum).\n",
        "  * Die Struktur eines Hilbertraums ist eindeutig festgelegt durch seine Hilbertraumdimension. Diese kann eine beliebige Kardinalzahl sein. Ist die Dimension endlich und betrachtet man als Körper die reellen Zahlen, so handelt es sich um einen euklidischen Raum.\n",
        "  * Hilberträume tragen durch ihr Skalarprodukt eine topologische Struktur. Dadurch sind hier im Gegensatz zu allgemeinen Vektorräumen Grenzwertprozesse möglich.\n",
        "\n",
        "* **Hardy-Space**: Untersucht man statt der messbaren Funktionen nur die holomorphen beziehungsweise die harmonischen Funktionen auf Integrierbarkeit, so werden die entsprechenden $L^{p}$-Räume [Hardy-Räume](https://de.m.wikipedia.org/wiki/Hardy-Raum) genannt, [L_p Space-Hardy](https://de.m.wikipedia.org/wiki/Lp-Raum#Hardy-R%C3%A4ume)\n",
        "\n",
        "* **F-Space**: The **space Lp for 0 < p < 1 is an [F-space](https://en.m.wikipedia.org/wiki/F-space)**: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ≥ 1. Some authors use the term [Fréchet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space) rather than F-space, but usually the term \"Fréchet space\" is reserved for locally convex F-spaces.\n",
        "\n",
        "* **Sobolev-Raum**: Ein [Sobolev-Raum](https://de.wikipedia.org/wiki/Sobolev-Raum), ist ein Funktionenraum von schwach differenzierbaren Funktionen, der zugleich ein Banachraum ist. Der Sobolev-Raum ist der Raum derjenigen reellwertigen Funktionen $u \\in L^{p}(\\Omega),$ deren gemischte partielle schwache Ableitungen bis zur Ordnung $k$ im Lebesgue-Raum $L^{p}(\\Omega)$ liegen.\n",
        "\n",
        "  * Aus der Variationsrechnung: Diese minimiert Funktionale über Funktionen - Sobolev-Räume bilden dabei die Grundlage der Lösungstheorie partieller Differentialgleichungen.\n",
        "\n",
        "  * A Sobolev space is a vector space of functions equipped with a norm that is a **combination of Lp-norms of the function together with its derivatives up to a given order**.\n",
        "\n",
        "  * The derivatives are understood in a suitable weak sense to make the space complete, i.e. a Banach space.\n",
        "\n",
        "  * **A Sobolev space is a space of functions**\n",
        "\n",
        "    * **possessing sufficiently many derivatives** for some application domain, such as partial differential equations,\n",
        "\n",
        "    * and **equipped with a norm** that measures both the size and regularity of a function.\n",
        "\n",
        "    * Their importance comes from the fact that **weak solutions of some important partial differential equations exist in appropriate Sobolev spaces**, even when there are no strong solutions in spaces of continuous functions with the derivatives understood in the classical sense.\n",
        "\n",
        "  * Partielle Differentialgleichungen betrachtet man meistens auf Sobolew-Räumen. In diesen Räumen werden Funktionen, die bis auf Nullmengen übereinstimmen, als gleich angesehen. Da der Rand eines Gebietes üblicherweise eine Nullmenge ist, ist der Begriff der Randbedingung problematisch. Lösungen für dieses Problem sind sobolewsche Einbettungssätze oder – allgemeiner – [**Spuroperatoren**](https://de.wikipedia.org/wiki/Sobolev-Raum#Spuroperator).\n",
        "\n",
        "* **$L^p$ (Lebesgue) Space** are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. **They are sometimes called Lebesgue spaces**. Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * [L<sup>p</sup>-Raum](https://de.m.wikipedia.org/wiki/Lp-Raum) sind spezielle Räume, die aus allen p-fach integrierbaren Funktionen bestehen. Das $p$ in der Bezeichnung ist ein reeller Parameter: Für jede Zahl $0 < p \\leq \\infty$ ist ein $L^{p}$ -Raum definiert. Die Konvergenz in diesen Räumen wird als Konvergenz im $p$ -ten Mittel bezeichnet.\n",
        "\n",
        "  * diese Räume werden über das Lebesgue-Integral definiert\n",
        "\n",
        "  * Im Fall Banachraum-wertiger Funktionen bezeichnet man sie auch als Bochner-Lebesgue-Räume.\n",
        "\n",
        "  * [**Konvergenz_im_p-ten_Mittel**](https://de.m.wikipedia.org/wiki/Konvergenz_im_p-ten_Mittel)\n",
        "\n",
        "  * ${\\mathcal {L}}^{p}$ mit Halbnorm und ${\\mathcal {L}}^{p}$ mit Norm\n",
        "\n",
        "  * Hilbertraum ${\\mathcal {L}}^{2}$\n",
        "\n",
        "  * Der **normierte Vektorraum** $L^{p}$ ist [vollständig](https://de.m.wikipedia.org/wiki/Vollständiger_Raum) und damit ein [Banachraum](https://de.m.wikipedia.org/wiki/Banachraum), die Norm $\\|\\cdot\\|_{L} p$ wird **$L^{p}$ Norm** genannt.\n",
        "\n",
        "  * Auch wenn man von sogenannten $L^{p}$ -Funktionen spricht, handelt es sich dabei um die gesamte Äquivalenzklasse einer klassischen Funktion. Allerdings liegen im Falle des Lebesgue-Maßes auf dem $\\mathbb{R}^{n}$ zwei verschiedene stetige Funktionen nie in der gleichen Äquivalenzklasse, so dass der $L^{p}$-Begriff eine natürliche Erweiterung des Begriffs stetiger Funktionen darstellt.\n",
        "\n",
        "  * The [**Lp spaces**](https://de.m.wikipedia.org/wiki/Lp-Raum) are [function spaces](https://en.m.wikipedia.org/wiki/Function_space) defined using a natural **generalization of the p-norm for finite-dimensional vector spaces**. They are sometimes called **Lebesgue spaces**.\n",
        "\n",
        "  * A normed vector space is automatically a metric space, by defining the metric in terms of the norm in the natural way. But a metric space may have no algebraic (vector) structure — i.e., it may not be a vector space — so the concept of a **metric space is a generalization of the concept of a normed vector space**.\n",
        "\n",
        "  * Lp spaces form an important class of [Banach spaces](https://en.m.wikipedia.org/wiki/Banach_space) in functional analysis, and of topological vector spaces.\n",
        "\n",
        "  * In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as [solutions to variational problems](https://en.m.wikipedia.org/wiki/Central_tendency#Solutions_to_variational_problems)\n",
        "\n",
        "  * An Lp space may be defined as a space of measurable functions for which the p-th power of the absolute value is Lebesgue integrable, where functions which agree almost everywhere are identified.\n",
        "\n",
        "  * More generally, let 1 ≤ p < ∞ and (S, Σ, μ) be a [measure space](https://en.m.wikipedia.org/wiki/Measure_space). Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, that\n",
        "\n",
        "  * $\\|f\\|_{p} \\equiv\\left(\\int_{S}|f|^{p} \\mathrm{d} \\mu\\right)^{1 / p}<\\infty$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SpYIJ602L38"
      },
      "source": [
        "###### *Metric Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gcLfqnPX33L"
      },
      "source": [
        "> **Metric $\\rightarrow$ measures distances**\n",
        "\n",
        "* *Difference Metric to Norm: Instead of distance between points, a norm gives us the length of a vector, as measured from the origin*\n",
        "\n",
        "* **Eine Metrik definiert Abstände zwischen Elementen des Vektorraumes.**\n",
        "\n",
        "* Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n",
        "\n",
        "* Eine Metrik (auch Abstandsfunktion) ist eine Funktion, die je zwei Elementen des Raums einen nicht negativen reellen Wert zuordnet, der als Abstand der beiden Elemente voneinander aufgefasst werden kann.\n",
        "\n",
        "* Sei M eine Menge. **<u>Eine Metrik ist eine Abbildung</u>** d: $M \\times M \\rightarrow \\mathbb{R}$ auf $M \\times M$ wenn folgende drei Axiome erfüllt sind:\n",
        "\n",
        "1. Beide zusammen bilde Positive Definitheit (**positive definiteness**):\n",
        "  * $d(x, y) \\geq 0$ (**non-negativity**) sowie\n",
        "  * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) für alle $x, y \\in M$.\n",
        "\n",
        "2. $d(x, y)=d(y, x)$ (**symmetry**) Symmetrie\n",
        "$d(x, y)=d(y, x) \\forall x, y \\in M$\n",
        "\n",
        "4. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "\n",
        "**Metriken geben einem Raum eine globale und eine lokale mathematische Struktur**:\n",
        "  * Die globale Struktur kommt in **geometrischen Eigenschaften wie der Kongruenz** von Figuren zum Ausdruck.\n",
        "  * Die lokale metrische Struktur, also die Definition kleiner Abstände, ermöglicht unter bestimmten zusätzlichen Voraussetzungen die Einführung von **Differentialoperationen**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdrlGVRmlmx"
      },
      "source": [
        "**Aus Normen erzeugte Metriken**\n",
        "\n",
        "* **A norm induces a (distance) metric by the formula d (x,y) = ‖ y-x ‖.**\n",
        "\n",
        "* Jede Norm auf einem Vektorraum induziert durch die Festlegung $d(x, y) \\equiv\\|x-y\\|$ eine Metrik. Somit ist jeder normierte Vektorraum (und erst recht jeder Innenproduktraum, Banachraum oder Hilbertraum) ein metrischer Raum.\n",
        "\n",
        "* **Aber Achtung**: nicht jede Metrik ist durch eine Norm induziert! Jede Norm induziert eine Metrik, aber nicht umgekehrt.\n",
        "\n",
        "* **Eine Metrik, die aus einer $p$ -Norm abgeleitet ist, heißt auch [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances)**. It is a metric in a normed vector space. p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold (which is possible, but then it's not a metric anymore). Wichtige Spezialfälle sind:\n",
        "  * [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "  * [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$\n",
        "  * [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "\n",
        "* Als eine [**Fréchet-Metrik**](https://de.m.wikipedia.org/wiki/Fréchet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. **Sie stellt eine Verbindung zwischen Metrik und Norm her.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUnISc3anLSR"
      },
      "source": [
        "**Nicht aus Normen erzeugte Metriken**\n",
        "\n",
        "* Auf jeder Menge lässt sich eine triviale Metrik, die sogenannte gleichmäßig diskrete Metrik (die sogar eine Ultrametrik ist) definieren: $d(x, y)=\\left\\{\\begin{array}{ll}0 & \\text { für } x=y \\\\ 1 & \\text { für } x \\neq y\\end{array}\\right.$\n",
        "\n",
        "* Im Allgemeinen **nicht durch eine Norm induziert ist die riemannsche Metrik**, die aus einer differenzierbaren Mannigfaltigkeit eine [riemannsche Mannigfaltigkeit](https://en.m.wikipedia.org/wiki/Riemannian_manifold) macht. (zB  Die kürzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geodäten) sind nicht zwingend Geradenstücke, sondern können gekrümmte Kurven sein. **Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch größer (z. B. Kugel) oder kleiner (hyperbolische Räume) als 180° sein**.\n",
        "\n",
        "* Die [französische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Französische_Eisenbahnmetrik).\n",
        "\n",
        "* Die [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man könnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zurück.\n",
        "\n",
        "* Der [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) ist eine Metrik auf dem Coderaum, die die Unterschiedlichkeit von (gleich langen) Zeichenketten angibt. Die [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) kann als Erweiterung des Hamming-Abstands angesehen werden. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro–Winkler distance](https://en.m.wikipedia.org/wiki/Jaro–Winkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "\n",
        "* Mehr Beispiele von nicht aus Normen erzeugten Metriken [hier](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-8s7XOx_Mxn"
      },
      "source": [
        "**Metric Space**\n",
        "\n",
        "* Unter einem [metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space) versteht man in der Mathematik eine Menge, auf der eine Metrik definiert ist.\n",
        "\n",
        "  * **Das Paar $(M, d)$ nennt man einen <u>[metrischen Raum](https://de.m.wikipedia.org/wiki/Metrischer_Raum) (metric space)</u>**.\n",
        "\n",
        "  * Beispiel fur einen metrischen Raum: Die Menge der reellen Zahlen $\\mathbb{R}$ mit der Abstandsmetrik $d(x, y):=|x-y|$ bilden einen metrischen Raum.\n",
        "\n",
        "* Jeder metrische Raum ist ein [Hausdorff-Raum](https://de.m.wikipedia.org/wiki/Hausdorff-Raum).\n",
        "\n",
        "\n",
        "* **[Isometrie](https://de.m.wikipedia.org/wiki/Isometrie)**\n",
        "\n",
        "  * Isomorphismen zwischen metrischen Räumen heißen Isometrien. Ein metrischer Raum heißt vollständig, falls alle Cauchy-Folgen konvergieren. Eine Isometrie ist eine Abbildung, die zwei metrische Räume aufeinander abbildet und dabei die Metrik – also die Abstände zwischen je zwei Punkten – erhält.\n",
        "\n",
        "  * Sind zwei metrische Räume $\\left(M_{1}, d_{1}\\right),\\left(M_{2}, d_{2}\\right)$ gegeben, und ist $f: M_{1} \\rightarrow M_{2}$ eine Abbildung mit der Eigenschaft\n",
        "\n",
        "  * $d_{2}(f(x), f(y))=d_{1}(x, y)$ für alle $x, y \\in M_{1}$\n",
        "\n",
        "  * dann heißt $f$ Isometrie von $M_{1}$ nach $M_{2}$. Eine solche Abbildung ist stets injektiv.\n",
        "\n",
        "  * Ist $f$ sogar bijektiv, dann heißt $f$ **isometrischer Isomorphismus**, und die Räume $M_{1}$ und $M_{2}$ heißen is isometrische Einbettung von $M_{1}$ in $M_{2}$. [Isometrische Isomorphie](https://de.m.wikipedia.org/wiki/Isometrische_Isomorphie) beschreibt in der Funktionalanalysis einen Zusammenhang zwischen zwei unterschiedlichen Räumen, die geometrisch identisch sind.\n",
        "\n",
        "* [**Vollständiger Raum**](https://de.m.wikipedia.org/wiki/Vollständiger_Raum)\n",
        "\n",
        "  * Ein vollständiger Raum ist in der Analysis ein metrischer Raum, in dem jede Cauchy-Folge von Elementen des Raums auf eine Zahl (element) innerhalb desselben raumes konvergiert.\n",
        "\n",
        "  * ZB alle rationalen Zahlen sollten nach einsetzung in die cauchy-folge mit einer bestimmten metrik (zB betragsmetrik) auch wieder als ziel in rationalen zahlen muenden. tun sie das nicht, ist es ein unvollständiger Raum.\n",
        "\n",
        "  * Andererseits ist der Raum der rationalen Zahlen mit der Betragsmetrik nicht vollständig, weil etwa die Zahl 2–√ nicht rational ist, es jedoch Cauchy-Folgen rationaler Zahlen gibt, die bei Einbettung der rationalen Zahlen in die reellen Zahlen gegen 2–√ und somit gegen keine rationale Zahl konvergieren. Es ist aber stets möglich, die Löcher auszufüllen, also einen unvollständigen metrischen Raum zu vervollständigen. Im Fall der rationalen Zahlen erhält man dadurch den Raum der reellen Zahlen.\n",
        "\n",
        "  * Die Menge Q der rationalen Zahlen ist mit der Betragsmetrik\n",
        "\n",
        "  * $\n",
        "d(x, y)=|x-y|\n",
        "$\n",
        "\n",
        "  * nicht vollständig, denn die Folge rationaler Zahlen $x_{1}=1, x_{n+1}=\\frac{x_{n}}{2}+\\frac{1}{x_{n}}$ ist eine Cauchy-Folge, deren Grenzwert (siehe Heron-Verfahren) die irrationale Zahl $\\sqrt{2}$ ist, die nicht in $\\mathbb{Q}$ liegt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Exkurs: Divergence, Distance and Metric*"
      ],
      "metadata": {
        "id": "SZkKviQcpLi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jensen-Shannon divergence is a symmetrization of the Kullback-Leibler divergence"
      ],
      "metadata": {
        "id": "GT7Rt3vQAB-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditions**\n",
        "\n",
        "1. d(x, y) ≥ 0     (non-negativity)\n",
        "2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n",
        "3. d(x, y) = d(y, x)     (symmetry)\n",
        "4. d(x, z) ≤ d(x, y) + d(y, z)     (subadditivity / triangle inequality).\n",
        "\n",
        "**Begriffsabgrenzungen**\n",
        "\n",
        "* **Divergence** fullfills property of positive definiteness (1 + 2)\n",
        "\n",
        "* **Distance** fullfills property of positive definiteness and symmetrie (1 + 2+ 3)\n",
        "\n",
        "* **Metric** fullfills property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3 + 4). Häufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion) bezeichnet. Metric Space: Together with the set, a metric makes up a metric space."
      ],
      "metadata": {
        "id": "EmfRw-AgpSZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergences in Machine Learning**\n",
        "\n",
        "* is a (contrast) function which establishes the \"distance\" of one probability distribution to the other on a statistical manifold.\n",
        "* divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.\n",
        "* The two most important divergences are the relative entropy (Kullback–Leibler divergence, KL divergence) and the squared Euclidean distance.\n",
        "* Minimizing these two divergences is the main way that linear inverse problem are solved, via the principle of maximum entropy and least squares, notably in logistic regression and linear regression.\n",
        "* The two most important classes of divergences are the f-divergences and Bregman divergences; however, other types of divergence functions are also encountered in the literature. The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence; the squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>), but not an f-divergence."
      ],
      "metadata": {
        "id": "oryOJsv-pW2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exkurs: Meaning of 'no symmetry' in divergences**\n",
        "\n",
        "*\n",
        "https://en.m.wikipedia.org/wiki/Divergence_(statistics)\n",
        "\n",
        "* The Kullback-Leibler divergence is not symmetric. Roughly speaking, it's because you should think of the two arguments of the KL divergence as different kinds of things: the first argument is empirical data, and the second argument is a model you're comparing the data to.\n",
        "\n",
        "* Take a bunch of independent random variables $X_{1}, \\ldots, X_{n}$ whose possible values lie in a finite set.\n",
        "\n",
        "* Say these variables are identically distributed, with $\\operatorname{Pr}\\left(X_{i}=x\\right)=p_{x}$. Let $F_{n, x}$ be the number of variables whose values are equal to $x$. The list $F_{n}$ is a random variable, often called the \"empirical frequency distribution\" of the $X_{i} .$ What does $F_{n}$ look like when $n$ is very large?\n",
        "\n",
        "* More specifically, let's try to estimate the probabilities of the possible values of $F_{n} .$ since the set of possible values is different for different $n$, take a sequence of frequency distributions $f_{1}, f_{2}, f_{3}, \\ldots$ approaching a fixed frequency distribution $f$. It turns out $^{* *}$ that\n",
        "\n",
        "> $\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\ln \\operatorname{Pr}\\left(F_{n}=f_{n}\\right)=-\\mathrm{KL}(f, p)$\n",
        "\n",
        "* In other words, the Kullback-Leibler divergence of $f$ from $p$ lets you estimate the probability of getting an empirical frequency distribution close to $f$ from a large number of independent random variables with distribution $p$.\n"
      ],
      "metadata": {
        "id": "vxh1T80ovldM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1l9_Erf5T5S"
      },
      "source": [
        "###### *Topological Space*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are similarity metrics for topology (not geometry)?**\n",
        "\n",
        "When comparing topological spaces, we often use a concept known as a \"topological invariant.\" A topological invariant is a property or characteristic of a space that remains unchanged under homeomorphisms, which are continuous transformations of the space that can be continuously undone. Here are a few examples of topological invariants that can serve as similarity metrics:\n",
        "\n",
        "1. Connectedness: A topological space is \"connected\" if there's no way to divide it into two disjoint nonempty open sets. For example, an interval of real numbers is connected, but a set consisting of two separate intervals is not. Connectedness is a topological invariant.\n",
        "\n",
        "2. Compactness: A space is \"compact\" if every open cover of the space has a finite subcover. This is a bit technical, but intuitively, compactness is a way of saying that the space is \"not too large.\" For example, an interval of real numbers is compact, but the entire set of real numbers is not. Compactness is another topological invariant.\n",
        "\n",
        "3. Euler Characteristic: For a simplicial complex (a space built out of points, lines, triangles, and their higher-dimensional analogues), the Euler characteristic is defined as the number of vertices, minus the number of edges, plus the number of triangles, minus the number of tetrahedra, and so on. The Euler characteristic is a topological invariant, which famously equals 2 for any convex polyhedron (like a cube or a sphere).\n",
        "\n",
        "4. Betti numbers and homology groups: These are more advanced concepts from algebraic topology, but they provide very powerful ways of distinguishing between different topological spaces. Roughly speaking, the Betti numbers count the number of \"holes\" of different dimensions in the space. For example, a sphere has one 2-dimensional hole (the inside of the sphere), while a doughnut has one 1-dimensional hole (the hole in the middle) and one 2-dimensional hole.\n",
        "\n",
        "5. Fundamental group: This is another tool from algebraic topology, which captures information about the \"loops\" in a space. Roughly speaking, the fundamental group of a space is the set of all loops in the space, where two loops are considered the same if one can be continuously deformed into the other.\n",
        "\n",
        "Comparing these invariants gives us a way to say whether two topological spaces are similar or not. However, it's important to note that two spaces could share the same invariants and still not be \"the same\" in a topological sense (i.e., homeomorphic). Topology is a deep and complex field, and telling whether two spaces are truly the same is a nontrivial task."
      ],
      "metadata": {
        "id": "qtRja4Hy5k1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what would be an example of two spaces could share the same invariants and still not be \"the same\" in a topological sense (i.e., homeomorphic)?**\n",
        "\n",
        "A classic example of two topological spaces sharing the same simple invariants but not being homeomorphic is the pair of the 2-dimensional torus (a doughnut shape) and a coffee mug.\n",
        "\n",
        "If we only consider the simple topological invariants like Euler characteristic and Betti numbers, both a torus and a coffee mug have the same values:\n",
        "\n",
        "1. They both have an Euler characteristic of 0.\n",
        "2. Their first Betti number (counting loops that can't be shrunk to a point) is 2: one for the loop around the hole of the doughnut (or handle of the mug) and one for the loop around the whole doughnut (or around the mug).\n",
        "3. They are both connected and compact.\n",
        "\n",
        "However, they are not homeomorphic. In a more formal topological analysis, they would have different properties - for instance, in terms of their embedding in 3D space, curvature, or other more advanced topological properties.\n",
        "\n",
        "The key point is that the Euler characteristic and Betti numbers are quite simple topological invariants, and there are many others (some of them much more complex) that can distinguish between different spaces. These more advanced invariants are needed when dealing with more complex spaces or when the simple invariants are the same for two non-homeomorphic spaces."
      ],
      "metadata": {
        "id": "aiSIUZDc5qvv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy5f4vGA-Llx"
      },
      "source": [
        "**General topology (point-set topology)**\n",
        "\n",
        "> Topology $\\rightarrow$ Lagebeziehungen wie „Nähe“ und „Streben gegen“ werden verallgemeinert\n",
        "\n",
        "The fundamental concepts in point-set topology are continuity, compactness, and connectedness:\n",
        "\n",
        "* **Continuous functions**, intuitively, take nearby points to nearby points.\n",
        "\n",
        "* **Compact sets** are those that can be covered by finitely many sets of arbitrarily small size.\n",
        "\n",
        "* **Connected sets** are sets that cannot be divided into two pieces that are far apart.\n",
        "\n",
        "**Once a choice of open sets is made, the properties of continuity, connectedness, and compactness, which use notions of nearness, can be defined using these open sets.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Trennungsaxiom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tNzMCPIBjYm"
      },
      "source": [
        "**Topological Vector Space**\n",
        "\n",
        "> **Man nennt $T$ eine Topologie auf $X$, und das Paar ($X$,$T$) einen topologischen Raum.**\n",
        "\n",
        "* a topological space ([Topologischer Raum](https://de.m.wikipedia.org/wiki/Topologischer_Raum)) may be defined as a set of points, along with a set of **neighbourhoods** for each point, satisfying a set of **axioms** relating points and neighbourhoods.\n",
        "\n",
        "* The definition of a topological space **relies only upon set theory** and is the most general notion of a mathematical space that allows for the definition of concepts such as **[continuity](https://en.m.wikipedia.org/wiki/Continuous_function#Continuous_functions_between_topological_spaces), [connectedness](https://en.m.wikipedia.org/wiki/Connected_space), and [convergence](https://en.m.wikipedia.org/wiki/Limit_of_a_sequence)**.\n",
        "\n",
        "* **Other spaces, such as manifolds and metric spaces, are specializations of topological spaces with extra structures or constraints.**\n",
        "\n",
        "* Topological spaces are **studied in Point-Set Topology** (General Topology)\n",
        "\n",
        "* ein topologischer Raum ist ein elementarer Gegenstand der Topologie\n",
        "\n",
        "* Durch die Einführung einer topologischen Struktur auf einer Menge lassen sich intuitive Lagebeziehungen wie **„Nähe“ und „Streben gegen“** aus dem [Anschauungsraum (Euklidischer Raum)](https://de.m.wikipedia.org/wiki/Euklidischer_Raum) auf sehr viele und sehr allgemeine Strukturen übertragen und mit präziser Bedeutung versehen.\n",
        "\n",
        "* Ein **[topologischer Vektorraum](https://de.m.wikipedia.org/wiki/Topologischer_Vektorraum)** / [Topological Vector Space](https://en.m.wikipedia.org/wiki/Topological_vector_space) ist ein Vektorraum, auf dem neben seiner algebraischen auch noch eine damit verträgliche topologische Struktur definiert ist.\n",
        "\n",
        "* Sei $\\mathbb{K} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. Ein $\\mathbb{K}$ -Vektorraum $E$, der zugleich topologischer Raum ist, heißt topologischer Vektorraum, wenn folgende Verträglichkeitsaxiome gelten:\n",
        "  * Die Vektoraddition $E \\times E \\rightarrow E$ ist stetig,\n",
        "  * Die Skalarmultiplikation $\\mathbb{K} \\times E \\rightarrow E$ ist stetig.\n",
        "\n",
        "* **Beispiele:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erklärt, dass wir eine Menge $\\Omega$ C $\\mathbb{R}$ offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen lässt.\n",
        "\n",
        "* **Separation Axioms**: Topologische Räume können [klassifiziert werden nach Kolmogorov](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms).\n",
        "\n",
        "*What are some examples of topological spaces which are not a metric space?*\n",
        "\n",
        "A set with a single element  {∙}\n",
        "{\n",
        "∙\n",
        "}\n",
        "  only has one topology, the discrete one (which in this case is also the indiscrete one…) So that’s not helpful.\n",
        "\n",
        "A set with two elements, however, is more interesting.  𝑋={∙,∘}\n",
        "X\n",
        "=\n",
        "{\n",
        "∙\n",
        ",\n",
        "∘\n",
        "}\n",
        "  can be topologized by having all subsets open (discrete), only the empty set and the whole space open (indiscrete), and also the topology in which  ∅,{∘}\n",
        "∅\n",
        ",\n",
        "{\n",
        "∘\n",
        "}\n",
        "  and  𝑋\n",
        "X\n",
        " are open (and the symmetric one with  {∙}\n",
        "{\n",
        "∙\n",
        "}\n",
        "  open).\n",
        "\n",
        "A metric on a two-element set forces us to declare that there’s a certain distance between the two points, rendering both singleton sets open (and closed). Indeed, a finite metric space has a discrete topology. But we just found three topologies on  𝑋\n",
        "X\n",
        "  which are not the discrete one, giving us three examples of a non-metrizable topological space.\n",
        "\n",
        "Note that the indiscrete topology on  𝑋\n",
        "X\n",
        "  can be regarded as coming from the pseudo-metric where the distance between the points is zero. However, the other two topologies, where one singleton set is open and the other is not, cannot be interpreted in this way. A metric is by definition symmetric, and these two topologies are not.\n",
        "\n",
        "*Source: [Quora](https://www.quora.com/What-are-some-examples-of-topological-spaces-which-are-not-a-metric-space)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mx_opXBBr-r"
      },
      "source": [
        "**Frechet-Räume**\n",
        "\n",
        "* [Fréchet-Spaces](https://de.m.wikipedia.org/wiki/Fréchet-Raum) sind Verallgemeinerungen des Banachraums und topologische Vektorraum mit speziellen Eigenschaften. Die Hauptvertreter von Fréchet-Räumen sind Vektorräume von glatten Funktionen. Diese Räume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bezüglich keiner Norm vollständig**, also keine Banachräume. Man kann auf ihnen aber eine Topologie definieren, sodass viele Sätze, die in Banachräumen gelten, ihre Gültigkeit behalten.\n",
        "\n",
        "* Es handelt sich um einen **topologischen Vektorraum** mit speziellen Eigenschaften, die ihn als **Verallgemeinerung des Banachraums** charakterisieren.\n",
        "\n",
        "* Die Hauptvertreter von [Fréchet-Spaces](https://de.m.wikipedia.org/wiki/Fréchet-Raum) sind Vektorräume von [glatten Funktionen](https://de.wikipedia.org/wiki/Glatte_Funktion).\n",
        "  * Eine glatte Funktion ist eine mathematische Funktion, die unendlich oft differenzierbar (insbesondere stetig) ist.\n",
        "\n",
        "* Diese Räume lassen sich zwar mit verschiedenen Normen ausstatten, **sind aber bezüglich keiner Norm vollständig**, also keine Banachräume. Man kann auf ihnen aber eine Topologie definieren, sodass viele Sätze, die in Banachräumen gelten, ihre Gültigkeit behalten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUGnGb0RB0L-"
      },
      "source": [
        "**Uniform Spaces**\n",
        "\n",
        "* **[Uniforme Räume](https://de.m.wikipedia.org/wiki/Uniformer_Raum) erlauben es zwar nicht Abstände einzuführen**, aber trotzdem Begriffe wie gleichmäßige Stetigkeit, Cauchy-Folgen, Vollständigkeit und Vervollständigung zu definieren. Jeder uniforme Raum ist auch ein topologischer Raum.\n",
        "\n",
        "* **Jeder topologische Vektorraum (egal ob metrisierbar oder nicht) ist auch ein uniformer Raum**. Allgemeiner ist jede kommutative topologische Gruppe ein uniformer Raum. Eine nichtkommutative topologische Gruppe trägt jedoch zwei uniforme Strukturen, eine links-invariante und eine rechts-invariante. Topologische Vektorräume sind in endlichen Dimensionen vollständig, in unendlichen Dimensionen im Allgemeinen aber nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_1$-Norm und $L^1$-Metrik*"
      ],
      "metadata": {
        "id": "6quxRLo3U9Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) im endlichdimensionalen Raum** (p=1, Lasso, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}\\right|$\n",
        "\n",
        "* zB Summennorm des reellen Vektors $x=(3,-2,6) \\in \\mathbb{R}^{3}$ ist $\\|x\\|_{1}=|3|+|-2|+|6|=11$\n",
        "\n",
        "* Von Summennorm abgeleitete Metrik ist [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik). Siehe auch [Taxicab_geometry](https://en.m.wikipedia.org/wiki/Taxicab_geometry).\n",
        "\n",
        "* Die Summennorm ist im Gegensatz zur euklidischen Norm (2-Norm) nicht von einem Skalarprodukt induziert.\n",
        "\n",
        "* Die Einheitssphäre der reellen Summennorm ist ein Kreuzpolytop mit minimalem Volumen über alle p-Normen. **Daher ergibt die Summennorm für einen gegebenen Vektor den größten Wert aller p-Normen**. (zB 3 + (-2) + 6 = 11 in Summennorm, aber = 7 in euklidischer Norm fur p=2)\n",
        "\n",
        "* Techniques which use an L1 penalty, like [LASSO](https://en.m.wikipedia.org/wiki/Lasso_(statistics)), encourage solutions where many parameters are zero\n",
        "\n",
        "\n",
        "*Summennorm in zwei Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Vector-1-Norm_qtl1.svg/316px-Vector-1-Norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "LdVhJHBAUSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summennorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "$\\ell^{1}-$ Norm (**Folgenraum**)\n",
        "\n",
        "* Die $\\ell^{1}$ -Norm ist die Verallgemeinerung der Summennorm auf den Folgenraum $\\ell^{1}$ der **betragsweise summierbaren Folgen** $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{N} .$\n",
        "\n",
        "* Hierbei wird lediglich **die endliche Summe durch eine unendliche ersetzt** und die $\\ell^{\\text {t }}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{1}}=\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|$\n",
        "\n",
        "$L^{1}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* Weiter kann die Summennorm auf den Funktionenraum $L^{1}(\\Omega)$ der auf einer Menge $\\Omega$ betragsweise integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zunächst wird die $\\mathcal{L}^{1}$ Norm einer **betragsweise Lebesgue-integrierbaren Funktion** $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}=\\int_{\\Omega}|f(x)| d x$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{1}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zunächst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Maß Null von der Nullfunktion unterscheiden, zu Null integriert werden.\n",
        "\n",
        "* Daher betrachtet man die Menge der Äquivalenzklassen von Funktionen $[f] \\in L^{1}(\\Omega)$, die fast überall gleich sind, und erhält auf diesem $L^{1}$ -Raum die $L^{1}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{1}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{1}(\\Omega)}$"
      ],
      "metadata": {
        "id": "MIi62WcSUgP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 - Manhattan Distance (Lasso)**\n",
        "\n",
        "* The Manhattan norm gives rise to the [Manhattan distance](https://de.m.wikipedia.org/wiki/Manhattan-Metrik), where the distance between any two points, or vectors, is the sum of the differences between corresponding coordinates.\n",
        "\n",
        "* **Die Manhattan-Metrik ist die von der Summennorm (1-Norm) eines Vektorraums erzeugte Metrik.**\n",
        "\n",
        "* ***Aber: Die Summennorm ist nicht von einem Skalarprodukt induziert.***\n",
        "\n",
        "* Die Manhattan-Metrik (auch Manhattan-Distanz, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz d zwischen zwei Punkten a und b als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird:\n",
        "\n",
        "> $d(a, b)=\\sum_{i}\\left|a_{i}-b_{i}\\right|$"
      ],
      "metadata": {
        "id": "n9vaUs0sXyOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Euclidean $p_2$-Norm und $L^2$-Metrik*"
      ],
      "metadata": {
        "id": "32OR42wtUS3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidische Norm](https://de.m.wikipedia.org/wiki/Euklidische_Norm) im endlichdimensionalen Raum** (p=2, Ridge, Standardnorm)\n",
        "\n",
        "> $\\|x\\|_{2}= \\left(x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}\\right)^{1 / 2} = \\sqrt{\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}}$\n",
        "\n",
        "* Ziel: berechnen die Länge (Betrag) eines Vektors in der euklidischen Ebene.  The length of a vector $x = (x_1, x_2, ..., x_n)$ in the $n$-dimensional real vector space $\\mathbb{R}^n$ is usually given by the Euclidean norm $||x||_{2}$.\n",
        "\n",
        "* Die euklidische Norm ist eine von einem Skalarprodukt induzierte Norm (im Gegensatz zur p1 Summennorm)\n",
        "\n",
        "* Beispiel: Vektor ${\\vec {v}}$ mit Komponenten $x$, $y$ und $z$ in drei Dimensionen durch ${\\vec {v}}=(x,y,z)$ wird die Länge berechnet durch:\n",
        "\n",
        "> $|{\\vec {v}}|={\\sqrt {x^{2}+y^{2}+z^{2}}}$\n",
        "\n",
        "* Die Euclidean Norm besitzt als eine von einem Skalarprodukt [induzierte Norm](https://de.m.wikipedia.org/wiki/Skalarproduktnorm) **neben den [drei Normaxiomen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Definition) eine Reihe weiterer Eigenschaften**:\n",
        "\n",
        "  * die Gültigkeit der [Cauchy-Schwarz-Ungleichung](https://de.m.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung)\n",
        "\n",
        "  * der [Parallelogrammgleichung](https://de.m.wikipedia.org/wiki/Parallelogrammgleichung)\n",
        "\n",
        "  * sowie eine Invarianz unter unitären Transformationen (Die euklidische Norm ändert sich also unter unitären Transformationen nicht. Für reelle Vektoren sind solche Transformationen beispielsweise Drehungen des Vektors um den Nullpunkt. Diese Eigenschaft wird zum Beispiel bei der numerischen Lösung linearer Ausgleichsprobleme über die **Methode der kleinsten Quadrate mittels QR-Zerlegungen genutzt**.)\n",
        "\n",
        "* Für orthogonale Vektoren erfüllt die euklidische Norm selbst eine allgemeinere Form des Satzes des Pythagoras.\n",
        "\n",
        "* Sieht man eine Matrix mit reellen oder komplexen Einträgen als entsprechend langen Vektor an, so kann die euklidische Norm auch für Matrizen definiert werden und heißt dann [**Frobeniusnorm**](https://de.m.wikipedia.org/wiki/Frobeniusnorm). Die euklidische Norm kann auch auf unendlichdimensionale Vektorräume über den reellen oder komplexen Zahlen verallgemeinert werden und hat dann zum Teil eigene Namen.\n",
        "\n",
        "*Euklidische Norm in zwei reellen Dimensionen:*\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Vector-2-Norm_qtl1.svg/316px-Vector-2-Norm_qtl1.svg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "p1rB9cfOJadt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euklidische Norm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "* Als [Folgennorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen) im [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum): Die $\\ell^{2}-$ Norm im Folgenraum ist die Verallgemeinerung der euklidischen Norm auf den [Folgenraum](https://de.m.wikipedia.org/wiki/Folgenraum) $\\ell^{2}$ der quadratisch summierbaren Folgen $\\left(a_{n}\\right)_{n} \\in \\mathbb{K}^{\\mathrm{N}} .$ Hierbei wird lediglich die endliche Summe durch eine unendliche ersetzt und die $\\ell^{2}$ -Norm ist dann gegeben als\n",
        "\n",
        "> $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{2}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{2}\\right)^{1 / 2}$\n",
        "\n",
        "* Die ℓ-p -Räume sind ein Spezialfall der allgemeineren Lp-Räume, wenn man das Zählmaß auf dem Raum N betrachtet.\n",
        "\n",
        "* Als [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im [Funktionenraum](https://de.m.wikipedia.org/wiki/Funktionenraum) $L^{2}(\\Omega)$-Norm der auf einer Menge $\\Omega$ quadratisch integrierbaren Funktionen verallgemeinert werden, was in zwei Schritten geschieht. Zunächst wird die $\\mathcal{L}^{2}$ Norm einer quadratisch Lebesgue-integrierbaren Funktion $f: \\Omega \\rightarrow \\mathbb{K}$ als\n",
        "\n",
        "> $\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{2} d x\\right)^{1 / 2}$\n",
        "\n",
        "* definiert, wobei im Vergleich zur $\\ell^{2}$ -Norm lediglich die Summe durch ein Integral ersetzt wurde. Dies ist zunächst nur eine Halbnorm, da nicht nur die Nullfunktion, sondern auch alle Funktionen, die sich nur an einer Menge mit Lebesgue-Maß Null von der Nullfunktion unterscheiden, zu Null integriert werden. Daher betrachtet man die Menge der Äquivalenzklassen von Funktionen $[f] \\in L^{2}(\\Omega),$ die fast überall gleich sind, und erhält auf diesem $L^{2}$ -Raum die $L^{2}$ -Norm durch\n",
        "\n",
        "> $\\|[f]\\|_{L^{2}(\\Omega)}=\\|f\\|_{\\mathcal{L}^{2}(\\Omega)}$\n",
        "\n",
        "* Der Raum $L^{2}(\\Omega)$ ist der [Hilbertraum fur L2](https://de.m.wikipedia.org/wiki/Lp-Raum#Der_Hilbertraum_L2) mit dem Skalarprodukt zweier Funktionen\n",
        "\n",
        "> $\\langle f, g\\rangle_{L_{2}(\\Omega)}=\\int_{\\Omega} \\overline{f(x)} \\cdot g(x) d x$"
      ],
      "metadata": {
        "id": "mXr0GKzWTwNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euclidian Distance](https://de.m.wikipedia.org/wiki/Euklidischer_Abstand) (Metrik) induziert aus der Euklidischen Norm**  (L2, Ridge)\n",
        "\n",
        "> $d_{2}:(x, y) \\mapsto\\|x-y\\|_{2}=\\sqrt{d_{\\mathrm{SSD}}}=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}$\n",
        "\n",
        "* Ziel: berechnen den Abstand zwischen zwei Vektoren in der euklidischen Ebene\n",
        "\n",
        "* Special case of the [Minkowski distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) with p=2\n",
        "\n",
        "* In Statistik siehe auch [Tikhonov Regularization (Ridge)](https://en.m.wikipedia.org/wiki/Tikhonov_regularization). Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small."
      ],
      "metadata": {
        "id": "3eQ4165xTtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Euklidian (Metric) Space](https://en.m.wikipedia.org/wiki/Euclidean_space)**\n",
        "\n",
        "* Together with the [Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance) the Euclidean space is a metric space (x element R, d). http://theanalysisofdata.com/probability/B_4.html\n"
      ],
      "metadata": {
        "id": "kbrT-D7PTrew"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Syw4dL96u31"
      },
      "source": [
        "###### *Special: Euclidean $p_{∞}$-Norm und $L^{∞}$-Metrik*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Maximumsnorm**](https://de.m.wikipedia.org/wiki/Maximumsnorm) (p $\\rightarrow \\infty$): $\\rightarrow$\n",
        "\n",
        "> $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$\n",
        "\n",
        "* Sie ist ein Spezialfall der [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm).\n",
        "\n",
        "* Anschaulich gesprochen ist **der aus der Maximumsnorm abgeleitete Abstand immer dann relevant, wenn man sich in einem mehrdimensionalen Raum in alle Dimensionen gleichzeitig und unabhängig voneinander gleich schnell bewegen kann**. (zB Rochade beim Schach)\n",
        "\n",
        "* Allgemeiner kann die Maximumsnorm benutzt werden, um zu bestimmen, wie schnell man sich in einem zwei- oder dreidimensionalen Raum bewegen kann, wenn angenommen wird, dass die Bewegungen in x-, y- (und z-)Richtung unabhängig, gleichzeitig und mit gleicher Geschwindigkeit erfolgen.\n",
        "\n",
        "* Noch allgemeiner kann man ein System betrachten, dessen Zustand durch n unabhängige Parameter bestimmt wird. An allen Parametern können gleichzeitig und ohne gegenseitige Beeinflussung Änderungen vorgenommen werden. **Dann „misst“ die Maximumsnorm in Rn die Zeit, die man benötigt, um das System von einem Zustand in einen anderen zu überführen**. Voraussetzung hierfür ist allerdings, dass man die Parameter so normiert hat, dass gleiche Abstände zwischen den Werten auch gleichen Änderungszeiten entsprechen. Andernfalls müsste man eine gewichtete Version der Maximumsnorm verwenden, die die unterschiedlichen Änderungsgeschwindigkeiten der Parameter berücksichtigt.\n",
        "\n",
        "* Für einen Vektor $x=\\left(x_{1}, \\ldots, x_{n}\\right) \\in \\mathbb{R}^{n}$ nennt man $\\|x\\|_{\\max }:=\\max \\left(\\left|x_{1}\\right|, \\ldots,\\left|x_{n}\\right|\\right)$ $\\rightarrow$ $\\|x\\|_{\\infty}=\\max _{i=1, \\ldots, n}\\left|x_{i}\\right|$ die Maximumsnorm von x.\n",
        "\n",
        "*Äquivalenz der euklidischen Norm (blau) und der Maximumsnorm (rot) in zwei Dimensionen:* [Source](https://de.m.wikipedia.org/wiki/Äquivalente_Normen)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Equiv_2-norm_max-norm_qtl1.svg/240px-Equiv_2-norm_max-norm_qtl1.svg.png)"
      ],
      "metadata": {
        "id": "rpLxgxJgWGVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximumsnorm im unendlichdimensionalen Vektorraum**\n",
        "\n",
        "\n",
        "[**Supremumsnorm**](https://de.m.wikipedia.org/wiki/Supremumsnorm)\n",
        "\n",
        "* Im Gegensatz zur Maximumsnorm wird die Supremumsnorm $\\|f\\|_{\\text {sup }}:=\\sup _{t \\in X}|f(t)|$ nicht für stetige, sondern für beschränkte Funktionen $f$ definiert.\n",
        "\n",
        "* In diesem Fall ist es nicht notwendig, dass $X$ kompakt ist; $X$ kann eine beliebige Menge sein.\n",
        "\n",
        "* **Da stetige Funktionen auf kompakten Räumen beschränkt sind, ist die Maximumsnorm ein Spezialfall der Supremumsnorm**.\n",
        "\n",
        "* Die Supremumsnorm (auch Unendlich-Norm genannt) ist in der Mathematik eine Norm auf dem Funktionenraum der beschränkten Funktionen. Im einfachsten Fall einer reell- oder komplexwertigen beschränkten Funktion ist die Supremumsnorm das Supremum der Beträge der Funktionswerte. Allgemeiner betrachtet man Funktionen, deren Zielmenge ein normierter Raum ist, und die Supremumsnorm ist dann das Supremum der Normen der Funktionswerte.\n",
        "\n",
        "* **Für stetige Funktionen auf einer kompakten Menge ist die Maximumsnorm ein wichtiger Spezialfall der Supremumsnorm.**\n",
        "\n",
        "*Die Supremumsnorm der reellen Arkustangens-Funktion ist π/2. Auch wenn die Funktion diesen Wert betragsmäßig nirgendwo annimmt, so bildet er dennoch die kleinste obere Schranke.*\n",
        "\n",
        "![alternativer Text](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Graf_arctg.svg/260px-Graf_arctg.svg.png)\n",
        "\n",
        "\n",
        "Supremumsnorm vs Maximumsnorm:\n",
        "\n",
        "* So ist etwa die **Supremumsnorm** der linearen Funktion $f(x)=x$ in diesem Intervall gleich $1 .$ Die Funktion nimmt diesen Wert zwar innerhalb des Intervalls nicht an, kommt inm jedoch beliebig nahe.\n",
        "\n",
        "* Wählt man stattdessen das abgeschlossene Einheitsintervall $M=[0,1]$, dann wird der Wert 1 angenommen und die Supremumsnorm entspricht der **Maximumsnorm**.\n",
        "\n",
        "\n",
        "$L^{∞}$ -Norm (**Funktionenraum**)\n",
        "\n",
        "* L∞ is a **function space** (Funktionenraum). Its elements are the essentially bounded measurable functions. More precisely, L∞ is defined based on an underlying measure space, (S, Σ, μ). Start with the set of all measurable functions from S to R which are essentially bounded, i.e. bounded up to a set of measure zero. Two such functions are identified if they are equal almost everywhere. Denote the resulting set by L∞(S, μ).\n",
        "\n",
        "\n",
        "* [Normen auf Operatoren](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_Operatoren)\n",
        "\n",
        "$\\ell^{∞}$ -Norm (**Folgenraum**)\n",
        "\n",
        "* The vector space ℓ∞ is a **sequence space** (Folgenraum) whose elements are the bounded sequences. The vector space operations, addition and scalar multiplication, are applied coordinate by coordinate.\n",
        "\n",
        "* $\\ell^{\\infty},$ the (real or complex) vector space of bounded sequences with the **[supremum norm](https://de.m.wikipedia.org/wiki/Supremumsnorm)**, and $L^{\\infty}=L^{\\infty}(X, \\Sigma, \\mu)$, the vector space of essentially bounded measurable functions with the **[essential supremum norm](https://de.m.wikipedia.org/wiki/Wesentliches_Supremum)**, are two closely related Banach spaces.\n",
        "\n",
        "* In fact the former is a special case of the latter. As a Banach space they are the continuous dual of the Banach spaces $\\ell_{1}$ of absolutely summable sequences, and $L^{1}=L^{1}(X, \\Sigma, \\mu)$ of absolutely integrable measurable functions (if the measure space fulfills the conditions of being localizable and therefore\n",
        "semifinite).\n",
        "\n",
        "* Pointwise multiplication gives them the structure of a Banach algebra, and in fact they are the standard examples of abelian Von Neumann algebras."
      ],
      "metadata": {
        "id": "jC0WO_F2VJou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L ∞ - Chebyshev Distance**\n",
        "\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance) (or Tchebychev distance), maximum metric, or L∞ metric is a metric defined on a vector space **where the distance between two vectors is the greatest of their differences** along any coordinate dimension.\n",
        "\n",
        "* The maximum norm gives rise to the **Chebyshev distance** or chessboard distance, the minimal number of moves a chess king would take to travel from x to y. The Chebyshev distance is the L∞-norm of the difference, a special case of the Minkowski distance where p goes to infinity. It is also known as Chessboard distance.\n",
        "\n",
        "> $d_{\\infty}:(x, y) \\mapsto\\|x-y\\|_{\\infty}=\\lim _{p \\rightarrow \\infty}\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{p}\\right)^{\\frac{1}{p}}=\\max _{i}\\left|x_{i}-y_{i}\\right|$"
      ],
      "metadata": {
        "id": "reDInvHxXs-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Distance Measures*"
      ],
      "metadata": {
        "id": "Cmqnes9brFPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/Quantum_mutual_information"
      ],
      "metadata": {
        "id": "watkPtQvrvvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**is there a connection between Hausdorff distance and diamond norm?**\n",
        "\n",
        "Yes, there is a connection between the Hausdorff distance and the diamond norm, as they are both ways to quantify the difference between two sets or operators, but they are used in different contexts and are defined differently.\n",
        "\n",
        "1. Hausdorff distance: The Hausdorff distance is a measure defined between two sets of points (usually in a metric space). It captures the greatest of all the distances from a point in one set to the closest point in the other set. It's often used in computer vision and image analysis to compare the similarity between two sets of points.\n",
        "\n",
        "2. Diamond norm: The diamond norm (also known as the completely bounded trace norm) is a measure defined between two quantum operations (or more generally, between two linear maps between operator spaces). It captures the maximum difference between the effects of the two operations, when applied to any input quantum state and when the output is measured in any possible way.\n",
        "\n",
        "The diamond norm is particularly important in quantum information theory because it is directly related to the ability to distinguish between two quantum operations in any physical experiment.\n",
        "\n",
        "While both the Hausdorff distance and the diamond norm are used to compare two things, they are defined in different mathematical spaces (sets of points vs quantum operations), and so they measure fundamentally different types of differences. However, there are mathematical connections between the two, in the sense that similar mathematical techniques (such as duality and the use of supremum) are used in their definitions. These similarities reflect the deep connections between geometry, quantum mechanics, and the theory of linear operators."
      ],
      "metadata": {
        "id": "yOCaxqzShnYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Dimension</font> (VC, Fat-shattering, Pseudo, Effective, Hausdorff)*"
      ],
      "metadata": {
        "id": "ijtvzpIDxEzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensions**\n",
        "\n",
        "[Dimension](https://en.m.wikipedia.org/wiki/Dimension).Dimensionality measures are used to quantify the complexity of a set of points or a function class. They are often used in machine learning to bound the sample complexity of learning algorithms.\n",
        "* in mathematics, is a particular way of describing the size of an object (contrasting with measure and other, different, notions of size). Dimensionality measures are also used to study the relationships between different complexity classes. For example, it is known that the complexity class PSPACE is contained in the complexity class EXPTIME. This can be shown using the following theorem: Theorem: The metric entropy of any hypothesis class with VC dimension d is at most d. This theorem tells us that the complexity class PSPACE, which contains all problems that can be solved by a polynomial space Turing machine, is contained in the complexity class EXPTIME, which contains all problems that can be solved by an exponential time Turing machine. Dimensionality measures are a powerful tool for studying the complexity of computational problems. They can be used to bound the resource requirements of algorithms, and to study the relationships between different complexity classes.\n",
        "\n",
        "*Examples:*\n",
        "\n",
        "* [Effective dimension](https://en.m.wikipedia.org/wiki/Effective_dimension) is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting. There are several variations (various notions of effective dimension) of which the most common is effective Hausdorff dimension.\n",
        "\n",
        "  * effective dimension is a modification of Hausdorff dimension and other fractal dimensions that places it in a computability theory setting\n",
        "\n",
        "  * to measure power / capacity of a model [Source](https://www.youtube.com/watch?v=fDIGmkq9xNE&t=2067s)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1398.png)\n",
        "\n",
        "* [Hausdorff dimension](https://en.m.wikipedia.org/wiki/Hausdorff_dimension) generalizes the well-known integer dimensions assigned to points, lines, planes, etc. by allowing one to distinguish between objects of intermediate size between these integer-dimensional objects.\n",
        "\n",
        "* [Intrinsic dimension](https://en.m.wikipedia.org/wiki/Intrinsic_dimension): The intrinsic dimension of a set of points is the smallest number of parameters needed to represent the points accurately.\n",
        "\n",
        "* [Fractal dimension](https://en.m.wikipedia.org/wiki/Fractal_dimension) provides a rational statistical index of complexity detail in a pattern. A fractal pattern changes with the scale at which it is measured. It is also a measure of the space-filling capacity of a pattern, and it tells how a fractal scales differently, in a fractal (non-integer) dimension.\n",
        "\n",
        "* [Lebesgue covering dimension](https://en.m.wikipedia.org/wiki/Lebesgue_covering_dimension) or topological dimension of a topological space is one of several different ways of defining the dimension of the space in a topologically invariant way\n",
        "\n",
        "* [Krull dimension](https://en.m.wikipedia.org/wiki/Krull_dimension)\n",
        "\n",
        "* [Inductive dimension](https://en.m.wikipedia.org/wiki/Inductive_dimension)\n",
        "\n",
        "* [Minkowski–Bouligand dimension](https://en.m.wikipedia.org/wiki/Minkowski–Bouligand_dimension) also known as Minkowski dimension or box-counting dimension, is a way of determining the fractal dimension of a set S in a Euclidean space $\\mathbb {R} ^{n}$, or more generally in a metric space (X,d).\n",
        "\n",
        "* [Information dimension](https://en.m.wikipedia.org/wiki/Information_dimension) is a measure of the fractal dimension of a probability distribution. It characterizes the growth rate of the Shannon entropy given by successively finer discretizations of the space.\n",
        "\n",
        "  * In 2010, Wu and Verdú gave an operational characterization of [Rényi information dimension = Rényi entropy](https://en.m.wikipedia.org/wiki/Rényi_entropy) as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder.\n",
        "\n",
        "  * Enge Verbindung zwischen Entropy und Dimension measures.\n",
        "\n",
        "* [Correlation dimension](https://en.m.wikipedia.org/wiki/Correlation_dimension) is a measure of the dimensionality of the space occupied by a set of random points, often referred to as a type of fractal dimension.\n",
        "\n",
        "* [Packing dimension](https://en.m.wikipedia.org/wiki/Packing_dimension)\n",
        "\n",
        "* [Equilateral dimension](https://en.m.wikipedia.org/wiki/Equilateral_dimension)\n",
        "\n",
        "* [Dimensions of commutative algebra](https://en.m.wikipedia.org/wiki/Glossary_of_commutative_algebra#dimension), like Embedding dimension"
      ],
      "metadata": {
        "id": "DFFd1liCm28M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vapnik-Chervonenkis, Pseudo- and Fat-Shattering Dimensions are three distinct notions of “dimension”. The phrase “dimension” is rather unfortunate, as the three “dimensions” have nothing to do with the dimension of a vector space, except in very special situations. Rather, these “dimensions” are combinatorial parameters that measure the “richness” of concept classes or function classes.*\n",
        "\n",
        "* [Vapnik–Chervonenkis dimension](https://en.m.wikipedia.org/wiki/Vapnik–Chervonenkis_dimension)\n",
        "  * It quantifies the expressive power or complexity of a hypothesis class in terms of its ability to \"shatter\" sets of points. It's a scalar value that provides insight into the capacity of a class of functions or models.\n",
        "  * A hypothesis class is said to \"shatter\" a set of points if, for every possible labeling of the points, there exists a hypothesis in the class that can perfectly classify those points according to that labeling. VC dimension is defined in terms of shattering.\n",
        "  * It is the most widely used complexity metric in PAC learning, and it has been used to prove a number of important theoretical results.\n",
        "  * However, the VC dimension is not a perfect metric for the complexity of a hypothesis class. For example, the VC dimension of the class of all linear classifiers is infinite, even though linear classifiers can be learned efficiently from a small number of training examples.\n",
        "  * In PAC learning: Let H be a hypothesis class with VC dimension d. Then, the sample complexity of PAC learning H is given by: $n >= O(d / ε^2 * ln(1/δ))$, where ε is the desired error tolerance and δ is the desired confidence level.\n",
        "  \n",
        "  * [*Bias-Variance Tradeoff*](https://en.m.wikipedia.org/wiki/Bias–variance_tradeoff): This is a key principle in statistical learning that helps to understand the tradeoff between model complexity and the risk of overfitting, which impacts the number of samples needed for learning.\n",
        "\n",
        "  * Higher VC-dimension of a class = more samples are required to learn that class. Bounds on sample complexity using VC-dimension are often given in the form, where d is the VC-dimension:\n",
        "\n",
        "  * $m \\geq \\frac{8}{\\epsilon} \\ln \\left(\\frac{4}{\\delta}\\right)+\\frac{4}{\\epsilon} d \\ln \\left(\\frac{2 e m}{d}\\right)$\n",
        "\n",
        "  * measure of the capacity: The more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data (-that was assumed for neural nets).\n",
        "\n",
        "  * The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data. sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C. measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm.\n",
        "\n",
        "  * A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data. The VC Dimension can provide an **upper bound on the sample complexity in terms of the size of the hypothesis class**, the desired error rate, and the confidence level.\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1397.png)\n",
        "\n",
        "  * *Shatter points = separate points: how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity. - Shattered:  A set of points is shattered by H if for every possible labeling of the points, there exists a hypothesis in H that agrees with that labeling.*\n",
        "\n",
        "  * One of the most fundamental results in learning theory is that the sample complexity of C is tightly determined by a combinatorial parameter called the VC dimension of C (Source: Optimal Quantum Sample Complexity of Learning Algorithms)\n",
        "\n",
        "  ![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1643.png)\n",
        "\n",
        "  * VC dimension is still a useful concept in machine learning. It can be used to understand the complexity of a neural network and to choose a neural network that is not too complex, so that it can generalize well to new data.\n",
        "\n",
        "  * Limitations of the VC dimension (VC dimension theorem would seem to imply that large neural networks would overfit very badly and never generalize well):\n",
        "    * The VC dimension is only a theoretical bound, and it is not always accurate in practice. The VC dimension theorem assumes that the training data is drawn from an i.i.d. (independent and identically distributed) distribution. However, in practice, the training data is often not i.i.d., and this can make the VC dimension theorem less accurate.\n",
        "    * The VC dimension does not take into account the complexity of the target function. The VC dimension is only a bound on the generalization error. It is possible for a neural network to have a large VC dimension and still generalize well, if the training data is large enough.\n",
        "    * The VC dimension does not take into account the optimization algorithm used to train the neural network. There are a number of techniques that can be used to prevent neural networks from overfitting, such as regularization and dropout. These techniques can help to reduce the complexity of the neural network and make it more likely to generalize well.\n",
        "\n",
        "  * https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_VC_Dimension_Shatter.php\n",
        "\n",
        "  * Code example: https://www.geeksforgeeks.org/vapnik-chervonenkis-dimension/\n",
        "\n",
        "  * Video: https://youtu.be/puDzy2XmR5c?si=FTPneApRNiWQkJey\n",
        "\n",
        "  * VC dimension is a method to measure model complexity. It is a measure of the capacity of a hypothesis space, which is the set of all possible hypotheses that can be learned by a machine learning algorithm. A hypothesis space with a higher VC dimension can learn more complex functions, but it will also be more likely to overfit the training data.\n",
        "\n",
        "  * The VC Dimension can provide an upper bound on the sample complexity in terms of the size of the hypothesis class, the desired error rate, and the confidence level.\n",
        "\n",
        "  * In [Vapnik–Chervonenkis theory](https://en.m.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory), the [Vapnik–Chervonenkis (VC) dimension](v) is a **measure of the capacity (complexity, expressive power, richness, or flexibility)** of a set of functions that can be learned by a statistical binary classification algorithm.\n",
        "\n",
        "  * It is defined as the cardinality of the largest set of points that the algorithm can [shatter](https://en.m.wikipedia.org/wiki/Shattered_set), which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points.\n",
        "\n",
        "  * Relationship:\tThe VC dimension can be used to derive bounds on the sample complexity.\tThe sample complexity can be used to estimate the VC dimension.\n",
        "\n",
        "  * The VC dimension of a set of functions can be used to bound the generalization error of a learning algorithm. The generalization error is the error that a learning algorithm makes on new data that it has not seen before. The VC dimension theorem states that the generalization error of a learning algorithm is bounded by the VC dimension of the hypothesis space divided by the number of training examples. In other words, **the more complex the hypothesis space, the more likely it is that the learning algorithm will overfit the training data and make errors on new data** (-that was assumed for neural nets). The VC dimension can be used to choose a hypothesis space that is not too complex, so that the learning algorithm can generalize well to new data.\n",
        "\n",
        "* [Sauer's Lemma (or Sauer–Shelah Lemma)](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma): A combinatorial bound that relates the growth function to the VC dimension and the number of data points. It says that if a hypothesis class has a VC dimension \\( d \\) and does not shatter any set of \\( d+1 \\) points, then the number of dichotomies it can produce on any set of \\( n \\) points is bounded by the sum of binomial coefficients from \\( i = 0 \\) to \\( d \\).\n",
        "\n",
        "* [Natarajan dimension](https://en.m.wikipedia.org/wiki/Natarajan_dimension): An extension of VC dimension to multiclass classification problems.\n",
        "\n",
        "* **Fat-shattering dimension**:\n",
        "\n",
        "  * An extension of the VC dimension that considers the ability of a hypothesis class to shatter sets of points with margins. It's useful for analyzing algorithms that make use of margins, like Support Vector Machines. / [Fat-shattering dimension](https://mlweb.loria.fr/book/en/fatshattering.html) at a certain scale of a function class is defined as the maximal number of points that can be fat-shattered by this function class at that scale. It can be bounded for popular function classes such as for the set of linear functions.\n",
        "\n",
        "  * https://mathoverflow.net/questions/307201/vc-dimension-fat-shattering-dimension-and-other-complexity-measures-of-a-clas\n",
        "\n",
        "  * Covering Numbers and Fat-Shattering Dimension: measures of complexity of function classes that can be used to **derive upper bounds on sample complexity** (bounds on expressivity of class of CPTP maps (or unitaries) that a quantum machine learning model (QMLM) can implement in terms of number of trainable elements)\n",
        "\n",
        "  * **fat-shattering dimension and VC dimension (Vapnik-Chervonenkis dimension)** fat-shattering dimension is not an example of an inequality. It is a measure of the complexity of a function class. A function class with a small fat-shattering dimension is said to be easy to learn, while a function class with a large fat-shattering dimension is said to be difficult to learn.\n",
        "\n",
        "  * The fat-shattering dimension is a complexity measure used in statistical learning theory. It's one way of quantifying the **complexity or expressive power of a class of functions, and it's used to derive bounds on the generalization error** of a learning algorithm.\n",
        "\n",
        "  * The formal definition of the fat-shattering dimension is a bit technical, but here's the general idea: suppose you have a class of functions, and you want to understand how complex this class is. One way of doing this is to look at how these functions can separate or \"shatter\" sets of points. If you can find large sets of points that can be arbitrarily labeled (i.e., \"shattered\") by functions in your class, then the class has high complexity.\n",
        "\n",
        "  * **Now, for the fat-shattering dimension, you add an additional constraint: you require a certain margin (the \"fatness\") between points that are labeled differently**. The largest set of points that can be shattered with a given margin is used to define the fat-shattering dimension of the function class.\n",
        "\n",
        "  * The fat-shattering dimension is related to the VC dimension (Vapnik-Chervonenkis dimension), another complexity measure that is widely used in statistical learning theory. However, **while the VC dimension only considers exact separation of points, the fat-shattering dimension takes into account this idea of a margin, which makes it more suitable for dealing with \"noisy\" or non-separable data.**\n",
        "\n",
        "  * the concept of the fat-shattering dimension is often used in the analysis of machine learning algorithms, particularly those based on empirical risk minimization. It can help to understand the trade-off between the expressive power of a function class (which often corresponds to the complexity of a learning algorithm) and the ability of the algorithm to generalize well to unseen data.\n",
        "\n",
        "* **Pseudo-Dimension**: Similar in spirit to the VC dimension but adapted for real-valued function classes rather than binary classifiers. / [Pseudo-dimension](https://link.springer.com/chapter/10.1007/978-1-4471-3748-1_4), also Pollard dimension, is a generalization of the VC-dimension to real-valued functions. The fat-shattering dimension, unlike the Pseudo-dimension, is a “scale-sensitive” measure of richness.\n"
      ],
      "metadata": {
        "id": "yKc3PDS6bXwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Inequality (bound, complexity)</font> (Cramér–Rao, Trace , Cauchy-Schwarz, Rademacher, Hoeffding's)*"
      ],
      "metadata": {
        "id": "ThL-AmiaxI8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Concentration inequalities**](https://en.m.wikipedia.org/wiki/Concentration_inequality) = bounds probability of estimator deviating from its expected value by a certain amount = provide guarantees on accuracy of estimator. Inequalities above just bound magnitude of deviation, bound probability. Will a learning algorithm converge to correct hypothesis with high probability, even if data is noisy or incomplete? Concentration inequalities can be used to bound VC dimension of hypothesis space to design ML algorithms that are guaranteed to generalize well to new data. And in regularization (to control complexity of ML models): analyze effects of regularization on generalization performance of ML models. Concentration inequalities don't guarantee learning a function with small error, but provide high degree of confidence that algorithm is likely to learn a good function, given a sufficient number of training samples and a suitable regularization scheme.\n",
        "\n",
        "* Matrix concentration inequalities: use by Ewin tang for dequantization, see also: https://arxiv.org/abs/1501.01571 - An Introduction to Matrix Concentration Inequalities. Some common matrix concentration inequalities that might appear in Tang's work include:\n",
        "\n",
        "  * Matrix Bernstein Inequality: Provides bounds on the deviation of the sum of random matrices from its expectation. It has variants tailored to handling sums of matrices that aren't independent.\n",
        "  * Matrix Hoeffding Inequality: Generalizes Hoeffding's inequality (which is for sums of random variables) to the matrix setting.\n",
        "  * Matrix Chernoff Bound: Gives tail bounds on how much a random matrix might deviate from its mean.\n",
        "\n",
        "\n",
        "* [Markov's inequality](https://en.m.wikipedia.org/wiki/Markov%27s_inequality): simplest concentration inequality. Bounds probability that random variable deviates from its expected value by a certain amount = bounds probability of errors in statistical estimators, proves convergence of learning algorithms, analyzes performance of randomized algorithms.\n",
        "\n",
        "* [Hoeffding's inequality](https://en.m.wikipedia.org/wiki/Hoeffding%27s_inequality):\n",
        "  * We want to guarantee that a hypothesis with a small training error will have good accuracy on unseen examples, and one way to do so is with Hoeffding bounds. This characterizes the deviation between the true probability of some event and its observed frequency over m independent trails. [Source](https://www.cis.upenn.edu/~danroth/Teaching/CS446-17/LectureNotesNew/colt/main.pdf)\n",
        "  * states that probability that average of a number of independent and identically distributed random variables deviates from its expected value by more than a certain amount is exponentially small in number of random variables.\n",
        "  * average of n independent random variables deviates from its expected value by more than ε at most $2e^{(-2nε^2)}$ probability and decays exponentially with number of random variables -> Probability of deviation becomes very small as number of random variables increases. **This inequality bounds generalization error of a learning algorithm, regardless of the model complexity or the sample complexity. Can show that sample complexity of learning a linear regression model with a certain level of accuracy is proportional to logarithm of number of features.**  \n",
        "  * Hoeffding's inequality is a **special case of the Azuma–Hoeffding inequality and McDiarmid's inequality**. It is **similar to the Chernoff bound**, but tends to be less sharp, in particular when the variance of the random variables is small. It is **similar to, but incomparable with, one of Bernstein's inequalities**.\n",
        "  * Example: bound sample complexity of online learning algorithm (with perceptron):\n",
        "    * $X_1, X_2, ..., X_n$ be independent random variables with mean $\\mu$ and variance $\\sigma^2$.\n",
        "    * For any $\\delta > 0$, $P(\\bar{X} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\bar{X}$ is average of $X_i$.\n",
        "    * $L_i$ be loss of Perceptron on $i$th training sample. Average loss of Perceptron on training data is: $\\bar{L} = \\frac{1}{n} \\sum_{i=1}^n L_i$.\n",
        "    * Objective: Use concentration inequality to show that average loss of Perceptron on training data is close to its expected value with high probability, given a sufficient number of training samples: $P(\\bar{L} - \\mu > \\delta) \\leq \\exp(-\\delta^2 n / 2 \\sigma^2)$ where $\\mu$ is expected loss of Perceptron on training data.\n",
        "    * With probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$, average loss of Perceptron on training data is within $\\delta$ of its expected value.\n",
        "    * Set $\\delta$ to be a small value, such as $0.01$, to ensure that average loss of Perceptron on training data is very close to its expected value with high probability.\n",
        "    * Hoeffdings inequality bounds sample complexity: $n \\geq \\frac{2 \\sigma^2 \\ln(1 / \\delta)}{\\delta^2}$ tells us that if we have $n$ training samples, then average loss of Perceptron on training data is within $\\delta$ of its expected value with probability at least $1 - \\exp(-\\delta^2 n / 2 \\sigma^2)$.\n",
        "\n",
        "* [Azuma's inequality](https://en.m.wikipedia.org/wiki/Azuma%27s_inequality)\n",
        "\n",
        "* [Chernoff bounds](https://en.m.wikipedia.org/wiki/Chernoff_bound): Bound probability of sum of independent random variables deviating from its expected value by more than a certain amount, even if the random variables are not identically distributed. Generalization of Hoeffding's inequality, used to bound tails of probability distributions.\n",
        "\n",
        "* [Bernstein's inequality](https://en.m.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)): generalization of Chernoff bounds. Bounds probability, even if the random variables are not identically distributed and have non-identical variances (used to bound tails of sub-Gaussian distributions).\n",
        "\n",
        "* [McDiarmid's inequality](https://en.m.wikipedia.org/wiki/McDiarmid%27s_inequality): bounds deviation between sampled value and expected value of certain functions (= functions that satisfy a bounded differences property, meaning that replacing a single argument to the function while leaving all other arguments unchanged cannot cause too large of a change in the value of the function) when they are evaluated on independent random variables.\n",
        "\n",
        "* [Bennett's inequality](https://en.m.wikipedia.org/wiki/Bennett%27s_inequality): provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. **Can be used to: Bound the generalization error of learning algorithms.**\n",
        "\n",
        "* [(Bienaymé–) Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality): provide upper bounds on the probability that a random variable deviates from its expected value by more than a certain amount. Chebyshev's inequality is particularly useful for bounding the deviations of random variables with finite mean and variance. One way to think about Chebyshev's inequality is that it provides a measure of how concentrated the probability distribution of a random variable is around its expected value. The more concentrated the probability distribution is, the lower the probability is that the random variable will deviate from its expected value by more than a certain amount. **Can be used to: Bound the probability of rare events**. Design confidence intervals for population parameters. Develop efficient algorithms for statistical testing.\n",
        "\n",
        "* [Vysochanskij–Petunin inequality](https://en.m.wikipedia.org/wiki/Vysochanskij%E2%80%93Petunin_inequality):  It is a refinement of Chebyshev's inequality that provides tighter bounds on the probability that a random variable deviates from its expected value by more than a certain amount, especially for random variables with unimodal distributions."
      ],
      "metadata": {
        "id": "2WMz6oFrnTD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Inequality (complexities)](https://en.m.wikipedia.org/wiki/Inequality_(mathematics)): bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.** See [Inequalities in information theory](https://en.m.wikipedia.org/wiki/Inequalities_in_information_theory), [Information geometry](https://en.m.wikipedia.org/wiki/Information_geometry), [Information projection](https://en.m.wikipedia.org/wiki/Information_projection), and [Confidence Interval](https://en.m.wikipedia.org/wiki/Confidence_interval).\n",
        "\n",
        "Evaluate performance of estimators: bounds on Prediction Errors, Sample Complexity, Model complexity, and Deviation of Estimators. By bounding error: ensure that model is not too far from optimal solution (Regularize models by bounding complexity of model: L1 norm to bound number of parameters, L2 norm to bound sum of squared parameters).\n",
        "\n",
        "* [Triangle inequality](https://en.m.wikipedia.org/wiki/Triangle_inequality). K-means clustering: cost function is sum of squared distances between data points and cluster centroids. Squared distances can be bounded by triangle inequality.\n",
        "\n",
        "* [Cramér–Rao bound](https://en.m.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound): lower bound (min value) on variance of an unbiased estimator (but cannot tell about probability of deviation) - this value is inverse of [Fisher information](https://de.m.wikipedia.org/wiki/Fisher-Information#Verwendung)(how much information data provides about parameters being estimated). Used to design estimators that are as efficient as possible.\n",
        "\n",
        "  * [*Quantum Cramér-Rao Bound*](https://en.m.wikipedia.org/wiki/Quantum_Cramér–Rao_bound)\n",
        "\n",
        "  * QFIM and Quantum Cramér-Rao Bound used in quantum metrology to **quantify ultimate limit to precision** that can be achieved in estimating parameters (CRB (derived from FIM) tells us smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator. Fisher Information is a measure of how much information a random variable provides about an unknown parameter).\n",
        "\n",
        "  * Is an inequality: bound probability of random variable deviating from its expected value. Analyze reliability and stability of algorithm. Computational complexity: analysis of randomized algorithms to bound error probabilities.\n",
        "\n",
        "  * The Cramér-Rao bound (CRB) belongs to **model complexity** = Bounds on Precision of Estimator. It gives a measure of the best possible precision that can be achieved when estimating that parameter from a given set of data. The **quantum Cramér-Rao bound provides a limit on the precision with which these properties can be estimated, given the inherent uncertainties of quantum mechanics**.\n",
        "\n",
        "  * It provides a lower bound on the covariance matrix of any unbiased quantum estimator (The covariance matrix determines the uncertainty or precision of the estimated parameter). It characterizes the best possible precision that can be achieved in estimating a parameter of interest from quantum measurements.\n",
        "\n",
        "  * The quantum Cramér-Rao bound serves as a powerful tool for understanding the fundamental limits of precision in quantum state estimation (without consoidering noise, imperfections etc).\n",
        "\n",
        "  * The quantum Cramér-Rao bound is given by the inverse of the **Fisher information matrix**, which in the quantum case is calculated using the quantum state and the POVM (Positive Operator-Valued Measure) elements describing the measurements. It can provide insights into the optimal measurement strategies for estimating the parameters of a quantum state, and can help guide the design of quantum sensors and other quantum technologies.\n",
        "\n",
        "  * The basic formula for calculating the quantum Cramér-Rao bound (QCRB) for parameter estimation:\n",
        "\n",
        "  * $QCRB = Tr(F^{-1} M)$\n",
        "\n",
        "  * where:\n",
        "\n",
        "    - QCRB is the quantum Cramér-Rao bound, which provides a lower bound on the covariance matrix of any unbiased estimator.\n",
        "    - $F$ is the Fisher information matrix, which quantifies the amount of information provided by the measurements about the parameter of interest.\n",
        "    - $M$ is the symmetric, positive semidefinite matrix representing the measurement operators' influence on the parameter estimation.\n",
        "\n",
        "  * To calculate the QCRB, you'll need to obtain the Fisher information matrix (F) and the measurement matrix (M) for the chosen estimation strategy. The Fisher information matrix can be computed based on the measurement operators and the quantum state being measured.\n",
        "\n",
        "  * Cramér-Rao is a lower bound on the variance of any unbiased estimator of a parameter, given a fixed model. The CRB depends on the Fisher information, which is a measure of how much information the data provides about the parameter. A higher Fisher information means that the data is more informative about the parameter, and therefore the CRB is lower.\n",
        "\n",
        "  * In general, the CRB is a more fundamental concept than the sample complexity. It is used to understand the fundamental limits of parameter estimation, regardless of the amount of data available. The sample complexity, on the other hand, is more practical, as it tells us how much data we need to achieve a certain level of accuracy.\n",
        "\n",
        "  * In machine learning, the Cramér-Rao Bound (CRB) is often used as a tool for **understanding the fundamental limits of learning algorithms, particularly in the field of parameter estimation and model selection**. Here are some specific applications:\n",
        "\n",
        "    1. Variance Estimation: Similar to financial economics, the CRB provides a theoretical lower limit for the variance of an unbiased estimator. This can help understand how well a learning algorithm might perform in terms of parameter estimation, given a certain amount of data.\n",
        "\n",
        "    2. Model Complexity: The CRB can provide insights into how model complexity affects estimation accuracy. A model with too many parameters may have a higher CRB (i.e., higher variance for the best possible estimator), indicating that it could be more prone to overfitting.\n",
        "\n",
        "    3. Algorithm Evaluation: Researchers might use the CRB to evaluate and compare the performance of different learning algorithms. If an algorithm's performance is close to the CRB, it might be deemed near-optimal.\n",
        "\n",
        "    4. Designing Neural Networks: In the design of neural networks, the CRB can be used to understand the limit of what the network can learn from the data, which can help in making decisions about network architecture and training strategies.\n",
        "\n",
        "* [Trace_inequality](https://en.m.wikipedia.org/wiki/Trace_inequality) bounds the trace of a matrix by the sum of the traces of its eigenvalues. Trace inequality, also known as the triangle inequality for the trace distance, states that for any three quantum states ρ, σ, and τ, the following inequality holds:\n",
        "  * $δ(ρ, σ) + δ(σ, τ) ≥ δ(ρ, τ)$\n",
        "  * where δ(ρ, σ) is the trace distance between ρ and σ.\n",
        "  * Bounding the error of quantum learning algorithms: The trace inequality can be used to derive upper bounds on the error of quantum learning algorithms. This is useful for understanding the performance of quantum learning algorithms and for comparing them to classical learning algorithms.\n",
        "  *  The trace inequality can be used to derive bounds on the error of a wide variety of quantum learning algorithms, including algorithms for learning linear classifiers, support vector machines, and neural networks.\n",
        "  * It is important to note that the bounds derived using the trace inequality are often loose. This is because the trace inequality is a very general tool, and it does not take into account the specific structure of the quantum learning algorithm or the training data. However, the trace inequality can still be used to get a rough estimate of the error of a quantum learning algorithm.\n",
        "\n",
        "* [Bohnenblust-Hille inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality):\n",
        "\n",
        "  * The Bohnenblust-Hille inequality says that the $\\ell^{\\frac{2 m}{m+1}}$-norm of the coefficients of an $m$-homogeneous polynomial $P$ on $\\mathbb{C}^n$ is bounded by $\\|P\\|_{\\infty}$ times a constant independent of $n$, where $\\|\\cdot\\|_{\\infty}$ denotes the supremum norm on the polydisc $\\mathbb{D}^n$. [Source](https://annals.math.princeton.edu/wp-content/uploads/annals-v174-n1-p13-s.pdf)\n",
        "\n",
        "\n",
        "* [Littlewood's 4/3 inequality](https://en.m.wikipedia.org/wiki/Littlewood%27s_4/3_inequality)\n",
        "\n",
        "  * is a norm inequality for multilinear forms and polynomials. It is used to study the behavior of random variables and multilinear forms. Can be used to derive concentration inequalities in some cases. For example, it can be used to prove that the coefficients of a random homogeneous polynomial are concentrated around their expected values. This can then be used to prove concentration inequalities for other random variables, such as the output of a neural network. Used in \"Learning to predict arbitrary quantum processes\".\n",
        "\n",
        "\n",
        "* [Golden–Thompson inequality](https://en.m.wikipedia.org/wiki/Golden%E2%80%93Thompson_inequality) bounds difference between logarithm of trace of a matrix and sum of logarithms of its eigenvalues. Used in 'A survey on the complexity of learning quantum states' page 17 - difference between real state and shadow tomography state)\n",
        "\n",
        "* [Cauchy–Schwarz inequality](https://en.m.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality). Cauchy-Schwarz inequality used to bound sum of squared residuals (cost function for linear regression) by the sum of the squared predicted values and the sum of the squared actual values. Used in *A Survey of Quantum Learning Theory* page 11 to compute upper bounds of learning algorithm.\n",
        "\n",
        "* [Chebyshev's inequality](https://en.m.wikipedia.org/wiki/Chebyshev%27s_inequality) bounds probability that a random variable deviates from its expected value by more than a certain amount (k standard deviations at most 1/k^2).\n",
        "\n",
        "* [Rademacher Complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity):\n",
        "  * upper bound on sample complexity = on learnability of function classes (deriving generalization bounds). Measures ability of functions in class to fit to random noise. [Rademacher Complexity PDF](https://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf).\n",
        "  * When the Rademacher complexity is small, it is possible to learn the hypothesis class H using [empirical risk minimization](https://en.m.wikipedia.org/wiki/Empirical_risk_minimization).\n",
        "\n",
        "* [Gaussian complexity](https://en.m.wikipedia.org/wiki/Rademacher_complexity#Gaussian_complexity):  similar complexity to Rademacher. Can be obtained from Rademacher using random variables $g_i$ instead of $\\sigma_i$, where $g_i$ are Gaussian i.i.d. random variables with zero-mean and variance 1, i.e. $g_i \\sim \\mathcal{N}(0,1)$. Gaussian and Rademacher are equivalent up to logarithmic factors.\n",
        "\n",
        "* [Fano's inequality](https://en.m.wikipedia.org/wiki/Fano%27s_inequality): is a lower bound on the average probability of error in a multiple hypothesis testing problem. It states that the average probability of error in a multiple hypothesis testing problem is at least as high as the entropy of the hypothesis distribution divided by the natural logarithm of two. It can be used to: Design efficient multiple hypothesis testing algorithms. Lower bound the generalization error of learning algorithms. Develop new algorithms for information compression and transmission. Fano's inequality is a powerful tool for analyzing the performance of multiple hypothesis testing algorithms and other machine learning algorithms.\n",
        "\n",
        "* [Data processing inequality](https://en.m.wikipedia.org/wiki/Data_processing_inequality): is a concept that states that the information content of a signal cannot be increased via a local physical operation. This can be expressed concisely as 'post-processing cannot increase information'. It is an inequality that relates the mutual information between two random variables before and after a data processing channel. It states that the mutual information between two random variables after a data processing channel cannot be greater than the mutual information between the two random variables before the channel. It can be used to: prove that certain learning problems are impossible to solve perfectly.\n",
        "Design efficient algorithms for information compression and transmission. It is a powerful tool for analyzing the flow of information through systems."
      ],
      "metadata": {
        "id": "r6LqFuIXrNmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A norm inequality (like Bohnenblust-Hille inequalities) is an inequality that involves the norms of vectors and matrices. Norms are functions that measure the size of vectors and matrices. There are many different types of norms, but some of the most common include the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "Norm inequalities are useful in many different areas of mathematics, including functional analysis, numerical linear algebra, and machine learning. For example, norm inequalities can be used to prove bounds on the error of numerical algorithms, to design efficient machine learning algorithms, and to analyze the stability of differential equations.\n",
        "\n",
        "Here are some examples of norm inequalities:\n",
        "\n",
        "* **Triangle inequality:** $\\|x + y\\| \\leq \\|x\\| + \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Cauchy-Schwarz inequality:** $\\|⟨x, y⟩\\| \\leq \\|x\\| \\|y\\|$ for all vectors $x$ and $y$.\n",
        "* **Hölder's inequality:** $\\left\\| \\sum_{i=1}^n a_i b_i \\right\\| \\leq \\left\\| \\sum_{i=1}^n |a_i|^p \\right\\|^{1/p} \\left\\| \\sum_{i=1}^n |b_i|^q \\right\\|^{1/q}$, where $p$ and $q$ are conjugate exponents (i.e., $1/p + 1/q = 1$).\n",
        "* **Submultiplicativity of the Frobenius norm:** $\\|AB\\|_F \\leq \\|A\\|_F \\|B\\|_F$ for all matrices $A$ and $B$.\n",
        "\n",
        "Norm inequalities can be used to prove many other important results in mathematics. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations. The Cauchy-Schwarz inequality can be used to prove the Bessel inequality, which is a fundamental result in Fourier analysis. Hölder's inequality can be used to prove the Minkowski inequality, which is another important result in functional analysis.\n",
        "\n",
        "Norm inequalities are a powerful tool in mathematics, and they have many applications in science and engineering.\n",
        "\n",
        "*Yes, the triangle inequality is a norm inequality. The triangle inequality states that the norm of the sum of two vectors is less than or equal to the sum of the norms of the two vectors. In other words, for any two vectors $x$ and $y$, we have:*\n",
        "\n",
        "```\n",
        "||x + y|| <= ||x|| + ||y||\n",
        "```\n",
        "\n",
        "This inequality holds for any norm, including the Euclidean norm, the $p$-norm, and the Frobenius norm.\n",
        "\n",
        "The triangle inequality is a fundamental property of norms, and it has many important consequences. For example, the triangle inequality can be used to prove the existence and uniqueness of solutions to differential equations, and it can also be used to design efficient machine learning algorithms.\n",
        "\n",
        "The triangle inequality is also related to the triangle inequality in Euclidean geometry. The triangle inequality in Euclidean geometry states that the sum of the lengths of any two sides of a triangle must be greater than or equal to the length of the third side. This inequality can be proven using the triangle inequality for norms.\n",
        "\n",
        "In conclusion, the triangle inequality is a norm inequality that has many important applications in mathematics, science, and engineering."
      ],
      "metadata": {
        "id": "6eeeLVg3nFxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what means that a generalization bound is uniform?**\n",
        "\n",
        "A generalization bound is said to be uniform if it holds for all hypotheses in a given hypothesis class, rather than just for a specific hypothesis. This is important because it allows us to make guarantees about the performance of our learning algorithm on any new data, regardless of the specific hypothesis that it learns.\n",
        "\n",
        "Uniform generalization bounds are typically derived using a technique called **Rademacher complexity**. Rademacher complexity is a measure of the complexity of a hypothesis class, and it can be used to bound the generalization error of any learning algorithm that uses that hypothesis class.\n",
        "\n",
        "Uniform generalization bounds are particularly useful for complex hypothesis classes, such as the class of all neural networks. These hypothesis classes have infinite VC-dimension, which makes it difficult to derive generalization bounds using traditional methods. However, uniform generalization bounds based on Rademacher complexity can be applied to any hypothesis class, regardless of its VC-dimension.\n",
        "\n",
        "Some examples of uniform generalization bounds include:\n",
        "\n",
        "* The generalization bound for uniformly stable learning algorithms\n",
        "* The generalization bound for overparameterized neural networks in kernel regimes\n",
        "\n",
        "Uniform generalization bounds are an important tool for understanding and analyzing the performance of machine learning algorithms. They allow us to make guarantees about the ability of our algorithms to generalize to new data, even when the hypothesis class is complex.\n",
        "\n",
        "Here is a simple analogy to help understand uniform generalization bounds:\n",
        "\n",
        "Imagine that you are trying to train a model to predict whether a given email is spam or not. You have a dataset of labeled emails, and you use this dataset to train a machine learning algorithm.\n",
        "\n",
        "Once the algorithm is trained, you want to be able to use it to predict the spam/ham label of any new email, even if that email is not in the training dataset.\n",
        "\n",
        "A uniform generalization bound tells you that the probability that your algorithm will make a mistake on any new email is less than a certain threshold. This threshold is independent of the specific email that you are trying to predict.\n",
        "\n",
        "In other words, a uniform generalization bound guarantees that your algorithm will generalize well to any new data, even if that data is very different from the training data."
      ],
      "metadata": {
        "id": "WLrjg16KskW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Norm</font> (Frobenius, Jacobian, Spectral, Trace)*"
      ],
      "metadata": {
        "id": "8SsJmnB0xNNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Norm](https://de.wikipedia.org/wiki/Norm_(Mathematik)) measure size or length of vectors**. Compute error of model or regularize models. Analyze complexity of algorithms in terms of space or time. A norm-based measure of complexity in computational learning theory is a **measure that quantifies the difficulty of learning a function based on the norm of the function**. The norm of a function is a measure of the size or magnitude of the function.\n",
        "* Both the Frobenius norm and the spectral norm can be used to derive generalization bounds for machine learning algorithms. **Norm-based generalization bounds provide a strong theoretical guarantee on the performance of a machine learning algorithm on unseen data.**\n",
        "* Specific machine learning task require specific norm-based measure of complexity. All norm-based measures of complexity provide a useful way to quantify the difficulty of learning a function.\n",
        "* Norm kann von [Skalarprodukt](https://de.wikipedia.org/wiki/Skalarprodukt) abgeleitet werden (['Skalarproduktnorm'](https://de.m.wikipedia.org/wiki/Skalarproduktnorm)). In diesem Fall: norm of vector is square root of inner product of vector by itself. Complete space with an inner product is called a [Hilbert space](https://de.m.wikipedia.org/wiki/Hilbertraum).\n",
        "* In optimization: define objective function and constraints. Computer science: define complexity of algorithms.\n",
        "* Classical vs Quantum probability theory:\n",
        "  * classical probability theory: 1-Norm: ∑ pi = ∑ | p | = || p || 1 = 1\n",
        "  * quantum probabiliyt theory: 2-Norm: || | ψ > || ^2 = 1\n",
        "* Reeller Vektor für 1-, 2-, 3- und $\\infty$-Normen des Vektors $x=(3,-2,6)$:\n",
        "  * $\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3|+|-2|+|6|=11 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3|^2+|-2|^2+|6|^2}=\\sqrt{49}=7 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3|,|-2|,|6|\\}=6\n",
        "\\end{aligned}\n",
        "$\n",
        "* Komplexer Vektor für 1-, 2-, 3- und $\\infty$-Normen des Vektors $x=(3-4 i,-2 i)$:\n",
        "  * $\n",
        "\\begin{aligned}\n",
        "& \\|x\\|_1=|3-4 i|+|-2 i|=5+2=7 \\\\\n",
        "& \\|x\\|_2=\\sqrt{|3-4 i|^2+|-2 i|^2}=\\sqrt{5^2+2^2}=\\sqrt{29} \\approx 5,385 \\\\\n",
        "& \\|x\\|_{\\infty}=\\max \\{|3-4 i|,|-2 i|\\}=\\max \\{5,2\\}=5\n",
        "\\end{aligned}$"
      ],
      "metadata": {
        "id": "n7MLTBvFGN7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Normen auf endlichdimensionalen Vektorräumen*](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_endlichdimensionalen_Vektorr%C3%A4umen)\n",
        "* [Zahlnorm](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Zahlnormen): zB [Betragsnorm](https://de.m.wikipedia.org/wiki/Betragsfunktion)\n",
        "* [Vektornormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Vektornormen):\n",
        "  * zB [$p$ -Normen](https://de.m.wikipedia.org/wiki/P-Norm) $\\|x\\|_{p}:=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$.\n",
        "  * [Summennorm](https://de.m.wikipedia.org/wiki/Summennorm) ,\n",
        "  * Euklidische Norm und [Maximumsnorm](https://de.m.wikipedia.org/wiki/Maximumsnorm). L2 norm more robust to outliers than L1 norm.\n",
        "* [Matrixnorm](https://de.m.wikipedia.org/wiki/Matrixnorm):\n",
        "  * Matrix norm is a function that measures the size of a matrix. It must satisfy the following properties:\n",
        "    * ||A|| ≥ 0 for all matrices A.\n",
        "    * ||A|| = 0 if and only if A is the zero matrix.\n",
        "    * ||cA|| = |c| ||A|| for all scalars c and all matrices A.\n",
        "    * ||A + B|| ≤ ||A|| + ||B|| for all matrices A and B.\n",
        "  * **The trace norm is a matrix norm**, but it is not the only one. **Other examples of matrix norms include the Frobenius norm and the spectral norm**.\n",
        "  * [Natürliche Matrixnorm](https://de.m.wikipedia.org/wiki/Nat%C3%BCrliche_Matrixnorm): größtmöglicher Streckungsfaktor, der durch Matrix auf Vektor entsteht. [Spektralnorm](https://de.m.wikipedia.org/wiki/Spektralnorm): größtmöglicher Streckungsfaktor, der durch Matrix auf Vektor der Länge Eins entsteht = maximalen Singulärwert, [Singulärwertzerlegung](https://de.m.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)\n",
        "  * Various matrix norms provide different ways to quantify the size or complexity of matrices. While the Fisher Information Matrix is not a matrix norm, it's still a matrix, and depending upon the application, different matrix norms might be used to analyze its properties.\n",
        "* $\\hookrightarrow$ [Frobenius norm](https://de.m.wikipedia.org/wiki/Frobeniusnorm):\n",
        "  * special case of matrix norm. Frobenius norm of a matrix is the square root of the sum of the squares of all the elements of the matrix. Relatively easy to compute. Used in quantum state tomography (see *A Survey of Quantum Learning Theory* page 15. [interesting article](https://www.inference.vc/generalization-and-the-fisher-rao-norm-2/).\n",
        "  * E.g. Learn linear regression model from set of training data. One way: use ordinary least squares (OLS) algorithm (minimizes sum of squared residuals to find model). Frobenius norm: derive generalization bound for OLS algorithm that guarantees that probability of OLS algorithm making a large mistake on unseen data is small.\n",
        "* $\\hookrightarrow$ [Schatten p-Norm](https://en.m.wikipedia.org/wiki/Schatten_norm):\n",
        "  * The Schatten norm of the difference between the unitary matrices generated by the circuit and a target unitary operation can be used as a measure of the expressivity. Smaller values of the Schatten norm indicate higher expressivity. Useful measure of complexity for learning linear models and neural networks.\n",
        "  * Schatten norm is a class of matrix norms that includes the trace norm as a special case. The Schatten norm of order p, denoted by ||A||_p, is defined as the pth root of the sum of the pth powers of the singular values of A.\n",
        "* $\\hookrightarrow$ **Jacobian norm**\n",
        "  * is the maximum norm of the Jacobian matrix of a function. It is a measure of how the function changes with respect to its inputs.\n",
        "  * Jacobian norm is a useful tool for measuring the sensitivity of a function to its inputs and for designing robust control systems.\n",
        "  * The Jacobian matrix represents how changes in input affect changes in output in a vector-valued function. The Fisher Information matrix can sometimes be expressed in terms of derivatives (like the Jacobian), particularly when trying to understand how parameter changes influence the model's predictive distribution.\n",
        "* $\\hookrightarrow$ **Spectral norm**\n",
        "  * special case of matrix norm\n",
        "  * spectral morm of a matrix is the largest singular value of the matrix. Spectral norm is a more restrictive measure of complexity than the Frobenius norm, and more difficult to compute.\n",
        "  * This measures the largest singular value of a matrix. In the context of neural networks, it has been used to understand the smoothness of the loss landscape, which can be somewhat related to Fisher Information by providing insights into how parameters changes impact the output.\n",
        "  * Spectral norm is a useful tool for analyzing the stability of numerical algorithms and for studying the behavior of dynamical systems.\n",
        "  * The spectral norm of the Jacobian matrix of a function is an upper bound on the Jacobian norm of the function itself. This means that the spectral norm of the Jacobian matrix can be used to measure the maximum amount by which the function can change with respect to its inputs. Here is a simple example: Consider the following function: f(x) = x^2\n",
        "    * The Jacobian matrix of this function is simply the matrix [2x]. The spectral norm of the Jacobian matrix is 2x, and the Jacobian norm of the function is simply |2x|.\n",
        "    * If we set x = 1, then the spectral norm of the Jacobian matrix is 2, and the Jacobian norm of the function is also 2. However, if we set x = 10, then the spectral norm of the Jacobian matrix is 20, but the Jacobian norm of the function is still 2.\n",
        "    * This shows that the spectral norm of the Jacobian matrix is an upper bound on the Jacobian norm of the function itself.\n",
        "* $\\hookrightarrow$ [Trace norm (Nuclear norm)](https://en.m.wikipedia.org/wiki/Matrix_norm#Schatten_norms):\n",
        "  * It is sum of singular values of a matrix. Useful measure of complexity for learning linear models and neural networks. Special case of the Schatten norm, which is a class of matrix norms.\n",
        "  * Common distance measures in quantum information theory: here, \"distance measure\" is used in an informal sense, only three of the quantities introduced below are actual \"distances\" in the sense of a metric. First, and maybe most naturally, we can equip the set $\\mathcal{S}\\left(\\mathbb{C}^d\\right)$ with the (for convenience scaled) trace norm $I_i / 2$. This allows us to measure the difference between two quantum states $\\rho, \\sigma \\in$ $S\\left(\\mathbb{C}^d\\right)$ via the trace distance $\\left.\\left.\\|\\rho-\\sigma\\|_1 / 2=\\operatorname{tr} \\| \\rho-\\sigma\\right]\\right] / 2$. In fact, this distance is not only intuitive from a mathematical perspective, it also has an operational interpretation: The maximal success probability in distinghuishing $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$, assuming that either of the two is prepared with probability $1 / 2$, by performing a 2-outcome measurement on a single copy of the unknown state is given by $\\sup _{E \\in \\mathcal{E}(\\mathrm{C})} \\operatorname{tr}[E(\\rho-\\sigma)]+\\frac{1}{2}=\\frac{\\|\\rho-\\sigma\\|_1+1}{2}$ (see, e.g., $[17$, Chapter 9$]$ for a derivation).\n",
        "  * The trace distance is defined as half of the trace norm of the difference of the matrices\n",
        "  * The trace norm (also known as the nuclear norm) is a specific case of Schatten p-norm when p=1. It's the sum of singular values of a matrix. Sometimes, in the context of regularizing machine learning models, these norms are considered to constrain the learning capacity of the model. The trace of the Fisher Information Matrix can give a measure of the average sensitivity of the log-likelihood to parameter changes.\n",
        "* **Basis-path norm**: https://arxiv.org/abs/1809.07122\n"
      ],
      "metadata": {
        "id": "u_QCvV6brYZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Normen auf unendlichdimensionalen Vektorräumen*](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Normen_auf_unendlichdimensionalen_Vektorräumen)\n",
        "* [Folgennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Folgennormen): $\\ell^{p}$ -Normen Verallgemeinerung der $p$ -Normen auf Folgenräume: $\\left\\|\\left(a_{n}\\right)\\right\\|_{\\ell^{p}}=\\left(\\sum_{n=1}^{\\infty}\\left|a_{n}\\right|^{p}\\right)^{1 / p}$. Mit Normen werden $\\ell$ - Räume zu vollständigen normierten Räumen.\n",
        "* [Supremumsnorm](https://de.m.wikipedia.org/wiki/Supremumsnorm): Für Grenzwert $p \\rightarrow \\infty$ ergibt sich Raum der beschränkten Folgen $\\ell^{\\infty}$\n",
        "* [Funktionennormen](https://de.m.wikipedia.org/wiki/Norm_(Mathematik)#Funktionennormen) im Funktionenraum, L-p-Raum.  $\\mathcal{L}^{p}$ -Normen definiert als $\\|f\\|_{\\mathcal{L}^{P}(\\Omega)}=\\left(\\int_{\\Omega}|f(x)|^{p} d x\\right)^{1 / p}$ (Summe durch Integral ersetzt).\n",
        "* [Diamond norm](https://en.m.wikipedia.org/wiki/Diamond_norm) (completely bounded trace norm) a measure defined between two quantum operations (two linear maps between operator spaces). Captures max difference between effects of two quantum operations.\n",
        "\n",
        "[*Normed Vector Spaces*](https://en.m.wikipedia.org/wiki/Normed_vector_space)\n",
        "* [L<sup>p</sup>-space](https://de.m.wikipedia.org/wiki/Lp-Raum) (Lebesgue Space) bestehen aus p-fach integrierbaren Funktionen. Für jede Zahl $0 < p \\leq \\infty$ ist $L^{p}$ -Raum definiert.\n",
        "* [Banach space](https://de.wikipedia.org/wiki/Banachraum): $\\mathbb{R}$<sup>n</sup> together with p-norm = vollständiger normierter Vektorraum (Lp-space over R^n). Viele Folgenräume $\\ell$ oder Funktionenräume $L$ sind unendlichdimensionale Banachräume.\n",
        "* [Hilbert space](https://de.wikipedia.org/wiki/Hilbertraum): Banachraum, dessen Norm durch Skalarprodukt induziert ist (p2-Norm, aber nicht p1 -> Prähilbertraum).\n",
        "* [Hardy space](https://de.m.wikipedia.org/wiki/Hardy-Raum): Untersucht man statt messbarer Funktionen nur holomorphe und harmonische Funktionen auf Integrierbarkeit im $L^{p}$-Raum\n",
        "* [F-space](https://en.m.wikipedia.org/wiki/F-space): for Lp-space 0 < p < 1. Admits complete translation-invariant metric with respect to which vector space operations are continuous.\n",
        "* [Fréchet space](https://en.m.wikipedia.org/wiki/Fr%C3%A9chet_space): locally convex F-spaces.\n",
        "* [Sobolev space](https://de.wikipedia.org/wiki/Sobolev-Raum): Funktionenraum von schwach differenzierbaren Funktionen. Variationsrechnung: zur Lösungstheorie partieller Differentialgleichungen"
      ],
      "metadata": {
        "id": "8WHLzF42GF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Inner Product](https://de.wikipedia.org/wiki/Skalarprodukt) (auch Dot Product / Scalar Product / Skalarprodukt) ordnet zwei Vektoren eine Zahl (Skalar) zu, die die Ähnlichkeit messen** (über Länge, Winkel von Vektoren, oder ob diese senkrecht zueinander stehen)\n",
        "  * Skalarprodukt zweier Vektoren: $\\vec{a} \\cdot \\vec{b}=|\\vec{a}||\\vec{b}| \\cos \\alpha(\\vec{a}, \\vec{b})$. Wenn $\\vec{a} \\cdot \\vec{b}= 0$, dann stehen orthogonal zueinander.\n",
        "  * [Dot product](https://en.m.wikipedia.org/wiki/Dot_product) or scalar product (special case): dot a ⋅ b.\n",
        "  * The [determinant](https://en.wikipedia.org/wiki/Determinant) is a scalar value that can be computed from the elements of a **square matrix** and encodes certain properties of the linear transformation described by the matrix. Geometrically can be viewed as volume scaling factor of linear transformation described by matrix.\n",
        "  * \"Inner Product of Functions\" says how similar two functions are (used in Fourier Transform): if they are orthogonal, then zero. if they are very similar, then they have a large inner product: $\\langle f(x), g(x)\\rangle=\\int_{a}^{b} f(x) g(x) d x$\n",
        "  * Take samples from functions - Up to infinity, you get integral (Riemann approximation of continuuos integral above): $\\langle f, g\\rangle=g^{\\top} {f}$ = $\\langle f, g \\rangle \\Delta x=\\sum_{k=1}^{n} f\\left(x_{n}\\right) g\\left(x_{n}\\right) \\Delta x$\n",
        "  * [Inner Product Space](https://en.m.wikipedia.org/wiki/Inner_product_space) (Prähilbertraum bzw. Skalarproduktraum) generalize Euclidean spaces (in which inner product is dot product = scalar product) to vector spaces of any (possibly infinite) dimension."
      ],
      "metadata": {
        "id": "FeFQyIqardUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Entropy</font> (Metric, Relative, Conditional, Cross)*"
      ],
      "metadata": {
        "id": "l-DiG5X3xTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper: [The Entropy-Based Quantum Metric](https://www.mdpi.com/1099-4300/16/7/3878)"
      ],
      "metadata": {
        "id": "VKof1_SSUMvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Entropy](https://en.m.wikipedia.org/wiki/Entropy_(information_theory) measures uncertainty or information content of random variable, randomness (of a quantum state). For feature selection, information retrieval, and anomaly detection, & analysis of informational complexity of algorithms, mMeasuring amount of information that can be stored in quantum system, determining security of quantum communication protocols**\n",
        "* [Shannon entropy](https://en.m.wikipedia.org/wiki/Entropy_(information_theory)): Measure of randomness, disorder, average unpredictability, expected value of information, complexity of dataset - Shannon entropy is a probabilistic measure of average unpredictability in data (stochastic random process).\n",
        "* [Von Neumann entropy](https://en.m.wikipedia.org/wiki/Von_Neumann_entropy): most commonly entropy function. Is a measure of the mixedness of a quantum state. It is defined as the von Neumann entropy of the density matrix of the state. Defined as Shannon entropy of eigenvalues of density matrix of quantum state (=probabilities of finding the system in the corresponding eigenstates). - Entanglement = von neumann entropy?\n",
        "  * The von Neumann entropy can be used to characterize the amount of information present in a quantum state. For a pure state, the von Neumann entropy is zero, indicating that the state is completely known. For a mixed state, the von Neumann entropy is greater than zero, indicating that the state is not fully known.\n",
        "  * The von Neumann entropy can also be used to quantify the amount of entanglement between two quantum systems. Entanglement is a non-local correlation between two quantum systems, such that the state of one system cannot be fully described without knowing the state of the other system. The von Neumann entropy of entanglement is defined as the difference between the von Neumann entropy of the joint system and the sum of the von Neumann entropies of the individual systems. A higher von Neumann entropy of entanglement indicates a stronger entanglement between the two systems.\n",
        "  * Example: Consider two qubits, A and B, which are entangled in the Bell state: $|Bell_{00}> = \\frac{1}{\\sqrt{2}} (|00> + |11>)$. The von Neumann entropy of this state is zero, indicating that it is a pure state. However, if we measure the state of qubit A, we will collapse the state of qubit B to either |0> or |1>, depending on the outcome of the measurement. This means that the state of qubit B is not fully known until we measure qubit A.The von Neumann entropy of entanglement between qubit A and qubit B is 1 bit, indicating that they are maximally entangled. This means that the correlation between the two qubits is as strong as possible.\n",
        "\n",
        "* [Gibbs entropy](https://en.m.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula): the Gibbs entropy expression of the statistical entropy is a discretized version of Shannon entropy. The von Neumann entropy formula is an extension of the Gibbs entropy formula to the quantum mechanical case.\n",
        "* [Tsallis divergence (entropy)](https://en.m.wikipedia.org/wiki/Tsallis_entropy): Generalization of Shannon entropy, allows for wider range of values / different degrees of non-linearity.\n",
        "* [Renyi entropy](https://en.m.wikipedia.org/wiki/R%C3%A9nyi_entropy): Generalization of Shannon entropy. It is defined as $S_\\alpha(\\rho) = \\frac{1}{1-\\alpha} \\log \\left( \\sum_i p_i^\\alpha \\right)$, where $p_i$ are the eigenvalues of the density matrix $\\rho$ and $\\alpha$ is a real number.\n",
        "  * [(Max) Quantum Rényi Divergence](https://de.m.wikipedia.org/wiki/R%C3%A9nyi-Entropie): KL divergence in classical, would be quantum relativ entropy, but too hard to compute. Better: Maximal Quantum Rényi Divergence. arxiv.org/abs/2106.09567 and [video](https://www.youtube.com/watch?v=01xvtDu94jM&list=WL&index=4&t=352s)\n",
        "\n",
        "* [Quantum relative entropy](https://en.m.wikipedia.org/wiki/Quantum_relative_entropy): Despite there being many other useful ways of determining distances between quantum states, we only discuss one more, namely the quantum relative entropy.\n",
        "  * The relative entropy H (P|Q) can, in some sense, be thought of as a measure of how much P and Q “resemble” each other. 72. Indeed, it takes its maximum value (i.e. 0) if and only if P = Q; it may become −∞ if P and Q have disjoint support, (i.e. when P (y)Q (y) = 0 for all y ∈ Y .)\n",
        "  * The quantum relative entropy between two quantum states $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$ that satisfy $\\operatorname{supp}(\\rho) \\cap \\operatorname{ker}(\\sigma)=\\emptyset$ is defined to be $D(\\rho \\| \\sigma):=\\operatorname{tr}[\\rho \\log (\\rho)]-\\operatorname{tr}[\\rho \\log (\\sigma)]$. Here, the logarithm is taken with base 2 . We can rewrite the relative entropy using the von Neumann entropy $S(\\rho)$, which is defined as $S(\\rho):=-\\operatorname{tr}[\\rho \\log (\\rho)]$.\n",
        "  * With this, the relative entropy becomes $D(\\rho \\| \\sigma)=-\\operatorname{tr}[\\rho \\log (\\sigma)]-S(\\rho)$. If $\\operatorname{supp}(\\rho) \\cap \\operatorname{ker}(\\sigma) \\neq \\emptyset$, then we define $D(\\rho \\| \\sigma):=+\\infty$. A useful result in quantum information is the non-negativity of the relative entropy, i.e., that $D(\\rho \\| \\sigma) \\geq 0$ holds for any two quantum states $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right)$. Moreover, for $\\rho, \\sigma \\in \\mathcal{S}\\left(\\mathbb{C}^d\\right), D(\\rho \\| \\sigma)=0$ implies $\\rho=\\sigma$ (see, e.g., [17, Theorem 11.7] for a proof).\n",
        "  * **However, the quantum relative entropy is neither symmetric nor does it satisfy a triangle inequality, so it does not define a metric**. Nevertheless, it is an important tool for comparing two quantum states. For example, when comparing a bipartite state $\\rho_{A B} \\in \\mathcal{S}\\left(\\mathbb{C}^{d_A} \\otimes \\mathbb{C}^{d_B}\\right)$ to $\\rho_A \\otimes \\rho_B$, the tensor product of its reduced density matrices, we obtain a measure for the correlation between the $A$ - and the $B$-system in the state $\\rho_{A B}$. This defines the quantum mutual information $I(A: B)_\\rho:=D\\left(\\rho_{A B} \\| \\rho_A \\otimes \\rho_B\\right)$.\n",
        "\n",
        "\n",
        "* [Kolmogov complexity](https://en.m.wikipedia.org/wiki/Kolmogorov_complexity): Measure randomness, disorder, or complexity in dataset - from algorithmic information theory to measure of computational resources needed to reproduce a piece of data, string etc. (length of shortest possible description of string, not computable, but gives absolute complexity of string independently of any specific probability distribution)\n",
        "* [Conditional entropy](https://en.m.wikipedia.org/wiki/Conditional_entropy): measures uncertainty of quantum state given knowledge of another quantum state. It is defined as $S(A|B) = S(\\rho_{AB}) - S(\\rho_B)$, where $\\rho_{AB}$ is the joint density matrix of the two quantum states, $\\rho_B$ is density matrix of second quantum state, and $S(\\rho)$ is entropy of density matrix $\\rho$.\n",
        "* Relative entropy: measures distinguishability between two quantum states. It is defined as $D(\\rho \\| \\sigma) = Tr (\\rho \\log \\rho) - Tr (\\rho \\log \\sigma)$, where $\\rho$ and $\\sigma$ are two quantum states.\n",
        "* [Metric entropy](https://en.m.wikipedia.org/wiki/Measure-preserving_dynamical_system#Measure-theoretic_entropy) measure of complexity of metric space. It is defined as the logarithm of the minimum number of open balls of radius δ needed to cover the space. Space with high metric entropy is more difficult to compress than a space with low metric entropy. Additionally, metric entropy can be used to estimate rate of convergence of certain algorithms. See [here](https://mathoverflow.net/questions/307201/vc-dimension-fat-shattering-dimension-and-other-complexity-measures-of-a-clas) metric entropy named under model complexity metrics\n",
        "* [Mutual information](https://en.m.wikipedia.org/wiki/Mutual_information) measures amount of information that one random variable X shares about another random variable Y. This information can be used to reduce amount of information required to describe X.\n",
        "* [Holevo's theorem](https://en.m.wikipedia.org/wiki/Holevo%27s_theorem) (Holevo's bound) upper limit on amount of classical information that can be extracted from quantum system: single qubit can exist in superposition of states with infinite amount of information, when we measure that qubit, we can extract at most 1 bit of classical information. Clear distinction between information capacity of quantum states and accessible information via measurements.\n",
        "* [Cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions over same underlying set of events measures average number of bits needed to identify an event drawn from set if a coding scheme used for set is optimized for an estimated probability distribution rather than true distribution. Cross-entropy will calculate a score that summarizes average difference between actual and predicted probability distributions for all classes. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* [Binary entropy](https://en.m.wikipedia.org/wiki/Binary_entropy_function): is defined as the entropy of a Bernoulli process with probability p of one of two values"
      ],
      "metadata": {
        "id": "kGLHOLSRrQ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Metrics</font> (Fisher information, Trace distance, Minkowski, Manhattan)*"
      ],
      "metadata": {
        "id": "oKvbzZ8cxOS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics: generally used to define a distance between two points in a metric space. Häufig wird auch eine Metrik als [Distanzfunktion](https://de.m.wikipedia.org/wiki/Distanzfunktion). See [more](https://franknielsen.github.io/Divergence/index.html). Properties:**\n",
        "  1. Positive Definitheit (**positive definiteness**):\n",
        "    * $d(x, y) \\geq 0$ (**non-negativity**)\n",
        "    * $d(x, y)=0$ if and only if $x=y$ (Gleichheit gilt genau dann, wenn $x=y$, **identity of indiscernibles**) für alle $x, y \\in M$.\n",
        "  2. $d(x, y)=d(y, x)$ (**symmetry**)\n",
        "  3. $d(x, z) \\leq d(x, y)+d(y, z)$ (**Dreiecksungleichung / subadditivity / triangle inequality**) $\\forall x, y, z \\in M$\n",
        "  * Divergence fullfills property of positive definiteness (1). Distance (pseudometric) fullfills property of positive definiteness and symmetrie (1 + 2, but not 3 triangle inequality. Metrics and Norms fullfill property of positive definiteness, symmetrie and triangle inequality (1 + 2 + 3)."
      ],
      "metadata": {
        "id": "B8Sz3kzJn6VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Aus Normen erzeugte Metriken:*\n",
        "* [Minkowski metrik / distance](https://en.m.wikipedia.org/wiki/Minkowski_distance) (L<sup>p</sup> Distances): aus einer $p$ -Norm abgeleitet (but p cannot be less than 1, because otherwise the triangle inequality does not hold). Wichtige Spezialfälle sind: Manhattan, Euclidean, Chebyshev.\n",
        "* [Manhattan-Metrik](https://de.m.wikipedia.org/wiki/Manhattan-Metrik) zu $p=1$,\n",
        "* [Euklidische Metrik (Euclidean distance)](https://en.m.wikipedia.org/wiki/Euclidean_distance) zu $p=2$. The Euclidean distance is often used in classification problems because it is a natural measure of similarity between two vectors.\n",
        "* [Maximum-Metrik (Chebyshev distance)](https://en.m.wikipedia.org/wiki/Chebyshev_distance) zu $p=\\infty$\n",
        "* der eindimensionale Raum der reellen oder komplexen Zahlen mit dem absoluten Betrag als Norm (mit beliebigem $p$ ) und der dadurch gegebenen **Betragsmetrik** $d(x, y)=|x-y|$\n",
        "* [Mahalanobis distance](https://de.m.wikipedia.org/wiki/Mahalanobis-Abstand) (see under 'Distances') is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables.\n",
        "\n",
        "* [Fréchet-Metrik](https://de.m.wikipedia.org/wiki/Fréchet-Metrik) wird gelegentlich eine Metrik $d(x, y)=\\rho(x-y)$ bezeichnet, die von einer Funktion $\\rho$ induziert wird, welche die meisten Eigenschaften einer Norm besitzt, aber nicht homogen ist. Sie stellt eine Verbindung zwischen Metrik und Norm her.\n",
        "* [Cayley–Klein metric](https://en.m.wikipedia.org/wiki/Cayley%E2%80%93Klein_metric) is used in a variety of mathematical applications, including geometry, physics, and computer science. For example, it is used to define the distance between two points on the Poincaré disk, which is a model of the hyperbolic plane. It is an example of [Left- / right-/ bi-invariant Riemann metric](https://ncatlab.org/nlab/show/invariant+metric). Used in [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004)."
      ],
      "metadata": {
        "id": "IJBkzU6eFx1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Nicht aus Normen erzeugte Metriken:*\n",
        "* [Riemannsche Mannigfaltigkeit (Metrik)](https://en.m.wikipedia.org/wiki/Riemannian_manifold), zB  Die kürzesten Strecken zwischen unterschiedlichen Punkten (die sogenannten Geodäten) sind nicht zwingend Geradenstücke, sondern können gekrümmte Kurven sein. Die Winkelsumme von Dreiecken kann, im Gegensatz zur Ebene, auch größer (z. B. Kugel) oder kleiner (hyperbolische Räume) als 180° sein.\n",
        "* [Hausdorff-Metrik](https://de.m.wikipedia.org/wiki/Hausdorff-Metrik) misst den **Abstand zwischen Teilmengen, nicht Elementen, eines metrischen Raums**; man könnte sie als Metrik zweiten Grades bezeichnen, denn sie greift auf eine Metrik ersten Grades zwischen den Elementen des metrischen Raums zurück.\n",
        "* [Fisher information metric](https://en.m.wikipedia.org/wiki/Fisher_information_metric):\n",
        "  * also: Fisher-Rao norm: as a common starting point for many measures of complexity currently studied in the literature (see work of Srebro’s group and Bartlett et al). https://www.mit.edu/~rakhlin/papers/myths.pdf, Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes (2017) Fisher-Rao Metric, Geometry, and Complexity of Neural Networks, https://arxiv.org/abs/1711.01530\n",
        "  * Fisher-Rao norm (or Fisher Information Metric) measures distance in the space of probability distributions, and it is defined in the context of information geometry. It is not a matrix norm per se\n",
        "  * The Fisher information metric is not a norm-induced metric. A norm-induced metric is a metric that can be defined from a norm on a vector space. The Fisher information metric is defined on a statistical manifold, which is a more general object than a vector space.\n",
        "  * To see why the Fisher information metric is not a norm-induced metric, consider the following. A norm-induced metric satisfies the triangle inequality, which states that the distance between two points is less than or equal to the sum of the distances between each point and a third point. However, the Fisher information metric does not satisfy the triangle inequality in general.\n",
        "  * For example, consider the statistical manifold of all Gaussian distributions with mean zero and variance one. The Fisher information metric on this manifold is given by the metric tensor $I^{-1}$, where $I$ is the Fisher information matrix. The Fisher information matrix for a Gaussian distribution with mean zero and variance one is given by $I = \\frac{1}{\\sigma^2}$, where $\\sigma$ is the standard deviation.\n",
        "  * Now, consider the following three points on this statistical manifold:\n",
        "    * $p_1$: The Gaussian distribution with mean zero and variance one.\n",
        "    * $p_2$: The Gaussian distribution with mean zero and variance two.\n",
        "    * $p_3$: The Gaussian distribution with mean zero and variance three.\n",
        "  * The Fisher information metric distances between these points are given by:\n",
        "    * $d(p_1, p_2) = \\sqrt{2}$\n",
        "    * $d(p_2, p_3) = \\sqrt{2}$\n",
        "    * $d(p_1, p_3) = \\sqrt{6}$\n",
        "  * We can see that the triangle inequality is not satisfied, since $\\sqrt{2} + \\sqrt{2} < \\sqrt{6}$. Therefore, the Fisher information metric is not a norm-induced metric.\n",
        "  * Although the Fisher information metric is not a norm-induced metric, it is still a useful metric for measuring the distance between points on a statistical manifold. It is often used in machine learning and statistics to quantify the amount of information that a set of data contains about a parameter.\n",
        "  * **The topic information geometry uses this to connect [Fisher information](https://en.m.wikipedia.org/wiki/Fisher_information) to differential geometry, and in that context, this metric is known as the Fisher information metric.**\n",
        "  * Fisher information is expected value of second derivative of log likelihood function. Fisher information measures how much the log likelihood function changes when the parameters of the distribution are changed (a measure of the sensitivity of the log likelihood function to changes in the parameters. The larger the Fisher information, the more sensitive the log likelihood function is to changes in the parameters).\n",
        "  * The Quantum Fisher Information Matrix (QFIM) is an extension of the concept of Fisher Information Matrix from classical statistics to quantum mechanics. It measures the amount of information that a quantum state carries about an unknown parameter. This parameter might be related to some aspect of the quantum state or a transformation applied to it.\n",
        "  * The Fisher Information Matrix (FIM) is a matrix whose entries are the expected values of the squared derivatives of the log-likelihood with respect to the parameters. The inverse of the FIM gives the Cramér-Rao Bound, which provides a lower limit on the covariance of any unbiased estimator. In other words, **the CRB (derived from the FIM) tells us the smallest possible variance (or covariance in the multivariate case) that we can expect from an unbiased estimator.**\n",
        "  * **Fisher Information is a measure of how much information a random variable provides about an unknown parameter**. The Fisher information is defined as the expected value of the squared derivative of the log-likelihood function with respect to the parameter.\n",
        "  * The Fisher information can be used to construct confidence intervals and hypothesis tests for the parameter. It can also be used to derive the asymptotic properties of maximum likelihood estimators.\n",
        "  * The Fisher information is a non-negative quantity. A random variable that provides no information about the parameter has a Fisher information of 0.\n",
        "  * Measure of redundancy. How much of model is active vs inactive.\n",
        "  * Fisher information: Sensitivity of my parameters to my model space --> measure of model capacity (?), see video from amira qhack 2022\n",
        "  * Fisher information is the expected value of the second derivative of the log likelihood function\n",
        "  * Given a parameterized quantum state ρ(θ), where θ is the parameter vector we are interested in, the QFIM is defined in terms of the symmetric logarithmic derivative (SLD), which is a Hermitian operator L(θ) that satisfies a particular equation related to the derivative of the state ρ(θ).\n",
        "  * The entries of the QFIM, denoted as H(θ), are given by:\n",
        "  * $H_ij(θ) = \\frac{1}{2} Tr [ρ(θ) {L_i(θ), L_j(θ)}]$\n",
        "  * where $L_i$ and $L_j$ are the SLDs corresponding to the parameters $θ_i$ and $θ_j$, respectively, and { , } denotes the anticommutator.\n",
        "  * Like the classical Fisher Information Matrix, the QFIM can be used to define a lower bound on the variance of an unbiased estimator for the parameters θ. This is the Quantum Cramér-Rao Bound.\n",
        "  * In practical terms, the QFIM and Quantum Cramér-Rao Bound are used in quantum metrology to quantify the ultimate limit to precision that can be achieved in estimating parameters, such as phase shifts or magnetic fields, based on quantum mechanical measurements.\n",
        "  * **Yes, the Fisher-Rao norm, or Fisher Information Metric**, is indeed a metric in the sense that it defines a distance function on the space of probability distributions. Specifically, it provides a way to measure the distance between different probability distributions in a manner that takes into account their geometrical structure.\n",
        "  * Mathematically, for a parametric family of probability distributions $\\( P_\\theta \\)$, the Fisher Information Metric $\\( g_{\\theta \\theta'} \\)$ is defined by taking the expectation of the outer product of the score (the gradient of the log-likelihood) with respect to the parameters $\\( \\theta \\)$ and $\\( \\theta' \\)$.\n",
        "  * $\\[ g_{\\theta \\theta'}(\\theta) = \\mathbb{E}\\left[ \\left( \\frac{\\partial \\log P_\\theta(X)}{\\partial \\theta} \\right) \\left( \\frac{\\partial \\log P_\\theta(X)}{\\partial \\theta'} \\right) \\right] \\]$\n",
        "  * Where the expectation is taken with respect to the distribution $\\( P_\\theta(X) \\)$.\n",
        "  * This matrix gives a Riemannian metric on the parameter space, defining a geometry that reflects how changes in parameters change the distributions. Consequently, it provides a natural distance measure between distributions (or models) that are nearby in parameter space.\n",
        "  * However, it's worth noting that while it's a \"metric\" in a geometrical sense, the Fisher Information Metric doesn’t satisfy all properties of a metric in the strict mathematical sense (like the triangle inequality). It does not measure \"distance\" in the conventional sense but quantifies the similarity or dissimilarity between statistical models or distributions based on their parametrizations. This concept and its implications are explored in the field of information geometry.\n",
        "* *what is a degenerate Fisher information matrix?*\n",
        "  * A degenerate Fisher information matrix is a Fisher information matrix that is not full rank. This means that there is at least one direction in which the Fisher information is zero. This can happen for a number of reasons, such as:\n",
        "    * When the model is overparameterized, meaning that there are more parameters than necessary to describe the data.\n",
        "    * When the parameters are not identifiable, meaning that there are multiple sets of parameters that can produce the same data.\n",
        "    * When the data is not informative enough to estimate all of the parameters.\n",
        "  * When the Fisher information matrix is degenerate, it means that there is some uncertainty in the estimation of the parameters that cannot be reduced by increasing the sample size. This is because the Fisher information matrix is a measure of the amount of information that the data contains about the parameters.\n",
        "  * Degenerate Fisher information matrices can be a problem in statistical inference, as they can lead to biased and inefficient estimates of the parameters. However, there are a number of ways to deal with degenerate Fisher information matrices, such as:\n",
        "    * Using regularization techniques to reduce the number of parameters in the model.\n",
        "    * Using constraints on the parameters to make them identifiable.\n",
        "    * Using Bayesian methods to estimate the parameters.\n",
        "  * It is important to note that a degenerate Fisher information matrix does not necessarily mean that the model is wrong. It is possible for a model to be correct even if the Fisher information matrix is degenerate. However, it is important to be aware of the potential problems that can arise from degenerate Fisher information matrices when interpreting the results of statistical analyses.\n",
        "  * Here are some examples of models that can have degenerate Fisher information matrices:\n",
        "    * A linear regression model with more predictors than observations.\n",
        "    * A logistic regression model with a constant predictor.\n",
        "    * A time series model with a seasonal component that is not identified.\n",
        "* [Trace distance](https://en.m.wikipedia.org/wiki/Trace_distance) is a **metric** on the space of density matrices and gives a measure of the distinguishability between two states. In quantum mechanics, the trace distance is used to measure the distinguishability between two quantum states. It is the quantum generalization of the Kolmogorov distance for classical probability distributions.\n",
        "  * The trace distance is defined as half of the [trace norm](https://en.m.wikipedia.org/wiki/Matrix_norm#Schatten_norms) of the difference of the matrices ${\\displaystyle T(\\rho ,\\sigma ):={\\frac {1}{2}}\\|\\rho -\\sigma \\|_{1}={\\frac {1}{2}}\\mathrm {Tr} \\left[{\\sqrt {(\\rho -\\sigma )^{\\dagger }(\\rho -\\sigma )}}\\right],}$\n",
        "  * Trace distance is a measure of the distinguishability between two quantum states. It is defined as half of the L1 norm of the difference between the density matrices of the two states:\n",
        "  * $δ(ρ, σ) = 1/2 ||ρ - σ||_1$\n",
        "  * where ||ρ - σ||_1 is the trace norm of ρ - σ.\n",
        "  * Trace distance is a metric on the space of density matrices, meaning that it satisfies the following properties:\n",
        "    * δ(ρ, σ) ≥ 0 for all ρ, σ\n",
        "    * δ(ρ, σ) = 0 if and only if ρ = σ\n",
        "    * δ(ρ, σ) = δ(σ, ρ)\n",
        "    * δ(ρ, τ) ≤ δ(ρ, σ) + δ(σ, τ) for all ρ, σ, τ\n",
        "  * The trace distance is a more general measure of distinguishability than the trace inequality. The trace inequality only states that the trace distance between two quantum states is at least as large as the sum of the trace distances between each state and a third state. The trace distance, on the other hand, can be used to measure the distinguishability between any two quantum states, regardless of whether there is a third state involved.\n",
        "  * Here is an example of how the trace distance can be used to measure the distinguishability between two quantum states: Suppose we have two quantum states ρ and σ that are represented by the following density matrices:\n",
        "    * $ρ = |0 > < 0| + | 1 > < 1|$\n",
        "    * $σ = | 0 > < 0| + 0.5 | 1 > <1|$\n",
        "  * The trace distance between ρ and σ can be calculated as follows:\n",
        "    * δ(ρ, σ) = 1/2 ||ρ - σ||_1\n",
        "    * = 1/2 ||(|0><0| + |1><1|) - (|0><0| + 0.5 |1><1|)||_1\n",
        "    * = 1/2 ||0.5 |1><1|)||_1\n",
        "    * = 1/2 * 0.5\n",
        "    * = 1/4\n",
        "  * This means that the two states ρ and σ are indistinguishable with a probability of 1/4.\n",
        "  * The trace distance is a powerful tool for analyzing quantum information processing protocols and for studying the effects of noise and decoherence on quantum systems. It is also used in quantum machine learning and quantum algorithms.\n",
        "  * the trace distance is a metric because:\n",
        "    * Non-negativity: The distance between two points is always non-negative.\n",
        "    * Identity: The distance between two identical points is zero.\n",
        "    * Symmetry: The distance between two points is the same as the distance between the points in the opposite order.\n",
        "    * Triangle inequality: The distance between two points is less than or equal to the sum of the distances between each point and a third point.\n",
        "* [Fubini–Study metric](https://en.m.wikipedia.org/wiki/Fubini–Study_metric): When extended to complex projective Hilbert space, the Fisher information metric becomes the Fubini–Study metric\n",
        "* [Bures metric](https://en.m.wikipedia.org/wiki/Bures_metric): when written in terms of mixed states, the Fisher information mettic is the quantum Bures metric. The Bures metric can be seen as the quantum equivalent of the Fisher information metric\n",
        "* [Französische Eisenbahnmetrik](https://de.m.wikipedia.org/wiki/Französische_Eisenbahnmetrik).\n",
        "* [Hamming-Abstand](https://de.m.wikipedia.org/wiki/Hamming-Abstand) Code space metric: gibt Unterschiedlichkeit von (gleich langen) Zeichenketten an (number of items that are different between two subsets).\n",
        "* [Levenshetin Distance](https://de.m.wikipedia.org/wiki/Levenshtein-Distanz) Extension of Hamming-Abstands. Die Levenshtein-Distanz kann als Sonderform der [Dynamic Time Warpening](https://de.m.wikipedia.org/wiki/Dynamic-Time-Warping) (DTW) betrachtet werden. Siehe auch [Lee distance](https://en.m.wikipedia.org/wiki/Lee_distance), [Jaro–Winkler distance](https://en.m.wikipedia.org/wiki/Jaro–Winkler_distance) & [Edit Distance](https://en.m.wikipedia.org/wiki/Edit_distance).\n",
        "* More [nicht aus Normen erzeugten Metriken](https://de.m.wikipedia.org/wiki/Metrischer_Raum#Nicht_durch_Normen_erzeugte_Metriken)"
      ],
      "metadata": {
        "id": "3HV9y-rvrhKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Measure</font> (Haar, Dirac, Radon, Hausdorff, Lebesgue)*"
      ],
      "metadata": {
        "id": "eLjOkdyQxLeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measures**\n",
        "\n",
        "A measure provides a description for how things are distributed in a mathematical set or space. From Video: [The Haar Measure | PennyLane Tutorial](https://www.youtube.com/watch?v=d4tdGeqcEZs&list=PLzgi0kRtN5sO8dkomgshjSGDabnjtjBiA&index=3)\n",
        "\n",
        "[Measure](https://de.m.wikipedia.org/wiki/Ma%C3%9F_(Mathematik)): Funktion, die Teilmengen einer Grundmenge Zahlen zuordnet, die als „Maß“ für die Größe dieser Mengen interpretiert werden können. In Stochastik werden Wahrscheinlichkeitsmaße verwendet, um zufälligen Ereignissen, die als Teilmengen eines Ergebnisraums aufgefasst werden, Wahrscheinlichkeiten zuzuordnen.\n",
        "\n",
        "  * See also [Geometric measure theory](https://en.m.wikipedia.org/wiki/Geometric_measure_theory) and [outer measure](https://en.m.wikipedia.org/wiki/Outer_measure).\n",
        "\n",
        "* [Lebesgue measure](https://en.m.wikipedia.org/wiki/Lebesgue_measure): is the standard way of assigning a measure to subsets of n-dimensional Euclidean space. For n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume. In general, it is also called n-dimensional volume, n-volume, or simply volume.\n",
        "\n",
        "  * [Hausdorff measure](https://en.m.wikipedia.org/wiki/Hausdorff_measure) is a generalization of the traditional notions of area and volume to non-integer dimensions, specifically fractals and their Hausdorff dimensions. It is a type of outer measure that assigns number in [0,∞] to each set in $\\mathbb {R} ^{n}$ or, more generally, in any metric space.\n",
        "\n",
        "  * [Jordan measure](https://en.m.wikipedia.org/wiki/Jordan_measure) is an extension of the notion of size (length, area, volume) to shapes more complicated than, for example, a triangle, disk, or parallelepiped.\n",
        "\n",
        "* [Radon measure](https://en.m.wikipedia.org/wiki/Radon_measure)\n",
        "\n",
        "* [Wiener measure](https://en.wikipedia.org/wiki/Wiener_process) zur Beschreibung des Wiener-Prozesses (Brownsche Bewegung). It is the probability law on the space of continuous functions g, with g(0) = 0, induced by the Wiener process.\n",
        "\n",
        "* [Random measure](https://en.m.wikipedia.org/wiki/Random_measure) is a measure-valued random element, for example used in the theory of random processes, where they form many important point processes such as Poisson point processes and Cox processes.\n",
        "\n",
        "* [Vector measure](https://en.m.wikipedia.org/wiki/Vector_measure) eine Verallgemeinerung des Maßbegriffes dar: Das Maß ist nicht mehr reellwertig, sondern vektorwertig. Vektormaße werden unter anderem in der Funktionalanalysis benutzt (Spektralmaß).\n",
        "\n",
        "  * [Projection-valued measure (Spektralmaß)](https://en.m.wikipedia.org/wiki/Projection-valued_measure) ist eine Abbildung, die gewissen Teilmengen einer fest gewählten Menge orthogonale Projektionen eines Hilbertraums zuordnet. Spektralmaße werden verwendet, um Ergebnisse in der Spektraltheorie linearer Operatoren zu formulieren, wie z. B. den Spektralsatz für normale Operatoren.\n",
        "\n",
        "  * [Complex measure](https://en.m.wikipedia.org/wiki/Complex_measure)\n",
        "\n",
        "  * [Signed measure](https://en.m.wikipedia.org/wiki/Signed_measure)\n",
        "\n",
        "* [Moment measure](https://en.m.wikipedia.org/wiki/Moment_measure)\n",
        "\n",
        "* [Dirac measure](https://en.m.wikipedia.org/wiki/Dirac_measure) assigns a size to a set based solely on whether it contains a fixed element x or not. It is one way of formalizing the idea of the Dirac delta function.\n",
        "\n",
        "* [Discrete measure](https://en.m.wikipedia.org/wiki/Discrete_measure) is similar to the Dirac measure, except that it is concentrated at countably many points instead of a single point. More formally, a measure on the real line is called a discrete measure (in respect to the Lebesgue measure) if its support is at most a countable set.\n",
        "\n",
        "* [Poisson random measure](https://en.m.wikipedia.org/wiki/Poisson_random_measure) and [Poisson-type random measure](https://en.m.wikipedia.org/wiki/Poisson-type_random_measure)\n",
        "\n",
        "* [Haar measure](https://en.m.wikipedia.org/wiki/Haar_measure) assigns invariant volume (left- and right-invariant = (bi-)invariant measure). Generalization of Lebesgue measure. Sample unitary matrix uniformly at random according to Haar measure, and then apply it to another unitary matrix, the resulting unitary matrix will also be sampled uniformly at random according to Haar measure.\n",
        "  \n",
        "  * **Define expectation value of an observable = average value of the observable** over all possible quantum states. e.g. in QML: define <u>**average complexity**</u> by sampling a random unitary matrix (who represent quantum operations, like gates and channels) from Haar measure (= unitary matrix that is chosen with equal probability from all possible unitary matrices), **and then measuring sample complexity of algorithm on the resulting quantum state**. Average complexity is expected value of sample complexity over all possible unitary matrices. (*Train QML on quantum data: sampling large number of random unitary operations and applying them to data. Haar measure ensures that all possible unitary operations have an equal chance of being sampled.*)\n",
        "\n",
        "  * VC dimension, Rademacher complexity and Kullback Leibler divergence can all be used to measure the <u>**worst-case complexity**</u> (sample complexity of algorithm on worst possible quantum state).\n",
        "\n",
        "  * Sample unitary matrices according to Haar measure: Metropolis-Hastings algorithm iteratively proposes new unitary matrix and accepts or rejects proposal based on probability distribution (chosen that it converges to sampling from Haar measure)\n",
        "\n",
        "* Gibbs measure\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wUzfIQ8Ddoxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Set Complexity</font> (Covering number, Maximal packing nets)*"
      ],
      "metadata": {
        "id": "V36uKfvAxRCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Complexity**\n",
        "\n",
        "* [Covering problems](https://en.m.wikipedia.org/wiki/Covering_problems)\n",
        "\n",
        "* [Covering number](https://en.m.wikipedia.org/wiki/Covering_number): smallest number of sets that can be used to cover a given set with a certain tolerance.\n",
        "  * Covering number of a set with respect to a metric quantifies how many balls of a specified radius are needed to cover the set. In learning theory, covering numbers can be used to analyze the complexity of function classes in certain metric spaces. [Covering number](https://en.m.wikipedia.org/wiki/Covering_number): helps in understanding the capacity of a hypothesis class, which can be crucial in understanding generalization errors and the learnability of a class. Specifically, it can provide bounds on the number of samples required to learn a target function to a desired accuracy. Covering numbers are from [Combinatorial (discrete) geometry](https://en.m.wikipedia.org/wiki/Discrete_geometry), like [Kissing number](https://en.m.wikipedia.org/wiki/Kissing_number) (Newton number or Contact number) and [Polygon number](https://en.m.wikipedia.org/wiki/Polygon_covering).\n",
        "  * Measure of the complexity of a set: Bound sample complexity of learning algorithms (meanwhile Lebesgue covering dimension is used to bound generalization error of learning algorithms. eg Covering number tells how many sets we need to cover unit circle, while Lebesgue covering dimension tells how many dimensions we need to represent unit circle). Covering number is more directly related to sample complexity and generalization error of learning algorithms.\n",
        "  * How can one calculate the covering number bounds for the space of pure output states of polynomial-size quantum circuits? **Lower bound** on covering number is number of pure states that can be represented by a polynomial-size quantum circuit: given by $2^{n_q}$, where $n_q$ is number of qubits in circuit. Lower bound is number of states that a polynomial-size quantum circuit can represent, and upper bound is number of states that any quantum circuit can represent.\n",
        "\n",
        "  * Covering numbers are a measure of the complexity of a set (in sample complexity). A set with a small covering number is said to be easy to cover, while a set with a large covering number is said to be difficult to cover\n",
        "\n",
        "  * Example: *we provide bounds on the expressivity of the class of CPTP maps (or unitaries) that a quantum machine learning model (QMLM) can implement in terms of the number of trainable elements used in the architecture. As a measure of expressivity, we choose covering numbers and metric entropies w.r.t. (the metric induced by) the diamond norm.* (from: Generalization in quantum machine learning from few training data)\n",
        "\n",
        "  * [The Most Important Concept in Topology and Analysis | Compactness](https://youtu.be/td7Nz9ATyWY)\n",
        "\n",
        "* [Kissing number](https://en.m.wikipedia.org/wiki/Kissing_number)\n",
        "\n",
        "* [Polygon covering](https://en.m.wikipedia.org/wiki/Polygon_covering)\n",
        "\n",
        "* [Spherical code](https://en.m.wikipedia.org/wiki/Spherical_code)\n",
        "\n",
        "* [Equilateral dimension](https://en.m.wikipedia.org/wiki/Equilateral_dimension)\n",
        "\n",
        "* [Packing problems](https://en.m.wikipedia.org/wiki/Packing_problems)\n",
        "\n",
        "  * **Maximal packing nets** - used in \"Information-theoretic bounds on quantum advantage in machine learning\". See (https://en.m.wikipedia.org/wiki/Close-packing_of_equal_spheres) and (https://en.m.wikipedia.org/wiki/Packing_density)\n",
        "  * \"measures the cardinality of a maximal packing net that depends on the input distribution\""
      ],
      "metadata": {
        "id": "w3xsatVa9aZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Divergence</font> (Kullback Leibler, Jensen-Shannon, Hellinger, Bregman, f, Bhattacharyya)*"
      ],
      "metadata": {
        "id": "hiezBxLBxVLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Divergence](https://en.m.wikipedia.org/wiki/Divergence_(statistics)) is a (contrast) function which measure the difference between two probability distributions (statistical manifolds). Is a weaker notion than that of the distance (not symmetric, no triangle inequality).**\n",
        "\n",
        "* [Kullback Leibler](https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence) (relative entropy): measure of how one probability distribution (not only Gaussian) differs from a baseline distribution or amount of information lost when one distribution is converted to another. Equivalent to multi-class cross-entropy in multi-class classification, but used to approximate more complex function, e.g. autoencoder for learning a dense feature representation. Minimizing KL divergence (+ squared Euclidean distance) is main way to solve linear inverse problem, via principle of max entropy & least squares (logistic + linear regression). Used in clustering, feature selection (find features that minimize KL divergence between model's distribution and  true distribution when features are excluded), anomaly detection, GANs (similarity between real and generated data). Fisher information is expected value of second derivative of log likelihood function. KL divergence is quadratic form whose coefficients are given by elements of Fisher information matrix. FIM defines local curvature of KL divergence. Fisher information can be used to estimate the KL divergence between two distributions.\n",
        "  \n",
        "\n",
        "* [Jensen-Shannon divergence](https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence): smoothed version of KLd = symmetrization of KL divergence and with finite values. Symmetric means that does not matter which distribution is source and which is target. More robust to noise than other measures of similarity like KLd. JSD is also non-parametric: does not make any assumptions about underlying distribution of data. Square root of Jensen–Shannon divergence is Jensen-Shannon distance. Used in dimensionality reduction, GAN, clustering, anomaly detection, novelty detection.\n",
        "\n",
        "* [Hellinger distance](https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to Bhattacharyya) quantifies similarity between two probability distributions. Type of f-divergence. Hellinger distance is good choice when goal is to measure difference between the cumulative distribution functions of two distributions.\n",
        "\n",
        "* [f-Divergence](https://en.m.wikipedia.org/wiki/F-divergence): Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific f-divergence between model and data distribution.\n",
        "\n",
        "* [Bregman divergences](https://en.m.wikipedia.org/wiki/Bregman_divergence): calculate bi-tempered logistic loss, performing better than softmax function with noisy datasets. The Mahalanobis distance is an example of Bregman, and Squared (Euclidean) distance is a special case for Bregman.\n",
        "  \n",
        "* [Squared Euclidean distance](https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance): special case of Bregman (corresponding to function x<sup>2</sup>) for certain choice of generating function when generating convex function is quadratic - represents measure of divergence between two points in space. The cost function for K-means clustering is sum of squared distances between data points and cluster centroids. Squared distances are more sensitive to large errors than absolute distances.\n",
        "\n",
        "* [Bhattacharyya distance](https://en.m.wikipedia.org/wiki/Bhattacharyya_distance): determine relative closeness of two samples. Measure separability of classes in classification and more reliable than Mahalanobis when standard deviations of classes are same. When two classes have similar means but different standard deviations, Mahalanobis would tend to zero, whereas Bhattacharyya grows depending on difference between standard deviations.\n",
        "\n",
        "* Maximal Quantum Rényi Divergence: see under entropy"
      ],
      "metadata": {
        "id": "CdhvTHoqrUCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Distances</font> (Wasserstein, Mahalanobis, Manhattan)*"
      ],
      "metadata": {
        "id": "IJCykPfwxdQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Statistical_distance\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Total_variation_distance_of_probability_measures\n",
        "\n",
        "from: https://en.m.wikipedia.org/wiki/Trace_distance"
      ],
      "metadata": {
        "id": "KAgLEAgDJYCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Distances](https://en.m.wikipedia.org/wiki/Distance), especially [Statistical distances](https://en.m.wikipedia.org/wiki/Statistical_distance): quantify separation between two elements. Can be defined on a wider variety of spaces than metrics and are often more interpretable than metrics. Distances can be more flexible than metrics: can be defined on a wider variety of spaces when data is not Euclidean (no natural notion of distance like in text, images, or graphs), data is noisy (more robust), or data is imbalanced. Is a weaker notion than that of the metric (there is no triangle inequality).**\n",
        "\n",
        "* [Manhattan distance (taxicab)](https://en.m.wikipedia.org/wiki/Taxicab_geometry): defined as sum of absolute values of the differences between the corresponding components of two vectors. The Manhattan distance is not symmetric, which means that the distance between two points is not the same as the distance between the reverse of the two points.\n",
        "* [Chebyshev distance](https://en.m.wikipedia.org/wiki/Chebyshev_distance): maximum absolute difference between corresponding components of two vectors. The Chebyshev distance is not symmetric, and it does not satisfy the triangle inequality.\n",
        "* [Jaccard distance](https://en.m.wikipedia.org/wiki/Jaccard_index): size of intersection of two sets divided by size of the union of the two sets. The Jaccard distance is not a metric because it does not satisfy the triangle inequality. The Jaccard distance is used for text classification tasks. It is defined as the size of the intersection of two sets divided by the size of the union of the two sets.\n",
        "* [Left- / right-/ bi-invariant Riemann metric](https://ncatlab.org/nlab/show/invariant+metric). Used in [The geometry of quantum computation](https://arxiv.org/abs/quant-ph/0701004). Example in discrete space: [Kendall tau distance](https://en.m.wikipedia.org/wiki/Kendall_tau_distance): is a measure of the similarity between two rankings of a set of objects. It is defined as the number of pairs of objects that are ranked in opposite order in the two rankings, divided by the total number of pairs of objects. The Kendall tau distance is not a metric because it does not satisfy the triangle inequality.\n",
        "* Lp-norm defined as the pth root of sum of the pth powers of the differences between the corresponding components of two vectors. The Lp norm is not a metric for p < 1.\n",
        "* [Wasserstein distance](https://en.m.wikipedia.org/wiki/Wasserstein_metric) (Earth Mover’s Distance): comparing probability distributions. The Wasserstein distance is not a metric for comparing vectors. Depending on the order parameter \\( p \\), it might not satisfy the triangle inequality property. When \\( p = 1 \\), it is a metric, but for other values of \\( p \\), it might not be. Is used for image segmentation tasks. It is defined as the minimum amount of work required to move one set of pixels to another set of pixels.  The Wasserstein distance is used where the data is represented as probability distributions. It is defined as the minimum cost of transporting one distribution to another.\n",
        "* Squared Euclidean distance is also a distance measure in a broader sense, as it quantifies the separation between two points in a Euclidean space. Squared Euclidean distance is also a divergence, specifically a special case of a Bregman divergence for a certain choice of generating function when the generating convex function is a quadratic function -  In this context, it represents a measure of divergence between two points in the space.\n",
        "* [Bhattacharyya distance](https://en.m.wikipedia.org/wiki/Bhattacharyya_distance)\n",
        "* [Mahalanobis distance](https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D, used in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set. In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance. The Mahalanobis distance does not satisfy the triangle inequality because it can be zero even if the two points are not the same. This can happen if the covariance matrix is singular. It is often used in multivariate anomaly detection, classification, and regression. [Mahalanobis distance](https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables.\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n"
      ],
      "metadata": {
        "id": "OTsj83x7GbyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Topological Invariants</font> (Homology Groups, Betti numbers, Characteristic classes)*"
      ],
      "metadata": {
        "id": "zXCXyX9DxfdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploiting topological properties of the loss landscape of symmetry-machine-learning (in the data and problem) as a preprocessing step to reduce Barren plateaus - How do topological properties of the loss landscape vary in a QML algorithm that learns symmetries of data and problems\n",
        "https://pennylane.ai/qml/demos/tutorial_geometric_qml/"
      ],
      "metadata": {
        "id": "w_Y-pFySypvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topological Invariants** (Distanz- und Ähnlichkeitsmaß um topologische Räumen / Objekte zu differenzieren)\n",
        "* **What is it used for?**: Similarity metrics for topology: A topological invariant is a property or characteristic of a space that remains unchanged under homeomorphisms, which are continuous transformations of the space that can be continuously undone. Topological invariants are used in:\n",
        "  * **Mathematics:** study properties of topological spaces and manifolds (Euler characteristic of surface to distinguish between different surfaces, such as spheres and tori)\n",
        "  * **Physics:** study the properties of quantum systems (Chern class of band insulator to determine whether the insulator is topological or not. also classify different types of insulators and superconductors)\n",
        "  * **Chemistry:** study properties of molecules and materials (winding number of a molecule to determine whether molecule is chiral or not. In materials science, to design new materials with desired properties, such as high strength and conductivity, or new drugs and catalysts)\n",
        "  * **Computer science:** study properties of algorithms and data structures ( genus of a graph to determine how difficult it is to color the graph)\n",
        "  * **Machine learning:** develop new machine learning algorithms more robust to noise and perturbations(TDA to extract features from data that are invariant to certain transformations)\n",
        "\n",
        "* **Topologische Räume:** Das einfachste Beispiel eines topologischen Raumes ist die Menge der reellen Zahlen. Dabei ist die Topologie, also das System der offenen Teilmengen so erklärt, dass wir eine Menge  Ω  C  R  offen nennen, wenn sie sich als Vereinigung von offenen Intervallen darstellen lässt. * [Separation Axioms](https://en.m.wikipedia.org/wiki/History_of_the_separation_axioms): Topologische Räume können klassifiziert werden nach Kolmogorov:\n",
        "  * Frechet Räume: sind Vektorräume von glatten Funktionen (unendlich oft differenzierbar, stetig). Diese Räume lassen sich zwar mit verschiedenen Normen ausstatten, sind aber bezüglich keiner Norm vollständig, also keine Banachräume. Man kann auf ihnen aber eine Topologie definieren, sodass viele Sätze, die in Banachräumen gelten, ihre Gültigkeit behalten.\n",
        "  * Uniforme Räume: erlauben es zwar nicht Abstände einzuführen, aber Begriffe wie gleichmäßige Stetigkeit, Cauchy-Folgen, Vollständigkeit und Vervollständigung zu definieren\n",
        "\n",
        "* **Basic topological invariants** (more \"Geometric\" or foundational / fundamental than the more algebraic invariants mentioned after)\n",
        "  * Connectedness: A space is connected if it cannot be divided into two disjoint non-empty open sets. This property is preserved under continuous maps, making it a topological invariant. There are several related notions as well, such as path-connectedness (there is a path connecting any two points in the space).\n",
        "  * Compactness: A space is compact if every open cover has a finite subcover. Like connectedness, this property is preserved under continuous functions, marking it as a topological invariant.\n",
        "\n",
        "* **Homotopy Invariants**\n",
        "These invariants are primarily concerned with the study of continuous deformations of topological spaces. Allows for classification of spaces based on their loop structures and offering a way to distinguish between different topological spaces\n",
        "  * **Fundamental Group (π₁)**, including the [fundamental group](https://en.m.wikipedia.org/wiki/Fundamental_group) (also known as the first homotopy group). It gives information about the loops in the space, essentially characterizing how one can loop around one point and come back to the same point.\n",
        "  * **Higher Homotopy Groups (π₂, π₃, ...)**: These groups generalize the concept of the fundamental group to higher dimensions, providing information about surfaces and higher-dimensional \"holes\" in a space.\n",
        "\n",
        "* **Homology and Cohomology**\n",
        "These invariants are associated with algebraic structures that help quantify various aspects of topological spaces.\n",
        "  * **Homology Groups** (including singular homology, simplicial homology, etc.)\n",
        "  * **Cohomology Groups** (including singular cohomology, sheaf cohomology, etc.)\n",
        "  * **Euler Characteristic** (can also be derived from homology)\n",
        "  * **Betti Numbers** (related to homology groups)\n",
        "\n",
        "* **[Characteristic class](https://en.m.wikipedia.org/wiki/Characteristic_class)**\n",
        "These are cohomology classes which help to study and classify fibred spaces and vector bundles (Higher dimensional invariants) are defined on vector bundles)\n",
        "  * [Chern classes](https://en.m.wikipedia.org/wiki/Chern_class) ( characteristic classes of complex vector bundles. To study complex geometry of manifolds.)\n",
        "  * [Chern Numbers](https://en.m.wikipedia.org/wiki/Chern_class#Chern_numbers) (they are associated with Chern classes, and give global information about the structure of a bundle)\n",
        "  * [Stiefel-Whitney classes](https://en.m.wikipedia.org/wiki/Stiefel%E2%80%93Whitney_class) (for real vector bundles, characteristic classes of principal bundles. Study the topology of manifolds)\n",
        "  * [Pontriagyn classes](https://en.m.wikipedia.org/wiki/Pontryagin_class) (characteristic classes of real vector bundles, particularly oriented ones. Study the real geometry of manifolds)\n",
        "  * [Segre classes](https://en.m.wikipedia.org/wiki/Segre_class): study of cones, a generalization of vector bundles (total Segre class is inverse to total Chern class, advantage of Segre class: it generalizes to more general cones, while Chern class does not)\n",
        "  * [Euler characteristic](https://en.m.wikipedia.org/wiki/Euler_characteristic) (a type of characteristic class associated with real vector bundles). It can be used to distinguish between orientable and non-orientable manifolds. It can distinguish between spheres and tori.\n",
        "  * Background: Characteristic classes are vector bundle invariants, while Betti numbers are defined on topological spaces.\n",
        "    * This means that **Betti numbers can only distinguish between topological spaces that are not homeomorphic, while characteristic classes can also distinguish between topological spaces that are homeomorphic but have different vector bundles**.\n",
        "    * For example, the sphere and the torus are homeomorphic, but they have different Euler classes. This means that the Betti numbers of the two spaces are the same, but they can be distinguished using characteristic classes.\n",
        "    * Characteristic classes can also be used to study the relationships between different manifolds and fiber bundles. For example, the Gysin homomorphism is a map between the cohomology groups of two manifolds that is defined using characteristic classes.\n",
        "    * Characteristic classes are global invariants, meaning that they do not change under local deformations of a manifold or fiber bundle. This makes them useful for classifying and differentiating topological spaces.\n",
        "\n",
        "* [Knot Invariants](https://en.m.wikipedia.org/wiki/Knot_invariant)\n",
        "These invariants are specific to the study of knots in three-dimensional space.\n",
        "  * [Jones Polynomial](https://de.m.wikipedia.org/wiki/Jones-Polynom)\n",
        "  * [Alexander Polynomial](https://de.m.wikipedia.org/wiki/Alexander-Polynom)\n",
        "  * [Winding Number](https://en.m.wikipedia.org/wiki/Winding_number)\n",
        "\n",
        "* **Differential Topology**\n",
        "These are invariants in the study of smooth manifolds and their properties.\n",
        "  * **De Rham Cohomology**\n",
        "  * **Differential Forms**\n",
        "\n",
        "* **Other Invariants Related to Manifold Geometry**\n",
        "  * **Genus** (related to the classification of surfaces)\n",
        "  * **Sectional Curvature**, **Ricci Curvature**, **Scalar Curvature** (these are invariants in Riemannian geometry, a field closely related to algebraic topology)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k_t6_qozAeTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Quantum Mechanics*"
      ],
      "metadata": {
        "id": "GL1iMe8igraR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Mathematical Formulation & Postulates of Quantum Mechanics***"
      ],
      "metadata": {
        "id": "iptu2LVks6If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1698.jpg)"
      ],
      "metadata": {
        "id": "76nXvHNKhJtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Bra–ket_notation\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Quantum_state\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Measurement_in_quantum_mechanics\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Wave_function"
      ],
      "metadata": {
        "id": "g5_S-_U6lMXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.linkedin.com/pulse/math-quantum-computing-hard-concept-simple-terms-teddy-porfiris/"
      ],
      "metadata": {
        "id": "VyXbQKw2Ktpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"There are (at least) two possible ways to formulate precisely (i.e. mathematically) elementary\n",
        "QM. The eldest one, historically speaking, is due to von Neumann in essence, and is formulated using the language of Hilbert spaces and the spectral theory of unbounded operators. A more recent and mature formulation was developed by several authors in the attempt to solve quantum field theory problems in mathematical physics. It relies on the theory of abstract algebras (*-algebras and C* -algebras) that are built mimicking the operator algebras defined and studied, again, by von Neumann (nowadays known as W* -algebras or von Neumann algebras), but freed from the Hilbert-space structure. The core result is the celebrated GNS theorem (after Gelfand, Najmark and Segal), that we will prove in Chap. 14. The newer formulation can be considered an extension of the former one, in a very precise sense that we shall not go into here, also by virtue of the novel physical context it introduces and by the possibility of treating physical systems with infinitely many degrees of freedom, i.e. quantum fields. In particular, this second formulation makes precise sense of the demand for locality and covariance of relativistic quantum field theories, and allows to extend quantum field theories to a curved spacetime.\"\n",
        "-Valter Moretti"
      ],
      "metadata": {
        "id": "VNk-lacMqzQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cs.cmu.edu/~odonnell/quantum15/lecture17.pdf\n",
        "\n",
        "https://quantum.phys.cmu.edu/QCQI/qitd463.pdf\n",
        "\n",
        "https://quantum.phys.cmu.edu/QCQI/qitd412.pdf"
      ],
      "metadata": {
        "id": "oQDonJmr5Q6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Ehrenfest and the Liouville correspondence*\n",
        "\n",
        "* The Ehrenfest theorem and the Liouville's theorem provide important connections between the microscopic (quantum) and the macroscopic (classical) descriptions of physical systems.\n",
        "\n",
        "* **Ehrenfest Theorem**: In quantum mechanics, the Ehrenfest theorem provides a link between the quantum mechanical description of the motion of particles and the classical motion as described by Newton's laws. Named after Paul Ehrenfest, who proved it in 1927, it demonstrates how the expectation values (average values) of quantum observables (like position and momentum) evolve with time. **Under some circumstances, when quantum effects are small, these expectation values follow trajectories similar to classical mechanics.**\n",
        "\n",
        "* Mathematically, the Ehrenfest theorem states that the time derivative of the expectation value of an operator equals the expectation value of the time derivative of the operator. For position (x) and momentum (p), it can be written as:\n",
        "\n",
        "* d〈x〉/dt = 〈p〉/m  (Ehrenfest's first theorem, analogous to velocity = momentum/mass)\n",
        "d〈p〉/dt = -〈dV/dx〉  (Ehrenfest's second theorem, analogous to Newton's second law)\n",
        "\n",
        "* where V is the potential energy and m is the mass of the particle.\n",
        "\n",
        "* **Liouville's Theorem**: In classical statistical mechanics, Liouville's theorem describes the time evolution of phase space distribution function (a function that gives the probability of the system to be in a certain state with given position and momentum) in a Hamiltonian system (a system governed by Hamilton's equations). The theorem states that the distribution function is constant along the trajectories of the system. That is, the total phase space volume is conserved.\n",
        "\n",
        "* This provides a bridge between microscopic (individual particle trajectories) and macroscopic (bulk or average behavior) properties of the system. The Liouville's theorem is a cornerstone in the development of statistical mechanics and the concept of entropy.\n",
        "\n",
        "* Both of these theorems provide important links between classical and quantum descriptions of physics. They show that in certain limits, classical and quantum mechanical descriptions become similar, providing a physical intuition for the correspondence principle, which states that quantum mechanics must reproduce classical physics in the limit of large quantum numbers."
      ],
      "metadata": {
        "id": "PN6ALFTxu9-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2023-01-scientists-quantum-harmonic-oscillator-room.html"
      ],
      "metadata": {
        "id": "6VMJ5J0fVOgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Video: [Map of Quantum Physics](https://youtu.be/gAFAj3pzvAA)"
      ],
      "metadata": {
        "id": "OEoP77S6dgsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://phys.org/news/2022-11-common-misconceptions-quantum-physics.html\n",
        "\n",
        "https://www.derstandard.de/story/2000140294674/wie-die-quantenphysik-mit-unserer-vorstellung-von-realitaet-aufraeumt\n",
        "\n",
        "https://physicsworld.com/a/how-the-stern-gerlach-experiment-made-physicists-believe-in-quantum-mechanics/"
      ],
      "metadata": {
        "id": "eT_D-OhRlWtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Korrespondenzprinzip](https://de.m.wikipedia.org/wiki/Korrespondenzprinzip): Klassische Größen werden durch Operatoren ersetzt. Die quantenmechanische Aufenthaltswahrscheinlichkeitsdichte eines Teilchens ist proportional zum Quadrat der Wellenfunktion der Materiewelle an jener Stelle. Für große Quantenzahlen geht die quantenmechanische Wahrscheinlichkeitsdichte asymptotisch in die klassische über.\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Korrespondenzprinzip.svg/640px-Korrespondenzprinzip.svg.png)\n"
      ],
      "metadata": {
        "id": "MSQgI9GjQAv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKKG0DSv8FhZ"
      },
      "source": [
        "*Postulate der Quantenmechanik (Kopenhagener Interpretation)*\n",
        "\n",
        "1. **Zustand**: Der Zustand eines physikalischen Systems zu einem Zeitpunkt $t_{0}$ wird durch die Angabe eines zum Zustandsraum $\\mathcal{H}$ gehörenden komplexen Zustandsvektors $\\left|\\psi\\left(t_{0}\\right)\\right\\rangle$ definiert. Vektoren, die sich nur um einen von 0 verschiedenen Faktor $c \\in \\mathbb{C}$ unterscheiden, beschreiben denselben Zustand. Der Zustandsraum des Systems ist ein Hilbertraum.\n",
        "\n",
        "2. **Observable**: Jede Größe $A$, die physikalisch , gemessen\" werden kann, ist durch einen im Zustandsraum wirkenden hermiteschen Operator $\\hat{A}$ beschrieben. Dieser Operator wird als Observable bezeichnet und hat ein reelles Spektrum mit einer vollständigen sogenannten Spektralschar, bestehend aus einem , diskreten\" Anteil mit Eigenvektoren und Eigenwerten (Punktspektrum) und aus einem Kontinuum.\n",
        "\n",
        "3. **Messresultat**: Resultat der Messung einer physikalischen Größe $A$ kann nur einer der Eigenwerte der entsprechenden Observablen $\\hat{A}$ sein oder bei kontinuierlichem Spektrum des Operators eine messbare Menge aus dem Kontinuum.\n",
        "\n",
        "4. **Messwahrscheinlichkeit im Fall eines diskreten nichtentarteten Spektrums**: Wenn die physikalische Größe $A$ an einem System im Zustand $|\\psi\\rangle$ gemessen wird, ist die Wahrscheinlichkeit $P\\left(a_{n}\\right)$, den nichtentarteten Eigenwert $a_{n}$ der entsprechenden Observable $\\hat{A}$ zu erhalten (mit dem zugehörigen Eigenvektor $\\left|u_{n}\\right\\rangle$ ) $P\\left(a_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$. Dabei seien $\\psi$ und $u_{n}$ normiert.\n",
        "\n",
        "5. **Die Zeitentwicklung des Zustandsvektors** $|\\psi(t)\\rangle$ ist gegeben durch die folgende Schrödingergleichung, wobei $\\hat{H}(t)$ die der totalen Energie des Systems zugeordnete Observable ist:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t}|\\psi(t)\\rangle=\\hat{H}(t)|\\psi(t)\\rangle$\n",
        "\n",
        "http://vergil.chemistry.gatech.edu/notes/quantrev/node20.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Mathematical_formulation_of_quantum_mechanics](https://en.m.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics)\n",
        "* [C*-algebra](https://en.m.wikipedia.org/wiki/C*-algebra)\n",
        "* [Quantum_geometry](https://en.m.wikipedia.org/wiki/Quantum_geometry)\n",
        "* [Noncommutative_geometry](https://en.m.wikipedia.org/wiki/Noncommutative_geometry)\n",
        "* [Geometric_quantization](https://en.m.wikipedia.org/wiki/Geometric_quantization)"
      ],
      "metadata": {
        "id": "wGmG22vunC0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_209.png)\n"
      ],
      "metadata": {
        "id": "q8MuXUSiqRrJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE--P8idzoP-"
      },
      "source": [
        "**Video: [Crash course in density matrices](https://www.youtube.com/watch?v=1tserF6VGqI)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWlXYxSBzPS3"
      },
      "source": [
        "**Single spin one half particle, focus on spin degrees of freedom & Pauli-matrices**\n",
        "\n",
        "* when the spin degrees of freedom interact with an electromagnetic field, the Pauli matrices come into play:\n",
        "\n",
        "> $\\sigma^{Z}=\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right) \\quad \\sigma^{X}=\\left(\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right) \\quad \\sigma^{Y}=\\left(\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right)$\n",
        "\n",
        "* we have chosen a basis in such a way that the Pauli Z matrix is diagonal. Here are its basis vectors, the spin up in the z direction and the spin down direction, written as column vectors:\n",
        "\n",
        "> $|\\uparrow\\rangle=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\quad |\\downarrow\\rangle=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "* we can re-express the basis vectors for the Pauli X matrix in either direction in terms of these vectors, but in the positive direction we can write it in the following way:\n",
        "\n",
        "> $|\\rightarrow\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle+|\\downarrow\\rangle)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvOyiyJtzRtd"
      },
      "source": [
        "**States we are used to use in quantum mechanics are called 'pure states'.**\n",
        "\n",
        "* Any state can be written as the linear combination of the up and down vectors in the z-direction\n",
        "\n",
        "> $|\\psi\\rangle=a|\\uparrow\\rangle+b|\\downarrow\\rangle$\n",
        "\n",
        "* a and b are complex numbers whose modulus squared sum to 1\n",
        "\n",
        "> $\\langle\\psi \\mid \\psi\\rangle=|a|^{2}+|b|^{2}$\n",
        "\n",
        "* a and b can be referred to as the probability amplitudes, where their modulus squared are the probabilities that the system is in a particular configuration.\n",
        "\n",
        "* If we perform an ensemble of measurements on the system, we will find that the mean or expected value, for example of the Pauli-Z matrix, will be given by:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZoOoevVzUPa"
      },
      "source": [
        "**Dynamics: if we want to study how a quantum system changes in time, we usually refer to the Schroedinger equation.**\n",
        "\n",
        "> $i \\frac{d}{d t}|\\psi(t)\\rangle=\\hat{H}|\\psi\\rangle$\n",
        "\n",
        "* the operator $\\hat{H}$ is called the Hamiltonian and it tells us about the total energy in a system, and how things in a system interact with each other.\n",
        "\n",
        "* the easiest way to solve this problem is to first solve the Eigenvalue problem, which involves finding the Eigenvectors and the Eigenvalues of the hamiltonian. We are able the different values of the Eigenvalues and Eigenvectors with the label k:\n",
        "\n",
        "> $\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* This allows us to know that the Eigenvectors, when plugged into the Schroedinger equation ...\n",
        "\n",
        "> $i \\frac{d}{d t}\\left|E_{k}\\right\\rangle=\\hat{H}\\left|E_{k}\\right\\rangle=E_{k}\\left|E_{k}\\right\\rangle$\n",
        "\n",
        "* ... just pick up a phase in time depending on the energy they correspond to:\n",
        "\n",
        "> $\\left|E_{k}(t)\\right\\rangle=e^{-i E_{k} t}\\left|E_{k}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitINkMkzWk-"
      },
      "source": [
        "**Now to sum more general situation with some generic state $\\psi$**\n",
        "\n",
        "* we can decompose $\\psi$ in terms of energy Eigenbasis:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* where $c_{m}$ is given by the inner product of the energy Eigenvector labelled by m and $\\psi$ itself:\n",
        "\n",
        "> $c_{m}=\\left\\langle E_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* We can then time-evolve the state by simply time-evolving the energy Eigen-Kets:\n",
        "\n",
        "> $|\\psi(t)\\rangle=\\sum_{m} c_{m} e^{-i E_{m} t}\\left|E_{m}\\right\\rangle$\n",
        "\n",
        "* Tracking the expectation value of an observable is quite easy: Simply applying the state vector in time to both sides of the matrix gives the following equation:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\sum_{m, n} \\bar{c}_{n} c_{m} A_{n, m} e^{i\\left(E_{n}-E_{m}\\right) t}$\n",
        "\n",
        "* Where we have labelled the matrix entries of a by the energy Eigenbasis in the following way:\n",
        "\n",
        "> $A_{n, m}=\\left\\langle E_{n}|A| E_{m}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utZdbq6QzZD9"
      },
      "source": [
        "**Propeties of the trace of a matrix**\n",
        "\n",
        "* We have a basis of states labeled by j $|j\\rangle, j=1,2$.. N that form a complete orthonormal basis. Then I write the trace of a matrix in the following way:\n",
        "\n",
        "> $\\operatorname{Tr}(A)=\\sum_{j}\\langle j|A| j\\rangle$\n",
        "\n",
        "* the trace is a linear mapping which tells us that we can separate the trace of the sum of two matrices apart like this, and we can also pull scalar multiples outside of the trace:\n",
        "\n",
        "> $\\operatorname{Tr}(A+B)=\\operatorname{Tr} A+\\operatorname{Tr} B$ with $\\operatorname{Tr}(c A)=c \\operatorname{Tr} A$\n",
        "\n",
        "* When we take the trace of two matrices multiplied by each other, the trace is invariant under swapping the two matrices inside of the trace.\n",
        "\n",
        "> $\\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Om09eqlza4O"
      },
      "source": [
        "**Density Matrices**\n",
        "\n",
        "* Going from a pure state represented by a Ket we can introduce the density matrix which is a completely equivalent way to represent the state of a quantum system\n",
        "\n",
        "* the density matrix in this context is just the outer product of the state with itself:\n",
        "\n",
        "> $\\rho=|\\psi\\rangle\\langle\\psi|$\n",
        "\n",
        "* the expectation value is rewritten in terms of a trace, but it is mathematically equivalent to the earlier expression:\n",
        "\n",
        "> $\\left\\langle\\sigma^{Z}\\right\\rangle=\\operatorname{Tr}\\left(\\rho \\sigma^{2}\\right)=\\left\\langle\\psi\\left|\\sigma^{Z}\\right| \\psi\\right\\rangle$\n",
        "\n",
        "* the density matrix has two fundamental properties: it's trace is 1 in the context of a pure state:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1$\n",
        "\n",
        "> $\\operatorname{Tr}|\\psi\\rangle\\left\\langle\\left.\\psi|=|\\langle\\psi \\mid \\psi\\rangle\\right|^{2}=1\\right.$\n",
        "\n",
        "* and it's a positive operator:\n",
        "\n",
        "> $\\rho \\geq 0$\n",
        "\n",
        "* If I take any state vector our state space and perform the following operation, then the result is always greater than or equal to zero:\n",
        "\n",
        "> $\\langle\\phi|\\rho| \\phi\\rangle=|\\langle\\phi \\mid \\psi\\rangle|^{2} \\geq 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuLO_P1u6FBJ"
      },
      "source": [
        "**Dynamics: von Neumann equation of time evolution**\n",
        "\n",
        "* show from the Schroedinger equation we can derive the von Neumann equation of time evolution:\n",
        "\n",
        "> $i \\frac{\\partial \\rho}{\\partial t}=[\\hat{H}, \\rho]$\n",
        "\n",
        "* It features the commutation relationship between the Hamiltonian and the density matrix itself\n",
        "\n",
        "* then the expectation value for an observable in time can be rewritten in the following way:\n",
        "\n",
        "> $\\langle A(t)\\rangle=\\operatorname{Tr}(\\rho(t) A)$\n",
        "\n",
        "* where the density matrix $\\rho$ evolves with the Hamiltonian being applied to it in time:\n",
        "\n",
        "> $\\rho(t)=e^{-i \\hat{H} t} \\rho e^{i \\hat{H} t}$\n",
        "\n",
        "*  similarly re-expressed in the basis of the energy Eigenvalues\n",
        "\n",
        "> $\\rho(t)=\\sum_{m, n} \\rho_{m, n} e^{-i\\left(E_{m}-E_{n}\\right) t}\\left|E_{m}\\right\\rangle\\left\\langle E_{n}\\right|$\n",
        "\n",
        "* where $\\rho_{m,n}$ can be written in the following way, where we have re-expressed its entries in terms of the energy Eigenbasis:\n",
        "\n",
        "> $\\rho_{m, n}=\\left\\langle E_{m}|\\rho| E_{n}\\right\\rangle$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmo42wRj8v6C"
      },
      "source": [
        "**Mixed States**\n",
        "\n",
        "* what if we are unsure of what pure state our system is in? (We are somehow ignorant / unwissend to what pure state we are in)\n",
        "\n",
        "* then we can describe a statistical ensemble of pure states which we call a mixed states\n",
        "\n",
        "> $\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|$\n",
        "\n",
        "* the ensemble is written as a sum of pure states with probabilities $p_j$ and the probabilities are greater than zero and sum up to one:\n",
        "\n",
        "> $p_{j} \\geq 0 \\quad \\sum_{j} p_{j}=1$\n",
        "\n",
        "* Mixed states are also trace 1 and are positive operators:\n",
        "\n",
        "> $\\operatorname{Tr} \\rho=1, \\quad \\rho \\geq 0$\n",
        "\n",
        "* Dynamics and the expectation values work in an identical way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUFROLA7BPGU"
      },
      "source": [
        "**Non-uniqueness of mixed states decomposition**\n",
        "\n",
        "* Interestingly it's not completely correct to interpret $p_j$ as the probability of being in a particular state labeled by j.\n",
        "\n",
        "* To see this consider the following example let $\\rho$ be a mixed state written in the following way:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* we say the system is in state down with probability 4/5 and in state up with 1/5.\n",
        "\n",
        "* What if I prepared two other states with the following new state vectors a and b with the following probability amplitudes of being in down or up state:\n",
        "\n",
        "> $|a\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle+\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "> $|b\\rangle=\\sqrt{\\frac{4}{5}}|\\downarrow\\rangle-\\frac{1}{\\sqrt{5}}|\\uparrow\\rangle$\n",
        "\n",
        "* Then if we prepare these states with probability one-half:\n",
        "\n",
        "> $\\rho=\\frac{1}{2}|a\\rangle\\left\\langle a\\left|+\\frac{1}{2}\\right| b\\right\\rangle\\langle b|$\n",
        "\n",
        "* We see that we get the same density matrix as before working this out expanding the definitions of a and b allows us to arrive at our original density matrix:\n",
        "\n",
        "> $\\rho=\\frac{4}{5}|\\downarrow\\rangle\\left\\langle\\downarrow\\left|+\\frac{1}{5}\\right| \\uparrow\\right\\rangle\\langle\\uparrow|$\n",
        "\n",
        "* Therefore two different ensembles of pure states can give rise to the same mixed state and we must be careful when we interpret $p_j$ as strictly probabilities of a particular system being strictly in its associated pure state in the sum.\n",
        "\n",
        "* Finally, if you get a matrix how could you tell if it's pure or mixed? Mathematically quite simple way to check: square the matrix and take its trace. If the trace is still 1, we say that the density matrix is pure if it is less than one then we say it's mixed. This is actually independent of the time evolution.\n",
        "\n",
        "> $\\operatorname{Tr}\\left(\\rho^{2}\\right) \\leq 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket $|\\psi\\rangle$ - State Space $V$(Vector)*** *- also: Wave Function (Pure State, Postulate I)*"
      ],
      "metadata": {
        "id": "jBh1Tr-vn_4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcIVsSgCSld"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate I of Quantum Mechanics</u>: The state of a physical system is characterized by a state vector that belongs to a complex vector space $\\mathcal{V}$, called the state space of the system**\n",
        "\n",
        "* State Space Formalism: unification of Schrodinger wave mechanics and matrix mechanics (Heisenberg, Born, Jordan), both are equivalent.\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin). Then it reduces to the rules of simple matrix multiplication**. Kronecker delta\n",
        "\n",
        "* **Schrodinger wave mechanics: for continuous basis (like position)** Dirac delta function\n",
        "\n",
        "* State Space: the vector space in which quantum systems live\n",
        "\n",
        "  * Euclidean space: classical physics, 3D, real, inner product (Hilbert Space)\n",
        "\n",
        "  * State space: quantum physics, infinite dimensions, complex numbers, inner product (Hilbert Space)\n",
        "\n",
        "Video: [Dirac notation: state space and dual space](https://www.youtube.com/watch?v=hJoWM9jf0gU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlC5y6Hpsj_E"
      },
      "source": [
        "<font color=\"blue\">*Ket Algebra - Quantum State Vector*\n",
        "\n",
        "**A Ket $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$, also called 'quantum state', <u>represents</u> the wave function of a quantum system (\"Psi\")**, consisting of **probability amplitudes**. This Quantum state is a **stochastic vector**, written as a column vector.\n",
        "\n",
        "> **A Ket is technically a pure quantum state**.\n",
        "\n",
        "* Wave function notation to describe superposition of Pure States.\n",
        "\n",
        "So, you have an (orthonormal) basis with 3 vectors and coefficients, to describe a vector in space (in Hilbert space):\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$2$</font>$\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)$<font color='blue'>$+3$</font>$\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)$<font color='blue'>$+0$</font>$\\left(\\begin{array}{l}0 \\\\ 0 \\\\ 1\\end{array}\\right)$\n",
        "\n",
        "The basis vectors will all entries in 0 and only one with 1 correspond to possibe measurement outcomes (spin up or down).\n",
        "\n",
        "The coefficients can be collected in one vector and are complex numbers:\n",
        "\n",
        "> $\\vec{v}=$<font color='blue'>$\\left(\\begin{array}{l}2 \\\\ 3 \\\\ 0\\end{array}\\right)$</font>\n",
        "\n",
        "An arbitrary state for a qubit can be written as a linear combination of the Pauli matrices, which provide a basis for $2 \\times 2$ self-adjoint matrices:\n",
        "\n",
        "> $\n",
        "\\rho=\\frac{1}{2}\\left(I+r_{x} \\sigma_{x}+r_{y} \\sigma_{y}+r_{z} \\sigma_{z}\\right)\n",
        "$\n",
        "\n",
        "* where the real numbers $\\left(r_{x}, r_{y}, r_{z}\\right)$ are the coordinates of a point within the unit ball and\n",
        "\n",
        "> $\n",
        "\\sigma_{x}=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{y}=\\left(\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right), \\quad \\sigma_{z}=\\left(\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & -1\n",
        "\\end{array}\\right)\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siehe auch: [Projective Hilbert Space](https://en.m.wikipedia.org/wiki/Projective_Hilbert_space) with rays or projective rays"
      ],
      "metadata": {
        "id": "_sFD45vxEg5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSzA4IwURCu"
      },
      "source": [
        "<font color=\"blue\">*Multiply Ket with a Scalar: Vector-Scalar-Multiplication*\n",
        "\n",
        "Vector-Scalar:\n",
        "\n",
        "$\\left[\\begin{array}{l}x_{0} \\\\ x_{1}\\end{array}\\right] \\otimes\\left[y_{0}\\right]=\\left[\\begin{array}{l}x_{0}\\left[y_{0}\\right] \\\\ x_{1}\\left[y_{0}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}x_{0} y_{0} \\\\ x_{1} y_{0}\\end{array}\\right]$\n",
        "\n",
        "\n",
        "<font color=\"blue\">*Multiply Ket with another Ket: Ket - Tensor Product (Vector-Vector-Multiplication, Kronecker Product)*\n",
        "\n",
        "> $\\mathbf{uv}$ = $\\left[\\begin{array}{c}u_{1} \\\\ u_{2}\\end{array}\\right]$ $\\otimes$ $\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\end{array}\\right]$ = $\\left[\\begin{array}{l}u_{1}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right] \\\\ u_{2}\\left[\\begin{array}{l}v_{1} \\\\ v_{2}\\end{array}\\right]\\end{array}\\right]$=  $\\left[\\begin{array}{c}u_{1} v_{1} \\\\ u_{1} v_{2}\\\\ u_{2} v_{1} \\\\ u_{2} v_{2}\\end{array}\\right]$\n",
        "\n",
        "*Zur Kombination von Quantum States:*\n",
        "\n",
        "> $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=|0\\rangle, \\quad\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=|1\\rangle$.\n",
        "\n",
        "We choose two qubits in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ $\\left [\\begin{array}{l}11 \\\\ 10 \\\\ 01 \\\\ 00\\end{array}\\right]$ = <font color=\"gray\">$\\left [\\begin{array}{l}3 \\\\ 2 \\\\ 1 \\\\ 0\\end{array}\\right]$</font> = <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Quits in two different states:\n",
        "\n",
        "> $|0\\rangle \\otimes|1\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}1\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH8JeO4mW4q6"
      },
      "source": [
        "**How do we represent Ket's in a particular basis?**\n",
        "\n",
        "> <font color=\"blue\">$|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle \\quad$ where: $c_{i}=\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "Video: [Representations in quantum mechanics](https://www.youtube.com/watch?v=rp2k2oR5ZQ8)\n",
        "\n",
        "\n",
        "> $\\left\\{c_{i}\\right\\}$ are the representation of $|\\psi\\rangle$ in the $\\left\\{\\left|u_{i}\\right\\rangle\\right\\}$ basis\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_210.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_211.png)\n",
        "\n",
        "* a und b coefficients in euclidean space sind wie $u_i$ coefficients in quantum state space. (difference is bra-ket, wo bra das conjugate complex ist als dot product mit ket: es ist im erstem argument antilinear!).\n",
        "\n",
        "* expansion coefficients c are given by projection of the Ket onto the basis states u:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_212.png)\n",
        "\n",
        "* Here we can take out $|\\Psi\\rangle$ because it doesnt explicitely depend on $_i$:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_213.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xfgmufRt0W"
      },
      "source": [
        "**Additional Ket-Algebra**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_202.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_203.png)\n",
        "\n",
        "For euclidean space: the scalar product is linear both in first and second argument:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_204.png)\n",
        "\n",
        "For state space: the scalar product is only linear in the second argument (and antilinear in the first argument):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_205.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra $\\langle \\psi |$ - Dual Space $V^{*}$ (Covector)***"
      ],
      "metadata": {
        "id": "cqdowsM5oHEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **$|\\psi\\rangle$**: This is a quantum state, often called a 'ket' in the Dirac notation. It represents the state of a quantum system, which could be a single qubit or a system of multiple qubits.\n",
        "* **|ψ⟩:** This is the quantum state of the system. The state represents the configuration of the system's components, and it can be represented by a complex vector in a Hilbert space.\n",
        "\n",
        "2. **$\\langle\\psi|$**: This is the bra corresponding to the ket $|\\psi\\rangle $. In Dirac notation, $\\langle\\psi|$) represents the conjugate transpose of $|\\psi\\rangle$.\n",
        "\n",
        "* **⟨ψ|:** This denotes the inner product of the quantum state |ψ⟩ with itself. This inner product represents the overlap between the two states |ψ⟩ and |ψ⟩, which is a measure of their similarity.\n"
      ],
      "metadata": {
        "id": "_g5-XOuYVQ1X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOxv3yFtjD1"
      },
      "source": [
        "**How do we represent Bra's in a particular basis?**\n",
        "\n",
        "* Bra $\\langle \\Psi|$ is an element of the dual vector space $V^*$\n",
        "\n",
        "* Bra Psi times the identity operator ($\\langle \\Psi| * \\mathbb{I}$), and then write the identity operator out (its resolution in the u basis)\n",
        "\n",
        "* psi doesnt explicitylt depend on i, so we can write it into the summation, which brings us to this expression\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi \\mid u_{i}\\right\\rangle\\left\\langle u_{i}\\right|$\n",
        "\n",
        "* This is an expression for the bra psi in terms of the basis bra's u, and then the expansion coefficients are the brackets between psi and u $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$\n",
        "\n",
        "\n",
        "If we look at these expansion coefficients $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$, we can use the conjugation property of the scalar product to rewrite them like this:\n",
        "\n",
        "> $\\left\\langle\\psi \\mid u_{i}\\right\\rangle$ = $\\left\\langle u_{i} \\mid \\psi \\right\\rangle^*$ = $c_i^*$\n",
        "\n",
        "**The expansion coefficients are the complex conjugates of the expansion coefficients of the Ket**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_214.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Bra-Ket $\\langle \\psi |\\psi\\rangle = c$ - Linear Form (Covector-Vector)*** *- also: Projective Measurement*"
      ],
      "metadata": {
        "id": "1fAAZcgWoiJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A6siRC6YyVY"
      },
      "source": [
        "> **Bra-Ket - Projective Measurement (Kovector-Vector-Multiplication, Born Rule)**\n",
        "\n",
        "* Dirac delta function (a distribution!) = quantum measurement\n",
        "\n",
        "* See also [wiki: Bra–ket notation](https://en.m.wikipedia.org/wiki/Bra–ket_notation)\n",
        "\n",
        "* From 'Exterior algebra' $\\rightarrow$ 'Multilinear Forms':\n",
        "\n",
        "  * **A row vector can be thought of as a function (as a form), rather than a row vector, that acts on another vector.**\n",
        "\n",
        "  * In Quantum mechanics: Linear functionals are particularly important in quantum mechanics. Quantum mechanical systems are represented by Hilbert spaces, which are [anti–isomorphic](https://en.m.wikipedia.org/wiki/Antiisomorphism) to their own dual spaces. A state of a quantum mechanical system can be identified with a linear functional. For more information see bra–ket notation.\n",
        "\n",
        "  * Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$: **Kovector-Vector-Multiplication**, Born Rule (Projective Measurement)\n",
        "\n",
        "  * ⟨0∣1⟩ und ⟨1∣0⟩ ergeben inner product 0 (orthogonal zueinander), zB $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$. Und ⟨0∣0⟩ und ⟨1∣1⟩ = 1.\n",
        "\n",
        "*See also: [dot product between a covector and a vector is a scalar, after all!](https://cafephysics35698708.wordpress.com/2017/12/23/moving-away-from-ortho-land-vectors-covectors-and-all-that/) but read also why [There are a couple ways to view a dot product as a linear map by changing your view slightly](https://math.stackexchange.com/questions/2856198/is-dot-product-a-kind-of-linear-transformation)*\n",
        "\n",
        "* Inner Product / Bra-Ket, conjugate transpose of Ket.\n",
        "You get a scalar as output.\n",
        "\n",
        "* The Bra-Ket $\\langle\\psi \\mid \\psi\\rangle$  represents the inner product in the Hilbert space\n",
        "\n",
        "* Zur Messung von Zustaenden in einer Basis (zB ich will die Probability wissen, mit der man den State=1 erhält)\n",
        "\n",
        "* Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.\n",
        "\n",
        "* **A state of a quantum mechanical system can be identified with a linear functional**.\n",
        "\n",
        "> $\\mathbf{u}^{\\top} \\mathbf{v}=\\left[\\begin{array}{llll}u_{1} & u_{2} & \\cdots & u_{n}\\end{array}\\right]\\left[\\begin{array}{c}v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n}\\end{array}\\right]=\\left[u_{1} v_{1}+u_{2} v_{2}+\\cdots+u_{n} v_{n}\\right]$ = scalar\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_248.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 1: Inner product of two basis vectors**:\n",
        "\n",
        "\n",
        "* ⟨0∣1⟩ und ⟨1∣0⟩ ergeben inner product 0 (orthogonal zueinander), im Detail fur $\\langle 0 \\mid 1\\rangle=[1,0]\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] = 0$\n",
        "\n",
        "* ⟨0∣0⟩ und ⟨1∣1⟩ ergeben inner product 1 (mit sich selbst multipliziert / die beiden Berechnungsbasisvektoren sind orthonormal)\n",
        "\n",
        "**Case 2: Berechne Wahrscheinlichkeit fur Messung eines Eigenstates (Born's Rule)**\n",
        "\n",
        "* Die orthonormalen Eigenschaften sind im folgenden Beispiel nützlich\n",
        "\n",
        "* Es gibt einen Zustand mit der Wave Function / Wahrscheinlichkeit fur beide Zustaende $|\\psi\\rangle=\\frac{3}{5}|1\\rangle+\\frac{4}{5}|0\\rangle$\n",
        "\n",
        "* die Wahrscheinlichkeit des Messens 1 ist dann (1 ist hierbei die basis, weil ich es messen will) [Source](https://docs.microsoft.com/de-de/azure/quantum/concepts-dirac-notation):\n",
        "\n",
        ">$\n",
        "|\\langle 1 \\mid \\psi\\rangle|^{2}=\\left|\\frac{3}{5}\\langle 1 \\mid 1\\rangle+\\frac{4}{5}\\langle 1 \\mid 0\\rangle\\right|^{2}=\\frac{9}{25}\n",
        "$\n",
        "\n",
        "* weil $\\langle 1 \\mid 0\\rangle=0$ faellt der zweite Term weg: $\\frac{4}{5}\\langle 1 \\mid 0\\rangle$ = $\\frac{4}{5} * 0$\n",
        "\n",
        "* Another example: $\\left[\\begin{array}{ll}2 & 1\\end{array}\\right]\\left(\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]\\right)=2 x+1 y$. **The covector [2, 1] can be thought of as a function, rather than a row vector, that acts on another vector.**\n",
        "\n",
        "> <font color=\"red\">*The probability of a particular measurement is then the absolute square of the scalar product with the basis vector that corresponds to the outcome (probability of measuring $X$ is). This is <u>Born's Rule</u>. This scalar product of the wave function with a basis vector is also sometimes called a <u>projection</u> on that basis vector:*\n",
        "\n",
        "> $|\\langle X \\mid \\Psi\\rangle|^{2}=a_{1} a_{1}^{*}$\n",
        "\n",
        "* where $\\mid \\Psi\\rangle$ is the wavefunction of the probability superposition and $\\langle X \\mid$ is the basis vector of one outcome.\n",
        "\n",
        "**Case 3: Sum over all basis vectors**\n",
        "\n",
        "> $\\langle\\Psi^* |\\Psi\\rangle$ = $ (a{_1}^*, a{_2}^*, a{_3}^*) \\left(\\begin{array}{l}a_{1} \\\\ a_{2} \\\\ a_{3}\\end{array}\\right)$ = $(a{_1}^*a_1 + a{_2}^*a_2 + a{_2}^*a_2)$\n",
        "\n",
        "**The probability to get ANY measurement outcome is equal to one, which means that the sum over the squared scalar products with all basis vectors has to be one. Which is just the length of the vector (all wave functions have length 1)**:\n",
        "\n",
        "> $1=a_{1} a_{1}^{*}+a_{2} a_{2}^{*}+a_{3} a_{3}^{*}=|\\langle \\Psi \\mid \\Psi\\rangle|^{2}$\n",
        "\n",
        "* With this bra-ket notation it's now very easy to write dot products = the **inner product between Bra and Ket which is $\\langle\\psi \\mid \\psi\\rangle$ = 1**, and it is normalized the result is 1(it's a particular way of writing the 2-norm when using complex inputs).\n",
        "\n",
        "* With the dot product of bra and ket you will get a **scalar as a result**, like total probability is 1, here for the coefficients (probability amplitudes):\n",
        "\n",
        "> $\\langle\\psi^* \\mid \\psi\\rangle = \\left|a_{0}\\right|^{2}+\\left|a_{1}\\right|^{2}= 1$\n",
        "\n",
        "* **We expand the Ket $\\psi$ in the basis u**, where the expansion coefficients c are given by the Braket between u and psi:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{i} c_{i}\\left|u_{i}\\right\\rangle = c_{1} * u_{1} + c_{2} * u_{2} .. + c_{i}* u_{i} = c_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)+{c_{2}}\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right)+.. c_{i}\\left(\\begin{array}{l}0 \\\\ 0 \\\\ i\\end{array}\\right) \\quad =\\left\\langle u_{i} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">This c cofficients are what we call **representation of the Ket psi in the u basis**.\n",
        "\n",
        "> $\\left(\\begin{array}{c}\\langle u_{1} \\mid \\psi\\rangle \\\\ \\left\\langle u_{2} \\mid \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{i} \\mid \\psi\\right\\rangle \\\\ \\vdots\\end{array}\\right)=\\left(\\begin{array}{c}c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{i} \\\\ \\vdots \\\\ \\end{array}\\right)$"
      ],
      "metadata": {
        "id": "JhSZ3kxevCQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Quantum Measurements from an Observable (via its Operator) - Use Case: What is the most likely Eigenvalue of an Observable like Momentum or Position?**\n",
        "\n",
        "Video: [Measurements in quantum mechanics || Concepts](https://www.youtube.com/watch?v=u1R3kRWh1ek)\n",
        "\n",
        "> **$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "* Physical quantity $\\mathcal{A} \\longrightarrow$ Observable $\\hat{A}$\n",
        "\n",
        "* To understand what it means to measure $\\hat{A}$ in quantum mechanics, the key equation to consider is the Eigenvalue equation of the operator $\\hat{A}$ here:\n",
        "\n",
        "> $\n",
        "\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle\n",
        "$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ When you apply a measurement operator in an Eigenstate, you will measure /get the Eigenvalue of this Eigenstate (postulate III of quantum mechanics). Challenge: we just dont know in which Eigenstate our system $|\\Psi\\rangle$ is.*</font>\n",
        "\n",
        "* with $\\lambda_{n}$ Eigenvalues and $u_{n}$ Eigenstates\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "\n",
        "* This means: when we want to measure property $\\hat{A}$ we first need to solve the corresponding Eigenvalue equation, which allows us to find all Eigenvalues $\\lambda_{1}$, $\\lambda_{2}$, .. $\\lambda_{n}$\n",
        "\n",
        "* So the operator $\\hat{A}$ encodes all the possible outcomes of the measurement, irrespective of what the state of the system is.\n",
        "\n",
        "* **So the question is: if we measure $\\hat{A}$ in state $|\\Psi\\rangle$: which Eigenvalue will we get?** (Postulate III will tell us we will get one of the Eigenvalues $\\lambda_{n}$, but not which one)\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate VI of quantum mechanics</u>: The measurement of $\\mathcal{A}$ in a system in normalized state $|\\psi\\rangle$ gives eigenvalue $\\lambda_{n}$ with probability:**\n",
        "\n",
        "\n",
        "> $\n",
        "P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\n",
        "$ $\\quad (= \\left|c_{n}\\right|^{2})$\n",
        "\n",
        "<font color =\"blue\">*$\\rightarrow$ $u_n$ are the possible Eigenstates to which we project our actual system state $|\\Psi\\rangle$. We square its norm and then see which one has the highest value = highest probability that the state is in this Eigenstate (with a givne Eigenvalue $\\lambda_{n}$). For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$*</font>\n",
        "\n",
        "\n",
        "* This means: for an operator $\\hat{A}$ we have a list of Eigenvalues, where each of them corresponds to an Eigenstate:\n",
        "\n",
        "  * $\\lambda_{1}$ for $|u_1\\rangle$,\n",
        "\n",
        "  * $\\lambda_{2}$ for $|u_2\\rangle$,\n",
        "\n",
        "  * ...\n",
        "\n",
        "  * $\\lambda_{n}$ for $|u_n\\rangle$\n",
        "\n",
        "* these relations and values come from the Eigenvalue equation of the specific operator $\\mathcal{A}$ and are independent of the state of our system.\n",
        "\n",
        "* **But when we measure $\\mathcal{A}$, we measure it in a specific state $|\\Psi\\rangle$**. We will get $\\lambda_{1}$ with $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* So what we get as result depends (1) on the intrinsic properties $\\mathcal{A}$ with its Eigenvalues and Eigenstates,  and (b) on the specific state $|\\Psi\\rangle$ in which our system is.\n",
        "\n",
        "* So with postulate 4: rather then telling us the precise outcome of a measurement it tells us the probability associated with any given outcome\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "\n",
        "**How does the state $|\\Psi\\rangle$ of a system encodes the possible outcomes of a measurement?**\n",
        "\n",
        "> <font color =\"blue\">$\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We can write the state $|\\Psi\\rangle$ in a complete basis of our state space and the Eigenstates $u_n$ over Hermitian operator like $\\hat{A}$ provide such a basis\n",
        "\n",
        "* this means we can write $|\\Psi\\rangle$ in the u-Basis (Eigenstates) like this:\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{n} c_{n}\\left|u_{n}\\right\\rangle$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ $\\left|c_{m}\\right|^{2}$ is the probability and hence $c_n$ the squared root of the probability of a given Eigenvalue for each Eigenstate $\\left|u_{n}\\right\\rangle$ from this part here above:\n",
        "\n",
        "> $\\begin{array}{cccccc} \\hat{A}: \\quad  & \\lambda_{1} & \\lambda_2 & \\lambda_3 & \\lambda_{n} \\\\ & \\left|u_{1}\\right\\rangle & \\left|u_{2}\\right\\rangle & \\left|u_{3}\\right\\rangle & \\left|u_{n}\\right\\rangle \\\\ |\\psi\\rangle: & \\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{2} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{3} \\mid \\psi\\right\\rangle\\right|^{2} & \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}\\end{array}$\n",
        "\n",
        "* and these expansion coefficients c which we call **the representation of $|\\Psi\\rangle$ in the u-Basis**, are given by the projection of $|\\Psi\\rangle$ onto the u-Basis states:\n",
        "\n",
        "> $c_{n}=\\left\\langle u_{n} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "* This means we can rewrite the probability p of measuring Eigenvalue $\\lambda_n$ as also equal to absolute value of $c_n$ squared:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{m}\\right|^{2}$\n",
        "\n",
        "<font color =\"blue\">$\\rightarrow$ For example we will get $\\lambda_{1}$ with probability $\\left|\\left\\langle u_{1} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "**What does this mean?**\n",
        "\n",
        "* imagine our system is in state $\\Psi\\rangle$ and we want to measure a property $|\\Psi\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow \\hat{A} \\longrightarrow ?$\n",
        "\n",
        "* Then what we do is to write the state $|\\Psi\\rangle$ in the basis of Eigenstates of $\\hat{A}$, and <font color =\"blue\">the expansion coefficient $c_n$ tell us the relative contribution of Eigenstate $|u_n\\rangle$ to state $|\\Psi\\rangle$, which in turn tell us how likely it is to measure the associated Eigenvalue $\\lambda_n$</font>\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} \\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* <font color=\"red\">here you can re-arrange and get an outer product (is this true???)\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} |u_{m}\\rangle \\langle u_{m}| \\psi\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "Passt zu weiter unten (video prof m mit projection operators):\n",
        "\n",
        "> $(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_199.png)\n",
        "\n",
        "Imagine we have n copies of the state $|\\Psi\\rangle$ and measure each. We get an Eigenvalue from the Eigenvalue equation, but with different probabilities. As p approaches $\\infty$ with N copies (= $p_n$), we will reach the most probable Eigenvalue P($\\lambda_m$) (postulates 4):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_200.png)\n",
        "\n",
        "**A special case if our system with state $|\\Psi\\rangle$ is in an Eigenstate of the property that we are measuring (the operator $\\hat{A}$, for example momentum, positon etc), say $|u_m\\rangle$**:\n",
        "\n",
        "> $|\\psi\\rangle=\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* This corresponds to the expansion coefficient $c_m$ = 1, while all other coefficients vanish.\n",
        "\n",
        "* In this particular case we do know with absolute certainty what the outcome of the measurement will be. Probability = 1:\n",
        "\n",
        "> $P\\left(\\lambda_{m}\\right)=\\left|c_{m}\\right|^{2}=1$\n",
        "\n",
        "**This means: If the system is in an Eigenstate of the property that we are measuring (=momentum, position etc, measured by an operator), then the outcome of the measurement is the associated Eigenvalue with probability 1.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_287.png)"
      ],
      "metadata": {
        "id": "nKVQn725YOeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi | = P$ - Linear Map (Vector-Covector) - Projection Operator***"
      ],
      "metadata": {
        "id": "cyrUknMLo8bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case 1: Projection Operator (Outer Product)**"
      ],
      "metadata": {
        "id": "vmUk0nAHzPCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A measurement outcome is actually a projection (to the first basis vector)**\n",
        "\n",
        "* So if we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$> bzw. | 0 > < 0 | $\\Phi$ >\n",
        "\n",
        "* this is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "> The probability of measuring a value with probability amplitude $\\phi$ is $1 \\geq|\\phi|^{2} \\geq 0$, where $|\\cdot|$ is the [modulus](https://en.m.wikipedia.org/wiki/Absolute_value#Complex_numbers).\n",
        "\n",
        "**Ket-Bra (also: Projection Operator, Outer Product or Density Matrix)**\n",
        "\n",
        "* Use Case 1: used for mixed states\n",
        "\n",
        "* Use Case 2: used when I want to measure a specific Eigenstate / Eigenvector to get its probability.\n",
        "\n",
        "* Is a pair: Vector-Covector\n",
        "\n",
        "> $\\mathbf{u v}^{T}=\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right]\\left[\\begin{array}{llll}v_{1} & v_{2} & \\cdots & v_{n}\\end{array}\\right]$= $\\left[\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{1}\\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\ldots \\left[\\begin{array}{c}u_{1} \\\\ u_{2} \\\\ \\vdots \\\\ u_{n}\\end{array}\\right] v_{2} \\right]$ = $\\left[\\begin{array}{cccc}u_{1} v_{1} & u_{1} v_{2} & \\cdots & u_{1} v_{n} \\\\ u_{2} v_{1} & u_{2} v_{2} & \\cdots & u_{2} v_{n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ u_{n} v_{1} & u_{n} v_{2} & \\cdots & u_{n} v_{n}\\end{array}\\right]$\n",
        "\n",
        "> $|0\\rangle\\langle 0| =$ $\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$\n",
        "\n",
        "* A measurement outcome is actually a projection (to the first basis vector)\n",
        "\n",
        "* If we want to model that we get the outcome 0, then we take the corresponding projection | 0 X 0 | and we apply it on the quantum state: | 0 X 0 | $\\Phi$>. This is how you pull out samples from a quantum state and how you apply measurement to this particular probability distribution\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_307.jpg)\n",
        "\n",
        "From this video: https://youtu.be/U6fn5LvevEE\n",
        "\n",
        "**Example: if you want to measure with which probability the quantum system is in state 0, you apply $|0\\rangle \\langle0|$ operator which is $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$, because: $|0\\rangle\\langle 0|=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\left[\\begin{array}{ll}1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right]$**\n",
        "\n",
        "* \"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "\"Projector\", \"projection\" and \"projection operator\" are the same thing. In quantum mechanics, one usually defines a projection operator as\n",
        "\n",
        "> $\n",
        "\\hat{P}=|\\psi\\rangle\\langle\\psi|\n",
        "$\n",
        "\n",
        "This operator then acts on quantum states (vectors) $|\\Psi\\rangle$ as\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=|\\psi\\rangle\\langle\\psi \\mid \\Psi\\rangle=\\langle\\psi \\mid \\Psi\\rangle|\\psi\\rangle\n",
        "$\n",
        "\n",
        "This is exactly the same as the projector you defined in matrix form, since **we can think of $|\\psi\\rangle\\langle\\psi|$ as the diagonal components of a matrix**. For example, if $|\\Psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ and $|\\psi\\rangle=|0\\rangle$, we would find that the projector projects out a particular state\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\alpha|0\\rangle\n",
        "$\n",
        "\n",
        "In matrix form this would be exactly the same as what you defined, since now\n",
        "\n",
        "> $\n",
        "\\hat{P}=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right), \\quad|\\Psi\\rangle=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And now\n",
        "\n",
        "> $\n",
        "\\hat{P}|\\Psi\\rangle=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{array}\\right)=\\left(\\begin{array}{l}\n",
        "\\alpha \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "https://physics.stackexchange.com/questions/394258/what-is-the-standard-definition-of-projector-projection-and-projection-ope\n",
        "\n",
        "The density matrix is a representation of a linear operator called the density operator. The density matrix is obtained from the density operator by choice of basis in the underlying space. In practice, the terms density matrix and density operator are often used interchangeably.\n",
        "\n",
        "In operator language, a density operator for a system is a positive semidefinite, Hermitian operator of trace one acting on the Hilbert space of the system. This definition can be motivated by considering a situation where a pure state $\\left|\\psi_{j}\\right\\rangle$ is prepared with probability $p_{j}$, known as an ensemble. The probability of obtaining projective measurement result $m$ when using projectors $\\Pi_{m}$ is given by\n",
        "\n",
        "> $\n",
        "p(m)=\\sum_{j} p_{j}\\left\\langle\\psi_{j}\\left|\\Pi_{m}\\right| \\psi_{j}\\right\\rangle=\\operatorname{tr}\\left[\\Pi_{m}\\left(\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\\right)\\right]\n",
        "$\n",
        "\n",
        "which makes the density operator, defined as\n",
        "\n",
        "> <font color=\"red\">$\n",
        "\\rho=\\sum_{j} p_{j}\\left|\\psi_{j}\\right\\rangle\\left\\langle\\psi_{j}\\right|\n",
        "$\n",
        "\n",
        "= probability of getting a state |0> for example with outer product / density matrix $\\hat{P}=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$ PLUS probability of getting state |1> etc."
      ],
      "metadata": {
        "id": "5bWbx2kRFnsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">*Projection Operator (Outer Product) & Eigenvalues in Systems of Linear Equations (HHL)*\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "> $A|x\\rangle=|b\\rangle \\quad$ (System of linear equations in a quantum state)\n",
        "\n",
        "And we want to find this:\n",
        "\n",
        ">$\n",
        "|x\\rangle=A^{-1}|b\\rangle \\quad \\text { (the solution is: }|x\\rangle=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1} b_{j}\\left|u_{j}\\right\\rangle \\text { ) }\n",
        "$\n",
        "\n",
        "We need to find the inverse matrix $A^{-1}$. We can get the matrix inverse via eigendecomposition. Since $A$ is Hermitian (normal!), it has a spectral decomposition:\n",
        "\n",
        "> <font color=\"blue\">$\n",
        "A=\\sum_{j=0}^{N-1} \\lambda_{j}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|, \\quad \\lambda_{j} \\in \\mathbb{R}\n",
        "$\n",
        "\n",
        "(if I want to know get the Eigenvector from a measurement, I create the outer product / density matrix of the Eigenvector like above. this is like a measurement)\n",
        "\n",
        "where $\\left|u_{j}\\right\\rangle$ is the $j^{t h}$ eigenvector of $A$ with respective eigenvalue $\\lambda_{j}$. Then,\n",
        "\n",
        "> $\n",
        "A^{-1}=\\sum_{j=0}^{N-1} \\lambda_{j}^{-1}\\left|u_{j}\\right\\rangle\\left\\langle u_{j}\\right|\n",
        "$"
      ],
      "metadata": {
        "id": "MXGA0EYlF9iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Case 1 - Bra-Ket (Inner Product)**: *I want to know the Eigenvalue of an observable (but I don't know which is the most probable Eigenvector)? Then I apply an operator to the quantum state (position operator, momentum operator etc). I will get the most probable Eigenvector / Eigenstate with an according Eigenvalue*\n",
        "\n",
        "> $\\hat{A}\\left|u_{n}\\right\\rangle=\\lambda_{n}\\left|u_{n}\\right\\rangle \\longrightarrow P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$\n",
        "\n",
        "= apply a certain operator (=observable) on a quantum state and you get the Eigenvalue of this state (here u can also be considered the eigenstate / eigenvector)\n",
        "\n",
        "<font color=\"blue\">**Case 2 - Ket-Bra (Outer Product)**: *I want to measure a specific Eigenstate / Eigenvector to get its probability? Then I take the outer product / density matrix of the desired outcome and make a measurement on the quantum state / system (=project it onto the desired Eigenvector)*\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m} c_{m}\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "*  mit $c_{m}=\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "> $|\\psi\\rangle=\\sum_{m}\\left\\langle u_{m} \\mid \\psi\\right\\rangle\\left|u_{m}\\right\\rangle$\n",
        "\n",
        "* here you can re-arrange and get an outer product (true?):\n",
        "\n",
        "> $\n",
        "|\\psi\\rangle=\\sum_{m}\\left|u_{m}\\right\\rangle\\left\\langle u_{m} \\mid \\psi\\right\\rangle$\n",
        "\n",
        "= create outer product of a certain eigenvector to take a measurement of it\n",
        "\n",
        "\n",
        "* Passt zu weiter unten (video prof $m$ mit projection operators):\n",
        "\n",
        "> $\n",
        "(|\\varphi\\rangle\\langle\\psi|)|x\\rangle=|\\varphi\\rangle(\\langle\\psi \\mid x\\rangle)=a|\\varphi\\rangle\n",
        "$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_250.png)\n",
        "\n",
        "Video: [Projection operators in quantum mechanics](https://www.youtube.com/watch?v=M9V4hhqyrKQ)\n",
        "\n",
        "> **Projection Operators project one quantum state on another or onto a subspace of the state space**\n",
        "\n",
        "* for example: when we measure a property of a quantum particle, then the state of the particle collapses onto a different state\n",
        "\n",
        "* the projection operator mathematically describes this collapse\n",
        "\n",
        "> **projection operator associated with a Ket Psi: $| \\Psi \\rangle$ is the outer product of psi with itself: $|\\Psi\\rangle \\langle \\Psi|$**\n",
        "\n",
        "* then we check what it does on an arbirary state phi\n",
        "\n",
        "* the projection operator projects an arbitrary state (here Phi) onto the reference state (here Psi).\n",
        "\n",
        "* the proportionality constant C is given by the overlap between the initial state phi and the state psi that defines the projection operator\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_264.png)\n",
        "\n",
        "Properties of the projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_265.png)\n",
        "\n",
        "Eigenvalues and Eigenstates of Projection operator:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_266.png)\n",
        "\n",
        "The Projection operator allows us to write any Ket as a sum of two other Kets:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_267.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_268.png)\n",
        "\n",
        "One common way in which the projection operator is used in quantum mechanics is to project onto a subspace of the whole state space.\n",
        "\n",
        "* Most convenient way to write this down is in terms of basis states of our state space.\n",
        "\n",
        "* For a basis u we consider a subset of basis states u1, to un which soon an n-dimensional subspace of the full state space.\n",
        "\n",
        "* We then write the projection operator onto this n-dimensional subspace as pn\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_269.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_270.png)\n",
        "\n",
        "**Projective measurement (PVM - Projection-valued measure)**\n",
        "\n",
        "* See [this article](https://en.m.wikipedia.org/wiki/Measurement_in_quantum_mechanics#Projective_measurement) and longer article on [PVM](https://en.m.wikipedia.org/wiki/Projection-valued_measure)\n",
        "\n",
        "* The eigenvectors of a von Neumann observable form an orthonormal basis for the Hilbert space, and each possible outcome of that measurement corresponds to one of the vectors comprising the basis. A density operator is a positive-semidefinite operator on the Hilbert space whose trace is equal to 1.\n",
        "\n",
        "> <font color =\"red\">**For each measurement that can be defined, the probability distribution over the outcomes of that measurement can be computed from the density operator. The procedure for doing so is the Born rule, which states that**</font>\n",
        "\n",
        "> $\n",
        "P\\left(x_{i}\\right)=\\operatorname{tr}\\left(\\Pi_{i} \\rho\\right)\n",
        "$\n",
        "\n",
        "* where $\\rho$ is the density operator, and $\\Pi_{i}$ is the [projection operator](https://en.m.wikipedia.org/wiki/Projection_(linear_algebra)) onto the basis vector corresponding to the measurement outcome $x_{i}$.\n",
        "\n",
        "* The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the [expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)) of that observable. For an observable $A$, the expectation value given a quantum state $\\rho$ is\n",
        "\n",
        ">$\n",
        "\\langle A\\rangle=\\operatorname{tr}(A \\rho)\n",
        "$\n",
        "\n",
        "* A density operator that is a rank-1 projection is known as a pure quantum state, and all quantum states that are not pure are designated mixed. Pure states are also known as wavefunctions."
      ],
      "metadata": {
        "id": "qy0sjtMGU9XY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kD-s4t82ez"
      },
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi |$ - Mixed States (Density Matrix)***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Use Case 2: Mixed States (Density Matrix)***\n",
        "\n",
        "Die [Quantenstatistik](https://de.m.wikipedia.org/wiki/Quantenstatistik) wendet zur Untersuchung makroskopischer Systeme die Methoden und Begriffe der klassischen statistischen Physik an und berücksichtigt zusätzlich die quantenmechanischen Besonderheiten im Verhalten der Teilchen.\n",
        "\n",
        "Wie die Quantenmechanik berücksichtigt auch die Quantenstatistik die folgende doppelte Unkenntnis:\n",
        "\n",
        "> 1. Kennt man den Zustand eines Systems genau – liegt also ein **reiner Zustand (pure state)** vor – und ist dieser kein Eigenzustand der Observablen, so kann man den Messwert einer Einzelmessung dennoch nicht exakt vorhersagen.\n",
        "\n",
        "* A pure state can be written in terms of a Ket state $|\\psi\\rangle$ $\\doteq$ $\\left[\\begin{array}{l}a_{0} \\\\ a_{1}\\end{array}\\right]$\n",
        "\n",
        "  * <font color =\"blue\">$|\\psi\\rangle$</font> $=\\sum_{i} c_{i}\\left|\\varphi_{i}\\right\\rangle$\n",
        "\n",
        "  * Spin up or spin down = **pure states**, can be written as a single wave function $\\mid \\psi\\rangle$.\n",
        "\n",
        "> 2. Kennt man den Zustand des Systems nicht genau, so muss von einem **gemischten Zustand (mixed state)** ausgegangen werden. Ebenso: when one wants to describe a physical system which is entangled with another, as its state can not be described by a pure state. For mixed states with noise in qubits.\n",
        "\n",
        "* A mixed state can be described with a density matrix:\n",
        "\n",
        "  * <font color =\"blue\">$\\rho$ </font>$=\\sum_{i} p_{i}\\left|\\varphi_{i} \\times \\varphi_{i}\\right|$\n",
        "  * Sum over probabilities times the corresponding projection operators onto certain basis states $\\varphi_{i}$\n",
        "\n",
        "  * It allows for the calculation of the probabilities of the outcomes of any measurement performed upon this system, using the Born rule.\n",
        "\n",
        "**There are two more ways to distinguish a pure state from a mixed state:**\n",
        "\n",
        "1. Take the trace of the square of the density matrix:\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]=1 \\rightarrow$ Pure state (like Summe der Eigenwerte entlang der Spur: 1-0-0-0..)\n",
        "\n",
        "  * $\\operatorname{Tr}\\left[\\rho^{2}\\right]< 1 \\rightarrow$ Mixed state. <font color=\"red\">But in the examples below the trace is always = 1? (and some off-diagonal elements)</font>\n",
        "\n",
        "2. Geometricaly, pure states lie on the surface of the Blochsphere. Mixed states are confined within the Bloch sphere.\n",
        "\n",
        "> $\n",
        "\\begin{array}{c|c|c}\n",
        "& \\begin{array}{c}\n",
        "\\text { Pure } \\\\\n",
        "\\text { State }\n",
        "\\end{array} & \\begin{array}{c}\n",
        "\\text { Mixed } \\\\\n",
        "\\text { State }\n",
        "\\end{array} \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Density } \\\\\n",
        "\\text { Matrix } \\\\\n",
        "|\\psi\\rangle\\langle\\psi|\n",
        "\\end{array} & \\color{green} ✔ & \\color{green}✔ \\\\\n",
        "\\hline \\begin{array}{c}\n",
        "\\text { Wave } \\\\\n",
        "\\text { Function } \\\\\n",
        "|\\Psi\\rangle\n",
        "\\end{array} & \\color{green}✔ & \\color{red} ❌\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Why do we need this alternative formalism, these density matrices?**\n",
        "\n",
        "* To illustrate the difference, think about this Ket $|\\psi\\rangle$, which is the equal superposition of 0 and 1. We can write out the vector form $\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right]$. And if we write the corresponding $\\rho$, it will have 0.5 for every element in the matrix.\n",
        "\n",
        "> $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+(1\\rangle)=\\left[\\begin{array}{l}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right] \\rightarrow $$\\rho=\\left[\\begin{array}{ll}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* On the other hand if we create the uniform distribution over the density matrix corresponding to the 0-Ket and the density matrix corresponding to the 1-Ket, then this density matrix will be different:\n",
        "\n",
        "> $\\rho^{\\prime}=\\frac{1}{2}(|0 \\times 0|+|1 \\times 1|)=\\left[\\begin{array}{ll}0.5 & 0 \\\\ 0 & 0.5\\end{array}\\right]$\n",
        "\n",
        "* It doesn't have off-diagonal elements. These off-diagonal elements are critical for many quantum operations, sometimes also called 'coherences'. This is then called a maximally mixed state = equivalent of a uniform distribution in classical probability theory. This means we have absolutely no predictive power of what's going to happen next. Entropy of the state is maximal.\n",
        "\n",
        "* Ideally we want quantum states with a high coherence, but in reality noise affects, and these coherences disappear.\n",
        "\n",
        "https://www.youtube.com/watch?v=BE8RxAESx5I&list=PLBn8lN0Dcvpla6a6omBni1rjyQJ4CssTP&index=47\n"
      ],
      "metadata": {
        "id": "XKh8o95dlsKS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiVIYzwp82e1"
      },
      "source": [
        "**Explanation 1: Density Matrix for Mixed States**\n",
        "\n",
        "*Source here point 2.1: https://qiskit.org/textbook/ch-quantum-hardware/density-matrix.html*\n",
        "\n",
        "* Let's consider the case where we initialize a qubit in the |0⟩ state, and then apply a Hadamard gate.\n",
        "\n",
        "* **Now, unlike the scenario we described for pure states, this Hadamard gate is not ideal: Due to errors in the quantum-computer hardware, only 80% of the times the state is prepared**, this Hadamard gate produces the desired state:\n",
        "\n",
        "> $\\left|\\psi_{1}\\right\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "* The remaining $20 \\%$ of the times, the pulse applied to rotate the state is either too short or too long by $\\frac{\\pi}{6}$ radians about the $x$-axis. This means that when we use this Hadamard gate, we could end up with following two undesired outcome states:\n",
        "\n",
        ">$\n",
        "\\left|\\psi_{2}\\right\\rangle=\\frac{\\sqrt{3}}{2}|0\\rangle+\\frac{1}{2}|1\\rangle, \\quad\\left|\\psi_{3}\\right\\rangle=\\frac{1}{2}|0\\rangle+\\frac{\\sqrt{3}}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "* The figure below shows the Bloch representation for the three possible states our qubit could take if the short pulse happens 10% of the time, and the long pulse the remaining 10% of the time:\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_198.png)\n",
        "\n",
        "* Since we do not know the outcome of our qubit everytime we prepare it, we can represent it as a mixed state of the form:\n",
        "\n",
        "**Step 1: how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:**\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "in this case:\n",
        "\n",
        "> $\n",
        "\\rho_{H}=\\frac{4}{5}\\left|\\psi_{1}\\right\\rangle\\left\\langle\\psi_{1}\\left|+\\frac{1}{10}\\right| \\psi_{2}\\right\\rangle\\left\\langle\\psi_{2}\\left|+\\frac{1}{10}\\right| \\psi_{3}\\right\\rangle\\left\\langle\\psi_{3}\\right|$\n",
        "\n",
        "* Here, the factors $\\frac{4}{5}, \\frac{1}{10}$ and $\\frac{1}{10}$ correspond to the classical probabilities of obtaining the states $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle$ and $\\left|\\psi_{3}\\right\\rangle$, respectively.\n",
        "\n",
        "**Step 2: Add the matrices together**: By replacing each of these three possible state vectors into  $\\rho$ , we can find the density matrix that represents this mixture:\n",
        "\n",
        "> $\\rho_{H}=\\frac{4}{5}\\left[\\begin{array}{ll}\\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{3}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{1}{4}\\end{array}\\right]+\\frac{1}{10}\\left[\\begin{array}{cc}\\frac{1}{4} & \\frac{\\sqrt{3}}{4} \\\\ \\frac{\\sqrt{3}}{4} & \\frac{3}{4}\\end{array}\\right]$\n",
        "$\\rho_{H}=\\left[\\begin{array}{cc}\\frac{1}{2} & \\frac{\\sqrt{3}}{20}+\\frac{2}{5} \\\\ \\frac{\\sqrt{3}}{20}+\\frac{2}{5} & \\frac{1}{2}\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWPEnYF82e2"
      },
      "source": [
        "**Explanation 2: Density Matrix for Mixed States**\n",
        "\n",
        "*Source: Parth G: https://www.youtube.com/watch?v=ZAOc4eMTQiw*\n",
        "\n",
        "* **Pain Point**:  when we see this notation for two states spin up and spin down: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$ we think of it as being in a superposition of both states. But sometimes we don't know if the system is actually in this superposition, or not already collapsed to one.\n",
        "\n",
        "* So, in practice before we measure a system, it can be in following three states, because we lack information about it. This is a mixed state:\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\uparrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\mid \\downarrow>$\n",
        "\n",
        "  * 33% probability: $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}|\\uparrow\\rangle+\\frac{1}{\\sqrt{2}}|\\downarrow\\rangle$\n",
        "\n",
        "* **Mixed state**: lack of information about the system, hence for example giving equal probability to each possible outcome.\n",
        "\n",
        "* **When dealing with a mixed state we need to represent it with what's known as a density operator or density matrix**.\n",
        "\n",
        "* Wave function notation $|\\psi\\rangle$ can only be used to describe pure states. But density matrices $|\\psi\\rangle\\langle\\psi|$ can be used to describe mixed and pure states.\n",
        "\n",
        "* and how do we get the density matrix? - Take the column vector, turn it into a row vector, and then multiply them both:\n",
        "\n",
        "> $(\\hat{\\rho})$ $=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\times\\left(\\begin{array}{ll}1 & 0\\end{array}\\right)$ = $\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 0\\end{array}\\right)$\n",
        "\n",
        "On top of that you can even add several mixed states to get one final mixed states. Mixed state: our knowledge of the system is limited. It could be either one of multiple psi states.\n",
        "\n",
        "* Image 1: We have to add up the density matrices of each possible spin state whilst making sure that we weight it with the probability of it being in that psi state\n",
        "\n",
        "* Image 2: That would give us the final density matrix, which is the mixture in the mixed state.\n",
        "\n",
        "* Image 3: it is for this reason that there is no way to write a mixed state as a single wave function or a single vector. We have to deal with matrices that represent the different pure states in which our system could be. And each of these density matrices has to be weighted by how likely it is that our system is on that pure that.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_006a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXuS8c94Iay"
      },
      "source": [
        "###### ***Ket-Bra $|\\psi\\rangle \\langle \\psi |$ - Expectation Values***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An observable in quantum mechanics is represented by a Hermitian operator. The spectral norm (or operator norm) of a Hermitian operator is equal to its largest eigenvalue in absolute value."
      ],
      "metadata": {
        "id": "5cVUHgYvHoiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**<u>Density matrix formalism</u> to calculate Expectation Values**\n",
        "\n",
        "\n",
        "* [Expectation value](https://en.m.wikipedia.org/wiki/Expectation_value_(quantum_mechanics)): The average of the eigenvalues of a von Neumann observable, weighted by the Born-rule probabilities, is the expectation value of that observable. What is the average value of quantum measurement outcomes?\n",
        "\n",
        "* Large number of particle interactions when we are trying to measure a total effect from an aggregation of subatomic particles: run an experiment involving the observation of energy levels in a type of atom N times under fixed conditions. The expectation value e of the experiment turns out to be an illegitimate energy level for that atom, but E = Ne will give the correct total energy assuming N is large enough.\n",
        "\n",
        "* Expectation value of an observable $Q$ (like the momentum of a particle) in a (normalized) state $|\\Psi\\rangle$ gives the average of all possible values (weighted by their corresponding probabilities) that one may expect to observe in an experiment designed to measure $Q$ in state $|\\Psi\\rangle$ over many experiments where the average of all values of $Q$ will approach the expectation value $\\langle\\Psi|Q| \\Psi\\rangle$ (one set of measurement perturbs the system)"
      ],
      "metadata": {
        "id": "NWbNfdo0aS0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**First: The expectation value of an operator $Q$ is given by $\\langle Q\\rangle=\\langle\\psi|Q| \\psi\\rangle$ = $\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$(spectral decomposition)**</font>\n",
        "\n",
        "* Given a state vector for a pure state $|s\\rangle$, with its complex-conjugate transpose is $\\langle s|$, the operator whose expectation value you are evaluating could be written $|A|$ and $|s\\rangle$ has a component which is a $\\lambda$-eigenstate for $A$.\n",
        "\n",
        "\n",
        "* A mixture of states is a weighted average of such operators, and its expectation value is the weighted average of the expectation values of the pure states.\n",
        "\n",
        "* Suppose, $\\left|\\psi_{1}\\right\\rangle,\\left|\\psi_{2}\\right\\rangle, \\ldots$ be the orthonormal basis of eigenstates of observable $Q$ with corresponding eigenvalues $\\lambda_{1}, \\lambda_{2}, \\ldots$ respectively. One can expand the given (normalized) state $|\\Psi\\rangle$ in the above basis as\n",
        "\n",
        "> $\n",
        "|\\Psi\\rangle=\\alpha_{1}\\left|\\psi_{1}\\right\\rangle+\\alpha_{2}\\left|\\psi_{2}\\right\\rangle+\\cdots+\\alpha_{n}\\left|\\psi_{n}\\right\\rangle+\\ldots\n",
        "$\n",
        "\n",
        "* According to a basic postulate of Quantum mechanics the above expansion implies that upon measuring $Q$ in state $|\\Psi\\rangle$ the outcome of the experiment will be eigenvalue $\\lambda_{1}$ with probability $\\left|\\alpha_{1}\\right|^{2}$, eigenvalue $\\lambda_{2}$ with probability $\\left|\\alpha_{2}\\right|^{2}$ and so on. The (weighted) average of all possible values of $Q$ that can be observed in state $|\\Psi\\rangle$ will be the expectation value of $Q$ in state $|\\Psi\\rangle$:\n",
        "\n",
        "> <font color=\"blue\">$\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$ = $\\left|\\alpha_{1}\\right|^{2} \\lambda_{1}+\\left|\\alpha_{2}\\right|^{2} \\lambda_{2}+\\ldots =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots| \\, | \\lambda_{1} \\alpha_{1} \\psi_{1}+\\lambda_{2} \\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\left\\langle\\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots|Q| \\alpha_{1} \\psi_{1}+\\alpha_{2} \\psi_{2}+\\ldots\\right\\rangle =\\langle\\Psi|Q| \\Psi\\rangle$\n",
        "\n",
        "* This is because operator $Q$ can be written as a sum over its Eigenvalues $a_i$ times projection operators onto its Eigenstates.\n",
        "\n",
        "> $Q=\\sum_{i} \\lambda_{n} \\left| u_{n} \\rangle \\langle \\ u_{n}\\right|$ $\\quad$ with $Q\\left| u_{n}\\right\\rangle= \\lambda_{n}\\left| u_{n}\\right\\rangle$\n",
        "\n",
        "* By using decomposition in the definition of the expectation value, we get this result:\n",
        "\n",
        "> $\\langle Q\\rangle_{\\psi}=\\langle\\psi|Q| \\psi\\rangle=$ $\\sum_{i} \\lambda_{n}\\langle\\psi| u_{n} \\rangle $ <font color =\"orange\">$\\langle u_{u}| \\psi \\rangle$</font> $=\\sum_{i} \\lambda_{n}$ <font color =\"blue\">$\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "* We sum over all Eigenvalues, which are the possible outcomes of a measurement, weighted by the probabilities that this particular Eigenvalue occurs if we start in the state $\\Psi$.\n",
        "\n",
        "  * <font color =\"orange\">$\\langle u_{n} | \\psi\\rangle$</font> - this is called the **transition amplitude**\n",
        "\n",
        "  * <font color =\"blue\">$|\\langle u_{n} | \\psi\\rangle|^{2}$</font> - this is called the **transition probability** (absolute square of the transition amplitude)\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ and since: $P\\left(\\lambda_{n}\\right)=\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}=\\left|c_{n}\\right|^{2}$:\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|c_{n}\\right|^{2}$</font>\n",
        "\n",
        "> $\\langle\\hat{A}\\rangle$ $=\\sum_{n} \\lambda_{n} \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$</font>\n",
        "\n",
        "\n",
        "> $|\\psi\\rangle \\longrightarrow\\langle\\hat{A}\\rangle_{\\psi}=\\langle\\hat{A}\\rangle=\\sum_{n} \\lambda_{n} P\\left(\\lambda_{n}\\right)$ <font color =\"blue\">$=\\sum_{n} \\lambda_{n}\\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$\n",
        "\n",
        "* <font color =\"blue\">$\\rightarrow$ die Summe der Eigenwerte $\\lambda_{n}$ jeweils multipliziert mit der Wahrscheinlichkeit $P\\left(\\lambda_{n}\\right) = \\left|\\left\\langle u_{n} \\mid \\psi\\right\\rangle\\right|^{2}$, den jeweiligen Eigenwert zu erhalten = Expectation Value</font>\n",
        "\n",
        "* Beispielrechnung mit interessantem Ergebnis: Eigenwert -1 mit Amplitude 0.25 und Eigenwert +1 mit Amplitude 0.25\n",
        "\n",
        "  * = $(-1) * (0.25)^2$ + $(+1) * (0.25)^2$\n",
        "\n",
        "  * = $(-1) * (0,5)$ + $(+1) * (0.5)$ = -0.5 + 0.5\n",
        "\n",
        "  * = 0 $\\rightarrow$ Expectation value, aber kein erlaubtes physikalisches Ergebnis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Im0HXIwpZSfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Second: The expectation value $\\langle \\psi|A| \\psi\\rangle$ can be evaluated as the trace of $|\\psi\\rangle\\langle \\psi| A$.**</font>\n",
        "\n",
        "* The operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1, and the operator $|\\psi\\rangle\\langle \\psi|$ is an operator of trace 1.\n",
        "\n",
        "* $\\rho = |\\psi\\rangle \\langle \\psi|$ is an operator of trace=1. We can calculate the expectation value of $A$ by taking the trace over $\\rho$ times $A$:\n",
        "\n",
        "> **$\\langle A\\rangle_{\\rho}:= \\langle \\psi|A| \\psi\\rangle = \\operatorname{Tr}[\\rho A] =\\operatorname{Tr}[\\, |\\psi\\rangle \\langle \\psi| \\, A]$**</font>\n",
        "\n",
        "* Taking the trace over an operator means to sum over the matrix elements of that operator between states of a complete basis $\\operatorname{Tr}[B]=\\sum_{i}\\left\\langle\\eta_{i}|B| \\eta_{i}\\right\\rangle$. We are adding $\\rho = |\\psi\\rangle \\langle \\psi|$ and move the two complex numbers $\\eta$, since they are not operators:\n",
        "\n",
        "> $\\langle A\\rangle_{\\rho}:=\\operatorname{Tr}[\\rho A]$ = $\\sum_{i} {\\left\\langle\\eta_{i}\\right| \\psi \\rangle}{\\left\\langle\\psi|A| \\eta_{i}\\right\\rangle}$\n",
        "\n",
        "* We sum over the basis states. Since $| \\eta_{i} \\rangle \\langle \\eta_{i} |$ = 1, we get $\\langle\\psi|A| \\psi\\rangle$, which is the same result from working with density matrices:\n",
        "\n",
        "> $\\sum_{i}\\left\\langle\\psi|A| \\eta_{i} \\rangle \\langle \\eta_{i} | \\psi\\right\\rangle$ = $\\langle\\psi|A| \\psi\\rangle=\\langle A\\rangle_{\\psi}$\n"
      ],
      "metadata": {
        "id": "taewaWhtaMqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_195.png)"
      ],
      "metadata": {
        "id": "lX-bFe39aNwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_201.png)"
      ],
      "metadata": {
        "id": "RyU5akyieNvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Eigenvalues, Observables & Operators*"
      ],
      "metadata": {
        "id": "gXZXzSSsrrUL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpO0HLIBkdmU"
      },
      "source": [
        "> <font color=\"blue\">**<u>Postulate II of quantum mechanics</u>: a physical quantity $\\mathcal{A}$ is associated with a hermitian operator $\\hat{A}$, that is called an observable.**\n",
        "\n",
        "> <font color=\"blue\">**<u>Postulate III of quantum mechanics</u>: The result of a measurement of a physical quantity is one of the Eigenvalues of the associated observable.**\n",
        "\n",
        "Video: [Eigenvalues and eigenstates in quantum mechanics](https://www.youtube.com/watch?v=p1zg-c1nvwQ)\n",
        "\n",
        "> We consider the action of $\\hat{A}$ on a special Ket $|\\Psi\\rangle$ such that the only way in which $\\hat{A}$ changes $|\\Psi\\rangle$ is by scaling it by a constant and we obtain $\\lambda |\\Psi\\rangle$\n",
        "\n",
        "The Eigenvector of an operator are those special directions in the vector space which the operator doesn't change.\n",
        "\n",
        "The probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Operator_(physics)#Operators_in_quantum_mechanics\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_257.png)\n",
        "\n",
        "> [C$*$-algebras](https://en.m.wikipedia.org/wiki/C*-algebra) were first considered primarily for their use in quantum mechanics **to model algebras of physical observables**. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. See also [Quantum Group](https://en.m.wikipedia.org/wiki/Quantum_group).\n",
        "\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_251.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Anti-)Commutators, Quantum Numbers und \"Vollständiger Satz kommutierender Observablen\"**\n",
        "\n",
        "Um einen quantenmechanischen Zustand eindeutig zu charakterisieren, sind oft mehrere Observablen notwendig. Beispielsweise ist es beim Wasserstoffatom nicht ausreichend, nur die Energie anzugeben (mittels der Hauptquantenzahl n), sondern es sind zwei weitere Observablen notwendig: der Betrag des Drehimpulses (Quantenzahl l) und die z-Komponente des Drehimpuls (Quantenzahl m). Diese drei Größen bilden dann einen vollständigen Satz kommutierender Observablen.\n",
        "Eine Menge von Observablen A, B, C,... bildet einen v.S.k.O., wenn eine orthonormale Basis des Zustandsraums aus gemeinsamen Eigenvektoren der Observablen existiert, und diese Basis (bis auf einen Phasenfaktor) eindeutig ist.\n",
        "\n",
        "Solch ein Verhalten ist in der Quantenmechanik allerdings eher die Ausnahme. Die meisten Paare von Observablen lassen sich nicht gleichzeitig beliebig genau messen, was eine Konsequenz aus der heisenbergschen Unschärferelation ist. Man spricht dann auch von komplementären Observablen.\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Vollständiger_Satz_kommutierender_Observablen"
      ],
      "metadata": {
        "id": "rRJWyBvzCSAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc9HeAnZvFPv"
      },
      "source": [
        "**Solving: How to find Eigenvalues and Eigenvectors?**\n",
        "\n",
        "* For an arbitrary operator (except the identity operator) it is generally not possible to figure out the eigenvalues and eigenstates simply by inspection of how the operator acts\n",
        "\n",
        "* More general approach needed:\n",
        "\n",
        "\t* consider basis u that is orthonormal.\n",
        "\n",
        "\t* Then we write the Eigenvalue equation in the u basis (just project both sides of the equation onto the basis states u).\n",
        "\n",
        "\t* next we insert the identity operator after the A operator on left and side\n",
        "\n",
        "\t* then write the resolution of the identity in the u basis\n",
        "\n",
        "\t* then take sum on the beginning\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_260.png)\n",
        "\n",
        "* the equation $\\sum_{j}\\left(A_{i j}-\\lambda \\delta_{i j}\\right) c_{j}=0$ is equivalent to the original Eigenvalue equation $\\hat{A}|\\psi\\rangle=\\lambda|\\psi\\rangle$, but now written in the u representation\n",
        "\n",
        "* finding the eigenvalues and eigenvectors now becomes finding the lambda and c in the new equation = \"matrix diagonalization\"\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_261.png)\n",
        "\n",
        "> **All we really need to do quantum mechanics is to get enough practice in diagonalizing matrices**\n",
        "\n",
        "* we have following operator A and state Psi\n",
        "\n",
        "* we want to find the eigenvalues and eigenvectors of the operator A\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_263.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYnM169ZFHeZ"
      },
      "source": [
        "<font color=\"blue\">**Operators acting on Quantum State Vectors: Matrix-Vector-Multiplication (Single Qubit)**\n",
        "\n",
        "*Applying a Hadamard gate to a single qubit (matrix-vector multiplication) - Simple dot product! You get a vector out.*\n",
        "\n",
        "$H |0\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 1 + 0\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right] = |+\\rangle$\n",
        "\n",
        "$H |1\\rangle = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}0 + 1 \\\\ 0 -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right] = |-\\rangle$\n",
        "\n",
        "*Diese Zustände können auch mithilfe der Dirac-Notation als Summen von |0⟩ und |1⟩ erweitert werden:*\n",
        "\n",
        "$|+\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$ weil <font color=\"gray\">wegen $|0\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ und $|1\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ daher:</font> $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "$|-\\rangle=\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$ weil: $\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{ll}1 - 0 \\\\ 0 - 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$\n",
        "\n",
        "*Zusammenfassend, das ist alles das gleiche, sieht aber komplett anders aus:*\n",
        "\n",
        "<font color=\"blue\">$H |0\\rangle$</font> $ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] =\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ 1\\end{array}\\right]$ <font color=\"blue\">$ \\,\\,= |+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "<font color=\"blue\">$H |1\\rangle$</font>$ = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]$ <font color=\"blue\">$ = |-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*$|0\\rangle$ und $|1\\rangle$ stellen hier die Basis dar, in der die Quantenzustaende berechnet werden.* ***Man kann allerdings auch eine andere Basis waehlen, zB $|+\\rangle$ und $|-\\rangle$:***\n",
        "\n",
        "$|0\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle+|-\\rangle)$ vergleiche: <font color=\"blue\">$|+\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle+|1\\rangle)$\n",
        "\n",
        "$|1\\rangle=\\frac{1}{\\sqrt{2}}(|+\\rangle-|-\\rangle)$ vergleiche: <font color=\"blue\">$|-\\rangle$ = $\\frac{1}{\\sqrt{2}}(|0\\rangle-|1\\rangle)$\n",
        "\n",
        "*Applying an Identity Operator:*\n",
        "\n",
        "$I |0\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]= \\left[\\begin{array}{ll}1 + 0 \\\\ 0 + 0\\end{array}\\right]=\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "$I |1\\rangle = \\left(\\begin{array}{cc}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]= \\left[\\begin{array}{ll}0 + 0 \\\\ 0 + 1\\end{array}\\right]=\\left[\\begin{array}{c}0 \\\\ 1\\end{array}\\right]$\n",
        "\n",
        "*Applying a Pauli-X Operator:*\n",
        "\n",
        "$X|0\\rangle=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{ll}0 + 0 \\\\ 1 + 0\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTK-u1Q4nrGv"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Vector-Multiplication (Multi Qubit)**\n",
        "\n",
        "> $A \\mathbf{x}=\\left[\\begin{array}{cccc}a_{11} & a_{12} & \\ldots & a_{1 n} \\\\ a_{21} & a_{22} & \\ldots & a_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m 1} & a_{m 2} & \\ldots & a_{m n}\\end{array}\\right]\\left[\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n}\\end{array}\\right]=\\left[\\begin{array}{c}a_{11} x_{1}+a_{12} x_{2}+\\cdots+a_{1 n} x_{n} \\\\ a_{21} x_{1}+a_{22} x_{2}+\\cdots+a_{2 n} x_{n} \\\\ \\vdots \\\\ a_{m 1} x_{1}+a_{m 2} x_{2}+\\cdots+a_{m n} x_{n}\\end{array}\\right]$\n",
        "\n",
        "**First create the tensor product of two operators**: For construction of the desired two-qubit gate, you need the same tensor product operation as you used for the vectors. Here where $H_1$ is a one-qubit Hadamard gate in the two-qubit space $(\\hat{H} \\otimes \\mathbf{I})$, where Hadamard is applied only to one Qubit:\n",
        "\n",
        "> $H_{1} \\equiv H_{0} \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "**Second, apply the new 'tensored' operator on the tensor product of two state vectors**:\n",
        "\n",
        "See the tensor product of two vectors in state $|0\\rangle$:\n",
        "\n",
        "> $|0\\rangle \\otimes|0\\rangle = \\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\otimes\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=$</font> $\\left[\\begin{array}{l}1\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ 0\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]\\end{array}\\right]=$ <font color=\"blue\">$\\left [\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]$\n",
        "\n",
        "Now applying $H_1$ you mix up the first qubit states and keep the second qubit state unchanged:\n",
        "\n",
        "> $H_{1}(|0\\rangle \\otimes|0\\rangle)=H_{1}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$= <font color=\"blue\">$\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right) \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)$ = $\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right)$</font>= $\\frac{1}{\\sqrt{2}}(|0\\rangle \\otimes|0\\rangle+|1\\rangle \\otimes|0\\rangle)$\n",
        "\n",
        "**Another way of writing this (=apply H to one qubit only in a 1 qubit in a 2 qubit system) for a joint state of two initialized qubits $\n",
        "|0\\rangle \\otimes|0\\rangle\n",
        "$ is:**\n",
        "\n",
        "> <font color=\"orange\">$\\hat{H}\\left|q_{0}\\right\\rangle \\otimes\\left|q_{1}\\right\\rangle$</font> = $\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\left(\\frac{1}{\\sqrt{2}}|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle\\right) \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "The tensor product is distributive, which in this case means it acts much like multiplication:\n",
        "\n",
        ">$\n",
        "\\mathbf{H}|0\\rangle \\otimes|0\\rangle=\\frac{1}{\\sqrt{2}}|0\\rangle \\otimes|0\\rangle+\\frac{1}{\\sqrt{2}}|1\\rangle \\otimes|0\\rangle\n",
        "$\n",
        "\n",
        "See [Tensor_product](https://en.wikipedia.org/wiki/Tensor_product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z7cDFlh_eL1"
      },
      "source": [
        "<font color=\"blue\">**Matrix-Matrix-Multiplication (Kronecker / Tensor)**\n",
        "\n",
        "Zur Kombination von Operators in einem Timestep (zB H bei qubit 1 und I bei Qubit 2). Two systems being described as a joint system.\n",
        "\n",
        "> $\\left[\\begin{array}{ll}a & b \\\\ c & d\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right]=\\left[\\begin{array}{lll}a\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & b\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] \\\\ c\\left[\\begin{array}{ll}e & f \\\\ g & h\\end{array}\\right] & d\\left[\\begin{array}{llll}e & f \\\\ g & h\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{llll}a e & a f & \\text { be } & b f \\\\ a g & a h & b g & b h \\\\ c e & c f & d e & d f \\\\ c g & c h & d g & d h\\end{array}\\right]$\n",
        "\n",
        "$H \\otimes I=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\otimes\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{ll}1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\\\ 1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) & -1\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{rrrr}1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1\\end{array}\\right)$\n",
        "\n",
        "$I \\otimes H=\\left(\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right) \\otimes \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) \\\\ 0\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right) & 1\\left(\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right)\\end{array}\\right)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cccc}1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1\\end{array}\\right)$\n",
        "\n",
        "$Y \\otimes X=\\left[\\begin{array}{cc}0 & -i \\\\ i & 0\\end{array}\\right] \\otimes\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]=\\left[\\begin{array}{ll}0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & -i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\\\ i\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] & 0\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right]\\end{array}\\right]=\\left[\\begin{array}{cccc}0 & 0 & 0 & -i \\\\ 0 & 0 & -i & 0 \\\\ 0 & i & 0 & 0 \\\\ i & 0 & 0 & 0\\end{array}\\right]$\n",
        "\n",
        "$X \\otimes H=\\left[\\begin{array}{ll}0 & 1 \\\\ 1 & 0\\end{array}\\right] \\otimes \\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] \\\\ 1 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right] & 0 \\times\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]\\end{array}\\right]=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cccc}0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & -1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & -1 & 0 & 0\\end{array}\\right] =\\left[\\begin{array}{cc}\n",
        "0 & H \\\\\n",
        "H & 0\n",
        "\\end{array}\\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnohitquaGyf"
      },
      "source": [
        "**<font color=\"blue\">Matrix-Matrix-Multiplication (Usual)**\n",
        "\n",
        "> Used in serially wired gates (so NOT gates in one time step - here we use tensor product to combine them, but serial gates!)\n",
        "\n",
        "A line in the circuit is considered as a quantum wire and basically represents a single qubit. The product of operators keeps the same dimension.\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Serially_wired_quantum_logic_gates.png/500px-Serially_wired_quantum_logic_gates.png)\n",
        "\n",
        "For example, putting the Pauli X gate after the Pauli Y gate, both of which act on a single qubit, can be described as a single combined gate C:\n",
        "\n",
        ">$\n",
        "C=X \\cdot Y=\\left[\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right] \\cdot\\left[\\begin{array}{cc}\n",
        "0 & -i \\\\\n",
        "i & 0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
        "i & 0 \\\\\n",
        "0 & -i\n",
        "\\end{array}\\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "X \\cdot X=\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\\left(\\begin{array}{ll}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right)=I\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89sYf-N0_6u"
      },
      "source": [
        "<font color=\"blue\">**Hamiltonian Operator $\\mathcal{H}$**\n",
        "\n",
        "> <font color=\"red\">**The sum of the possible outcomes of kinetic and potential energy of this entire system in quantum mechanics is referred to the Hamiltonian $\\mathcal{H}$ (to calculate the lowest total energy of a two atom system)**\n",
        "\n",
        "* der [Hamiltonoperator](https://de.m.wikipedia.org/wiki/Hamiltonoperator) (Energieoperator) ist in der Quantenmechanik ein Operator, **der (mögliche) Energiemesswerte und die Zeitentwicklung angibt = describes the total energy of a system or particle**. <font color=\"red\">Er liefert beispielsweise die Energieniveaus des Elektrons im Wasserstoffatom.</font>\n",
        "\n",
        "* In der Quantenmechanik wird jeder Zustand des betrachteten physikalischen Systems durch einen zugehörigen Vektor $\\psi$ im Hilbertraum angegeben. Seine Zeitentwicklung wird nach der Schrödingergleichung durch den Hamiltonoperator $\\hat{H}$ bestimmt:\n",
        "\n",
        ">$\n",
        "\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\psi(t)=\\hat{H} \\psi(t)\n",
        "$\n",
        "\n",
        "* **For every problem there is a different Hamiltonian and a different corresponding Eigenspectrum**. Spektrum: Bereich der möglichen Messwerte. Eigenvalues = stabile Energielevel = Zustande, die die Elektronen in den Orbitalen beschreiben. Siehe auch [Hamilton-Funktion](https://de.m.wikipedia.org/wiki/Hamilton-Funktion).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwSGedvzNov"
      },
      "source": [
        "<font color=\"blue\">**Time Evolution Operator $\\mathcal{U}$ bzw. $\\mathcal{T}$**\n",
        "\n",
        "* Der [Zeitentwicklungsoperator](https://de.m.wikipedia.org/wiki/Zeitentwicklungsoperator) $\\mathcal{U}$ bzw. $\\mathcal{T}$ ist ein quantenmechanischer Operator, mit dem sich die zeitliche Entwicklung eines physikalischen Systems berechnen lässt. Siehe auch [Time evolution](https://en.m.wikipedia.org/wiki/Time_evolution)\n",
        "\n",
        "* Der quantenmechanische Operator ist eng verwandt mit dem [Propagator](https://de.m.wikipedia.org/wiki/Propagator) in der Quantenfeld- oder Vielteilchentheorie. Üblicherweise wird er als $U\\left(t, t_{0}\\right)$ geschrieben und bezeichnet die Entwicklung des Systems vom Zeitpunkt $t_{0}$ zum Zeitpunkt $t$.\n",
        "\n",
        "Der Zeitentwicklungsoperator $U\\left(t, t_{0}\\right)$ wird definiert über die Zeitentwicklung eines beliebigen Zustandes $|\\psi\\rangle$ zu einem Zeitpunkt $t_{0}$ bis zum Zeitpunkt $t$ :\n",
        "\n",
        ">$\n",
        "|\\psi(t)\\rangle=U\\left(t, t_{0}\\right)\\left|\\psi\\left(t_{0}\\right)\\right\\rangle \\quad \\forall|\\psi\\rangle\n",
        "$\n",
        "\n",
        "Einsetzen in die Schrödingergleichung liefert einen Satz gewöhnlicher Differentialgleichungen 1. Ordnung:\n",
        "\n",
        ">$\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} U\\left(t, t_{0}\\right)=H(t) U\\left(t, t_{0}\\right)$\n",
        "\n",
        "Diese Gleichungen sind zur Schrödingergleichung insofern äquivalent, als sie die Erweiterung des Zeitentwicklungsoperators um einen infinitesimalen Zeitschritt $\\delta t$ beschreiben:\n",
        "\n",
        ">$\n",
        "U\\left(t+\\delta t, t_{0}\\right)=\\left(1-\\frac{i}{\\hbar} H(t) \\delta t\\right) U\\left(t, t_{0}\\right)+O\\left(\\delta t^{2}\\right)\n",
        "$\n",
        "\n",
        "mit dem Hamiltonoperator $H$, der den Erzeuger der Zeitentwicklungen darstellt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBWmyGiyVUq"
      },
      "source": [
        "<font color=\"blue\">**Position Operator $\\hat{x}$**\n",
        "\n",
        "Der [Ortsoperator](https://de.m.wikipedia.org/wiki/Ortsoperator) gehört in der Quantenmechanik zur Ortsmessung von Teilchen.\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch gegeben durch den zugehörigen Vektor eines Hilbertraumes $\\mathrm{H}$.\n",
        "\n",
        "* Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathrm{H}$ dargestellt.\n",
        "\n",
        "Speziell ist der Ortsoperator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{x}}=\\left(\\hat{x}_{1}, \\hat{x}_{2}, \\hat{x}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{x}_{j}\\right)=\\left\\langle\\hat{x}_{j} \\Psi, \\Psi\\right\\rangle_{\\mathrm{H}}, \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der j-ten Ortskoordinate des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_j3K_Pef4GA"
      },
      "source": [
        "<font color=\"blue\">**Momentum Operator $\\hat{p}$**\n",
        "\n",
        "Video: [Why Momentum in Quantum Physics is Complex](https://www.youtube.com/watch?v=kG-iihrYCG4&list=WL&index=22)\n",
        "\n",
        "Classical Momentum\n",
        "\n",
        "> p = m * v\n",
        "\n",
        "Quantum Mechanics: apply a measurement operator on wave function (eigenvalue equation:)\n",
        "\n",
        "> $\\hat{p}|\\Psi\\rangle=\\lambda|\\Psi\\rangle$\n",
        "\n",
        "Momentum measurement operator\n",
        "\n",
        "> $\\hat{p}=-i \\hbar \\frac{\\partial}{\\partial x}$ $\\quad$ with: $\\, i=\\sqrt{-1}$\n",
        "\n",
        "Der [Impulsoperator](https://de.m.wikipedia.org/wiki/Impulsoperator) $\\hat{p}$ ist in der Quantenmechanik der Operator zur Impulsmessung von Teilchen. In der Ortsdarstellung ist der Impulsoperator in einer Dimension gegeben durch (mit $\\frac{\\partial}{\\partial x}$ die partielle Ableitung in Richtung der Ortskoordinate $x$):\n",
        "\n",
        ">$\n",
        "\\hat{p}_{x}=-\\mathrm{i} \\hbar \\frac{\\partial}{\\partial x}=\\frac{\\hbar}{i} \\frac{\\partial}{\\partial x}\n",
        "$\n",
        "\n",
        "Mit dem Nabla-Operator $\\nabla$ erhält man in drei Dimensionen den Vektor:\n",
        "\n",
        ">$\n",
        "\\hat{\\mathbf{p}}=-\\mathrm{i} \\hbar \\nabla\n",
        "$\n",
        "\n",
        "* Der physikalische Zustand $\\Psi$ eines Teilchens ist in der Quantenmechanik mathematisch durch einen zugehörigen Vektor eines Hilbertraumes $\\mathcal{H}$ gegeben. Dieser Zustand wird folglich in der Bra-Ket-Notation durch den Vektor $|\\Psi\\rangle$ beschrieben.\n",
        "\n",
        "* Die Observablen werden durch selbstadjungierte Operatoren auf $\\mathcal{H}$ dargestellt. Speziell ist der Impuls-Operator die Zusammenfassung der drei Observablen $\\hat{\\mathbf{p}}=\\left(\\hat{p}_{1}, \\hat{p}_{2}, \\hat{p}_{3}\\right)$, so dass\n",
        "\n",
        ">$\n",
        "E\\left(\\hat{p}_{j}\\right)=\\left\\langle\\Psi\\left|\\hat{p}_{j}\\right| \\Psi\\right\\rangle \\quad j=1,2,3\n",
        "$\n",
        "\n",
        "der Mittelwert (Erwartungswert) der Messergebnisse der $j$ -ten Komponente des Impulses des Teilchens im Zustand $\\Psi$ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">**Translation Operator $\\hat{T}$**\n",
        "\n",
        "* the translation operator is the operator that allows us to move quantum states from one point to another\n",
        "\n",
        "* it allows us to understand many properties of wave functions:\n",
        "\n",
        "> **the wave function in real space is related to the wave function in momentum space by a Fourier transform**\n",
        "\n",
        "> $[\\hat{x}, \\hat{p}]=i \\hbar$\n",
        "\n",
        "* position $\\hat{x}$ and momentum $\\hat{p}$ operators. Their most important property is their commutator which is equal to $i \\hbar$\n",
        "\n",
        "* **Translation operator**:\n",
        "\n",
        "> $\\hat{T}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} \\quad \\alpha \\in \\mathbb{R}$\n",
        "\n",
        "* this is an operator that translates by an amount $\\alpha$\n",
        "\n",
        "* What does it mean to have a **function of an operator**, like the exponential function here: the function of an operator is defined by its Taylor expansion\n",
        "\n",
        "* adjoint of an operator: tells us about what the operator looks like in the dual space (NOT hermitian as you can see):\n",
        "\n",
        "> $\\begin{aligned} \\hat{T}^{t}(\\alpha)=& e^{i \\alpha \\hat{r}^{\\dagger} / \\hbar}=e^{i \\alpha \\hat{p} / \\hbar}=e^{-i(-\\alpha) \\hat{p} / \\hbar}=\\hat{T}(-\\alpha) \\\\ & \\hat{p}^{+}=\\hat{p} \\end{aligned}$\n",
        "\n",
        "* Let's look at the action of T dagger alpha on T alpha:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=\\mathbb{1}$\n",
        "\n",
        "> $[\\hat{p}, \\hat{p}]=0$\n",
        "\n",
        "* remember: in general we cannot combine exponents of operators like if they were numbers, but here we can because the two exponents commute because the P operator commutes with itself\n",
        "\n",
        "* An operator whose adjoint is equal to its inverse is called a unitary operator\n",
        "\n",
        "> $\\left.\\begin{array}{l}\\hat{T}^{\\dagger}(\\alpha) \\hat{T}(\\alpha)=e^{i \\alpha \\hat{p} / \\hbar} e^{-i \\alpha \\hat{p} / \\hbar}=I \\\\ {[\\hat{p}, \\hat{p}]=0} \\\\ \\hat{T}(\\alpha) \\hat{T}^{\\dagger}(\\alpha)=e^{-i \\alpha \\hat{p} / \\hbar} e^{i \\alpha \\hat{p} / \\hbar}=I\\end{array}\\right\\} \\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)$\n",
        "\n",
        "* Overall:\n",
        "\n",
        "> $\\hat{T}^{\\dagger}(\\alpha)=\\hat{T}^{-1}(\\alpha)=\\hat{T}(-\\alpha)$"
      ],
      "metadata": {
        "id": "kHsMw4tA4gRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$\\hookrightarrow$ States*"
      ],
      "metadata": {
        "id": "VHiwFI7ovP7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* phase states\n",
        "* Gibbs quantum states\n",
        "* Matrix product states\n",
        "* pure state, mixed state\n",
        "* Dicke state\n",
        "* Set of non-orthogonal quantum states"
      ],
      "metadata": {
        "id": "0zax9nK6B4zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are the eigenvalues of density matrix of quantum state?**\n",
        "\n",
        "The eigenvalues of the density matrix of a quantum state represent the probabilities of finding the system in the corresponding eigenstates. The sum of all the eigenvalues must be equal to 1, which reflects the fact that the system must be in one of the eigenstates.\n",
        "\n",
        "For a pure state, the density matrix is a projector onto the state vector, and therefore has only one eigenvalue of 1, with the rest being 0. This means that the system is certain to be in the corresponding eigenstate.\n",
        "\n",
        "For a mixed state, the density matrix has at least two non-zero eigenvalues, indicating that the system has a non-zero probability of being in either of the corresponding eigenstates.\n",
        "\n",
        "The eigenvalues of the density matrix can also be used to characterize the degree of mixedness of the state. A state with only one non-zero eigenvalue is a pure state, while a state with many non-zero eigenvalues is a highly mixed state.\n",
        "\n",
        "Here are some examples of the eigenvalues of the density matrix for different quantum states:\n",
        "\n",
        "* **Pure state:**\n",
        "    * Spin-up state of a qubit: $\\lambda_1 = 1, \\lambda_2 = 0$\n",
        "    * Ground state of a harmonic oscillator: $\\lambda_0 = 1, \\lambda_1 = 0, \\lambda_2 = 0, ...$\n",
        "* **Mixed state:**\n",
        "    * Equal mixture of spin-up and spin-down states of a qubit: $\\lambda_1 = \\lambda_2 = 1/2$\n",
        "    * Thermal state of a harmonic oscillator: $\\lambda_0 > \\lambda_1 > \\lambda_2 > ... > 0$\n",
        "\n",
        "The eigenvalues of the density matrix can be used to calculate the expectation values of observables, as well as the probabilities of different measurement outcomes. For example, the probability of measuring the outcome $a$ when measuring the observable $A$ is given by:\n",
        "\n",
        "> $P(a) = \\sum_i \\lambda_i |\\langle a | \\psi_i \\rangle|^2$\n",
        "\n",
        "where $\\lambda_i$ and $\\psi_i$ are the eigenvalues and eigenstates of the density matrix, respectively.\n",
        "\n",
        "The eigenvalues of the density matrix are an important tool for understanding and characterizing quantum states. They are used in a wide variety of quantum applications, including quantum information theory, quantum computation, and quantum statistical mechanics."
      ],
      "metadata": {
        "id": "xxcXDJqv7Ctb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An $n$-dimensional **pure state** is $|\\psi\\rangle=\\sum_{i=1}^n \\alpha_i|i\\rangle$, where $|i\\rangle$ is the $n$-dimensional unit vector that has a 1 only at position $i$, the $\\alpha_i$ 's are complex numbers called the amplitudes, and $\\sum_{i \\in \\mid n]}\\left|\\alpha_i\\right|^2=1$.\n",
        "\n",
        "An $n$-dimensional **mixed state** (or density matrix) $\\rho=\\sum_{i=1}^n p_i\\left|\\psi_i\\right\\rangle \\psi_i \\mid$ is a mixture of pure states $\\left|\\psi_1\\right\\rangle, \\ldots,\\left|\\psi_n\\right\\rangle$ prepared with probabilities $p_1, \\ldots, p_n$, respectively. The eigenvalues $\\lambda_1, \\ldots, \\lambda_n$ of $\\rho$ are non-negative reals and satisfy $\\sum_{i \\in[n]} \\lambda_i=1$.\n",
        "\n",
        "If $\\rho$ is pure (i.e., $\\rho=|\\psi\\rangle \\psi \\mid$ for some $|\\psi\\rangle)$, then one of the eigenvalues is 1 and the others are 0 .\n",
        "\n",
        "To obtain classical information from $\\rho$, one could apply a **POVM (positive-operator-valued measure)** to the state $\\rho$. An $m$-outcome POVM is specified by a set of positive semidefinite matrices $\\left\\{M_i\\right\\}_{i \\in[m]}$ with the property $\\sum_i M_i=I d$. When this POVM is applied to the mixed state $\\rho$, the probability of the $j$-th outcome is given by $\\operatorname{Tr}\\left(M_j \\rho\\right)$."
      ],
      "metadata": {
        "id": "ce-HCRxhCNZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set of non-orthogonal quantum states**\n",
        "\n",
        "Sure, here is an example of a set of non-orthogonal quantum states:\n",
        "\n",
        "```\n",
        "|ψ⟩ = |0⟩ + |1⟩\n",
        "|ϕ⟩ = |0⟩ - |1⟩\n",
        "```\n",
        "\n",
        "These two states are non-orthogonal because their inner product is not zero:\n",
        "\n",
        "```\n",
        "⟨ψ|ϕ⟩ = (|0⟩ + |1⟩) ⟨0| - |1⟩) = 0 + 0 = 0\n",
        "```\n",
        "\n",
        "In other words, it is not possible to distinguish between these two states with certainty.\n",
        "\n",
        "Another example of a set of non-orthogonal quantum states is the set of all qubit states that lie on the Bloch sphere. The Bloch sphere is a unit sphere in three dimensions, and each point on the sphere represents a possible state of a qubit. Any two points on the Bloch sphere that are not diametrically opposite are non-orthogonal.\n",
        "\n",
        "Non-orthogonal quantum states are a fundamental part of quantum mechanics. They play a role in many quantum phenomena, such as quantum entanglement and quantum computing.\n",
        "\n",
        "\n",
        "*When do they occur?*\n",
        "\n",
        "Non-orthogonal quantum states occur whenever two or more quantum states are not perfectly aligned. This can happen in a variety of ways, such as:\n",
        "\n",
        "* When a quantum state is prepared in a superposition of two or more states.\n",
        "* When a quantum state is subjected to a measurement that does not perfectly distinguish between the different states.\n",
        "* When a quantum state is passed through a noisy channel.\n",
        "\n",
        "Non-orthogonal quantum states can be difficult to distinguish, but there are a number of techniques that can be used to do so.\n",
        "\n",
        "* One common technique is to use a projective measurement, which is a measurement that projects the quantum state onto one of a set of mutually orthogonal states. Another technique is to use a quantum algorithm, such as the Shor algorithm, which can be used to distinguish between non-orthogonal states with a high degree of accuracy.\n",
        "\n",
        "* Non-orthogonal quantum states play an important role in many quantum phenomena, such as quantum entanglement and quantum computing. In quantum entanglement, two or more quantum systems are linked together in such a way that they cannot be described independently. This is possible because the quantum states of the entangled systems are non-orthogonal.\n",
        "\n",
        "* In quantum computing, non-orthogonal quantum states are used to store information and to perform calculations. For example, a quantum computer can use non-orthogonal quantum states to represent the solutions to a problem. By performing measurements on these states, the quantum computer can then find the solution to the problem.\n",
        "\n",
        "The study of non-orthogonal quantum states is a rapidly developing field. As our understanding of quantum mechanics grows, we are finding new and exciting ways to use non-orthogonal quantum states to solve problems and to create new technologies."
      ],
      "metadata": {
        "id": "qSrKoh9rBnHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the difference between mixed state and product state?**\n",
        "\n",
        "A mixed state and a product state are two different types of quantum states that arise in quantum mechanics and quantum information theory, each with its own distinct properties and interpretations.\n",
        "\n",
        "1. **Mixed State**: A mixed state describes a statistical mixture of different quantum states, each with a certain probability. Mathematically, a mixed state is represented by a density matrix, which is a positive-semidefinite Hermitian operator with trace 1. If a system is in a mixed state, it means we have some classical uncertainty about which specific quantum state it's in. For example, a system might be in state |0⟩ with probability 1/2 and in state |1⟩ with probability 1/2. This would correspond to the mixed state represented by the density matrix ρ = 1/2|0⟩⟨0| + 1/2|1⟩⟨1|.\n",
        "\n",
        "2. **Product State**: A product state refers to a state of a composite system that can be written as the tensor product of states of its subsystems. For example, if you have a two-qubit system and the first qubit is in state |0⟩ and the second qubit is in state |1⟩, the overall state of the system is the product state |0⟩⊗|1⟩, often written as |01⟩. A product state implies that there are no correlations between the subsystems, i.e., the subsystems are not entangled. If a state of a composite system cannot be written as a tensor product of states of its subsystems, it's called an entangled state.\n",
        "\n",
        "So, in summary, the difference lies in what each state represents: a mixed state represents classical uncertainty about which pure state a system is in, while a product state represents a separable (i.e., non-entangled) state of a composite system. Notably, these are not mutually exclusive: you can have a mixed state of product states, for example."
      ],
      "metadata": {
        "id": "Yyjr8c5B6SuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a product state in quantum mechanics and quantum computing?**\n",
        "\n",
        "A product state in quantum mechanics is a state of a multi-qubit system that can be written as a product of individual qubit states. In other words, the state of each qubit is independent of the state of the other qubits. For example, the following is a product state of a two-qubit system:\n",
        "\n",
        "```\n",
        "|Ψ⟩ = |0⟩ |1⟩\n",
        "```\n",
        "\n",
        "This state represents a system in which the first qubit is in the |0⟩ state and the second qubit is in the |1⟩ state.\n",
        "\n",
        "Product states are typically less entangled than other types of quantum states, such as entangled states. Entanglement is a phenomenon in which the states of two or more qubits are linked together in such a way that they cannot be described independently.\n",
        "\n",
        "Product states are important in quantum computing because they are relatively easy to simulate and manipulate. Many quantum algorithms are designed to operate on product states.\n",
        "\n",
        "Here are some examples of product states in quantum computing:\n",
        "\n",
        "* The state of a quantum register that has been initialized to all zeros:\n",
        "\n",
        "```\n",
        "|Ψ⟩ = |0⟩⊗n\n",
        "```\n",
        "\n",
        "where n is the number of qubits in the register.\n",
        "\n",
        "* The state of a quantum register that has been used to store a classical bitstring:\n",
        "\n",
        "```\n",
        "|Ψ⟩ = |b1⟩⊗|b2⟩⊗...⊗|bn⟩\n",
        "```\n",
        "\n",
        "where bi is either 0 or 1, depending on the ith bit in the classical bitstring.\n",
        "\n",
        "* The state of a two-qubit system that is used to represent a Bell state:\n",
        "\n",
        "```\n",
        "|Ψ⟩ = (|00⟩ + |11⟩)/√2\n",
        "```\n",
        "\n",
        "This state is a product state, but it is also entangled. This is because the two qubits are correlated in such a way that measuring one qubit will instantly reveal the state of the other qubit.\n",
        "\n",
        "Product states are an essential concept in quantum mechanics and quantum computing. They are used in a wide variety of quantum algorithms and applications."
      ],
      "metadata": {
        "id": "9I8HNzzl61pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix product state (MPS)**\n",
        "\n",
        "No, a product state is not the same as a matrix product state (MPS). A product state is a state of a multi-qubit system in which the state of each qubit is independent of the state of the other qubits. An MPS is a more general type of quantum state that can be used to represent a wide variety of entangled and non-entangled states.\n",
        "\n",
        "An MPS is represented by a tensor network, which is a graph of tensors that are connected to each other. The tensors in the network represent the local states of the qubits, and the connections between the tensors represent the interactions between the qubits.\n",
        "\n",
        "MPSs are useful for representing quantum states because they are efficient to simulate and manipulate. This is because the tensor network representation of an MPS can be truncated to a finite size without losing too much accuracy.\n",
        "\n",
        "Here is an example of an MPS for a two-qubit system:\n",
        "\n",
        "```\n",
        "|Ψ⟩ = T_1⊗T_2\n",
        "```\n",
        "\n",
        "where T_1 and T_2 are tensors of dimension d. This MPS represents a state in which the two qubits are entangled.\n",
        "\n",
        "MPSs are used in a variety of quantum algorithms and applications, including:\n",
        "\n",
        "* Quantum simulation\n",
        "* Quantum machine learning\n",
        "* Quantum error correction\n",
        "* Quantum cryptography\n",
        "\n",
        "MPSs are a powerful tool for representing and manipulating quantum states. They are particularly useful for simulating and manipulating large quantum systems, such as those used in quantum computing.\n",
        "\n",
        "To summarize, the key difference between a product state and an MPS is that a product state is a state in which the qubits are independent, while an MPS is a state in which the qubits can be entangled. MPSs are more general than product states, and they are used in a variety of quantum algorithms and applications."
      ],
      "metadata": {
        "id": "bxiVJbIb7KN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**quantum k-uniform state**\n",
        "  * multipartite quantum state whose reduced state on any k parties is maximally mixed = the state is completely uncorrelated between the k parties, and each party has equal probability of being in any of its possible states.\n",
        "  * Quantum k-uniform states are a special class of entangled states that are particularly useful for certain quantum information processing tasks. For example, they can be used to create quantum secret sharing schemes, which allow a group of parties to share a secret without revealing it to any individual party.\n",
        "  * One way to construct quantum k-uniform states is from quantum orthogonal arrays (QOAs). A QOA is a collection of quantum states that are orthogonal to each other when any k of them are considered together. Quantum k-uniform states can be obtained by taking the partial trace of a QOA over any k parties.\n",
        "  * have a number of interesting properties. For example, they are known to be distillable, which means that they can be used to create more entangled states. They are also known to be useful for quantum communication tasks such as teleportation and superdense coding.\n",
        "  Here are some additional things to know about quantum k-uniform states:\n",
        "    * They are a generalization of the well-known Bell states, which are 2-qubit quantum states that are maximally entangled.\n",
        "    * They can be used to create quantum error-correcting codes, which can be used to protect quantum information from errors.\n",
        "    * They can be used to create quantum cryptography protocols, which can be used to securely transmit information over long distances.\n",
        "    * They are still a relatively new area of research, and there is much that is still unknown about them. However, they have the potential to revolutionize the field of quantum information processing."
      ],
      "metadata": {
        "id": "LJQauktmIZFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a Gibbs state?**\n",
        "\n",
        "Learning an unknown Hamiltonian from its Gibbs state has been studied in statistical physics and machine learning [CL68, HS+86, Tan98, AS14] for many decades, known as the “**inverse Ising problem**”.\n",
        "\n",
        "A survey on the complexity of learning quantum states, page 8\n",
        "\n",
        "A Gibbs state is a probability distribution over the states of a system that is consistent with the system's energy and the constraints that are imposed on it. It is named after Josiah Willard Gibbs, who introduced it in his work on statistical mechanics.\n",
        "\n",
        "A Gibbs state can be defined as follows:\n",
        "\n",
        "```\n",
        "P(x) = \\frac{e^{-H(x)/kT}}{\\sum_y e^{-H(y)/kT}}\n",
        "```\n",
        "\n",
        "where $x$ is a state of the system, $H(x)$ is the energy of the system in state $x$, $T$ is the temperature of the system, and $k$ is Boltzmann's constant.\n",
        "\n",
        "The Gibbs state is the equilibrium distribution of the system if the system is allowed to evolve over time according to the laws of statistical mechanics. This means that if the system is initially in any state, it will eventually converge to the Gibbs state as time goes on.\n",
        "\n",
        "Gibbs states are used in many different areas of physics, chemistry, and biology. They are used to describe the equilibrium distribution of molecules in a gas, the distribution of atoms in a crystal, and the distribution of genes in a population.\n",
        "\n",
        "Here are some of the applications of Gibbs states:\n",
        "\n",
        "* In statistical mechanics, Gibbs states are used to calculate the equilibrium properties of systems such as the pressure, temperature, and entropy.\n",
        "* In chemistry, Gibbs states are used to calculate the equilibrium constant for a chemical reaction.\n",
        "* In biology, Gibbs states are used to calculate the equilibrium distribution of genes in a population.\n",
        "* In machine learning, Gibbs states are used to train probabilistic models such as hidden Markov models and Bayesian networks.\n",
        "\n",
        "**can quantum systems also be in states that are not Gibbs states?**\n",
        "\n",
        "\n",
        "Yes, quantum systems can also be in states that are not Gibbs states. This is because quantum systems can exist in superpositions of states, which means that they can be in more than one state at the same time. A Gibbs state is a state where all the possible states of the system are equally likely, but a quantum system can be in a superposition of states where some states are more likely than others.\n",
        "\n",
        "For example, a quantum system with two possible states, $A$ and $B$, can be in a superposition of the two states, such that the system is equally likely to be in state $A$ or state $B$. This is called a coherent state. A Gibbs state for this system would be a state where the system is equally likely to be in state $A$ or state $B$, but a coherent state is not a Gibbs state because it is more likely to be in one state than the other.\n",
        "\n",
        "Another example is a quantum system with three possible states, $A$, $B$, and $C$. The Gibbs state for this system would be a state where the system is equally likely to be in state $A$, state $B$, or state $C$. However, a quantum system can be in a superposition of the three states, such that the system is more likely to be in state $A$ than state $B$ or state $C$. This is called an entangled state. An entangled state is not a Gibbs state because it is not possible to describe the state of the system in terms of the probabilities of the individual states.\n",
        "\n",
        "In general, quantum systems can be in any state that is consistent with the laws of quantum mechanics. This means that they can be in states that are not Gibbs states. Gibbs states are a special type of state that is often used in statistical mechanics, but they are not the only possible state for a quantum system."
      ],
      "metadata": {
        "id": "JcYlKISGf_ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a Gibbs state?**\n",
        "\n",
        "A Gibbs state is a probability distribution over the states of a system that is consistent with the system's energy and the constraints that are imposed on it. It is named after Josiah Willard Gibbs, who introduced it in his work on statistical mechanics.\n",
        "\n",
        "A Gibbs state can be defined as follows:\n",
        "\n",
        "```\n",
        "P(x) = \\frac{e^{-H(x)/kT}}{\\sum_y e^{-H(y)/kT}}\n",
        "```\n",
        "\n",
        "where $x$ is a state of the system, $H(x)$ is the energy of the system in state $x$, $T$ is the temperature of the system, and $k$ is Boltzmann's constant.\n",
        "\n",
        "The Gibbs state is the equilibrium distribution of the system if the system is allowed to evolve over time according to the laws of statistical mechanics. This means that if the system is initially in any state, it will eventually converge to the Gibbs state as time goes on.\n",
        "\n",
        "Gibbs states are used in many different areas of physics, chemistry, and biology. They are used to describe the equilibrium distribution of molecules in a gas, the distribution of atoms in a crystal, and the distribution of genes in a population.\n",
        "\n",
        "Here are some of the applications of Gibbs states:\n",
        "\n",
        "* In statistical mechanics, Gibbs states are used to calculate the equilibrium properties of systems such as the pressure, temperature, and entropy.\n",
        "* In chemistry, Gibbs states are used to calculate the equilibrium constant for a chemical reaction.\n",
        "* In biology, Gibbs states are used to calculate the equilibrium distribution of genes in a population.\n",
        "* In machine learning, Gibbs states are used to train probabilistic models such as hidden Markov models and Bayesian networks.\n"
      ],
      "metadata": {
        "id": "EbBba3xBtVPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**can quantum systems also be in states that are not Gibbs states?**\n",
        "\n",
        "Yes, quantum systems can also be in states that are not Gibbs states. This is because quantum systems can exist in superpositions of states, which means that they can be in more than one state at the same time. A Gibbs state is a state where all the possible states of the system are equally likely, but a quantum system can be in a superposition of states where some states are more likely than others.\n",
        "\n",
        "For example, a quantum system with two possible states, $A$ and $B$, can be in a superposition of the two states, such that the system is equally likely to be in state $A$ or state $B$. This is called a coherent state. A Gibbs state for this system would be a state where the system is equally likely to be in state $A$ or state $B$, but a coherent state is not a Gibbs state because it is more likely to be in one state than the other.\n",
        "\n",
        "Another example is a quantum system with three possible states, $A$, $B$, and $C$. The Gibbs state for this system would be a state where the system is equally likely to be in state $A$, state $B$, or state $C$. However, a quantum system can be in a superposition of the three states, such that the system is more likely to be in state $A$ than state $B$ or state $C$. This is called an entangled state. An entangled state is not a Gibbs state because it is not possible to describe the state of the system in terms of the probabilities of the individual states.\n",
        "\n",
        "In general, quantum systems can be in any state that is consistent with the laws of quantum mechanics. This means that they can be in states that are not Gibbs states. Gibbs states are a special type of state that is often used in statistical mechanics, but they are not the only possible state for a quantum system."
      ],
      "metadata": {
        "id": "TIdqE2X1tZgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dicke States (with Garbage)*\n",
        "\n",
        "Dicke states are a class of quantum states that are defined as the equal-amplitude superposition of all n-qubit computational basis states with Hamming weight k. In other words, a Dicke state |D_n^k⟩ is a quantum state of n qubits where exactly k of the qubits are in the |1⟩ state and the remaining (n-k) qubits are in the |0⟩ state.\n",
        "\n",
        "Dicke states are important for quantum computing for a number of reasons. First, they are a type of entangled state, which means that the qubits in a Dicke state are correlated with each other in a way that cannot be explained by classical physics. This entanglement can be exploited to perform quantum algorithms that are much faster than any possible classical algorithm.\n",
        "\n",
        "Second, Dicke states are relatively easy to prepare on a quantum computer. This is because there are a number of efficient quantum circuits that can be used to create Dicke states. This makes them a good candidate for use in practical quantum computing applications.\n",
        "\n",
        "Third, Dicke states have a number of other applications in quantum information science. For example, they can be used to create quantum memories, quantum communication channels, and quantum simulators.\n",
        "\n",
        "Here are some specific examples of the applications of Dicke states in quantum computing:\n",
        "\n",
        "* **Quantum algorithms:** Dicke states can be used to speed up a variety of quantum algorithms, such as the Shor's algorithm for factoring large numbers and the Grover's algorithm for searching an unsorted database.\n",
        "* **Quantum communication:** Dicke states can be used to create quantum communication channels that are more secure than classical communication channels.\n",
        "* **Quantum simulation:** Dicke states can be used to simulate the behavior of physical systems, such as molecules and materials.\n",
        "\n",
        "Overall, Dicke states are an important class of quantum states with a wide range of applications in quantum computing. They are relatively easy to prepare on a quantum computer and can be used to speed up a variety of quantum algorithms and create secure quantum communication channels."
      ],
      "metadata": {
        "id": "SgkG11YTdwAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dicke states with garbage are a type of quantum state that is used in quantum computing to protect against noise and errors. They are created by adding a number of \"garbage\" qubits to a Dicke state. The garbage qubits are initialized in a random state, and they do not contribute to the computation. However, they help to protect the useful qubits from noise and errors.\n",
        "\n",
        "Here is an example of how Dicke states with garbage can be used to protect against noise and errors. Suppose we want to create a Dicke state of n qubits with k qubits in the |1⟩ state. We can do this by first initializing n qubits in the |0⟩ state. Then, we can add m garbage qubits to the system, where m is much larger than k. We can then initialize the garbage qubits in a random state. Finally, we can apply a Hadamard gate to each of the qubits.\n",
        "\n",
        "The resulting state will be a Dicke state with garbage. The useful qubits will be in the |1⟩ state with high probability, even if the noise and errors are very high. This is because the garbage qubits will absorb the noise and errors, and they will not affect the useful qubits.\n",
        "\n",
        "Dicke states with garbage are a powerful tool for protecting against noise and errors in quantum computing. They are relatively easy to implement, and they can be used to protect a wide range of quantum algorithms."
      ],
      "metadata": {
        "id": "ZLxTRNxIdy3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$\\hookrightarrow$ Measurements*"
      ],
      "metadata": {
        "id": "dcFMdt_xthGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Square root measurement\n",
        "* Separable measurement\n",
        "* Random Clifford measurement (classical shadows based on random Clifford measurements)\n",
        "* POVM measurement\n",
        "* projective measurements\n",
        "* two-copy Bell basis measurement"
      ],
      "metadata": {
        "id": "i-e0Kydw0ZCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square root measurement (pretty good measurement)**\n",
        "\n",
        "The square root measurement (SRM), also known as the pretty good measurement (PGM), is a measurement introduced by Paul Hausladen and William Wootters in 1994. It is a quantum measurement that can be **used to distinguish between a set of non-orthogonal quantum states**.\n",
        "\n",
        "* The SRM is constructed by first finding the square roots of the inner products between the different states. These square roots are then normalized to form the measurement operators.\n",
        "\n",
        "* The SRM has several desirable properties. It is simple to construct, and it is asymptotically optimal for distinguishing between a large number of non-orthogonal states. It is also \"pretty good\" in the sense that it can distinguish between almost orthogonal states with a low probability of error.\n",
        "\n",
        "* The SRM has been used in a variety of applications in quantum computing, including quantum cryptography, quantum communication, and quantum machine learning.\n",
        "\n",
        "Here are some of the key properties of the square root measurement:\n",
        "\n",
        "* It is a projective measurement, which means that it always gives a definite result.\n",
        "* It is unbiased, which means that the probability of each outcome is equal.\n",
        "* It minimizes the probability of error for distinguishing between a set of non-orthogonal states.\n",
        "* It is asymptotically optimal, which means that its performance approaches the optimal as the number of states increases.\n",
        "\n",
        "The square root measurement is a powerful tool that can be used to perform a variety of tasks in quantum computing. It is simple to construct and efficient to implement, and it has been shown to be effective in a variety of applications.\n",
        "\n",
        "**Yes, there is a difference between the square root measurement and the positive-operator-valued measure (POVM).**\n",
        "\n",
        "A square root measurement is a special type of POVM. It is a projective measurement, which means that it always gives a definite result. The square root measurement is constructed by first finding the square roots of the inner products between the different states. These square roots are then normalized to form the measurement operators.\n",
        "\n",
        "A POVM is a more general type of measurement. It is not required to be projective, and it can give a range of possible results. The POVM is defined by a set of operators, called effect operators, that satisfy certain conditions.\n",
        "\n",
        "The main difference between the square root measurement and the POVM is that the square root measurement is always projective, while the POVM is not. This means that the square root measurement always gives a definite result, while the POVM can give a range of possible results.\n",
        "\n",
        "The square root measurement is a more powerful tool than the POVM, but it is also more restrictive. The square root measurement can only be used to distinguish between non-orthogonal states, while the POVM can be used to distinguish between any set of states.\n",
        "\n",
        "In general, the POVM is a more versatile tool than the square root measurement. However, the square root measurement is often a better choice when it is important to obtain a definite result.\n",
        "\n",
        "Here is a table summarizing the key differences between the square root measurement and the POVM:\n",
        "\n",
        "| Feature | Square root measurement | POVM |\n",
        "|---|---|---|\n",
        "| Projective | Yes | No |\n",
        "| Definite result | Always | Not always |\n",
        "| Generality | Less | More |\n",
        "| Versatility | Less | More |\n",
        "| Usefulness | When it is important to obtain a definite result | When it is not important to obtain a definite result or when the states to be distinguished are not non-orthogonal |"
      ],
      "metadata": {
        "id": "c0aVdWf8BlYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://en.m.wikipedia.org/wiki/Weyl%27s_inequality"
      ],
      "metadata": {
        "id": "Z2gY8WGxdr3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a separable measurement?**\n",
        "\n",
        "In quantum mechanics, a separable measurement is a measurement that can be performed on a multipartite quantum state without affecting the entanglement of the state.\n",
        "\n",
        "A multipartite quantum state is a state that describes the joint state of two or more quantum systems. Entanglement is a property of quantum states that allows them to be correlated in ways that are not possible in classical physics.\n",
        "\n",
        "A separable measurement is a measurement that can be performed on each subsystem of a multipartite quantum state independently of the other subsystems. This means that the measurement does not affect the entanglement between the subsystems.\n",
        "\n",
        "For example, consider a system of two qubits, which are quantum systems with two possible states. The state of the system can be described by a density matrix, which is a matrix that contains all the information about the state.\n",
        "\n",
        "A separable measurement on this system would be a measurement that could be performed on each qubit independently. This would mean that the measurement would not affect the entanglement between the qubits.\n",
        "\n",
        "Separable measurements are important in quantum information theory. They can be used to create protocols for quantum communication and computation that are robust to noise and decoherence.\n",
        "\n",
        "Here are some of the key properties of separable measurements:\n",
        "\n",
        "* They can be performed on each subsystem of a multipartite quantum state independently of the other subsystems.\n",
        "* They do not affect the entanglement between the subsystems.\n",
        "* They can be used to create protocols for quantum communication and computation that are robust to noise and decoherence.\n",
        "\n",
        "Separable measurements are a fundamental concept in quantum mechanics. They play an important role in quantum information theory and quantum computing."
      ],
      "metadata": {
        "id": "sr4dD1xctKnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a random Clifford measurement?**\n",
        "\n",
        "In the field of quantum computing, a Clifford measurement involves measuring a quantum state in a basis given by a Clifford operation.\n",
        "\n",
        "The Clifford group is a specific set of operations in quantum mechanics that map Pauli operators to other Pauli operators under conjugation. It includes operations like the Hadamard gate, phase gate, CNOT gate, and others. The Clifford group has important applications in quantum error correction and quantum cryptography.\n",
        "\n",
        "A random Clifford measurement is a measurement process where the measurement basis is chosen randomly from the set of all possible Clifford operations. Random Clifford operations can be used to create a form of \"scrambling\" that can be used to probe the properties of quantum systems, and they also find use in protocols for testing quantum computers, like the so-called randomized benchmarking procedure.\n",
        "\n",
        "When applied to quantum state tomography, random Clifford measurements can be used to extract information about the quantum state in a way that can be more efficient or robust against certain types of errors. For example, they might be used in a protocol for \"shadow tomography\", where a small number of random measurements are used to estimate properties of the quantum state. The specifics of how these measurements are used and their performance characteristics can depend on the details of the quantum system and the tomography protocol."
      ],
      "metadata": {
        "id": "EM6MX6sk-qUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a POVM measurement? (vs projective measurements)**\n",
        "\n",
        "A Positive Operator-Valued Measure (POVM) is a mathematical formalism used in quantum mechanics to describe the process of making a measurement on a quantum system. **Unlike projective measurements (also called von Neumann measurements), which are limited to a specific set of states (the eigenstates of an observable), POVMs can represent more general kinds of measurements.**\n",
        "\n",
        "A POVM is a collection of positive semi-definite operators {E_i} acting on the state space of a quantum system, which sum up to the identity operator, that is, ∑i E_i = I. These operators E_i are also called \"effects\".\n",
        "\n",
        "When you perform a measurement described by a POVM on a quantum system in a state represented by a density matrix ρ, the probability of obtaining outcome i (associated with operator E_i) is given by tr(E_i ρ), where tr() denotes the trace operation.\n",
        "\n",
        "The ***key aspect of POVMs is that they allow for the description of measurements that do not have a complete set of orthogonal eigenstates, which are typical in situations like indirect or incomplete measurements***, measurements involving ancillary systems, and measurements involving quantum entanglement. This makes POVMs an essential tool in quantum information theory and quantum computing.\n",
        "\n",
        "* <font color=\"red\">mathematical examples?\n",
        "\n",
        "* <font color=\"red\">code examples?"
      ],
      "metadata": {
        "id": "E8ZcEaFmtmWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a POVM measurement on the state E(|xi⟩⟨xi|)?**\n",
        "\n",
        "*Von Neumann Measurement vs POVM*\n",
        "\n",
        "A Positive Operator-Valued Measure (POVM) is a type of generalized measurement used in quantum mechanics. When we want to make a measurement in quantum mechanics, we typically talk about observables, which are represented by Hermitian operators, and the associated eigenstates and eigenvalues. The eigenvalues are the possible outcomes of the measurement, and the state collapses into the corresponding eigenstate upon measurement.\n",
        "\n",
        "However, there are situations where a standard von Neumann measurement (which directly associates observables with Hermitian operators) is not sufficient. For example, if we want to consider more general types of measurements or if we're working with a subset of a larger system, we might need to use a POVM.\n",
        "\n",
        "A POVM is a set of positive semi-definite operators {E_i} that sum to the identity operator. Each E_i corresponds to a potential outcome of the measurement. The probability of each outcome i when measuring a state ρ is given by tr(E_i ρ).\n",
        "\n",
        "So if you have a state represented by E(|xi⟩⟨xi|) and a POVM {E_i}, the probability of getting the outcome corresponding to E_i would be tr(E_i E(|xi⟩⟨xi|)).\n",
        "\n",
        "The states E_i themselves don't necessarily need to be orthogonal or even pure states, which makes POVMs a very powerful and general measurement framework in quantum mechanics. They are often used in quantum information theory and quantum computation where complex measurement schemes are more common.\n"
      ],
      "metadata": {
        "id": "f70p5CAjtsMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a two-copy Bell basis measurement?**\n",
        "\n",
        "A two-copy Bell basis measurement is a measurement in the Bell basis on two copies of a quantum state. This is an important concept in quantum information theory and quantum computing, especially in the context of entanglement and quantum teleportation.\n",
        "\n",
        "The Bell basis consists of four maximally entangled two-qubit states, known as Bell states. These states are:\n",
        "\n",
        "1. |Φ+⟩ = 1/√2 (|00⟩ + |11⟩)\n",
        "2. |Φ-⟩ = 1/√2 (|00⟩ - |11⟩)\n",
        "3. |Ψ+⟩ = 1/√2 (|01⟩ + |10⟩)\n",
        "4. |Ψ-⟩ = 1/√2 (|01⟩ - |10⟩)\n",
        "\n",
        "When we talk about a \"two-copy Bell basis measurement\", we are referring to performing a Bell basis measurement on two copies of a quantum state. Let's consider a state ρ. We have two copies of ρ, let's say ρ1 and ρ2. The two-copy Bell basis measurement would involve performing a Bell basis measurement on the combined system of ρ1 and ρ2.\n",
        "\n",
        "This kind of measurement could be used to assess properties such as the fidelity of the state ρ, or in protocols like entanglement distillation. It is also a key part of certain quantum error correction codes, where it's used to detect and correct errors without destroying the quantum information."
      ],
      "metadata": {
        "id": "LPat9lHG-_HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are quantum states using the fixed POVM?**\n",
        "\n",
        "In quantum mechanics, a Positive Operator-Valued Measure (POVM) is a set of measurement operators that act on quantum states. These operators are designed such that they satisfy certain mathematical properties that make them correspond to physical measurements. Specifically, each operator in the set is positive (its eigenvalues are nonnegative), and the sum of all the operators is the identity operator.\n",
        "\n",
        "A POVM provides a generalization of the concept of a quantum measurement. In the simplest case, quantum measurements are described by a set of projection operators that project onto different states in a quantum system. However, in more complex situations, such as when dealing with noise or imperfect detection, a more general type of measurement is needed, which is where POVMs come in.\n",
        "\n",
        "A \"fixed\" POVM would refer to a specific, predetermined set of measurement operators. For instance, in a quantum computing experiment, a researcher might choose a fixed POVM to measure the output states of a quantum circuit.\n",
        "\n",
        "As for \"quantum states using the fixed POVM,\" it is a bit unclear without more context. However, it could mean the process of applying this predetermined set of measurement operators to certain quantum states. The result of such a measurement would be a set of probabilities (one for each operator in the POVM), indicating the likelihood of each possible measurement outcome. These probabilities would then be used to infer properties of the quantum state or the quantum system.\n"
      ],
      "metadata": {
        "id": "ZPd_qh2D3AeV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYVZpfsfPmTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$\\hookrightarrow$ Eigenvalues*"
      ],
      "metadata": {
        "id": "cPAo7HRV06Ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is meant by an observable with constant spectral norm?**\n",
        "\n",
        "An observable in quantum mechanics is represented by a Hermitian operator. **The spectral norm (or operator norm) of a Hermitian operator is equal to its largest eigenvalue in absolute value**.\n",
        "\n",
        "So, if an observable is described as having a \"constant spectral norm,\" it means that the maximum absolute value of its possible measurement outcomes (eigenvalues) is a constant.\n",
        "\n",
        "For example, the Pauli matrices that represent observables in a two-level quantum system (like a qubit) have a spectral norm of 1, because their eigenvalues are +1 and -1. So, these could be described as observables with constant spectral norm.\n",
        "\n",
        "In practice, this property could be useful because it gives a bound on the possible outcomes of measurements, which can simplify calculations or analyses. It also ensures that the observable is a bounded operator, which is an important property in functional analysis and quantum mechanics."
      ],
      "metadata": {
        "id": "_l8OqycaFhln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by trace distance of the state prior to any measurements?**\n",
        "\n",
        "In the context of quantum mechanics and quantum information theory, the trace distance is a measure of the distinguishability of two quantum states. In a practical sense, it gives you the maximum probability over all possible measurements that you can correctly tell the two states apart.\n",
        "\n",
        "The trace distance between two quantum states ρ and σ is defined as:\n",
        "\n",
        "D(ρ, σ) = 1/2 * Tr[|ρ - σ|]\n",
        "\n",
        "where Tr denotes the trace, and |A| refers to the absolute value of the matrix A (which is defined as the square root of the product of A with its adjoint, or A*A†).\n",
        "\n",
        "The trace distance has a value between 0 and 1, where 0 indicates that the two states are identical, and 1 means the states are completely distinguishable.\n",
        "\n",
        "When the trace distance is mentioned in relation to \"the state prior to any measurements\", it generally refers to assessing how different two quantum states are before any action is taken to measure or otherwise alter these states. This can be important when considering the evolution of quantum systems, or assessing the impact of quantum noise or errors on a quantum state."
      ],
      "metadata": {
        "id": "2g56bk2w8b_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Page 9: \"**the final states of our algorithm on inputs v and w are Ω(1) apart in trace norm**\"\n",
        "  * The statement means that the trace distance between the final states of the algorithm on inputs v and w **is at least a constant**. **The trace distance is a measure of how different two quantum states are**.\n",
        "  * In other words, the **statement is saying that the final states of the algorithm on inputs v and w are not the same, and they are not even close to being the same**. The constant Ω(1) is a lower bound on the trace distance, and it means that the trace distance is at least some constant value, no matter how small that value is.\n",
        "  * **This statement is important in quantum learning theory because it shows that quantum algorithms can be used to distinguish between different inputs**. If the final states of the algorithm on different inputs were always the same, then quantum algorithms would not be able to learn anything about the inputs.\n",
        "  * The statement \"the final states of our algorithm on inputs v and w are Ω(1) apart in trace norm\" is a more formal way of saying that the final states of the algorithm on different inputs are distinguishable."
      ],
      "metadata": {
        "id": "5O2i38FthBHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def trace_distance(state1, state2):\n",
        "  \"\"\"Calculates the trace distance between two quantum states.\n",
        "\n",
        "  Args:\n",
        "    state1: A numpy array representing the first quantum state.\n",
        "    state2: A numpy array representing the second quantum state.\n",
        "\n",
        "  Returns:\n",
        "    The trace distance between the two quantum states.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the difference between the two quantum states.\n",
        "  difference = state1 - state2\n",
        "\n",
        "  # Calculate the trace of the absolute value of the difference.\n",
        "  trace = np.trace(np.abs(difference))\n",
        "\n",
        "  # Return the square root of the trace.\n",
        "  return 0.5 * np.sqrt(trace)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Create two quantum states.\n",
        "  state1 = np.array([[1, 0], [0, 0]])\n",
        "  state2 = np.array([[0, 0], [0, 1]])\n",
        "\n",
        "  # Calculate the trace distance between the two quantum states.\n",
        "  trace_distance = trace_distance(state1, state2)\n",
        "\n",
        "  # Print the trace distance.\n",
        "  print(\"The trace distance is:\", trace_distance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNBVKC-xlyKE",
        "outputId": "7a3f9505-360e-4db2-e20e-de9fc30315d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The trace distance is: 0.7071067811865476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trace_distance = 0.5 * np.sqrt(np.trace(np.abs(state1 - state2)))"
      ],
      "metadata": {
        "id": "jsMwtjDjmYyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring a quantum state in the computational basis is a Clifford measurement. A Clifford measurement is a measurement that can be performed by a circuit consisting only of Clifford gates. The computational basis measurements are the Pauli Z and Pauli X measurements, which are both Clifford gates. Therefore, measuring a quantum state in the computational basis is a Clifford measurement. The computational basis measurements are equivalent to the Pauli Z and Pauli X measurements"
      ],
      "metadata": {
        "id": "-KUhgNi_tMGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation Value and Backpropagation**\n",
        "\n",
        "The average value (expectation value) of the measurement result is given by the\n",
        "Born rule: **bold text**\n",
        "\n",
        "> $\\langle B\\rangle=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle$\n",
        "\n",
        "Just linear algebra! Every step is a matrix-vector or matrix-matrix multiplication\n",
        "\n",
        "Expectation values depend continuously on the gate parameters\n",
        "\n",
        "**Backpropagating Through Quantum Circuits**\n",
        "\n",
        "However, as long as we don't \"zoom in\" to what is happening in the quantum circuit, backpropagation can treat the quantum circuit as a single indivisible function\n",
        "\n",
        "The expectation value of a quantum circuit is a differentiable function\n",
        "\n",
        "> $\n",
        "f(\\theta)=\\left\\langle\\psi\\left|U^{\\dagger}(\\theta) B U(\\theta)\\right| \\psi\\right\\rangle=\\langle B\\rangle$\n",
        "\n",
        "Running on hardware and using the parameter-shift rule, we can provide both ingredients needed by backpropagation\n",
        "\n",
        "> $\n",
        "\\left(\\langle B\\rangle, \\frac{\\partial}{\\partial \\theta}\\langle B\\rangle\\right)\n",
        "$"
      ],
      "metadata": {
        "id": "NOFPaQ7LDw1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectral Gap**\n",
        "\n",
        "Minimum eigenvalue gap of my hamiltonian (spectral gap decrease polynomial or exponentially as a function of the number of particles?). Eigenvalue gap can become exponentially small.\n",
        "\n",
        "The spectral gap, in the context of quantum mechanics or graph theory, is the difference between the lowest two eigenvalues of a system’s Hamiltonian or the adjacency matrix of a graph. It provides crucial information about the system’s dynamics or the graph’s connectivity.\n",
        "\n",
        "In the context of Hamiltonian mechanics, the eigenvalues represent the possible energy levels of the system. The \"minimum eigenvalue gap\" of a Hamiltonian refers to the smallest difference between any two of these energy levels.\n",
        "\n",
        "The importance of this spectral gap is deeply connected to the behavior of quantum systems. If there is a non-zero gap between the lowest energy state (the ground state) and the rest of the spectrum, the system is said to be gapped. If the gap is zero, the system is gapless.\n",
        "\n",
        "In a gapped system, at low temperatures, thermal fluctuations aren't typically strong enough to push the system out of its ground state. This makes the system's behavior at low temperatures easier to predict.\n",
        "\n",
        "On the other hand, in a gapless system, even small fluctuations can cause the system to change its state, resulting in complex behavior.\n",
        "\n",
        "In the field of quantum computation, the spectral gap plays a crucial role in the speed and feasibility of quantum algorithms. For example, in adiabatic quantum computing, the algorithm's running time is inversely proportional to the square of the spectral gap. A smaller spectral gap implies a slower-running algorithm, which is not desirable."
      ],
      "metadata": {
        "id": "ZLCKV2c6c6b0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting back to 𝑓(𝜌,𝑂) = tr(𝑂E(𝜌)) : why do I need to calculate the eigenvalues (trace)?**\n",
        "\n",
        "*From paper: Information-theoretic bounds on quantum advantage in machine learning*\n",
        "\n",
        "* Trace = sum of Eigenvalues (in this context the trace gives a weighted sum of all outcomes – in other words, the average or expected value.)\n",
        "* Eigenvalues = each eigenvalue of an observable represents a possible outcome of a measurement\n",
        "\n",
        "In the expression 𝑓(𝜌,𝑂) = tr(𝑂E(𝜌)), the function tr denotes the trace of a matrix, not its eigenvalues. The trace of a matrix is defined as the sum of the elements on its main diagonal (from the top left to the bottom right). This is true for any square matrix, not just for those representing quantum states or operations.\n",
        "\n",
        "Now, **for the specific case of this expression, the trace is used to calculate the expected value of an observable for a quantum state that has undergone some operation E.** Here's why:\n",
        "\n",
        "In quantum mechanics, the expected (or average) value of an observable O for a state ρ is given by the formula tr(𝑂𝜌). This comes from the Born rule, one of the key postulates of quantum mechanics, which relates the probabilities of measurement outcomes to the state of the system. <font color=\"red\">The observable O is represented by a Hermitian operator, and the state ρ is represented by a density operator.</font>\n",
        "\n",
        "In your expression, E(𝜌) represents the state after some operation E has been applied. So tr(𝑂E(𝜌)) is the expected value of the observable O for the state resulting from E(𝜌).\n",
        "\n",
        "The trace is crucial here because it ensures that we're summing up the probabilities of all possible outcomes. In a physical sense, it is analogous to summing over all possible states that the system might be in. **Because each eigenvalue of an observable represents a possible outcome of a measurement, and the corresponding element of the density matrix gives the probability of that outcome**, taking the trace in this context gives a weighted sum of all outcomes – in other words, the average or expected value."
      ],
      "metadata": {
        "id": "bwYUoViG5BQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you accurately predict expectation values of Pauli observables in an unknown n-qubit quantum state ρ?**\n",
        "\n",
        "In quantum mechanics, the expectation value of an observable represented by an operator A on a state ρ can be calculated using the formula tr(Aρ), where \"tr\" denotes the trace operation.\n",
        "\n",
        "The Pauli matrices (or Pauli operators), named after Wolfgang Pauli, are a set of 2 × 2 matrices which are fundamental in quantum mechanics. They are defined as:\n",
        "\n",
        "- Pauli-X: σ_x = [[0, 1], [1, 0]]\n",
        "- Pauli-Y: σ_y = [[0, -i], [i, 0]]\n",
        "- Pauli-Z: σ_z = [[1, 0], [0, -1]]\n",
        "\n",
        "These operators can be extended to n-qubit systems to form a complete basis for Hermitian operators on that system. A Pauli observable on an n-qubit system is a tensor product of Pauli matrices, one for each qubit. For example, Z ⊗ Z is a Pauli observable for a 2-qubit system that measures the parity of the two qubits.\n",
        "\n",
        "***In order to predict the expectation value of a Pauli observable in an unknown state ρ, you would generally need to perform quantum state tomography***, which involves making many measurements on many identically-prepared quantum systems, and using the results to reconstruct the state ρ. Once you have ρ, you can use the formula tr(Aρ) to calculate the expectation value of any observable A.\n",
        "\n",
        "However, ***full quantum state tomography is not always practical, especially for large systems, due to the number of measurements required***. There are other techniques like direct estimation of the expectation value via sampling, adaptive methods, or compressed sensing techniques which can estimate the expectation value with fewer measurements.\n",
        "\n",
        "In a practical quantum computing scenario, the expectation value of a Pauli observable would often be estimated by preparing many copies of the state, measuring each in the basis corresponding to the observable, and taking the average of the results. These measurements can be implemented with the appropriate rotations and then a computational basis measurement.\n",
        "\n",
        "Remember, this is all assuming you have physical access to the quantum state or quantum system. If you only have classical access to information about the system, you can't accurately predict the expectation values without additional assumptions or information. Quantum mechanics fundamentally limits what you can know about a quantum system without direct quantum access to the system."
      ],
      "metadata": {
        "id": "q345WRdUtkcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what are 𝑘-body reduced density matrices?**\n",
        "\n",
        "In quantum mechanics, the state of a system of particles can be represented by a density matrix, which is a complex, positive semi-definite matrix with a trace equal to 1.\n",
        "\n",
        "For a system of N particles, the full density matrix lives in a high-dimensional Hilbert space (the tensor product of the individual particle's Hilbert spaces), and contains complete information about the state of the system. However, it can often be difficult to work with such high-dimensional objects.\n",
        "\n",
        "A k-body reduced density matrix is a lower-dimensional object that contains information about only a subset of the particles in the system.\n",
        "\n",
        "Formally, the k-body reduced density matrix is obtained by taking the partial trace over N-k of the particles in the system. This process \"averages out\" the information about these N-k particles, leaving behind a density matrix that describes the remaining k particles.\n",
        "\n",
        "The k-body reduced density matrix is a useful tool in many areas of quantum physics, including quantum chemistry and quantum information theory. It allows us to study the behavior of small subsystems within a larger quantum system, even when we can't (or don't want to) keep track of the full state of all the particles. For example, the 1-body reduced density matrix (also known as the one-particle density matrix) is often used in quantum chemistry to describe electron behavior in molecular systems."
      ],
      "metadata": {
        "id": "780q8PXoJZja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$\\hookrightarrow$ Maps*"
      ],
      "metadata": {
        "id": "3hazOulegQat"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB17Qq-GLGBT"
      },
      "source": [
        "**Changing Basis (Map: Overlap Matrix)**\n",
        "\n",
        "* <font color=\"blue\">**For example: A particles position can be expressed as the superposition of momentum states**</font>\n",
        "\n",
        "* Goal: choose a 'good' basis that makes the maths as simple as possible\n",
        "\n",
        "Video: [Changing basis in quantum mechanics](https://www.youtube.com/watch?v=CDmXvPDMIFs)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_252.png)\n",
        "\n",
        "If we go from one representation to another: we need to calculate the overlaps between the corresponding basis states, with the [overlap matrix](https://www.chemeurope.com/en/encyclopedia/Overlap_matrix.html):\n",
        "\n",
        "* The Overlap matrix is used to calculate the overlap integral, which is a measure of the similarity between two basis functions. The overlap integral is important in quantum chemistry because it is used to construct the Hamiltonian matrix, which is used to solve the Schrödinger equation.\n",
        "* The overlap matrix is a square matrix, used in quantum chemistry to describe the inter-relationship of a set of basis vectors of a quantum system.\n",
        "* **If the vectors are orthogonal to one another, the overlap matrix will be diagonal.**\n",
        "* In addition, if the basis vectors form an orthonormal set, the overlap matrix will be the identity matrix.\n",
        "* The overlap matrix is always n×n, where n is the number of basis functions used. It is a kind of Gramian matrix.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_253.png)\n",
        "\n",
        "How do we get back from the new to the old basis?\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_254.png)\n",
        "\n",
        "How we do transform the representation of operators between basis? (resolve identities in the u basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_255.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_256.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CPTP Maps in Quantum Machine Learning**\n",
        "\n",
        "*A quantum channel is a completely positive, trace-preserving (CPTP) map. A CPTP map is mapping one density matrix (input) to another (Output)*\n",
        "\n",
        "*Source: Information-theoretic bounds on quantum advantage in machine learning*\n",
        "\n",
        "*tldr: in ML experiments for an on average error quantum ML  (which operates on quantum data) cannot outperform classical ML (which operaties on classical data. qQuantum ML can outperform only for worst-case prediction error (rather than a small average prediction error), where an exponential separation becomes possible.*\n",
        "\n",
        "We are interested in predicting functions of the form\n",
        "\n",
        "$f(x)=\\operatorname{tr}(O \\mathcal{E}(|x\\rangle\\langle x|))$\n",
        "\n",
        "* where $x$ is a classical input, e.g., chemicals involved in the reaction, a description of the molecule, or the intensity of lasers that con- trol the neutral atoms\n",
        "* $\\mathcal{E}$ is an arbitrary (possibly unknown) completely positive and trace preserving (CPTP) map - it characterizes a quantum evolution happening in the lab. Depending on the parameter x, it produces the quantum state $\\mathcal{E}$(|x⟩⟨x|)\n",
        "* $O$ is a ceratin known observable (what the experimentalist measures at the end of the experiment.)\n",
        "\n",
        "This equation  encompasses any physical process that takes a classical input and produces a real number as output. The goal is to construct a function h(x) that accurately approximates f(x) after accessing the physical process $\\mathcal{E}$ as few times as possible.\n",
        "\n",
        "The goal is to predict the measurement outcome for new physical experiments, with new values of x that have not been encountered during the training process.\n",
        "\n",
        "1. all problems that are approximately learnable by a quantum ML model are also approximately learnable by some restricted clas- sical ML model which executes the quantum process $\\mathcal{E}$ a comparable number of times. This applies in particular, to predicting outputs of quantum- mechanical processes.\n",
        "\n",
        "  * For the task of learning classical Boolean circuits, fundamental limits on quantum advantage have been established in previous work [11–13, 31, 80, 94]. Theorem 1 generalizes these existing results to the task of learning outcomes of quantum processes.\n",
        "\n",
        "  * Therefore finding that classical and quantum ML have comparable power (for average-case prediction) boosts our hopes that the combination of classical ML and near-term quan- tum algorithms [47, 52, 53, 76, 79] may fruitfully ad- dress challenging quantum problems in physics, chem- istry, and materials science.\n",
        "\n",
        "2. quantum ML can have an exponential ad- vantage over classical ML for certain problems where the objective is achieving a specified worst-case prediction error.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OgKDwE8CoMNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the cptp map 𝑓(𝜌,𝑂) = tr(𝑂E(𝜌))?**\n",
        "\n",
        "*Source: Learning to predict arbitrary quantum processes*\n",
        "\n",
        "The expression you've given, 𝑓(𝜌,𝑂) = tr(𝑂E(𝜌)), doesn't directly represent a completely positive trace-preserving (CPTP) map. Rather, it looks like the representation of an expectation value in the context of quantum mechanics or quantum information theory.\n",
        "\n",
        "In the given equation:\n",
        "- 𝜌 is a density operator representing the state of a quantum system.\n",
        "- 𝑂 is an observable, which is a Hermitian operator corresponding to a physical quantity that can be measured.\n",
        "- E(𝜌) is some operation being performed on the state 𝜌; this could potentially represent a quantum channel or some other operation. The nature of this operation isn't specified by your question.\n",
        "- tr( ) denotes the trace, which for an operator is the sum of its eigenvalues.\n",
        "\n",
        "So, tr(𝑂E(𝜌)) is the expected value (or mean value) of the measurement of the observable 𝑂 in the state resulting from the operation E(𝜌).\n",
        "\n",
        "A CPTP map, on the other hand, describes the evolution of quantum states when they interact with an environment. It is a map that takes a density operator to another density operator, and it has two properties:\n",
        "1. It's completely positive, meaning that it preserves the positive semi-definiteness of the quantum state (ensuring that the state remains a valid physical state).\n",
        "2. It's trace-preserving, which ensures that the total probability of all possible outcomes remains equal to 1.\n",
        "\n",
        "Given this, if E were to represent a CPTP map, then tr(𝑂E(𝜌)) still wouldn't itself be a CPTP map, but instead the expectation value of an observable given a quantum state evolved by a CPTP map."
      ],
      "metadata": {
        "id": "bxF65qHK441R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**calculate the density matrices and quantum channels of those classical operations that are represented as a quantum operation?**\n",
        "\n",
        "Yes, you can. A classical operation represented as a quantum operation can indeed have a density matrix and a quantum channel that describes it. Let's take the classical NOT operation as an example, which can be represented as a quantum operation using the Pauli-X (or NOT) gate.\n",
        "\n",
        "1. **Density Matrix**: The density matrix represents the state of a quantum system. If we start with a single qubit in the state |0⟩ (which can represent a classical bit in the state 0), the density matrix is given by |0⟩⟨0|, which is:\n",
        "\n",
        "    ```\n",
        "    ρ_0 = |0⟩⟨0| = [1, 0]\n",
        "                      [0, 0]\n",
        "    ```\n",
        "\n",
        "    If we apply the X gate (the quantum equivalent of the classical NOT operation), the qubit goes to the state |1⟩, and its density matrix is given by |1⟩⟨1|:\n",
        "\n",
        "    ```\n",
        "    ρ_1 = Xρ_0X† = |1⟩⟨1| = [0, 0]\n",
        "                                [0, 1]\n",
        "    ```\n",
        "\n",
        "    where X† is the conjugate transpose (also known as the Hermitian adjoint) of X, and it's equal to X because X is a Hermitian operator.\n",
        "\n",
        "2. **Quantum Channel**: The quantum channel describes a quantum operation. It is a map that takes the density matrix of the input state to the density matrix of the output state. For the X gate, the map is given by:\n",
        "\n",
        "    ```\n",
        "    Λ_X(ρ) = XρX†\n",
        "    ```\n",
        "\n",
        "    where ρ is the density matrix of the input state. This map tells us how the X gate transforms any given input state.\n",
        "\n",
        "In the above examples, we have treated a classical bit as a qubit in a definite state (either |0⟩ or |1⟩), and we have treated a classical NOT operation as a quantum X operation. This shows how classical operations and states can be embedded in the quantum framework. However, remember that this doesn't capture many of the unique features of quantum states and operations, such as superposition and entanglement."
      ],
      "metadata": {
        "id": "mNZHZ4RQ4wC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the function space in a quantum computing operation (cptp map)?**\n",
        "\n",
        "In quantum computing, operations (also known as quantum gates or quantum channels) are often represented as Completely Positive Trace-Preserving (CPTP) maps. These maps represent the evolution of a quantum state in a quantum computation.\n",
        "\n",
        "**A quantum state is typically described as a density matrix**, which is a positive semidefinite operator with trace 1 on a Hilbert space. A Hilbert space is a complex vector space equipped with an inner product operation, and it is a key structure in quantum mechanics as it provides the stage on which quantum states exist and evolve.\n",
        "\n",
        "> **The \"function space\" in this context is the set of all possible quantum states, i.e., the set of all density matrices on the Hilbert space.**\n",
        "\n",
        "A CPTP map is a linear, completely positive, and trace-preserving transformation on this function space. It takes an initial quantum state (a point in the function space) and produces a final quantum state (another point in the same function space), describing the evolution of the state due to the quantum operation.\n",
        "\n",
        "In other words, **a CPTP map is a function that maps the function space of initial quantum states to the function space of final quantum states. Therefore, it is said to act on the \"function space\" of quantum states**.\n",
        "\n",
        "To make this a bit more concrete, consider a single qubit. Its Hilbert space is a two-dimensional complex vector space, and the set of density matrices on this space is the set of all 2x2 positive semidefinite matrices with trace 1. A single-qubit quantum operation is a CPTP map on this set of density matrices.\n",
        "\n",
        "CPTP maps are crucial in quantum information theory because they can represent not only idealized quantum gates but also realistic quantum operations that include effects such as decoherence and noise.\n"
      ],
      "metadata": {
        "id": "ztGyMd0ZfllE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not all physical errors can be linear CPTP maps in a correlation space (quantum channels)**\n",
        "\n",
        "https://www.nature.com/articles/srep00508\n",
        "\n",
        "In the framework of quantum computational tensor network, which is a general framework of measurement-based quantum computation, the resource many-body state is represented in a tensor-network form (or a matrix-product form) and universal quantum computation is performed in a virtual linear space, which is called a correlation space, where tensors live. Since any unitary operation, state preparation and the projection measurement in the computational basis can be simulated in a correlation space, it is natural to expect that fault-tolerant quantum circuits can also be simulated in a correlation space. However, we point out that not all physical errors on physical qudits appear as linear completely-positive trace-preserving errors in a correlation space. Since the theories of fault-tolerant quantum circuits known so far assume such noises, this means that the simulation of fault-tolerant quantum circuits in a correlation space is not so straightforward for general resource states.\n",
        "\n"
      ],
      "metadata": {
        "id": "hvypLz1NhWFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are quantum channel and Kraus operators?**\n",
        "\n",
        "In quantum mechanics, a quantum channel is a mathematical model used to describe the evolution of quantum states due to physical processes, which could be the deterministic dynamics of an isolated quantum system, or could include the effects of noise, dissipation, and decoherence that result from interactions with an environment. Quantum channels are crucial in quantum information theory and quantum computing, where they are used to model realistic, noisy quantum operations.\n",
        "\n",
        "A quantum channel is a completely positive, trace-preserving (CPTP) map. These properties reflect the basic requirements that any physical process must satisfy:\n",
        "\n",
        "- **Complete positivity** ensures that the evolution is physically valid not only for individual quantum states, but also for quantum states that are part of larger systems. This is essential because quantum systems can be entangled, meaning that the state of one system can't be described independently of the state of another system.\n",
        "- **Trace preservation** ensures that the total probability remains 1, reflecting the probabilistic interpretation of quantum mechanics.\n",
        "\n",
        "One common way to represent a quantum channel is by using Kraus operators. Given a quantum channel Λ, we can find a set of operators {K_i} such that the action of the channel on any state ρ is given by\n",
        "\n",
        "Λ(ρ) = Σ_i K_i ρ K_i†\n",
        "\n",
        "where † denotes the conjugate transpose, and the sum is over all i. The operators K_i are called the Kraus operators, or Kraus representation, of the channel.\n",
        "\n",
        "The Kraus operators must satisfy the completeness condition Σ_i K_i† K_i = I, where I is the identity operator, to ensure that the channel is trace-preserving.\n",
        "\n",
        "The Kraus representation is not unique; there can be many different sets of Kraus operators that represent the same quantum channel. The specific form of the Kraus operators can depend on the physical interpretation of the channel. For example, in a quantum system subject to dissipative processes or measurement, each Kraus operator might represent a different possible outcome or error process.\n"
      ],
      "metadata": {
        "id": "8XKy0JpymlW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is a quantum channel?**\n",
        "\n",
        "A quantum channel is a mathematical construct used in quantum information theory to describe the process of transmitting quantum information from one location to another, or more generally, the evolution of quantum states in a system over time. This evolution can be due to the natural dynamics of the system or because of an interaction with an environment, which might cause phenomena like decoherence or noise.\n",
        "\n",
        "Mathematically, a quantum channel is a completely positive, trace-preserving (CPTP) map. It takes a density matrix (which represents a quantum state) as input and returns another density matrix as output.\n",
        "\n",
        "The \"completely positive\" part means that the map preserves the positive semi-definiteness of the density matrix, which is a requirement for it to represent a valid physical state. The \"trace-preserving\" part means that the total probability of all possible outcomes (which is represented by the trace of the density matrix) remains equal to 1.\n",
        "\n",
        "One common example of a quantum channel is the depolarizing channel, which introduces a certain probability of error into each qubit of information transmitted through it. There are many other types of quantum channels that model different kinds of physical processes, including lossy channels, dephasing channels, amplitude damping channels, and more. Each channel has a different impact on the quantum states that pass through it, often reflecting the effects of different types of environmental noise or decoherence."
      ],
      "metadata": {
        "id": "u6AwpEdL1zvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you give me an example of a quantum channel?**\n",
        "\n",
        "Certainly, a straightforward example of a quantum channel is the \"bit-flip channel.\" This channel models the possible flipping of a qubit due to errors in a quantum computing environment.\n",
        "\n",
        "Let's denote the basis states of our qubit as |0⟩ and |1⟩. Suppose we have a single qubit quantum state ρ which we send through the bit-flip channel. After going through the channel, each qubit can either stay the same or be flipped, i.e., |0⟩ goes to |1⟩ and |1⟩ goes to |0⟩.\n",
        "\n",
        "Let's also say that the probability of a flip occurring is p. Then the probability that nothing happens is (1-p).\n",
        "\n",
        "The behavior of the bit-flip channel Λ can be described as:\n",
        "\n",
        "Λ(ρ) = (1-p)ρ + p XρX\n",
        "\n",
        "where X is the Pauli-X gate, which flips the state of a qubit.\n",
        "\n",
        "This description of the channel is essentially a probabilistic mixture of the identity operation (which leaves the state ρ unchanged) with weight (1-p), and the bit-flip operation (XρX) with weight p.\n",
        "\n",
        "This is a completely positive, trace-preserving (CPTP) map: it's \"completely positive\" because it preserves the positive semi-definiteness of the density matrix, and it's \"trace-preserving\" because the sum of the probabilities of all outcomes is 1 ((1-p) + p = 1), ensuring that the total quantum state remains normalized.\n",
        "\n",
        "As a consequence of going through this channel, quantum information can be lost or corrupted, which is a significant consideration in the design of quantum computing systems and quantum error correction codes."
      ],
      "metadata": {
        "id": "qANEZaHQ2g4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Channels and How could you formalize stochastic maps between basis changes into quantum circuits?**\n",
        "\n",
        "Basis changes in quantum circuits are usually modeled with unitary transformations. A unitary transformation is a linear transformation that preserves the inner product in the space, and in quantum mechanics, it corresponds to a deterministic evolution of the quantum state.\n",
        "\n",
        "However, if we're looking to introduce stochastic elements into quantum circuits, there are ways to do so, primarily through the use of quantum channels.\n",
        "\n",
        "A quantum channel is a completely positive, trace-preserving (CPTP) map that represents a physical process in quantum mechanics. While unitary transformations correspond to ideal, noiseless operations, quantum channels can represent realistic, noisy operations. This includes both coherent operations, represented by unitary transformations, and incoherent or stochastic operations, represented by non-unitary transformations.\n",
        "\n",
        "To introduce stochasticity, you can consider the notion of a mixed state, represented by a density matrix, which can be understood as a statistical mixture of different pure states. A quantum channel can take a pure state and produce a mixed state, representing a stochastic process.\n",
        "\n",
        "More precisely, a quantum channel Λ can be represented by a set of Kraus operators {K_i}. These Kraus operators act on a state ρ as follows:\n",
        "\n",
        "Λ(ρ) = Σ_i K_i ρ K_i†\n",
        "\n",
        "where the sum is over all i, and † denotes the Hermitian adjoint (conjugate transpose). The Kraus operators are required to satisfy the completeness condition Σ_i K_i† K_i = I, where I is the identity operator.\n",
        "\n",
        "To encode a stochastic map into a quantum circuit, you'd need to find a suitable set of Kraus operators representing the stochastic process, and incorporate them into your circuit model.\n",
        "\n",
        "Keep in mind, however, that not all stochastic maps can be physically realized as quantum channels. The map must satisfy the conditions for a CPTP map, i.e., it must be completely positive and preserve the trace of the density matrix.\n",
        "\n",
        "Also note that while quantum channels provide a way to model stochastic processes, they are not usually implemented directly in quantum circuits on current quantum computers, which typically only perform unitary transformations directly. However, they can be used to model the effects of noise and errors in these circuits, and can be implemented indirectly using techniques like quantum error correction and quantum teleportation.\n",
        "\n"
      ],
      "metadata": {
        "id": "hOo54ImZma0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *$\\hookrightarrow$ Operators*"
      ],
      "metadata": {
        "id": "EBxdSfTogMN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical: what is a signal-processing operator?**\n",
        "\n",
        "In signal processing, an operator is a mathematical function that acts on a signal to transform it in some way. Signal processing operators are used to perform a wide variety of tasks, such as filtering, modulation, and compression.\n",
        "\n",
        "**Types of Signal Processing Operators**\n",
        "\n",
        "There are many different types of signal processing operators, but they can be broadly categorized into two groups: linear and nonlinear.\n",
        "\n",
        "* **Linear operators** satisfy the principle of superposition, which means that if the operator is applied to the sum of two signals, the result is the sum of the operator's outputs on the two individual signals. Linear operators are also characterized by the fact that they preserve the magnitude and phase of the signal's frequency components. Examples of linear operators include filters, amplifiers, and delay lines.\n",
        "\n",
        "* **Nonlinear operators** do not satisfy the principle of superposition. Nonlinear operators can change the shape of the signal's waveform, introduce distortion, and even create new signal components. Examples of nonlinear operators include squaring, clipping, and thresholding.\n",
        "\n",
        "**Applications of Signal Processing Operators**\n",
        "\n",
        "Signal processing operators are used in a wide variety of applications, including:\n",
        "\n",
        "* **Audio signal processing:** Filtering, equalization, noise reduction, compression, and enhancement\n",
        "* **Speech processing:** Speech recognition, speaker identification, and speech synthesis\n",
        "* **Image processing:** Noise reduction, sharpening, compression, and enhancement\n",
        "* **Radar and sonar processing:** Target detection, tracking, and classification\n",
        "* **Telecommunications:** Signal modulation, demodulation, and error correction\n",
        "\n",
        "**Examples of Signal Processing Operators**\n",
        "\n",
        "Here are some examples of common signal processing operators:\n",
        "\n",
        "* **Convolution:** Combines two signals by summing their overlapping segments.\n",
        "* **Correlation:** Measures the similarity between two signals by calculating the sum of their product over time or frequency.\n",
        "* **Fourier transform:** Converts a signal from the time domain to the frequency domain, or vice versa.\n",
        "* **Discrete Fourier transform (DFT):** A specific implementation of the Fourier transform for discrete-time signals.\n",
        "* **Fast Fourier transform (FFT):** A more efficient algorithm for computing the DFT.\n",
        "* **Laplace transform:** Converts a signal from the time domain to the frequency-domain, or vice versa, using the Laplace transform function.\n",
        "* **Z-transform:** Converts a discrete-time signal from the time domain to the frequency-domain using the Z-transform function.\n",
        "\n",
        "These are just a few examples of the many signal processing operators that are used in a variety of applications."
      ],
      "metadata": {
        "id": "emRVcbLzqs98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is the rank of an operator?**\n",
        "\n",
        "* The term \"rank\" in the context of matrices refers to the dimension of the column space (or equivalently, the row space) of the matrix. Specifically, the rank of a matrix is the maximum number of linearly independent column vectors (or equivalently, row vectors) in the matrix. It gives a measure of the \"information content\" of the matrix, in the sense of how many independent directions it spans.\n",
        "\n",
        "* However, when discussing quantum mechanics and, more broadly, linear operators in a Hilbert space, the term \"rank\" is less commonly used in the same way as for matrices. Nonetheless, for operators that can be represented as matrices (as is the case in finite-dimensional Hilbert spaces), the rank of the operator is equivalent to the rank of its matrix representation.\n",
        "\n",
        "* For a given operator \\( \\hat{O} \\) with a matrix representation \\( O \\), the rank of \\( \\hat{O} \\) is the rank of \\( O \\). In this context, the rank of the operator essentially tells you the number of non-zero eigenvalues (counted with multiplicity) or, equivalently, the dimension of the subspace onto which the operator has non-trivial action.\n",
        "\n",
        "* In summary, when talking about operators in quantum mechanics or other areas of functional analysis, the rank of an operator (when it's represented as a matrix) is the dimension of the subspace in which the operator acts non-trivially.\n",
        "\n",
        "The rank of an operator is the maximum number of linearly independent rows or columns of the matrix that represents the operator. In other words, it is the dimension of the subspace of the Hilbert space that the operator maps to itself.\n",
        "\n",
        "* **Rank 1 operators:** Rank 1 operators are also known as dyads. They are operators that can only project onto a one-dimensional subspace of the Hilbert space. For example, the Pauli X operator is a rank 1 operator. (->questionable)\n",
        "\n",
        "* **Rank 2 operators:** Rank 2 operators are operators that can project onto a two-dimensional subspace of the Hilbert space. For example, the Pauli Z operator is a rank 2 operator. (->questionable)\n",
        "\n",
        "* **Rank 3 operators:** Rank 3 operators are operators that can project onto a three-dimensional subspace of the Hilbert space. For example, the Pauli Y operator is a rank 3 operator. (->questionable)\n",
        "\n",
        "Higher-rank operators can also exist, but they are less common.\n",
        "\n",
        "Rank 1, 2, and 3 operators are important in quantum computing because they can be used to implement a variety of quantum algorithms and protocols. For example, the Hadamard gate is a rank 3 operator, and it is used in many quantum algorithms, such as Shor's algorithm.\n",
        "\n",
        "Here are some examples of rank 1, 2, and 3 operators in quantum computing:\n",
        "\n",
        "* **Rank 1:** Pauli X, Pauli Y, Pauli Z\n",
        "* **Rank 2:** Hadamard gate, Controlled-NOT gate\n",
        "* **Rank 3:** Toffoli gate, Swap gate\n",
        "\n",
        "I hope this helps!\n"
      ],
      "metadata": {
        "id": "OZ5QxG31fR3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**super-operator**\n",
        "\n",
        "In the field of quantum mechanics and quantum information theory, a superoperator is a type of operator that acts on other operators, rather than on states. In the context of quantum mechanics, these \"other operators\" are usually the density matrices that represent quantum states.\n",
        "\n",
        "This concept is needed because in quantum mechanics, the evolution of a system can often be represented as an operator acting on the state of the system. However, when we consider more complicated situations, like a quantum system interacting with an environment (leading to phenomena such as decoherence), the evolution of the system can involve operations that change the nature of the operator that represents the state itself. This kind of operation is represented by a superoperator.\n",
        "\n",
        "Formally, a superoperator Φ acting on an operator A would be written as Φ(A), and this results in another operator.\n",
        "\n",
        "One common example of a superoperator is the Liouville superoperator, which appears in the Liouville-von Neumann equation describing the time evolution of a quantum system. If H is the Hamiltonian of the system and ρ is the density operator, the Liouville superoperator L acting on ρ is defined as:\n",
        "\n",
        "L(ρ) = -i[H, ρ]\n",
        "\n",
        "where [ , ] denotes the commutator.\n",
        "\n",
        "***Another important class of superoperators are the completely positive trace preserving (CPTP) maps***, which are used to describe the evolution of quantum systems in the presence of decoherence. A quantum operation (also called a quantum channel) is a CPTP map and can be represented as a superoperator acting on the density matrix of a quantum state."
      ],
      "metadata": {
        "id": "1-daOZv2mFJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For what do you need a kraus operator?**\n",
        "\n",
        "Kraus operators are a useful tool for representing and working with quantum channels, which are mathematical models that describe the evolution of quantum states due to physical processes. These processes can include both the deterministic dynamics of an isolated quantum system, and the effects of noise, dissipation, and decoherence due to interactions with an environment.\n",
        "\n",
        "Here are some specific uses for Kraus operators:\n",
        "\n",
        "1. **Modeling realistic quantum operations**: In quantum computing and quantum information theory, we often need to model not just ideal, noiseless operations (which are represented by unitary transformations), but also realistic, noisy operations. Kraus operators provide a way to do this. Each Kraus operator can represent a different possible outcome or error process that can occur during the operation.\n",
        "\n",
        "2. **Calculating the effect of a quantum channel**: Given the Kraus operators for a quantum channel, we can calculate how the channel will transform any quantum state. This is useful for predicting the results of quantum computations or quantum communications in the presence of noise.\n",
        "\n",
        "3. **Studying quantum decoherence**: Kraus operators provide a mathematical framework for studying decoherence, which is the process by which a quantum system loses its quantum properties due to interactions with its environment. Each Kraus operator can represent a different type of interaction with the environment.\n",
        "\n",
        "4. **Quantum error correction**: Kraus operators can represent different types of errors that can occur in a quantum system. This is crucial for designing quantum error correction codes, which are schemes for detecting and correcting errors in quantum computations or communications.\n",
        "\n",
        "5. **Quantum process tomography**: This is a technique for experimentally determining the Kraus operators of a quantum channel, which can provide insights into the physical processes occurring in a quantum system.\n",
        "\n",
        "Overall, Kraus operators provide a flexible and powerful mathematical tool for describing and analyzing the dynamics of quantum systems, especially in the presence of noise and other non-ideal effects.\n"
      ],
      "metadata": {
        "id": "n4Impp-7m0bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are Lindblad operators?**\n",
        "\n",
        "Lindblad operators (also sometimes referred to as \"jump\" operators) are a fundamental component of Lindblad equations, which are used in the field of quantum mechanics to describe the evolution of quantum systems that interact with an environment. This interaction leads to effects such as decoherence and dissipation, which are not captured by the basic Schrödinger equation.\n",
        "\n",
        "The general form of a Lindblad equation is:\n",
        "\n",
        "dρ/dt = -i[H, ρ] + Σ_k γ_k (L_k ρ L_k† - 1/2 {L_k† L_k, ρ})\n",
        "\n",
        "Here:\n",
        "\n",
        "- H is the Hamiltonian of the system, which describes its energy and governs its unitary (non-dissipative) evolution.\n",
        "- ρ is the density matrix of the system, which encapsulates the state of the system.\n",
        "- The brackets [ , ] and { , } denote the commutator and anticommutator respectively.\n",
        "- The L_k are the Lindblad operators, which describe how the system interacts with its environment.\n",
        "- The γ_k are positive decay rates associated with each Lindblad operator.\n",
        "\n",
        "Each Lindblad operator L_k represents a certain type of interaction between the quantum system and its environment. The precise form of these operators depends on the specifics of the system and its environment. For example, in a system of quantum harmonic oscillators, the Lindblad operators might be the creation and annihilation operators for each oscillator.\n",
        "\n",
        "The non-unitary part of the Lindblad equation (the second term on the right-hand side) describes how these interactions cause the system to decay towards a steady state, and is responsible for phenomena such as relaxation and decoherence. The Lindblad equation is the most general form for the time evolution of a quantum system that ensures complete positivity and trace preservation of the density matrix, two crucial physical requirements."
      ],
      "metadata": {
        "id": "2TfI0tinl43W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What‘s the difference between kraus operator and lindblatt operator?**\n",
        "\n",
        "Kraus operators and Lindblad operators are both tools used to represent and study the evolution of quantum systems, especially when these systems are interacting with an environment. However, they are used in slightly different contexts and represent different types of processes.\n",
        "\n",
        "1. **Kraus Operators**: Kraus operators are used to represent quantum channels, which are completely positive, trace-preserving (CPTP) maps that describe the evolution of quantum states. The Kraus representation is a way to express a quantum channel as a sum of terms, each of which consists of a Kraus operator acting on the quantum state. The Kraus operators can represent different types of interactions with the environment or different possible outcomes of a quantum process.\n",
        "\n",
        "2. **Lindblad Operators**: Lindblad operators are used specifically to represent the generators of Markovian, open quantum systems in the Lindblad equation (also called the Gorini–Kossakowski–Sudarshan–Lindblad equation). The Lindblad equation describes the time evolution of a quantum system that is undergoing continuous, irreversible dynamics due to its interaction with a large, uncontrolled environment (a process called \"decoherence\"). Each Lindblad operator represents a different type of interaction with the environment.\n",
        "\n",
        "So, in brief: Kraus operators are used to represent discrete transformations of quantum states in general quantum channels, while Lindblad operators are used to represent continuous, Markovian dynamics in open quantum systems.\n",
        "\n",
        "It's also worth noting that the Lindblad equation is one way to derive Kraus operators for a quantum channel, specifically when the channel represents the evolution of a system over a small time interval under continuous, Markovian dynamics. In this case, the Lindblad operators determine the form of the Kraus operators."
      ],
      "metadata": {
        "id": "hFbdLJLMm4re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can one say that Kraus operators are bounded, and Lindblad operators unbounded? Or is that wrong?**\n",
        "\n",
        "In quantum mechanics, both Kraus and Lindblad operators act on a finite-dimensional Hilbert space (when we're considering finite quantum systems such as qubits, which are the building blocks of quantum computers), and hence they are indeed bounded operators. A bounded operator is an operator that doesn't amplify any vector in the Hilbert space by more than a certain factor, the operator norm.\n",
        "\n",
        "When the quantum system is finite-dimensional, the concepts of \"bounded\" and \"unbounded\" don't really come into play, because all operators on a finite-dimensional Hilbert space are automatically bounded.\n",
        "\n",
        "However, if you are considering infinite-dimensional systems, such as quantum fields or harmonic oscillators, then it becomes meaningful to talk about bounded and unbounded operators. In this case, Lindblad operators (the ones appearing in the Lindblad equation, governing the dynamics of open quantum systems) are typically unbounded.\n",
        "\n",
        "This difference arises because the Lindblad equation is a differential equation, and the Lindblad operators often represent physical quantities such as position and momentum, which are unbounded operators on infinite-dimensional Hilbert spaces. On the other hand, a Kraus representation of a quantum channel represents the action of the channel on the quantum state at a specific point in time, and this action can usually be expressed in terms of bounded operators.\n",
        "\n",
        "But to summarize, in the finite-dimensional case (which is the relevant case for most quantum computing applications), both Kraus and Lindblad operators are bounded. In the infinite-dimensional case, Kraus operators are generally still bounded, while Lindblad operators can be unbounded.\n"
      ],
      "metadata": {
        "id": "0SJBdUtqnADp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do you have an example of a Kraus operator and a Lindblad operator in the context of quantum computing?**\n",
        "\n",
        "Sure, I can provide some simple examples of both Kraus and Lindblad operators in the context of quantum computing.\n",
        "\n",
        "1. **Kraus Operators**: Kraus operators are used to describe the evolution of quantum states in the presence of noise or decoherence. They form a set of matrices {K_i} such that the sum of the Hermitian conjugates of each K_i times K_i is the identity matrix.\n",
        "\n",
        "    For instance, let's consider a single qubit undergoing bit flip error with a certain probability p. The error can be described by two Kraus operators:\n",
        "\n",
        "    K_0 = sqrt(1-p) * I,\n",
        "    K_1 = sqrt(p) * X,\n",
        "\n",
        "    where I is the identity operator and X is the Pauli-X operator. These operators satisfy the condition K_0†K_0 + K_1†K_1 = I, which ensures the resulting quantum state is still a valid density matrix.\n",
        "\n",
        "2. **Lindblad Operators**: Lindblad operators (also known as jump operators) are used in the Lindblad equation, which describes the time evolution of an open quantum system.\n",
        "\n",
        "    For instance, a common Lindblad operator for a single qubit undergoing amplitude damping (energy relaxation) is:\n",
        "\n",
        "    L = sqrt(γ) * |0><1|,\n",
        "\n",
        "    where |0> and |1> are the computational basis states of the qubit, and γ is the damping rate. This Lindblad operator represents the process of the qubit transitioning from the excited state |1> to the ground state |0>.\n",
        "\n",
        "    In the context of a Lindbladian superoperator (describing the complete evolution), you might see this operator appearing in a term like L ρ L† in the master equation.\n",
        "\n",
        "These are quite simplified examples. In practice, the Kraus and Lindblad operators for a realistic quantum system could be more complex and might depend on the specific physical details of the system.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyvxk6BPnnh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an example of unbounded operators that lead to pathological behavior in the study of infinite-dimensional spaces in functional analysis?**\n",
        "\n",
        "Unbounded operators in infinite-dimensional spaces often require careful treatment because they can lead to \"pathological\" behavior, i.e., behavior that violates our usual intuitions or expectations based on finite-dimensional spaces.\n",
        "\n",
        "One classical example is the momentum operator in quantum mechanics, which is defined on the space of square-integrable functions over the real numbers (which is an infinite-dimensional space). The momentum operator is given by -iħd/dx, where i is the imaginary unit, ħ is the reduced Planck constant, and d/dx is the derivative operator.\n",
        "\n",
        "This operator is unbounded, meaning there's no finite number B such that the norm of the momentum operator acting on any state is less than or equal to B times the norm of the state. As a result, the momentum operator can't be defined on the entire Hilbert space, but only on a dense subset (the set of differentiable functions, in this case).\n",
        "\n",
        "One \"pathological\" behavior of the momentum operator is that it doesn't have any eigenstates that belong to the Hilbert space. In quantum mechanics, we often want to find eigenstates of operators because these correspond to \"observable\" states of the system with definite values of the associated physical quantity. For the momentum operator, the eigenstates are plane waves, which are not square-integrable and hence do not belong to the Hilbert space.\n",
        "\n",
        "This behavior is quite different from what we see with bounded operators, where the eigenstates always belong to the same space as the operator. In order to deal with this kind of issue, physicists often have to introduce additional mathematical structures, such as rigged Hilbert spaces, or use indirect methods, such as the spectral theorem for unbounded self-adjoint operators, to make sense of the physical meaning of unbounded operators like the momentum operator."
      ],
      "metadata": {
        "id": "0ZubXnvmnE2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sign structure in quantum operators**\n",
        "\n",
        "The term \"sign structure\" is not standard terminology in quantum mechanics, so it may be interpreted in different ways depending on the context. In general, the mathematical operators used in quantum mechanics can have complex values and can act on complex-valued wavefunctions or on operators themselves, so they don't have a \"sign\" in the way that real numbers do.\n",
        "\n",
        "However, there are a few contexts where the notion of sign can be important in quantum mechanics:\n",
        "\n",
        "1. **Parity**: The parity operator is a specific quantum mechanical operator that represents spatial inversion (flipping the sign of the spatial coordinates). Eigenstates of the parity operator can have positive or negative eigenvalues, often interpreted as \"even\" or \"odd\" parity.\n",
        "\n",
        "2. **Spin**: In quantum mechanics, particles like electrons have a property called spin, which can take on positive or negative values (typically +1/2 or -1/2 for electrons).\n",
        "\n",
        "3. **Commutation relations**: Certain pairs of quantum operators have commutation relations that involve the imaginary unit \"i\", which could be interpreted as a kind of \"sign\" structure. For example, the position operator x and momentum operator p have the commutation relation [x, p] = iħ, where ħ is the reduced Planck's constant.\n",
        "\n",
        "4. **Phase factors**: In quantum mechanics, the overall sign (or more generally, phase factor) of a wavefunction is not observable, which leads to the principle of quantum superposition and interference effects.\n",
        "\n",
        "If you're asking about a specific aspect of quantum mechanics or a specific type of quantum operator, I'd be happy to provide more information!"
      ],
      "metadata": {
        "id": "fgSc_9VsmNdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what is meant by the sign of a trace?**\n",
        "\n",
        "The \"sign\" of a trace simply refers to whether the trace of a matrix (or operator in the context of quantum mechanics) is positive, negative, or zero.\n",
        "\n",
        "The trace of a matrix is the sum of its diagonal elements. If this sum is greater than zero, we say that the trace (and hence the matrix or operator) is positive. If the sum is less than zero, the trace is negative. If the sum is exactly zero, the trace is zero.\n",
        "\n",
        "This concept is often used in the context of quantum mechanics, where the trace of an operator can provide important physical information. For example, in the context of quantum density matrices, a trace of one is associated with a valid physical state.\n",
        "\n",
        "Please note that the sign of the trace of a matrix does not necessarily indicate that all elements, or even all eigenvalues, of the matrix are of the same sign. It's possible for a matrix to have both positive and negative eigenvalues but for their sum (i.e., the trace) to be positive, negative, or zero. Similarly, it's possible for a matrix to have positive and negative elements but for their diagonal sum (i.e., the trace) to be positive, negative, or zero."
      ],
      "metadata": {
        "id": "2LjS8JA23j-T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscthBMzOLUM"
      },
      "source": [
        "###### *Schrödinger Equation*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Hamiltonian Time Evolution](https://youtu.be/1zrtUmXR7Ew?si=bL-UX6NIPzbu1zV2)"
      ],
      "metadata": {
        "id": "IcfleABCelIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understand effects of environment (noise)**\n",
        "\n",
        "A spin does not indefinitely precess around the magnetic field (according to the Schrödinger equation), because it interacts with the environment (electrons, photons, nuclear spins from outside), which makes it stop. (Lots of noise = degrees of freedom that interact)\n",
        "\n",
        "[Universal Lindblad equation for open quantum systems - Frederik Nathan](https://www.youtube.com/watch?v=j5yW2MtOwwA&list=WL&index=10&t=294s)\n",
        "\n",
        "Impossible to track evolution of entire universe: If the universe is 10^26 degrees of freedom then we need 2^10^26 numbers to just keep track of the wave function.\n",
        "\n",
        "Impossible to solve the Schrodinger equation for an open quantum system\n",
        "\n",
        "> $\\begin{gathered}H=H_{\\mathrm{S}}+H_{\\mathrm{B}}+\\sqrt{\\gamma} X B \\\\ \\left|\\psi_0\\right\\rangle=\\left|\\psi_S\\right\\rangle \\times\\left|\\psi_B\\right\\rangle \\quad \\partial_t|\\psi(t)\\rangle=-i H|\\psi(t)\\rangle\\end{gathered}$\n",
        "\n",
        "instead: use reduced density matrix that encodes all information of the system:\n",
        "\n",
        "> $\\rho(t)=\\operatorname{Tr}_{\\mathrm{B}}|\\psi(t)\\rangle(\\psi(t) \\mid, \\quad\\langle(t))=\\operatorname{Tr}[O \\rho(t)]$"
      ],
      "metadata": {
        "id": "8FPtRRg90PbB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIrMWsUSPKj2"
      },
      "source": [
        "> for fast moving electrons or electrons in electro-magnetic fields, the Schroedinger equation gives the wrong answers (see quantum field theory, spinor field)\n",
        "\n",
        "<font color=\"blue\">**From Newton's 2nd Law of Motion to the Schrödinger Equation**\n",
        "\n",
        "**Step 1: Starting with Newton's 2nd law of motion**\n",
        "\n",
        "* let's assume a particle is moving along an x-axis and we apply several forces on it $\\overrightarrow{F}_1$, $\\overrightarrow{F}_2$, .. $\\overrightarrow{F}_n$\n",
        "\n",
        "* these forces depend on the position $x$ of the particle and the time elapsed $t$\n",
        "\n",
        "Then we can find the particle's position $x$ as a function of time using **Newton's 2nd law**:\n",
        "\n",
        "> $\\vec{F}_{net}=\\sum \\vec{F}_{i}(x, t)=m \\vec{a}  \\quad(\\text { Newton's 2nd law })$\n",
        "\n",
        "But the law of acceleration $\\vec{a}$ can be also written as the second time derivative of position, so we end up with a [governing equation](https://en.wikipedia.org/wiki/Governing_equation) like this, the **Equation of Motion**:\n",
        "\n",
        "> $m \\frac{d^{2} x}{d t^{2}}=\\sum_{i=1}^{n} F_{i}(x, t) \\quad(\\text { Equation of Motion })$\n",
        "\n",
        "* (another way of writing it is: $m \\ddot x = -kx$, see Colab 'Variationsrechnung')\n",
        "\n",
        "* Once we solve this equation for the particle's position, we could infer many things about the particle's state, such as its velocity, kinetic energy etc.\n",
        "\n",
        "* *Exkurs: The governing equations of a mathematical model describe how the values of the unknown variables (i.e. the dependent variables) change when one or more of the known (i.e. independent) variables change*\n",
        "\n",
        "**Step 2: Schrödinger Equation & Operator /Functionals**\n",
        "* The goal of quantum mechanics is to solve the Schrödinger Equation, which is very similar conceptually\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "\n",
        "* in contrast to classical equation of motion, where we solve for position and then get velocity, kinetic energy etc about particle's state, **in the Schrodinger equation we solve for wavefunction** $\\psi$! (because that is kind of it's position :)\n",
        "\n",
        "  * $\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}$ this is the Kinetic energy operator $L$ on the wavefunction $\\psi$\n",
        "\n",
        "  * $V$ represents the potential energy operator\n",
        "\n",
        "<font color=\"red\">**In quantum mechanics we use the term operator because you generally don't get fixed numerical values for kinetic and potential energy.**</font>\n",
        "\n",
        "* Instead you need to perform operations on the wavefunction to extract those kinetic and potential energy values. That's what these operators do.\n",
        "\n",
        "* You can think of Schrodinger equation as a statement of energy conservation: kinetic energy + potential energy = total energy (which is on the left side of the v)\n",
        "\n",
        "**Step 3: Wavefunction & (Dirac) Delta Function**\n",
        "\n",
        "* the wavefunction represents the state of a system - related to the probability of finding a particle at a particular region in the domain which it occupies\n",
        "\n",
        "* the square of the norm of the wavefunction gives the probability density function of a particle.\n",
        "\n",
        "* if you integrate this norm squared over the entire domain you will get 1\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$\n",
        "\n",
        "* you can't tell exactly where the position of a particle is before measurement, just a probability, same for velocity, momentum, kinetic energy etc.\n",
        "\n",
        "* But if you **apply the measurement operator, you change the wavefunction**! after one measurement, or further measurements will always get you the same result\n",
        "\n",
        "**So instead of being a probability distribution that covers multiple values, <font color=\"red\">by taking a measurement I change the wavefunction to a [delta function](https://de.wikipedia.org/wiki/Delta-Distribution) with one spike at what my measurement gave me. If I take more measurement on the same system, the delta function doesnt change.</font> This is called the wavefunction collapse.**\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_174.png)\n",
        "\n",
        "* However, if I let the system settle so that it eventually occupies its original wavefunction it had and then if I take my measurement, I might get something different according to the probability distribution corresponding to my original wavefunction\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_175.png)\n",
        "\n",
        "**Step 4: Partial Differential Equation**\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "* when solving a partial differential equation we also need auxiliary conditions in order to determine the unknown constants that you get from the integration process that's inherent in solving a differential equation\n",
        "\n",
        "* auxiliary conditions = initial conditions and boundary conditions\n",
        "\n",
        "* however the Schrodinger equation doesn't come with your typical boundary conditions that you might be used to seeing\n",
        "\n",
        "**Instead,the auxiliary condition we have on our solution is the normalization constraint: $\\int_{-\\infty}^{\\infty}|\\Psi|^{2} d x=1$**\n",
        "\n",
        "* Solutions to the Schrodinger equation have two be normalizable because they are wavefunctions, and in a way they represent probability density functions\n",
        "\n",
        "\t* if the solutions to Schrodinger's equation are not normalizable, we can't use them to represent a physical system\n",
        "\n",
        "\t* the trivial solution $\\psi$ (x,t) = 0 is not normalizable, because its integral from negative infinity to infinity will always be 0, it can never be 1, which is why $\\psi$ (x,t) = 0 is an unphysical solution, because the particle has to be somewhere\n",
        "\n",
        "**Step 5: Solving Schrodinger's Equation under the normalization condition**\n",
        "\n",
        "Let's say we solve Schrodinger's equation and we get following solution:\n",
        "\n",
        "> $\\Psi (x,t) = A f(x,t)$ with $A$ being an arbitrary constant\n",
        "\n",
        "* The process of normalization is to find the value of the constant $A$ so that the solution obeys the normalization condition:\n",
        "\n",
        "> $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=\\int_{-\\infty}^{\\infty}|A f|^{2} d x = 1$\n",
        "\n",
        "* we might end up with a difficult task if the wavefunction is dependent on time, which means it has a different shape for different times, then wouldn't the normalization constant change with time as well?\n",
        "\n",
        "* the answer is NO!\n",
        "\n",
        "**Theorem**: If you normalize the wavefunction once then you don't need to normalize for other times = the normalization stays preserved !\n",
        "\n",
        "**Proof**:\n",
        "\n",
        "* the norm squared of a wavefunction is just the complex conjugate of that wavefunction time the wavefunction: $|\\psi|^{2}=(\\psi)^{*} \\psi$\n",
        "\n",
        "* if we go back to the Schrodinger equation we would see that the complex conjugate of the equation would be something like this with all they size turned into their conjugates and all the imaginary terms with their signs switched:\n",
        "\n",
        "> $i \\hbar \\frac{\\partial \\Psi}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi}{\\partial x^{2}}+V \\psi$\n",
        "\n",
        "> $-i \\hbar \\frac{\\partial \\psi^{*}}{\\partial t}=\\frac{-\\hbar^{2}}{2 m} \\frac{\\partial^{2} \\psi^{*}}{\\partial x^{2}}+V \\psi^{*}$\n",
        "\n",
        "* the goal of the proof is to show that this normalization integral $\\int_{-\\infty}^{\\infty}|\\psi|^{2} d x=$ const doesn't change with time, it stays the same:\n",
        "\n",
        "* the way to do this is to take the time derivate: if we can show that the time derivative of the normalization integral is 0 then our proof is complete $\\frac{d}{a t}\\left[\\int_{-\\infty}^{\\infty}|\\varphi|^{2} d x\\right]=0$\n",
        "\n",
        "* see complete proof in [video](https://www.youtube.com/watch?v=kUm4q0UIpio&list=WL&index=72&t=651s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HCh0gYIJv8x"
      },
      "source": [
        "<font color=\"blue\">**Schrödinger Equation (Time Independent / Eigenvalue Equation)**</font>\n",
        "\n",
        "Time independent = Total energy of a system does NOT change with time\n",
        "\n",
        "**The Schroedinger equation can be written as a type of Eigenvalue equation**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle= -i \\hbar \\frac{d}{d t}|\\psi\\rangle =\\frac{\\hbar}{i} \\frac{d}{d t}|\\psi\\rangle$\n",
        "\n",
        "**Simplified (when system is not changing over time: time-independent Schroedinger equation):**\n",
        "\n",
        "> $\\hat{H}|\\psi\\rangle=E |\\psi\\rangle$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_151.png)\n",
        "\n",
        "> $E \\Psi(x)=\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}+V \\Psi(x)$\n",
        "\n",
        "* E = **Energy the electron** is allowed to have\n",
        "\n",
        "* $\\Psi$ = **Wavefunction** (most likely position of an electron)\n",
        "\n",
        "* **Kinetic energy**: $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi(x)}{d x^{2}}$ (klassische Form: $K E=\\frac{1}{2} m v^{2}$)\n",
        "\n",
        "* **Potential energy**: $V \\Psi(x)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ELVzEm879_7"
      },
      "source": [
        "*What would a typical Schroedinger solution look like? - All the solutions to the wave function take these two forms:*\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$ when $n=1,3,5 \\ldots$ (is odd $)$\n",
        "\n",
        "> $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\sin \\left(\\frac{\\pi n x}{L}\\right)$ when $n=2,4,6 \\ldots$ (is even)\n",
        "\n",
        "*Now looking at $\\psi$, the probable position of an electron:*\n",
        "\n",
        "* central question: where is the electron?\n",
        "\n",
        "* n is the energy state / level of an electron (look above at quantum numbers)\n",
        "\n",
        "* When an electron is state n=1 (its first energy state) we apply the first formula: $\\Psi(x)=\\sqrt{\\frac{2}{L}} \\cos \\left(\\frac{\\pi n x}{L}\\right)$\n",
        "\n",
        "* then we get wave function for the electron that is in a given box in this case:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_147.png)\n",
        "\n",
        "* And if we square it, we get the probability distribution (the probable position of an electron):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_148.png)\n",
        "\n",
        "\n",
        "* And here some wave functions and probability densities for other energy states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_149.png)\n",
        "\n",
        "*And the solution that popped out was this:*\n",
        "\n",
        "> $E=\\frac{\\hbar^{2} n^{2} \\pi^{2}}{2 m L^{2}}$\n",
        "\n",
        "* Everything is a constant ($\\hbar$, $\\pi$, 2, m, L) or a whole number (here: n, which stands for the different states of an electron)\n",
        "\n",
        "* which means that energy E can ony have certain discrete (=quantum) values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIv5tk_B7h9Q"
      },
      "source": [
        "<font color=\"blue\">**How to solve the Schrödinger Equation (Time Independent)**\n",
        "\n",
        "Assumption: particle is moving on one line between 0 and a and limited by infinite blocks left and right, where its probability is 0 and the potential energy infinite = this means that the particle can only be found on the line between 0 and a.\n",
        "\n",
        "[SOLVING the SCHRODINGER EQUATION | Quantum Physics by Parth G](https://www.youtube.com/watch?v=sPZWtZ8vt1w)\n",
        "\n",
        "**Step 1: Take Schrodinger Equation, remove $V$ and focus on differential equation of kinetic energy term**\n",
        "\n",
        "* now we want to compute the wavefunction of this particle with the (Time Independent) Schrödinger Equation\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}+V \\Psi=E \\Psi$\n",
        "\n",
        "* the potential energy V is zero between 0 and a because nothing influences the particle, so it becomes this **differential equation** that we need to solve:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m} \\frac{d^{2} \\Psi}{d x^{2}}=E \\Psi$\n",
        "\n",
        "* $\\frac{d^{2} \\Psi}{d x^{2}}$ is second derivative of $\\Psi$ with respect to x\n",
        "\n",
        "We want to solve this equation:\n",
        "\n",
        "> $\\frac{-\\hbar^{2}}{2 m}$ <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> $=E \\Psi$\n",
        "\n",
        "* Normally try to solve <font color=\"red\">$ \\frac{d^{2} \\Psi}{d x^{2}}$</font> which is second derivative of $\\Psi$ with respect to x, when you know what $\\Psi$ is,\n",
        "\n",
        "* but we don't. We want to go the other way around, which is trickier.\n",
        "\n",
        "**Step 2: Rearrange the constants**\n",
        "\n",
        "Luckily we have two constants in our equation (blue):\n",
        "\n",
        "> <font color=\"blue\">$\\frac{-\\hbar^{2}}{2 m}$</font> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$E$</font> $\\Psi$\n",
        "\n",
        "Which means we can rearrange the equation to this:\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$\n",
        "\n",
        "Now we can combine all constants in one constant $-k^2$ = $\\frac{-2m E}{\\hbar^{2}}$\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "**Step 3: Identify suitable function for this equation**\n",
        "\n",
        "* So which type of function obeys this relation $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$-k^2$</font>  $\\Psi$? - Would be a [sinusoid](https://de.wikipedia.org/wiki/Sinusoid)!\n",
        "\n",
        "* $\\frac{d^2 y}{d x^{2}}=-y$ - when you start with a sine and differentiate it twice you still end up with a sinusoidal term\n",
        "\n",
        "* so if we carefully account for the constants in our equation, our solution is going to look like a sinusoid:\n",
        "\n",
        "> $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ and replacing <font color=\"blue\">$\\frac{\\sqrt{2 m E}}{\\hbar}$</font> with $k$ $\\rightarrow$ $\\Psi$ = $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "*Compare this with before (above is no minus and root taken is first term):*\n",
        "\n",
        "> $ \\frac{d^{2} \\Psi}{d x^{2}}$ = <font color=\"blue\">$\\frac{-2m E}{\\hbar^{2}}$</font>  $\\Psi$ =  <font color=\"blue\">$-k^2$</font>  $\\Psi$\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_176.png)\n",
        "\n",
        "**Step 4: Work our the boundaries to solve the differential equation, which means to get wavefunction**\n",
        "\n",
        "\n",
        "**When x = 0 $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* so at the wall at point a $\\Psi$ = 0, so that we first derivative is not getting infinite!\n",
        "\n",
        "* this works quite nice with sinusoidal, since $\\Psi = sin(kx)$ $\\rightarrow$ 0 = sin(k(0)) = 0\n",
        "\n",
        "**When x = a $\\rightarrow$ $\\Psi$ = 0**\n",
        "\n",
        "* 0 = sin(k(a)) since $\\Psi$ = sin(kx) $\\rightarrow$ 0 = sin(k(a)) = 0\n",
        "\n",
        "* we essentially find a restriction on the kind of sine wave that we can have as a solution\n",
        "\n",
        "* for example half a sine wave is a possible solution\n",
        "\n",
        "  * it's y=0 at point x=0 and x=a (at the walls), so $sin(ka) = 0$\n",
        "\n",
        "  * so we went through half a sine wave which means that this part in brackets (ka) must be equal to 180 degrees (because that's half a sine wave) $y = \\frac{1}{2}sin(x)$\n",
        "\n",
        "  * and if we use radians instead of degrees, which is the other unit of measuring angles, and a much more natural unit of measuring angles, then 180 degrees is actually equal to <font color=\"blue\">$\\pi$ radians = (ka)</font>\n",
        "\n",
        "  * So: $y = \\frac{1}{2}sin(x) = \\pi$\n",
        "\n",
        "* this means that this equation holds true if our wavefunction is half a sine wave <font color=\"blue\">$(ka)$ = $\\pi$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$</font> , recall: k = $\\frac{\\sqrt{2m E}}{\\hbar}$\n",
        "\n",
        "* from earlier: $\\Psi$ = $\\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$ =  $\\sin ($ <font color=\"blue\">$k$</font> $x)$\n",
        "\n",
        "**Step 5: Rearrange that equation to get the energy value**\n",
        "\n",
        "\n",
        "* and if we rearrange that <font color=\"blue\">$(ka)$ = $\\frac{\\sqrt{2m E}}{\\hbar}a$ = $\\pi$ </font> we have something that tells us the value of the energy $E$:\n",
        "\n",
        "> $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* (with reduced Planck constant: $\\hbar=\\frac{h}{2 \\pi} =1.054571817 \\ldots \\times 10^{-34} \\mathrm{~J} \\cdot \\mathrm{s}$)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_178.png)\n",
        "\n",
        "* in other words: if our wavefunction looks like this (half a sine function), then the energy of our particle is this $E=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* another possibe solution is a full sine wave fitting into this region, just that the value at the end of the wall is 360 degrees, because we went through the whole sine wave = 2*$\\pi$ radians\n",
        "\n",
        "  * if the wavefunction looks like a whole sine wave <font color=\"blue\">$\\frac{\\sqrt{2m E}}{\\hbar}a$</font> = 2*$\\pi$\n",
        "\n",
        "  * then the  energy of the particle is $E=\\frac{4h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "* We can continue doing this for lots of half sine waves, so we could have three or four half fine waves in our region - and in each case we can calculate the energy of a particle when its wavefunction looks like those sine waves.\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_177.png)\n",
        "\n",
        "* This phenomenom is called \"Quantization\".\n",
        "\n",
        "  * because we can only have specific wave functions, and they correspond to specific energies, a particle can therefore only have specific energies\n",
        "\n",
        "  * so it cannot be anyhting in between and it cannot be less than the minimum of half a sine wave $E_{1}=\\frac{h^{2} \\pi^{2}}{2 m a^{2}}$\n",
        "\n",
        "  * this is also why for this particular setup cosine doesnt work (normally it does though) $\\Psi=\\cos \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "**Step 6: Normalization to get probabilities**\n",
        "\n",
        "*Normalization of the wavefunction*\n",
        "\n",
        "* there is one more thing to consider when finding a solution to the Schrodinger equation: Normalization\n",
        "\n",
        "> $\\Psi = \\sqrt{\\frac{2}{a}} \\sin \\left(\\frac{\\sqrt{2 m E}}{\\hbar} x\\right)$\n",
        "\n",
        "* it adds a factor of $\\sqrt{\\frac{2}{a}}$ to our solution\n",
        "\n",
        "* physical meaning: if our particle is in the lowest energy level $E_1$. Then in our specific setup with the two walls the wavefunction looks like half a sine wave. And remember the wavefunction corresponds directly to the probability of us finding that particle at a particular point in space. And this relationshipn is if we square our wavefunction $|\\Psi|^2$ (we take the square modulus), then we get the probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnCYfCNzqmG9"
      },
      "source": [
        "<font color=\"blue\">**Schrödinger Equation (Time Dependent)**\n",
        "\n",
        "**Consider: Difference of probability in the position basis (changes over time) and the energy basis (doesn't change):**\n",
        "\n",
        "> <font color=\"red\">**The electron is still in the same shell, represented by the principal quantum number for example, because if the electron changes the shell, energy needs to be added or removed from the overall system. however if energy stays the same, it means the electron is still in the same shell, but \"moving\" around = probability distribution of finding it somewhere in this shell changes over time which is represented by the rotation $e^{i \\frac{\\hat{H} * t}{\\hbar}}$**</font>\\\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right] \\psi(\\vec{r})=E \\psi(\\vec{r})\n",
        "$\n",
        "\n",
        "The object on the left that acts on $\\psi(x)$ is an example of an operator.\n",
        "\n",
        ">$\n",
        "\\left[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\\right]\n",
        "$ = Operator\n",
        "\n",
        "In effect, what is says to do is \"take the second derivative of $\\psi(x)$, multiply the result by $-\\left(\\hbar^{2} / 2 m\\right)$ and then add $V(x) \\psi(x)$ to the result of that.\"\n",
        "\n",
        "Quantum mechanics involves many different types of operators. This one, however, plays a special role because it appears on the left side of the Schrödinger equation. **It is called the Hamiltonian operator and is denoted as**\n",
        "\n",
        "> $\n",
        "\\hat{H}=-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})\n",
        "$\n",
        "\n",
        "**Therefore the time-dependent Schrödinger equation can be written as**:\n",
        "\n",
        "> $\n",
        "\\hat{H} \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "with $\\hat{H}$ = $(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r}))$ will be:\n",
        "\n",
        "> $\n",
        "(-\\frac{\\hbar^{2}}{2 m} \\nabla^{2}+V(\\vec{r})) \\; \\psi(x, t)=i \\hbar \\frac{\\partial}{\\partial t} \\psi(x, t)\n",
        "$\n",
        "\n",
        "bzw. rewritten:\n",
        "\n",
        "> $\\left[-\\frac{\\hbar^{2}}{2 m} \\frac{\\partial^{2}}{\\partial x^{2}}+V(x, t)\\right] \\Psi(x, t) = i \\hbar \\frac{\\partial}{\\partial t} \\Psi(x, t)$\n",
        "\n",
        "bzw written in another way (single particle variant):\n",
        "\n",
        "> <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> + <font color=\"green\">$V(x, t)$</font> ]\n",
        " <font color=\"blue\">$|\\psi\\rangle$</font> = $i \\hbar \\frac{\\partial}{\\partial t}$ <font color=\"blue\">$|\\psi\\rangle$</font>\n",
        "\n",
        "* <font color=\"red\">$[-\\frac{\\hbar^{2}}{2 m} \\nabla^{2} $</font> Kinetic energy\n",
        "\n",
        "* <font color=\"green\">$V(x, t)$</font> Potential energy\n",
        "\n",
        "* <font color=\"blue\">$|\\psi\\rangle$</font>  the wave function\n",
        "\n",
        "<font color=\"blue\">*Schrodinger equation: Derivation and how to use it (in Time Evolution)*\n",
        "\n",
        "Important rules of physics:\n",
        "\n",
        "* Conservation of energy -> deeply integrated into Schrodinger equation\n",
        "* total energy doesn't change\n",
        "* you can't make of destroy energy\n",
        "\n",
        "**Since we can write a quantum state $|\\Psi \\rangle$ in whatever basis we want, we can choose the energy Eigenbasis**. <font color=\"red\">what is meant by that? is that the principle quantum number for example</font>\n",
        "\n",
        "* We can write a state as the superposition of different energies.\n",
        "\n",
        "* And if we measure the energy of the particle it will be one of these with their probability\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* with probability for example $|\\beta|^2$ for measuring second state\n",
        "\n",
        "**Say the state evolves in time, in other words we apply the time evolution $U$ (or $T$) to $|\\Psi \\rangle$, so $T |\\Psi \\rangle$**\n",
        "\n",
        "* what condition do we want to impose on the new energies of the state?\n",
        "\n",
        "* In other words: how we want conservation of energy to look in quantum mechanics?\n",
        "\n",
        "**Let's start where a particle just has one energy $E$ when we start**\n",
        "\n",
        "* means: it is an energy Eigenstart !!\n",
        "\n",
        "* we evolve it forward in time and look at the energy of the new state. That energy should be also $E$, otherwise energy wouldn't be conserved (Like in classical mechanics).\n",
        "\n",
        "> $|\\Psi \\rangle$ = $|1 \\rangle$ $\\rightarrow$ $T|\\Psi \\rangle$\n",
        "\n",
        "* Now also the average energy shouldn't change after some time, otherwise the energy wouldn't be conserved either.\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$\n",
        "\n",
        "* if you measured athe particle's energy initially with a certain probability $|\\beta|^2$, and then after time evolution again, it should be the same probability to measure that energy!\n",
        "\n",
        "* this is so strong, it gives us the schroedinger equation\n",
        "\n",
        "\n",
        "**We need to how the coefficients have changed in the new equation after time evolution**:\n",
        "\n",
        "> $|\\Psi \\rangle$ = $\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle$ (before)\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $\\alpha' |\\Psi \\rangle + \\beta' |\\Psi \\rangle + \\gamma' |\\Psi \\rangle$ (after)\n",
        "\n",
        "* We want the probability to be the same, but that probability is just the lenght of this compex number squared $|\\gamma|^2 = |\\gamma'|^2 = 1$\n",
        "\n",
        "> <font color=\"red\">**So each coefficient can be represented as an arrow with equal length $|\\gamma|^2$ and $|\\gamma'|^2$ (hence the probability of measuring that energy this state is still the same!!), BUT $|\\gamma'|^2$ may be rotated by an angle $\\phi$**. This angle is new vector = rotation * old vector:</font>\n",
        "\n",
        "> <font color=\"red\">$\\gamma' = e^{i\\phi}\\gamma$</font>\n",
        "\n",
        "Let's plug that rotation $e^{i\\phi}$ in to our previous equation:\n",
        "\n",
        "> <font color=\"red\">$T |\\Psi \\rangle$ = $e^{i\\phi_1}\\alpha |\\Psi \\rangle + e^{i\\phi_2}\\beta |\\Psi \\rangle + e^{i\\phi_3}\\gamma |\\Psi \\rangle$</font>\n",
        "\n",
        "* where the angles / rotations $e^{i\\phi}$ are different for every energy = they are all rotated by a different amount !! Otherwise the rotation can be brought out and present and future state would be the essentially same:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}\\alpha |\\Psi \\rangle + e^{i\\phi}\\beta |\\Psi \\rangle + e^{i\\phi}\\gamma |\\Psi \\rangle$ = $e^{i\\phi} (\\alpha |\\Psi \\rangle + \\beta |\\Psi \\rangle + \\gamma |\\Psi \\rangle)$ (this is showing that it's wrong!)\n",
        "\n",
        "The overall rotation wouldn't affect any measurement outcomes. Means no matter in which crazy situation you brought the particle in, it does nothing, which can't be right.\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i\\phi}|\\Psi \\rangle$ (this is showing that it's wrong!\n",
        "\n",
        "\n",
        "* also the amount of rotation depends on time (little going forwardf = little rotation). That suggests the right amount of angle to rotate is Energy x Time. Plus some constants to deal with units and scaling etc.\n",
        "\n",
        "> <font color=\"red\">$\\phi = \\frac{E * t}{\\hbar}$</font>\n",
        "\n",
        "* And that's what the Schroedinger equation will tell you will happen to the state:\n",
        "\n",
        "> $T |\\Psi \\rangle$ = $e^{i \\frac{E * t}{\\hbar}}\\alpha |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\beta |\\Psi \\rangle + e^{i \\frac{E * t}{\\hbar}}\\gamma |\\Psi \\rangle$\n",
        "\n",
        "* And that's the same: (with $\\hat{H}$ for energy measurement operator, Hamiltonian):\n",
        "\n",
        "> <font color=\"red\">$T(t) |\\Psi \\rangle = e^{i \\frac{\\hat{H} * t}{\\hbar}}|\\Psi \\rangle$</font>\n",
        "\n",
        "*Common result for 2 observations:*\n",
        "\n",
        "Time Evolution per each step, observer 1:\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_1$ = $e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$</font>\n",
        "\n",
        "Time Evolution at the end for observer 2 (not seeing time step 1):\n",
        "\n",
        "\n",
        "> $| \\Psi \\rangle$ $\\rightarrow$ at $t_2$ = <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font>\n",
        "\n",
        "Where:\n",
        "\n",
        "> <font color=\"orange\">$e^{\\frac{i \\mathcal{H} (t_1 + t_2)}{\\hbar}} | \\Psi \\rangle$</font> = <font color=\"blue\">$e^{\\frac{i \\mathcal{H} t_2}{\\hbar}} (e^{\\frac{i \\mathcal{H} t_1}{\\hbar}} | \\Psi \\rangle)$\n",
        "\n",
        "Why? - because our angle of rotation depends on $t$ (and not $t^2$ or anything): $T\\left(t_{1}+t_{2}\\right)=T\\left(t_{2}\\right) T\\left(t_{1}\\right)$\n",
        "\n",
        "Taken from [Schrodinger equation comment response and homework answers video](https://www.youtube.com/watch?v=M_2h5uQ0SIc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Harmonic Oscillator (Hamiltonians / Solution of Schrodinger equation)**\n",
        "\n",
        "* The nuclear motion Schrödinger equation can be solved in a space-fixed (laboratory) frame, **but then the translational and rotational (external) energies are not accounted for**. Only the (internal) atomic vibrations enter the problem.\n",
        "\n",
        "* Further, for molecules larger than triatomic ones, it is quite common to introduce the **harmonic approximation, which approximates the potential energy surface** as a [quadratic function](https://en.m.wikipedia.org/wiki/Quadratic_function) of the atomic displacements. **This gives the harmonic nuclear motion Hamiltonian**.\n",
        "\n",
        "* Making the harmonic approximation, we can **convert the Hamiltonian into a sum of uncoupled one-dimensional [harmonic oscillator](https://en.m.wikipedia.org/wiki/Harmonic_oscillator) Hamiltonians**.\n",
        "\n",
        "> **The one-dimensional harmonic oscillator is one of the few systems that allows an exact solution of the Schrödinger equation.**\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Nullpunktsenergie#Harmonischer_Oszillator\n",
        "\n",
        "https://de.m.wikipedia.org/wiki/Harmonischer_Oszillator_(Quantenmechanik)"
      ],
      "metadata": {
        "id": "EKeKuaJkf4Pw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sf5tauFtttx"
      },
      "source": [
        "###### ***Matrix Mechanics*** *(Heisenberg, discrete basis, spin representation, Kronecker delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Mechanics (spin representation - discrete basis) - (Heisenberg)**\n",
        "\n",
        "* Matrix Mechanics (spin representation - discrete basis)\n",
        "\n",
        "* Video: [Matrix formulation of quantum mechanics](https://www.youtube.com/watch?v=wIwnb1ldYTI)\n",
        "\n",
        "* **Matrix mechanics ('matrix formulation'): Most useful when we deal with finite, discrete bases (like spin representation).**\n",
        "\n",
        "* **Matrix formulation of quantum mechanics reduces to the rules of simple matrix multiplication.**\n",
        "\n",
        "> $\\hat{A}=\\sum_{i j} A_{i j}\\left|u_{i}\\right\\rangle\\left\\langle u_{j}\\right| \\quad A_{i j}=\\left\\langle u_{i}|\\hat{A}| u_{j}\\right\\rangle$\n",
        "\n",
        "* An operator A can be written in the u basis as the sum over the outer products of the basis states\n",
        "* And the expansion coefficients Aij are given by the matrix elements of A with respect to the basis states\n",
        "* The expansion coefficients for an operator are labeled by 2 indices, so we will arrange them in a form of a square matrix, with the first index denoting the row of the matrix and the second index the column of the matrix\n",
        "* There operators are written as matrices\n",
        "\n",
        "> $\\left(\\begin{array}{ccccc}A_{11} & A_{12} & \\cdots & A_{1 j} & \\cdots \\\\ A_{21} & A_{22} & \\cdots & A_{2 j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\\\ A_{i 1} & A_{i 2} & \\cdots & A_{i j} & \\cdots \\\\ \\vdots & \\vdots & & \\vdots & \\end{array}\\right)$\n",
        "\n",
        "Kets are written as column vectors:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_218.png)\n",
        "\n",
        "Matrix formulation of Bra's: re-arrange complex, conjugate coefficient as a row vector:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_219.png)\n",
        "\n",
        "An operator A can be written in the u basis as the sum over the outer products of the basis states. An Aij are the expansion coefficient in this case. Operators are written as matrices:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_220.png)\n",
        "\n",
        "Summary of the matrix formulation of quantum mechanics for kets, bras and operators:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_221.png)\n",
        "\n",
        "We will see how simple matrix multiplication rules work for the following 4 operations:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_222.png)\n",
        "\n",
        "First, in the matrix formulation of quantum mechanics, a bracket is the matrix product of a row vector with a column vector, and gives a scalar:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_223.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_224.png)\n",
        "\n",
        "Adjoint operator: describe the action of an operator in the dual space:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_225.png)\n",
        "\n",
        "Write an operator as an outer product of two states:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_226.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_227.png)"
      ],
      "metadata": {
        "id": "FdyLdYgFeqFK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdJ5yNTZibYy"
      },
      "source": [
        "###### ***Wave Mechanics*** *(Schrödinger, continuous basis, position representation, Dirac delta function)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYIaQDNv0C1c"
      },
      "source": [
        "**First: let's go from discrete basis $u_i$ to continuous basis $v_{\\alpha}$**\n",
        "\n",
        "* Much used **discrete basis**: Spin of quantum particles\n",
        "\n",
        "* Much used **continuous basis**: position of quantum particles (this one leads to the idea of wave function)\n",
        "\n",
        "> **The generalization is straightforward: it amounts to replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$ of two continuous variables and sum over these indices by integrals over continuous indices**.\n",
        "\n",
        "* first concept: we work with an orthonormal basis $\\left\\langle u_{i} \\mid u_{j}\\right\\rangle=\\delta_{i j}$. replacing Kronecker delta functions $\\delta_{i j}$ of two discrete variables with the Dirac delta function $\\delta (\\alpha - \\beta)$\n",
        "\n",
        "* then we look at the expansion of Ket in a particular basis: replacing a sum over i with an integral over alpha\n",
        "\n",
        "* in yellow: just a proof why we would write a Dirac delta function only under an integral sign\n",
        "\n",
        "* last part: representation of an operator in a particular basis (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_217.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWBTMFaxI7Ie"
      },
      "source": [
        "**Wave Mechanics (Schrödinger)**\n",
        "\n",
        "* Wave Mechanics: Wave Function (position representation - continuous basis)\n",
        "\n",
        "* Check also part under Operator: Translation Operators (**Wave Mechanics: Translation Operator**)\n",
        "\n",
        "* Video: [Wave functions in quantum mechanics](https://www.youtube.com/watch?v=2lr3aA4vaBs)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Wave_function\n",
        "\n",
        "* wave functions is one possible way looking at a quantum system\n",
        "\n",
        "* is the so called position representation of quantum mechanics\n",
        "\n",
        "* leads to wave mechanics (for continuous basis)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_232.png)\n",
        "\n",
        "Computing scalar between two states and the normalization:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_233.png)\n",
        "\n",
        "How to get from the position representation to the momentum representation (via a first order differential equation):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_234.png)\n",
        "\n",
        "**Transformation matrix $\\langle x|p\\rangle$ to go from the position representation to the momentum representation**:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_235.png)\n",
        "\n",
        "To change between both representations we use Fourier transform:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_236.png)\n",
        "\n",
        "If we go to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_237.png)\n",
        "\n",
        "Summary:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_238.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XCE4EGuJJij"
      },
      "source": [
        "**Wave Mechanics: Position and momentum operators acting on wave functions**\n",
        "\n",
        "* Video: [Position and momentum operators acting on wave functions](https://www.youtube.com/watch?v=Yw2YrTLSq5U)\n",
        "\n",
        "* Action of position and momentum operators on wave functions\n",
        "\n",
        "* The action of the position operator: it multiplies a wave function by x:\n",
        "\n",
        "> $x \\psi(x)$\n",
        "\n",
        "* the action of a momentum operator: it acts by calculating the derivative of a wave function\n",
        "\n",
        "> $-i \\hbar \\frac{d \\psi(x)}{d x}$\n",
        "\n",
        "* First: describe the act of a position operator in the position basis, and the act of a momentum operator in the momentum basis:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_239.png)\n",
        "\n",
        "Second: describe the act of the momentum operator on a state Psi when written in the position representation (=basis) is such that the momentum operator calculates the derivative of the wavefunction and then multiplies the result by minus i h-bar (we need the translation operator):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_240.png)\n",
        "\n",
        "Third: what happens when we act with the position operator in the momentum basis (proof: the momentum space wavefunction is related to the real space wavefcunction by a Fourier transform):\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_241.png)\n",
        "\n",
        "Generalize this to 3 dimensions:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_242.png)\n",
        "\n",
        "Summary: as you can see there are different representations for the same thing, but the maths is differently difficult. So the task is to find a representation that is easy for a certain problem:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_243.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-w7Fv4O91T"
      },
      "source": [
        "###### *Wavefunction & Global Phase Difference (Why we factor it out and leave phase difference only)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe you have, but from where I stand it was the first time to see the global phase represented in a Bloch Sphere. Thanks Professor Ioannis G. Karafyllidis for this insight. ⬇\n",
        "\n",
        "☸ The global phase is actually an inclination of the Bloch Sphere.\n",
        "\n",
        "⚛ In quantum field theory, the concept of global phase in a one-qubit system can be understood in the context of symmetries, particularly the global U(1) phase symmetry. U(1) is a mathematical group that represents a continuous symmetry associated with global phase transformations. We can outline how quantum field theory incorporates this symmetry to explain the concept:\n",
        "\n",
        "✅ Global U(1) Phase Symmetry:\n",
        "In quantum field theory, we work with fields and operators that are often complex-valued. The global U(1) phase symmetry refers to the ability to multiply the entire quantum state by a complex phase factor without affecting the physical observables. Mathematically, for a one-qubit system, the global U(1) phase transformation can be represented as follows:\n",
        "|ψ⟩ → e^(iθ)|ψ⟩ where, θ is a real number representing the phase transformation.\n",
        "\n",
        "✅ No Observable Impact: The key feature of this global phase transformation is that it does not change the probabilities of measurement outcomes or any other physically observable quantities. The global phase is, in a sense, \"gauged out\" because it affects both the quantum state and its complex conjugate, so it cancels out in terms of probabilities.\n",
        "\n",
        "✅ Conservation of Probability: The conservation of probability is ensured by the U(1) phase symmetry. The probabilities of measuring a qubit in any state remain constant under global phase transformations. This property is closely related to the conservation of probability in quantum systems.\n",
        "\n",
        "✅ Interference and Relative Phases: While the global phase itself is not physically meaningful, relative phases between different quantum states are significant. Quantum field theory allows for interference effects, which arise from relative phases between states. This interference is crucial in understanding phenomena like quantum entanglement and quantum superposition.\n",
        "\n",
        "✴ In summary, quantum field theory incorporates the global U(1) phase symmetry to explain the concept of global phase in a one-qubit system. This symmetry allows for phase transformations that do not affect the probabilities of measurement outcomes, highlighting the importance of relative phases in quantum interference effects and quantum phenomena.\n"
      ],
      "metadata": {
        "id": "163At1BtkCqb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIwU3Qnjifn-"
      },
      "source": [
        "**How does the Wavefunction (one type) look like?**\n",
        "\n",
        "> $\\psi=e^{\\frac{1}{\\hbar}(px - Et)}$\n",
        "\n",
        "* p = momentum in direction x, x = position along x direction, E = energy, t = time\n",
        "\n",
        "* e is the exponential function, normally it doesn't look like a wave, like $e^{-x}$ or $e^{x}$\n",
        "\n",
        "* but the imaginary number $i=\\sqrt{-1}$ turns an exponential function into a wave\n",
        "\n",
        "* sinoisdal functions (sine and cosine) can be written in terms of the exponential function with $i$ in the exponent\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_244.png)\n",
        "\n",
        "* we could take one complex wave function and break it down into simpler waves, then we apply same maths on the simpler waves:\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_245.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVcEGLc6rS9p"
      },
      "source": [
        "Global phase factor $e^{i\\theta}$:\n",
        "\n",
        "* Eigenvalues and Eigenvectors exist also in other vector spaces than state spaces, but because state space is a complex vector space there is one important extra subtlety compared to real vector spaces which has to do with the global phase factor $e^{i\\theta}$\n",
        "\n",
        "* after choosing alpha to make the length of an Eigenstate equal to 1, we still have some extra freedom in the Eigenstate\n",
        "\n",
        "* multiplying $|\\Psi\\rangle$ with a Global phase factor $e^{i\\theta}$ makes the length of the resulting $|\\Psi'\\rangle$ still = 1\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_258.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens when we square the wave function:\n",
        "* the function is an oscillation in quantum possibilities moving through space and time\n",
        "* but it's a complex wave with one real and one imaginary component\n",
        "* **the components oscillate in sync with each other - but they are offset, shifted in phase by a constant amount**\n",
        "> phase is just the wave's current state in its up-down oscillation\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0964.png)"
      ],
      "metadata": {
        "id": "KiDe4HzbgNh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we apply the Born rule we are squaring these two waves and adding them together\n",
        "* but it turns out that this value doesn't depend on phase. The magnitude squared of the real and imaginary components stays the same, even as those components move up and down\n",
        "* It is that magnitude squared that we can observe, it determines the particles position\n",
        "* the phase itself is fundamentally unobservable. You can shift phase by any amount and you wouldnt change the resulting position of the particle, as long as you do the same shift to both the real and the imaginary components.\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0965.png)\n"
      ],
      "metadata": {
        "id": "T6Kn28ZZgUsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact as long as you make the same shift across the entire wave function, all the observables are unchanged.\n",
        "* We call this form of transformation a global phase shift, and it's analogous to transforming our altitude zero point up or down by the same amount everywhere.\n",
        "* the equations of quantum mechanics have what we call **global phase invariance**\n",
        "* **Global phase is a Gauge symmetry of the system**\n",
        "\n",
        "![geometry](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_0966.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "70R04sWsgbFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminder: Trigonometrie:\n",
        "\n",
        "> $\\begin{aligned} \\mathrm{e}^{\\mathrm{i} x} &=\\sum_{k=0}^{\\infty} \\frac{(\\mathrm{i} x)^{k}}{k !}=\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l}}{(2 l) !}+\\sum_{l=0}^{\\infty} \\frac{(\\mathrm{i} x)^{2 l+1}}{(2 l+1) !} \\\\ &=\\underbrace{\\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l}}{(2 l) !}}_{\\cos x}+\\underbrace{\\mathrm{i} \\sum_{l=0}^{\\infty}(-1)^{l} \\frac{x^{2 l+1}}{(2 l+1) !}}_{\\sin x} \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos x+\\mathrm{i} \\sin x \\\\  \\mathrm{e}^{\\mathrm{i} x}&=\\cos \\varphi+\\mathrm{i} \\sin \\varphi \\\\ \\mathrm{e}^{\\mathrm{i} x}&=x+\\mathrm{i} y \\end{aligned}$ Das ist die sogenannte [Eulerformel](https://de.m.wikipedia.org/wiki/Eulersche_Formel)!\n",
        "\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Sine_cosine_one_period.svg/600px-Sine_cosine_one_period.svg.png)\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_040.jpg)\n",
        "\n",
        "![ggg](https://upload.wikimedia.org/wikipedia/commons/3/3b/Circle_cos_sin.gif)"
      ],
      "metadata": {
        "id": "QX7SeO65gi0O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBGbzlHLPGlp"
      },
      "source": [
        "* The only reason phase is important is because it brings about interference effects. And interference effects are only dependent on the difference in phase between the two waves (we will abstract basis states to waves for now).\n",
        "\n",
        "> **Therefore, we can say that it’s the difference that counts, and not the absolute value.**\n",
        "\n",
        "* For example, if the phase difference is π radians then the waves would cancel each other out.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sd8WkT9PbZX"
      },
      "source": [
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/quantum_179.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8WPjzqBPfSd"
      },
      "source": [
        "We can conclude that the absolute value of the phase shift for both waves is meaningless to the interference. For as long as they both retain the phase difference, then the interference effect will be constant, and that’s what matters!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOiHZhJKPkMX"
      },
      "source": [
        "**Why the global phase doesn’t matter**\n",
        "\n",
        "As observed above, what matters is the phase difference. So think about it, if you’re describing the phase of two waves, it’s redundant to state both phases. **The better approach is to just state the phase difference**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjLdM9G6PojT"
      },
      "source": [
        "* The global phase is the absolute value of the phase shift for both waves.\n",
        "\n",
        "* For example, wave one and two each have a phase of (π/2) radians and (3π/2) radians, respectively. In this case, the phase (π/2) is a global phase since the phase difference (what actually counts) is π radians → (3π/2 - π/2 ).\n",
        "\n",
        "* Using the same example, we can define the relative phase (also known as the local phase) as the phase difference (π rad)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUI_HZzP43M"
      },
      "source": [
        "We’ve found a method to **bypass the need for a fourth dimension by factoring out the global phase** and replacing the second phase with the relative phase. We can represent this mathematically.\n",
        "\n",
        "A general qubit state can be written\n",
        "\n",
        "> $|\\psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$\n",
        "\n",
        "with complex numbers, α and β, and the normalization constraint require that:\n",
        "\n",
        "\n",
        "> $|\\alpha|^{2}+|\\beta|^{2}=1$\n",
        "\n",
        "As previously stated, we can express the amplitudes in polar coordinates as (General equation for a qubit state):\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha} e^{i \\phi_{\\alpha}}|0\\rangle+r_{\\beta} e^{i \\phi_{\\beta}}|1\\rangle$\n",
        "\n",
        "with four real parameters:\n",
        "\n",
        "> $r_{\\alpha}, \\phi_{\\alpha}, r_{\\beta}$ and $\\phi_{\\beta}$\n",
        "\n",
        "**However, the only measurable quantities are the probabilities |α|² and |β|², so multiplying the state by an arbitrary factor $e^{iγ}$ (global phase) has no observable consequences, because**:\n",
        "\n",
        "> $\\left|e^{i \\gamma} \\alpha\\right|^{2}=\\left(e^{i \\gamma} \\alpha\\right)^{*}\\left(e^{i \\gamma} \\alpha\\right)=\\left(e^{-i \\gamma} \\alpha^{*}\\right)\\left(e^{i \\gamma} \\alpha\\right)=\\alpha^{*} \\alpha=|\\alpha|^{2}$\n",
        "\n",
        "Therefore, we can factor out $e^{iΦ}$ from the general equation:\n",
        "\n",
        "> $|\\psi\\rangle=e^{i \\phi_{\\alpha}}\\left(r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle\\right)$\n",
        "\n",
        "Now, if you calculate the amplitude |ψ|², the factor ($e^{iΦ_α}$) in front will vanish by the argument above. This is why we called it the global phase. However, the relative phase is the phase difference noted as (Φ_α - Φ_β). This is an observable-ish quantity which manifests through interference effects.\n",
        "\n",
        "Let’s consolidate the above equation into:\n",
        "\n",
        "> $|\\psi\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i\\left(\\phi_{\\beta}-\\phi_{\\alpha}\\right)}|1\\rangle=r_{\\alpha}|0\\rangle+r_{\\beta} e^{i \\phi}|1\\rangle$\n",
        "\n",
        "> $r_{\\alpha} \\in \\mathbb{R}, r_{\\beta} \\in \\mathbb{R}, \\phi \\in \\mathbb{R} \\mid \\phi=\\phi_{\\beta}-\\phi_{\\alpha}$\n",
        "\n",
        "where r_α, r_β and Φ all real parameters.\n",
        "\n",
        "**Notice that this equation can be represented in 3-D, as the global phase is gone.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odWaJ8hJPDMa"
      },
      "source": [
        "https://pavanjayasinha.medium.com/but-what-is-a-quantum-phase-factor-d05c15c321fe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8P-evW3U0Vx"
      },
      "source": [
        "**Physical Meaning of Phase**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Remember: $e^{2 \\pi i}$ = 1 (Identity)"
      ],
      "metadata": {
        "id": "lQpy1dj0iHji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNcqwo6R4lV"
      },
      "source": [
        "* In quantum mechanics, a phase factor is a complex coefficient $e^{i \\theta}$ that multiplies a ket $|\\psi\\rangle$ or bra $\\langle\\phi|$.\n",
        "\n",
        "* <font color=\"blue\">**It does not, in itself, have any physical meaning**, since the introduction of a phase factor does not change the expectation values of a Hermitian operator.\n",
        "\n",
        "> That is, the values of $\\langle\\phi|A| \\phi\\rangle$ and $\\left\\langle\\phi\\left|e^{-i \\theta} A e^{i \\theta}\\right| \\phi\\right\\rangle$ are the same.\n",
        "\n",
        "* <font color=\"red\">However, differences in phase factors between two interacting quantum states can sometimes be measurable (such as in the Berry phase) and this can have important consequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syr76IkvRoPD"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Phase_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T3nptBTPNo"
      },
      "source": [
        "* When people say that the phase doesn't matter, they mean the overall, \"global\" phase. In other words, the state $|0\\rangle$ is equivalent to $e^{i \\theta}|0\\rangle$, the state $|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime}}|1\\rangle$, and the state $|0\\rangle+|1\\rangle$ is equivalent to $e^{i \\theta^{\\prime \\prime}}(|0\\rangle+|1\\rangle)$.\n",
        "\n",
        "> Siehe auch Eulersche Formel: $e^{i \\phi}$ https://mathepedia.de/Eulersche_Formel.html\n",
        "\n",
        "> Note that \"equivalence\" is not preserved under addition, since $e^{i \\theta}|0\\rangle+e^{i \\theta^{\\prime}}|1\\rangle$ is not equivalent to $|0\\rangle+|1\\rangle$, because there can be a relative phase $e^{i\\left(\\theta-\\theta^{\\prime}\\right)}$.\n",
        "\n",
        "* If we wanted to describe this very simple fact with unnecessarily big words, we could say something like \"the complex projective Hilbert space of rays, the set of equivalence classes of nonzero vectors in the Hilbert space under multiplication by complex phase, cannot be endowed with the structure of a vector space\".\n",
        "\n",
        "* Because the equivalence doesn't play nicely with addition, **it's best to just ignore the global phase ambiguity whenever you're doing real calculations**. Finally, when you're done with the entire calculation, and arrive at a state, you are free to multiply that final result by an overall phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBLDuK4vRpXR"
      },
      "source": [
        "https://physics.stackexchange.com/questions/552796/the-importance-of-the-phase-in-quantum-mechanics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hew4HucSHKJ"
      },
      "source": [
        "* Phase: Any one point or portion in a recurring series of changes, as in the changes of motion of one of the particles constituting a wave or vibration; one portion of a series of such changes, in distinction from a contrasted portion, as the portion on one side of a position of equilibrium, in contrast with that on the opposite side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxiSY8ddRqi1"
      },
      "source": [
        "https://courses.lumenlearning.com/boundless-chemistry/chapter/orbital-shapes/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CThWPZdFEXlO"
      },
      "source": [
        "In principle, we need four real numbers to describe a qubit, two for $\\alpha$ and two for $\\beta$. The constraint $|\\alpha|^{2}+|\\beta|^{2}=1$ reduces to three numbers.\n",
        "\n",
        "In quantum mechanics, two vectors that differ from a global phase factor are considered equivalent. A global phase factor is a complex number of unit modulus multiplying the state. By eliminating this factor, a qubit can be described by two real numbers $\\theta$ and $\\phi$ as follows:\n",
        "\n",
        ">$\n",
        "|\\psi\\rangle=\\cos \\frac{\\theta}{2}|0\\rangle+\\mathrm{e}^{\\mathrm{i} \\phi} \\sin \\frac{\\theta}{2}|1\\rangle\n",
        "$\n",
        "\n",
        "where $0 \\leq \\theta \\leq \\pi$ and $0 \\leq \\phi<2 \\pi .$ In the above notation, state $|\\psi\\rangle$ can be represented by a point on the surface of a sphere of unit radius, called Bloch sphere. Numbers $\\theta$ and $\\phi$ are spherical angles that locate the point that describes $|\\psi\\rangle$, as shown in Fig. A.1. The vector showed there is given by\n",
        "\n",
        "> $\\left[\\begin{array}{c}\\sin \\theta \\cos \\phi \\\\ \\sin \\theta \\sin \\phi \\\\ \\cos \\theta\\end{array}\\right]$\n",
        "\n",
        "When we disregard global phase factors, there is a one-to-one correspondence between the quantum states of a qubit and the points on the Bloch sphere. State $|0\\rangle$ is in the north pole of the sphere, because it is obtained by taking $\\theta=0 .$ State $|1\\rangle$ is in the south pole. States\n",
        "\n",
        "> $\n",
        "|\\pm\\rangle=\\frac{|0\\rangle \\pm|1\\rangle}{\\sqrt{2}}\n",
        "$\n",
        "\n",
        "are the intersection points of the $x$-axis and the sphere, and states $(|0\\rangle \\pm \\mathrm{i}|1\\rangle) / \\sqrt{2}$ are the intersection points of the $y$-axis with the sphere.\n",
        "\n",
        "The representation of classical bits in this context is given by the poles of the Bloch sphere and the representation of the probabilistic classical bit, that is, 0 with probability $p$ and 1 with probability $1-p$, is given by the point in $z$-axis with coordinate $2 p-1$. The interior of the Bloch sphere is used to describe the states of a qubit in the presence of decoherence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">**Machine Learning**"
      ],
      "metadata": {
        "id": "xEVSiuftty-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rückschlag für US-Sanktionen: Chinesische Wissenschaftler entwickeln optischen \"McChip\", https://www.telepolis.de/features/Rueckschlag-fuer-US-Sanktionen-Chinesische-Wissenschaftler-entwickeln-optischen-McChip-9716392.html"
      ],
      "metadata": {
        "id": "tAx06z4f2bg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-io-announcements?hl=en"
      ],
      "metadata": {
        "id": "ujuL5SM22xwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imperative programming style (tensorflow, also object-oriented languages) and functional programming style (jax, form of declarative programming)\n",
        "\n",
        "https://learn.microsoft.com/en-us/dotnet/standard/linq/functional-vs-imperative-programming"
      ],
      "metadata": {
        "id": "wAHhZS2uxF5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video: [Attention in Transformers](https://youtu.be/eMlx5fFNoYc?si=C7mPpdqouDtZpDN5)"
      ],
      "metadata": {
        "id": "RXwcRdnpNXKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formal Aspects of Language Modeling\n",
        "Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu and Li Du\n",
        "Thursday 9th November, 2023"
      ],
      "metadata": {
        "id": "OmZK-lF9zgGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Foundation Model*"
      ],
      "metadata": {
        "id": "86rBYa_u4urx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A foundation model development cheatsheet, from foundation model developers.\n",
        "\n",
        "The folks over at EleutherAI, in collaboration with MIT, AI2, HuggingFace, Stanford, Princeton, and more, have released the foundation model development cheatsheet, a quick-start guide to familiarize new developers with useful tools and resources for developing new open models.\n",
        "\n",
        "You can use it to easily find, for example, pre-training data sources, tools and frameworks for data preparation, model training, model evaluation, and more.\n",
        "\n",
        "This is a great initiative aligned with EleutherAI's mission to lower barriers to entry of research and development of foundation models.\n",
        "\n",
        "Cheat sheet (interactive): https://fmcheatsheet.org/\n",
        "Github repo: https://lnkd.in/dSDkbY-K\n",
        "Paper: https://lnkd.in/d5DRns4i\n",
        "Blog: https://lnkd.in/dg4drU_3"
      ],
      "metadata": {
        "id": "E433hvia4xsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Finetune LLMs*"
      ],
      "metadata": {
        "id": "JgPN5UHup5lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mixer of Experts(MOE) — Modern architecture for divide and concur learning\n",
        "\n",
        "https://medium.com/@sthanikamsanthosh1994/the-mixer-of-experts-moe-modern-architecture-for-divide-and-concur-learning-dbe10ffa8436"
      ],
      "metadata": {
        "id": "dgUkkXpDvYie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Overview*"
      ],
      "metadata": {
        "id": "CBU1tb4vs16v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1704.png)"
      ],
      "metadata": {
        "id": "2WTi1ggcqbc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Finetuning LLMs*\n",
        "\n",
        "* Supervised\n",
        "* RLHF\n",
        "* Distillation\n",
        "* deprecated: Parameter-Efficient Fine-Tuning using PEFT: https://arxiv.org/abs/2312.12148\n",
        "* pending (?): Lora and QLora\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/\n",
        "\n",
        "https://www.deeplearning.ai/short-courses/finetuning-large-language-models/"
      ],
      "metadata": {
        "id": "XEjwjS3Lsp2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Parameter efficient Turning (PEFT)*"
      ],
      "metadata": {
        "id": "WikX85R-r4I2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised"
      ],
      "metadata": {
        "id": "TqEuagbhr7fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This is a good primer go/parameter-efficient-tuning-primer\n",
        "* what I didn't get very well is why the candidate related int-8 with LoRA tuning\n",
        "* for hosting on CPU\n",
        "* I think the candidate mixed-up things\n",
        "* LoRA tuning is to do fine-tuning efficiently by just changing some of the weights inside the model. The amount of weights to change is decided by its rank, currently in Vertex we only offer rank 1, but this is hidden. OpenAI offers a way larger rank, and it's more powerful\n",
        "* there's QLoRA as well for quantized models.\n",
        "* What actually reduces memory is quantization, and there you can go to int-8 and host it inCPU if the model permits it\n",
        "* In fact, gemini-pro and bison@002 are quantized\n",
        "* obviously as these are bigs the quantization still requires a good amount of TPUs 😀\n"
      ],
      "metadata": {
        "id": "rE0R1YjchIO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ *Prompt Engineering (incl. Chain-of-Thoughts - CoT)*"
      ],
      "metadata": {
        "id": "bkXljK6xhBPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ploomber.io/blog/prompt-engineering-techniques/"
      ],
      "metadata": {
        "id": "gQtUYa9whALK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2401.14423 Prompt Design and Engineering: Introduction and Advanced Methods"
      ],
      "metadata": {
        "id": "caVuz9hgGMIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering: Längere \"Chain of Thoughts\" verbessern die Leistung von Sprachmodellen\n",
        "\n",
        "https://the-decoder.de/prompt-engineering-laengere-chain-of-thoughts-verbessern-die-leistung-von-sprachmodellen/"
      ],
      "metadata": {
        "id": "LPk_jDKukQNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ *Low-Rank Adaptation (Lora and qLora)*"
      ],
      "metadata": {
        "id": "Cqusdv76O-Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just wrote a new article on \"Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch\": https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch\n"
      ],
      "metadata": {
        "id": "wBbEKuA9zESS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "qLora and Lora to move from floating to integers in models (can damage a bit performance but still acceptable), but we can use then CPUs to train some of these models instead of only GPUs.\n",
        "\n",
        "can one use qLora and Lora to move from floating to integers in LLM models in order to use CPUs to train LLMs instead of only GPUs.\n",
        "\n",
        "Yes, qLora and Lora are techniques that can potentially enable the use of CPUs for training large language models (LLMs) by moving from floating-point to integer representations. Here's a breakdown of how this works and the considerations involved:\n",
        "\n",
        "**Traditional LLM Training and GPUs**\n",
        "\n",
        "* **Floating-point precision:** LLMs are typically trained using floating-point numbers (e.g., float32) because they offer a wide dynamic range necessary to represent the subtle gradients and relationships between parameters during the learning process.\n",
        "* **GPU advantage:** GPUs are highly optimized for parallel computations involving floating-point numbers. This makes them significantly faster than CPUs for traditional LLM training.\n",
        "\n",
        "**qLora and Lora for CPU Training**\n",
        "\n",
        "* **Quantization:** qLora (quantized Lora) is a method for reducing the precision of LLM parameters. Instead of storing them as full 32-bit floating-point numbers, it's possible to represent them using lower-precision integers (e.g., 8-bit integers). This drastically reduces memory footprint and computational cost.\n",
        "* **Lora:** Lora (Low-Rank Adaptation) is a technique that decomposes weight matrices in LLMs into smaller, more manageable components. This decomposition can then be exploited during quantization for efficiency.\n",
        "* **CPU efficiency:**  By reducing the memory and computational requirements through quantization, it becomes feasible to train, or at least perform inference of, LLMs on CPUs, which are more generally available and cost-effective than GPUs in many scenarios.\n",
        "\n",
        "**Challenges and Considerations**\n",
        "\n",
        "* **Accuracy trade-off:** Quantization generally leads to a slight loss in model accuracy compared to full-precision floating-point representations. The goal is to find the right balance between quantization level and acceptable performance.\n",
        "* **Optimization:** Techniques like qLora and Lora are relatively new, and ongoing research is focused on making them more efficient and minimizing the accuracy loss.\n",
        "* **Deployment:**  If you aim to deploy a quantized LLM on CPUs,  you'll likely need specialized software and hardware support to ensure optimal performance.\n",
        "\n",
        "**In Conclusion**\n",
        "\n",
        "qLora and Lora offer a promising path towards making LLM training and inference more accessible by potentially allowing the use of CPUs. However, it's essential to be aware of the potential accuracy trade-offs and the fact that this field is still under active development.\n",
        "\n",
        "**Let me know if you'd like more details on any specific aspect or have another related question!**\n"
      ],
      "metadata": {
        "id": "1kQP9sfnhvO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lora*\n",
        "\n",
        "\"𝗟𝗼𝗥𝗔, which stands for Low-Rank Adaptation, is a popular technique to finetune #llms more efficiently. Instead of adjusting all the parameters of a deep neural network, LoRA focuses on updating only a small set of low-rank matrices.\"\n",
        "\n",
        "https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?utm_source=tldrai\n",
        "\n",
        "  * LoRA: Low-Rank Adaptation of Large Language Models\n",
        "  * Edward J. Hu*, Yelong Shen*, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen\n",
        "  * Paper: https://arxiv.org/abs/2106.09685\n",
        "  * Video explainer: https://www.youtube.com/watch?v=DhRoTONcyZE\n",
        "  * https://github.com/microsoft/LoRA"
      ],
      "metadata": {
        "id": "n3TCWKhJZvRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ *Dora*"
      ],
      "metadata": {
        "id": "0MXqdUuKJZ37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While everyone is talking about Sora, there's a potential successor to LoRA (low-rank adaptation) called DoRA. Here's a closer look at the \"DoRA: Weight-Decomposed Low-Rank Adaptation\" paper: https://arxiv.org/abs/2402.09353\n",
        "LoRA is probably the most widely used parameter-efficient finetuning method for LLMs and vision transformers, and DoRA can be seen as an improvement or extension of LoRA that is built on top of it.\n",
        "\n",
        "A brief LoRA recap: Assuming we have pretrained model weights W, LoRA uses low-rank matrices to approximate weight changes ΔW. I.e., in regular finetuning we have W' = W + ΔW, and in LoRA, we approximate ΔW with BA.\n",
        "\n",
        "Now, the DoRA method first decomposes the pretrained weight matrix into a magnitude vector (m) and a directional matrix (V). Then, it takes the directional matrix V and applies standard LoRA to it, i.e., W' = m (V + ΔV)/norm = m (W + BA)/norm.\n",
        "\n",
        "The motivation for developing this method is based on analyzing and comparing the LoRA and full finetuning learning patterns. They found that LoRA either increases or decreases magnitude and direction updates proportionally but seems to lack the capability of making only subtle directional changes as found in full finetuning. Hence, the researchers propose the decoupling of magnitude and directional components. In other words, their DoRA method aims to apply LoRA only to the directional component (while also allowing the magnitude component to be trained separably.)\n",
        "\n",
        "Note that introducing the magnitude vector m in DoRA adds 0.01% more parameters than standard LoRA. However, across both LLM and vision transformer benchmarks, they found that DoRA even outperforms LoRA if the DoRA rank is halved, i.e., when DoRA only uses half the parameters of regular LoRA.\n",
        "\n",
        "Overall, I am actually quite impressed by the results and need to toy with this method in practice. It should not be too big of a lift to upgrade a LoRA implementation to DoRA."
      ],
      "metadata": {
        "id": "Z4vmfiEHJdKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ (?) *Proxy-tuning*"
      ],
      "metadata": {
        "id": "PFZKZOdMPBfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Proxy-tuning*\n",
        "\n",
        "Proxy-tuning is a way to adapt LLMs without changing the model's weights. This is especially attractive if a given LLM is too resource-intensive to train or if a user doesn't have access to the LLM's weights.\n",
        "Following up on the proxy-tuning paper discussion from last week, I implemented it in code and gave it a try. It actually works!\n",
        "\n",
        "In a nutshell, it works like this:\n",
        "\n",
        "1. Select a base LLM (e.g., an untuned 7B Llama 2 model) smaller and cheaper than the target LLM (e.g., a 10x larger, untuned 70B Llama 2 model).\n",
        "\n",
        "2. Finetune this smaller base LLM to obtain a small finetuned LLM (e.g., instruction-finetune a 7B Llama 2 model to get a finetuned 7B model).\n",
        "\n",
        "3. Compute the output difference between the base model (step 1) and the tuned model (step 2).\n",
        "\n",
        "4. Add this difference in outputs to the target LLM's outputs.\n",
        "\n",
        "5. Normalize the modified outputs from step 4, and then generate the answer.\n",
        "\n",
        "I tried the following query:\n",
        "\n",
        "\"If I have 5 apples and eat 2, but then find 3 more on my way home, how many do I have?\"\n",
        "\n",
        "The proxy-tuned model was indeed able to answer correctly, whereas the base models failed: \"You start with 5 apples and eat 2, so you have 5 - 2 = 3 apples left. Then, you find 3 more apples on your way home, so you have 3 + 3 = 6 apples in total.\"\n",
        "\n",
        "Using the same approach, it was also possible to give Llama 2 13B coding abilities via CodeLlama 7B.\n",
        "\n",
        "More details and code in my write-up and associated code Studio:\n",
        "\n",
        "https://lightning.ai/lightning-ai/studios/improve-llms-with-proxy-tuning?view=public"
      ],
      "metadata": {
        "id": "RJJgo0tk_4aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While proxy-tuning and prompting have connections and some potential overlap, they are not precisely the same approach. Here's a breakdown of the differences and where they might intersect:\n",
        "\n",
        "**Proxy-Tuning**\n",
        "\n",
        "* **Core Focus:** Parameter-efficient fine-tuning of large language models (LLMs). The goal is to adapt a pre-trained LLM to a new task without fully fine-tuning its vast number of parameters.\n",
        "* **Mechanism:** Introduces a small set of learnable \"proxy\" parameters. These might be:\n",
        "    * Additional vectors that learn relationships between input embeddings.\n",
        "    * Small adapter modules placed between layers of the LLM.\n",
        "* **Advantage:** Reduces computational cost and memory footprint of fine-tuning compared to training the entire LLM.\n",
        "\n",
        "**Prompting**\n",
        "\n",
        "* **Core Focus:** Eliciting a desired response or behavior from an LLM by carefully crafting the input text.  Prompts include instructions, examples, or textual structures that guide the model's generation.\n",
        "* **Mechanism:** No internal model parameters are directly modified.  Prompts guide the LLM to use its pre-existing knowledge and abilities to complete the task.\n",
        "* **Advantage:** Highly flexible and can be applied to a wide range of tasks without fine-tuning.\n",
        "\n",
        "**Connections and Overlaps**\n",
        "\n",
        "* **Semantic Tuning:**  Proxy-tuning could be employed with  prompts that have rich semantic representations. This fine-tuning could refine the proxy parameters to better  align with complex queries or instructions provided in the prompts.\n",
        "* **Prompt Design as Lightweight Fine-tuning:** Sometimes,  prompting involves finding prompts that essentially 'trick' the LLM into performing a task it  wasn't explicitly trained for.  This fine-grained task adaptation through crafting prompts can have a similar spirit to proxy-tuning, in the sense that both aim to modify how the LLM behaves without major internal changes.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Proxy-tuning and prompting are distinct but complementary techniques. Proxy-tuning focuses on efficient internal parameter modification of LLMs. Prompting focuses on steering model output through clever input design.  There could be interesting potential in the use of these two approaches together."
      ],
      "metadata": {
        "id": "Ej7C082gtwyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Adapter Tuning*"
      ],
      "metadata": {
        "id": "0IdZ1gStv7HP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models"
      ],
      "metadata": {
        "id": "1etW7dnRGOp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://services.google.com/fh/files/misc/adaptation_of_foundation_models_whitepaper_google_cloud.pdf"
      ],
      "metadata": {
        "id": "6QyPscgUGCu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters"
      ],
      "metadata": {
        "id": "_-8jp5b6FbOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2106.03164 On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation"
      ],
      "metadata": {
        "id": "i5vNXkQPF0d3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of Adapter Tuning, its advantages, and how it works:\n",
        "\n",
        "**What is Adapter Tuning?**\n",
        "\n",
        "Adapter tuning is a technique used to fine-tune large language models (LLMs) for new tasks more efficiently. Here's the core idea:\n",
        "\n",
        "* **Pre-trained Models:** Large language models have been trained on vast amounts of text data, giving them a deep understanding of language patterns.\n",
        "* **Bottleneck:** Fine-tuning these massive models to a specific task (like question answering or translation) often means updating millions or billions of parameters, which is computationally expensive and time-consuming.\n",
        "* **Adapters to the Rescue:** Adapter tuning introduces compact \"adapter\" modules (small sets of new neural network layers) within the pre-trained model's architecture.  Instead of updating the entire model, only the parameters within these adapters are trained for the new task.\n",
        "\n",
        "**Advantages of Adapter Tuning**\n",
        "\n",
        "* **Efficiency:** Adapter tuning greatly reduces the number of parameters needing updates, making fine-tuning significantly faster and less computationally demanding.\n",
        "* **Preserving Knowledge:** By freezing most of the original model's weights, adapter tuning preserves the language knowledge the model has acquired during pre-training.\n",
        "* **Multi-Task Learning:** A single pre-trained model can have multiple adapters, each trained for a different task. This allows the model to switch between tasks easily without needing to retrain the entire model.\n",
        "\n",
        "**How Adapter Tuning Works**\n",
        "\n",
        "1. **Pre-trained Model:** Start with a large, pre-trained language model.\n",
        "2. **Insert Adapters:** Add small adapter modules (new layers) between existing layers of the pre-trained model.\n",
        "3. **Freeze the Original Model:** Keep the weights of the pre-trained model frozen.\n",
        "4. **Train the Adapters:** Train only the parameters of the newly added adapters on the data for the specific task at hand.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Adapter tuning is a clever way to fine-tune large language models. It saves time and computing resources, maintains the model's core knowledge, and facilitates efficient multi-task capabilities.\n",
        "\n",
        "**Let me know if you'd like a more technical explanation or examples of how adapter tuning is used!**\n"
      ],
      "metadata": {
        "id": "dfCkiw8zwImy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *ASPIRE*"
      ],
      "metadata": {
        "id": "3cyZhA5TuEuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.research.google/2024/01/introducing-aspire-for-selective.html"
      ],
      "metadata": {
        "id": "DPpnWeV-uGA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes! \"Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs\" (ASPIRE) is primarily a framework for finetuning LLMs. Here's why:\n",
        "\n",
        "**Focus of ASPIRE:**\n",
        "\n",
        "* **Selective Prediction:**  ASPIRE aims to teach LLMs to evaluate their own output and decide whether to provide an answer or abstain (\"I don't know\"). This aligns with finetuning goals of improving model behavior rather than directly changing its architecture.\n",
        "* **Self-Evaluation:** The framework trains the LLM on target-task data to learn to assess its own predictions, rather than relying on external retrieval mechanisms.\n",
        "\n",
        "**How ASPIRE Finetunes LLMs:**\n",
        "\n",
        "ASPIRE involves finetuning a pre-trained LLM on a portion of question-answering datasets like CoQA. During finetuning, the model learns to output both an answer and a selection score. The selection score indicates the model's confidence in its answer.  This approach directly modifies the LLM's output behavior, aligning with finetuning.\n",
        "\n",
        "**Difference from RAG:**\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) frameworks enhance LLMs by teaching them to retrieve relevant information from external sources during answer generation.  ASPIRE, in contrast, focuses on internal self-assessment.\n",
        "\n",
        "**ASPIRE vs. RLHF:**\n",
        "\n",
        "While both ASPIRE and RLHF (Reinforcement Learning from Human Feedback) aim to improve the quality and reliability of LLM responses, they differ in approach:\n",
        "\n",
        "* **RLHF:** Uses human feedback to directly reward desirable responses and penalize undesirable ones.\n",
        "* **ASPIRE:** Focuses on the LLM learning to evaluate itself based on data.\n",
        "\n",
        "**In conclusion, ASPIRE falls firmly within the domain of finetuning LLMs by teaching them the crucial skill of selective prediction.**\n"
      ],
      "metadata": {
        "id": "0qZ6IWEiuO_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*RHLF*"
      ],
      "metadata": {
        "id": "l_kkJp3mtIyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud?hl=en\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf"
      ],
      "metadata": {
        "id": "iXpqxCF4lcDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RLHF stands for \"Reinforcement Learning from Human Feedback\"**\n",
        "\n",
        "* is a crucial technique for improving the performance and alignment of these models. It involves a combination of methods that leverage human feedback to refine and guide the learning process of the model.\n",
        "\n",
        "* Here's how RLHF is applied to LLMs:\n",
        "\n",
        "1. **Supervised Fine-Tuning (SFT)**: Initially, the language model is trained on a large dataset of text from the internet. This phase is purely supervised learning, where the model learns to predict the next word in a sequence. However, this dataset might not perfectly represent desirable outputs, as it contains a wide range of materials, including low-quality or harmful content.\n",
        "\n",
        "2. **Reward Modeling**: After the initial training, human reviewers assess the outputs of the model, providing feedback on its responses. This feedback could be in the form of rankings, ratings, or direct corrections. This human feedback is then used to create a reward model. The reward model essentially predicts how a human would rate a given output, serving as a proxy for human judgment.\n",
        "\n",
        "3. **Proximal Policy Optimization (PPO)**: With the reward model in place, the LLM is further trained using reinforcement learning, specifically a method called Proximal Policy Optimization. In this phase, the model learns to generate outputs that are more likely to receive higher ratings from the reward model. This step is crucial for aligning the model's outputs with human values and preferences.\n",
        "\n",
        "4. **Iterative Refinement**: The process can be iterative, with further rounds of human feedback and model refinement. This ongoing interaction helps in continuously improving the quality and relevance of the model's responses, ensuring they align well with ethical guidelines, factual accuracy, and user expectations.\n",
        "\n",
        "Applying RLHF to LLMs is important for several reasons:\n",
        "\n",
        "- **Alignment with Human Values**: It helps ensure that the model's outputs are aligned with human ethical standards and societal norms.\n",
        "- **Reduction of Harmful Outputs**: By training the model with human feedback, it's possible to reduce the likelihood of generating unsafe, biased, or otherwise undesirable content.\n",
        "- **Enhanced Relevance and Utility**: Feedback-driven training can make the model more useful and relevant to users by focusing on generating high-quality, contextually appropriate responses.\n",
        "\n",
        "This approach represents a significant advancement in the development of more responsible and effective AI language models."
      ],
      "metadata": {
        "id": "iXz7p2YntKQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Direct Preference Optimization (DPO)*"
      ],
      "metadata": {
        "id": "qoluFPb7O8Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Direct Preference Optimization (DPO)**\n",
        "\n",
        "Interesting article from Andrew Ng about a new research paper that proposes DPO (direct preference optimization) over RLHF. A promise is that you won‘t need to deal with a separately represented reward function -- you just need the LLM transformer -- and you can train the LLM directly and more efficiently to optimize the same objective as RLHF. Enjoy the read 🤓🙌\n",
        "\n",
        "https://www.deeplearning.ai/the-batch/issue-231/\n",
        "\n",
        "https://browse.arxiv.org/html/2305.18290v2\n",
        "\n",
        "https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023\n",
        "\n",
        "and here his thoughts more ind details in case you dont want to read the paper: https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\n",
        "\n",
        "\n",
        "Direct Preference Optimization (DPO) is pretty amazing. Surely, it can't be this straightforward to replace the whole reinforcement learning on human feedback (RLHF) phase?\n",
        "1. Treat a foundational (or instruction tuned) LLM\n",
        "as the reference LLM\n",
        "1.    Generate pairs of outputs to prompts and have humans choose which one they like (human feedback)\n",
        "2.    Add a linear layer to the LLM so that it outputs a scalar value, and tune this new model with a new loss function (compute log-ratio of scalar outputs of the reference LLM and the one being tuned, multiply by a divergence parameter ... -- very cool math insight here, but end-of-the-day this is just a new training loss function called dpo_loss or something that Keras and Pytorch will ship with)\n",
        "3.    Drop the last linear layer, and you have a fine tuned LLM on human feedback\n",
        "Amazing stuff. RLHF was the most dicey part of LLM training and the one that needed the most art vs science. Expect a lot of convergence between LLMs trained on the same data\n",
        "\n",
        "---\n",
        "\n",
        "Direct Preference Optimization (DPO) in the context of machine learning, and specifically for large language models (LLMs) like GPT-3 or GPT-4, is an approach that focuses on optimizing the model's outputs based directly on human preferences. This method is a part of the broader framework of Reinforcement Learning from Human Feedback (RLHF). Here's a closer look at how DPO works and its significance:\n",
        "\n",
        "1. **Collecting Human Preferences**: The first step in DPO involves gathering data on human preferences. This is typically done by presenting human reviewers with pairs of outputs generated by the model in response to the same prompt or query. The reviewers are then asked to indicate which of the two responses they prefer.\n",
        "\n",
        "2. **Training a Preference Model**: The collected preference data is used to train a preference model. This model learns to predict the likelihood of one output being preferred over another by humans. Essentially, it aims to understand and quantify what makes one response more favorable or acceptable than another.\n",
        "\n",
        "3. **Optimizing the Language Model**: Once the preference model is trained, it is used to guide the training of the language model. The language model is optimized to generate outputs that are more likely to align with the preferences indicated by humans. This optimization can be done using various techniques, including reinforcement learning algorithms.\n",
        "\n",
        "4. **Iterative Process**: Like other RLHF techniques, DPO is often iterative. The model's outputs can be continuously evaluated and compared by human reviewers, and the preference model can be updated based on new data. This leads to ongoing improvements in the language model's alignment with human preferences.\n",
        "\n",
        "The significance of DPO lies in its focus on aligning the model's outputs with what humans explicitly prefer. This is particularly important for applications where the quality of an output is subjective and can vary based on context and the specific needs or values of the user.\n",
        "\n",
        "For example, in a conversational AI, different users might have different preferences for the style, tone, and type of responses they find engaging or helpful. DPO allows the model to be tuned to these preferences, potentially leading to more personalized and satisfactory interactions.\n",
        "\n",
        "Overall, Direct Preference Optimization represents a powerful method for increasing the alignment of AI systems with human values, preferences, and expectations, thereby enhancing their usefulness, safety, and acceptance in various applications."
      ],
      "metadata": {
        "id": "5VEtMTUkQz3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Distillation*"
      ],
      "metadata": {
        "id": "JC96LcJUvxwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/distill-text-models"
      ],
      "metadata": {
        "id": "d5qpyD1AHvFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models"
      ],
      "metadata": {
        "id": "v6w_qgUoGs4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#tune-distill"
      ],
      "metadata": {
        "id": "DstE8mSrGp7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning and distillation - Distillation is a good option if you have a large model that you want to make smaller without degrading its ability to do what you want. The process of distilling a model creates a new, smaller trained model that costs less to use and has lower latency than the original model"
      ],
      "metadata": {
        "id": "M66NW8rjGlAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/1503.02531.pdf Distilling the Knowledge in a Neural Network"
      ],
      "metadata": {
        "id": "zvawWRmev2O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.research.google/2024/02/intervening-on-early-readouts-for.html?m=1"
      ],
      "metadata": {
        "id": "3jWTvDr-v0EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Self-instruct*"
      ],
      "metadata": {
        "id": "w4FmSc4QvQsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context - Consider a LLM use-case where we only have access to a handful of HQ few-shot examples that are not sufficient for a full fine-tuning (or even LoRA). Then, is it possible to use an LLM to generate similar examples that can then be used for fine-tuning the model?\n",
        "\n",
        "Concerns\n",
        "1) Model bias - using a model to generate examples and then fine-tuning it based on those sounds like a classic case of introducing bias.\n",
        "\n",
        "2) Role of fine-tuning - If the model is capable of generating the additional examples, then is fine-tuning even necessary? it seems like the model is already able to generalize to the use-case quite well.\n",
        "\n",
        "https://arxiv.org/pdf/2212.10560.pdf\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Yes, this approach is quite effective for training.\n",
        "\n",
        "(1).  Yes, you need to ensure you aren't entrenching existing idiosyncrasies.  Typically this is done in one of three ways: (a) Human review+annotation of machine-generated examples or (b) spending more compute time at inference to generate+filter/select only the best examples for the next round of training, and discarding the low-quality generations.\n",
        "\n",
        "(2).  Generally, given enough time and temperature, all models are capable of generating anything (monkies on a typewriter).  The goal of additional training is always and exclusively to improve the quality of the results in a pass@1 fashion.  If you sample from pass@100, use additional compute to rank the results, and select the top 1% of generations for additional training, then you are increasingly biasing the model toward the highest quality results (which is the purpose of training).  It is true the model is already \"able to generalize to the use-case quite well if given enough compute\", but training is all about paying at training time in order to increase the ROI at inference time.\n",
        "\n",
        "---\n",
        "This sounds like go/datasynth. You can start with a few high quality examples and use LLMs to generate a synthetic dataset for fine-tuning. They also provide tools to mitigate bias you mentioned in #1 (e.g. diversity scores, topic visualizations, etc).  \n"
      ],
      "metadata": {
        "id": "Knw1fQeYvUUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-instruct is a fascinating technique in the realm of large language models (LLMs) that straddles the line between fine-tuning and augmentation. Here's what it entails:\n",
        "\n",
        "**Core Idea**\n",
        "\n",
        "Self-instruct aims to improve an LLM's ability to follow instructions and perform tasks by having it generate its own training data. Essentially, the LLM acts as both teacher and student.\n",
        "\n",
        "**The Process**\n",
        "\n",
        "1. **Task Generation:** The LLM is given a few initial examples of tasks and desired solutions. It then attempts to generate new, similar tasks itself.\n",
        "2. **Solution Generation:** The LLM also tries to provide solutions or completions to the tasks it has created.\n",
        "3. **Filtering:** Human experts or automated quality checks filter out poorly generated tasks and solutions.\n",
        "4. **Instruction Fine-tuning:** The remaining high-quality, self-generated data is used to fine-tune the LLM using instruction-following techniques.\n",
        "\n",
        "**Why It's Both Fine-tuning and Augmentation**\n",
        "\n",
        "* **Fine-tuning:**  Self-instruct directly modifies the LLM's parameters based on the data it generates, just like other fine-tuning processes. This refines its ability to execute instructions.\n",
        "* **Augmentation:** The main difference is that self-instruct augments the existing training process by creating a novel and customized dataset that caters specifically to the desired task format.\n",
        "\n",
        "**Benefits**\n",
        "\n",
        "* **Addresses Dataset Limitations:** Human-written instruction datasets are often limited in scale and diversity. Self-instruct expands these training resources.\n",
        "* **Reduces Expert Effort:** Creating large datasets manually is time-consuming. Self-instruct lessens the burden on human annotators.\n",
        "* **Iterative Improvement:** As the LLM becomes better at following instructions, its self-generated training data also improves, creating a positive feedback loop.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Imagine you want the LLM to summarize articles.\n",
        "\n",
        "* **Initial Input:**  A few articles with provided human-written summaries.\n",
        "* **Self-Instruct:** The LLM generates new articles and creates summaries for them. These are filtered for quality.\n",
        "* **Fine-tuning:**  The LLM is fine-tuned on the self-generated dataset to improve its summarization skills.\n",
        "\n",
        "**Let me know if you'd like to delve into the technical details of self-instruct methods or want to explore practical applications!**\n"
      ],
      "metadata": {
        "id": "uXbRtyYZvWES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Token*"
      ],
      "metadata": {
        "id": "ON5x3x5CPe7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Token*\n",
        "\n",
        "* basic unit of text representation.\n",
        "* sequences of characters that are treated as individual elements in the LLM's internal representation of the input text.\n",
        "* used to build up more complex language structures, such as words, phrases, and sentences.\n",
        "* Tokenization is the process of breaking down a piece of text into tokens. This can be done using various methods, such as word-level tokenization, character-level tokenization, or subword tokenization. The choice of tokenization method depends on the specific task and the characteristics of the input text.\n",
        "* LLMs use tokens to extract features from the input text and to generate output text. For example, when generating text, the LLM will consider the surrounding tokens to determine the most likely next word or phrase. Tokens also play a role in LLM performance evaluation. Evaluation metrics such as accuracy and F1 score are often calculated based on the similarity between predicted and ground truth tokens.\n",
        "  * Text-to-text translation: When translating a sentence from one language to another, the LLM breaks down the input sentence into tokens and then uses these tokens to generate the corresponding tokens in the target language.\n",
        "  * Text summarization: When summarizing a document, the LLM identifies the most important tokens in the document and then uses these tokens to generate a concise summary.\n",
        "  * Question answering: When answering a question about a text, the LLM extracts the relevant tokens from the text and uses these tokens to identify the answer to the question."
      ],
      "metadata": {
        "id": "C2x7aHgiWuOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I get better results with no training. See how, at https://mltblog.com/42l51rJ\n",
        "Eventually, I will add training: user based (self-tuning) rather than based on algorithms. Of course, fully automated and transparent to the user.\n"
      ],
      "metadata": {
        "id": "A0wqPksljzJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *SLM (Small Language Model)*"
      ],
      "metadata": {
        "id": "jK7vTB6GIQpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://the-decoder.de/microsoft-prognostiziert-drei-grosse-ki-trends-fuer-2024/\n",
        "\n"
      ],
      "metadata": {
        "id": "5YNYNHmIH-7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Microsoft investiert in diesen Bereich, hat bereits die SLMs Phi und Orca vorgestellt und soll seine eigenen Abteilungen auf effizientere KI-Modelle ausgerichtet haben."
      ],
      "metadata": {
        "id": "6iMpdYbDID5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://the-decoder.de/microsofts-mini-llm-phi-2-ist-jetzt-open-source-und-angeblich-besser-als-google-gemini-nano/"
      ],
      "metadata": {
        "id": "KBKBgFLeIJZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://the-decoder.de/microsofts-orca-2-ist-ein-kleines-ki-modell-mit-verbesserter-denkfaehigkeit/"
      ],
      "metadata": {
        "id": "C_vrvQL9IMrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://the-decoder.de/generative-ki-ist-teuer-und-microsoft-will-die-kosten-senken/"
      ],
      "metadata": {
        "id": "EPit7Ws7IPr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[How to Build an LLM from Scratch | An Overview](https://www.youtube.com/watch?v=ZLbVdvOoTKM&list=WL&index=8&t=480s)"
      ],
      "metadata": {
        "id": "UdSIcDIUu6uH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Augment LLMs*"
      ],
      "metadata": {
        "id": "QaGfSVYXp_gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Grounding*"
      ],
      "metadata": {
        "id": "Y7EMAgb2PJDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grounding**: Out-of-the-box grounding against structured and unstructured data\n"
      ],
      "metadata": {
        "id": "wK-YLsg4sTJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Grounding*\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/overview\n",
        "\n",
        "Grounding in large language models (LLMs) refers to the process of connecting their responses to real-world entities and concepts. This is crucial for ensuring that LLMs generate accurate, relevant, and meaningful outputs. LLMs are trained on massive datasets of text and code, but this knowledge is often abstract and lacks specific context. Grounding allows LLMs to access and incorporate external information to make their responses more grounded in reality.\n",
        "\n",
        "There are several different approaches to grounding LLMs, but they typically involve some combination of the following techniques:\n",
        "\n",
        "* **Knowledge base access:** LLMs can be connected to knowledge bases, such as Wikidata or Google Search, to access information about real-world entities and concepts. This information can be used to refine and improve their responses.\n",
        "* **Data retrieval:** LLMs can be used to retrieve relevant data from external sources, such as databases or web pages. This data can then be used to ground their responses in real-world context.\n",
        "* **Physical interaction:** In some cases, LLMs can be directly connected to the real world through sensors and actuators. This allows them to interact with their environment and gather information directly.\n",
        "\n",
        "Grounding is an essential part of developing reliable and useful LLMs. By grounding their responses in real-world knowledge, LLMs can be more accurate, relevant, and informative. This makes them more suitable for a wider range of applications, such as question answering, summarization, and natural language generation.\n",
        "\n",
        "Here are some of the benefits of grounding LLMs:\n",
        "\n",
        "* **Improved accuracy:** Grounding can help to reduce the number of factual errors in LLM responses.\n",
        "* **Increased relevance:** Grounding can help to ensure that LLM responses are relevant to the context in which they are generated.\n",
        "* **Enhanced creativity:** Grounding can help LLMs to generate more creative and interesting text formats.\n",
        "* **Expanded capabilities:** Grounding can allow LLMs to perform tasks that require more knowledge of the real world, such as answering questions about physical entities or events.\n",
        "\n",
        "Overall, grounding is a powerful technique for improving the performance and usefulness of large language models. As LLMs continue to develop, grounding is likely to play an even more important role in their development and application."
      ],
      "metadata": {
        "id": "HcdUr67tZ6JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Retrieval-Augmented Generation (RAG)*"
      ],
      "metadata": {
        "id": "6Z0vHScrO3Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-1?hl=en"
      ],
      "metadata": {
        "id": "Mnmh9oFDYQjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Long tokens vs RAG**\n",
        "\n",
        "https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n",
        "\n",
        "https://youtu.be/wa0MT8OwHuk?si=4DeetM9gi9yI1i-T\n",
        "\n",
        "the demo is using a movie. That explains why the 1.5 version landed on the Multimodal part of Studio. Will that context window not apply to the text-only endpoint?\n",
        "\n",
        "Good thing I didn't invest too much time learning about chunking strategies with RAG 😅\n",
        "\n",
        "Apparently Deepmind tested 10M token successfully in research\n",
        "\n",
        "I think chunking will still make sense for RAG use cases, especially if you care about latency or have significantly more than 1M tokens. But it's great to see that there are lots of improvements on the input token limit."
      ],
      "metadata": {
        "id": "wyDsltggM6_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1710.png)"
      ],
      "metadata": {
        "id": "x6b56-TiMMHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.linkedin.com/posts/alexwang2911_genai-llm-technology-activity-7164438306871603200--mgt/?utm_source=share&utm_medium=member_ios"
      ],
      "metadata": {
        "id": "Onb9kYmoIzDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-1?hl=en"
      ],
      "metadata": {
        "id": "mdFXP_TgVQA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/rags-powered-by-google-search-technology-part-2?hl=en"
      ],
      "metadata": {
        "id": "UkJKK25Xlvbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/ai-machine-learning/context-aware-code-generation-rag-and-vertex-ai-codey-apis?hl=en"
      ],
      "metadata": {
        "id": "HYCk0zmUl4Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have a question about chatbot and RAG. Is there any good/easy approach which we can add RAG layer between data source and conversation AI app? The challenge is today we show great demo in Playbooks with cleaned data but in the reality, customers usually have more complicated/messy data source (source data against each others), so the first step of doing data retrieval is critical.  Say customer is ok to use our DFCX/Playbooks but they need to add extra RAG layer. Any experience ?\n"
      ],
      "metadata": {
        "id": "iorG22NB44ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval Augmented Generation (RAG)**: Fully managed vector database for high-scale, low latency vector search\n",
        "\n",
        "[What is Retrieval-Augmented Generation (RAG)?](https://www.youtube.com/watch?v=T-D1OfcDW1M&list=WL&index=9&t=23s)\n"
      ],
      "metadata": {
        "id": "YTf3gJSGsUsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG stands for **Retrieval-Augmented Generation** in the context of Large Language Models (LLMs). Here's a breakdown of what it is and why it's important:\n",
        "\n",
        "**How LLMs Typically Work**\n",
        "\n",
        "* Traditional LLMs are trained on massive amounts of text data. They learn to predict the next word in a sequence based on patterns they've seen during training.\n",
        "* This makes them great at generating text, translating languages, etc.\n",
        "* **Limitation:** LLMs can't easily access and incorporate new knowledge or information that wasn't part of their original training data.\n",
        "\n",
        "**How RAG Solves This**\n",
        "\n",
        "RAG combines the power of LLMs with traditional information retrieval systems:\n",
        "\n",
        "1. **Retrieval:** When you ask a question, RAG first searches through a knowledge base (like Wikipedia, internal company documents, news articles, etc.) to find relevant passages of text.\n",
        "2. **Augmentation:** This relevant text is appended to your original question, creating a richer, more informative prompt.\n",
        "3. **Generation:** The LLM uses this combined information (original question + retrieved knowledge) to generate a more comprehensive and accurate answer.\n",
        "\n",
        "**Benefits of RAG**\n",
        "\n",
        "* **Up-to-date Information:** RAG models aren't stuck with the knowledge they were trained on. They can access and integrate fresh information on demand.\n",
        "* **Factual Grounding:** Answers generated by RAG models can be linked back to the knowledge sources they are based on, providing sources or citations.\n",
        "* **Domain-Specific Adaptation:** A RAG model can be customized to work well in specific fields (legal, medical, etc.) by using a knowledge base tailored to that domain.\n",
        "* **Improved Accuracy:** RAG models often significantly outperform traditional LLMs on tasks that require external knowledge\n",
        "\n",
        "**In summary:**  RAG is a technique that allows LLMs to go beyond their internal training data, access external knowledge, and provide more informed, accurate, and trustworthy responses.\n"
      ],
      "metadata": {
        "id": "m0FaHYgS1ygi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Prompting and RAG*\n",
        "\n",
        "Prompting and RAG are equivalent to feature engineering (data augmentation to be exact) in the traditional ML paradigm, while finetuning is equivalent to model training. So, all three are needed for the best result.\n",
        "* rag is more like a key point selection based on features. That's more like an attention mechanism rather than feature engineering.\n",
        "* When you do RAG, you use the user's original input to find relevant pieces of external information and add them to the input you send to the model. This is by definition feature engineering.\n"
      ],
      "metadata": {
        "id": "tq-iqkJhyA1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a framework for improving the accuracy and reliability of large language models (LLMs) by incorporating external knowledge bases during the generation process. It combines the strengths of two approaches: dense retrieval and sequence-to-sequence modeling.\n",
        "\n",
        "**Dense retrieval** is used to identify relevant documents from a knowledge base that are related to the query or prompt provided to the LLM. This ensures that the LLM has access to the most up-to-date and accurate information before generating its response.\n",
        "\n",
        "**Sequence-to-sequence modeling** is used to generate the final output text, taking into account the retrieved documents and the original query. This allows the LLM to incorporate factual information from the knowledge base into its response while maintaining its natural language fluency and coherence.\n",
        "\n",
        "RAG has been shown to improve the performance of LLMs on a variety of tasks, including question answering, summarization, and translation. It has also been found to be more robust to errors in the knowledge base than traditional LLM approaches.\n",
        "\n",
        "**Benefits of RAG:**\n",
        "\n",
        "* **Improved accuracy:** RAG can help to prevent LLMs from generating incorrect or misleading information by providing them with access to a broader range of information.\n",
        "\n",
        "* **Reduced factual errors:** RAG can reduce the number of factual errors in LLM responses by incorporating information from a knowledge base.\n",
        "\n",
        "* **Improved coherence:** RAG can help to improve the coherence of LLM responses by ensuring that they are consistent with the information in the knowledge base.\n",
        "\n",
        "**Drawbacks of RAG:**\n",
        "\n",
        "* **Increased computational complexity:** RAG is more computationally complex than traditional LLM approaches, as it requires both dense retrieval and sequence-to-sequence modeling.\n",
        "\n",
        "* **Dependency on knowledge base:** RAG is dependent on the quality of the knowledge base that is used. If the knowledge base is inaccurate or incomplete, RAG's performance will suffer.\n",
        "\n",
        "Despite these drawbacks, RAG is a promising approach for improving the accuracy and reliability of LLMs. As knowledge bases continue to improve, RAG is likely to become an even more valuable tool for natural language processing."
      ],
      "metadata": {
        "id": "2L0kvygNz4MO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ *LangChain*"
      ],
      "metadata": {
        "id": "ut_14zjENSyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/blog/products/databases/build-rag-applications-with-langchain-and-google-cloud"
      ],
      "metadata": {
        "id": "JTmQ6pFyNQqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### $\\hookrightarrow$ *LLamaindex*"
      ],
      "metadata": {
        "id": "QFiC9iq1uchk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLamaindex**\n",
        "\n",
        "Video: [Llamaindex webinar](https://youtu.be/aoLtTIYAafY?si=r2iEJC_bqTsPRlnf)\n",
        "\n",
        "LlamaIndex is a powerful data framework designed to streamline the development of applications powered by Large Language Models (LLMs). Here's a breakdown of what it is and why it's important:\n",
        "\n",
        "**Key Functions**\n",
        "\n",
        "* **Data Ingestion & Management:**\n",
        "    * **Diverse Formats:** Handles structured, semi-structured, and unstructured data sources (documents, PDFs, APIs, SQL databases, etc.).\n",
        "    * **Organization:** Structures data in ways LLMs can easily understand and access.\n",
        "* **Indexing:** Creates efficient indices (like a virtual library) of your data, allowing quick retrieval of relevant information.\n",
        "* **Query Interface:** Provides a user-friendly way to interact with your data using prompts or questions. The responses are augmented with knowledge retrieved from your indexed data.\n",
        "\n",
        "**Why It Matters (Benefits)**\n",
        "\n",
        "* **Retrieval-Augmented Generation (RAG):** LlamaIndex is core to implementing RAG, a method where LLMs first consult an external knowledge base before generating a text response. This leads to more informed, grounded, and less likely to hallucinate answers.\n",
        "* **Custom Applications:** It simplifies building a range of LLM-driven applications:\n",
        "    * Question-answering systems over specific information (company docs, product manuals, etc.).\n",
        "    * Sophisticated chatbots or conversational AI.\n",
        "    * Intelligent tools that interact with your personal data.\n",
        "* **Flexibility:** Offers both high-level APIs for easy use and low-level APIs for finer control.\n",
        "\n",
        "**How it Works (Simplified)**\n",
        "\n",
        "1. **Import & Structure:** You import data into LlamaIndex, where it's organized.\n",
        "2. **Indexing:**  LlamaIndex creates efficient indices to speed up search within your data.\n",
        "3. **Query:** You give the system a question or prompt.\n",
        "4. **Retrieval:** LlamaIndex uses vector search techniques to find relevant bits of information in your data.\n",
        "5. **Augmented Generation:** The LLM takes your question AND the retrieved knowledge to generate a comprehensive response.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "LlamaIndex offers a powerful framework for harnessing the capabilities of LLMs with your own diverse data. It simplifies the process of building LLM-based applications that rely on external knowledge sources."
      ],
      "metadata": {
        "id": "f-MDnUAvuPmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Extensions*"
      ],
      "metadata": {
        "id": "KcuadzY1rHe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extensions**: Integrate easily real-time data and real-world actions, e.g. Ingest internal codebases and automatically lookup evolving security threats in real time\n"
      ],
      "metadata": {
        "id": "YPwk16g1rKKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### <font color=\"orange\">*Function Calling*"
      ],
      "metadata": {
        "id": "hrYb8_OIPhhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Calling**: enables LLMs to interact with external tools, databases, and APIs or code snippets that perform specific tasks (e.g., weather lookup, math calculations, sending emails) to improve model responses"
      ],
      "metadata": {
        "id": "6h-cCeTcsX-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Function Calling*\n",
        "\n",
        "https://codelabs.developers.google.com/codelabs/gemini-function-calling#0\n"
      ],
      "metadata": {
        "id": "KhkJHDb7j29X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of the differences between function calling and extensions in the context of LLMs:\n",
        "\n",
        "**Function Calling**\n",
        "\n",
        "* **Focus:** Executing specific, well-defined tasks or retrieving information from external sources.\n",
        "* **Nature:** Dynamic and on-demand. Functions are called in response to specific prompts and instructions.\n",
        "* **Scope:** Actions are usually limited to external tools and APIs the LLM is connected to.\n",
        "* **Example:** A function to look up the current temperature in a specific city.\n",
        "\n",
        "**Extensions**\n",
        "\n",
        "* **Focus:** Expanding the core capabilities of the LLM with new features or knowledge domains.\n",
        "* **Nature:** More persistent. Extensions are installed or integrated into the model.\n",
        "* **Scope:** Can fundamentally change how the LLM processes information and generates responses.\n",
        "* **Example:** An extension designed for summarizing scientific papers, allowing the LLM to better understand and answer questions about complex research.\n",
        "\n",
        "**Key Differences Table**\n",
        "\n",
        "| Feature            | Function Calling                                    | Extensions                                                   |\n",
        "|--------------------|-----------------------------------------------------|--------------------------------------------------------------|\n",
        "| Purpose            | Perform external actions, retrieve real-world data  | Expand the LLM's capabilities and knowledge                   |\n",
        "| Nature             | Dynamic, task-specific                              | Persistent, integrated into the LLM                           |\n",
        "| Scope              | Focused on external tools the LLM can access       | Can affect the LLM's internal processing and understanding    |\n",
        "| Implementation     | LLM generates code to call external functions      | Often require installing code or packages within the LLM's environment |\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "* **Function calling** is about letting the LLM interact with the outside world to get things done.\n",
        "* **Extensions** are about making the LLM itself smarter or more capable by adding new skills or knowledge.\n",
        "\n",
        "**Let me know if you'd like more clarification or examples!**\n"
      ],
      "metadata": {
        "id": "cJbYYtJO11zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Vector Retrieval / Vector Search*"
      ],
      "metadata": {
        "id": "r2-fNN88Cb7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Retrieval / Vector Search both refer to the core process of finding similar items within a large dataset of vectors generated by an LLM. Here's how it works:\n",
        "\n",
        "1. Embedding Generation: The LLM takes a query (e.g., a piece of text or an image) and creates a vector representation of it.\n",
        "2. Vector Database: The LLM has a pre-computed database of vectors representing items like text passages, images, or code snippets.\n",
        "3. Similarity Calculation: The LLM calculates the similarity (often using cosine similarity) between the query vector and all the vectors in the database.\n",
        "4. Retrieval: The vectors with the highest similarity scores (closest distance to the query vector) are retrieved. These returned items are considered the most relevant matches to the original query.\n",
        "\n",
        "Applications - Vector retrieval (a.k.a. vector search) is a powerful tool used across various LLM-based applications:\n",
        "\n",
        "* Semantic Search: Searching for information based on meaning rather than just keywords.\n",
        "* Recommendation Systems: Recommending products, content, etc. based on a user's preferences and similarity between items.\n",
        "* Question Answering: Finding relevant passages in a large corpus of text to answer a question.\n",
        "* Image Search: Searching for images similar to a query image."
      ],
      "metadata": {
        "id": "wbIfqyYxr7at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of ANN and MMR in the context of information retrieval using LLMs:\n",
        "\n",
        "**ANN (Approximate Nearest Neighbor)**\n",
        "\n",
        "* **Purpose:**  ANN algorithms are designed to find items (\"neighbors\") in a dataset that are most similar to a given query, even in very high-dimensional spaces. In LLM retrieval, the query would be the user's input, and the items in the dataset are potential text passages containing relevant information.\n",
        "\n",
        "* **Role in LLM Retrieval:**\n",
        "    * **Efficiency:**  ANN is crucial for LLM-based retrieval because naively comparing a query to every piece of text in a large database would be extremely computationally expensive. ANN makes this process tremendously faster.\n",
        "    * **Understanding Similarity:** ANN focuses on finding passages that are semantically similar to the user's query, even if they don't contain exact keyword matches. This aligns well with LLMs' ability to understand nuanced meaning.\n",
        "\n",
        "* **Types of ANN:** Several ANN algorithms exist (e.g., HNSW, FAISS), each with trade-offs in accuracy, speed, and memory usage.\n",
        "\n",
        "**MMR (Maximal Marginal Relevance)**\n",
        "\n",
        "* **Purpose:** MMR is a ranking algorithm used to ensure diversity and avoid redundancy in search results. In LLM retrieval, it works after ANN has identified potential relevant passages.\n",
        "\n",
        "* **Role in LLM Retrieval:**\n",
        "    * **Diversification of Results:** MMR aims to prevent the top results from all being too similar to each other. Instead, it promotes including passages that, while relevant, also cover different aspects or perspectives related to the query.\n",
        "    * **Improving User Experience:** This diversification helps the LLM provide a more comprehensive and informative set of responses to the user, reducing the likelihood of receiving repetitive information.\n",
        "\n",
        "**How ANN and MMR Work Together**\n",
        "\n",
        "1. **User Query:** The user enters a query or question.\n",
        "2. **ANN Retrieval:** Using the ANN algorithm, a set of potentially relevant passages is efficiently retrieved from a large text corpus.\n",
        "3. **MMR Ranking:** MMR analyzes the retrieved passages and reranks them, prioritizing both relevance to the query and diversity within the results.\n",
        "4. **Return to LLM:** The top-ranked passages may be fed back into the LLM for further processing, allowing the LLM to generate a more informed and helpful response.\n",
        "\n",
        "**Let me know if you'd like more details on the technical aspects of specific ANN algorithms or how MMR calculates diversity!**\n"
      ],
      "metadata": {
        "id": "weryLPVwuvxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your favorite cloud data warehouse now supports vector search and vector indexes, see https://lnkd.in/g3ztYWWy. How cool is that? BigQuery vector search lets you find semantically similar items in a table using by comparing the distance of  vector embeddings. Find similar customers, articles, products, locations. This is super useful for integrating LLMs with your data and validating LLM output. The best thing is that you don't need another product to do it."
      ],
      "metadata": {
        "id": "lVvlo-1MCdjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " are announcing the Public Preview of BigQuery Vector Search. This functionality, also commonly referred to as approximate nearest-neighbor search, is key to empowering numerous new data and AI use cases such as semantic search, similarity detection, and retrieval-augmented generation (RAG) with a large language model (LLM).\n",
        "\n",
        "Vector search is often performed on high-dimensional numeric vectors, a.k.a. embeddings, which incorporate a semantic representation for an entity and can be generated from numerous sources, including text, image, or video. BigQuery vector search relies on an index to optimize the lookups and distance computations required to identify closely matching embeddings.\n",
        "\n",
        "Our framework supports multiple Vector Index types, with the first implemented type (IVF) combining (a) an optimized [BQML](https://cloud.google.com/bigquery/docs/bqml-introduction) KMeans clustering model with (b) an inverted row locator, which is automatically updated as the underlying table data mutates (similar to [BigSearch](https://cloud.google.com/bigquery/docs/search-index)). It works with BigQuery’s embedding-generation capabilities, notably via LLM-based or pre-trained models. Yet the generic interface allows users to use embeddings generated via other means as well.\n",
        "\n",
        "New optimizations, features, index types, and the GA launch are expected in Q2/Q3 2024.\n",
        "\n",
        "Public Launch Material\n",
        "* Launch blog: https://cloud.google.com/blog/products/data-analytics/introducing-new-vector-search-capabilities-in-bigquery?hl=en\n",
        "* Public documentation: https://cloud.google.com/bigquery/docs/vector-search-intro\n",
        "* Tutorial 1 for using with existing embeddings: https://cloud.google.com/bigquery/docs/vector-search\n",
        "* Tutorial 2 for combining with embedding generation and RAG: https://cloud.google.com/bigquery/docs/vector-index-text-search-tutorial\n",
        "* LangChain integration: https://python.langchain.com/docs/integrations/vectorstores/bigquery_vector_search\n",
        "\n",
        "What does the introduced syntax and usage look like?\n",
        "1. Create Vector Index\n",
        "CREATE OR REPLACE VECTOR INDEX <my_index> ON <my_table>(embedding_col)\n",
        "OPTIONS (distance_type=\"COSINE\", index_type=\"IVF\");\n",
        "2. Vector SearchSELECT <columns> FROM VECTOR_SEARCH (TABLE <my_table>, 'embedding_col', (SELECT embedding_col FROM <test_table> WHERE <>));\n",
        "\n",
        "3. (Optional) Monitor Indexing Progress AnytimeSELECT table_name, index_name, coverage_percentage, unindexed_row_count\n",
        "FROM `<myproject.mydataset>.INFORMATION_SCHEMA.VECTOR_INDEXES` WHERE index_status = 'ACTIVE';\n",
        "4. Example RAG query, combining embedding generation, vector search, and LLM text generation:SELECT ml_generate_text_llm_result AS generated, prompt\n",
        "FROM ML.GENERATE_TEXT(\n",
        "   MODEL `<LLM_text_generation_model>`,\n",
        "   (SELECT CONCAT('Propose some project ideas to improve user password security using the context below: ', STRING_AGG(FORMAT(\"patent title: %s, patent abstract: %s\", base.title, base.abstract), ',\\n')) AS prompt, FROM VECTOR_SEARCH(\n",
        "         TABLE `<patents_my_embeddings_table>`, 'text_embedding',\n",
        "         (SELECT text_embedding, content AS query\n",
        "           FROM ML.GENERATE_TEXT_EMBEDDING(\n",
        "               MODEL `<LLM_embedding_model>`,\n",
        "               (SELECT 'improving password security' AS content))), top_k => 5)\n",
        "   ), STRUCT(0.4 AS temperature, 300 AS max_output_tokens, 0.5 AS top_p, 5 AS top_k, TRUE AS flatten_json_output));\n"
      ],
      "metadata": {
        "id": "LHnqM83Capic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vector Retrieval*\n",
        "\n",
        "https://arxiv.org/abs/2401.09350: Foundations of Vector Retrieval"
      ],
      "metadata": {
        "id": "nRhWlzbKrZ5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Special: Knowledge Graph*"
      ],
      "metadata": {
        "id": "NxI60357-N2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://neo4j.com/labs/genai-ecosystem/"
      ],
      "metadata": {
        "id": "-QrtL9Cs-QXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Evaluate LLMs*"
      ],
      "metadata": {
        "id": "J4ZBVKqKqAbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Evaluation*\n",
        "\n",
        "**Automatic Side-by-Side (AutoSxS)**\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/side-by-side-eval\n",
        "\n",
        "**Metrics-based evaluation**\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models\n",
        "\n",
        "**Vertex AI Model Evaluation**\n",
        "\n",
        "Vertex AI Model Evaluation provides a comprehensive framework for evaluating LLMs across a wide range of tasks, including natural language understanding (NLU), natural language generation (NLG), and question answering. It allows you to define custom evaluation metrics and compare performance over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "d2RwjKyMVbg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*MLOps*"
      ],
      "metadata": {
        "id": "vOqtFAy5GKtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's for example a reference architecture for an infrastructure to run a retrieval-augmented generation (RAG) application using Vertex AI. Obviously not the only way to do it, but a good way to do it.\n",
        "\n",
        "This architecture includes three components:\n",
        "- Data ingestion subsystem\n",
        "- Serving subsystem\n",
        "- Quality evaluation subsystem\n",
        "\n",
        "https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai?hl=de"
      ],
      "metadata": {
        "id": "m46jcaVOtJMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/general/deployment?hl=de\n",
        "\n",
        "https://blog.ml6.eu/vertex-pipelines-vertex-ai-vs-ai-platform-6c2f799d6e2e?gi=5a1dbe25d735"
      ],
      "metadata": {
        "id": "mdsVo5zvjjd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excited to share that a new tutorial on Ray on Vertex AI is now available! 🔥\n",
        "\n",
        "This tutorial will take you through the process of distributing PyTorch model training using Ray on Vertex AI and deploying it to Vertex AI Endpoint.\n",
        "\n",
        "To learn more, you can find the notebook link on GitHub 👇 . And if you have any ideas for more Ray on Vertex AI content you'd like to see, let me know.\n",
        "\n",
        "Get started with PyTorch on Ray on Vertex AI\n",
        "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/ray_on_vertex_ai/get_started_with_pytorch_rov.ipynb"
      ],
      "metadata": {
        "id": "J5QrNO-MIfPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058"
      ],
      "metadata": {
        "id": "Fq4EBdvhFC13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning"
      ],
      "metadata": {
        "id": "8MmOgfMSHSYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/start/introduction-mlops"
      ],
      "metadata": {
        "id": "It97BIplHWWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Programming*"
      ],
      "metadata": {
        "id": "OnneO2AhgoGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distributed Training*"
      ],
      "metadata": {
        "id": "uM9V1BBQGHTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=NbID_AqdZXM"
      ],
      "metadata": {
        "id": "i4DIXwcbHFVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=S1tN9a4Proc"
      ],
      "metadata": {
        "id": "ZJ7hEebdHDXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/distributed_training"
      ],
      "metadata": {
        "id": "tiCklVoxGoFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/keras/distributed_training"
      ],
      "metadata": {
        "id": "BmRIRKb9G3yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/distribute/keras"
      ],
      "metadata": {
        "id": "VA8BRTjDHArx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Quantum Libraries*"
      ],
      "metadata": {
        "id": "crQgUFkRk-XB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qualtran**\n",
        "\n",
        "https://github.com/quantumlib/Qualtran/blob/main/qualtran/surface_code/magic_state_factory.py\n",
        "\n",
        "https://github.com/quantumlib/Qualtran/tree/main/qualtran"
      ],
      "metadata": {
        "id": "K-VJ5ea6lArD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QuTip**\n",
        "\n",
        "https://qutip.org/qutip-tutorials/#visualizations\n",
        "\n",
        "https://nbviewer.org/urls/qutip.org/qutip-tutorials/tutorials-v4/visualization/bloch_sphere_with_colorbar.ipynb"
      ],
      "metadata": {
        "id": "lCeP13HblFCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QubiC**\n",
        "\n",
        "An open source FPGA-based control and measurement system for superconducting quantum information processors, https://arxiv.org/pdf/2101.00071.pdf"
      ],
      "metadata": {
        "id": "UVy4V3ayu4Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Speichersichere Programmiersprachen*"
      ],
      "metadata": {
        "id": "6uUn645o-PmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://m.winfuture.de/news/141447\n",
        "\n",
        "* Konkret hat das White House Office of the National Cyber Director (ONCD) einen Bericht (PDF) veröffentlicht (via Infoworld), in dem man Entwicklern nahelegt, \"speichersichere Programmiersprachen\" zu verwenden - und dazu zählen C und C++ eben nicht. Laut ONCD können IT-Unternehmen mit der Einführung speichersicherer Programmiersprachen verhindern, \"dass ganze Klassen von Schwachstellen in das digitale Ökosystem eindringen\".\n",
        "\n",
        "* Der Begriff bezieht sich auf Fehler und Schwachstellen bzw. deren Schutz, die den Speicherzugriff betreffen, dazu zählen **Buffer Overflows, Out-of-Bound-Reads und Memory Leaks**. Mit dieser Meinung ist das Weiße Haus auch nicht alleine, da auch Microsoft und Google in Studien festgestellt haben, dass rund 70 Prozent aller Schwachstellen auf das Konto von Speicherproblemen zurückgehen.\n",
        "\n",
        "* [Laut der National Security Agency (NSA)](https://m.winfuture.de/news/132951) werden folgende Programmiersprachen als \"Memory-Safe\" bzw. sicher eingestuft:\n",
        "Rust\n",
        "Go\n",
        "C#\n",
        "Java\n",
        "Swift\n",
        "JavaScript\n",
        "Ruby"
      ],
      "metadata": {
        "id": "xUdNWhEP-ShT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *JAX*"
      ],
      "metadata": {
        "id": "EkdQBmsWyXyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/jax-101/index.html"
      ],
      "metadata": {
        "id": "TyH5oxOchjqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
      ],
      "metadata": {
        "id": "MNTTlgnLyXBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html"
      ],
      "metadata": {
        "id": "sKF2rI6NhaHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Linux*"
      ],
      "metadata": {
        "id": "NpX_msD1vmi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.digitalocean.com/community/tutorials/linux-commands"
      ],
      "metadata": {
        "id": "D1N-de6GwqUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ls - The most frequently used command in Linux to list directories\n",
        "* pwd - Print working directory command in Linux\n",
        "* cd - Linux command to navigate through directories\n",
        "* mkdir - Command used to create directories in Linux\n",
        "* mv - Move or rename files in Linux\n",
        "* cp - Similar usage as mv but for copying files in Linux\n",
        "* rm - Delete files or directories\n",
        "* touch - Create blank/empty files\n",
        "* ln - Create symbolic links (shortcuts) to other files\n",
        "* cat - Display file contents on the terminal\n",
        "* clear - Clear the terminal display\n",
        "* echo - Print any text that follows the command\n",
        "* less - Linux command to display paged outputs in the terminal\n",
        "* man - Access manual pages for all Linux commands\n",
        "* uname - Linux command to get basic information about the OS\n",
        "* whoami - Get the active username\n",
        "* tar - Command to extract and compress files in Linux\n",
        "* grep - Search for a string within an output\n",
        "* head - Return the specified number of lines from the top\n",
        "* ail - Return the specified number of lines from the bottom\n",
        "* diff - Find the difference between two files\n",
        "* cmp - Allows you to check if two files are identical\n",
        "* comm - Combines the functionality of diff and cmp\n",
        "* sort - Linux command to sort the content of a file while outputting\n",
        "* export - Export environment variables in Linux\n",
        "* zip - Zip files in Linux\n",
        "* unzip - Unzip files in Linux\n",
        "* ssh - Secure Shell command in Linux\n",
        "* service - Linux command to start and stop services\n",
        "* ps - Display active processes\n",
        "* kill and killall - Kill active processes by process ID or name\n",
        "* df - Display disk filesystem information\n",
        "* mount - Mount file systems in Linux\n",
        "* chmod - Command to change file permissions\n",
        "* chown - Command for granting ownership of files or folders\n",
        "* ifconfig - Display network interfaces and IP addresses\n",
        "* traceroute - Trace all the network hops to reach the destination\n",
        "* wget - Direct download files from the internet\n",
        "* ufw - Firewall command\n",
        "* iptables - Base firewall for all other firewall utilities to interface with\n",
        "* apt, pacman, yum, rpm - Package managers depending on the distro\n",
        "* sudo - Command to escalate privileges in Linux\n",
        "* cal - View a command-line calendar\n",
        "* alias - Create custom shortcuts for your regularly used commands\n",
        "* dd - Majorly used for creating bootable USB sticks\n",
        "* whereis - Locate the binary, source, and manual pages for a command\n",
        "* whatis - Find what a command is used for\n",
        "* top - View active processes live with their system usage\n",
        "* useradd and usermod - Add new user or change existing users data\n",
        "* passwd - Create or update passwords for existing users"
      ],
      "metadata": {
        "id": "uO0J0iBAwjsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Python*"
      ],
      "metadata": {
        "id": "-bgPYHGr1kCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For (Loop)**"
      ],
      "metadata": {
        "id": "bHxVacZSzNRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute n^3\n",
        "\n",
        "example_list = [1,4,9]\n",
        "cube_list = []\n",
        "\n",
        "for value in example_list:\n",
        "    cube_list.append(value**3)\n",
        "\n",
        "print(cube_list)"
      ],
      "metadata": {
        "id": "HEOrl3Il0Y-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3068cf41-07be-4f60-968d-ffca9b658a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 729]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Range**\n",
        "\n",
        "https://www.freecodecamp.org/news/python-for-loop-for-i-in-range-example/"
      ],
      "metadata": {
        "id": "axn8EIN20d2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9JzPC-Xzf8L",
        "outputId": "dd541f76-1c0c-4c6e-e316-0fe38a849606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,6,2): # Start, Stop, Steps\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T76OMRBtzjHJ",
        "outputId": "5b5fdd74-eb6c-4466-ed24-6bdec39359e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Now outsource one value to get generic function:*"
      ],
      "metadata": {
        "id": "y1phzBwszy2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  for i in range(0,x,2): # Start, Stop, Steps\n",
        "    print(i)\n",
        "\n",
        "x = 10\n",
        "my_function(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMM3qKJyOFqF",
        "outputId": "802138dc-c551-4c3d-e85e-483749ee6862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**While** (Loop)\n",
        "\n",
        "* while = as long as: *While Loop (simplest loop): is given a condition, and executes the loop until the condition is false. (If the condition is false, it does nothing.)*\n",
        "\n",
        "* https://www.w3schools.com/python/python_while_loops.asp\n"
      ],
      "metadata": {
        "id": "D3C3Frwh0FU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Berechne den Restwert in der Modulurechnung\n",
        "\n",
        "# Setup Variables\n",
        "modulus = 51 # moduland\n",
        "modulo = 7\n",
        "\n",
        "# Compute Modulus\n",
        "while modulus > modulo:\n",
        "    modulus = modulus - modulo\n",
        "\n",
        "# Show Restwert\n",
        "print(modulus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vhM4pD40A0K",
        "outputId": "e519a1bd-437f-49d1-f054-60564d4186f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Break**\n",
        "\n",
        "*Break escapes the current loop, and moves on after it. This is mostly used for searching for particular values - iterating until something happens.*"
      ],
      "metadata": {
        "id": "IYhDJ4SI0tBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  if i==5:\n",
        "    break\n",
        "print(i)"
      ],
      "metadata": {
        "id": "65DXNSnQ0n0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Else** (Rekursion)\n",
        "\n",
        "*It is a better pattern than while loops for high intensity computing because of your easy ability to set an upper limit for loop length. But if the calculation fails, you need a way to determine it: fortunately, for supports an else clause which triggers if and only if the loop exists without hitting a break statement.*"
      ],
      "metadata": {
        "id": "HFsqRuLw053E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 101\n",
        "\n",
        "for i in range(100):\n",
        "  if i==n:\n",
        "    break\n",
        "else:\n",
        "  print(\"Unable to find\" + str(n))\n",
        "\n",
        "print(i)"
      ],
      "metadata": {
        "id": "PSva2CdA09cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continue**\n",
        "\n",
        "*Break only breaks out of the closest loop. There is no way to easily break out of a higher loop just using break. Continue works similarly, but instead of escaping the loop, it starts the next iteration immediately. This is great for discarding cases, and is a common task in the algorithm:*"
      ],
      "metadata": {
        "id": "49wpgaI61FV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  if i==5:\n",
        "    continue\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "L03w27k71Gov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fd5a1f-52be-480a-d4e9-13cf6d27189a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# != means not equal\n",
        "# i%3 means\n",
        "# Restwerte berechnen in Modulo hiermit: 51%7\n",
        "# \\leq means ≤, so less than or equal to ..\n",
        "\n",
        "for i in range(10):\n",
        "  if i%3 != i%2:\n",
        "    continue\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "od2pk8MN1VGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print and Return**\n",
        "\n",
        "* Return keeps value\n",
        "\n",
        "* [More about print statement](https://realpython.com/python-print/#printing-in-a-nutshell)"
      ],
      "metadata": {
        "id": "NjmO1Y-0zJyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flow Control: Conditionals**"
      ],
      "metadata": {
        "id": "tJqcIKCr3Dss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=3\n",
        "if x>10:\n",
        "    print(\"x is big.\")\n",
        "if x<= 10:\n",
        "    print(\"x is small.\")"
      ],
      "metadata": {
        "id": "wGXplj2U3Eh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=3\n",
        "if x>10:\n",
        "    print(\"x is big.\")\n",
        "elif x>0:\n",
        "    print(\"x is small.\")\n",
        "else:\n",
        "    print(\"x is not positive.\")"
      ],
      "metadata": {
        "id": "TwrTdspJ3IqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yield**\n",
        "\n",
        "* Generators: like yield: https://www.python-kurs.eu/generatoren.php\n",
        "\n",
        "* there is one reason we must bring them up; you cannot create tuples with a comprehension directly; they instead create a generator.\n",
        "\n",
        "* You can think of generators as single-use lists. This makes them considerably faster, and with a lower memory footprint, in many use cases. We have already talked about a few generators - in Python 3, range is a generator.\n",
        "\n",
        "* These can be used as arguments without the enclosing. For example, you could construct a string containing the integers with:"
      ],
      "metadata": {
        "id": "aOjvMkgg3iWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "    \"\"\"Ein Fibonacci-Zahlen-Generator\"\"\"\n",
        "    a, b, counter = 0, 1, 0\n",
        "    while True:\n",
        "        if (counter > n): return\n",
        "        yield a\n",
        "        a, b = b, a + b\n",
        "        counter += 1\n",
        "f = fibonacci(5)\n",
        "for x in f:\n",
        "  print x,\n",
        "print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "Sz47xQO936wm",
        "outputId": "21a1c7b8-a0ef-416d-d51c-4c5a910d3614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-fb5705716190>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print x\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(x)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def firstn(g, n):\n",
        "\tfor i in range(n):\n",
        "\t\tyield g.next()\n",
        "\n",
        "\n",
        "firstn(5, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUCdS4iv5iXL",
        "outputId": "8f79513d-6a67-47cf-8473-516cc3d7c2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object firstn at 0x7f162b6846d0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.python-kurs.eu/generatoren.php"
      ],
      "metadata": {
        "id": "3-kxvfnn5TZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enumerate** (Aufzählen)\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Enumeration\n",
        "\n",
        "* https://realpython.com/python-enumerate/"
      ],
      "metadata": {
        "id": "oaYQE4Q23jMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [\"a\", \"b\", \"c\"]\n",
        "for count, value in enumerate(values):\n",
        "  print(count, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5FfJ9pf4hzk",
        "outputId": "7db098ae-d0c5-4de9-c8dc-822c59db9384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 a\n",
            "1 b\n",
            "2 c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Miscellaneous**"
      ],
      "metadata": {
        "id": "hkDgUCb546zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 10\n",
        "a += 5\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpKfGQeX46UN",
        "outputId": "2298cd82-0f47-4b44-a6cf-afa98c889f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Functions in Maths & Physics I (Def)**"
      ],
      "metadata": {
        "id": "fLMArogly99J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.w3schools.com/python/python_functions.asp\n",
        "\n",
        "http://www-personal.umich.edu/~mejn/computational-physics/"
      ],
      "metadata": {
        "id": "w_B7I77I6aKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  return x + 2\n",
        "\n",
        "my_function(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdyUIe0tK7O8",
        "outputId": "b4df5afd-c957-4444-ffe2-4cb99a548dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  for i in range(1,x):\n",
        "    print(i**2)\n",
        "\n",
        "my_function(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUCfTgihCl9v",
        "outputId": "ea876fcd-2cca-4912-c05d-eee817785753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "4\n",
            "9\n",
            "16\n",
            "25\n",
            "36\n",
            "49\n",
            "64\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def isqrt(n):\n",
        "  return int(math.sqrt(n))\n",
        "\n",
        "n = 100\n",
        "isqrt(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjS5fc4C2UMM",
        "outputId": "c78c2f48-21f9-43ce-c03d-b6acda0c8392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(n):\n",
        "  if n = (1):\n",
        "    return 1,\n",
        "  else:\n",
        "    return alg2d(n-1) + n\n",
        "\n",
        "alg2d(3)"
      ],
      "metadata": {
        "id": "ZkLNDfQ99hSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  return 5 * x\n",
        "\n",
        "print(my_function(3))\n",
        "print(my_function(5))\n",
        "print(my_function(9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgVKnVjc7o4u",
        "outputId": "6b9b66cc-90c4-40b5-eb37-b31084f93ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "25\n",
            "45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def selectionsort(seq):\n",
        "  for i in range(len(seq) - 1):\n",
        "    k = i\n",
        "    for j in range(i, len(seq)):\n",
        "      if seq[j] < seq[k]:\n",
        "        k = j\n",
        "    seq[i], seq[k] = seq[k], seq[i]\n",
        "\n",
        "result = selectionsort([1, 3, 5, 2, 4])\n",
        "result"
      ],
      "metadata": {
        "id": "qs_2cqJU8Ifu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def function(input):\n",
        "  for i in range(len(input) + 1):\n",
        "\n",
        "function([1, 2, 4])"
      ],
      "metadata": {
        "id": "BCUcl2Yf7vwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the Fibonacci numbers up to 1000\n",
        "f1,f2 = 1,1\n",
        "while f2<1000:\n",
        "    print(f2)\n",
        "    f1,f2 = f2,f1+f2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqI9EP3s68wt",
        "outputId": "24c3f412-b883-49c1-f4f7-6a77bcd51f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "5\n",
            "8\n",
            "13\n",
            "21\n",
            "34\n",
            "55\n",
            "89\n",
            "144\n",
            "233\n",
            "377\n",
            "610\n",
            "987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the wavelengths of hydrogen lines\n",
        "R = 1.097e-2\n",
        "for m in range(1,4):\n",
        "    print(\"Series for m =\",m)\n",
        "    for n in range(m+1,m+6):\n",
        "        invlambda = R*(1/m**2-1/n**2)\n",
        "        print(\"  \",1/invlambda,\"nm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA7mkKRZ8fcy",
        "outputId": "589eb076-750b-48c0-f174-875a916d72c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series for m = 1\n",
            "   121.5436037678517 nm\n",
            "   102.55241567912488 nm\n",
            "   97.23488301428137 nm\n",
            "   94.95594044363415 nm\n",
            "   93.76220862091418 nm\n",
            "Series for m = 2\n",
            "   656.3354603463993 nm\n",
            "   486.1744150714068 nm\n",
            "   434.084299170899 nm\n",
            "   410.2096627164995 nm\n",
            "   397.04243897498225 nm\n",
            "Series for m = 3\n",
            "   1875.2441724182836 nm\n",
            "   1281.9051959890612 nm\n",
            "   1093.8924339106654 nm\n",
            "   1005.013673655424 nm\n",
            "   954.6697605038536 nm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the internal energy of a quantum simple harmonic oscillator at temperature T\n",
        "from math import exp\n",
        "\n",
        "terms = 1000\n",
        "beta = 1/100\n",
        "S = 0.0\n",
        "Z = 0.0\n",
        "for n in range(terms):\n",
        "    E = n + 0.5\n",
        "    weight = exp(-beta*E)\n",
        "    S += weight*E\n",
        "    Z += weight\n",
        "\n",
        "print(S/Z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr3JAICQ7I80",
        "outputId": "835890b7-3433-4fee-e584-8bd48e8d10ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.95543134093475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_5():\n",
        "  for i in range(100):\n",
        "    for j in range(20):\n",
        "      if i == 5:\n",
        "        return i\n",
        "  print(\"Couldn't find 5.\")"
      ],
      "metadata": {
        "id": "7p_dm_Bi1iOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def natural_language(message):\n",
        "    print(message)\n",
        "\n",
        "message = \"Hello World\"\n",
        "natural_language(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDLQQt7E2IeL",
        "outputId": "1cdaf4c1-bc75-4bbd-e1ff-a68eac58c8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Functions in Maths & Physics II (Class)**"
      ],
      "metadata": {
        "id": "xEP2z9S719Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Video: Methoden in Klassen](https://www.youtube.com/watch?v=58IjjwHs_4A)"
      ],
      "metadata": {
        "id": "aBZdbhCcFgF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tweet:\n",
        "  def __init__(self):\n",
        "    print('Hi')\n",
        "\n",
        "result = Tweet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq8z1hDi18sH",
        "outputId": "a5330983-f9ae-4e1f-9e6d-45e9a5ed0b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class and function / self is referring to the instances below\n",
        "class Tweet:\n",
        "  def __init__(self, message):\n",
        "    self.x = message\n",
        "\n",
        "# Define messages (instances)\n",
        "a = Tweet('Hello 42')\n",
        "b = Tweet('Hello 88')\n",
        "\n",
        "# Call messages\n",
        "print('a is ' + a.x)\n",
        "print('b is ' + b.x)"
      ],
      "metadata": {
        "id": "mZqS96oX2gLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class and function\n",
        "class Tweet:\n",
        "  def __init__(self, message):\n",
        "    self.message = message\n",
        "  def print_tweet(self):\n",
        "    print(self.message)\n",
        "\n",
        "# Define messages (objects?)\n",
        "c = Tweet('Hello 100')\n",
        "\n",
        "# Call messages\n",
        "c.print_tweet()"
      ],
      "metadata": {
        "id": "5PpAf4es2hmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Car():\n",
        "\n",
        "    # init method or constructor\n",
        "    # (these are not functions, because functions have no self and also within class they are called methods)\n",
        "    def __init__(self, model, color):\n",
        "        self.model = model\n",
        "        self.color = color\n",
        "\n",
        "    def show(self):\n",
        "        print(\"Model is\", self.model )\n",
        "        print(\"color is\", self.color )\n",
        "\n",
        "# both objects have different self which\n",
        "# contain their attributes\n",
        "audi = Car(\"audi a4\", \"blue\")\n",
        "ferrari = Car(\"ferrari 488\", \"green\")\n",
        "\n",
        "audi.show()     # same output as car.show(audi)\n",
        "ferrari.show()  # same output as car.show(ferrari)\n",
        "\n",
        "# Behind the scene, in every instance method\n",
        "# call, python sends the instances also with\n",
        "# that method call like car.show(audi)"
      ],
      "metadata": {
        "id": "y0r0UEgi2kB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "self parameter: https://www.youtube.com/watch?v=CLoK-_qNTnU\n",
        "\n",
        "klassen und objekte: https://www.youtube.com/watch?v=XxCZrT7Z3G4\n",
        "\n",
        "methoden: https://www.youtube.com/watch?v=58IjjwHs_4A"
      ],
      "metadata": {
        "id": "cSeBZR6g2mU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Functions**"
      ],
      "metadata": {
        "id": "DSkCPO-97ZAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scriptverse.academy/tutorials/python-matplotlib-plot-function.html"
      ],
      "metadata": {
        "id": "y1ALPQD18XwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(10, 6), \"lines.linewidth\": 1.0})\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "x = np.linspace(0,10,10)\n",
        "\n",
        "# the function\n",
        "y = x**2\n",
        "\n",
        "# setting the axes at the centre\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "# plot the function\n",
        "#plt.plot(x,y, 'g')\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cbiVzNHZDJmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(10, 6), \"lines.linewidth\": 1.0})\n",
        "\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "x = np.linspace(0,1,100)\n",
        "x = np.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# the function\n",
        "y = x**20\n",
        "y = np.sin(x)\n",
        "\n",
        "# setting the axes at the centre\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "# plot the function\n",
        "#plt.plot(x,y, 'g')\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "plt.plot(x,2*y, 'c', label='y=2sin(x)')\n",
        "plt.plot(x,3*y, 'r', label='y=3sin(x)')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "8JuGtYMGPwpx",
        "outputId": "abc363b7-cec6-4d43-df48-94aa3acdb440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 linearly spaced numbers\n",
        "x = np.linspace(-5,5,100)\n",
        "\n",
        "# the function, which is y = x^3 here\n",
        "y = x**2\n",
        "y = np.exp(x)\n",
        "\n",
        "# setting the axes at the centre\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.spines['left'].set_position('center')\n",
        "ax.spines['bottom'].set_position('center')\n",
        "ax.spines['right'].set_color('none')\n",
        "ax.spines['top'].set_color('none')\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "# plot the function\n",
        "plt.plot(x,y, 'g')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "Rt99Vbv1OmFX",
        "outputId": "7394fe92-6469-4dd6-b58b-b53bd63b0009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vFQCvRyaxKAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*Google*"
      ],
      "metadata": {
        "id": "EV6x8bY7pvDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks offers the most extensive overview of activation functions to date. It aims to streamline the selection process for researchers and practitioners, reducing redundancy and the unintentional rediscovery of functions.\n",
        "\n",
        "https://arxiv.org/abs/2402.09092\n",
        "\n",
        "\n",
        "Unifying Large Language Models and Knowledge Graphs: A Roadmap\n",
        "\n",
        "https://arxiv.org/abs/2306.08302"
      ],
      "metadata": {
        "id": "JKXH9meQFzR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Google Products*"
      ],
      "metadata": {
        "id": "WnAvpRxbPSwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vertex AI! 🎉\n",
        "\n",
        "🔎 You can now query an index from the Vector Search console, making it even easier to validate your retrievals.\n",
        "\n",
        "🌻 New models have been added to the Model Garden, including Stable Diffusion XL LCM, LLaVA 1.5, PyTorch-ZipNeRF, WizardLM, and more.\n",
        "\n",
        "🎥 Multimodal Embeddings video support went GA, giving you the possibility to extend your AI applications.\n",
        "\n",
        "✨ Vertex AI Gemini 1.0 Pro and Gemini 1.0 Pro Vision multimodal language models also went GA!\n",
        "\n",
        "Overview of multimodal models\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview\n",
        "\n",
        "Multimodal video embeddings\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#vid-img-txt-request\n",
        "\n",
        "Vertex AI Vector search\n",
        "https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index"
      ],
      "metadata": {
        "id": "J_TrPlyPP6Um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview\n",
        "\n",
        "https://www.linkedin.com/in/deltorobarba/recent-activity/reactions/\n",
        "\n",
        "![sciences](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1699.jpeg)"
      ],
      "metadata": {
        "id": "ImUnO-bDTkNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "book: Official Google Cloud Certified Professional Machine Learning Engineer Study Guide, https://www.amazon.com/dp/1119944465?ref_=cm_sw_r_mwn_dp_QEQREW6ES28470H5333A&language=en-US"
      ],
      "metadata": {
        "id": "ehkWx65LkI5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.linkedin.com/pulse/use-mistral-llms-locally-your-computer-lm-studio-windows-molina-ldmje"
      ],
      "metadata": {
        "id": "tkRhOULDj_Xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.linkedin.com/pulse/gemini-look-my-food-give-me-new-ideas-filipe-gracio-phd-tl4be"
      ],
      "metadata": {
        "id": "EiWGjWMykCYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lumiere*\n",
        "\n",
        "The first generative video goodness of 2024 is in: Lumiere. A text-to-video, image-to-video, and stylized video generation model from Google Research.\n",
        "\n",
        "Lumiere demonstrates state-of-the-art video generation results with a new model design that facilitates a wide range of content creation tasks such as video editing, video in-painting, and stylized generation.\n",
        "\n",
        "While existing video models first synthesize distant keyframes then apply temporal super-resolution to make videos look temporally consistent, Lumiere generates the entire temporal duration of the video at once, through a single pass in the model. This makes the videos look more realistic.\n",
        "\n",
        "Paper: https://lnkd.in/dvcWNCtp\n",
        "Video: https://lnkd.in/deQHkxFj"
      ],
      "metadata": {
        "id": "3fNy3RrnyTa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Appendix: Models*"
      ],
      "metadata": {
        "id": "1Amg0O9YxNcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Differentiable Programming and Probabilistic Programming*\n",
        "\n",
        "![ggg](https://raw.githubusercontent.com/deltorobarba/repo/master/sciences_1701.jpg)\n",
        "\n",
        "https://www.lokad.com/blog/2024/1/29/probabilistic-exponential-smoothing-for-explainable-ai-in-supply-chain/"
      ],
      "metadata": {
        "id": "or8sN-Vzjadz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <font color=\"blue\">*(Vector) Norms for Regularization*"
      ],
      "metadata": {
        "id": "fk2nIlNKjMY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- List of 17 similarity metrics: https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681\n",
        "- Advantages and disadvantages of 9 distance metrics: https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "Mahalanobis distance is useful when dealing with variables measured in different scales (so the units of measure become standardized) and also, in order to avoid correlation issues between these variables."
      ],
      "metadata": {
        "id": "qLE1qfquKDn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Regression*"
      ],
      "metadata": {
        "id": "F4uWOy1kiJIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms/notebook\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Residual_sum_of_squares"
      ],
      "metadata": {
        "id": "4x7F_UT4iLrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "h8VPlSUFiaHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "z7hExSBmilec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "2LHQ82bvinIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‘sigmoid‘ activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "CuQf_KldipMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen–Shannon divergence**\n",
        "\n",
        "* It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erfüllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "aPzEvAhTirT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Distances in Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "sCrxJgguiyCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "zym317e9i6qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I’ve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It’s used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That’s why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) — a D-dimensional vector, transform this sample into a d-dimensional vector, where d ≪ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification — it doesn’t impose any constraints on the distribution on the model’s inner representation of the given data — i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag — a Boolean label, specifying whether these two samples are “similar” or “dissimilar”. So the input must be not one, but 2 images.\n",
        "  * It penalizes “similar” samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * “Dissimilar” samples are penalized by being to close to each other, but in a somewhat different way — Contrastive Loss introduces the concept of “margin” — a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It’s a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it’s a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "mTcFGaEmi8kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "fiBHTDOMi-iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a “Closed-set” task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case — i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a “Open-set” task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features — features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning — and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people’s faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "E7EvqMAjjAgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost, loss, risk or error function*"
      ],
      "metadata": {
        "id": "F1FnAXf0DeCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost, loss, risk or error function**\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/cost.jpg)\n",
        "\n",
        "* The loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set.\n",
        "\n",
        "* Also: objective function, error, cost & loss function. A loss function measures the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).\n",
        "gradient of cost function tells each weight how to change to improve overall prediction\n",
        "MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.\n",
        "\n",
        "* We want to find the local minimum of the cost function\n",
        "\n",
        "*  Quadratic cost (mean squared error MSE):\n",
        "also maximum likelihood, and sum squared error.\n",
        "Most common. Used in regression.\n",
        "Mean squared error is appropriate to regression (line/curve fitting) where the goal is to minimize the mean squared error between the training set (points) and the fitted curve.\n",
        "\n",
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n",
        "\n",
        "* In most cases, our parametric model defines a distribution […] and we simply use the **principle of maximum likelihood**. This means we use the cross-entropy between the training data and the model’s predictions as the cost function.\n",
        "\n",
        "* It is important, therefore, that the function faithfully represent our design goals. If we choose a poor error function and obtain unsatisfactory results, the fault is ours for badly specifying the goal of the search.\n",
        "\n",
        "**Maximum Likelihood Estimation**\n",
        "\n",
        "* Maximum likelihood seeks to find the optimum values for the parameters by maximizing a likelihood function derived from the training data.\n",
        "\n",
        "* Given input, the model is trying to make predictions that **match the data distribution of the target variable**. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
        "\n",
        "* One way to interpret maximum likelihood estimation is to view it as **minimizing the dissimilarity** between the empirical distribution […] defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. […] **Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions**.\n",
        "\n",
        "* Under appropriate conditions, the maximum likelihood estimator has the **property of consistency** […], meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter.\n",
        "\n",
        "* Under the framework maximum likelihood, the error between two probability distributions is measured using cross-entropy. Under maximum likelihood estimation, we would seek a set of model weights that minimize the difference between the model’s predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
        "\n",
        "When using the framework of maximum likelihood estimation, we will implement a cross-entropy loss function, which often in practice means:\n",
        "* a **cross-entropy** loss function for classification problems and\n",
        "* a **mean squared error** loss function for regression problems.\n",
        "\n",
        "* Under the framework of maximum likelihood estimation and assuming a **Gaussian distribution for the target variable**, mean squared error can be considered the cross-entropy between the distribution of the model predictions and the distribution of the target variable.\n",
        "\n",
        "* Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer.\n",
        "\n",
        "* Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
        "\n",
        "* For example, **mean squared error is the cross-entropy between the empirical distribution and a Gaussian model**\n",
        "\n",
        "\n",
        "https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
        "\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Loss_function\n",
        "\n",
        "* https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "* https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "\n",
        "* https://allenkunle.me/deriving-ml-cost-functions-part1\n",
        "\n",
        "\n",
        "* Properties of ideal Cost functions:\n",
        "  * smooth,\n",
        "  * continuous,\n",
        "  * symmetric (but i.e. Non-symmetric losses: e.g., for spam classification)\n",
        "  * differentiable\n",
        "\n",
        "**Similarity learning** is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, identity of indiscernibles, symmetry and subadditivity (or the triangle inequality). **In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.**\n",
        "\n",
        "> $\\min _{W}\\left\\{L(W):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(W ; x_{i}, y_{i}\\right)+\\lambda r(W)\\right\\}$\n",
        "\n",
        "**Similarity Learning & Distance Metric Learning**\n",
        "\n",
        "* Ähnlichkeitsmaße werden für nominal oder ordinal skalierte Variablen genutzt\n",
        "\n",
        "* Distanzmaße werden für metrisch skalierte Variablen (d. h. für Intervall- und Verhältnisskala) genutzt.\n",
        "\n",
        "Complete list of [Loss / Cost Functions in TF](https://www.tensorflow.org/api_docs/python/tf/keras/losses/)"
      ],
      "metadata": {
        "id": "w3-c_-luhCKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Regression & Forecasting (mostly distance-based)*"
      ],
      "metadata": {
        "id": "DjYXE5LZhFXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions that belong to the category \"distance-based\" are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.\n",
        "\n",
        "> Great overview: http://juliaml.github.io/LossFunctions.jl/stable/losses/distance/\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/regression_loss.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm_tzCTnhHNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Linear) Least Squares**\n",
        "\n",
        "* Least Squares: Deren Parameter werden so bestimmt, dass die Summe der Abweichungsquadrate e der Beobachtungen y von den Werten der Funktion minimiert wird.\n",
        "\n",
        "* Da die Kleinste-Quadrate-Schätzung die Residuenquadratsumme minimiert, ist es dasjenige Schätzverfahren, welches das [Bestimmtheitsmaß](https://de.wikipedia.org/wiki/Bestimmtheitsmaß) maximiert.\n",
        "\n",
        "* Das Bestimmtheitsmaß der Regression, auch empirisches Bestimmtheitsmaß, ist eine dimensionslose Maßzahl die den Anteil der Variabilität in den Messwerten der abhängigen Variablen ausdrückt, der durch das lineare Modell „erklärt“ wird. Mithilfe dieser Definition können die Extremwerte für das Bestimmtheitsmaß aufgezeigt werden. Für das\n",
        "Bestimmtheitsmaß gilt, dass es umso năher am Wert 1 ist, je kleiner die Residuenquadratsumme ist. Es wird maximal gleich 1 wenn $\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=0$ ist, also alle Residuen null sind. In diesem Fall ist die Anpassung an die Daten perfekt, was bedeutet, dass für jede Beobachtung $y_{i}=\\hat{y}_{i}$ ist.\n",
        "\n",
        "* [Least Squares](https://en.wikipedia.org/wiki/Least_squares) / [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) & [Linear Least Squares](https://en.wikipedia.org/wiki/Linear_least_squares)"
      ],
      "metadata": {
        "id": "cNJH7IHehJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gauss–Markov theorem (BLUE)**\n",
        "\n",
        "*  states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, **if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero**.\n",
        "\n",
        "* stellt eine theoretische Rechtfertigung der Methode der kleinsten Quadrate dar\n",
        "\n",
        "* Der Satz besagt, dass in einem linearen Regressionsmodell, in dem die **Störgrößen (error term) einen Erwartungswert von null und eine konstante Varianz haben sowie unkorreliert sind** (Annahmen des klassischen linearen Regressionsmodells), der Kleinste-Quadrate-Schätzer – vorausgesetzt er existiert – ein bester linearer erwartungstreuer Schätzer ist (englisch Best Linear Unbiased Estimator, kurz: BLUE).\n",
        "\n",
        "* Hierbei bedeutet der „beste“, dass er – innerhalb der Klasse der linearen erwartungstreuen Schätzer – die „kleinste“ Kovarianzmatrix aufweist und somit minimalvariant ist. Die Störgrößen müssen nicht notwendigerweise normalverteilt sein. Sie müssen im Fall der verallgemeinerten Kleinste-Quadrate-Schätzung auch nicht unabhängig und identisch verteilt sein.\n",
        "\n",
        "The Gauss-Markov assumptions concern the set of error random variables, $\\varepsilon_{i}:$\n",
        "\n",
        "1. They have mean zero: $\\mathrm{E}\\left[\\varepsilon_{i}\\right]=0$\n",
        "\n",
        "2. They are homoscedastic, that is all have the same finite variance: $\\operatorname{Var}\\left(\\varepsilon_{i}\\right)=\\sigma^{2}<\\infty$ for all $i$,\n",
        "3. Distinct error terms are uncorrelated: $\\operatorname{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0, \\forall i \\neq j$.\n",
        "\n",
        "A linear estimator of $\\beta_{j}$ is a linear combination $\\widehat{\\beta}_{j}=c_{1 j} y_{1}+\\cdots+c_{n j} y_{n}$\n",
        "\n",
        "* The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance).\n",
        "\n",
        "* The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the [James–Stein estimator](https://en.wikipedia.org/wiki/James–Stein_estimator) (which also drops linearity), [ridge regression(Tikhonov_regularization)](https://en.wikipedia.org/wiki/Tikhonov_regularization), or simply any [degenerate estimator](https://en.wikipedia.org/wiki/Degenerate_distribution).\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Gauss–Markov_theorem"
      ],
      "metadata": {
        "id": "wqLJa7izhK3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "* Ordinary least squares is a type of linear least squares method for estimating the unknown parameters in a linear regression model.\n",
        "\n",
        "* “Ordinary Least Squares” (OLS) method is used to find the best line intercept (b) and the slope (m). [in y = mx + b, m is the slope and b the intercept]\n",
        "\n",
        "\n",
        "> $m=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}$\n",
        "\n",
        "> $b=\\bar{y}-m * \\bar{x}$\n",
        "\n",
        "* In other words → with OLS Linear Regression the goal is to find the line (or hyperplane) that minimizes the vertical offsets. We define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n.\n",
        "\n",
        "* OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
        "\n",
        "* The OLS method minimizes the sum of squared residuals, and leads to a [closed-form expression](https://en.wikipedia.org/wiki/Closed-form_expression) for the estimated value of the unknown parameter vector β.\n",
        "\n",
        "* It is important to point out though that OLS method will work for a univariate dataset (ie., single independent variables and single dependent variables). Multivariate dataset contains a single independent variables set and multiple dependent variables sets, requiring a machine learning algorithm called “Gradient Descent”.\n",
        "\n",
        "* [Wiki](https://en.wikipedia.org/wiki/Ordinary_least_squares) & [Medium](https://medium.com/@jorgesleonel/linear-regression-307937441a8b)"
      ],
      "metadata": {
        "id": "YvnQ8gFphMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Least Squares (WLS)**\n",
        "\n",
        "* are used when heteroscedasticity is present in the error terms of the model.\n",
        "* https://en.wikipedia.org/wiki/Weighted_least_squares"
      ],
      "metadata": {
        "id": "0xWob81xhO3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized Least Squares (GLS)**\n",
        "\n",
        "* is an extension of the OLS method, that **allows efficient estimation of β when either heteroscedasticity, or correlations, or both are present among the error terms of the model**, as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
        "\n",
        "* To handle heteroscedasticity when the error terms are uncorrelated with each other, GLS minimizes a weighted analogue to the sum of squared residuals from OLS regression, where the weight for the ith case is inversely proportional to var(εi). This special case of GLS is called \"weighted least squares\".\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Generalized_least_squares"
      ],
      "metadata": {
        "id": "L1U8vBsUhRcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SE, SAE & SSE**\n",
        "\n",
        "**Sum of Errors (SE)** the difference in the predicted value and the actual value.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma(\\hat{Y}-Y)$\n",
        "\n",
        "Errors terms cancel each other out.\n",
        "\n",
        "**Sum of Absolute Errors (SAE)** takes the absolute values of the errors for all iterations.\n",
        "\n",
        "$\\mathbf{L}=\\Sigma (|\\hat{Y}-Y|)$\n",
        "\n",
        "This loss function is not differentiable at 0.\n",
        "\n",
        "**Sum of Squared Errors (SSE)** is differentiable at all points and gives non-negative errors. But you could argue that why cannot we go for higher orders like 4th order or so. Then what if we consider to take 4th order loss function, which would look like:\n",
        "\n",
        "$\\mathbf{L}=\\left[\\Sigma(\\hat{Y}-Y)^{2}\\right]$\n",
        "\n",
        "The gradient of the loss function will vanish at minima & maxima. And the error will grow with the sample size.\n",
        "\n",
        "![xxx](https://raw.githubusercontent.com/deltorobarba/repo/master/sumoferrors.png)\n",
        "\n",
        "* Minimizing Sum of Squared Errors / SSE ([wiki](https://de.m.wikipedia.org/wiki/Residuenquadratsumme) and [medium](https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511)).  We can think of the SSE loss as the (unscaled) variance of the model errors.\n",
        "* Therefore **minimizing the SEE loss is equivalent to minimizing the variance of the model residuals**. For this reason, the sum of squares loss is often referred to as the Residual Sum of Squares error (RSS) for linear models. We can think of minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.\n",
        "* Ideal when distribution of residuals in normal: the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the [best unbiased estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) for the parameters .\n",
        "* Problem: Because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss\n"
      ],
      "metadata": {
        "id": "QegFiqDEhTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE (L2) & RMSE (Squared Euclidean Distance)**\n",
        "\n",
        "* Squared Euclidean distance is of central importance in estimating parameters of statistical models, where it is used in the method of least squares, a standard approach to regression analysis.\n",
        "\n",
        "* The corresponding loss function is the squared error loss (SEL), and places progressively greater weight on larger errors. The corresponding risk function (expected loss) is mean squared error (MSE).\n",
        "\n",
        "* **Squared Euclidean distance is not a metric**, as it does not satisfy the triangle inequality. However, **it is a more general notion of distance, namely a divergence** (specifically a Bregman divergence), and can be used as a statistical distance.\n",
        "\n",
        "https://en.m.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/3d-function-2.svg/566px-3d-function-2.svg.png)\n",
        "\n",
        "*A paraboloid, the graph of squared Euclidean distance from the origin*\n",
        "\n",
        "![bb](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/3d-function-5.svg/566px-3d-function-5.svg.png)\n",
        "\n",
        "*A cone, the graph of Euclidean distance from the origin in the plane*"
      ],
      "metadata": {
        "id": "nPIA8NHThWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "$\\mathrm{MSE}={\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Mean Squared Error (L2 or Quadratic Loss). Error decreases as we increase our sample data as the distribution of our data becomes more and more narrower (referring to normal distribution). The more data we have, the less is the error.\n",
        "* Can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better. It is always non – negative and values close to zero are better. The MSE is the second moment of the error (about the origin) and thus incorporates both the variance of the estimator and its bias.\n",
        "* Problem: Sensitive to outliers and the order of loss is more than that of the data. As my data is of order 1 and the loss function, MSE has an order of 2 (squared). So we cannot directly correlate data with the error.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate)\n",
        "\n",
        "**Mean Squared Logarithmic Error (MSLR)**\n",
        "\n",
        "* Mean Squared Logarithmic Error\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError\n",
        "\n",
        "**RMSE** (Root-Mean-Square Error)\n",
        "\n",
        "$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n}\\left(y_{j}-\\hat{y}_{j}\\right)^{2}}$\n",
        "\n",
        "* Root-Mean-Square Error is the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
        "* The **RMSE is directly interpretable in terms of measurement units**, and so is a better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit. Metric can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable\n",
        "* https://www.sciencedirect.com/science/article/pii/S096014811831231X\n",
        "* The **RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian**.\n",
        "https://www.geosci-model-dev-discuss.net/7/C473/2014/gmdd-7-C473-2014-supplement.pdf\n",
        "* When both metrics are calculated, the MAE tends to be much smaller than the RMSE because the RMSE penalizes large errors while the MAE gives the same weight to all errors.\n",
        "* They summarized that the **RMSE tends to become increasingly larger than the MAE** (but not necessarily in a monotonic fashion) as the distribution of error magnitudes becomes more variable. The RMSE tends to 1 grow larger than the MAE with n2 since its lower limit is fixed at the MAE and its upper 11 limit (n2 · MAE) increases with n2 .\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Root-mean-square_deviation) & [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)"
      ],
      "metadata": {
        "id": "VTqsbCowhYav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE (L1) & MAPE**\n",
        "\n",
        "$\\mathrm{MAE}=\\frac{1}{n} \\sum_{j=1}^{n}\\left|y_{j}-\\hat{y}_{j}\\right|$\n",
        "\n",
        "* If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. MBE can convey useful information, but should be interpreted cautiously because positive and negative errors will cancel out.\n",
        "\n",
        "* Mean Absolute Error (L1 Loss)\n",
        "* Computes the mean of absolute difference between labels and predictions\n",
        "* measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "* On some regression problems, the **distribution of the target variable may be mostly Gaussian, but may have outliers**, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "* Metric can range from 0 to ∞ and are indifferent to the direction of errors. It is  negatively-oriented scores, which means lower values are better.\n",
        "* Extremwerte als Ausreißer mit geringerem Einfluss auf das Modell ansehen: MAE loss is useful if the training data is corrupted with outliers (i.e. we erroneously receive unrealistically huge negative/positive values in our training environment, but not our testing environment).\n"
      ],
      "metadata": {
        "id": "eX6fU9qohaM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE vs MSE**\n",
        "\n",
        "* One big problem in using MAE loss (for neural nets especially) is that its gradient is the same throughout, which means the gradient will be large even for small loss values.\n",
        "\n",
        "* This isn’t good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate.\n",
        "\n",
        "* The gradient of MSE loss is high for larger loss values and decreases as loss approaches 0, making it more precise at the end of training (see figure below.)\n",
        "\n",
        "![xx](https://raw.githubusercontent.com/deltorobarba/repo/master/mae_vs_mse.PNG)"
      ],
      "metadata": {
        "id": "F71Ij_KchcFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "$\\mathrm{M}=\\frac{1}{n} \\sum_{t=1}^{n}\\left|\\frac{A_{t}-F_{t}}{A_{t}}\\right|$\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is a statistical measure of **how accurate a forecast** system is.\n",
        "\n",
        "* It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values. Where At is the actual value and Ft is the forecast value.\n",
        "\n",
        "* The mean absolute percentage error (MAPE) is the most common measure used to forecast error, and works best if there are no extremes to the data (and no zeros).\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "MZzrxESQheD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
        "\n",
        "* There are 3 different definitions of sMAPE. Two of them are below:\n",
        "\n",
        "$\\operatorname{SMAPE}=\\frac{100 \\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(\\left|A_{t}\\right|+\\left|F_{t}\\right|\\right) / 2}$\n",
        "\n",
        "* Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
        "\n",
        "* At is the actual value and Ft is the forecast value\n",
        "\n",
        "* The absolute difference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n.\n",
        "\n",
        "* Armstrong's original definition is as follows:\n",
        "\n",
        "$\\mathrm{SMAPE (old)}=\\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\left|F_{t}-A_{t}\\right|}{\\left(A_{t}+F_{t}\\right) / 2}$\n",
        "\n",
        "* The problem is that it can be negative (if ${\\displaystyle A_{t}+F_{t}<0}$) or even undefined (if ${\\displaystyle A_{t}+F_{t}=0}$). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator.\n",
        "\n",
        "* In contrast to the mean absolute percentage error, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator)\n",
        "\n",
        "* One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the second SMAPE formula:\n",
        "\n",
        "  * Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76%\n",
        "\n",
        "  * Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) & [Wiki2](https://wiki2.org/en/Symmetric_mean_absolute_percentage_error) & [other](https://www.brightworkresearch.com/the-problem-with-using-smape-for-forecast-error-measurement/)"
      ],
      "metadata": {
        "id": "QsaM9Z0whfuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean absolute scaled error (MASE)**\n",
        "\n",
        "* mean absolute scaled error (MASE) is a measure of the accuracy of forecasts.\n",
        "\n",
        "*  It is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast. It was proposed in 2005.\n",
        "\n",
        "* The mean absolute scaled error has the following desirable propertie: [Wiki](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error)"
      ],
      "metadata": {
        "id": "6HSFPTdVhhon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Huber Loss (Smooth Mean Absolute Error)**\n",
        "\n",
        "* TLDR: will better find a minimum than L1, but less exposed to outliers than L2. However one has to tune the hyperparameter delta. The larger (3+), the more it is L2, the smaller (1), the more it is L1.\n",
        "\n",
        "* The Huber loss **combines the best properties of MSE and MAE** (Mean Absolute Error). It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter.\n",
        "\n",
        "* It's **less sensitive to outliers** in data than the squared error loss. It’s **also differentiable at 0**. It’s basically absolute error, which becomes quadratic when error is small.  How small that error has to be to make it quadratic depends on a hyperparameter 𝛿.\n",
        "\n",
        "* Once differentiable.\n",
        "\n",
        "$L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
        "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
        "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
        "\\end{array}\\right.$\n",
        "\n",
        "* **Huber loss approaches MSE when 𝛿 ~ 0 and MAE when 𝛿 ~ ∞**\n",
        "\n",
        "* The choice of delta is critical because it determines what you’re willing to consider as an outlier. Residuals larger than delta are minimized with L1 (which is less sensitive to large outliers), while residuals smaller than delta are minimized “appropriately” with L2.\n",
        "\n",
        "* One big problem with using MAE for training of neural nets is its constantly large gradient, which can lead to missing minima at the end of training using gradient descent. For MSE, gradient decreases as the loss gets close to its minima, making it more precise.\n",
        "Huber loss can be really helpful in such cases, as it curves around the minima which decreases the gradient. And it’s more robust to outliers than MSE. Therefore, **it combines good properties from both MSE and MAE**.\n",
        "\n",
        "* However, the problem with Huber loss is that we might need to train hyperparameter delta which is an iterative process.\n",
        "\n",
        "* [Wiki](https://en.m.wikipedia.org/wiki/Huber_loss) * [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
        "\n",
        "* https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
        "\n",
        "* The biggest problem with using MAE to train neural networks is the constant large gradient, which may cause the minimum point to be missed when the gradient descent is about to end. For MSE, the gradient will decrease as the loss decreases, making the result more accurate.\n",
        "\n",
        "* In this case, Huber loss is very useful. It will fall near the minimum value due to the decreasing gradient. It is more robust to outliers than MSE. Therefore, Huber loss combines the advantages of MSE and MAE. However, the problem with Huber loss is that we may need to constantly adjust the hyperparameters\n",
        "\n",
        "* https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "* When you compare this statement with the benefits and disbenefits of both the MAE and the MSE, you’ll gain some insights about how to adapt this delta parameter:\n",
        "\n",
        "* **If your dataset contains large outliers**, it’s likely that your model will not be able to predict them correctly at once. In fact, it might take quite some time for it to recognize these, if it can do so at all. This results in large errors between predicted values and actual targets, because they’re outliers. Since MSE squares errors, large outliers will distort your loss value significantly. If outliers are present, you likely don’t want to use MSE. Huber loss will still be useful, but you’ll have to use small values for 𝛿.\n",
        "\n",
        "* If it does not contain many outliers, it’s likely that it will generate quite accurate predictions from the start – or at least, from some epochs after starting the training process. In this case, you may observe that the errors are very small overall. Then, one can argue, it may be worthwhile to let the largest small errors contribute more significantly to the error than the smaller ones. In this case, MSE is actually useful; hence, with Huber loss, you’ll likely want to use quite large values for 𝛿.\n",
        "\n",
        "* If you don’t know, you can always start somewhere in between – for example, in the plot above, 𝛿 = 1 represented MAE quite accurately, while 𝛿 = 3 tends to go towards MSE already. What if you used 𝛿 = 1.5 instead? You may benefit from both worlds.\n",
        "\n",
        "https://www.machinecurve.com/index.php/2019/10/12/using-huber-loss-in-keras/\n",
        "\n",
        "* For target = 0, the loss increases when the error increases. However, the speed with which it increases depends on this 𝛿 value. In fact, Grover (2019) writes about this as follows: Huber loss approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞ (large numbers.)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/320px-Huber_loss.svg.png)\n",
        "\n",
        "*Huber loss (green,\n",
        "δ\n",
        "=\n",
        "1) and squared error loss (blue) as a function of\n",
        "y\n",
        "−\n",
        "f\n",
        "(\n",
        "x\n",
        ")*\n",
        "\n",
        "![huber](https://raw.githubusercontent.com/deltorobarba/repo/master/huberloss.jpg)"
      ],
      "metadata": {
        "id": "ZBE611qIhjXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-Cosh Loss**\n",
        "\n",
        "* TLDR: Similar to MAE, will not be affected by outliers. Log-Cosh has all the points of Huber loss, and no need to set hyperparameters. Compared with Huber, Log-Cosh derivation is more complicated, requires more computation, and is not used much in deep learning.\n",
        "\n",
        "* * Log-cosh is another function used in regression tasks that’s smoother than L2 (is smoothed towards large errors (presumably caused by outliers) so that the final error score isn’t impacted thoroughly.)\n",
        "* Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. “Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.” (Grover, 2019). Oops, that’s not intuitive but nevertheless quite important – this is the maths behind Logcosh loss:\n",
        "\n",
        "> $\\log \\cosh (t)=\\sum_{p \\in P} \\log (\\cosh (p-t))$\n",
        "\n",
        "* Similar to Huber Loss, but twice differentiable everywhere\n",
        "* [Wiki Hyperbolic Functions](https://en.m.wikipedia.org/wiki/Hyperbolic_functions), [TF Class](https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh), [Machinecurve](https://www.machinecurve.com/index.php/2019/10/23/how-to-use-logcosh-with-keras/)\n",
        "\n",
        "* However, Log-Cosh is second-order differentiable everywhere, which is still very useful in some machine learning models. For example, XGBoost uses Newton's method to find the best advantage. Newton's method requires solving the second derivative (Hessian). Therefore, for machine learning frameworks such as XGBoost, the second order of the loss function is differentiable. But the Log-cosh loss is not perfect, and there are still some problems. For example, if the error is large, the first step and Hessian will become fixed, which leads to the lack of split points in XGBoost.\n",
        "\n",
        "https://www.programmersought.com/article/86974383768/\n",
        "\n",
        "![logcosh](https://raw.githubusercontent.com/deltorobarba/repo/master/logcosh.jpeg)"
      ],
      "metadata": {
        "id": "vP9h6RbchlyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile Loss (Pinball Loss)**\n",
        "\n",
        "* TLDR: for Heteroskedastizität, i.e. for risk management when variance changes. Quantile Loss, you can set different quantiles to control the proportion of overestimation and underestimation in loss.\n",
        "\n",
        "* estimates conditional “quantile” of a response variable given certain values of predictor variables\n",
        "* is an extension of MAE (**when quantile is 50th percentile, it’s MAE**)\n",
        "* Im Gegensatz zur Kleinste-Quadrate-Schätzung, die den Erwartungswert der Zielgröße schätzt, ist die Quantilsregression dazu geeignet, ihre Quantile zu schätzen.\n",
        "* Fitting models for many percentiles, you can estimate the entire conditional distribution. Often, the answers to important questions are found by modeling percentiles in the tails of the distribution. For that reason **quantile regression provides critical insights in financial risk management & fraud detection**.\n",
        "* [Wikipedia](https://de.m.wikipedia.org/wiki/Quantilsregression), [TF Class](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/PinballLoss) & [TF Function](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/pinball_loss)\n",
        "\n",
        "* project where predictions were subject to high uncertainty. The client required for their decision to be driven by both the predicted machine learning output and a measure of the potential prediction error. The quantile regression loss function solves this and similar problems by replacing a single value prediction by prediction intervals.\n",
        "\n",
        "* The quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times.\n",
        "\n",
        "* For q equal to 0.5, under-prediction and over-prediction will be penalized by the same factor, and the median is obtained. The larger the value of q, the more over-predictions are penalized compared to under-predictions. For q equal to 0.75, over-predictions will be penalized by a factor of 0.75, and under-predictions by a factor of 0.25. The model will then try to avoid over-predictions approximately three times as hard as under-predictions, and the 0.75 quantile will be obtained.\n",
        "\n",
        "* The usual regression algorithm is to fit the expected or median training data, and the quantile loss function can be used to fit different quantiles of training data by giving different quantiles.\n",
        "\n",
        "![sdd](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss.jpg)\n",
        "\n",
        "* Set different quantiles to fit different straight lines: This function is a piecewise functio., γ is the quantile coefficient. y is the true value, f(x) is the predicted value. According to the size of the predicted value and the true value, there are two cases to consider.\n",
        "\n",
        "* y> f(x) For overestimation, the predicted value is greater than the true value;\n",
        "\n",
        "* y< f(x) to underestimate, the predicted value is smaller than the real value.\n",
        "\n",
        "* Use different pass coefficients to control the weight of overestimation and underestimation in the entire loss value.\n",
        "\n",
        "* Especially when γ=0.5 When the quantile loss degenerates into the mean absolute error MAE, **MAE can also be regarded as a special case of quantile loss-median loss**. The picture below is taken with different median points [0.25,0.5,0.7] Obtaining different quantile loss function curves can also be seen as MAE at 0.5.\n",
        "\n",
        "![fgfgf](https://raw.githubusercontent.com/deltorobarba/repo/master/quantileloss2.jpg)\n",
        "\n",
        "![xx](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Pinball_Loss_Function.svg/320px-Pinball_Loss_Function.svg.png)\n",
        "\n",
        "*Pinball-Verlustfunktion mit\n",
        "τ\n",
        "=0,9. Für\n",
        "ε\n",
        "<\n",
        "0 beträgt der Fehler\n",
        "−\n",
        "0\n",
        ",\n",
        "1\n",
        "ε, für\n",
        "ε\n",
        "≥\n",
        "0 beträgt er\n",
        "0\n",
        ",\n",
        "9\n",
        "ε.*\n",
        "\n",
        "* https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/"
      ],
      "metadata": {
        "id": "geqM6xx5hnqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson Loss**\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-103abfddc312\n",
        "\n",
        "* https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459\n"
      ],
      "metadata": {
        "id": "-qbix6xwhpXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Classification (mostly entropy-/ divergence-based or margin-based)*"
      ],
      "metadata": {
        "id": "UvgTnPqOhs3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for classification**\n",
        "\n",
        "* Types: Margin-based, Cross-Entropy-based and Divergence-based\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
        "\n",
        "* https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/#loss-functions-for-classification\n",
        "\n",
        "* https://arxiv.org/pdf/1702.05659.pdf\n",
        "\n",
        "* http://cs229.stanford.edu/extra-notes/loss-functions.pdf"
      ],
      "metadata": {
        "id": "xZOPqB8Bhuad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Margin-based Loss**\n",
        "\n",
        "* Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction.\n",
        "\n",
        "* Instead they penalize predictions based on how well they agree with the sign of the target.\n",
        "\n",
        "* http://juliaml.github.io/LossFunctions.jl/stable/losses/margin/\n",
        "\n",
        "* Methods:\n",
        "\n",
        "  * **Exponential Loss**\n",
        "\n",
        "  * **Hinge Loss** (tf.keras.losses.hinge(y_true, y_pred)): The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "  * **Squared Hinge Loss**: A popular extension of the Hinge Loss is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate."
      ],
      "metadata": {
        "id": "6CYGi1_hhwM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Entropy-based Losses**\n",
        "\n",
        "* the [cross entropy](https://en.m.wikipedia.org/wiki/Cross_entropy) between two probability distributions p and q **over the same underlying set of events** measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "* Binary Cross-Entropy\n",
        "* Conditional entropy\n",
        "* Joint entropy\n",
        "* Cross entropy (Log loss or logistic regression):\n",
        "  * https://en.m.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
        "  * https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83\n",
        "\n",
        "**Method 1: Binary Classification: Cross-Entropy or Log-Loss (Logistic Loss/ negative log-likelihood)**\n",
        "\n",
        "  * It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "  * So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value.  A perfect model would have a log loss of 0.\n",
        "  * Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "  * Cross-entropy is the default loss function to use for binary classification problems.\n",
        "  * It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "  * Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "  * Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "  * The function requires that the output layer is configured with a single node and a ‘sigmoid‘ activation in order to predict the probability for class 1.\n",
        "\n",
        "**Method 2: Multiclass Classification: Sparse Categorical Cross-Entropy**\n",
        "\n",
        "* Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "* In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "* Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "* Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "* A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process.\n",
        "\n",
        "* For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "* Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "> loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "**Cross-Entropy vs KL Divergence vs Logloss**\n",
        "\n",
        "* Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from **KL divergence** that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "* Cross-entropy is also related to and often confused with **logistic loss, called log loss**. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably."
      ],
      "metadata": {
        "id": "nhpMRXmPhyIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divergence-based**\n",
        "\n",
        "> In machine learning, many optimization problems formulated as minimization problems wrt Kullback-Leibler divergence.\n",
        "interpreted as *information projections*, uniqueness projections proved by a generalization of the Pythagoras' theorem.\n",
        "PDF: https://Inkd.in/g9ETtTQp\n",
        "\n",
        "**Kullback-Leibler Divergence (Multiclass)**\n",
        "\n",
        "* [Kullback Leibler Divergence](https://en.m.wikipedia.org/wiki/Kullback–Leibler_divergence), or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "\n",
        "* The only divergence that is both an f-divergence and a Bregman divergence is the Kullback–Leibler divergence\n",
        "\n",
        "* Use for example as **loss function in variational autoencoder**\n",
        "\n",
        "  * https://www.kaggle.com/debanga/statistical-distances\n",
        "\n",
        "  * https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
        "\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to **approximate a more complex function than simply multi-class classification**, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred.\n",
        "\n",
        "* Nevertheless, it can be used for **multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy**.\n",
        "\n",
        "* Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "* A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "* As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "**Jensen–Shannon divergence**\n",
        "\n",
        "* It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance\n",
        "* use in GAN's for example (Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks. NIPS. arXiv:1406.2661. Bibcode:2014arXiv1406.2661G)\n",
        "* https://en.m.wikipedia.org/wiki/Generative_adversarial_network\n",
        "* https://en.m.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "\n",
        "**f-Divergence**\n",
        "\n",
        "* Probabilistic models are often trained by maxi- mum likelihood, which corresponds to minimiz- ing a specific f-divergence between the model and data distribution.\n",
        "\n",
        "* In light of recent suc- cesses in training Generative Adversarial Networks, alternative non-likelihood training crite- ria have been proposed.\n",
        "\n",
        "* https://arxiv.org/pdf/1907.11891.pdf and https://arxiv.org/pdf/1905.12888.pdf\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/F-divergence\n",
        "\n",
        "* The Hellinger distance is a type of f-divergence\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Hellinger_distance\n",
        "\n",
        "* https://www.mis.mpg.de/fileadmin/pdf/geoasp_2008_petz.pdf\n",
        "\n",
        "**Hellinger Distance**\n",
        "\n",
        "* the [Hellinger distance](\n",
        "https://en.m.wikipedia.org/wiki/Hellinger_distance) (closely related to, although different from, the Bhattacharyya distance) is used to **quantify the similarity between two probability distributions**.\n",
        "\n",
        "* **It is a type of f-divergence.**\n",
        "\n",
        "* (?) ist vielleicht sogar eine metric weil es triangle inequality erfüllt.\n",
        "\n",
        "**Bregman Divergence**\n",
        "\n",
        "* In machine learning, [Bregman divergences](\n",
        "https://en.m.wikipedia.org/wiki/Bregman_divergence) are used to calculate the bi-tempered logistic loss, performing better than the softmax function with noisy datasets\n",
        "\n",
        "* The squared Euclidean divergence is a Bregman divergence (corresponding to the function x<sup>2</sup>, but not an f-divergence\n",
        "\n",
        "* COST-SENSITIVE CLASSIFICATION BASED ON BREGMAN DIVERGENCES: https://core.ac.uk/download/pdf/29402554.pdf\n",
        "\n",
        "**Bhattacharyya distance**\n",
        "\n",
        "* In statistics, the [Bhattacharyya distance](\n",
        "https://en.m.wikipedia.org/wiki/Bhattacharyya_distance) measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations.\n",
        "\n",
        "* The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the ***Mahalanobis distance is a particular case of the Bhattacharyya distance** when the standard deviations of the two classes are the same.\n",
        "\n",
        "* Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n",
        "\n",
        "* under certain conditions does not obey the triangle inequality\n",
        "\n",
        "* https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7\n",
        "\n",
        "**Mahalanobis distance**\n",
        "\n",
        "* The [Mahalanobis distance](\n",
        "https://en.m.wikipedia.org/wiki/Mahalanobis_distance) is a measure of the distance between a point P and a distribution D\n",
        "\n",
        "* If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
        "\n",
        "* In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n",
        "\n",
        "* Bregman divergence: **the Mahalanobis distance is an example of a Bregman divergence**\n",
        "\n",
        "* **Bhattacharyya distance related, for measuring similarity between data sets (and not between a point and a data set** - Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same.)\n",
        "\n",
        "* Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.\n",
        "\n",
        "* It is an extremely useful metric having, excellent applications **in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification**.\n",
        "\n",
        "![alternativer Text](https://raw.githubusercontent.com/deltorobarba/repo/master/mahalanobis.jpg)\n",
        "\n",
        "* If the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
        "\n",
        "* The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
        "\n",
        "* This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
        "\n",
        "* **What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution.**"
      ],
      "metadata": {
        "id": "iM9yRTVah0Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Cost Function: Metric Learning (Similarity Learning)*"
      ],
      "metadata": {
        "id": "_cMWr-Beh3wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Learning (Similarity Learning) & Ranking Loss**\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning\n",
        "\n",
        "* https://en.m.wikipedia.org/wiki/Similarity_learning#Metric_learning\n",
        "\n",
        "* https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5\n",
        "\n",
        "* If you'd like some theory with your contrastive losses, the first author of SimCLR and SimCLR v2 Ting Chen and Lala Li (both at Google Brain) have an interesting new paper. https://arxiv.org/pdf/2011.07876.pdf"
      ],
      "metadata": {
        "id": "0kz2AVcBh5at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions for Metric Learning**\n",
        "https://gombru.github.io/2019/04/03/ranking_loss/\n",
        "\n",
        "Ranking Losses are essentialy the ones explained above, and are used in many different aplications with the same formulation or minor variations. However, different names are used for them, which can be confusing. Here I explain why those names are used.\n",
        "\n",
        "* Ranking loss: This name comes from the information retrieval field, where we want to train models to rank items in an specific order.\n",
        "* Margin Loss: This name comes from the fact that these losses use a margin to compare samples representations distances.\n",
        "* Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for * Pairwise Ranking Loss, but I’ve never seen using it in a setup with triplets.\n",
        "* Triplet Loss: Often used as loss name when triplet training pairs are employed.\n",
        "* Hinge loss: Also known as max-margin objective. It’s used for training SVMs for classification. It has a similar formulation in the sense that it optimizes until a margin. That’s why this name is sometimes used for Ranking Losses.\n",
        "* **Triplet loss** is probably the most popular loss function of metric learning. (a loss function for machine learning algorithms) is often used for learning similarity for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning. https://en.m.wikipedia.org/wiki/Triplet_loss\n",
        "* **Contrastive Loss**: Contrastive loss was first introduced in 2005 by Yann Le Cunn et al. in this paper and its original application was in Dimensionality Reduction. Now, if you recall, the general goal of a Dimensionality reduction algorithm can be formulated like this:\n",
        "  * Given a sample (a data point) — a D-dimensional vector, transform this sample into a d-dimensional vector, where d ≪ D, while preserving as much information as possible.\n",
        "  * The difference is that Cross-entropy loss is a classification loss which operates on class probabilities produced by the network independently for each sample, and Contrastive loss is a metric learning loss, which operates on the data points produced by network and their positions relative to each other.\n",
        "  * This is also part of the reason a **cross-entropy loss is not usually used for metric learning tasks** like Face Verification — it doesn’t impose any constraints on the distribution on the model’s inner representation of the given data — i.e. the model can learn any features regardless of whether similar data points would be located closely to each other or not after the transformation.\n",
        "  * for each class/group of similar points (in case of Face Recognition task it would be all the photos of the same person) the **maximum intra-class distance is smaller than the minimum inter-class distance.**\n",
        "  * It operates on pairs of embeddings received from the model and on the ground-truth similarity flag — a Boolean label, specifying whether these two samples are “similar” or “dissimilar”. So the input must be not one, but 2 images.\n",
        "  * It penalizes “similar” samples for being far from each other in terms of Euclidean distance (although other distance metrics could be used).\n",
        "  * “Dissimilar” samples are penalized by being to close to each other, but in a somewhat different way — Contrastive Loss introduces the concept of “margin” — a minimal distance that dissimilar points need to keep. So it penalizes dissimilar samples for beings closer than the given margin.\n",
        "* **Ranking & Learning to Rank**: Ranking.. (triplet loss mit similarity learning wird im ranking verwendet, weil es ordinal ist im ggs zu distance learning..). See also [Ranking (information_retrieval)](https://en.m.wikipedia.org/wiki/Ranking_(information_retrieval)), [Learning_to_rank](https://en.m.wikipedia.org/wiki/Learning_to_rank),\n",
        "* CosineEmbeddingLoss. It’s a Pairwise Ranking Loss that uses cosine distance as the distance metric. Inputs are the features of the pair elements, the label indicating if it’s a positive or a negative pair, and the margin.\n",
        "* MarginRankingLoss. Similar to the former, but uses euclidian distance.\n",
        "* TripletMarginLoss. A Triplet Ranking Loss using euclidian distance.\n",
        "* contrastive_loss. Pairwise Ranking Loss.\n",
        "* triplet_semihard_loss. Triplet loss with semi-hard negative mining."
      ],
      "metadata": {
        "id": "R2dM3lxKh7QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectures: Siamese Nets or Triplet Nets**\n",
        "\n",
        "* Siamese and triplet nets are training setups where Pairwise Ranking Loss and Triplet Ranking Loss are used. But those losses can be also used in other setups.\n",
        "\n",
        "* Siamese nets are built by two identical CNNs with shared weights (both CNNs have the same weights). Each one of these nets processes an image and produces a representation. Those representations are compared and a distance between them is computed. Then, a Pairwise Ranking Loss is used to train the network, such that the distance between representations produced by similar images is small, and the distance between representations of dis-similar images is big.\n",
        "\n",
        "* Triplet nets: The idea is similar to a siamese net, but a triplet net has three branches (three CNNs with shared weights). The model is trained by simultaneously giving a positive and a negative image to the corresponding anchor image, and using a Triplet Ranking Loss. That lets the net learn better which images are similar and different to the anchor image.\n",
        "\n",
        "* Example: Ranking Loss for Multi-Modal Retrieval"
      ],
      "metadata": {
        "id": "vlb98XbTh9Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Learning vs Regression & Classification**\n",
        "\n",
        "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
        "\n",
        "**Classification vs Metric Learning**\n",
        "\n",
        "* **Classification is a “Closed-set” task**. can't add new labels without complete retraining.  The model here is trying to learn separable features in this case — i.e. features, that would allow to assign a label from a predefined set to a given image. The model is trying to find a hyperplane, a rule that separates given classes in space.\n",
        "\n",
        "* **Metric Learning** is a “Open-set” task. This one means that we do indeed have some predefined set of labels for training, but the model can be applied to any unseen data and it should generalize. In this case the model is trying to solve a metric-learning problem: to learn some sort of similarity metric, and for that it needs to extract discriminative features — features that can be used to distinguish between different people on any two (or more) images. The model is trying not to separate images with a hyperplane, but rather reorganize the input space, pull the similar images together in some form of a cluster while pushing dissimilar images away.\n",
        "\n",
        "* This is somewhat reminiscent of clustering problem in Unsupervised Learning — and indeed you can use a model trained on a metric-learning task to create a distance matrix for new data, and than run algorithms like DBSCAN on it to, e.g., cluster images of people’s faces, where each cluster would correspond to a new person.\n",
        "\n",
        "* https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246"
      ],
      "metadata": {
        "id": "5yivnYJoh-uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### *Regularization*"
      ],
      "metadata": {
        "id": "G1TQJG1ufr-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*An L2-regularized version of the cost function used in SGD for NN [Source](https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db)*\n",
        "\n",
        "> $J_{\\text {regularited }}=\\underbrace{-\\frac{1}{m} \\sum_{i=1}^m\\left(y^{(i)} \\log \\left(a^{[L](i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-a^{[L](i)}\\right)\\right)}_{\\text {crossentropy cost }}+\\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum_l \\sum_l \\sum_j W_{i, j}^{[m 2}}_{\\text {L. reguatization cos }}$\n"
      ],
      "metadata": {
        "id": "OCuAdZfBf4BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "* Regularization is a technique for preventing a model from overfitting\n",
        "* Overfitting = a complicated model that gives worse predictions than a simpler model\n",
        "* Solution: e.g. preventing over-fitting by penalizing a model for having large weights (A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output)\n",
        "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called [weight regularization](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/).\n",
        "\n",
        "**Trivial Regularization Approaches**\n",
        "\n",
        "* Add more data\n",
        "* Simpler model (reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data)\n",
        "* Use ensemble models\n",
        "\n",
        "**Particular Regularization Techniques**\n",
        "* Weight regularization\n",
        "* Vectornorm (L1, L2, or Elastic Net): Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but vectornorm regularization is a great alternative when dealing with a large set of features.\n",
        "* Dropout\n",
        "* Jitter (add noise)\n",
        "* Batch size\n",
        "* Early stopping (this is not a formal regularization method, but can effectively limit overfitting).\n",
        "\n",
        "**Overfitting: Consider Variance-Bias-Tradeoff**: [Regularization and Geometry](https://towardsdatascience.com/regularization-and-geometry-c69a2365de19) & [The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)\n",
        "\n",
        "**Benefits of regularization from a mathematical optimization point of view**\n",
        "\n",
        "* Minimize a cost function. Neural networks are non-convex cost functions. Numerical optimization methods (gradient descent) can easily get stuck in local minima (stationary points)\n",
        "\n",
        "* Regularization can be used as a way of ‚convexifying‘ a non-convex cost function.\n",
        "\n",
        "* The L2 regularizer, being an upward-facing convex function, can unflatten flat regions and curve up some stationary points without severely changing the minimum locations (e.g L2 regularized cost no longer has an issue with saddle points, as the region surrounding it has been curved upwards).\n",
        "\n",
        "* Regularization can also help with the optimization of convex machine learning problems, when is not invertible. For example the solution to the L2 regularized version of linear regression is given by is the regularization parameter, which can be set large enough so that becomes invertible.\n",
        "\n",
        "**Theoretical Foundation**\n",
        "\n",
        "Modify cost function J by adding 'preference' to certain parameter values:\n",
        "\n",
        "$J(\\underline{\\theta})=\\frac{1}{2}\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right) \\cdot\\left(\\underline{y}-\\underline{\\theta} \\underline{X}^{T}\\right)^{T}+\\alpha \\theta \\theta^{T}$\n",
        "\n",
        "New solution (derive the same way) - problem is now well-posed for any degree:\n",
        "\n",
        "$\\underline{\\theta}=\\underline{y} \\underline{X}\\left(\\underline{X}^{T} \\underline{X}+\\alpha I\\right)^{-1}$\n",
        "\n",
        "* Shrinks parameters towards zero\n",
        "* Alpha large: we prefer small theta to small MSE\n",
        "* Regularization term is independent of the data: paying more attention reduces variance."
      ],
      "metadata": {
        "id": "cjZzhH2ef5uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 (Lasso) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(Y_{i}-\\sum_{j=1}^{p} X_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|$\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
        "* Learn more on [Google Course](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda): Regularization for Simplicity: Lambda\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n}\\left|u_{i}\\right|=\\sum_{i=1}^{n}\\left|y_{i}-b_{0}-b_{1} x_{i}\\right|$\n",
        "</p><br>\n",
        "\n",
        "\n",
        "$d_{1} \\equiv d_{\\mathrm{SAD}}:(x, y) \\mapsto\\|x-y\\|_{1}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|$\n",
        "\n",
        "* **Synonyms**: Lasso, Manhatten distance, least absolute deviations (LAD method), least absolute errors (LAE)\n",
        "* **Fun Fact**: L1 Regularization is analytical equivalent to Laplacean prior\n",
        "* **Summary**: Sum of the absolute weights. Gives sparse solutions, since it does not take all features. Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "* **Advantages**: less influenced by outliers (robust). Can shrink some coefficients to zero while lambda increases, performing variable selection. generates sparse feature vectors (Sparse: only very few entries in a matrix or vector is non-zero. L1-norm has property of producing many coefficients with zero values or very small values with few large coefficients). Sparse is sometimes good eg. in high dimensional classification problems. sparsity properties: calculation more computationally efficient.\n",
        "* **Disadvantages**: L1 regularization doesn’t easily work with all forms of training. gives a solution with more large residuals, and a lot of zeros in the solution.\n",
        "* **Use Cases**:\n",
        "  * if only a subset of features are correlated with the label, as in lasso model some coefficient can be shrunken to zero.\n",
        "  * very useful when you want to understand exactly which features are contributing to a decision.\n",
        "  * if you can ignore the ouliers in your dataset or you need them to be there.\n",
        "  * use L1 when constraints on feature extraction: easily avoid computing a lot of computationally expensive features  at the cost of some of the accuracy, since the L1-norm will give us a solution which has the weights for a large set of features set to zero (real-time detection or tracking of an object/face/material using a set of diverse handcrafted features with a large margin classifier like an SVM in a sliding window fashion - you'd probably want feature computation to be as fast as possible in this case).\n",
        "* **Bayesian**: L1 usually corresponds to setting a Laplacean prior: Some of the coefficients will shrink to zero: similar effect would be achieved in Bayesian linear regression using a Laplacian prior (strongly peaked at zero) on each of the beta coefficients.\n",
        "\n"
      ],
      "metadata": {
        "id": "mUERgs6Hf8C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 (Ridge) Vectornorm Penalty Term to Cost Function**\n",
        "\n",
        "$\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} x_{i j} \\beta_{j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$\n",
        "\n",
        "* Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
        "* If lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen.\n",
        "\n",
        "<p>\n",
        "$\\sum_{i=1}^{n} u_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-b_{0}-b_{1} x_{i}\\right)^{2}$\n",
        "</p><br>\n",
        "\n",
        "* **Synonyms**: Weight Decay, Ridge Regression, KQ-Methode, kleinste Quadrate, [Tikhonov regularization](https://en.m.wikipedia.org/wiki/Tikhonov_regularization), Euclidean distance, least squares error (LSE)\n",
        "* **Fun Fact**: L2 Regularization is analytically equivalent to Gaussian prior\n",
        "* **Summary**: Sum of the squared weights. Is the most common type of regularization, also called simply “weight decay,” with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.\n",
        "* **Advantages**: Shrinks all the coefficient by the same proportions, but eliminates none. Leads to small distributed weights in neural networks. The L2 regularization heavily penalizes \"peaky\" weight vectors and prefers diffuse weight vectors. Empirically performs better than L1. The fit for L2 will be more precise than L1. Works with all forms of training. Smoother: fewer large residual values along with fewer very small residuals as well. L2-norm has analytical solution - allows the L2-norm solutions to be calculated computationally efficiently.\n",
        "* **Disadvantages**: Sensitive to outliers, since L2 wants all errors to be tiny and heavily penalizes anyone who doesn't obey. Computation heavy compared to the L1 norm. Doesn’t give you implicit feature selection.\n",
        "* **Use Cases**: Use ridge if all the features are correlated with the label, as the coefficients are never zero in ridge.\n",
        "* **Bayesian**: L2 similarly corresponds to Gaussian prior. As one moves away from zero, the probability for such a coefficient grows progressively smaller. The square loss penalty can be seen as putting a Gaussian prior on your weights.\n"
      ],
      "metadata": {
        "id": "sfAtZ0ekf-Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special: Analytical Equivalence**\n",
        "\n",
        "* Why is L2 Regularization is analytically equivalent to Gaussian prior?\n",
        "\n",
        "* https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior/163450#163450\n",
        "\n",
        "\n",
        "* Method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, at the \"only\" cost of introducing another hyperparameter to tune (see Hastie's paper on stanford.edu).\n",
        "* Overcome limitations of L1: in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others.\n",
        "* Solution in elastic net: add quadratic part to penalty (L2). quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum.\n",
        "* Naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed λ2 it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by (1+λ2)."
      ],
      "metadata": {
        "id": "Dnq1CBbfgAFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "\n",
        "* Ziel: Overfitting vermeiden\n",
        "* Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
        "* Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less. With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
        "At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average). Two papers that explain this much better are:\n",
        "* Hinton et al, [1207.0580] Improving neural networks by preventing co-adaptation of feature detectors, 2012 (probably the original paper on dropout)\n",
        "* Warde-Farley et al, [1312.6197] An empirical analysis of dropout in piecewise linear networks, 2014 (analyzes dropout specially for the case of using ReLU as activation function -arguably the most popular- , and checks the behavior of the geometric mean for ensemble averaging).\n",
        "* Andrew Ng: dropout is nothing more than an adaptive form of L2 regularization and that both methods have similar effects\n",
        "* The dropout will randomly mute some neurons in the neural network and we therefore have a sparse network which hugely decreases the possibility of overfitting. More importantly, the dropout will make the weights spread over the input features instead of focusing on some features. https://hackernoon.com/is-the-braess-paradox-related-to-dropout-in-neural-nets-270ecb97cdeb https://de.m.wikipedia.org/wiki/Dropout_(künstliches_neuronales_Netz)\\\n",
        "\n",
        "**Is dropout outdated?**\n",
        "\n",
        "Neural Network:  Dropout\n",
        "\n",
        "https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n",
        "\n",
        "Don’t Use Dropout in Convolutional Networks\n",
        "https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16\n",
        "\n",
        "Instead you should insert batch normalization between your convolutions. This will regularize your model, as well as make your model more stable during training.\n",
        "\n",
        "First, dropout is generally less effective at regularizing convolutional layers: The reason? Since convolutional layers have few parameters, they need less regularization to begin with. Furthermore, because of the spatial relationships encoded in feature maps, activations can become highly correlated. This renders dropout ineffective. ([Source](https://www.reddit.com/r/MachineLearning/comments/5l3f1c/d_what_happened_to_dropout/))\n",
        "\n",
        "Second, what dropout is good at regularizing is becoming outdated: Large models like VGG16 included fully connected layers at the end of the network. For models like this, overfitting was combatted by including dropout between fully connected layers. Unfortunately, [recent architectures](https://arxiv.org/pdf/1512.03385.pdf) move away from this fully-connected block. By replacing dense layers with global average pooling, modern convnets have reduced model size while improving performance.\n",
        "\n",
        "**Use Dropout along with L1/L2 Regularization?**\n",
        "\n",
        "* You can, but it is still not clear whether using both at the same time acts synergistically or rather makes things more complicated for no net gain.\n",
        "* While ℓ 2 regularization is implemented with a clearly-defined penalty term, dropout requires a random process of “switching off” some units, which cannot be coherently expressed as a penalty term and therefore cannot be analyzed other than experimentally.\n",
        "* they both try to avoid the network’s over-reliance on spurious correlations, which are one of the consequences of overtraining that wreaks havoc with generalization. But more detailed research is necessary to determine whether and when they can “work together” or rather end up “fighting each other”. So far, it seems the results tend to vary in a case-by-case fashion. Using both can increase accuracy: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf (Hinton paper 2014)"
      ],
      "metadata": {
        "id": "dC-z_kHhgB8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jitter (Noise)**\n",
        "\n",
        "* adding annealed Gaussian noise by decaying the variance works better than using fixed Gaussian noise"
      ],
      "metadata": {
        "id": "DW1FugwRgDxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization & Size**\n",
        "\n",
        "* https://towardsdatascience.com/understanding-batch-normalization-for-neural-networks-1cd269786fa6\n",
        "\n",
        "* Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003), perhaps due to the noise they add to the learning process.\n",
        "\n",
        "* Using a smaller batch size is like using some regularization to avoid converging to sharp minimizers. The gradients calculated with a small batch size are much more noisy than gradients calculated with large batch size, so it's easier for the model to escape from sharp minimizers, and thus leads to a better generalization. Generalization error is often best for a batch size of 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.)\n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        "* Batch normalization is another method to regularize a (convolutional) network.\n",
        "* On top of a regularizing effect, batch normalization also gives your convolutional network a resistance to vanishing gradient during training. This can decrease training time and result in better performance.\n",
        "* Batch Normalization Combats Vanishing Gradient\n",
        "* Batch normalization replaces dropout.\n",
        "* Even if you don’t need to worry about overfitting there are many benefits to implementing batch normalization. Because of this, and its regularizing effect, batch normalization has largely replaced dropout in modern convolutional architectures.\n",
        "* “We presented an algorithm for constructing, training, and performing inference with batch-normalized networks. The resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require Dropout for regularization.” -[Ioffe and Svegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "Why use batches?\n",
        "To avoid that small datasets increase overfitting to this datasets and worsen overall accuracy. But batch size shouldnt be too big either (computation time, speed of convergence of an algorithm)\n",
        "\n",
        "* Research 1: a low batch size means a very noisy gradient (because computed on a very small subset of the dataset), and a high learning rate means noisy steps.\n",
        "https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914\n",
        "\n",
        "* Research 2: How do you choose your batch size in deep learning/SGD? - An interesting concept so-called \"generalization gap\": Train longer, generalize better: closing the generalization gap in large batch training of neural networks:\n",
        "https://arxiv.org/abs/1705.08741\n",
        "\n",
        "**Covariate Shift**\n",
        "\n",
        "pending...\n",
        "\n"
      ],
      "metadata": {
        "id": "PokQre2wgFWs"
      }
    }
  ]
}